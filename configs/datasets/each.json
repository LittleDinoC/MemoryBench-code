{
    "Locomo-0": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-1": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-2": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-3": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-4": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-5": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-6": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-7": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-8": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "Locomo-9": {
        "dataset_name": "Locomo-0",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/Locomo/locomo10.json",
        "test_metrics": ["f1"],
        "max_output_len": 8192,
        "class_name": "Locomo.Locomo_Dataset"
    },
    "DialSim-friends": {
        "dataset_name": "DialSim-friends",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/DialSim",
        "dataset_size": 300,
        "test_metrics": ["accuracy"],
        "max_output_len": null,
        "class_name": "DialSim.DialSim_Dataset"
    },
    "DialSim-bigbang": {
        "dataset_name": "DialSim-bigbang",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/DialSim",
        "dataset_size": 300,
        "test_metrics": ["accuracy"],
        "max_output_len": null,
        "class_name": "DialSim.DialSim_Dataset"
    },
    "DialSim-theoffice": {
        "dataset_name": "DialSim-theoffice",
        "task_tag": "Long-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/DialSim",
        "dataset_size": 300,
        "test_metrics": ["accuracy"],
        "max_output_len": null,
        "class_name": "DialSim.DialSim_Dataset"
    },
    "JuDGE": {
        "dataset_name": "JuDGE",
        "task_tag": "Short-Long",
        "domain_tag": "Legal",
        "data_path": "./raw/JuDGE",
        "test_metrics": ["reasoning_meteor", "judge_meteor", "reasoning_bert_score", "judge_bert_score", "crime_recall", "crime_precision", "crime_f1", "penalcode_index_recall", "penalcode_index_precision", "penalcode_index_f1", "time_score", "amount_score"],
        "max_output_len": 2048,
        "class_name": "JuDGE.JuDGE_Dataset"
    },
    "LexEval-Summarization": {
        "dataset_name": "LexEval-Summarization",
        "task_tag": "Long-Short",
        "domain_tag": "Legal",
        "data_path": "./raw/LexEval/5_1.json",
        "test_metrics": ["rougel"],
        "max_output_len": 8192,
        "class_name": "LexEval.LexEval_Dataset"
    },
    "LexEval-Judge": {
        "dataset_name": "LexEval-Judge",
        "task_tag": "Long-Long",
        "domain_tag": "Legal",
        "data_path": "./raw/LexEval/5_2.json",
        "test_metrics": ["rougel"],
        "max_output_len": 8192,
        "class_name": "LexEval.LexEval_Dataset"
    },
    "LexEval-QA": {
        "dataset_name": "LexEval-QA",
        "task_tag": "Short-Short",
        "domain_tag": "Legal",
        "data_path": "./raw/LexEval/5_4.json",
        "test_metrics": ["rougel"],
        "max_output_len": 8192,
        "class_name": "LexEval.LexEval_Dataset"
    },
    "WritingBench-Politics&Law": {
        "dataset_name": "WritingBench-Politics&Law",
        "task_tag": "Long-Long",
        "domain_tag": "Legal",
        "data_path": "./raw/WritingBench/benchmark_all.jsonl",
        "test_metrics": ["score"],
        "critic_model_path": "AQuarterMile/WritingBench-Critic-Model-Qwen-7B",
        "max_output_len": 16000,
        "class_name": "WritingBench.WritingBench_Dataset"
    },
    "HelloBench-Academic&Knowledge-QA": {
        "dataset_name": "HelloBench-Academic&Knowledge-QA",
        "task_tag": "Short-Long",
        "domain_tag": "Academic&Knowledge",
        "data_path": "./raw/HelloBench",
        "test_metrics": ["avg_score"],
        "max_output_len": 16384,
        "class_name": "HelloBench.HelloBench_Dataset"
    },
    "HelloBench-Academic&Knowledge-Writing": {
        "dataset_name": "HelloBench-Academic&Knowledge-Writing",
        "task_tag": "Long-Long",
        "domain_tag": "Academic&Knowledge",
        "data_path": "./raw/HelloBench",
        "test_metrics": ["avg_score"],
        "max_output_len": 16384,
        "class_name": "HelloBench.HelloBench_Dataset"
    },
    "IdeaBench": {
        "data_path": "./raw/IdeaBench",
        "task_tag": "Long-Short",
        "domain_tag": "Academic&Knowledge",
        "num_ref": 3,
        "all_ref": false,
        "bert_score_model": "microsoft/deberta-xlarge-mnli",
        "test_metrics": ["bert_score", "llm_rating_score", "llm_novelty_ranking_score", "llm_feasibility_ranking_score"],
        "max_output_len": 8192,
        "class_name": "IdeaBench.IdeaBench_Dataset"
    },
    "JRE-L": {
        "dataset_name": "JRE-L",
        "task_tag": "Short-Short",
        "domain_tag": "Academic&Knowledge",
        "data_path": "./raw/JRE-L/test.json",
        "bert_score_model": "roberta-base",
        "test_metrics": ["Rouge-L", "BERTScore-F1", "CLI", "FKGL", "DCRS"],
        "max_output_len": 4096,
        "class_name": "JRE-L.JRE_L_Dataset"
    },
    "LimitGen-Syn": {
        "dataset_name": "LimitGen-Syn",
        "task_tag": "Long-Short",
        "domain_tag": "Academic&Knowledge",
        "data_path": "./raw/LimitGen",
        "test_metrics": ["accuracy", "rating"],
        "max_output_len": 500,
        "class_name": "LimitGen.LimitGen_Dataset"
    },
    "WritingBench-Academic&Engineering": {
        "dataset_name": "WritingBench-Academic&Engineering",
        "task_tag": "Long-Long",
        "domain_tag": "Academic&Knowledge",
        "data_path": "./raw/WritingBench/benchmark_all.jsonl",
        "test_metrics": ["score"],
        "critic_model_path": "AQuarterMile/WritingBench-Critic-Model-Qwen-7B",
        "max_output_len": 16000,
        "class_name": "WritingBench.WritingBench_Dataset"
    },
    "HelloBench-Creative&Design": {
        "dataset_name": "HelloBench-Creative&Design",
        "task_tag": "Long-Long",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/HelloBench",
        "test_metrics": ["avg_score"],
        "max_output_len": 16384,
        "class_name": "HelloBench.HelloBench_Dataset"
    },
    "WritingPrompts": {
        "dataset_name": "WritingPrompts",
        "task_tag": "Short-Long",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/WritingPrompts/test-00000-of-00001-16503b0c26ed00c6.parquet",
        "test_metrics": ["meteor"],
        "max_output_len": 8192,
        "class_name": "WritingPrompts.WritingPrompts_Dataset"
    },
    "WritingBench-Creative&Design": {
        "dataset_name": "WritingBench-Creative&Design",
        "task_tag": "Long-Long",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/WritingBench/benchmark_all.jsonl",
        "test_metrics": ["score"],
        "critic_model_path": "AQuarterMile/WritingBench-Critic-Model-Qwen-7B",
        "max_output_len": 16000,
        "class_name": "WritingBench.WritingBench_Dataset"
    },
    "NFCats": {
        "dataset_name": "NFCats",
        "task_tag": "Short-Short",
        "domain_tag": "Open-Domain",
        "data_path": "./raw/NFCats/test.csv",
        "test_metrics": ["score"],
        "max_output_len": 8192,
        "class_name": "NFCats.NFCats_Dataset"
    }
}