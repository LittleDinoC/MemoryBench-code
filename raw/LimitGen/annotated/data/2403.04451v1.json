{
    "title": "Membership Inference Attacks and Privacy in Topic Modeling",
    "abstract": "Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Deep learning models’ propensity to memorize training data presents many privacy concerns. Notably, large language models are particularly susceptible to privacy attacks that exploit memorization [6  ###reference_b6###, 5  ###reference_b5###]. In this paper, we aim to investigate whether simpler probabilistic models, such as topic models, raise similar concerns. Our specific focus lies on examining probabilistic topic models like Latent Dirichlet Allocation (LDA) [2  ###reference_b2###].\nTopic modeling is an unsupervised machine learning (ML) method that aims to identify underlying themes or topics within a corpus. Despite the recent successes of large language models (LLMs), Probabilistic topic models are still widely used due to their interpretability and straightforward implementation for text analysis. Furthermore, topic models continue to be developed for a variety of applications and downstream tasks like document classification, summarization, or generation [3  ###reference_b3###, 32  ###reference_b32###].\nResearchers apply topic models across a variety of domains where privacy concerns arise. For example, many studies in the medical domain use topic models to analyze data sets with sensitive attributes [21  ###reference_b21###]. Additionally, topic models are widely used for various government or national defense applications. For instance, researchers applied LDA to better understand Twitter activity aimed at discrediting NATO during the Trident Juncture Exercises in 2018 [29  ###reference_b29###]. To ensure ethical and responsible use of ML, it is crucial to consider the privacy implications associated with topic modeling.\nProbabilistic topic models like LDA serve as a basic Bayesian generative model for text. Neural topic models and language models can learn complex generative processes for text based on observed patterns in the training data. However, LDA requires the generative process for documents to be explicitly defined a priori with far fewer parameters and a Bag-of-Words (BoW) text representation. Investigating LDA’s susceptibility to privacy attacks provides broader context for the privacy vulnerabilities researched in contemporary ML models. Additionally, our work informs practitioners who may opt to use simpler topic models under an unjustified impression that they do not share the same vulnerabilities of LLMs.\nTo explore the privacy in topic modeling, we conduct membership inference attacks (MIAs) which infer whether or not a specific document was used to train LDA. We propose an attack based on an LDA-specific query statistic designed to exploit memorization. This query statistic is integrated into the Likelihood Ratio Attack (LiRA) framework introduced by Carlini et al. [4  ###reference_b4###]. We show that our attack can confidently infer the membership of documents included the training data of LDA which indicates that the privacy risks in generative modeling are not restricted to large neural models.\nTo mitigate such vulnerabilities, we explore differential privacy (DP) for topic modeling. DP acts a direct defense against MIAs by providing a statistical guarantee that the output of some data analysis is indistinguishable regardless of any one user’s inclusion in the analysis. While several DP topic modeling algorithms in the literature attempt to protect individual privacy, previous works largely disregard the privacy of the model’s accompanying vocabulary set [33  ###reference_b33###, 19  ###reference_b19###, 18  ###reference_b18###, 9  ###reference_b9###, 22  ###reference_b22###, 31  ###reference_b31###]. Instead, they consider the vocabulary set as given or public information. However, in practice, the vocabulary set is derived from the training data and can leak sensitive information. We propose an algorithm for DP topic modeling provides privacy in both the vocabulary selection and learning method. By incorporating DP vocabulary selection into the private topic modeling workflow, our algorithm enhances privacy guarantees and bolsters defenses against the LiRA, while having minimal impact on practical utility."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Notation",
            "text": "To begin, let us establish notation and provide a formal definition of topic models.\nFor a vocabulary size of  and  topics, a topic model  is a matrix whose rows sum to 1 representing each topic as a distribution over words.\nWe let  denote the process of learning the model parameterized by latent variables in  on a corpus  with  documents drawn from the underlying data distribution . The function  incorporates the learning algorithm to estimate ’s latent variables and returns the learned topic model . Our definition assumes that the primary objective of topic modeling is to estimate . While  is also associated with other latent variables,  best achieves the goals associated with topic modeling by summarizing the relevant themes in the corpus."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Latent Dirichlet Allocation",
            "text": "Blei et al. [2  ###reference_b2###] define LDA where each document is assumed to contain a mixture of topics. LDA assumes that each document  is represented by a -dimensional document-topic distribution , and the entire corpus is represented by  topic-word distributions . Furthermore, it models each word in a document as generated by sampling a topic from the document’s topic distribution and then sampling a word from the chosen topic’s distribution over words.\nThe process of estimating  presents a Bayesian inference problem typically solved using collapsed Gibbs sampling or variational inference [14  ###reference_b14###, 15  ###reference_b15###]. Each entry  represents the probability of drawing word  from topic . The entire topic model  represents the likelihood of each word appearing in each topic, which can be used to estimate the document-topic distribution  for any given document."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Memorization and Privacy",
            "text": "Understanding memorization in machine learning is critical for trustworthy deployment. Neural model’s large parameter space and learning method introduce training data memorization which increases with model size or observation duplication [5  ###reference_b5###, 12  ###reference_b12###]. Memorization introduces vulnerabilities to attacks on privacy, such as membership inference, attribute inference or data extraction attacks [25  ###reference_b25###, 27  ###reference_b27###, 6  ###reference_b6###].\nSchofield et al. [23  ###reference_b23###] note that document duplication in topic models concentrates the duplicated document’s topic distribution  over less topics, and briefly refer to this phenomenon as memorization. Their findings are consistent with studies on memorization and text duplication in large language models [5  ###reference_b5###]. In this work, we investigate the memorization and privacy of topic models using membership inference attacks (MIA)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Membership Inference Attacks",
            "text": "In an MIA the adversary learns if a specific observation appeared in the model’s training data [16  ###reference_b16###, 25  ###reference_b25###]. These attacks serve as the foundation for other attacks like data extraction, and could violate privacy alone if the adversary learns than an individual contributed to a data set with a sensitive global attribute (i.e. a data set containing only patients with disease X).\nPrior to attacking ML models, researchers explored exploiting aggregate statistics released in genome-wide association studies (GWAS) [16  ###reference_b16###]. Shokri et al. [25  ###reference_b25###] introduced one of the first MIAs on ML models.\nWhile most MIAs exploit deep learning models, few studies investigate topic models. Huang et al. [18  ###reference_b18###] propose three simple MIAs to evaluate their privatized LDA learning algorithm. However, they evaluate their attacks using precision and recall and fail to show that their attack confidently identify members of the training data.\nTo evaluate an MIA’s ability to confidently infer membership, we examine the attack’s true positive rate (TPR) at low false positive rates (FPR) [4  ###reference_b4###]. Receiver operator characteristic (ROC) and area under the curve (AUC) analysis is useful, but we must be wary of the axes. To accurately capture the TPR at the low FPRs of interest, we visualize ROC curves using log-scaled axes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MIAs Against Topic Models",
            "text": "In this section, we detail our MIA against topic models based on the LiRA framework, and empirically demonstrate its effectiveness."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Threat Model",
            "text": "We consider a threat model where the adversary has black-box access to the learned topic-word distribution , but not other latent variables captured by . The adversary can not observe intermediate word counts, learning method, or hyper-parameters used while learning . We also assume that the adversary has query access to the underlying data distribution , which allows them to train shadow models on data sets drawn from .\nUnder the typical query model, we could consider queries as a request to predict a document-topic distribution  given a document. However,  would hold little meaning without access to . Furthermore, a clever adversary could query many specially selected documents to reconstruct . Therefore, we remove this level of abstraction and assume that the adversary is given direct access to  as the output of our topic model."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Attack Framework",
            "text": "In the LiRA framework of Carlini et al. [4  ###reference_b4###], the adversary performs hypothesis testing to determine whether or not a target document  was in the training data. Specifically, we consider  and  where  is the distribution of  learned with , and  is the distribution of  learned without . Given an observed , would like to construct the likelihood ratio test statistic  as follows:\nwhere  is the probability density function of  under  [4  ###reference_b4###].\nHowever, the ration in Equation 1  ###reference_### is infeasible to calculate directly because all  represent a set of  -dimensional distributions. Instead, the adversary applies a carefully chosen query statistic  on all  to reduce the problem to 1-dimension. The test proceeds by calculating the probability density of  under estimated normal distributions from  and :\nwhere  and  are the mean and variance of  and .\nIn the online LiRA, the adversary must train  shadow topic models with and without  to estimate  and . Appendix A  ###reference_### contains the full online LiRA algorithm. To ease the computational burden of training  shadow topic models for each target document, Carlini et al. propose an offline variant of the LiRA. In the offline LiRA, the adversary can evaluate any  after learning a collection of  shadow topic models once by comparing  to a normal estimated from  with a one-sided hypothesis test. Appendix B  ###reference_### contains the full offline LiRA algorithm.\nDesigning  is crucial to the attack’s success. In supervised learning scenarios, the adversary can directly query the model and use the loss of an observation to inform the statistic [4  ###reference_b4###]. However, because topic models are unsupervised learning tasks, the adversary encounters the challenge of identifying an informative statistic that can be computed efficiently using . Hence,  must be careful tailored to topic models for the attack to be effective."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Designing an Effective Query Statistic",
            "text": "Effective model query statistics for the LiRA framework must satisfy the following criteria:\nThe statistic should increase for  when  is included in the training data () to enable the offline LiRA.\nThe estimated distributions  and  are approximately normal to allow for parametric modeling.\nIdeally, the statistic should also be related to how the model may memorize the training data to provide an intuitive interpretation for attack performance.\nFirst, we consider statistics from the simple MIAs on LDA presented by Huang et al. [18  ###reference_b18###]. They propose three statistics derived from the target document’s estimated topic distribution : the entropy of , the standard deviation of , and the maximum value in . The intuition behind their chosen statistics comes from the observation that a document’s topic distribution tends to concentrate in a few topics when included (or duplicated) in the training data [23  ###reference_b23###]. However, Huang et al. apply global thresholds to these statistics instead of using them as query statistics to derive the LiRA test statistic as in Equation 2  ###reference_###.\nWe propose an attack that leverages the LiRA framework with an improved query statistic to directly exploit LDAs generative process. Our query statistic is a heuristic for the target document’s log-likelihood under LDA. To compute this statistic on  for a given document , the adversary maximizes the log-likelihood of the document over all possible document-topic distributions such that:\nwhere we optimzie over  s.t. . In practice, we use SciPy’s out-of-the-box optimization methods to estimate  [30  ###reference_b30###].\nThe intuition behind our statistic is that a document is more likely generated by the target model when included the training data. Therefore, using  allows us to reason about memorization in topic models. We verify our criteria for the proposed statistic in Appendix D  ###reference_###. Additionally, Appendix D  ###reference_### contains an evaluation of other candidate statistics where we show that our statistic significantly outperforms other statistics based on [18  ###reference_b18###]\nThe LiRA with statistic  directly exploits memorization and per-example hardness. The attack accounts for the natural differences in  on various documents by estimating the likelihood ratio under distributions based on  and . This enables the LiRA to outperform simple attacks like in [18  ###reference_b18###] that use global thresholds on the other query statistics based on . Therefore, we propose the LiRA with statistic  as a stronger alternative to attack topic models."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Memorization and Per-Example Hardness",
            "text": "To verify aspects of memorization and per-example hardness for probabilistic topic models, we estimate and visualize  and  in an experiment similar to [4  ###reference_b4###] and [12  ###reference_b12###]. Consistent with their findings, we show that outlying and hard-to-fit observations tend to have a large effect on the learned model when included in the training set for LDA. The histograms in Figure 1  ###reference_### display the estimated distributions for various types of documents.\n###figure_1### When included in the training data for , longer and outlying documents increase . We note that the word probability  in certain topics dramatically changes for words that appear infrequently in  or words that occur many times in . Together, these factors affect the learned topic model  and shift .\nThe simple fact that model is more likely to generate specific documents after inclusion in the training data hints toward memorization. Because the generative process for documents in LDA is extremely simple compared to language models, they do not learn verbatim sequences of words. Instead, topic model’s ability to “memorize\" the training data is due to learning based on word co-occurrences.\nLong document’s are inherently harder to fit than shorter documents. Because the log-likelihood for a document  is the sum of independent log-probabilities for generating each word in , the magnitude of  is naturally higher for longer documents. Additionally, the model may struggle to capture coherent topic structures for longer documents because they tend to contain words from a wider range of topics.\nThe LiRA turns our observations on memorization and per-example hardness into a MIA. When the words in a document become more common in the training set,  will reflect the new word co-occurrences in a few specific topics which tends to increase . This effect is amplified when the document contains many rare words from the vocabulary set. Therefore, the attacker can easily differentiate between  and  for long and outlying documents."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Attack Evaluation Set-Up",
            "text": "We evaluate our attack against three data sets: TweetRumors, 20Newsgroup and NIPS111The source for each dataset is included in the Availability section. To initiate the attack, we randomly sample half of the data to learn and release. Next, we train shadow topic models by repeatedly sampling half of the data set to simulate sampling from. In this scenario, the shadow model training data and the target model’s training data likely overlap. This is a strong assumption made to accommodate the smaller size of our data sets. As observed by [4 ###reference_b4###], we do not expect attack performance to drop significantly using disjoint data sets. We compare our LiRA against each of the attacks presented by [18 ###reference_b18###]. To replicate their attacks, we randomly sample half of the data set to learn. Using, we estimate each documents’ topic-distribution and compute the maximum posterior, standard deviation, and entropy on. We directly threshold each of these statistics to evaluate membership for every document in the data set. For each experiment, we learn using scikit-learn’s implementation of LDA with default learning parameters.333https://scikit-learn.org/ ###reference_scikit-learn.org/### For TweetSet we set the number of topics, 20Newsgroup we set, and for NIPS we vary. We learn shadow models, replicate each experiment 10 times and report our results across all iterations. We interpret as a predicted membership score where a higher value indicates that the document is more likely to be a member of the training data. We empirically estimate the attack’s TPR at low FPRs and plot the attack’s ROC curve on log-scaled axes."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Attack Evaluation Results",
            "text": "First, we compare our online attack performance against the attacks in [18  ###reference_b18###] at an FPR of 0.1% in Table 1  ###reference_###. Figure 2  ###reference_### displays the ROC curves for the online and offline variant of our attacks. Table 1  ###reference_### and Figure 2  ###reference_### demonstrate that our LiRA outperforms each of attacks in [18  ###reference_b18###].\nBecause the LiRA considers the likelihood-ratio of , it accounts for per-example hardness and dominates the attack in Huang et al [18  ###reference_b18###] at all FPR’s. As noted by [4  ###reference_b4###], attacks that directly apply global thresholds do not consider document level differences on the learned model and fail to confidently identify members of the training data. Stronger attacks, like our LiRA, should be used to empirically evaluate the privacy associated with topic models.\n###figure_2### To understand how the number of topics in  influence attack performance, we vary the number of topics  on NIPS and attack the resulting model. Figure 3  ###reference_### shows the ROC of the offline attack while varying . As  increases, we see that TPR increases at all FPRs. Furthermore, based on the Table 2  ###reference_###, we note minor differences between online and offline attack performance.\n###figure_3### The number of parameters in  grows linearly by the length of the vocabulary set as  increases. With more topics and more parameters, document’s word co-occurrence structure is typically better represented in the document’s learned topic distribution. Consequently, increasing  enhances the impact of including the document in the training data on the likelihood of a document resulting in better attack performance.\nOverall, our findings demonstrate that despite their simple architecture, topic models exhibit behavior that resembles memorization and are vulnerable to strong MIAs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Private Topic Modeling",
            "text": "Designing private ML solutions is an active field of research. In topic modeling, the literature is largely focused on differential privacy (DP). DP acts as a direct defense against privacy attacks like MIAs by limiting the effect one data point can have on the learned model. Furthermore, DP provides a clear, quantifiable method for reasoning about privacy loss."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Differential Privacy",
            "text": "DP provides strong theoretical guarantees of individual-level privacy by requiring the output of some data analysis to be indistinguishable (with respect to a small multiplicity factor) between any two adjacent data sets  and .\nLet  be a randomized algorithm. For any  and , we say that  is -differentially private if for all neighboring databases  and every\nThe notion of adjacency is critical. For any text data set, we say that two corpora  and  are author-level adjacency if they differ by one author’s documents. This notion enforces DPs promise of individual level privacy by considering adjacency at the user level.\nIf two corpora  and  differ by one document, then adjacency is at the document-level. This relaxed notion of adjacency can satisfy author-level adjacency if we assume that each document has a unique author. For corpora the most relaxed notion of adjacency is word-level adjacency. Two corpora  and  are word-level adjacent if they differ by one word in one document.\nTo achieve differential privacy we must add noise whose magnitude is scaled to the worst-case sensitivity of the target statistics from all adjacent data sets. Researchers have explored word-level adjacency to control sensitivity, but this approach offers weak privacy guarantees. For instance, most DP collapsed Gibbs sampling algorithms for learning LDA rely on word-level adjacency [33  ###reference_b33###, 19  ###reference_b19###, 18  ###reference_b18###].\nThere are a variety of other algorithms for DP topic modeling: [22  ###reference_b22###] propose DP stochastic variational inference to learn LDA, [9  ###reference_b9###] use a privatized spectral algorithm to learn LDA, and [31  ###reference_b31###] satisfy DP by adding noise directly to . These algorithms satisfy various notions of DP and use differing notions of adjacency. Table 5  ###reference_### in Appendix E  ###reference_### summarizes each DP topic modeling implementation.\nWhile each DP topic modeling algorithm has their advantages and disadvantages, there are some common themes across implementations. First, the iterative nature of learning algorithms for LDA posses a difficult composition issue when managing the privacy loss. Next, many implementations use relaxed notions of adjacency to control sensitivity. Finally, none of the existing methods address the privacy concerns with releasing the vocabulary set corresponding to ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "DP Vocabulary Selection",
            "text": "Releasing the topic-word distribution  without ensuring the privacy of the accompanying vocabulary set can lead to privacy violations. Topic models with comprehensive vocabulary sets tend to include many infrequently used words, and section 3.4  ###reference_### shows that documents with infrequently used words are more vulnerable to MIAs. Furthermore, even if the values in  are private, the overall release of the topic-word distribution can not satisfy DP because the vocabulary set accompanied by  is not private.\nTo illustrate this point, consider a scenario where we have a corpus  with a single document . If we apply DP topic modeling to release  without changing the vocabulary set, then we release all of the words in . This clearly compromises the privacy of the document in . If we chose not to release the vocabulary set,  loses practical interpretability. To address this issue, it is necessary to explore methods for differentially private vocabulary selection.\nDP vocabulary selection can be formalized as the DP Set-Union (DPSU) [13  ###reference_b13###]. In the DPSU each author  contributes a subset  of terms in their documents, and our goal is to design a -DP algorithm to release a vocabulary set  such that the size of  is as large as possible. Carvalho et al. propose a solution for DPSU which is particularly well fit for vocabulary selection because their method allows for one user to contribute the same term multiple times [7  ###reference_b7###]."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Fully Differentially Private Topic Modeling",
            "text": "We propose a high-level procedure for fully DP topic modeling (FDPTM), in Algorithm 1  ###reference_###. FDPTM composes the privately selected vocabulary set  and the private topic-word distribution . Theorem 4.1  ###reference_theorem1### states the privacy guarantees associated with FDPTM. The proof is deferred to Appendix F  ###reference_###.\nIf  for selecting the vocabulary set satisfies -DP and  for topic modeling satisfies -DP, then the overall release of  satisfies -DP.\nImplementing FDPTM requires a few key considerations. First, the process depends on carefully tuning the privacy parameters and other parameters within  and . Second, the data curator must ensure that  and  satisfy the same notion of differential privacy and adjacency.\nOverall, FDPTM is a modular framework for releasing meaningful topic-word distributions. Although our evaluations focus on LDA, other topic models for specific uses can easily fit into the proposed procedure."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "FDPTM Evaluation Set-Up",
            "text": "We empirically evaluate our FDPTM using the TweetRumors data set with . We ensure author-level privacy via document-level adjacency under the assumption that each document has a unique author. For vocabulary selection we use the DPSU solution proposed in [7  ###reference_b7###]. We use the DP LDA learning algorithm proposed by [34  ###reference_b34###]. For DPSU, we fix the privacy parameter  and choose the cut-off value  with the parameter , following [7  ###reference_b7###].\nTo evaluate utility, we first vary the privacy loss parameter for DPSU and analyze utility when LDA is non-private. We study the interaction between DP vocabulary selection and DP LDA by fix the privacy loss parameter for one mechanism and vary the privacy loss parameter for the other mechanism. Specifically, we first fix  for DP LDA and vary  for DPSU. Then, we fix  and vary .\nWe evaluate the utility of the FDPTM using topic coherence [20  ###reference_b20###], which can be a useful proxy for measuring the interpretability of topics. Topic coherence for a topic  is defined as\nwhere  is the document frequency of word  (i.e. the number of documents where word  occurs),  is the co-document frequency of words  and  (i.e. the number of documents where  and  both occur), and  is list of the  most probable words in topic . In our analysis, we select each topic’s top  words and report the average across each topic.\nWe also test FDPTM against the online LiRA to empirically evaluate the privacy of FDPTM. For each attack we learn 64 shadow models. Like before, we conduct attacks while varying  and fixing . Then, we fix  and vary . We repeat each experiment 10 times and report the results across each iteration."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "FDPTM Evaluation Results",
            "text": "Figure 4  ###reference_### displays topic coherence as we increase . Figure 5  ###reference_### displays topic coherence as we vary either  and hold the other constant. We include the average vocabulary size as we increase  in Appendix G  ###reference_###.\n###figure_4### ###figure_5### Topic coherence remains stable after , suggesting that increasing the privacy loss for DPSU may not affect topic coherence after a certain point. When comparing coherence while increasing , we see that  decreases coherence regardless of . In Figure 5  ###reference_###, coherence plateaus around  while increasing  continues to boost coherence. Consequently, increasing  yields diminishing utility returns faster.\nFigure 6  ###reference_### presents the ROC curves that explore the how the interaction between DP vocabulary selection and DP LDA affect attack performance. We gain some empirical privacy under FDPTM by decreasing LiRA performance at all FPRs. Generally, we see that attack performance decreases similarly as we decrease either privacy loss parameter. Furthermore, we gain empirical privacy by decreasing LiRA performance at all FPRs.\n###figure_6### DPSU decreases attack performance because we shrink the size of the vocabulary set, limiting the number of parameters in , and forcing each documents’ topic distribution to look more like other documents’ topic distributions. The nature of learning on word co-occurrences forces  to reflect identifiable changes when infrequently used words suddenly become more common to the training data. By removing these words with DPSU, we limit the ability for outliers to have large effects on the topic model.\nOur results indicate that dedicating most of the privacy budget to DP LDA, rather than DPSU, increases utility at the same privacy level. Intuitively, we can inject less noise into DP LDA, and more into the vocabulary selection algorithm () to increase model interpretability at the same global privacy loss () and similar empirical privacy against the LiRA. This approach allows us balance model privacy and utility in topic modeling."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In our work, we show that topic models exhibit aspects of memorization and successfully implement strong MIAs against them. Although probabilistic topic models do not memorize verbatim sequences of text like language models, they do memorize word frequency and occurrence. In some instances, knowledge of the frequency of certain terms in a document constitutes a privacy violation regardless of their word ordering.\nTo combat MIAs and memorization, we propose a modular framework for implementing better private topic models using differential privacy. Overall, the addition of DP vocabulary selection to the DP topic modeling work-flow is important for guaranteeing private, interpretable releases of . Not only does DP vocabulary selection allow for fully DP releases of , it also provides an effective defense against MIAs.\nWe highlight the greater need for continued development in the field of privacy-preserving ML. If simple probabilistic topics models for text memorize their training data, then it’s only inevitable that large language models and neural topic models memorize their training data. As ML models continue to become more sophisticated and widely used, privacy concerns become increasingly relevant."
        }
    ]
}