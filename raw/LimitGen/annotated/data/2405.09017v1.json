{
    "title": "A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining",
    "abstract": "Using crowdsourcing, we collected more than 10,000 URL pairs\n(parallel top page pairs) of bilingual websites that contain\nparallel documents and created a Japanese-Chinese parallel corpus of\n4.6M sentence pairs from these websites. We used a Japanese-Chinese\nbilingual dictionary of 160K word pairs for document and sentence\nalignment. We then used high-quality 1.2M Japanese-Chinese sentence\npairs to train a parallel corpus filter based on statistical\nlanguage models and word translation probabilities. We compared the\ntranslation accuracy of the model trained on these 4.6M sentence\npairs with that of the model trained on Japanese-Chinese sentence\npairs from CCMatrix (12.4M) Schwenk et al. (2021b), a\nparallel corpus from global web mining. Although our corpus is only\none-third the size of CCMatrix, we found that the accuracy of the\ntwo models was comparable and confirmed that it is feasible to use\ncrowdsourcing for web mining of parallel data.111Work in\nprogress",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Parallel data is vital in machine translation for traditional\nencoder-decoders and recent large language models. From an analysis of\nPalm’s training data, Briakou et al. (2023  ###reference_b2###) showed that\nlarge language models could translate because their training data\ncontain parallel data incidentally.\nRelatively small large language models of around 10B parameters have\npoor translation accuracy. However, Xu et al. (2024  ###reference_b36###) proposed\nALMA, a method of fine-tuning an LLM on a large amount of monolingual\ndata, followed by fine-tuning on a small amount of high-quality\nbilingual data. They achieved translation accuracy comparable to GPT-3\nusing relatively small LLMs. Guo et al. (2024  ###reference_b9###) showed that\ncontinuous pre-training of large language models on large amounts of\nparallel data before fine-tuning on high-quality bilingual data\nimproves translation accuracy over ALMA.\nThis paper discusses a method for collecting Japanese and Chinese\nparallel sentence pairs from the web. Translation between Japanese and\nChinese is considered one of the most important non-English language\npairs in terms of the number of speakers and the scale of the economy.\nWe specifically report on the effectiveness of crowdsourcing in\ncollecting URLs of websites containing parallel data.\nMorishita et al. (2022b  ###reference_b17###) proposed a method of collecting\nparallel URLs (parallel page pairs) using cloud workers for domain\nadaptation of machine translation. We collected parallel top page URL\npairs of bilingual websites by specifying only language pairs to the\ncrowd workers, with no particular restriction on the target domain.\nThe experiment results show crowdsourced websites can collect more\nparallel sentence pairs with less crawling than automatically\ncollected bilingual websites using Common Crawl. We also show that\nthe translation accuracy achieved using the Japanese-Chinese parallel\ncorpus created using crowdsourcing is comparable to that achieved\nusing Japanese-Chinese pairs of CCMatrix\nSchwenk et al. (2021b  ###reference_b27###), a parallel corpus created by global\nweb mining. It is worth noting that our corpus only contains\none-third of the data present in CCMatrix.\nWe release a 4.6M Japanese-Chinese parallel corpus created using\ncrowdsourcing as JParaCrawl Chinese v2.0 for research purposes only.\n222https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/  ###reference_rawl/###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Web mining for parallel data",
            "text": "Research on collecting bilingual data by mining the web began around\n2000 Resnik (1999  ###reference_b24###); Uszkoreit et al. (2010  ###reference_b32###). We can\nbroadly divide current research into hierarchical mining (local\nmining) and global mining.\nIn hierarchical mining (or local mining), based on the web’s\nhierarchical structure, we first search for websites that include\nparallel documents, then search for parallel document pairs within a\nwebsite, and then search for parallel sentence pairs within a parallel\ndocument pairs. In global mining, we consider the web a flat, massive\nset of sentences. We use similarity based on a multilingual sentence\nembedding model to find a sentence’s translation among all sentences\nin the web in different languages. ParaCrawl\nBañón et al. (2020  ###reference_b1###) is a prime example of the former, and\nCCMatrix Schwenk et al. (2021b  ###reference_b27###) is a prime example of the\nlatter.\nMost previous parallel corpora are created by local mining. Like\nEuroParl Koehn (2005  ###reference_b10###) and OpenSubtitles\nLison and Tiedemann (2016  ###reference_b12###), we first identify a\nwebsite that contains parallel documents, extract bilingual document\npairs based primarily on metadata, and then perform sentence\nalignment.\nThe first successful example of global mining is WikiMatrix\nSchwenk et al. (2021a  ###reference_b26###), which collected bilingual text\npairs from Wikipedia in various languages using\nLASER333https://github.com/facebookresearch/LASER  ###reference_###, a\nmultilingual sentence embedding model. CCMatrix\nSchwenk et al. (2021b  ###reference_b27###) applies global mining to CCNet\nWenzek et al. (2020  ###reference_b35###), a monolingual corpus of various\nlanguages extracted from Common Crawl. In the No Language Left Behind\n(NLLB) project Team et al. (2022  ###reference_b29###), they extended CCMatrix to over 200\nlanguages.\nPerforming global mining for the entire web requires enormous\ncomputational resources. We consider local mining a more realistic\napproach to collecting bilingual data for specific language pairs, as\nin the case of JParaCrawl\nMorishita et al. (2020  ###reference_b16###, 2022a  ###reference_b15###),\nwhich collected parallel data between Japanese and English.\nIn local mining, many previous works on document and sentence\nalignment exist, but little research has been done on how to find\nwebsites that contain parallel data. ParaCrawl\nBañón et al. (2020  ###reference_b1###) used the Common Crawl archives to\ncollect websites that contain bilingual text. They applied a language\ndetector on each site’s web pages and looked for sites that contained\napproximately the same amount of text in the language pair to be\ncollected. CCAligned El-Kishky et al. (2020  ###reference_b6###) analyzed\nthe Common Crawl archives using language-identifiable strings in URLs\nas clues and collected parallel URL pairs.\nMorishita et al. (2022b  ###reference_b17###) used a cloud worker to collect\nparallel URLs for domain adaptation of machine translation."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Japanese-Chinese parallel corpora",
            "text": "The most widely used Japanese-Chinese parallel corpus for research is\nJapanese-Chinese ASPEC (Asian Scientific Paper Excerpt Corpus), which\nhas 0.68 million sentence pairs consisting of abstracts of Japanese\nscientific papers and their manual translation into Chinese\nNakazawa et al. (2016  ###reference_b18###). The JPO-NICT Chinese-Japanese\nparallel\ncorpus444https://alaginrc.nict.go.jp/jpo-outline.html  ###reference_ml###,\nwhich has about 130 million Japanese-Chinese patent sentence pairs,\nextracted from patent applications in Japan and China based on patent\nfamilies. ASPEC and JPO-NICT are parallel corpora for specific fields\nand unsuitable for general translation between Japanese and Chinese.\nWCC-JC 3.0 Zhang et al. (2022  ###reference_b38###, 2023  ###reference_b39###) is\na Japanese-Chinese parallel corpus of approximately 3M sentence pairs\ncollected from the web, including movie and TV subtitles, lyrics, and\nnews articles.\nIt is available for research purposes by sending an email request to\nthe authors.\nJapanese-Chinese bilingual data collected from Wikipedia include\nLinguaTools-WikiTitles v2014 (1.7M sentence pairs), which is contained\nin OPUS Tiedemann (2012  ###reference_b31###), a collection of open parallel\ncorpora, WikiMatrix (1.3M sentence pairs)\nSchwenk et al. (2021a  ###reference_b26###), and Wikipedia Chinese-Japanese\nParallel Corpus (0.13M sentence pairs)\nChu et al. (2014  ###reference_b3###, 2015  ###reference_b4###). OpenSubtitles\nv2018\nLison and Tiedemann (2016  ###reference_b12###); Lison et al. (2018  ###reference_b13###),\ncollected from movie subtitles, contains 1.1M Japanese-Chinese\nsentence pairs.\nThe largest publicly available Japanese-Chinese parallel data\ncollected from the web is CCMatrix Schwenk et al. (2021b  ###reference_b27###),\nwith approximately 12M sentence pairs. JParaCrawl Chinese v1.0\nMorishita et al. (2020  ###reference_b16###) contains 83K Japanese-Chinese\nsentence pairs. The Asian Language Treebank\nThu et al. (2016  ###reference_b30###) translates English Wikinews into\nJapanese, Chinese, and other Asian languages and contains\napproximately 20,000 sentences divided into train, dev, and test sets."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Parallel website mining",
            "text": "Our procedure for collecting parallel data is the same as ParaCrawl\nBañón et al. (2020  ###reference_b1###) and follows the pipeline of\nBitextor555https://github.com/bitextor/bitextor  ###reference_###, which\nconsists of web crawling, document alignment, sentence alignment, and\nparallel corpus filtering.\nIn ParaCrawl, they determine which websites to crawl by analyzing the\nCommon Crawl archives. They first apply CLD2\n666https://github.com/CLD2Owners/cld2  ###reference_### to each web page\nto identify its language and extract websites containing approximately\nthe same target language pair texts. They then crawl the extracted\nwebsites with Heritrix\n777https://github.com/internetarchive/heritrix3  ###reference_x3###.\nIn this study, we analyzed 12 sets of Common Crawl archives (104TB in\ntotal) published from September 2021 to June 2023 using the language\ndetector CLD2 and enumerated about 40,000 websites that contain\nroughly equal amounts of Japanese and Chinese text in order of total\ntext volume in a website.\nWe used Extractor888https://github.com/paracrawl/extractor  ###reference_### from the ParaCrawl project for this procedure.\nMorishita et al. (2022b  ###reference_b17###) proposed a method of collecting\nparallel URLs (parallel web pages) using cloud workers for domain\nadaptation of machine translation. We asked crowd workers to collect\nwebsites containing parallel pages, specifying only the language\npairs, and report the pair of URLs of the Japanese and Chinese top\npages for each website.\nFor both parallel top page URL pairs collected using crowdsourcing and\nbilingual website URLs obtained from Common Crawl, we used Heritrix to\ncrawl each website for up to 48 hours, crawling Word and PDF files as\nwell as HTML."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Parallel corpus filtering",
            "text": "The ParaCrawl project has two parallel corpus filters, Bicleaner and\nBicleaner AI. Bicleaner\nSánchez-Cartagena et al. (2018  ###reference_b25###); Ramírez-Sánchez et al. (2020  ###reference_b22###)\nextracts features using word translation probabilities and statistical\nlanguage models and trains a random-forest classifier to classify\nwhether a sentence pair is parallel. Bicleaner AI\nZaragoza-Bernabeu et al. (2022  ###reference_b37###) uses a pre-trained\nmultilingual language model to train a binary classifier. Both\nmethods require high-quality parallel data to train the classifier.\nIn this study, we used\nBicleaner131313https://github.com/bitextor/bicleaner  ###reference_### to\nminimize the use of external resources and improve computational\nefficiency. We used mecab and pkuseg\nLuo et al. (2019  ###reference_b14###)141414https://github.com/lancopku/pkuseg-python  ###reference_###\nfor Japanese and Chinese word segmentation to compute word frequency\nand obtain word alignment from high-quality parallel sentence pairs.\nWe used AWESOME-align Dou and Neubig (2021  ###reference_b5###) for word alignment\nto compute word translation probabilities.\nTo train the parallel corpus filter, we used an in-house\nJapanese-Chinese parallel corpus (1.2M sentence pairs) consisting of\ntravel conversations, dictionary examples, literary works, and\nnewspaper articles. Among the in-house Japanese-Chinese parallel\ncorpus, the Basic Travel Expression Corpus (BTEC, about 0.5M sentence\npairs) Takezawa et al. (2002  ###reference_b28###) is the largest, followed by\nthe dictionary example sentences (about 260,000 sentence pairs).\nWe used the Japanese-Chinese Bicleaner model to compute scores for\nbilingual sentence pairs and extract sentence pairs with a threshold\nvalue of 0.5 or higher. We further calculated each sentence’s vector\nusing the multilingual sentence embedding model LaBSE\nFeng et al. (2022  ###reference_b8###) and filtered out sentence pairs with a\ncosine distance below a threshold of 0.7.\nFor URLs obtained from Common Crawl and URL pairs obtained from\ncrowdsourcing, Table 1  ###reference_### shows the number of\nsites we successfully crawled, the number of sites that yielded any\nparallel sentence pairs, and the total number of parallel sentence\npairs obtained. The percentage of successful parallel sentence pair\nextraction was considerably higher for websites obtained from\ncrowdsourcing, at 74.5 percent, compared to 27.2 percent for those\nobtained from Common Crawl. The total number of parallel sentence\npairs obtained for websites from Common Crawl and those from\ncrowdsourcing is 2.8M and 4.6M sentence pairs, respectively,\nindicating that crowdsourcing can collect more sentence pairs with\nless crawling than analyzing Common Crawl."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Translation experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We examined the accuracy of Japanese-to-Chinese and\nChinese-to-Japanese translations to assess the quality of parallel\nsentence pairs collected using crowdsourcing.\nTable 2  ###reference_### shows the Japanese-Chinese parallel\ndatasets used in the translation experiments.\nWe used CCMatrix Schwenk et al. (2021b  ###reference_b27###),\nWikiTitles Tiedemann (2012  ###reference_b31###),\nWikiMatrix Schwenk et al. (2021a  ###reference_b26###), and\nOpenSubtitles2018 Lison et al. (2018  ###reference_b13###) for\ncomparison because they have more than one million sentence pairs and\nare readily available.\nWe combined WikiTitles, WikiMatrix, and OpenSubtitles2018 into one\n(wt-wm-os), and trained three models from ccmatrix, wt-wm-os, and\ncrowdsourcing.\nWe further trained one model from all five corpora.\nFor the development set, we used\nnews-commentary-v18 (1,677 sentence\npairs)151515https://data.statmt.org/news-commentary/v18.1/  ###reference_8.1/###,\nthe dev set of Asian Language Treebank Parallel Corpus (1,000 sentence\npairs)161616https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/index.html  ###reference_mutiyama/ALT/index.html###,\nand the dev set of FLORES-200 (997 sentence\npairs)171717https://github.com/facebookresearch/flores  ###reference_###.\nFor the test sets, we used the test set of Asian Language Treebank\n(1,000 sentence pairs), the test set of ASPEC-JC (2107 sentence pairs),\nthe devtest of FLORES-200 (1012 sentence pairs), and NTREX-128 (1997\nsentence pairs) Federmann et al. (2022  ###reference_b7###). We also used as our\ntest set 1,000 sentences randomly sampled from our in-house\nJapanese-Chinese parallel corpus (bitext_cj) and our in-house Chinese\ntranslations of news (495 sentences) and question answering (497\nsentences) from the WMT2023 Japanese-English test set (wmt2023j). The\nsource language of these test sets is Japanese for aspec-jc and\nwmt2023j, a mixture of Japanese and Chinese for bitext_cj, and\nEnglish for the others."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiment condition",
            "text": "We used fairseq Ott et al. (2019  ###reference_b19###) as the translation\nsoftware and transformer big Vaswani et al. (2017  ###reference_b34###) as the\ntranslation model.\nTable 3  ###reference_### shows the hyper parameters of Transformer.\nWe used sentencepiece Kudo and Richardson (2018  ###reference_b11###) to\ntokenize training, development, and test data.\nThe vocabulary size is 32K for both Japanese and Chinese.\nWe evaluated translation accuracy using sacreBLEU\nPapineni et al. (2002  ###reference_b20###); Post (2018  ###reference_b21###) and COMET\n(wmt22-comet-da) Rei et al. (2020  ###reference_b23###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Translation accuracy",
            "text": "Table 4  ###reference_### shows the translation accuracy from Japanese to\nChinese, and Table 5  ###reference_### shows that from Chinese to Japanese.\nAmong the three translation models, ccmatrix, wt-wm-os, and\ncrowdsourcing, ccmatrix and crowdsourcing have about the same translation\naccuracy, while wt-wm-os is less accurate.\nBetween ccmatrix and crowdsourcing, crowdsourcing has higher\ntranslation accuracy from Japanese to Chinese, and ccmatrix has higher\naccuracy from Chinese to Japanese.\nCreating a single translation model from all bilingual data yields a\nhigher translation accuracy than these three models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Crowdsourcing (4.6M) has only one-third of sentence pairs of CCMatrix\n(12.4M), but the translation accuracy is about the same. This\nindicates that parallel sentence pairs obtained using crowdsourcing\nhave higher quality than CCMatrix.\nFor Japanese-to-Chinese translation, the higher accuracy of our\nparallel corpus collected using crowdsourcing compared to CCMatrix is\nprobably due to the fact that the crowdsourcing was done in Japan by\nJapanese crowd workers. Many of the websites collected by Japanese\ncrowd workers are Japanese websites that include pages translated into\nChinese. More diversity may be needed when translating Chinese into\nJapanese.\nThis study evaluated the translation accuracy of parallel sentence\npairs collected using crowdsourcing (4.6M). However, we expect that\nadding parallel sentence pairs collected using Common Crawl (2.8M)\nwill increase diversity and improve translation accuracy from Chinese\nto Japanese. Another issue is that evaluating Chinese-to-Japanese\ntranslation accuracy could be more reliable if we had a test set whose\nsource sentences originated from Chinese and whose reference sentences\nare direct manual translations from Chinese to Japanese."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper describes an attempt to create a Japanese-Chinese parallel\ncorpus from the web by collecting URL pairs of parallel websites\nthrough crowdsourcing. We collected 4.6M sentence pairs and showed\nthat we could achieve the same level of translation accuracy as the\nCCMatrix (12.4M) with one-third of the data.\nIn the future, we will create an adult filter to filter the parallel\nsentence pairs (2.8M) collected using Common Crawl and add these to\nmake a Japanese-Chinese parallel corpus with more diverse content. We\nwill also train a machine translation model using the sentence pairs\ncreated with bilingual dictionary-based document and sentence\nalignment to perform machine translation-based document and sentence\nalignment, which could improve the quality of parallel sentence pairs."
        }
    ]
}