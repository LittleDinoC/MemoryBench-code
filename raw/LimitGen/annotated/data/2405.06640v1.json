{
    "title": "Linearizing Large Language Models",
    "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost.\nHowever, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets.\nAs a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA).111We borrow the term “uptraining” from Ainslie et al. (2023) to refer to continued training with a modified architecture, as opposed to fine-tuning, which usually refers to continued training on a different dataset. We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost.\nWe find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Over the last few years, Transformers (Vaswani et al., 2017  ###reference_b41###) have displaced Recurrent Neural Networks (RNNs) in sequence modeling tasks, owing to their highly parallel training efficiency and unmatched scaling performance (Kaplan et al., 2020  ###reference_b18###). However, this training efficiency comes at the cost of inference cost that scales linearly with the number of tokens, compared to the fixed-cost inference of RNNs. The memory-intensive nature of transformers has led to renewed interest in recurrence—the fixed-size hidden state remains an attractive modeling proposition to reduce the cost of inference for language and multimodal models.\nSeveral recent works, starting with Linear Transformers (Katharopoulos et al., 2020  ###reference_b20###), have observed a relationship between a linearized form of attention and recurrence, leading to a duality between transformers and RNNs: models can be trained with sequence parallelism (i.e. as transformers, avoiding backpropagation through time), but can operate as RNNs at inference time. Although this architecture allows efficient training of RNNs, softmax transformers continue to outperform linear transformers across natural language understanding benchmarks.\nA number of novel RNN architectures have attempted to bridge this performance gap. These include RWKV (Peng et al., 2023a  ###reference_b27###), Retentive Networks (Sun et al., 2023  ###reference_b38###), TransNormer (Qin et al., 2022a  ###reference_b32###),\nand more recently, Griffin (De et al., 2024  ###reference_b9###) and RecurrentGemma (Griffin Team et al., 2024  ###reference_b12###). These models are pre-trained on the same pre-training datasets as transformers and show promising results.\nState-space models (Gu et al., 2021  ###reference_b14###) (SSMs) are another recurrent alternative to softmax transformers, combining RNNs and convolutional networks to efficiently model long sequences. The Mamba (Gu & Dao, 2023  ###reference_b13###) architecture is a SSM that shows impressive performance at smaller scales, matching or exceeding the performance of softmax transformers on a number of natural language understanding (NLU) benchmarks. However, a gap remains for long-context NLU tasks, showing a persistent advantage of softmax attention.\nArchitecture search at the scale of large language models is expensive.\nRather than pre-training linear models, another approach is to convert an existing transformer into an RNN; Kasai et al. (2021  ###reference_b19###) proposed to uptrain encoder-decoder transformers into RNNs by introducing an approximating MLP attention module. Zhang et al. (2024  ###reference_b45###) improved on this method by adding a loss to match softmax attention to approximate more closely the base transformer.\nWhile approximating attention is an intriguing approach to re-using pre-trained transformers, it leads to instability and poor performance when uptraining large-scale models. We instead take a different approach: rather than approximate softmax attention, we replace it with a linear kernel and a normalization strategy to uptrain the most performant LLMs into RNNs (see Figure 2  ###reference_###). We take advantage of models trained on high-quality, proprietary datasets\nfor trillions of tokens (e.g. Mistral (Jiang et al., 2023  ###reference_b17###) and Llama2 (Touvron et al., 2023  ###reference_b40###)). Fine-tuning these models on publicly available data for a small fraction of pre-training tokens (see Figure 1  ###reference_###), we obtain linear models that are competitive with the best linear transformers for a fraction of the compute. We call our approach Scalable UPtraining for Recurrent Attention (SUPRA).\n###figure_1### Our contributions are as follows:\nWe propose Scalable UPtraining for Recurrent Attention (SUPRA), a linearization strategy to uptrain state-of-the-art LLMs into performant RNNs.\nWe show that this simple uptraining technique is competitive with the strongest pre-trained recurrent LLMs.\nWe investigate the limitations of recurrent LLMs, comparing pre-trained and uptrained RNNs to transformers, revealing a persistent gap for in-context learning and long-context tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section we review linear transformers, and describe the linearization technique of  Kasai et al. (2021  ###reference_b19###) as it lays the groundwork for our approach. Finally, we present SUPRA, our method for uptraining large transformers into RNNs.\nLinear attention can be expressed as an RNN that updates a state  and a normalization factor  at each time step. Katharopoulos et al. (2020  ###reference_b20###) call these terms the attention memory and normalized memory. This RNN formulation is mathematically equivalent to linear attention, allowing the user to choose the most efficient one for a given task and hardware.\nConsider a stream of tokens we want to generate\n. At inference time, we use the following update rule, where subscripts denote timestep in the recurrence (calling , etc):\nThe state  acts as a constant-size KV cache. Instead of appending new values to the cache, the state is updated. This allows for inference cost that is constant in the number of generated tokens."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Background: Linear Attention",
            "text": "Linear Transformers (Katharopoulos et al., 2020  ###reference_b20###) establish a connection between transformers and RNNs, generalizing the definition of attention by replacing the softmax dot-product attention  with a more generic similarity function  between the queries  and keys :\nStandard softmax attention is a special case, using .\nThe authors explore several alternative functions for , including a linear kernel. Their main architecture uses the similarity function  with a fixed exponential linear unit kernel . They show the computational benefits of linear attention and, more importantly for this work, they demonstrate how such a model can be expressed as an RNN in the case of attention with causal masking.\nLinear attention can be expressed as an RNN that updates a state  and a normalization factor  at each time step. Katharopoulos et al. (2020  ###reference_b20###  ###reference_b20###) call these terms the attention memory and normalized memory. This RNN formulation is mathematically equivalent to linear attention, allowing the user to choose the most efficient one for a given task and hardware.\nConsider a stream of tokens we want to generate\n. At inference time, we use the following update rule, where subscripts denote timestep in the recurrence (calling , etc):\nThe state  acts as a constant-size KV cache. Instead of appending new values to the cache, the state is updated. This allows for inference cost that is constant in the number of generated tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Finetuning a Transformer into an RNN",
            "text": "Kasai et al. (2021  ###reference_b19###) introduced a linear transformer uptraining procedure that converts a pre-trained softmax transformer into an RNN by approximating the attention computation with multi-layer perceptrons (MLPs). The method (T2R) starts with a softmax attention model, and linearizes the softmax operation. Recall the kernel linear attention similarity function:\nInstead of choosing  as a simple non-linearity, the authors use a trainable layer:\nThe weights are shared between keys and queries for a given attention head. By using  and rearranging the operations, attention can be written as:\nThis allows the recurrent inference described in Section 2.1  ###reference_###.\nHowever, this formulation has a number of drawbacks. First, it requires a significant re-training of the model, using approximately  of pre-training tokens for conversion, while suffering a  drop in performance on language benchmarks. Furthermore, this approach was tested on relatively small models ( scale). Because it mimics the attention formulation closely, it suffers from stability issues at larger scales. To address these issues, we modify the approach to adapt it to large-scale model uptraining."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "SUPRA: Scalable UPtraining for Recurrent Attention",
            "text": "Rather than pre-training linear models from scratch, we choose to instead uptrain state-of-the-art transformers. Leveraging models that take advantage of high-quality (but proprietary) pre-training datasets, we linearize them using a modest fraction of pre-training data (see Figure 1). We build on T2R, identifying two major issues and proposing SUPRA, an approach to fine-tuning very large transformers into RNNs.\n\nNext we note that linear attention suffers more with absolute positional encoding than softmax attention, and a modern relative positional encoding scheme like RoPE (Su et al., 2021) is crucial for competitive performance. Rather than training a linear transformer from scratch incorporating these findings (RetNet, TransNormer) we use MLP kernels to convert large language models into RNNs.\n\nStarting with the pre-trained model, we add weights shared between keys and queries, and use the rotary positional embedding (RoPE (Su et al., 2021)) such that the similarity function becomes\n\nWe use a fixed decay vector, with heads, as in Sun et al. (2023).\n\nThis leads to the following attention formulation (see Figure 2 for a graphical representation): These new parameters are trained jointly with the rest of the network; at test time, we use the recurrent formulation for inference."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We uptrain a variety of models from the 1B to 7B range into RNNs (Llama2 (Touvron et al., 2023  ###reference_b40###) and Mistral (Jiang et al., 2023  ###reference_b17###)), and evaluate our models in two settings: standard language understanding benchmarks and long-context evaluations. We compare the results of different architectural choices and training strategies, and then show the limitations of linear models on various benchmarks, describing the persistent gap between vanilla attention and recurrence. We choose Llama2-7B and Mistral-7B as our base models for uptraining, but our recipe is general to any transformer model.\nWe compare our procedure to a variety of pre-trained recurrent models. Given that the largest available state-space models are at the 2.8B scale, we also train a Mamba model on the RefinedWeb (Penedo et al., 2023  ###reference_b26###) dataset from scratch for 1.2T tokens, to serve as a strong baseline for a pre-trained recurrent model 222The Mistral-SUPRA  ###reference_a### and Mamba-7B  ###reference_### models are released along with the code  ###reference_###..\nWe use a fork of OpenLM  ###reference_### (Gururangan et al., 2023  ###reference_b15###) for all training and fine-tuning. Please see Section 7  ###reference_### for hyperparameters and further details on reproducibility.\n###table_1### In Table 1  ###reference_### we report results on standard NLU evaluations using the Eleuther evaluation harness (Gao et al., 2023  ###reference_b11###). We primarily compare to transformers and linear models at the 7B scale, and we train a Mamba model at 7B for comparison with RWKV-5.\nAs our model is initialized from strong pre-trained transformers (Llama2 and Mistral-7B), it preserves performance on most benchmarks (except MMLU; see Section 4  ###reference_### for a discussion below).\nOur technique outperforms RWKV-5 with minimal uptraining and is competitive with our 7B Mamba trained from scratch on 1.2T tokens.\nRecurrent models were thought to perform well on long-context tasks because of their ability to preserve performance beyond their training sequence size. However, their downstream performance on long-context tasks has not been well-documented.\nPrior studies either do not conduct long-context evaluations (Katharopoulos et al., 2020  ###reference_b20###; Kasai et al., 2021  ###reference_b19###), evaluate only on perplexity (Sun et al., 2023  ###reference_b38###; De et al., 2024  ###reference_b9###; Gu & Dao, 2023  ###reference_b13###), or evaluate on datasets which require task-specific training (Peng et al., 2023a  ###reference_b27###).\nInstead, we consider downstream natural language tasks from the SCROLLS benchmark (Shaham et al., 2022a  ###reference_b35###). Specifically, in Table 2  ###reference_### we present two tasks – Qasper (Dasigi et al., 2021  ###reference_b8###) and NarrativeQA (Kočiský et al., 2018  ###reference_b21###) – from the set of tasks evaluated in the Llama2-Long report (Xiong et al., 2023  ###reference_b43###).\nWe evaluate both tasks with an input context cut-off at different lengths. A strong long-context model should perform better given more context. However, the training context lengths for these models do not go beyond 8k tokens. Transformer models show the strongest results up to the context length they were trained for but degrade beyond that. Interestingly, applying the YaRN trick (Peng et al., 2023b  ###reference_b28###) enables transformers to scale beyond their training context quite well. RWKV shows a strong ability to handle much longer context than its training. Our Mamba model on the contrary is not able to generalize beyond its training context length. Surprisingly, the RecurrentGemma model (Griffin Team et al., 2024  ###reference_b12###) shows degrading performances even within its training context length.\nFinally, our Mistral-SUPRA  model preserves some performance at larger context lengths but we believe it to result from the decay along the context length that shortens the effective context. This is discussed in more details below.\nWe find a significant gap in performance between transformers and available linear models, including models uptrained from strong long-context transformers. We speculate that more sophisticated recurrent state update rules may be required to perform well at this task. Ideas such as gating strategies (De et al., 2024  ###reference_b9###), higher order linear attention (Mercat, 2020  ###reference_b24###), or associative binding (Munkhdalai et al., 2024  ###reference_b25###) could be explored.\nThe default decay factors proposed in Qin et al. (2024  ###reference_b34###) gives better results than no decay on short context benchmarks but at a long range, the decay cancels out the influence of the context (). This can be related to a smooth version of window attention (Beltagy et al., 2020  ###reference_b4###). However, as more context is given to the model, the long-range evaluation performances plateau. When using the values proposed in Sun et al. (2023  ###reference_b38###), that allow longer range attention, we observe a performance drop on short-context benchmarks and no substantial improvement on long-context evaluation.\nTable 3  ###reference_### compares transformers pre-trained on 100B tokens to Mamba (Gu & Dao, 2023  ###reference_b13###), T2R (Kasai et al., 2021  ###reference_b19###), and our approach. At this scale, with 100B tokens of training, the Mamba model performs best and other models show similar performance.\nThe second half of Table 3  ###reference_### shows results for uptraining from a pre-trained transformer. TheT2R (Kasai et al., 2021  ###reference_b19###) uptraining was unstable, yielding poor results compapred to SUPRA. This confirms that normalization is key for maintaining performance of the base LLM when uptraining.\nTo test the hypothesis that linear attention approximates the softmax attention, we experimented with a 2-step approach. The first step trains only the new parameters such that the model could learn to approximate the softmax. The second step fine-tunes all the weights. The results show no benefit from the two steps approach and indicates that the softmax is not approximated. See Appendix A  ###reference_### for a different approach to compare softmax attention and linear attention.\nFinally, we compare the results of SUPRA uptrainings from two pre-trained softmax models. It appears that pre-training a linear model for a 100B token yields better results than fine-tuning a softmax model that was trained with the same budget.\nThese results also shows, along with the comparison of LLama2-SUPRA and Mistral-SUPRA in Table 1  ###reference_###, that SUPRA benefits significantly from a stronger pre-trained transformer. Thus, given a limited training budget, using SUPRA from a strong pre-trained model is the best option."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Comparison to pre-training SSMs/RNNs. With only 20B tokens of training, which represents  of RWKV and RetNet training cost, we obtain a model that outperforms both on HellaSwag and that is competitive on other benchmarks (see Table 1  ###reference_###).\nGiven the existing performance gap between the strongest transformer models and the most performant linear models, SUPRA is a simple recipe for conversion, allowing the study of strong RNNs with limited uptraining.\nComparison to Transformers on Short-Context Tasks. Our approach does not explicitly approximate attention from the base transformer model (see Appendix A  ###reference_###), we do see a modest drop in performance across all benchmarks compared to softmax transformers. This could be partially explained by the lower quality of our data compared to the pre-training mix used to train models like Mistral-7B. It is also likely that linear transformers are inherently less expressive. However, the performance drop is relatively modest on most benchmarks, and significantly smaller than the drop from T2R uptraining, which shows the relevance of our approach.\nLong Context Comparisons.\nPrior work on linear attention showcased similar or better validation set perplexity to transformer models over long context (e.g. Sun et al. (2023  ###reference_b38###)) but did not evaluate linear models on natural language long-context evaluations like SCROLLS (Shaham et al., 2022b  ###reference_b36###). The results in Table 2  ###reference_### show that recurrent models generally maintain performance beyond their training context (except for Mamba-7b) while transformers (without modification) do not. However, Table 2  ###reference_### also demonstrates that simple linear scaling of the rotary positional embedding (Peng et al., 2023b  ###reference_b28###; emozilla, 2023  ###reference_b10###) can allow for context scaling beyond the window used for training a given transformer model, effectively nullifying the performance edge of these linear models. Furthermore, transformers generally outperform linear models at their maximum training context length.\nFurther research is needed into extending linear models to long-context inference to take full advantage of the lower inference cost relative to vanilla transformers.\nLimitations.\nSince our method relies on initializing with strong pre-trained transformers, our models inherit any of the biases and weaknesses of their base models. Additionally, models that are already instruct-tuned do not linearize as well as base models. Our models suffer from poor performance on MMLU which requires in-context learning (5-shot), a weakness of linear models (Akyürek et al., 2024  ###reference_b2###). We leave the investigation of these weaknesses of linear models to future work and hope that our proposed uptraining approach can help facilitate and accelerate the research in this area."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The linear transformers introduced in Katharopoulos et al. (2020  ###reference_b20###) lagged behind vanilla transformers in downstream performance, and subsequent architectures such as TransNormer (Qin et al., 2022a  ###reference_b32###) and RetNet (Sun et al., 2023  ###reference_b38###) narrow the gap, but do not demonstrate competitive results with modern transformers at scale.\nRWKV (Peng et al., 2023a  ###reference_b27###), a linear transformer that takes inspiration from LSTM (Hochreiter & Schmidhuber, 1997  ###reference_b16###), is competitive with compute-matched transformer-based models, but lags behind on a number of NLU benchmarks.\nGriffin (De et al., 2024  ###reference_b9###) is a concurrent model that takes a hybrid approach, combining a sliding window with linear attention shows impressive performance relative to vanilla transformers, but is trained on a high-quality proprietary dataset.\nAnother thread in the literature focuses on efficient attention alternatives (Performers (Choromanski et al., 2020  ###reference_b6###), Cosformer (Qin et al., 2022b  ###reference_b33###), LUNA (Ma et al., 2021  ###reference_b23###), RFA (Peng et al., 2021  ###reference_b29###), Attention-free Transformer (Zhai et al., 2021  ###reference_b44###)). All of these approaches sacrifice performances for efficiency. Efficiency improvements for vanilla transformers have narrowed the capabilities gap between vanilla and linear transformers. The KV cache Pope et al. (2023  ###reference_b31###)greatly narrows the inference efficiency gap between linear and vanilla transformers. RingAttention Liu et al. (2023  ###reference_b22###) allows for very long context scaling of vanilla attention without approximation.\nState-space models (SSMs) such as H3 (Dao et al., 2022  ###reference_b7###), Hyena (Poli et al., 2023  ###reference_b30###), and Mamba (Gu & Dao, 2023  ###reference_b13###) are recent alternatives to vanilla transformers, combining the strengths of convolutional and recurrent models with efficient hardware implementations.\nInstead of parallelizing training over the sequence, they produce an efficient way to train the sequential RNN. While these models are competitive with vanilla transformers on some tasks, we show that SSMs share the limitations of linear transformers on several in-context learning and long-context tasks.\nHedgehog (Zhang et al., 2024  ###reference_b45###) builds on the work of Kasai et al. (2021  ###reference_b19###), identifying three different ways of training linear transformers – from scratch, uptraining quadratic transformers for a specific task, and uptraining generally.\nThe authors focus on the first two, and we focus on the third. Moreover, they aim at approximating the softmax attention matrices with linear alternatives. In this work, we do not aim to approximate softmax attention, we replace it with a linear alternative (see ablation above and appendix A  ###reference_###).\nTheir method is only tested for smaller scale models and with parameter-efficient fine-tuning for larger models, but presents challenges for scaling for two reasons: (1) their training strategy involves comparing full attention matrices which is computationally expensive, and not feasible for full fine-tuning of large models with long sequences and (2) their method also inherits the gradient instabilities of linear transformers studied in Sun et al. (2023  ###reference_b38###), while our normalization setup leads to stable uptraining of large models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced SUPRA, a technique for converting large-scale pre-trained softmax transformers into recurrent neural networks, enabling the study of the strengths and limitations of recurrent models at scale with minimal compute cost. Compared to pre-training linear models from scratch, the SUPRA strategy produces competitive models comparable to the best available recurrent LLMs (RWKV and Mamba) at the 7B scale.\nWe identify the strengths of linear models on standard NLU benchmarks but also the enduring limitations on in-context (i.e. MMLU) and long-context (NarrativeQA, Qasper) tasks, showing that linearized models do not inherit these capabilities from the base softmax transformers.\nOne possible path to rectifying these limitations is explicitly training for in-context learning (Akyürek et al., 2024  ###reference_b2###). We leave explorations of specialized and instruct data in the context of linear transformers to future work. More sophisticated gating mechanisms as in in Peng et al. (2023a  ###reference_b27###) and De et al. (2024  ###reference_b9###) are promising alternatives to our simple linearization.\nUsing our uptraining method would greatly reduce the necessary time and cost of such experimentation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Reproducibility",
            "text": "We train our linear models using our fork  ###reference_### of OpenLM (Gururangan et al., 2023  ###reference_b15###) that we modify to include a linear attention function (printed below). We use Lightning Attention 2 (Qin et al., 2024  ###reference_b34###) that offers a fast Triton (Tillet et al., 2019  ###reference_b39###) kernel for linear attention computation. Evaluations are done with the Eleuther evaluation harness (Gao et al., 2023  ###reference_b11###).\nWe train and uptrain models on RefinedWeb (Penedo et al., 2023  ###reference_b26###)(with 2 epochs for our Mamba training), which we tokenize with the pre-trained model’s tokenizers. When training from scratch, we used the GPT-NeoX-20B (Black et al., 2022  ###reference_b5###) tokenizer. We tokenize with sequence packing and use a default sequence length of 2048.\nWe use square matrices with biases for the linear layers in the kernel  functions to keep the same feature dimension in the queries and keys. We use the same kernel, with the same weights for both queries and keys and apply a ReLU activation.\nWe use 1000 steps of linear learning rate warmup and a cosine learning rate decay from  to  for our 7B uptrainings and from  to  for our 1B uptrainings and for trainings from scratch. We use the Adam optimizer with  and .\nWe trained our models with mini-batches totaling 2M tokens each.\nOur default RoPE frequency uses the Llama value of . For longer sequence lengths, we use a RoPE frequency of .\nDepending on the model size and the availability, we use from 4 to 32 nodes of 8 GPUs Nvidia H100 with Pytorch FSDP. We use a mixed precision strategy from OpenLM that automatically selects between bfloat 16 and float 32 for different operations. A linear 7B parameter model uptraining throughput is around  tokens per second per GPU.\nOur results can be reproduced by following the same training recipe or using the model weights that we release: Mistral-SUPRA  ###reference_a### and Mamba-7b  ###reference_###."
        }
    ]
}