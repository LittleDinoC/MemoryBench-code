{
    "title": "Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English",
    "abstract": "People communicate in more than 7,000 languages around the world, with around 780 languages spoken in India alone. Despite this linguistic diversity, research on Sentiment Analysis has predominantly focused on English text data, resulting in a disproportionate availability of sentiment resources for English. This paper examines the performance of transformer models in Sentiment Analysis tasks across multilingual datasets and text that has undergone machine translation. By comparing the effectiveness of these models in different linguistic contexts, we gain insights into their performance variations and potential implications for sentiment analysis across diverse languages. We also discuss the shortcomings and potential for future work towards the end.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The term Sentiment Analysis refers to the process of analyzing text to determine the emotional tone of the message. More generally, it can be understood as assessing an individual towards a particular target.\nMachine translation refers to the conversion of text from a source language to a target language via the use of computer algorithms.\nBert and XLM Roberta are Large language models based on the transformer architecture introduced by Google, trained on a huge corpus of data, ideal for fine-tuning downstream tasks such as Sentiment Analysis and Machine Translation.\nGiven the accessibility and usefulness of these models, they find their application in various languages other than English and various multilingual settings, where models are tuned to understand context in multiple languages.\nDue to easy accessibility and the plethora of literature available in English, tasks like Sentiment Analysis have extensively been researched on English texts, which naturally leads to many sentiment resources for English texts but less so for texts in other languages.\nIn our project, we aim to compare Sentiment Analysis performance on the original language texts for the languages French, German, Spanish, Japanese and Chinese while also comparing machine translation performance of models across different languages.\nOther work done with similar objectives, we believe, often falls short of creating a robust pipeline to process multilingual datasets and often implements simple rudimentary pipelines that don’t use underlying datasets to the fullest. We identify certain gaps and interesting areas in which we could expand existing research done on this topic and thus present the major contributions of our project:\nWe present robust pipelines that incorporate and compare various state-of-the-art sentiment analysis and machine translation models.\nWe provide domain-tuned versions of large language models on a subset of the Multilingual Amazon Reviews Corpus[10  ###reference_b10###].\nWe analyse the translation models in different languages by their ability to recreate the baseline for sentiment analysis of English models. This allows us to understand the progress of NLP in different languages compared to English.\nWe explore if it is viable to use cross-lingual over uni-lingual models and if significant performance can be achieved by machine translation to transform the dataset into English, in which models are, in general, better.\nFor all tasks, we use transformer-based models that have been pre-trained on an enormous corpus of text prior to our deployment and pertaining."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sentiment Analysis and Emotion Detection from Text",
            "text": "There have been several studies exploring sentiment analysis and emotion detection techniques being applied to textual data. The paper from Nandwani and Verma provides insights into different methodologies used for analyzing sentiments and detecting emotions. They delve into traditional machine learning algorithms and deep learning models, highlighting their strengths and limitations on the above-mentioned task.[14  ###reference_b14###].\nMohammad et al. examine how translation alters sentiment, focusing on the impact of machine translation on sentiment analysis across languages [13  ###reference_b13###]. They observe that machine translation can significantly alter the sentiment expressed in a sentence and highlight the challenges of accurately capturing sentiment in a multilingual setting.\nAraujo et al. evaluate machine translation for multilingual sentence-level sentiment analysis and assess the effectiveness of different machine translation models in preserving sentiment across languages [2  ###reference_b2###]. However, their findings underscore the importance of considering the quality of machine translation while conducting sentiment analysis in multilingual contexts."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Challenges in Multilingual Sentiment Analysis",
            "text": "One of the key challenges identified by both Mohammad et al. and Araujo et al. in accurately interpreting emotions from text is the inherent complexity of language, which includes nuances, context, and cultural disparities. This challenge is more inherent in a multilingual setting, where the translation of text to English may not fully capture the original sentiment or emotion.\nMoreover, recent advancements in sentiment analysis and emotion detection have focused on integrating multimodal data and leveraging pre-trained language models to enhance accuracy. These developments are particularly relevant in the context of multilingual sentiment analysis, where incorporating additional modalities such as images or audio can provide valuable context for understanding emotions expressed in text.\nOverall, the insights provided by Nandwani and Verma, Mohammad et al., and Araujo et al. offer valuable considerations for evaluating sentiment analysis across languages, both before and after machine translation to English."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "BERT",
            "text": "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google [8  ###reference_b8###]. It is based on a multi-layer bidirectional transformer encoder, which generates contextualized representations of input text. BERT is pre-trained on a large corpus of text and is fine-tuned on specific tasks, achieving state-of-the-art results in various natural language processing tasks.\nFor languages other than English, we used the following instances of Bert from hugging face:\n1. bert-base-german-cased[5  ###reference_b5###] - This is a German language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of German text.\n2. dccuchile/bert-base-spanish-wwm-cased[4  ###reference_b4###] - This is a Spanish language model based on the BERT architecture. It was developed by the University of Chile and has been fine-tuned on a large corpus of Spanish text.\n3. dbmdz/bert-base-french-europeana-cased[16  ###reference_b16###] - is a French language model based on the BERT architecture. It was developed by the researchers at the Center for Information and Language Processing (CIS), LMU Munich.\n4. cl-tohoku/bert-base-japanese[18  ###reference_b18###] - This is a Japanese language model based on the BERT architecture. It was developed by Tohoku University and has been fine-tuned on a large corpus of Japanese text.\n5. bert-base-chinese(addition of original bert paper [9  ###reference_b9###] - This is a Chinese language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of Chinese text.\nBERT has been shown to be effective in sentiment classification tasks, achieving high accuracy by leveraging its contextualized representations and fine-tuning on specific datasets [15  ###reference_b15###]. BERT fine-tuning has led to remarkable state-of-the-art results on various downstream tasks, including sentiment analysis."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "XLM RoBERTa",
            "text": "XLM-RoBERTa[6  ###reference_b6###] is a multi-lingual language model that combines the strengths of XLM [7  ###reference_b7###] and RoBERTa [12  ###reference_b12###]. The architecture is based on the RoBERTa model, with a modified XLM encoder that enables cross-lingual transfer learning. This allows XLM-RoBERTa to leverage pre-training in multiple languages and fine-tune on specific tasks, achieving state-of-the-art results in various natural language processing tasks.\nXLM-RoBERTa has been shown to be effective in sentiment classification tasks, achieving high accuracy in multiple languages [3  ###reference_b3###]. By leveraging its multi-lingual capabilities and fine-tuning on our dataset, we achieve high performance in sentiment classification."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We meticulously constructed a robust pipeline to fulfill the project’s objectives of comparing transformer performance in sentiment analysis across multilingual data and machine-translated text. This pipeline used advanced transformer architectures such as BERT and XLM-RoBERTa.\nBERT, short for Bidirectional Encoder Representations from Transformers, is an encoder-only model that utilises masked language modeling and next-sentence prediction techniques. BERT models are primarily trained in a single language context.\nXLM-RoBERTa is an advancement over XLM, leveraging the robust RoBERTa architecture. Through extensive pre-training on a larger dataset and prolonged training sessions, XLM-RoBERTa excels in various NLP tasks. It inherits XLM’s cross-lingual capabilities and benefits from RoBERTa’s enhanced representation learning, making it highly proficient in handling multilingual data and achieving superior performance across diverse NLP tasks.\nTo achieve our machine translation objective, we employed the OPUS-MT machine translation model, which utilizes state-of-the-art transformer-based neural machine translation techniques. By leveraging the usage of transformers, OPUS-MT achieves good-quality translations and fluency across multiple languages. This choice aligns perfectly with our project’s focus on evaluating transformer performance on machine-translated text, ensuring robustness and reliability in our analyses.\nTo compare the model performance, we utilized F1-scores as the metric since they give a holistic review of the model’s performance across classes.\nThe pipeline we used during the project is as follows, we first used 50000 entries from each language due to limited computational capability to fine-tune BERT and XLM-RoBERTa models and evaluate their performance using F1-score. In the second part we translated 20000 entries from each language using OPUS-MT to English and then combined them together to form a dataset of 100000 entries. We fine-tuned our BERT and XLM-RoBERTa models on this combined dataset and then evaluated the model’s performance using the F1-score as the evaluation metric.\nBy employing these state-of-the-art transformer models within our pipeline, we aimed to comprehensively evaluate their efficacy in sentiment analysis tasks across multilingual datasets and machine-translated texts."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "The dataset used in this study is the Multilingual Amazon Reviews Corpus, as described by Keung et al. [10  ###reference_b10###]. This corpus represents a rich collection of product reviews gathered from the Amazon platform, spanning multiple languages, including English, Spanish, French, German, Japanese, Chinese, and Italian.\nThe Multilingual Amazon Reviews Corpus contains reviews across a wide range of product categories, such as electronics, books, movies, home appliances, and more. Each review entry within the dataset is accompanied by comprehensive metadata, including the product ID, reviewer ID, review text, star rating, and review date. Additionally, the dataset contains information about the geographic location of reviewers, providing insights into regional variations in sentiment expression.\nOne of the notable features of the Multilingual Amazon Reviews Corpus is its extensive coverage of languages and product categories, making it a valuable resource for studying sentiment analysis and multilingual natural language processing tasks. Researchers can leverage this dataset to explore the complexities and nuances of sentiment analysis across diverse linguistic and cultural contexts.\nIn this study, we selected the Multilingual Amazon Reviews Corpus due to its multilingual nature and diverse product categories, which provided us with an opportunity to investigate the challenges and opportunities of sentiment analysis across different languages and domains."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Language\nBefore Machine Translation\nAfter Machine Translation\n\n\n\nSpanish(ES)\n\n\n\nSpanish_XLM\n0.90461\n\nSpanish_BERT\n0.88878\n\n\n\n\nSpanish_XLM\n0.89965\n\nSpanish_BERT\n0.89875\n\n\nGerman (DE)\n\n\n\nGerman_XLM\n0.90585\n\nGerman_BERT\n0.90730\n\n\n\n\nGerman_XLM\n0.89420\n\nGerman_BERT\n0.89672\n\n\nFrench (FR)\n\n\n\nFrench_XLM\n0.90294\n\nFrench_BERT\n0.87775\n\n\n\n\nFrench_XLM\n0.88740\n\nFrench_BERT\n0.89131\n\n\nChinese (ZH)\n\n\n\nChinese_XLM\n0.86715\n\nChinese_BERT\n0.86844\n\n\n\n\nChinese_XLM\n0.83660\n\nChinese_BERT\n0.82391\n\n\nJapanese(JA)\n\n\n\nJapanese_XLM\n0.90161\n\nJapanese_BERT\n0.89231\n\n\n\n\nJapanese_XLM\n0.83049\n\nJapanese_BERT\n0.82479\nLanguage\nBefore Machine Translation\nAfter Machine Translation\n\n\n\nSpanish(ES)\n\n\n\nSpanish_XLM\n0.60697\n\nSpanish_BERT\n0.58337\n\n\n\n\nSpanish_XLM\n0.58155\n\nSpanish_BERT\n0.62267\n\n\nGerman (DE)\n\n\n\nGerman_XLM\n0.64942\n\nGerman_BERT\n0.62421\n\n\n\n\nGerman_XLM\n0.61658\n\nGerman_BERT\n0.63514\n\n\nFrench (FR)\n\n\n\nFrench_XLM\n0.61209\n\nFrench_BERT\n0.58065\n\n\n\n\nFrench_XLM\n0.56994\n\nFrench_BERT\n0.60932\n\n\nChinese (ZH)\n\n\n\nChinese_XLM\n0.62267\n\nChinese_BERT\n0.54087\n\n\n\n\nChinese_XLM\n0.54025\n\nChinese_BERT\n0.61938\n\n\nJapanese(JA)\n\n\n\nJapanese_XLM\n0.60377\n\nJapanese_BERT\n0.51262\n\n\n\n\nJapanese_XLM\n0.52043\n\nJapanese_BERT\n0.59127\nIt can be observed that machine translation didn’t affect if not significantly improve or derail performance of models in their downstream applications for languages like Spanish, German and French, which as clearly European languages and share a lot of semantic similarities with english. On the other hand Machine Translation significantly, affects the performance of languages Chinese and Japanese in a negative way, as they are significantly unique to english. However no model could reach the fine tuned performance of the english baseline of about 0.91 for english reviews, which could be due to gaps in Machine Translation models, as theoretically they translated sentences in english should be able to convey full meaning of original sentence and reach english benchmarks.However failure to do so even after intensive fine-tuning, leads us to conclude that either more robust fine-tuning on a much larger dataset rather than a subset or a more robust pipeline should help, which would have been outside the scope of our project given the resource, and time."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Observations",
            "text": "Table 1 and Table 2 show the F1 scores for our tasks for each of the language specific model before and after machine Translation.\nWe can infer that XLM-RoBERTA performs slightly better than BERT for each language dataset as expected due to XLM’s cross-lingual capabilities. For the task of Sentiment Analysis, the average F1 score over all the models was 0.89. Before Machine Translation, the German model got the best F1 score across all the models, while the Chinese performed the worst. After Translation, the performance of all the models degraded with the Japanese Model being the most affected. The Spanish model got the best F1 score after machine Translation.\nFor the task of Star Rating Prediction, the average F1 score over all the models was 0.61. Before Machine Translation, the German model outperformed all the models, while the Japanese performed the worst. After Translation, the performance of all the models degraded with the Japanese and Chinese Models being the most affected. The German model still got the best F1 score after machine Translation across all the models."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "During the course of the project, we developed language-specific models and models utilizing translated texts. There was no significant difference between the models developed during the project(language-specific and models using translated tests) as they achieved similar performance. However, the slight difference that occurred can be due to the following shortcomings.\nLanguages like Spanish, German, and French share many similarities with English regarding sentence structure sharing the same SVO word order with English. As a result, machine translation from these languages to English may preserve the original meaning well, leading to consistent sentiment analysis results. However, Asian languages like Japanese and Chinese have different linguistic structures. Japanese syntax follows SOV word order while Chinese sentence structure is characterized by its lack of inflectional morphology and grammatical markers, relying heavily on word order and context for conveying meaning. Machine translation may struggle to accurately capture such semantic meaning, leading to loss of information and thus Higher Semantic difference.\nAsian cultures, for example, may have unique ways of conveying sentiment that differ from Western cultures.So Cultural difference also plays a role in Machine Translation.\nLastly, The availability and quality of training data may vary across languages. English sentiment analysis models may have been trained on larger and more diverse datasets compared to models for other languages. This discrepancy in training data quality can impact the effectiveness of sentiment analysis after machine translation, especially for languages with less available data.\nIn summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, and data availability. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences.\nTo improve the work done, there can be further experimentation, which involves, firstly, fine-tuning machine translation models specifically for sentiment-related tasks. This could involve adding sentiment-specific data or annotations into the fine-tuning process to improve the efficacy of translations, especially for languages with high linguistic differences from English. Extensive literature is available to improve sentiment analysis of models by training them on general or domain-specific Knowledge graphs[11  ###reference_b11###], such as ConceptNet[17  ###reference_b17###]. Secondly, utilizing multimodal approaches incorporating visual and textual information for sentiment analysis across languages. Exploring how images or videos can complement machine-translated text to improve the performance of sentiment analysis, especially in languages where textual data may be limited or unreliable. Some preliminary work on this could be explored in work done by Yoon et al. [19  ###reference_b19###]. Finally, focusing on improving sentiment analysis performance in low-resource languages with a lack of training data. Experimentation can be done by exploring transfer, semi-supervised, or unsupervised learning to adapt to sentiment analysis tasks for languages with limited labelled data.\nIn summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, data availability, and translation quality. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences."
        }
    ]
}