{
    "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
    "abstract": "The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Recent Large Language Models (LLMs) have demonstrated performance levels close to that of humans, but experimental results showed that they lacked planning ability through thought or reasoning (Bubeck et al., 2023  ###reference_b6###). Consequently, a key question in recent language model research is: Can LLMs think? To address this question, new benchmarks for measuring reasoning abilities such as MathVista (Lu et al., 2024  ###reference_b25###), Bonagard-Logo (Nie et al., 2020  ###reference_b33###), and Raven (Zhang et al., 2019  ###reference_b59###) have been proposed. Among them, the Abstract and Reasoning Corpus (ARC) (Chollet, 2019  ###reference_b8###) emerged to be one of the representative benchmarks for assessing reasoning abilities. As shown in Fig. 1  ###reference_### below, each task in ARC consists of 2–5 example pairs and a problem input grid. The goal is to infer rules from given example pairs and apply them to the problem input grid. Input and output grid size can vary from a minimum of  to a maximum of , with each grid having up to 10 different colors.\n###figure_1### ARC remains an unsolved challenge despite its seemingly simple content and evaluation methods. It demands a high level of abstraction and multiple reasoning steps, reasons why conventional deep learning techniques have not achieved success. The best-performing models to date have only achieved an accuracy of 30% (Lab42, 2024  ###reference_b22###), while LLMs have shown an accuracy of around 10% (Mirchandani et al., 2023  ###reference_b31###). Compared to the average human accuracy of 80% (Johnson et al., 2021  ###reference_b20###), these results suggest significant differences in reasoning and abstraction capabilities between humans and LLMs. However, in-depth research into how LLMs reason and how their reasoning differs from humans is lacking. This has led to calls for a shift from a results-focused evaluation to a more nuanced analysis of the process (Chang et al., 2024  ###reference_b7###; Huang and Chang, 2023  ###reference_b18###; Xue et al., 2023  ###reference_b57###), indicating a need for a new perspective that evaluates reasoning abilities based on the process rather than just the outcome.\nTo overcome the limitations of result-oriented analysis in artificial intelligence, this study embraces the Language of Thought Hypothesis (LoTH) (Fodor, 1975  ###reference_b12###), which is predominantly applied in the philosophy of mind to explain human reasoning. According to LoTH, effective reasoning encompasses three essential characteristics: logical coherence, the ability to follow basic logical principles; compositionality, the capability to construct complex ideas from simpler components; and productivity, the capacity to formulate an indefinite number of thoughts or solutions using a finite set of elements.\n###figure_2### While there have been studies questioning LLM’s reasoning abilities (Bubeck et al., 2023  ###reference_b6###; Valmeekam et al., 2024  ###reference_b42###), a detailed analysis is lacking. Therefore, this study aims to utilize ARC to examine their reasoning skills from three perspectives of LoTH. To achieve this goal, we designed three separate experiments as follows:\nLogical Coherence: Using prompting techniques, LLMs are set to solve ARC tasks. By analyzing the types of ARC tasks it can solve and the process of solving them, we aim to determine whether LLMs are capable of logical reasoning and whether its logic is consistent.\nCompositionality:\nARC tasks are known to be solvable through the application of step-by-step functions (Hodel, 2023a  ###reference_b16###). With such functions provided, we aim to ascertain whether LLMs can identify the combinations of functions that are needed to solve a task. This process can be broken down into two parts: understanding how LLMs manipulate the problem input and determining whether they can achieve the desired results through multiple steps of manipulation.\nProductivity: We tested if LLMs can create new input-output pairs for ARC tasks. We selected tasks with multiple inputs leading to the same output, devised prompts for inverse transformation, and assessed LLMs’ ability to generate varied inputs based on these prompts.\nAs a result, we have confirmed that the current level of LLM possesses a basic understanding of images and is capable of simple types of compositional object manipulations. However, compared to human reasoning abilities, LLM lags in three areas: 1) It is weak in understanding aspects such as objectness in images. 2) Its logical reasoning abilities, especially in a step-by-step manner, are weak. 3) It struggles with understanding and generating unseen representations.\nFinally, this study summarizes and presents recent trends proposed to address the weaknesses in abstraction abilities and reasoning capabilities. Analyzing the reasoning abilities of LLMs according to the components of human reasoning and discussing how to enhance each component represents a differentiated approach from previous research. It offers a fresh perspective for measuring and advancing the reasoning capabilities of LLMs in the future."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries",
            "text": "This section aims to explain why we chose the LoTH perspective and ARC before starting a detailed evaluation of LLM’s reasoning capabilities. First, we will look at existing definitions of reasoning abilities and show why LoTH is useful in the perspective of measuring intelligence in Section 2.1  ###reference_###. Then, by looking at ARC benchmarks and how they vary in Section 2.2  ###reference_###, we highlight the need for using both the building blocks of meaning and the rules of combining them to tackle ARC tasks. This approach matches the LoTH perspective, showing that ARC is a good benchmark for studying LLM through the point of view of human reasoning."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Assessing Reasoning Ability of LLMs",
            "text": "Efforts to evaluate LLMs’ capabilities continue, underscoring strengths in image and text generation. Especially, analysis confirms LLMs possess elements of a World Model (Gurnee and Tegmark, 2023  ###reference_b14###), indicating potential in inference tasks. Despite these capabilities, challenges in reasoning are noted (Valmeekam et al., 2024  ###reference_b42###), with errors such as distortion and incomplete reasoning highlighted (Li et al., 2024  ###reference_b24###). Research suggests these reasoning abilities can improve through methodological adjustments. Furthermore, studies indicate that complex compositionality remains challenging (Dziri et al., 2023  ###reference_b11###).\nThe divergent claims about the abilities of LLMs stem from result-centric measurement methods. Turing was the first figure to shift the approach to inference towards consequential direction (Turing, 1950  ###reference_b41###). Subsequently, Wiener (Wiener, 1950  ###reference_b52###), McCulloch and Pitts (McCulloch and Pitts, 1943  ###reference_b29###), and Rosenblatt (Rosenblatt, 1958  ###reference_b37###) shifted to studying methods for measuring performance rather than focusing on the process. Recently, Chollet attempted to quantify inference abilities from a consequential perspective (Chollet, 2019  ###reference_b8###). However, these studies all focus on what reasoning can achieve using a result-oriented approach, without specifying the elements that constitute reasoning ability. West et al. (West et al., 2024  ###reference_b51###) raised concerns about evaluating the reasoning ability of LLMs from a consequentialist perspective, as the generation capability of LLMs may not necessarily depend on comprehension abilities.\nTherefore, a new perspective is needed to evaluate AI’s inference processes; Language of Thought Hypothesis (LoTH) enhances discussions by integrating reasoning components with quantitative metrics. LoTH posits that inference involves manipulating mental representations, a view that dominates the philosophy of mind due to its explanatory power over logical coherence, compositionality, and productivity observed in human cognition. These mental representations are believed to have a compositional syntax and combinatorial semantics. Our study compares with prior works and evaluates LLMs’ inference capabilities through the LoTH, marking progress by assessing aspects like logical coherence, compositionality, and productivity.\nExploring deeper into the three perspectives of LoTH offers strong justification for improving reasoning capabilities. These principles help in developing the ability to process information and solve tasks similar to human reasoning. Logical coherence ensures LLMs can reason without contradictions, compositionality allows LLMs to adapt known knowledge to new scenarios, and productivity enhances LLMs’ capacity to generate results based on given rules. Thus, adopting these perspectives of LoTH aids LLMs in achieving more human-like reasoning, enabling them to address complex problems with innovative and valid results."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Advantages of using ARC as Reasoning Benchmark",
            "text": "In exploring benchmarks suitable for evaluating inference abilities through the lens of the Language of Thought Hypothesis (LoTH), the Abstraction and Reasoning Corpus (ARC) emerges as a compelling candidate. ARC tasks, necessitating combinatorial syntax and compositional semantics, align with LoTH’s emphasis on logical coherence, compositionality, and productivity, validating its role as an appropriate benchmark for reasoning abilities."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1. Comparison Between ARC and Its Variants",
            "text": "When comparing ARC with its variant benchmarks, it becomes evident that ARC poses problems that require a combination to solve. Despite its simple rules, ARC remains a benchmark with relatively low accuracy. LLM achieves an accuracy of 15% (Qiu et al., 2024  ###reference_b35###), while non-LLM AI models reach a maximum of 31% (Lab42, 2024  ###reference_b22###), which significantly differs from the human average accuracy of 80% (Johnson et al., 2021  ###reference_b20###). The excessively high complexity of abstraction and reasoning is a major challenge of ARC. Consequently, various attempts to lower the difficulty of ARC by securing intermediate stages have naturally emerged. Those variants mainly fall into two categories: one focuses on reducing the complexity of abstraction by simplifying input images, while the other aims to reduce the complexity of reasoning dimensions by decreasing the number of steps needed to solve. Below are examples of variant benchmarks.\n1D-ARC (Xu et al., 2023b  ###reference_b56###) is a benchmark that reduces the dimensionality of ARC from 2D to 1D, aiming to simplify the complexity of the ARC. This transformation simultaneously maintains the core prior knowledge of ARC while reducing the dimensionality, thus lowering the complexity of the existing ARC task for enhanced research feasibility. Most importantly, the one-dimensionality of 1D-ARC effectively tackles the challenge of object cohesion. The unresolved object cohesion, regarded as one of the fundamental aspects of human cognition, poses a challenge in solving ARC tasks. Due to its successful handling of object cohesion, LLMs exhibited high accuracy in solving 1D-ARC tasks.\n###figure_3### MC-LARC (Shin et al., 2023  ###reference_b39###) adopts a multiple-choice format, featuring a rule sentence derived from ARC tasks alongside similar, but incorrect rule sentences. Solvers are tasked with selecting the accurate rule from five textual options, using the ARC examples as a reference. Effective resolution of MC-LARC requires solvers to semantically link these examples to the text options and distinguish the correct choice from the false ones. The pivotal shift from ARC to MC-LARC is transitioning from generative tasks to multiple-choice tasks.\n###figure_4### Mini-ARC (Kim et al., 2022  ###reference_b21###) is a benchmark dataset limiting the grid size to 5  5. It simplifies the ARC task by maintaining the core prior knowledge of ARC. It is eliminating the unnecessary process of varying input/output grid sizes. However, unlike the previously introduced 1D-ARC and MC-LARC, Mini-ARC remains similar to ARC in that it retains the characteristics of a 2D generative task. As a result, Mini-ARC remains challenging similar to ARC.\n###figure_5### GPT-4 showed strong performance in MC-LARC, with approximately 75% (Shin et al., 2023  ###reference_b39###), and in 1D-ARC, with about 90% (Xu et al., 2023b  ###reference_b56###). However, it exhibited lower performance in Mini-ARC, around 15% (Qiu et al., 2024  ###reference_b35###). As shown in Fig. 6  ###reference_###, MC-LARC and 1D-ARC reduced the difficulty on a reasoning step level, whereas Mini-ARC differs by reducing image complexity as a benchmark. The significant decrease in difficulty of benchmarks that reduced step-by-step reasoning complexity supports the idea that ARC tasks requires a combination of sequential transformations. The main difference between MC-LARC, 1D-ARC, Mini-ARC, and ARC lies in whether specific functions must be combined to solve the task. 1D-ARC is designed around relatively straightforward transformations, thereby obviating the necessity for devising complex sequences of transformation combinations. Similarly, MC-LARC provides multiple choices, easily solvable by comparing options without the need for multiple stages of inference. However, Mini-ARC presents a unique challenge. Although the grid size is fixed at , the steps required to solve the task as complex as those in the standard ARC. Therefore, the performance difference between MC-LARC, 1D-ARC, and Mini-ARC implies the necessity of combinatorial syntax to combine functions to solve ARC.\n###figure_6###"
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2. Improvements on ARC Accuracy By Adding Object Information",
            "text": "ARC is a benchmark that requires extracting a large amount of semantic information from given images. Two prior research findings support this: 1) the improvement observed in studies providing additional information to solve ARC, and 2) comparative research results with other benchmarks. In one experiment, incorporating object information represented as graphs led to a notable increase in accuracy, with results nearly doublin  (Xu et al., 2023b  ###reference_b56###). This significant increase in accuracy underscores the importance of semantic information as a critical factor in resolving ARC tasks. Further research indicate that ARC contains more abstract information compared to other benchmarks, as illustrated in Table 1  ###reference_###. Chollet, who proposed this benchmark, also argued that traditional feature extraction methods alone would not be sufficient to solve ARC, as it requires complex shapes and even additional information about transformation processes (Chollet, 2019  ###reference_b8###). These observations confirm the importance of employing strategies that can interpret the abstract content essential to solving ARC challenges.\nIn summary, the variation in LLM accuracy with different ARC variations supports the need for combinatorial syntax in solving ARC, while the changes in accuracy with object information highlight the necessity of compositional semantics. These results demonstrate that ARC effectively represents the LoTH perspective, thereby validating the rationale for using it in this study."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Evaluating the Inferential Capabilities of LLMs Using the ARC Benchmark",
            "text": "To evaluate whether LLMs possess inferential capabilities, one could compare these capabilities to human reasoning. As explained in Section 2.1  ###reference_###, according to the Language of Thought Hypothesis (LoTH), human reasoning can be broadly divided into three main components: Logical Coherence (Section 3.1  ###reference_###), Compositionality (Section 3.2  ###reference_###), and Productivity (Section 3.3  ###reference_###). To examine each aspect of reasoning capabilities in LLMs, we utilized ARC, aiming to assess both the validity as a benchmark for evaluating reasoning capabilities and the inferential abilities of LLMs themselves.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_### illustrates this point: a task classified as ‘Entry’, only requires a single step of coloring, while a task classified as ‘Hard’, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. ‘Easy’ and ‘Medium’ are tasks that require relatively more complex steps than ‘Entry’ and fewer steps than ‘Hard’. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_7### ###table_1### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_8### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_9### ###figure_10### To begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_11### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome.\nDirect replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs’ failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_12###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Capability of LLMs 1: Logical Coherence",
            "text": "In Section 3.1  ###reference_###, we aim to evaluate LLMs’ logical coherence, a first fundamental aspect of the LoTH. Logical coherence is the ability to understand a given logic and apply it consistently across different contexts. This concept is crucial in human cognitive processes as it facilitates the construction of sentence structures based on consistent logic, which is essential for solving various tasks. Such an ability is particularly relevant to the rule inference required in ARC tasks, where the challenge is to identify common logical patterns among given examples and use them to deduce the most logically coherent answer.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_###  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_###  ###reference_### illustrates this point: a task classified as ‘Entry’, only requires a single step of coloring, while a task classified as ‘Hard’, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. ‘Easy’ and ‘Medium’ are tasks that require relatively more complex steps than ‘Entry’ and fewer steps than ‘Hard’. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_13### ###table_2### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_###  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_14### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_15### ###figure_16###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1. Experiment",
            "text": "The perceived deficiency in LLMs’ logical reasoning has been a recurrent critique, with direct attempts at solving ARC tasks yielding success rates below 10% (Mirchandani et al., 2023  ###reference_b31###). To mitigate this, enhancements in LLMs’ logical reasoning are being pursued through prompting techniques like Chain of Thought (CoT) (Wei et al., 2022  ###reference_b50###), Least to Most (LtM) (Zhou et al., 2023  ###reference_b61###), and Tree of Thought (ToT) (Yao et al., 2023  ###reference_b58###).\nThese strategies have been shown to effectively leverage LLMs’ reasoning capabilities (Wang et al., 2019  ###reference_b43###), and has the advantage of allowing for a more transparent analysis for humans, as they involve a step-by-step reasoning process. Therefore, in this experiment, we assess the impact of these prompting strategies on LLMs’ logical coherence by solving ARC tasks. We utilized advanced models, namely GPT-4 and GPT-4-32k, to test their logical reasoning with CoT, LtM, and ToT prompts.\n###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21###"
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3. Results",
            "text": "Table 2  ###reference_### displays the performance of LtM, CoT, and ToT when applied to 100 random tasks from the evaluation set. 222Task IDs and used prompts are available at https://bit.ly/Prompt-ARC  ###reference_bit.ly/Prompt-ARC###.\nWe conducted five iterations, reporting the percentage of correctly answered questions for each. Accuracy outside parentheses indicates correct answers only, while inside parentheses indicates both correct answers and processes, as assessed by humans.\nCoT demonstrated 10.6% accuracy, while LtM and ToT exhibited around 6%.\nConsidering both correct answers and processes, accuracy fell to 2–4%, revealing that solution processes often lacked correct reasoning, regardless of prompting technique. This suggests LLMs’ logical reasoning capabilities diverge from human reasoning.\n###table_3### Further analysis from three perspectives was conducted on the LLMs’ responses to ARC tasks to probe their inference mechanisms. We evaluated the tasks LLMs solved and failed, assessed cases with correct answers but flawed processes, and scrutinized the logical coherence when solving the same task type.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_###  ###reference_###  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###  ###reference_b5###  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_###  ###reference_###  ###reference_### illustrates this point: a task classified as ‘Entry’, only requires a single step of coloring, while a task classified as ‘Hard’, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. ‘Easy’ and ‘Medium’ are tasks that require relatively more complex steps than ‘Entry’ and fewer steps than ‘Hard’. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_22### ###table_4### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_###  ###reference_###  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_23### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###  ###reference_###  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_24### ###figure_25###"
        },
        {
            "section_id": "3.1.4",
            "parent_section_id": "3.1",
            "section_name": "3.1.4. Conclusion",
            "text": "In Section 3.1  ###reference_###, we assessed the logical coherence of LLMs by solving 100 ARC tasks using three different prompting techniques. Our results, which showed an accuracy range from 4% to 12%, reveal variability in the reasoning performance depending on the prompting approach employed. While the LLMs displayed a rudimentary level of logical ability on simpler tasks, a deeper qualitative examination exposed underlying inconsistencies.\nOur analysis of problem-solving demonstrated that LLMs could navigate certain tasks with logical precision. Yet, further scrutiny of correctly solved tasks with flawed processes unveiled gaps in their reasoning, suggesting that the LLMs’ logic could be superficial. Moreover, the observation of LLMs faltering on tasks similar to ones they had previously solved indicates a lack of robust logical structure in their reasoning.\nHowever, it is crucial to acknowledge that this study focused on assessing logical capabilities only through varying prompting techniques. Alternative strategies such as domain-specific model fine-tuning or exploring diverse LLM architectures might yield different insights into their logical abilities and coherence. Thus, additional research and experimentation are warranted to fully understand the scope and limitations of LLMs’ inferential skills."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Capability of LLMs 2: Compositionality",
            "text": "In Section 3.2  ###reference_###, we investigate compositionality, the second concept of LoTH. Compositionality refers to the ability to generate complex linguistic expressions using simpler ones. This characteristic allows individuals to effectively tackle more complex tasks by breaking sub-tasks down into simpler steps, supporting the notion that humans can solve more complex tasks when faced with them. Strong compositionality enables the resolution of complex tasks and facilitates transparent descriptions of the process, which is also an important aspect from the perspective of LLMs. This section tests compositionality by treating ARC tasks as stepwise compositions of simpler functions.\nTo begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_26### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Experiment",
            "text": "We test the compositional ability of LLMs through ARC. Compositionality consists of two steps: 1) understanding the meaning of the given expressions and 2) combining appropriately the expressions to obtain the desired output. Therefore, we verify whether LLMs understand the meaning of the functions provided for ARC tasks and whether they can combine the functions appropriately to produce the desired results. The result of this experiment indicates that while LLMs sufficiently understand the functions and their relationship with images, their ability to decompose and combine functions to achieve the desired outcome is weak.\n###figure_27###"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Method",
            "text": "This experiment investigates the compositionality of LLMs by providing basic Domain-Specific Languages (DSLs) representing logical concepts to generate solutions for given ARC tasks. In the context of ARC, DSLs refer to functions that can solve ARC tasks. Chollet has demonstrated that humans can solve ARC using a web interface equipped with basic functions (Chollet, 2019  ###reference_b8###). However, attempts to solve ARC using DSLs through computer algorithms, such as those in prior research have only achieved about 2–30% accuracy (Hodel, 2023b  ###reference_b17###; Wind, 2020  ###reference_b53###). This indicates that ARC tasks can be solved through combinations of DSLs, and while humans can find suitable combinations, computers seem unable to do so, which ultimately relates to the absence of compositional ability.\nTo analyze whether appropriate DSL is selected, we used the ToT prompting technique to generate results step by step. ToT consists of two stages: 1) generating multiple proposals for the next step and 2) selecting the most promising one. In the proposal phase, LLM receives four inputs: the current state of the ARC task, Python-formatted DSLs, natural language descriptions and examples of DSL usage, and definitions of parameters required by the DSLs. Since object information plays a significant role in ARC tasks (Xu et al., 2023a  ###reference_b55###), using the Push-and-Pull clustering algorithm, additional object information based on the current state was provided. (Park et al., 2023  ###reference_b34###). The form of the ARC grid is a 2D array, where object information comes in the form of (x, y) coordinates representing the position of points in the 2D array. The input DSL consists of a total of eight types: ‘rotate’, ‘flip’, ‘coloring object’, ‘rotate object’, ‘color object’, ‘move object’, ‘flip object’, ‘draw a line diagonally from a given point to the end of the grid’, ‘draw a line between two points’, and ‘color a single point’. In the evaluation phase, LLM selects four suitable DSLs to create candidates. Each of the four candidates is scored from 0.001 to 20 points, and the two candidates with the highest scores are applied to generate the next state using a separate simulator. The output is then used as an input prompt recursively provided to LLM for five iterations. The grid obtained from the final output is compared to the correct answer for evaluation."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3. Results",
            "text": "The experiment results are as follows: out of 99 tasks, LLM was not able to solve any tasks. The fact that humans can solve 80% of ARC tasks by combining similar DSLs implies a significant difference in compositional ability between humans and LLMs (Johnson et al., 2021  ###reference_b20###). To understand the reasons for this difference, we analyzed whether 1) LLM can understand the functionality of the DSLs, and 2) LLM can find the proper combination of DSLs to solve the task.\nTo begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_28### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4. Conclusion",
            "text": "In Section 3.2  ###reference_###, we conducted experiments to evaluate the compositionality of LLMs in solving ARC tasks using DSLs. The results lead to the following two conclusions. First, LLM contains an ability to understand inputs including DSLs and grids. Second, the ability to analyze the combination of steps to decompose the task into smaller sub-tasks and achieve the desired result is weak. This indicates that while LLMs have a good understanding ability, they cannot still perform step-by-step planning.\nThe result of the experiment with a specific DSL for addressing ARC tasks also shows that the compositionality of LLMs is insufficient. Over the DSL used in this experiment, there are other DSLs designed to evaluate the compositional capabilities of LLMs such as Hodel (Hodel, 2023b  ###reference_b17###) and Wind (a.k.a. icecuber) (Wind, 2020  ###reference_b53###). However, even if LLMs could fully understand and use these more detailed DSLs, LLMs will be evaluated as lacking in compositional abilities due to the described limitations(the tendency to repeat the same DSL, the inability to find the right combination of DSLs for the answer). Furthermore, if the compositional abilities of the current LLM validator, which evaluates steps, are found lacking, then for LLMs to autonomously combine DSLs to solve ARC problems would be extremely challenging."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Capability of LLMs 3: Productivity",
            "text": "In Section 3.3  ###reference_###, we investigate the third concept of LoTH, productivity. Productivity refers to the ability to generate unseen representations based on observed data. This characteristic allows humans to imagine different situations even from a single phenomenon, thus allowing for efficient learning without the need to learn from data inefficiently each time. Similarly, when equipped with this ability, LLMs are expected to excel in unseen tasks, making it one of the key functions of essential reasoning. The ability to generate new pairs within a limited set of rules could be helpful while solving ARC tasks, highlighting the need for productivity. In this section, we will test productivity by evaluating the validity of created examples based on given example pairs of ARC tasks.\nDirect replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs’ failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_###  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_29###"
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1. Experiment",
            "text": "We conduct experiments using ARC tasks to understand how well LLMs can generate new expressions based on inherent logical concepts. Productivity involves two main steps: inferring specific rules for generating images from example images and natural language expressions and using those rules to generate new, unseen images. However, solving ARC tasks, experimented on in the previous sections so far, is unsuitable for confirming these two processes. For precise evaluation, we propose a new experiment: Given an ARC task and a basic rule shared with similar ARC tasks, can LLMs generate valid examples of the given task? If LLMs can understand a relationship between the given ARC task and the abstract rule, they should be able to infer specific rules for the given task and generate new valid examples. Through this, we want to see if LLMs can imitate human thinking productivity.\n###figure_30###"
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2. Method",
            "text": "To precisely evaluate whether LLMs can infer their own generation rules given ARC examples and create new tasks by appropriately applying these rules, we rigorously controlled the prompts. LLMs receive two types of prompts: example pairs included in the ARC task and descriptions of abstract rules applicable to similar ARC tasks. However, in this case, one example pair was used as the basis for generation, and the remaining examples were used for inferring specific rules for the task. Based on the category of ConceptARC (Moskvichev et al., 2023  ###reference_b32###), which organizes a subset of ARC tasks into 16 distinct categories according to human classification criteria, we developed abstract rules. For each category within ConceptARC, we crafted a corresponding abstract rule, ensuring that tasks within the same category adhere to the identical abstract rule. An example of this abstract rule is shown at the top of the Inverse Transformation Prompt panel in Fig. 15  ###reference_###.\nWe proposed the Inverse Transformation Prompt (ITP), a prompting technique for this experiment. ITP instructs LLMs to generate multiple valid examples using the ARC task and its abstract rules. Fig. 15  ###reference_### demonstrates the process by which LLMs generate new examples given the ARC task and the corresponding ITP. LLMs generate multiple inputs that can form pairs with the output from one example of the task. This example, as mentioned earlier, is selected to serve as the basis for generation and is not included in ITP. If LLMs understood the specific rules corresponding to the ARC task given through ITP, the new example pairs generated by LLMs would be suitable as examples of the task.\nITP is based on many-to-one corresponding to elicit two advantages. First, the method of generating only input is more data-efficient than the method of generating both input and output because the output of the existing ARC task can be used as is. Since all tasks in ARC have example pairs, reusing these examples can be said to make full use of the given data. ITP allows for the reuse of a single ARC task multiple times, making it data-efficient. In particular, using ITP can further increase data efficiency by allowing one ARC task to be reused multiple times by changing the order of examples. Secondly, ITP increases the likelihood of generating valid responses. Through simple simulations, we have seen that inferring inputs from output tends to be more likely to generate valid results than inferring outputs from input. Because generating input from output is subject to relatively less stringent constraints, there is often a wide range of acceptable outcomes.\nIn the process of creating ITP, we encounter two challenges. First, according to the ConceptARC category, there could be multiple solutions within one category. Fig. 15(a)  ###reference_.sf1### illustrates that there are various types of tasks with the same category. Abstract rules given in the same sentences for each category may not be sufficient to cover various types of tasks. Second, there were ARC tasks that made not possible to infer multiple inputs from a single output (Fig. 15(b)  ###reference_.sf2###). In such cases, there was only one valid input. Although we tried to take these cases into account while writing the ITP, these challenges nevertheless harmed the experimental results.\n###figure_31### ###figure_32### Before analyzing the experimental results, it was necessary to redefine the evaluation metric as the object was changed from solving ARC tasks to generating valid examples. As explained earlier, for one example of a particular ARC task, we generated valid inputs that could be paired with the output of that example. To properly generate inputs, LLM must understand the specific rules of the given ARC task through its ITP and apply the understood rules to the output for generating valid inputs. In this experiment, we evaluated whether all generated inputs were valid for each ARC task. This evaluation metric can check both whether the LLM understands the correct rules and whether it generates valid examples based on the rules it understands. Therefore, this experiment systematically evaluates the logical and valid demo pair generation capability of LLMs, contributing to our understanding of their ability to generate new representations."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3. Results",
            "text": "Based on 160 ARC tasks classified by ConceptARC, we evaluated the validity of a total of 2,913 generated examples. The valid generation ratio on average was approximately 17.1%, while the rest were invalid. As we mentioned before, the validity of results was determined by human judgment regarding whether the generated task adhered to the given rule. Results in Table 4  ###reference_### indicate that LLMs demonstrate a certain level of performance in generating examples consistent with the rule. However, since the criteria for validating the generated results as valid or invalid are weak, there is a limitation that results cannot be used before data post-processing even if an infinite number of results can be created. We analyzed the generated inputs to investigate why LLMs were unable to generate valid inputs and identified the following two sources of error: LLMs struggled with rule inference and were weak in step-by-step generation.\n###table_5### Direct replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs’ failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_###  ###reference_###  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_33###"
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4. Conclusion",
            "text": "In Section 3.3  ###reference_###, we conducted experiments to confirm the productivity of LLMs by assessing whether they can understand given ARC tasks in abstracted representations and generate valid new examples based on abstracted rules. Although it is known that LLMs have great strengths in creating creative works, our experimental results reveal that LLMs are weak in understanding rules and producing creations that adhere to those rules. These results indicate that the process by which LLMs generate outputs is closer to mimicking human-generated results and achieving human-level generation abilities for LLMs is challenging.\nFurther research is needed to understand the specific mechanisms behind LLMs’ generation and how to elevate it to the human level. The experiments proposed in this study only examined whether LLMs can generate the given ARC tasks with simple rules and examples. However, it is difficult to determine whether the generated tasks were created following a human-like generation process or if they simply appear valid. Similarly, there is a need to analyze whether the process of incorrect generation resembles that of humans or not."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Discussion",
            "text": "Through the three experiments in Section 3  ###reference_###, we have observed that LLMs demonstrate strengths in understanding and manipulating both image and text inputs. However, they still exhibit weaknesses in logical inference, sequential planning based on understanding, and generating unseen images according to predefined rules. We will conclude by introducing the current research directions aimed at further enhancing LLMs’ ability and outlining the goals after solving ARC."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. What Should LLMs Possess to Solve ARC?",
            "text": "Based on the experimental results of Section 3  ###reference_###, it is evident that LLMs still cannot solve ARC effectively. This is attributed to the deficiencies in logical coherence, compositionality, and productivity. How can we improve the inference capabilities of LLMs? In this section, we explore directions to enhance LLMs from the perspectives of abstraction knowledge and reasoning."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Abstract Knowledge",
            "text": "To solve ARC, the first challenge is the ability to extract the implicit information contained within ARC. Xu et al. argued that object-based representation is crucial for solving ARC and proposed ARGA (Xu et al., 2023a  ###reference_b55###), which converts given example grids into graphs. Their follow-up study (Xu et al., 2023b  ###reference_b56###) involved LLM solving ARC tasks using information obtained from ARGA and showed notable performance for object-based ARC tasks. However, these studies have a fundamental weakness in that they cannot be applied to ARC tasks without objects. Since only about 40% of ARC tasks contain object concepts (Xu et al., 2023a  ###reference_b55###), this method cannot be applied to more than half of the tasks. Wang et al. on the other hand, improved the abstraction ability of LLMs to some extent with a graph-form dataset consisting of 221K textual descriptions, called AbsPyramid (Wang et al., 2023a  ###reference_b48###), and also proposed a framework called AbsInstruct (Wang et al., 2024a  ###reference_b47###) utilizing this dataset. Attempting to structure sentences can be an effective abstraction method for natural language, but its effectiveness cannot be seen in tasks that do not contain sentences."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Reasoning",
            "text": "Another challenge for LLMs in the context of ARC is the vast search space. One method gaining attention to address this is to enable LLMs to generate DSLs themselves. Rajani et al. introduced CAGE (Rajani et al., 2019  ###reference_b36###), which prompts LLMs to generate explanations before generating answers. Subsequently, Wang et al. (Wang et al., 2024b  ###reference_b45###) reported improved results by having LLMs generate DSLs based on hypotheses they set themselves. Additionally, active research is underway on prompting techniques applying algorithmic approaches. Zhou et al. (Zhou et al., 2022  ###reference_b62###) demonstrated enhanced inference performance in LLMs by applying in-context learning. Follow-up research is actively being conducted following CoT and ToT. For example, CoT-SC (Wang et al., 2023b  ###reference_b46###) is a study that selects results through voting from multiple instances of CoT, GoT (Besta et al., 2023  ###reference_b4###) secures flexibility by enabling the generation of graph-like thought nodes, and XoT (Ding et al., 2023  ###reference_b10###) uses the thought tree while Monte Carlo tree search and refines the tree with reinforcement learning. However, these attempts are closer to additional learning for LLMs, and more researches are needed to ascertain whether fundamental improvements in LLMs’ reasoning abilities are achievable."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Limitation of ARC",
            "text": "Does solving ARC signify the completion of human-like AI? To answer this question, two doubts need to be appropriately addressed: 1) Will the ARC solver possess human-level problem-solving abilities? and 2) Will that solver think like humans to solve ARC? It’s not easy to imagine how the ARC solver operates without human-level reasoning. At this point, what we can assume is that the model will have the three properties of LoTH, and the model could be capable of several types of reasoning included in ARC. With this hypothesis, we attempt to address the following questions."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. Will the Model Possess Human-Level Problem-Solving Abilities?",
            "text": "Being capable of reasoning does not necessarily equate to having human-level problem-solving abilities. In other words, even if a model can reason to a level that can solve ARC, it may not have human-level problem-solving capabilities. Various tasks that humans face are generally more complex than ARC and involve various other cognitive factors besides reasoning. Therefore, even models that can solve ARC may have the following limitations compared to human-level problem-solving abilities.\nFirst, with the current ARC criteria, it’s still unknown whether the model that solved it can solve more complex types of tasks. This is because ARC tasks focus on just reasoning and are therefore presented in a relatively simple environment. Whether the reasoning ability learned through ARC would also work in more complex environments has not been revealed. Second, solving ARC does not imply the presence of other components of intelligence beyond reasoning. While reasoning is undoubtedly a core aspect of cognitive processes, it is not the entirety of intelligence. There is research shows that solving human-level complex tasks requires various cognitive abilities (Gardner, 2011  ###reference_b13###)."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. Will the Model Think Like Humans?",
            "text": "Even if we assume that the ARC solver can reason in terms of LoTH, we cannot guarantee whether this solver’s process is human-like for the following two reasons. Firstly, the current ARC provides a performance measure that rewards only for solving a task. It’s important to recognize that such a measure might instigate a wrong purpose, leading to what is known as the King Midas problem (Russell and Norvig, 1995  ###reference_b38###). This problem emphasizes the risk of AI achieving its given objective too literally, leading to unintended negative consequences, underscoring the importance of aligning AI’s goals with human values and the broader context. The policy of rewarding only the results, excluding the solution process, makes it difficult to evaluate whether the solution process is similar to human reasoning. Therefore, models trained on current ARC likely differ in how they solve tasks compared to humans. The second reason is that directly comparing the reasoning processes of humans and language models is challenging. The process by which humans solve ARC tasks has not been investigated, making it unclear how the solving process differs between humans and artificial intelligence. Furthermore, there is a lack of metrics for comparing the solving processes, making direct comparisons difficult."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Future Direction After Solving ARC",
            "text": "To summarize, solving ARC tasks does not directly imply achieving human-level artificial intelligence. Moreover, there is a challenge in comparing task-solving approaches with those of humans. Thus, we suggest three alternatives to more accurately measure human-level inference abilities."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1. Using Different Benchmarks",
            "text": "One limitation of ARC is its simple environment. SQA3D (Ma et al., 2023  ###reference_b26###), for instance, addresses inference tasks in a 3D domain by extending them into question-answering tasks using simulators like ScanNet (Dai et al., 2017  ###reference_b9###). Additionally, benchmarks such as TGIF-QA (Jang et al., 2017  ###reference_b19###), MovieQA (Tapaswi et al., 2016  ###reference_b40###), TVQA (Lei et al., 2018  ###reference_b23###), and STAR (Wu et al., 2021  ###reference_b54###), which append question-answering to videos, have been proposed. Such benchmarks mimicking real-world inference scenarios could serve as supplements to measure complex abstractions not covered by ARC."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2. Quantification of ARC Task-Solving Processes",
            "text": "Chollet, the creator of ARC, argued that ARC maximizes generality while minimizing prior and experience (Chollet, 2019  ###reference_b8###), but these components have not been quantitatively evaluated. As a result, the quantitative assessment of factors such as the generality achieved by models solving ARC, the level of prior knowledge, and the components of prior knowledge remains elusive. One possible way to quantitatively evaluate the process of solving ARC tasks is to quantify the model’s achievement of prior, experience, and generality."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3. Adding Evaluation Methods to Compare Task-Solving Processes with Human Approaches",
            "text": "Recent ARC research has focused on finding ways for AI to solve tasks. However, there are doubts about how similar these solutions are to those of humans. The initial paper by Johnson et al. (Johnson et al., 2021  ###reference_b20###) analyzed human ARC solutions. Subsequently, LARC (Acquaviva et al., 2022  ###reference_b2###) was proposed to analyze how tasks are solved through the language-based explanation of human solutions. Tools for facilitating the collection of human data are also continuously being developed. Kim et al. (Kim et al., 2022  ###reference_b21###), for instance, have analyzed how tasks are solved through O2ARC. It is suggested to not only calculate simple correctness for each ARC task but also to add similarity with human data to the evaluation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusions",
            "text": "In this study, we addressed the limitation of existing research, which predominantly analyzed LLM’s inference ability from a deterministic perspective, by introducing the LoTH perspective to ensure a fair evaluation of the process as well. By comparing and analyzing arguments across various domains of inference, we confirmed that logicality, compositionality, and productivity are quantifiable components to evaluate inference ability. Next, we proposed three experiments using the ARC dataset to quantitatively evaluate these three components from the LoTH perspective. The results showed that although current LLMs exhibit outstanding performance, they lack logicality, compositionality, and productivity in their processes, suggesting that they are closer to probabilistic mimicry rather than possessing autonomous inference abilities. Finally, we explored meaningful research directions for LLM to acquire inference capabilities from the LoTH perspective, as well as alternative approaches beyond ARC. This attempt to quantitatively evaluate the inference process from the LoTH perspective through experimental methods represents a differentiated approach not present in previous research. It contributes by providing a new perspective on how to handle inference capabilities in the field of computer science, going beyond LLMs."
        }
    ]
}