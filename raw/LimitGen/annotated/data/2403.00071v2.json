{
    "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
    "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position better and more robustly. Our extensive LLM experiments also show superior performance after applying Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on both upstream language modeling tasks and a variety of downstream long-text applications.111https://github.com/sheryc/resonance_rope.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in Large Language Models (LLMs) have demonstrated their potential across a wide spectrum of natural language processing tasks, showcasing their ability to handle complex interactions, document analyses, professional writing, and advanced reasoning with a unified approach (OpenAI, 2023  ###reference_b15###; Touvron et al., 2023a  ###reference_b26###, b  ###reference_b27###; Jiang et al., 2024  ###reference_b9###). As these models are increasingly adapted for complex applications, challenges arise in scenarios requiring the comprehension or generation of long texts. Specifically, the train-short-test-long (TSTL) scenario (Press et al., 2022  ###reference_b17###) highlights a limitation where LLMs, pre-trained on shorter sequences, struggle with out-of-distribution (OOD) token positions in longer sequences, impacting their performance in real-world applications (Zhao et al., 2023  ###reference_b32###).\nRecent efforts to enhance TSTL performance have focused on LLMs equipped with Rotary Position Embedding (RoPE) (Su et al., 2024  ###reference_b24###), such as LLaMA (Touvron et al., 2023a  ###reference_b26###, b  ###reference_b27###) and Mistral (Jiang et al., 2023  ###reference_b8###), owing to their exceptional capabilities and widespread adoption. These initiatives aim to refine the test-time computation of RoPE position embedding by introducing a scaling factor to either the position index of each token (Chen et al., 2023  ###reference_b5###) or RoPE’s base value (Xiong et al., 2023  ###reference_b31###; Liu et al., 2024  ###reference_b12###; Peng et al., 2024  ###reference_b16###). These methods ensure that the position embeddings for out-of-distribution (OOD) positions remain within the range experienced during pre-training. This minimizes the need for the model to adapt to new position embedding value ranges, a task that is inherently difficult.\nIn this paper, we introduce Resonance RoPE, a novel technique designed to further narrow the generalization gap on position embeddings in TSTL scenarios. Recognizing that RoPE’s position embedding is governed by a complex, non-linear function, we posit that minimizing extrapolation on OOD positions, while crucial, is insufficient. We argue that it is equally vital to address the interpolation of RoPE features at the OOD positions. By implementing Resonance RoPE, we slightly scale each RoPE feature to correspond to an integer wavelength. This adjustment aligns each RoPE feature’s wavelength with a specific token span length, enabling it to \"resonate\" with a particular local context length. This simple modification effectively reduces the generalization gap for over half of the position embedding features in LLaMA and LLaMA2 under TSTL scenarios. Furthermore, our approach is compatible with RoPE and any RoPE-based scaling techniques, enhancing their performance in TSTL situations without the need for additional computational resources during training or inference.\nAdditionally, to facilitate further research on position embeddings, we present a new synthetic benchmark tailored for TSTL scenarios, named PosGen. Improving position embeddings for TSTL requires a detailed analysis of the cause of failures in handling longer contexts. However, current benchmarks, such as those measuring perplexity in long context (Rae et al., 2020  ###reference_b18###; Huang et al., 2021  ###reference_b7###; Wu et al., 2022  ###reference_b30###) and most synthetic TSTL tasks (Liu et al., 2023  ###reference_b11###; Kazemnejad et al., 2023  ###reference_b10###) face a common issue: the difficulty of generating the next token increases with context length. This makes it difficult to determine whether a model’s failure is due to its inability to generate more complex tokens or its failure to recognize out-of-distribution (OOD) positions. PosGen addresses this limitation by standardizing the difficulty level of token generation across all positions. This ensures that any observed shortcomings are directly related to the model’s inability to identify and handle new token positions effectively.\nOur contributions in this study are threefold:\nWe propose Resonance RoPE, an innovative modification to RoPE based on an in-depth analysis of the wavelengths of RoPE features, aiming to narrow the generalization gap in TSTL scenarios across RoPE and similar RoPE-based scaling techniques, without necessitating extra computational resources during runtime.\nWe present PosGen, a newly developed synthetic benchmark tailored for TSTL scenarios. This benchmark is specifically designed to disentangle the complexities associated with generating tokens in longer contexts from the challenges posed by recognizing new positions or position embedding values.\nThrough rigorous testing of Resonance RoPE on both RoPE and YaRN within the PosGen benchmark, we demonstrate its ability to enhance performance on out-of-distribution (OOD) positions, surpassing existing methods that do not include Resonance RoPE. Moreover, when applied to YaRN, Resonance RoPE further improves LLM’s length extrapolation ability, as evidenced by lower perplexity in upstream TSTL language modeling and enhanced outcomes in downstream tasks involving lengthy contexts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Scaling of RoPE Position Encoding",
            "text": "Recent efforts in extending LLMs’ context window focus on manipulating position embedding (PE), particularly RoPE (Su et al., 2024  ###reference_b24###), which is used in LLMs like LLaMA (Touvron et al., 2023a  ###reference_b26###, b  ###reference_b27###) and Mistral (Jiang et al., 2023  ###reference_b8###). Main strategies include embedding scaling (Chen et al., 2023  ###reference_b5###; Liu et al., 2024  ###reference_b12###; Peng et al., 2024  ###reference_b16###) and randomizing token positions (Ruoss et al., 2023  ###reference_b22###; Zhu et al., 2024  ###reference_b34###). Our emphasis is on the embedding scaling strategies.\nExisting embedding scaling strategies adjust position embedding for longer sequences to match the pre-training range, avoiding feature extrapolation. For instance, Chen et al. (2023  ###reference_b5###) compresses position indices to fit the pre-training range, extending LLaMA’s (Touvron et al., 2023a  ###reference_b26###) context to 16K with 1,000 steps of fine-tuning. Alternatively, Liu et al. (2024  ###reference_b12###); Rozière et al. (2023  ###reference_b21###); Xiong et al. (2023  ###reference_b31###) modify RoPE’s rotary base and employ fine-tuning on extended sequences, termed Adjusted Base Frequency (ABF) or \"NTK-aware\" scaling. Code LLaMA (Rozière et al., 2023  ###reference_b21###) achieved 16K context length with this method after 10,000 fine-tuning steps. YaRN (Peng et al., 2024  ###reference_b16###) improved NTK-aware scaling by segmenting RoPE features and applying tailored extrapolation strategies, achieving 64K context length for LLaMA2 (Touvron et al., 2023b  ###reference_b27###) with 400 fine-tuning steps. Distinguishingly, our Resonance RoPE focus on reducing feature interpolation on OOD positions, which we argue is another important factor in improving the length extrapolation capability of Transformer."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Long Context Evaluations",
            "text": "Evaluations of Transformer-based LLMs’ long-context capabilities are twofold: synthetic task assessments for length extrapolation strategies and real-world task evaluations at the LLM scale. Synthetic evaluations target simple tasks such as long sequence classification Tay et al. (2021  ###reference_b25###) and arithmetic language modeling (Liu et al., 2023  ###reference_b11###; Kazemnejad et al., 2023  ###reference_b10###).\nLLM scale evaluations measure metrics such as perplexity (PPL) in extensive text corpora (e.g., PG19 (Rae et al., 2020  ###reference_b18###), GovReport (Huang et al., 2021  ###reference_b7###), GitHub (Wu et al., 2022  ###reference_b30###)) and complex tasks including summarization, question answering, and mathematical reasoning (An et al., 2023  ###reference_b1###; Bai et al., 2023  ###reference_b3###; Shaham et al., 2023  ###reference_b23###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Rotary Position Embedding (RoPE)",
            "text": "In Transformers (Vaswani et al., 2017  ###reference_b28###), the self-attention scores are softmax-normalized scaled attention logits :\nSuppose the input to a single attention head is , where  is the sequence length and  is the dimension of an attention head. RoPE injects the position information of each token into the  and  vectors by the following equations in the complex space:\nwhere  are trainable parameters, and  is a constant called the rotary base, which is set to   (Su et al., 2024  ###reference_b24###) or other integers or fractions (Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###). This form makes the dot product between the -th query  and -th key  only depend on the input  and their relative distance :\nRoPE’s real-number implementation divides the -dimension space into multiple -dimensional subspaces and applies real rotation matrix to each of them. Formally, define\na  block-diagonal matrix:\nwhere , and each  is a  rotation matrix:\nRoPE computes the attention logit  as follows:\nFor each two dimensions  of  and , its corresponding  reflects a temporal wavelength . This wavelength describes the token length for the corresponding RoPE features to encounter approximately the same rotary angle  in Equation 3  ###reference_###:\nAs an example, the wavelengths of LLaMA / LLaMA2’s RoPE features range from  for  to  for ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Critical Dimensions of RoPE",
            "text": "In a TSTL scenario (Press et al., 2022  ###reference_b17###), one takes a model trained on texts with lengths up to , and tests it on a task with input lengths up to , with the scaling factor .\nRecently, Liu et al. (2024  ###reference_b12###) discovered that there may exist two “critical dimensions” in RoPE features, which correspond to the dimensions  that satisfies  and . The dimensions of RoPE features above and below the critical dimension (which we denote as “post-critical dimensions” and “pre-critical dimensions”, respectively) have different behaviors in TSTL: for post-critical dimensions (i.e., ), since their wavelengths satisfy , the training corpus does not cover all possible rotary angles  on a unit circle. Thus, these dimensions will encounter OOD value range on longer sequences. This is not an issue for pre-critical dimensions due to their shorter temporal wavelengths.\nThe concept of RoPE’s critical dimensions implicitly guides the development of RoPE scaling methods. For example, previous RoPE scaling methods (Chen et al., 2023  ###reference_b5###; Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###) mainly focus on reducing or avoiding value extrapolation on post-critical dimensions, and minimize post-training modifications to the pre-critical dimensions."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Yet another RoPE extensioN (YaRN)",
            "text": "YaRN (Peng et al., 2024  ###reference_b16###) is the current state-of-the-art RoPE scaling method for TSTL. It introduces the “NTK-by-parts” scaling for RoPE, which applies different scaling strategies to each RoPE feature according to its temporal wavelength.\nIn a TSTL scenario with scaling factor , YaRN scales the wavelength of the -th RoPE feature  to  and further fine-tune the model:\nwhere  is a piece-wise function depending on its corresponding wavelength , and two hyperparameters  and :\nEmpirically, for the LLaMA family, Peng et al. (2024  ###reference_b16###) suggests using  and . This setting avoids value range extrapolation on post-critical dimensions, while reducing modifications to the original pre-critical dimensions.\nIn addition to the “NTK-by-parts” RoPE scaling strategy mentioned above, YaRN also comprises a scaling strategy on the attention scores, which reduces the change in the entropy of the attention score on longer sequences. We maintain the complete design of YaRN in our experiments, but our analysis will focus on its RoPE scaling strategy."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Method: Resonance RoPE",
            "text": "In this section, we introduce Resonance RoPE, a universal improvement for RoPE and RoPE-based scaling methods to (further) improve their length extrapolation performance.\nSuppose we abstract RoPE’s Equation 4  ###reference_###, 5  ###reference_###: for any , we define . In a TSTL scenario where we generalize an LLM from length  to length , let us denote a scaled RoPE function by . To perform well on OOD positions it should reduce the feature gap  between token features seen during training and token features after scaling that we can define for each -th feature as:\nwhere  and  is the set of feature vectors to which we apply a position embedding. Note that the formulation of the feature gap is similar to the “embedded vector distance” metric proposed by Xiong et al. (2023  ###reference_b31###). However, these two metrics target totally different aspects of RoPE scaling methods. A more detailed comparison can be found in Appendix B  ###reference_###.\nExisting RoPE scaling methods (Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###) mainly focus on the post-critical dimensions  of RoPE, since the rotary angle  on these dimensions extrapolates on OOD positions, hence creating a feature gap. In this section, we argue that reducing RoPE’s feature interpolation on the pre-critical dimensions  is also beneficial for better length extrapolation.\nDue to a non-linear relationship between RoPE feature  and the token position  in Equation 3  ###reference_###, the interpolation on RoPE features is potentially hard for the model to generalize to.\nWe found that such potentially hard interpolation appears on the pre-critical dimensions , which have wavelengths  shorter than the pre-trained sequence length . By default, the rotary base  of RoPE features is an integer or a fraction, which makes their wavelength  not an integer. As the position index  increases, a phase shift of  occurs for the rotary angle  after each full rotation. This could potentially result in a large distribution gap between the RoPE features on positions seen during training and the OOD positions. This phenomenon is illustrated in Figure 1  ###reference_###.\n###figure_1### We tackle this issue by developing a synergistic modification to the conventional RoPE embedding, referred to as Resonance RoPE. It aims to identify the optimal angular frequency that minimizes the interpolation gap, which ensures the corresponding wavelength closely matches the original one while imposing alignment of the wavelength to an integer.\nMore specifically, for a given angular frequency set of RoPE , we round their wavelengths to their nearest integer to eliminate new rotary angles on each feature. We provide a pseudocode for Resonance RoPE in Algorithm 1  ###reference_###.\nAfter applying this technique, each RoPE feature repeats after  tokens, and therefore “resonates” with a specific span length and eliminates the interpolation gap between pre-trained and OOD positions on pre-critical dimensions. We illustrate the effect of Resonance RoPE on RoPE’s feature gap on one of the pre-critical dimensions in Figure 1  ###reference_###. Moreover, we can prove the feature gap reducing ability of our method. As for above, we formalize Resonance RoPE’s computation rule as .\nFor a RoPE-equipped model with context window , Resonance RoPE  reduces the feature gap on pre-critical dimensions to . Specifically, , , we have:\nfor all .\nSee the proof in Appendix A  ###reference_###.\nNote that although each pre-critical RoPE feature  repeats, the combination of all\n\nonly repeats after the least common multiple (LCM) of all pre-critical dimensions’s wavelengths. For LLaMA2, this LCM value is greater than .\nBecause of its simplicity, Resonance RoPE can be applied on top of RoPE and all RoPE-based scaling methods to reduce their feature gap in TSTL and further improve their performance. Meanwhile, this method only involves an offline computation of the scaled , thus introducing no online computation overhead."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluating Position Embeddings with PosGen",
            "text": "###figure_2### In this section, we propose our new position embedding evaluation suite: PosGen, based on an analysis of common failure patterns on existing position embedding evaluation methods.\nWe consider a next token prediction task, where we expect the model to generate the token  given the input sequence . In TSTL scenarios, when a model succeeds in correctly generating a token up to position  but fails systematically afterwards, we observe two failure patterns:\nFailure due to harder algorithmic difficulty on generating later tokens.\nThe rule of generating a new token  may vary with the sequence length .\nGenerally, tokens placed later in the sequence depend on more context tokens, which incurs a more complex dependency pattern. During training on shorter sequences, the model only learns the token dependency rules involving up to  tokens, and might fail on longer sequences because it has never been exposed to the more complex dependency rules.\nFailure due to unrecognized new token positions. The difference between training and testing lengths in the TSTL setting creates a feature gap between the position indices or position embeddings in training and inference. This feature gap makes it difficult for the model to generalize to new positions due to unrecognized features. RoPE scaling methods mainly focus on reducing this type of length extrapolation failure.\nCurrently, neither perplexity-based evaluations (Rae et al., 2020  ###reference_b18###; Huang et al., 2021  ###reference_b7###; Wu et al., 2022  ###reference_b30###) nor synthetic TSTL evaluations (Kazemnejad et al., 2023  ###reference_b10###; Liu et al., 2023  ###reference_b11###) can effectively distinguish these two failure patterns, since the token generation difficulty tends to increase with respect to the sequence length in these tasks. To facilitate research on better position representations, we design PosGen, which controls the difficulty in generating tokens throughout the sequence to be identical, which effectively distinguishes the two types of TSTL failures. Failures in this benchmark are only due to the inability to recognize new token positions in TSTL scenarios.\nOur PosGen framework comprises three sub-tasks, with each extracting the general token dependency pattern of a different type of reasoning task. Suppose that we define a fixed function , where  is the model’s vocabulary and  are predefined constants controlling the task’s difficulty. The three subtasks of PosGen are as follows:\nRecursive. This task simulates the token dependency pattern of generating a Fibonacci-style sequence, where new tokens depend on  neighboring tokens only:  when .\nChain-of-Thought (CoT). This task simulates the token dependency pattern of CoT reasoning (Wei et al., 2022  ###reference_b29###), where new tokens depend on  neighboring tokens (simulating the previous reasoning step) and  tokens in the front (simulating the original question):  when .\nSemi-recursive. This task simulates the token dependency pattern of the last-letter concatenation task (Zhou et al., 2023  ###reference_b33###), where new tokens depend on both  neighboring tokens (simulating the current progress) and  tokens with varied distances according to a specific rule (simulating the word sequence):  when .\nBased on the equation for each subtask, when given the first  tokens, one can generate a sequence with unlimited length as the ground truth sequence. We show an example of PosGen in Figure 2  ###reference_###. As a TSTL benchmark, we train a model on a subtask with sequence length up to , and evaluate the model’s accuracy on a longer sequence with length  generated by the same rule on the unseen positions , which we refer to as the “OOD Accuracy” (OOD Acc).\nThis metric measures how well a model can recognize the OOD positions and continue following the generation rule learned during training.\nAs a benchmark for position embeddings, a standard usage of this benchmark is to train a small Transformer (e.g., a 2-layer Transformer as used in our experiments) with different position embeddings on its training set with only short sequences, and test its OOD Accuracy on the test set with longer sequences.\nWe provide our experiment setting for PosGen in more details in Section 6.1.1  ###reference_.SSS1### and Appendix C.1  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate Resonance RoPE on three different TSTL tasks: a small-scale evaluation on our proposed PosGen task, and LLM-scale evaluations with LLaMA2-Chat (Touvron et al., 2023b  ###reference_b27###) on both language modeling perplexity and real-world long context applications."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Synthetic Task Evaluation",
            "text": "###figure_3###"
        },
        {
            "section_id": "6.1.2",
            "parent_section_id": "6.1",
            "section_name": "6.1.2 Results and Analysis",
            "text": "Table 1  ###reference_### displays the comparison of the OOD accuracy. In most cases, Resonance RoPE and Resonance YaRN outperform their counterparts lacking the Resonance technique, showcasing significantly better performance and reduced variance in OOD scenarios. This improvement indicates a superior adaptation to OOD position embeddings through minimized Positional Encoding (PE) interpolation. An exception is observed when applying Resonance RoPE to the Recursive subtask, likely due to the dominance of extrapolated post-critical dimensions in OOD positions. This issue can be mitigated by employing a RoPE scaling technique such as YaRN, which effectively counters the extrapolation of post-critical dimensions. Among all configurations, Resonance YaRN exhibits the highest OOD performance, demonstrating the synergy between RoPE scaling methods and the Resonance technique.\nFigure 3  ###reference_### plots validation losses against training epochs for different PEs, illustrating the training dynamics. The introduction of the Resonance technique leads to a reduction in the lowest validation loss for both RoPE and YaRN, with Resonance RoPE achieving even lower validation losses than YaRN in the Semi-Recursive subtask. Furthermore, the validation loss trajectories for Resonance RoPE and Resonance YaRN remain lower than those of their counterparts in all subtasks, further demonstrating the enhanced OOD generalization capability of our approach."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "LLM Fine-tuning Evaluation",
            "text": ""
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Experiment Setup",
            "text": "In this section, we apply our proposed Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN (Peng et al., 2024  ###reference_b16###).\nMore specifically, we replace the original position embeddings of LLaMA2 7B and 13B (Touvron et al., 2023b  ###reference_b27###) with a series of scaled position embeddings, including the NTK-Aware scaling (bloc97, 2023  ###reference_b4###; Xiong et al., 2023  ###reference_b31###; Liu et al., 2024  ###reference_b12###), Dynamic NTK-Aware Scaling (Peng et al., 2024  ###reference_b16###; Rozière et al., 2023  ###reference_b21###), and YaRN (Peng et al., 2024  ###reference_b16###).\nFor YaRN and Resonance YaRN, We use a scaling factor of  and  for LLaMA2 7B and 13B to extend their context window from K to K and K, respectively.\nFor the configurations that require fine-tuning, we fine-tune the LLM with the scaled position embedding on the training set of PG19 (Rae et al., 2020  ###reference_b18###) with the fine-tuning setting and hyperparameters adopted directly from YaRN (Peng et al., 2024  ###reference_b16###), with the only difference being that we control the total training token count to be approximately M. A more detailed fine-tuning setting can be found in Appendix C.2  ###reference_###. We test the model’s performance on two TSTL scenarios: language modeling evaluation on long-text sequences and long-text downstream application performance."
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Perplexity on Long Sequence",
            "text": "###figure_4### We evaluate the model’s language modeling performance on GovReport (Huang et al., 2021  ###reference_b7###) and Proofpile (Azerbayev, 2022  ###reference_b2###). We randomly select  samples from each dataset and report the final perplexity in text fragments of gradually increased length. We report the results in Figure 4  ###reference_###. Of the tested methods, Resonance YaRN achieves the lowest perplexity across all context lengths. Especially, Resonance YaRN achieves a lower perplexity compared to YaRN with the same set of hyperparameters optimized for YaRN, demonstrating the benefit of applying the Resonance technique to existing RoPE scaling methods."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 Real-world Task Evaluation",
            "text": "Lastly, we test the real-world task performance of LLaMA2-Chat 7B and 13B’s performance with different RoPE scaling strategies on L-Eval An et al. (2023  ###reference_b1###)’s close ended task suite, a long-text LLM benchmark covering a wide range of domains such as school lectures, long conversations and novels. We fine-tune the model with different RoPE scaling strategies using two different strategies: training on shorter sequences (4K length) for more epochs, and training on longer sequences (32K or 16K length) for less epochs. All settings requiring fine-tuning keep the training token count to be approximately 100M. The results are listed in Table 2  ###reference_###.\nAlthough no single setting in the experiment achieves the best result on all subtasks, we observe that applying Resonance YaRN achieves better average performance in different training settings and model sizes compared to its counterpart YaRN setting. This further proves the compatibility of the Resonance technique and RoPE scaling methods, and the better length extrapolation performance brought by our proposed method."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce Resonance RoPE, a novel enhancement of RoPE that focuses on minimizing the interpolation of RoPE features for OOD positions, thereby reducing the generalization gap and improving LLM’s performance on train-short-test-long (TSTL) scenarios.\nAdditionally, we present a novel synthetic benchmark, PosGen, which provides a fine-grained analysis of the model’s TSTL performance regarding various token dependency patterns.\nExtensive experiments on our proposed PosGen and two LLM-based evaluations demonstrate Resonance RoPE’s efficacy in identifying OOD positions and its compatibility with current RoPE scaling strategies.\nFuture work includes exploring Resonance RoPE’s performance on other foundational models, and the identification of more optimal wavelength combinations for RoPE features."
        }
    ]
}