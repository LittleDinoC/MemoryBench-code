{
    "title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing1footnote 11footnote 1Our replication package can be accessed at https://doi.org/10.7910/DVN/QTT84C.",
    "abstract": "The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rise of pretrain-finetune paradigm has greatly reshaped the landscape of natural language processing (NLP) (E. Hu \\BOthers., \\APACyear2022  ###reference_b3###). This new paradigm, characterized by the application of large pretrained language models, most notably BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###), and high efficacy on finetuned tasks even in the face of relatively few training samples, is now being widely applied in social sciences (Zhang \\BOthers., \\APACyear2021  ###reference_b18###; Wang, \\APACyear2023\\APACexlab\\BCnt1  ###reference_b15###, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###). While earlier works have recommended a minimum of 3,000 samples for NLP tasks using the bag-of-words approach (Wang \\BOthers., \\APACyear2022  ###reference_b17###), recent research applying the pretrain-finetune paradigm has shown that finetuned large models with just a few hundred labeled samples could yield competitive performance (Wang, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###).\n###figure_1### In the current tutorial, we aim to provide (1) an overview of the pretrain-finetune paradigm and (2) illustrative applications of the pretrain-finetune paradigm to research questions in social sciences (Figure 1  ###reference_###). We first provide an introduction to the key concepts in pretraining and finetuning, such as tokenization and encoding. We then use some of the most recent applications of the pretrain-finetune paradigm in social sciences to illustrate how to use this paradigm to advance quantitative research in our discipline."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Pretrain",
            "text": "Pretraining is the process of training a model with unlabeled raw data with no particular downstream tasks. Some of the most widely used pretrained models include BERT and RoBERTa. These large language models usually contain hundreds of millions of parameters. Pretraining these models from scratch requires access to large amounts of raw data and specialized hardware like graphics processing units (GPUs). The pretraining process consists of the following steps: (1) tokenization, (2) encoding, and (3) pretraining tasks, such as masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence-order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "Unlike earlier methods such as bag of words (Wang \\BOthers., \\APACyear2022  ###reference_b17###) or word embeddings (Mikolov \\BOthers., \\APACyear2013  ###reference_b10###), large langauge models mostly use subwords as a token, such as wordpieces in BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and Byte-Pair Encoding (BPE) in RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###) and GPT-3 (Brown \\BOthers., \\APACyear2020  ###reference_b1###). To illustrate how words are broken into subwords, let’s use a text snippet from the Journal’s description and tokenize it using the BPE tokenizer used in GPT-3.5 and GPT-4.222The snippet is taken from https://journals.sagepub.com/description/AMP. The tokenizer is available at https://platform.openai.com/tokenizer and was accessed on January 6th 2024..\nThe word “AMPPS” is split into “AM”, “PP” and “S”. The word “methodological” is split into “method”, “ological”. And lastly the word “non-methodology” is split into “non”, “-method”, and “ology”. All other words, including punctuation marks, remain individual units without further splitting. These units are then turned into tokens represented with non-negative integers.\nThese units are then turned into tokens represented with integers."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Encoding",
            "text": "Once we have tokenized the input text into tokens, we then retrieve the corresponding embeddings for these tokens, where the token id, a non-negative integer, would serve the retrieval key. These token embeddings are vector representations. For the BERT-large model, each such vector contains 1024 float numbers. These sets of vectors are then fed into the encoder layers.333BERT models use encoders and are the focus of this tutorial. Note that GPT models use decoders.\n###figure_2### The key component of the encoders is self-attention (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###). Intuitively, it computes how much attention each token pays to the other tokens and generates the new representation for a token by calculating the weighted average of all the tokens where the weight is based on self-attention.444For a similar illustration, readers can refer to Kjell \\BOthers. (\\APACyear2023  ###reference_b6###). For a more in-depth illustration, please refer to the original paper (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###) and the tutorial on transformers at http://jalammar.github.io/illustrated-transformer/. Such encoders are then put on top of each other in a sequential manner. For example, in BERT-large models there are 24 encoder layers. In BERT-base models there are 12 encoder layers."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Pretraining tasks",
            "text": "Now that we have a good understanding of tokenization and encoding, let’s proceed to discuss how these parameters in these layers are trained using pretraining tasks. The goal is to train these large language models from scratch and embue them with world knowledge that exists in the training corpus (Longpre \\BOthers., \\APACyear2020  ###reference_b9###). Commonly used pretraining tasks include masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###).555Later research has shown that the next sentence prediction does not contribute much to the pretraining process (Liu \\BOthers., \\APACyear2019  ###reference_b8###).\n###figure_3### Let’s see an example of how masked language modeling works. Suppose we have the following input text: “I enjoy trying new activities”. During training, some of the tokens will be randomly masked and the language model is tasked with predicting these masked tokens. In Figure 3  ###reference_###, the token “activities” is masked. The language model then is tasked with predicting this masked token. All tokens in the vocabulary are eligible candidates. Suppose the model predicts “activities” with a probability of 0.2. If we use cross entropy loss, then the loss term for this prediction is -log(0.2), which is its negative log-likelihood.666For details, please see https://github.com/google-research/bert/blob/master/run_pretraining.py#L303. Note that the higher probability the model assigns to the corrected token, the lower the loss. The language model is trained on this task over the entire corpus for a few times until the loss in prediction stops decreasing."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Finetuning",
            "text": "Finetuning is the process of training an already-pretrained model on a downstream task. It is characterized by (1) adding a small set new parameters to accommodate the new tasks and (2) a relatively small learning rate given the fact that the vast majority of the parameters are already reasonably trained during the pretraining stage.777In terms of training procedures, it is the same as training a supervised model from scratch. Readers could refer to a recent tutorial on how to train supervised models by Pargent \\BOthers. (\\APACyear2023  ###reference_b12###). Downstream tasks at this stage can be broadly grouped into classification and regression. Examples of classification tasks include sentiment analysis and topic classification; examples of regression include fatality prediction. This is the step where quantitative psychologists need to apply a pretrained model to a particular task, such as depression detection and personality classification. For each specific task, researchers need to specify the model’s input, targeted output (including the number of labels), and whether this is a classification task or a regression task.\n###figure_4### In Figure 4  ###reference_###, we illustrate how a k-class classification works. From the encoder layers, we retrieve a 1 by N vector for each input sample, where N is 768 in the case of BERT-base and 1024 in the case of BERT-large. Then we project this vector to 1 by K using an N by K matrix. This layer is often referred to as the fully connected layer. What follows next depends on whether the task is classification or regression. In the case of classification, where K is equal to or greater than 2, we apply a softmax function such that each of the K elements represents the probability of that corresponding class being the predicted class:888For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html.\nIn the case of regression, where K is 1, we can apply the loss function directly to the 1 by K vector (now 1 by 1). Mean squared error is a commonly used loss function for regression tasks.999For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Practical Exercises",
            "text": "In this section, we provide two practical exercises to illustrate how researchers can finetune a large language model with relative ease to achieve state-of-the-art results in classification and regression, respectively.101010Some of the original results on classification and regression have been published at Political Analysis. All our exercises are written in Python in the format of Jupyter notebooks so that readers can follow along in an interactive manner.111111https://jupyter.org. For easy reproducibility and access to computation resources, all our computation is done on Google Colab.121212https://colab.research.google.com."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Multi-class classification",
            "text": "###figure_5### When researchers need to classify text inputs into more than two classes, this is what we call multi-class classification. In the following case study, we classify a given text into one of eight topics. The texts are the speech transcripts from the New Zealand Parliament from 1987 to 2002 (Osnabrügge \\BOthers., \\APACyear2021  ###reference_b11###). In Figure 5  ###reference_###, we illustrate the data input and list all the eight classes. We use using 2,915 annotated speech transcripts for training, 625 for validation, and another 625 for testing.\nWe finetune a RoBERTa-base model (Liu \\BOthers., \\APACyear2019  ###reference_b8###) for topic classification. RoBERTa-base has 12 layers of transformers and 125 million parameters in total. On top of its 12 layers of transformers, we add a classification layer for 8-topic classification. We use cross entropy as the loss function. We finetune the RoBERTa-base model for 20 epochs with a learning rate of 2e-5, a batch size of 16, and an input sequence length of 512 on an A100 GPU. We use the validation set’s cross entropy loss to select the best epoch and the optimal checkpoint. We then use the optimal checkpoint to make inferences on the test set with a batch size of 64 (see Listing 1  ###reference_###). As our baseline, we use results from Osnabrügge \\BOthers. (\\APACyear2021  ###reference_b11###), where the authors use 115,410 related but out-of-domain policy statements from Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States to train a regularized multinomial logistic regression model. For easy comparison, we use the same evaluation metrics as in Osnabrügge \\BOthers. (\\APACyear2021  ###reference_b11###).\nAcross all metrics, finetuning the RoBERTa model with 2,915 New Zealand parliamentary speeches substantially outperforms the cross-domain topic classifier by Osnabrügge \\BOthers. (\\APACyear2021  ###reference_b11###), which is trained using 115,420 annotated policy statements (Table 1  ###reference_###). For example, for top-1 accuracy the finetuned RoBERTa model outperforms the cross-domain baseline by 26%. For top-3 accuracy the finetuned RoBERTa model outperforms the cross-domain baseline by 10%."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The pretrain-finetune paradigm has revolutionized the field of natural language processing. In this tutorial, we have provided an intuitive and thorough walk-through of the key concepts therein. We also introduced some of the most common use cases that the pretrain-finetune paradigm supports. To help facilitate the wider adoption of this new paradigm, we have included easy-to-follow examples and made the related datasets and scripts publicly available. Practitioners and scholars in quantitative psychology and psychology more broadly should find our tutorial useful."
        }
    ]
}