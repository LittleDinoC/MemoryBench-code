{
    "title": "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
    "abstract": "We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical.\nExisting work has had mixed success using LLMs to accurately model human behavior in different contexts.\nWe benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the last decade, large language models (LLMs) have evolved significantly for natural language processing tasks, code generation, and as conversational UIs (Zhao et al., 2023  ###reference_b22###).\nExisting work generally finds that it is possible to elicit strong agreement between LLM responses and human responses (Dubois et al., 2023  ###reference_b7###). This concordance suggests a possibility to\nuse LLMs as a statistical proxy to study human beliefs, preferences, and behaviors.\nThis would allow leveraging models that\nwere trained on large, internet-scale datasets in narrow domains where comparatively small amounts of data are\navailable.\nThis might be used, for example, for a company to leverage a comparatively small survey sample\nto understand customer preferences with respect to possible new products, to conduct virtual surveys, and to pilot behavioral interventions, such interventions to drive the adoption of\nsustainable technology. The value for intervention research is especially notable in the case when interventions would be impractical or unethical, such as building large amounts of infrastructure or restricting access to infrastructure. To enable such applications, it is crucial to ensure\nthat LLMs exhibit behavior that is a statistically accurate model of real human behaviors.\nExisting literature on this matter finds conflicting results. For example, Serapio-García et al. (2023  ###reference_b16###) find that it is possible to prompt LLMs in the pathways language model (PaLM) family to exhibit consistent and clearly measurable personality traits.\nConversely, Gui & Toubia (2023  ###reference_b9###) find significant challenges in emulating human behaviors, specifically in the context of simulated demand for Coca-Cola. These experiments show that\nin general, it is difficult to say a priori whether a pre-trained LLM will accurately model a behavior of interest. In this paper, we consider the problem of aligning the beliefs and preferences of a language model so that it can serve as a statistical proxy for a real human population. We consider a macro, population-wide metric as well as a micro, per-individual metric, and we propose a novel penalty term in the loss function to improve performance on numerical survey questions.\nWe emphasize that the ultimate goal of this work is not to produce a survey-answering bot, but instead to align a language model with the beliefs and preferences of real humans as expressed in a survey. The ultimate goal is to arrive at interactive models that enable the study of a target population. Our results indicate that it is easier to model population-wide statistics than individuals, suggesting that one-on-one interviews may be difficult to replicate. However, our population-wide models may still be useful in the context of population-wide studies, for example in the context of marketing, or community-wide simulations, such as those of  Park et al. (2023  ###reference_b13###).\nIn our experiments, we leverage an existing survey on human beliefs and preferences about battery-electric vehicles (BEVs) (Arechiga et al., 2022  ###reference_b2###). This survey includes interventions intended to increase the preference for BEVs. The contributions of this paper are as follows.\nUsing the survey data of  Arechiga et al. (2022  ###reference_b2###) on real human study participants, we demonstrate the use of parameter-efficient fine-tuning techniques to improve the agreement of LLMs to human preferences as expressed in survey data.\nWe investigate the effects of model size, and find that larger pre-trained models provide the best out-of-the-box performance, but this advantage largely disappears after fine-tuning.\nWe investigate the effects of quantization and sampling temperature. We find that quantizing fine-tuning techniques such as QLoRA (Dettmers et al., 2023  ###reference_b4###) provide minimal degradation but large savings in computation. We find that the temperature parameter allows trading off between agreement with population-wide statistics vs matching per-individual responses.\nWe propose and evaluate a novel penalty term on the loss function to improve model performance on survey questions that require a numerical response.\nWe benchmark against two baseline algorithms trained de novo on given survey data, and demonstrate that the fine-tuned LLMs are able to outperform these baseline algorithms under specific settings. Section 2  ###reference_### describes related work, Section 3  ###reference_### provides the technical background of our work. Section 4  ###reference_### describes the problem statement and Section 5  ###reference_### describes our proposed approach. Section 6  ###reference_### presents our experiments and Section 7  ###reference_### concludes and describes directions for future work."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The possibilities of simulating humans and human behaviors using LLMs (Kaddour et al., 2023  ###reference_b12###), or role-play (Shanahan et al., 2023  ###reference_b17###; Wu et al., 2023  ###reference_b21###), have been discussed in recent work. By applying established psychometrics,\nSerapio-García et al. (2023  ###reference_b16###) demonstrated that LLMs can reliably simulate personalities and that LLM-generated personality traits can be shaped and controlled to imitate specific personality profiles.\nDillion et al. (2023  ###reference_b6###) showed a strong alignment between GPT-3.5 and humans in moral judgments, with a correlation of 0.95. In addition, LLM-based generative agents, when organized as a collective in an interactive sandbox environment, were found to be able to produce believable behaviors not only on an individual level but also on a social level (Park et al., 2023  ###reference_b13###).\nThe ability of LLMs to generate human-like personalities, judgments, and behaviors hints at the opportunity of constructing synthetic human participants in behavioral studies. Several recent works show initial attempts in this direction.\nAher et al. (2023  ###reference_b1###) applied LLMs to simulate human subjects and found that they can reproduce three out of four economic, psycholinguistic, and social psychology experiments and replicate findings from prior studies with real human participants.\nHämäläinen et al. (2023  ###reference_b10###) evaluated LLMs’ potential of generating synthetic human-computer interaction research data in the form of open-ended questionnaire responses and revealed their capability of generating plausible, human-like self-report data regarding subjective experiences.\nHowever, previous work simply measured concordance between LLMs and participant data, and other work such as that of  Gui & Toubia (2023  ###reference_b9###) find a lack of agreement in domains such as predicting product pricing. Our work provides a framework to align LLMs to human preferences and explores various techniques to improve the level of agreement."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Auto-regressive large language models\nAutoregressive language models learn to predict the next token in a stream of language tokens. Formally, given a sequence of tokens\n, the model learns to predict a probability distribution . An important property of these models is\nthat they can be trained in an unsupervised way, i.e., no manually produced labels are required. All that is required is a large corpus of natural text. Many such\ncorpora have been assembled from the internet, for example Gao et al. (2020  ###reference_b8###) and Computer (2023  ###reference_b3###). A commonly used architecture is the\ntransformer architecture (Vaswani et al., 2017  ###reference_b19###), and specifically the decoder-only transformer with a causal mask, which prevents the model from using information from future tokens (Radford et al., 2018  ###reference_b15###).\nFine-tuning large language models\nA common approach to use LLMs is to pre-train on a large text corpus, and then fine-tune the resulting model for a specific downstream task (Radford et al., 2018  ###reference_b15###; Devlin et al., 2018  ###reference_b5###). Downstream tasks may include sentiment analysis, question answering, text summarization, etc. The fine-tuning procedure involves updating all of the parameters of the model, and can be computationally expensive for large model sizes.\nLow-Rank Adaptation (LoRA) (Hu et al., 2021  ###reference_b11###) is a technique for fine-tuning large language models\nthat relies on freezing pre-trained model weights and adding low-rank trainable matrices\nat various points throughout the model. This procedure dramatically reduces the\ncomputational cost of fine-tuning since only the low-rank adaptation matrices\nhave associated gradients at training time.\nQuantized LoRA (QLoRA) (Dettmers et al., 2023  ###reference_b4###) is a variation of LoRA that quantizes the model weights\nas 4-bit NormalFloats, a data type that efficiently compresses the model weights\nwhile discarding as little information as possible. QLoRA also introduces a number of memory optimization techniques, such as double quantization and paged optimizers to manage memory spikes."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Problem setting",
            "text": "The problem we consider assumes that a small amount of survey data is available from\na representative sample of a target human population. We further assume that\ndemographic information that is relevant to characterizing the target population is available.\nFormally, the answer  of a participant  who has demographics  is generated with .\n denotes the questionnaire .\nDemographics, such as age, gender, income, etc., are characteristics of each individual participant. The available demographics as well as their distribution within the target\ndata should be appropriate to the modeling task at hand.\nAs a specific example of the survey, we will use the EV-shift dataset (Arechiga et al., 2022  ###reference_b2###).\nThe EV-shift dataset examines the impact of interventions on the preference for electric vehicles (EVs) when compared to internal combustion vehicles.\nThis dataset resulted from a study aimed at identifying how effectively different text-based interventions changed people’s preferences for EVs.\nTable 1  ###reference_### shows the number of answers and tokens in this dataset.\nIn the study, subjects began by providing an initial preference for EVs, which was a numerical rating from 0 to 100 (with higher numbers indicating greater preference for EVs). Subjects were then shown one of 35 text-based interventions aimed at increasing their preferences for EVs. After the intervention, subjects provided a post-intervention preference, which was also a numerical rating from 0 to 100.\nEach subject also provided demographic information.\nIn total, the dataset contains demographic information, one initial preference rating for each of the 4,045 subjects, the interventions seen by each subject (5 for most subjects), and the post-intervention preference ratings provided by subjects after each intervention."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Proposed method",
            "text": "Our proposed approach provides virtual survey participants with a prompted, fine-tuned LLM\nto generate survey responses that statistically match those of a target population.\nIn this section, we describe the implementation of the subjects by prompting and the fine-tuning procedure.\nFormalization and implementation\nEach virtual participant in the population is implemented by prompting an LLM to behave as a person with given demographic information.\nFormally, the virtual participants with a language model represent the distribution , where  is the generated token sequence. This generated token sequence corresponding to the answer from token sequence  corresponding to the demographics and survey question.\n is the vocabulary of the language model.\nAdditionally, let  be a function that preprocesses the output sequence  to produce an answer . This function is useful when the expected answer to a survey question is a structured output (e.g., a numerical rating, or a multiple choice answer), but the LLM embeds its answer within a longer explanation. The complexity of  may become quite high. In our experiments, we adhere to a simple function that allows some flexibility interpreting the model outputs, without becoming overly complex. We define\n as the function that extracts the first (possibly multi-digit) number that appears in the symbol sequence as a simple implementation.\n###figure_1### Figure 1  ###reference_### shows an example of formulating the prompt text from survey data.\nTo set the properties of the virtual participant, we use the following prompt,\n{itembox}[l]System prompt\nI want you to act as the following character. Answer all of the following questions from the point of view of this character, do not break character. {demographics}\n\n{demographics} contains a list of demographic characteristics.\nFor example, a man aged 18–25 is represented as {age: 18–25, gender: man}.\nOur implementation allows generating virtual participants with demographics drawn from any distribution, enabling targeting to different populations of interest, including demographics that a company believes are likely to be customers for a specific product. Although prompt engineering techniques are a rich area of inquiry, they are not the focus of our work, and for this reason we limit our prompts to the minimum amount of information required to explain the setting to the LLM. Naturally, our techniques can be combined with prompt engineering techniques to enhance their effectiveness.\nOur survey concerns preferences for battery electric vehicles. To check the preferences of a virtual participant given demographics, we use the following prompt,\n{itembox}[l]Survey prompt\n{intervention} On a scale from 0 to 100, what is your current preference for battery electric vehicles (BEVs)? Please reply with just a single number rating and no additional words or explanations. Score:\n\n{intervention} is blank for the initial preference question, and contains intervention text to produce post-intervention preferences.\nFor example, intervention sentence is ”Some BEV manufacturers may start offering free charging” (interventions are described in Appendix A  ###reference_###).\nFine-tuning large language model with survey data\nNext, the pre-trained LLM is fine-tuned to emulate the preferences of the human survey participants.\nThe LLMs used for fine-tuning are auto-regressive LLMs for text generation.\nEach text corresponds to a set of question and answer for a specific subject, and the text contains a system prompt, a survey prompt, and an answer.\nNote that a single dataset contains multiple questions, i.e., both initial preference and post-intervention preference questions.\nFor most of our experiments, we use the conventional cross-entropy loss function.\nNumeric penalty function\nIn 6.4  ###reference_###, we seek to enhance model performance by adding a penalty term to the cross-entropy loss function. This specific penalty term is novel (to the best of our knowledge), and is represented by Equation 1  ###reference_###.\nFor this penalty term to be well-defined, we require that the vocabulary contain separate tokens for each of the possible numerical output tokens. Since we are using the Llama 2 family of models, and these models have separate tokens for the digits 0 through 9, we need to scale the survey data so that the possible answers are in the single-digit range in order to be able to use this penalty term.\nOur numerical penalty term is calculated by weighting the log generation probability of the answer set containing the subject’s answers by a value  for each answer. This value is equal to  when the generated numerical answer  is exactly equal to the true answer , zero when  is further than a hyperparameter value  from , and a computed intermediate value in between, as shown in Equation 2  ###reference_###. This hyperparameter controls how much information the penalty term provides to nearby numerical values.\nThe goal of this penalty term is to improve the performance of a token generation model over questions that require a numerical response. The cross-entropy loss term merely provides feedback about whether a generated numerical token was correct or not. Our penalty term additionally provides feedback about whether the generated numerical token was close to or distant from the correct answer.\nThe combined loss function is .\n denotes the data,  denotes the parameters of the language model, and  denotes a mixing coefficient between the two loss terms.\nPerformance Measures\nFinally, we measure the agreement between LLMs and humans.\nWe use the test data portion of the survey data for this measurement.\nThe LLM responses are generated using the same demographics distribution as the survey data to be measured. The metrics we use to measure similarity between LLM responses and survey responses are KL-divergence and root mean square error (RMSE).\nIntuitively, we can think of the KL-divergence as measuring model agreement with the statistics of the population as a whole, whereas the RMSE measures model agreement with individual responses."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In these experiments, we use Llama 2 (Touvron et al., 2023  ###reference_b18###).\nThe model sizes are 7B, 13B, and 70B, using chat models published on HuggingFace111https://huggingface.co/meta-llama  ###reference_huggingface.co/meta-llama###.\nWe set LoRA , and LoRA dropout  for fine-tuning.\nWe use 3 epochs across all experiments.\nWe split our dataset randomly into a training set, a validation set, and a test set using an 8:1:1 split by subject (so the number of subjects in train data is 3,237). Critically, subjects that occur in the training set do not appear in the test set.\nEvaluation on the test data was performed using the following procedure. Although the prompt asks the model for a numerical answer, sometimes the model replies with additional text. Since we do not want to construct arbitrarily complex logic to understand all possible\ngenerations, we sample a maximum of 8 output tokens,\nand the first number that appears from the beginning of the sentence is considered the answer.\nIf a number is not included in the generated sentence or is not an integer in the correct range, the generation is considered to have failed.\nKL-divergence and RMSE are calculated except in cases where the generation fails. KL-divergence is obtained by generated numbers that are discretized.\nThe discretization width is 10 in Section 6.1  ###reference_### to 6.3  ###reference_### and 1 in Section 6.4  ###reference_### for adapting a scale of the numerical answer.\nTo understand the performance of our language models, we compare the model against\nthree baselines that are not language models.\nThe first two are supervised learning algorithms, support vector regression (SVR) and CatBoost  (Prokhorenkova et al., 2018  ###reference_b14###). These algorithms are trained on the survey data, and they learn to map a vector of demographic information to a predicted preference value. The purpose of these benchmarks is to situate the performance of the language models with respect to highly effective supervised learning models. We note that in many configurations cases, the baseline models outperform the language models. For our use-case, however, the supervised learning algorithms cannot be used in downstream tasks, such as follow-up questions that involve conversational responses or user surveys.\nThe third benchmark model is a model that generates a random answer.\nThese baselines will be conducted to evaluate the positioning of LLM performance at individual-level and population-level by showing the curve of possible solutions that can be achieved by non-language methods, directly mapping from demographic characteristics to survey responses.\nFor SVR and CatBoost, we will refer to the resulting curve of performance values from models trained with multiple hyperparameters as the baseline curves.\nWe chose SVR as one of our baselines because it is a commonly used supervised learning method for regression problems, and CatBoost because it is a powerful gradient-boosting method for categorical variables.\nSVR and CatBoost are fitted with the EV-shift dataset for each questionnaire.\nFor SVR, categorical variables such as demographics and intervention text are converted to dummy variables.\nThe predicted preference is normalized between 0 to 1 for SVR and CatBoost.\nThe hyperparameters for SVR and CatBoost are shown in Appendix G  ###reference_###.\nWe detail our experiments in the sections below Section 6.1  ###reference_### investigate the effects of model size, Section 6.2  ###reference_### investigates the effect of model quantization, Section 6.3  ###reference_### investigates the effects of sampling temperature, and Section 6.4  ###reference_### investigates the effect of our proposed penalty term."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Effects of model size",
            "text": "First, we explore the effects of fine-tuning different model sizes. For this experiment, we will fine-tune three different model sizes (7B, 13B, and 70B) with QLoRA. We trained the model for 3 epochs, but rolled back to the 1 epoch checkpoint due to increases in the validation loss (see Appendix B  ###reference_###).\nThe RMSE-KL plots for each model and baseline are shown in Figure 2  ###reference_###.\nWe consider two sampling temperatures at the output of the model, corresponding to a temperature of zero and a temperature of one. These values are chosen due to their natural interpretations. A temperature of zero corresponds to greedily taking the token with the highest output probability, which can be interpreted as the token that the model most strongly believes is the correct value. We denote this setting as greedy sampling. A temperature of one corresponds to sampling output tokens with the probability distribution that is obtained by directly applying a softmax function to the logits of the neural network. Since in this case output tokens will appear with a distribution that corresponds exactly to the softmax distribution at the model output, we denote this setting as calibrated sampling. We will investigate the role of temperature in greater detail in Section 6.3  ###reference_###.\nComparing the results of the pre-trained model and QLoRA, the use of greedy sampling tends to reduce both KL-divergence and RMSE, while the use of calibrated sampling significantly reduces the KL-divergence.\nComparing results by model size, 70B had the best (lowest) performance on both metrics among the pre-trained models, and QLoRA improved KL-divergence for all model sizes.\nHowever, when calibrated sampling was used, the QLoRA model outperformed the baseline KL-divergence for all sizes.\nIn other words, with and without fine-tuning, 70B has a smaller KL-divergence than the other sizes, but the difference is smaller when fine-tuning is used.\nThese results indicate that fine-tuning not only reduces both RMSE and KL-divergence, but also can exceed the KL-divergence of the non-language model under some sampling conditions. Also, larger models tend to display lower KL-divergence. Although our language models with greedy sampling do not outperform the supervised learning benchmarks, using a language model is more versatile than a supervised learning model, since the language model can be queried with natural-language follow-up questions, or asked to provide natural-language responses in a user interview.\n###figure_2###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Quantization effects",
            "text": "Next, we show the impact of the choice of fine-tuning method, specifically comparing LoRA (Hu et al., 2021  ###reference_b11###) and QLoRA (Dettmers et al., 2023  ###reference_b4###), which differ mainly in that QLoRA introduces parameter quantization.\nIn this experiment, we focus on the 7B parameter model.\nThe comparison of KL-divergence and RMSE for each question is shown in Table 5  ###reference_###.\nLoRA tended to produce lower (better) KL-divergence than QLoRA for initial preference questions, but higher (worse) KL-divergence for post-intervention questions. On the RMSE metric, LoRA performs slightly worse than QLoRA on the initial preference questions but slightly better on the post-preference questions.\nHowever, these differences are fairly small. When the preferences for each question were compared, Spearman’s correlation coefficients were 0.9 and 0.81, indicating very high correlations.\nThese results indicate that the effect of quantization on the responses is small. Since QLoRA provides higher computational efficiency at fine-tuning time, our experiments corroborate the view that QLoRA is able to efficiently provide fine-tuning capabilities."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Temperature effects",
            "text": "In this section, we show the impact of the decoding temperature on our performance metrics.\nAccording to Wiher et al. (2022  ###reference_b20###), there are various decoding strategies. In this paper, (ancestral) sampling is used as calibrated sampling.\nIn these experiments, we fix our attention on a 7B model fine-tuned with QLoRA.\nThe RMSE-KL plots for varying the temperature parameter of the stochastic sampling are shown in Figure 4  ###reference_###.\nThe results show that greedy sampling has the lowest RMSE, and increasing temperature (and randomness) tends to decrease (improve) KL-divergence and increase (worsen) RMSE.\nFrom these results, we can see that the population-wide metric of KL-divergence and the per-individual metric of RMSE trade off against each other, and that the choice of temperature allows fine-grained control over this trade-off.\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "We have investigated the use of LLMs to model the beliefs and preferences of a human population. This can be useful, for example, to conduct simulated focus groups for new products, conduct virtual surveys, or pilot interventions that would be unethical or impractical to conduct on real humans. We found that out-of-the box pre-trained models provide comparatively poor performance at predicting the responses of human survey participants, but that the LLMs can be fine-tuned to provide a better model of the target population. We investigated the effects of model size, and found that larger models provide better performance, but this advantage shrinks after task-specific fine-tuning. We investigated the effects of quantization on the fine-tuning process, and found that the resulting degradation was minimal, confirming that quantization is a viable technique to reduce computation costs. We investigated the role of sampling temperature, and found that the temperature allows trading off the population-wide metric of KL-divergence against the per-individual metric of RMSE. Finally, we introduced a penalty loss term to improve the performance of the model on questions that require a numerical output, providing the model with additional information at training time about the relative correctness of different numerical responses.\nAlthough our approach demonstrates that it is possible to match responses on survey data, in future work we will study the extent to which this shift successfully aligns the model to unseen behavioral scenarios."
        }
    ]
}