{
    "title": "MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness",
    "abstract": "This paper presents the MasonTigers’ entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches to semantic textual relatedness across 14 languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from  to  in Track A, from  to  in Track B, and from  to  in Track C. Adhering to the task-specific constraints, our best performing approaches utilize an ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In this modern era of information retrieval and NLP, understanding semantic relatedness is fundamental for refining and optimizing diverse applications. Semantic relatedness refers to the degree of similarity and cohesion Hasan and Halliday (1976  ###reference_b24###) in meaning between two words, phrases, or sentences. Semantic relatedness allows systems to grasp the contextual and conceptual connections between words or expressions. Various NLP tasks and applications can benefit from modeling semantic relatedness such as question answering Park et al. (2014  ###reference_b38###), knowledge transfer Rohrbach et al. (2010  ###reference_b42###), text summarization Rahman and Borah (2023  ###reference_b40###), machine translation Ali et al. (2009  ###reference_b10###), and content recommendation Piao et al. (2016  ###reference_b39###).\nWhile significant research has been conducted on semantic relatedness in English, more recently the interest in semantic relatedness in other languages has been steadily growing Baldissin et al. (2022  ###reference_b12###). This reflects an increasing awareness of the need for developing models to languages English other than English. NLP is evolving rapidly and we have been witnessing the emergence of language-specific transformer, the release of datasets for downstream tasks in diverse languages, and the development of multilingual models designed to handle linguistic diversity.\nSemEval-2024 Task 1 - Semantic Textual Relatedness Ousidhoum et al. (2024b  ###reference_b37###) aims to determine the semantic textual relatedness (STR) of sentence pairs across 14 diverse languages. Track A focuses on nine languages (Algerian Arabic, Amharic, English, Hausa, Kinyarwanda, Marathi, Moroccan Arabic, Spanish, Telugu) using a supervised approach where systems are trained on labeled training datasets. Track B adopts an unsupervised approach, prohibiting the use of labeled data to indicate similarity between text units exceeding two words.\nThis track encompasses 12 languages (Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Modern Standard Arabic, Moroccan Arabic Punjabi, and Spanish). Track C involves cross-lingual analysis across the 12 aforementioned languages.\nParticipants in this track must utilize labeled training data from another track for at least one language, excluding the target language. Evaluation across all three tracks involves using Spearman Correlation between predicted similarity scores and human-annotated gold scores. We conduct distinct experiments for each track using statistical machine learning approaches along with the embeddings generated by transformer based models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Understanding the level of semantic relatedness between two languages has been regarded as essential for grasping their meaning. Notable studies on the topic including Agirre et al. (2012  ###reference_b7###, 2013  ###reference_b8###, 2014  ###reference_b5###, 2015  ###reference_b4###, 2016  ###reference_b6###); Dolan and Brockett (2005  ###reference_b18###) and Li et al. (2006  ###reference_b30###) have introduced datasets like STS, MRPC, and LiSent. These datasets have been pivotal in advancing research in tasks such as text summarization and plagiarism detection.\nFinding semantic relatedness and semantic similarity, as well as determining sentence pair similarity using existing datasets or paired annotation, are integral in understanding the nuances of language comprehension. Previous studies describe how words and sentences are perceived to convey similar meanings Halliday and Hasan (2014  ###reference_b23###); Morris and Hirst (1991  ###reference_b33###); Asaadi et al. (2019  ###reference_b11###); Abdalla et al. (2021  ###reference_b1###); Goswami et al. (2024  ###reference_b21###).\nMethodologies like paired comparison represent the most straightforward type of comparative annotations Thurstone (1994  ###reference_b44###), David (1963  ###reference_b17###). Best-Worst Scaling (BWS) Louviere and Woodworth (1991  ###reference_b32###) a comparative annotation schema, offer insights into methods for evaluating relatedness through pairwise comparisons. The utilization of these methods aids in generating ordinal rankings of items based on their semantic relatedness. Kiritchenko and Mohammad (2016  ###reference_b27###, 2017  ###reference_b28###) highlight the effectiveness of such techniques, emphasizing the importance of reliable scoring mechanisms derived from comparative annotations for understanding the intricacies of semantic relatedness in natural language processing tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "The shared task comprises three tracks: Supervised, Unsupervised, and Cross-Lingual.\nThe dataset Ousidhoum et al. (2024a  ###reference_b36###) is comprised of two columns: the initial column, labeled \"text,\" containing two full sentences separated by a special character, and the second column, labeled as \"score\", which includes degree of semantic textual relatedness for the corresponding pair of sentences. In the supervised track (Track A), there are 9 languages, and for each language, train, dev, and test sets are provided. The specifics of the dataset for this track can be found in Table 1  ###reference_###.\nIn the unsupervised track (Track B), there are 12 languages and for all the languages dev and test set is provided. The details of the dataset of this track is available in Table 2  ###reference_###.\nFinally, in the cross-lingual track (Track C), there are 12 languages and for all the languages dev and test set is provided and they are same as the unsupervised track. Here the training dataset is not provided. Hence, for each individual language of this track, we select 5 languages from supervised track (different from the target language) and merge training data of those five languages to create the training dataset for each of the languages of cross-lingual track. The details of the dataset of this track is available in Table 3  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use statistical machine learning along with language specific BERT-based models to find the sentence embeddings and predict relatedness between pair of sentences. Additionally, we use sentence transformers for the supervised track. Our experiments are described in detail in the next sections."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Track B - Unsupervised",
            "text": "For unsupervised track, we find the embedding of the development data using Term Frequency - Inverse Document Frequency (TF-IDF) Aizawa (2003  ###reference_b9###) and Positive Point-wise Mutual Information (PPMI) Church and Hanks (1990  ###reference_b15###) separately. Also we find the embeddings using language specific BERT based models. For Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Modern Standard Arabic, Moroccan Arabic, Punjabi and Spanish - AfricanBERTa, DziriBERT Abdaoui et al. (2021  ###reference_b2###), AmRoBERTa Yimam et al. (2021  ###reference_b45###), RoBERTa Liu et al. (2019  ###reference_b31###), HauRoBERTa Adelani et al. (2022  ###reference_b3###), HindiBERT Joshi (2022a  ###reference_b25###), IndoBERT Koto et al. (2020  ###reference_b29###), KinyaBERT Nzeyimana and Niyongabo Rubungo (2022  ###reference_b35###), ArabicBERT Safaya et al. (2020  ###reference_b43###), DarijaBERT Gaanoun et al. (2024  ###reference_b20###), PunjabiBERT Joshi (2022a  ###reference_b25###), and SpanishBERT Cañete et al. (2020  ###reference_b14###) are used.\nThen for each development embedding generated by these three approaches, we calculate cosine similarity Rahutomo et al. (2012  ###reference_b41###) between the pairs. In the development phase, we find the Spearman correlation Myers and Sirois (2004  ###reference_b34###) of these values calculated on embeddings found by three different procedures and perform an average ensemble of the calculated results to get our ensembled Spearman correlation in development phase. We also perform this approach on the test data and find our best Spearman correlation in the evaluation phase."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Track C - Cross-Lingual",
            "text": "For each language in cross-lingual track, we select 5 different languages from Supervised Track to use as training data. The details of the language selection is provided in Table 3  ###reference_###. The we find the embedding of the training data using Term Frequency - Inverse Document Frequency (TF-IDF) Aizawa (2003  ###reference_b9###) and Positive Point-wise Mutual Information (PPMI) Church and Hanks (1990  ###reference_b15###) separately. Also we find the embeddings using language specific (unrelated to the target language) BERT based models. For Afrikaans, Amharic, Hausa and Kinyarwanda - we use ArabicBERT Safaya et al. (2020  ###reference_b43###), for Algerian Arabic, Modern Standard Arabic and Moroccan Arabic - we use AfricanBERTa111https://huggingface.co/mrm8488/AfricanBERTa  ###reference_a###, for English, Hindi, Indonesian, Punjabi and Spanish - SpanishBERT Cañete et al. (2020  ###reference_b14###), BanglaBERT Bhattacharjee et al. (2022  ###reference_b13###), RoBERTa-tagalog Cruz and Cheng (2021  ###reference_b16###), HindiBERT Joshi (2022a  ###reference_b25###) and RoBERTa Liu et al. (2019  ###reference_b31###) are used. Then for each training embedding generated by these three approaches, we calculate cosine similarity Rahutomo et al. (2012  ###reference_b41###) between the pairs. After that we apply ElasticNet Zou and Hastie (2005  ###reference_b46###) and Linear Regression Groß (2003  ###reference_b22###) separately on these embeddings and predict the similarity of the sentence pairs in the development phase. We clip the predicted values to ensure the prediction range from 0 to 1. In the development phase, we find the Spearman correlation of these six predictions (three each by ElasticNet and Linear Regression) and perform an average ensemble of the predictions to get our ensembled Spearman correlation in development phase. We also perform this approach on the test data and find our best Spearman correlation in the evaluation phase."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "For all the tracks, ensemble of the predictions prove helpful in terms of achieving better Spearman correlation.\nFor Track A sentence transformer LaBSE along with Linear Regression performs the best among the eight combinations for all the languages. Then the weighted ensemble improves the result 1% - to 3% in development phase and 1% - 2% in evaluation phase - depending on the languages. For English this method performs the best in terms of ranking with  rank while the worst for Moroccan Arabic with  rank. On test Spearman correlation, English is the best securing 0.84 and Kinyarwanda is the worst with 0.37. Detailed results are shown in Table 4  ###reference_### of Appendix.\nFor Track B, embedding generated by language specific BERT based models provide the best result among the three combinations for all the languages. Then the average ensemble improves the result 0% - to 3% in development phase and 0% - 2% in evaluation phase - depending on the languages. For Kinyarwanda this method performs the best in terms of ranking with  rank while the worst for English with  rank. On test Spearman correlation, English is the best securing 0.77 and Punjabi is the worst with 0.02. Detailed result is shown in Table 5  ###reference_### of Appendix.\nFor Track C embedding generated by language specific (unrelated to target language) BERT based models provide the best result among the six combinations for all the languages. Then the average ensemble improves the result 0% - to 2% in both development and evaluation phases depending on the languages. For Punjabi this method performs the best in terms of ranking with  rank while the worst for Hausa and Kinyarwanda with  rank. On test Spearman correlation, Spanish is the best securing 0.56 and Punjabi is the worst with 0.02. Detailed result is shown in Table 6  ###reference_### of Appendix."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Error Analysis",
            "text": "###figure_1### ###figure_2### ###figure_3### For Track A, Algerian Arabic, Moroccan Arabic and Spanish test Spearman Correlation Coefficient decreases in the evaluation phase. This happens because the dev set was around 7.5%-9% and the test set is around 39% - 46% size of the train set.\nFor Track B, amount of dev data was only 20 for Afrikaans which is the reason of a very big difference between the result of development and evaluation phase. Algerian Arabic, Amharic, Modern Standard Arabic, Moroccan Arabic have a very small amount of dev data (less than 100) which is reason of decreased Spearman Correlation Coefficient in the evaluation phase. Hindi also faces the same issue but as it had more dev data the test Spearman Correlation Coefficient is only 4% less than the development period.\nFor Track C, Algerian Arabic, Indonesian, Kinyarwanda, Modern Standard Arabic faced bigger drop of the Spearman Correlation Coefficient from the development phases. The main issue here is the BERT based models that doesn’t know the target languages generate the embeddings that are not as good as what we observed in unsupervised track for the models with the knowledge of target language. Also the diversity of the train and test data make it more challenging to score better Spearman Correlation Coefficient. In addition, due to the unavailability of the text label, only the ensemble performance of Spanish language for all the tracks are shown.\nRegarding the result of the Punjabi language in the both unsupervised and cross-lingual track, it was the most challenging language where the provide baseline was less than zero. Though our system achieves 0.02 Spearman Correlation Coefficient for for this language, the ranking is quite impressive which also proves the struggle of other teams to cope up with this language.\nMoreover, ElasticNet and Linear Regression exhibit limitations as assumption of linearity may not align with the intricate and nonlinear relationships inherent in the textual data. The issue of dimensionality poses a challenge, especially when dealing with a large number of features. The difference between the gold and predicted semantic relatedness scores for the three tracks are shown in Figure 1  ###reference_###, Figure 2  ###reference_###, and Figure 3  ###reference_###."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We experimented with various methodologies on the dataset provided by the organizers, including statistical machine learning approaches, transformer-based models, language-specific BERTs, and sentence BERT. In the supervised task (Track A), with no restrictions on the model or data, we utilized the available training dataset. Conversely, the unsupervised task (Track B), lacking training data, presented challenges, leading us to use language-specific BERTs and statistical machine learning approaches. The cross-lingual track (Track C) imposed more stringent restrictions, requiring us to use training data from other languages in Track A, excluding the target language. In addition to statistical ML models, we integrated language-specific BERTs closely aligned with the geography and culture of the target language, as the use of LLMs was constrained due to unknown training data.\nWe show that our ensemble approach exhibited superior performance compared to individual model experiments. However, the task’s inherent difficulty became evident in instances where relatively small datasets presented challenges for effective model learning. Semantic textual relatedness tasks face challenges like subjectivity, context dependency, and ambiguity due to multiple meanings and cultural differences. Limited data, domain specificity, short texts, and biases hinder accuracy. Ongoing research is crucial to address these limitations and improve model accuracy."
        }
    ]
}