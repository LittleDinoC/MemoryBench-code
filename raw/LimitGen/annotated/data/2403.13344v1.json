{
    "title": "USE: Dynamic User Modeling with Stateful Sequence Models",
    "abstract": "User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling.\nAs users continuously interact with the apps, user embeddings should be periodically updated to account for users’ recent and long-term behavior patterns.\nExisting methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead.\nTo address this limitation, we introduce the User Stateful Embedding (USE). USE generates user embeddings and reflects users’ evolving behaviors without the need for exhaustive reprocessing by storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future -behavior prediction to transcend the limitations of next-token prediction by forecasting a broader horizon of upcoming user behaviors. By combining it with the Same User Prediction, a contrastive learning-based objective that predicts whether different segments of behavior sequences belong to the same user, we further improve the embeddings’ distinctiveness and representativeness.\nWe conducted experiments on 8 downstream tasks using Snapchat users’ behavioral logs in both static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically updated user behavior sequences) settings. We demonstrate USE’s superior performance over established baselines. The results underscore USE’s effectiveness and efficiency in integrating historical and recent user behavior sequences into user embeddings in dynamic user modeling.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The era of digital transformation has ushered in an unprecedented emphasis on personalization, primarily driven by the ability to understand and predict user behavior. In this context, user embeddings – numerical vector representations of user characteristics, behavioral patterns, and preferences – have become indispensable. These embeddings are central to a myriad of applications, from recommendation systems to targeted advertising (Chen et al., 2018a  ###reference_b5###; Wu et al., 2020  ###reference_b31###; Modell et al., 2021  ###reference_b19###), and their effectiveness directly influences user experience and engagement. In the present work, we study general-purpose user embeddings that can be directly used for various downstream tasks without fine-tuning the upstream user embedding model, as opposed to task-specific user embeddings (Fan et al., 2019  ###reference_b9###; Waller and Anderson, 2019  ###reference_b30###; Zheng et al., 2017  ###reference_b34###; Liu et al., 2010  ###reference_b16###).\nUser embeddings can be calculated from various data sources such as demographic data (e.g., age, gender, etc.) and user-created content (e.g., photos and messages). In this work, we focus on behavior-based user embedding models (see Figure 1  ###reference_### as an example), which take solely behavior sequences (e.g., open_app -> open_camera -> apply_filter -> close_app) as input to compute user embeddings. Such sequences reflect the actions users take in an app. A good user embedding should reflect both their recent behavior as well as more long-term and potentially recurring behavior. Thus, as users continuously interact with an app, their embeddings should be periodically updated in response to their newly extended behavior sequences.\nThis requires that user embedding models be efficiently updated with new behavioral data without sacrificing information about past user behaviors.\n###figure_1### Existing techniques for behavior-based user embeddings (Zhang et al., 2020  ###reference_b33###; Pancha et al., 2022  ###reference_b20###; Chu et al., 2022  ###reference_b8###) predominantly rely on stateless models (e.g., Transformers (Vaswani et al., 2017  ###reference_b29###)), which generate outputs purely based on current inputs without the memorization of historical inputs going back further than the context window of the current input. While these methods are powerful in capturing complex patterns in user behavior, they exhibit significant limitations in dynamic environments due to their inability to efficiently incorporate both historical and new data when updating user embeddings. When confronted with new behavior sequences, these models face a trade-off: either disregard historical data for efficiency, leading to a loss of valuable long-term behavioral insights or compute embeddings from scratch by processing all available data, incurring substantial computational costs and delays. In the first case, a possible strategy to include historical information is pooling the old embeddings with new embeddings based on the incoming data. However, our empirical analysis reveals that computing new embeddings without conditioning on historical user data can still result in substantial information loss, as detailed in Table 2  ###reference_### and 3  ###reference_###. This problem is even more challenging in traffic-intensive apps where often hundreds of events are generated by each user every day.\nTo address this challenge, we introduce USE, which stands for User Stateful Embeddings, an approach that can efficiently produce user embeddings equivalent to explicitly incorporating all available data (including historical information) as model input, while maintaining constant computational costs regardless of the amount of user historical data.\nSpecifically, USE retains a state of past behaviors for each user; as new behavior data comes in, it efficiently computes new user embeddings by updating the previous user state, without the need to explicitly use all available data as model input.\nTo implement USE, we consider several aspects. The first is model architecture. We achieve statefulness by adopting the Retentive Network (RetNet) (Sun et al., 2023  ###reference_b27###). RetNet was originally designed for natural language processing. It can be trained in parallel like Transformers while making inferences sequentially like Recurrent Neural Networks (RNN). The Transformer-like architecture ensures training scalability and user representation capability, while the sequential inference allows efficient updates of user embeddings, making RetNet an ideal choice for dynamic user modeling.\nSecond, we consider the training target. Next-token prediction has become the default pre-training target for almost all existing large language models (LLM) (Radford et al., 2019  ###reference_b22###; Sun et al., 2023  ###reference_b27###). This training objective closely matches the natural language generation process, yet it is less suitable for user embedding learning. Given the stochastic nature of user behavior and the absence of consistent syntax and grammar in user behavior sequences, accurately predicting the exact next user behavior is not only less feasible but also likely detrimental to the model’s ability to capture longer term user interests.\nThus, we relax the order component in traditional next-token prediction and introduce a unique training objective named Future -Behaviors Prediction (FBP).\nInstead of predicting the exact next behavior, FBP trains the model to predict the presence of all the behaviors in the future  user behaviors, where a larger  emphasizes longer-term user engagements and a smaller  emphasizes shorter-term user engagements.\nFBP thus prevents the model from overfitting to the idiosyncrasies in user behavior sequences, while allowing the embeddings to capture longer-term user intentions.\nThird, we consider the encoding of both user states and user traits. Ideal behavioral user embeddings should encode both the state component of a user (what the user might do based on the current state, as captured with future -behaviors prediction) and the trait (habitual) component of user behavior (what the user tends to do most of the time). To this end, we incorporate the Same User Prediction (SUP), a contrastive learning-based training objective that predicts whether different segments of behavior sequences belong to the same user. This way, the model 1) is forced to learn to represent stable user features (traits) in addition to representing momentary behaviors (states), and 2) improves the distinctiveness and representational capability of the embeddings for individual users.\nTo validate our approach, we conduct extensive empirical analyses using behavioral sequences from users of Snapchat, a popular multimedia instant messaging platform, with about 400 million daily active users as of the second quarter of 2023 (Statista, 2023  ###reference_b25###). Overall, our analyses demonstrate the superiority of USE over strong baselines in both static and dynamic settings. Specifically, by evaluating USE on  downstream tasks, we illustrate the effectiveness of the combined future -behavior prediction and same user prediction training objectives over typical NLP training objectives in the context of user modeling.\nFurthermore, by comparing user embeddings generated using identical models but with different updating strategies, we demonstrate both the effectiveness and efficiency of USE in seamlessly integrating historical and recent user behavior data into user embeddings.\nCompared to other approaches, USE not only substantially reduces computational costs but also consistently delivers higher-quality user embeddings across nearly all evaluation settings.\nOur contributions are threefold: 1) we introduce stateful\nuser embeddings, 2) we develop novel training objectives to enhance these embeddings, and 3) we empirically demonstrate the superiority of USE over existing methods. This work not only advances user modeling research but also sets a new standard for efficient and effective personalization in dynamic online environments.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Behavior-based User Modeling",
            "text": "User embeddings (fixed-size numerical vectors representing user characteristics and preferences) play a crucial role in personalization systems. These systems encompass a range of applications, including user understanding, detection of malicious users, friend suggestions, and item recommendations. Various methodologies have been developed to derive user embeddings from different types of user data (Modell et al., 2021  ###reference_b19###; Fan et al., 2019  ###reference_b9###; Waller and Anderson, 2019  ###reference_b30###; Zheng et al., 2017  ###reference_b34###; Liu et al., 2010  ###reference_b16###).\nThis work focuses on behavior-based user modeling. Our objective is to compute user embeddings based on their behavior sequences, while deliberately avoiding the use of demographic data (e.g., age, race, nationality, and gender) or user-generated content (e.g., posts and messages), which are commonly used in the field of user modeling.\nAdditionally, deriving user embeddings exclusively from natural interactions with the app reduces the need for active user input, like filling out surveys, thereby minimizing user burden and enhancing user experience.\nLet us define  as the set of  unique user behaviors, where each behavior  represents a distinct type of user interaction with the app (for example, opening the app or sending a message). Our model solely relies on the type of these user interactions. Let  denote the behavior sequence of the -th user , with each . The behavior-based user models take  as input to generate a fixed-size vector , representing the embedding of user ."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Stateless and Stateful User Modeling",
            "text": "A key challenge in behavior-based user modeling lies in the dynamic nature of user behavior sequences. As users interact with the app over time, their behavior sequences expand with new entries. To reflect the latest user behaviors accurately, it is essential to periodically update user embeddings.\nFor notation simplicity, let  be the initial behavior sequence for user , where  denotes the index of the last behavior up to the first computation period (period ). The user’s embedding  is derived from . After some period (e.g., a week), a new behavior sequence  is generated, necessitating the recomputation of the user embedding, now based on both  and . This process repeats with each new period, accumulating more behavior data for embedding calculations.\nIn this dynamic setting, a stateless model (such as Transformer (Vaswani et al., 2017  ###reference_b29###) and Convolutional Neural Networks (He et al., 2016a  ###reference_b10###)), which computes outputs based solely on current input sequences, can adopt one of three strategies for periodic embedding updating: Recent Only, Pool Embeddings, and Recompute All (visualized in Figure 2  ###reference_###).\nRecent Only discards historical data, relying only on the latest user behavior sequences for embedding generation. Pool Embeddings enhances the first strategy by combining new embeddings with previously calculated ones. Recompute All always uses the entire user behavior sequence as input to compute embeddings at each period. While Recent Only and Pool Embeddings are computationally efficient, they disregard historical behavior sequences when computing outputs for sequences in the new period and may thus sacrifice effectiveness. Recompute All, while utilizing complete user history, incurs high computational costs.\nThe concept of a stateful user model is meant to address these limitations of stateless models. By maintaining and utilizing historical intermediate results in future computations, a stateful model achieves high computational efficiency without losing historical information. At period , the stateful model calculates the user embedding  and produces a state —the model’s memory of the user’s relevant history. In the subsequent period , the model processes the new behavior sequence  alongside the last state , thus generating embeddings that encompass the entire user behavior history efficiently and effectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "While formulating FBP as a regression task (predicting the frequency of each behavior) is possible, it may skew the model’s focus towards more frequent events, overshadowing less common behaviors. Given the skewed behavior distributions in our data, we opt for a binary classification approach.\nWhich behaviors to predict depends on the application. For instance, a model aimed at ad click prediction might focus on ad-related behaviors. However, for broader downstream applications, we avoid manually selecting specific behaviors and instead include all possible behaviors for greater generalizability."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Model Architecture",
            "text": "Theoretically, USE can be implemented with any model architecture that enables recurrent inference.\nWe choose the implementation of RetNet (Sun et al., 2023  ###reference_b27###) for its demonstrated effectiveness and efficiency over earlier methods in natural language processing research. The key difference introduced by RetNet is replacing the Attention operation with Retention, a operation than can be equivalently formatted in parallel and recurrent ways. Retention’s parallel form empowers RetNet with Transformer-level scalability and representation power, while its recurrent form enables the modeling of states and efficient inference.\nIn this section, we focus on the core idea behind our implementation of stateful embeddings with RetNet. Please refer to the original paper Sun et al. (2023  ###reference_b27###) for more technical details.\nLet  define an input sequence of length  and , , and  respectively define the query, key, and value matrices of  at a Transformer/RetNet layer. Let  and  define the output of  after respectively the attention and retention calculation, where the attention operation is defined as follows:\nBecause of the non-linear softmax function, to calculate the -th output , we need to perform the dot product between  and all the previous  before applying softmax. This leads to a computational complexity of . The retention operation, however, removes the softmax function, so that computation between , , and  can be reordered by performing  first, enabling the following definition of the retention operation:\nwhere  is a hyperparameter between  and  that explicitly reduces the importance of distant tokens in current output. Let  and , we have . Equation 2  ###reference_### can be written as . In other words, current output  only depends on current query , , , and the latest state , leading to computational complexity of . More importantly, computing embeddings in this way results in identical outputs as feeding the entire behavior sequence as input.\nThis operation can be further extended to chunk-wise recurrent inference. When the latest state  is pre-computed on input , the computational cost on the input sequence  only depends on , regardless of the historical behavior length . Note that, in the chunk-wise recurrent inference, the output of each  is performed in parallel. Therefore, compared to purely recurrent models such as RNNs, it is much more efficient without sequential dependency within .\nIn dynamic user modeling, we start with a user behavior sequence  for user . We initialize the first state  as an all-zero matrix and perform a chunk-wise recurrent forward pass to get the last hidden states of each input behavior  as well as new state . The user embedding is calculated as . After a certain period, when new behavior sequence  is available, we perform another chunk-wise recurrent forward pass with  and  as input and obtain  and . The user embedding is then calculated as , where  is the average of new hidden states."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training Objectives",
            "text": "We aim to train a user model that can predict users’ future engagements with the app and discriminate against different users, such that the user model can handle a wider range of downstream tasks. Specifically, we reason that user behavior forecasting may allow for more accurate item/ad recommendations and early detection of bad actors (e.g., users that violate rules of operations), while user discrimination may empower better personalization and user re-identification. With such design principles, we introduce two model training objectives: Future -Behavior Prediction (FBP) and Same User Prediction (SUP).\nWhile formulating FBP as a regression task (predicting the frequency of each behavior) is possible, it may skew the model’s focus towards more frequent events, overshadowing less common behaviors. Given the skewed behavior distributions in our data, we opt for a binary classification approach.\nWhich behaviors to predict depends on the application. For instance, a model aimed at ad click prediction might focus on ad-related behaviors. However, for broader downstream applications, we avoid manually selecting specific behaviors and instead include all possible behaviors for greater generalizability."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Future -Behavior Prediction",
            "text": "In user modeling, encoding users’ long-term engagements is crucial. Typically, causal language modeling (CLM) is employed as a training objective, which focuses on predicting the immediate next user behavior based on a given behavior sequence. While CLM is a prevalent pre-training objective in natural language processing, it might not be optimal for behavior-based user models. Unlike natural languages with strict syntax and grammar, user behaviors are much more random and noisy. Forcing a model to predict the exact order of next user behaviors may lead to overfitting to the idiosyncrasies in the data and thus compromise effective user representation learning."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Same User Prediction",
            "text": "The capability of discriminating different users is crucial for personalization, yet the FBP objective does not explicitly train the model for this. Thus, we introduce the same user prediction (SUP) objective. SUP encourages the model to assign similar embeddings to behavior sequences from the same user and dissimilar embeddings to behavior sequences from different users. We train the model with contrastive learning (Chen et al., 2020  ###reference_b7###), which aims to increase the similarity between similar pairs of data while decreasing the similarity between dissimilar pairs. We randomly extract one pair of non-overlapping behavior sequences from each user to constitute positive samples and use in-batch negative sampling to obtain negative samples.\nLet  define a batch of pairs of non-overlapping behavior sequences from the same user, where  is the batch size,  and  are the sequence lengths. Let  denote the embedding of the behavior sequence .\nWe adopt the SimCLR (Chen et al., 2020  ###reference_b7###) loss to implement SUP. Specifically, the loss function regarding anchor  is defined as follows:\nwhere  and  represent the indices of the anchor and the behavior sequence from the same user,  is the temperature hyperparameter and  is the cosine similarity of  and .\nFor every positive pair , we respectively take  and  as the contrastive anchor to calculate the contrastive loss. We also perform future -behavior prediction on both  and .\nWeighting both losses equally, the final loss function per batch is thereby:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Implementation",
            "text": "The USE model consists of  Retention layers,  retentive heads, a hidden size of , and an intermediate size of . We set the future window size  as  for future -behavior prediction (In Section B.1  ###reference_###, we show the impact of choosing different ). To construct pairs of non-overlapping behavior sequences as inputs, we filtered out users with shorter than -length (*2+*2) behavior sequences. Each input behavior sequence has a sequence length of , and we ensure a distance of at least  behaviors between the pair of input behavior sequences to avoid information leakage. We train the model on  pairs of behavior sequences for  epochs, with a global batch size of  and a learning rate of . The learning rate linearly increases from  to peak in the first  percent steps and linearly decreases to 0 at the end. Training the user model takes about  hours on  NVIDIA V100 GPUs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we detail our experimental design and show empirical results. We focus on three research questions.\nRQ1: How do different training objectives impact the downstream performance of user embeddings in static settings?\nRQ2: Does the stateful approach generate better embeddings than stateless approaches in dynamic settings?\nRQ3: How efficient is the stateful approach compared to stateless approaches in dynamic settings?\nIn static settings, user behavior sequences remain unchanged, whereas in dynamic settings, users continuously generate new behaviors and we periodically update user embeddings in response to the new user data, approximating real-world scenarios.\nWe describe our data and baseline models in Sections 4.1  ###reference_### and 4.2  ###reference_###, respectively. In Section 1  ###reference_###, we present and discuss model performance on various tasks in static settings (RQ1). Finally, in Section 4.4  ###reference_### we delve into model performances in dynamic settings (RQ2&3).\nThis is a ranking task that evaluates a model’s ability to distinguish between different users. The goal is to retrieve the correct user behavior sequence from a pool of 100 candidates, given a query behavior sequence. Each sample comprises 101 user behavior sequences: 1 query and 100 candidates. Among the candidates, there is 1 positive candidate corresponding to the same user as the query and 99 negative candidates belonging to other users. We represent each behavior sequence as a vector and rank the candidates based on their cosine similarity with the query. The model’s performance is measured using Mean Reciprocal Rank (MRR): , where  is the number of samples and  is the rank of the positive candidate in the -th sample. Each behavior sequence contains 512 user behaviors. To increase task difficulty, negative samples are chosen from behavior sequences with a TF vector cosine similarity over  with the query (hard negatives).\nThis multi-label classification task assesses a model’s ability to predict future user behaviors. Specifically, it involves predicting whether a user will exhibit each behavior in their next 512 actions, akin to the future -behavior prediction objective with a future window of 512. The dataset training, validation and testing ratio is 3:1:1. AUC is the evaluation metric.\nThese four tasks involve predicting users who get reported by others, whose accounts get locked, who view an ad for a certain duration, and who delete their accounts voluntarily, respectively, on a given date (which we refer to as label date). For each task, we vary the number of days (from 0 to 7) between the date of the last available user behavior and the label date. Here,  corresponds to the users’ behavior sequences that end on the label date but still before the timestamp of the event to predict. Hence, for each task, we have eight evaluation datasets. Within each task, we evaluate the same set of users and ensure each dataset is balanced on class labels. Due to space limits, we only report the models’ average performance across the 8 datasets in this section. The models’ detailed performance on each task is presented in Appendix B.2  ###reference_###. We use AUC as the evaluation metric.\nExcept for user retrieval, all tasks involve training a single-layer MLP (multi-layer perception) classifier with user embeddings as input. Cross-validation with random search is employed for hyperparameter selection for the MLP classifier on each dataset. The input user behavior sequence length for all tasks is fixed at 512.\n###table_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data",
            "text": "We use user behavior sequences from Snapchat.\nTo construct these sequences, we selected a total of  distinct user behaviors, such as sending a chat, utilizing the camera, and applying a filter. We added a \"new_session\" behavior marker in each user’s behavior sequence to signify the commencement of a session. An example sequence is: \"new_session, open_app, open_camera, read_message, close_app\".\nThe dataset for training user models comprises of behavior sequences from a large, randomly selected sample222Note that we avoid reporting the actual sample sizes for all the analyses on purpose. However, we make sure that the sample sizes are up to industry and academic standards. of U.S.-based adult users active between April 1, 2023, and April 14, 2023. \"Active\" is defined as engaging with Snapchat for a cumulative duration exceeding one minute in this time frame. This sample was randomly divided into training and validation sets at a 19:1 ratio. In our downstream evaluations, to prevent information leakage, we only included behavior sequences from users who were not present in the user model training phase. All experiments were conducted across  random seeds, and the results presented are the averages of these trials."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare USE with a range of baseline models.\nTerm Frequency (TF) and Term Frequency - Inverse Document Frequency (TF-IDF), which are traditional methods for vector representation.\nSkip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013  ###reference_b18###), a model that learns a fixed vector for each user behavior by predicting context behaviors from a given target behavior.\nUntrained user representations, where each unique user behavior is represented by a randomly generated fixed vector. This approach has demonstrated competitive performance in various natural language tasks (Arora et al., 2020  ###reference_b3###).\nTransformer Encoder trained with masked language modeling (Trans-MLM) and Transformer Decoder trained with causal language modeling (Trans-CLM). These are our implementations of BERT (Kenton and Toutanova, 2019  ###reference_b14###) and GPT2 (Radford et al., 2019  ###reference_b22###) for behavior-based user modeling, utilizing architectures equivalent to BERT-base and GPT2-117M.\nVariants of USE, each trained with the same data and architecture but different training objectives: causal language modeling (USE-CLM), future -behavior prediction (USE-FBP), and same user prediction (USE-SUP)\nTF and TF-IDF baselines generate fixed-length vectors representing entire behavior sequences, which can directly serve as user embeddings. Other models, such as SGNS, Untrained, BERT, and GPT2, learn vector representations for separate behaviors. Consequently, it is necessary to aggregate these behavior-level vectors into a sequence-level vector (i.e., a user embedding). For all models, we employ mean pooling to aggregate behavior vectors as user embeddings. Additionally, for autoregressive models like GPT2 and USE, we explored the effectiveness of using the last non-padded behavior’s embedding as the user embedding. However, this method showed significantly inferior performance compared to mean pooling."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Static User Modeling",
            "text": "###table_2### In this section, we aim to answer RQ1. We evaluate model performance on the  downstream tasks that utilize static user behavior sequences. In each task, user behavior sequences are fixed (i.e., not updated). For USE and each baseline, we compute user embeddings and use those as input for downstream evaluation.\nThis is a ranking task that evaluates a model’s ability to distinguish between different users. The goal is to retrieve the correct user behavior sequence from a pool of 100 candidates, given a query behavior sequence. Each sample comprises 101 user behavior sequences: 1 query and 100 candidates. Among the candidates, there is 1 positive candidate corresponding to the same user as the query and 99 negative candidates belonging to other users. We represent each behavior sequence as a vector and rank the candidates based on their cosine similarity with the query. The model’s performance is measured using Mean Reciprocal Rank (MRR): , where  is the number of samples and  is the rank of the positive candidate in the -th sample. Each behavior sequence contains 512 user behaviors. To increase task difficulty, negative samples are chosen from behavior sequences with a TF vector cosine similarity over  with the query (hard negatives).\nThis multi-label classification task assesses a model’s ability to predict future user behaviors. Specifically, it involves predicting whether a user will exhibit each behavior in their next 512 actions, akin to the future -behavior prediction objective with a future window of 512. The dataset training, validation and testing ratio is 3:1:1. AUC is the evaluation metric.\nThese four tasks involve predicting users who get reported by others, whose accounts get locked, who view an ad for a certain duration, and who delete their accounts voluntarily, respectively, on a given date (which we refer to as label date). For each task, we vary the number of days (from 0 to 7) between the date of the last available user behavior and the label date. Here,  corresponds to the users’ behavior sequences that end on the label date but still before the timestamp of the event to predict. Hence, for each task, we have eight evaluation datasets. Within each task, we evaluate the same set of users and ensure each dataset is balanced on class labels. Due to space limits, we only report the models’ average performance across the 8 datasets in this section. The models’ detailed performance on each task is presented in Appendix B.2  ###reference_###  ###reference_###. We use AUC as the evaluation metric.\nExcept for user retrieval, all tasks involve training a single-layer MLP (multi-layer perception) classifier with user embeddings as input. Cross-validation with random search is employed for hyperparameter selection for the MLP classifier on each dataset. The input user behavior sequence length for all tasks is fixed at 512.\n###table_3###"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Evaluation Tasks",
            "text": "This is a ranking task that evaluates a model’s ability to distinguish between different users. The goal is to retrieve the correct user behavior sequence from a pool of 100 candidates, given a query behavior sequence. Each sample comprises 101 user behavior sequences: 1 query and 100 candidates. Among the candidates, there is 1 positive candidate corresponding to the same user as the query and 99 negative candidates belonging to other users. We represent each behavior sequence as a vector and rank the candidates based on their cosine similarity with the query. The model’s performance is measured using Mean Reciprocal Rank (MRR): , where  is the number of samples and  is the rank of the positive candidate in the -th sample. Each behavior sequence contains 512 user behaviors. To increase task difficulty, negative samples are chosen from behavior sequences with a TF vector cosine similarity over  with the query (hard negatives).\nThis multi-label classification task assesses a model’s ability to predict future user behaviors. Specifically, it involves predicting whether a user will exhibit each behavior in their next 512 actions, akin to the future -behavior prediction objective with a future window of 512. The dataset training, validation and testing ratio is 3:1:1. AUC is the evaluation metric.\nThese four tasks involve predicting users who get reported by others, whose accounts get locked, who view an ad for a certain duration, and who delete their accounts voluntarily, respectively, on a given date (which we refer to as label date). For each task, we vary the number of days (from 0 to 7) between the date of the last available user behavior and the label date. Here,  corresponds to the users’ behavior sequences that end on the label date but still before the timestamp of the event to predict. Hence, for each task, we have eight evaluation datasets. Within each task, we evaluate the same set of users and ensure each dataset is balanced on class labels. Due to space limits, we only report the models’ average performance across the 8 datasets in this section. The models’ detailed performance on each task is presented in Appendix B.2  ###reference_###  ###reference_###  ###reference_###. We use AUC as the evaluation metric.\nExcept for user retrieval, all tasks involve training a single-layer MLP (multi-layer perception) classifier with user embeddings as input. Cross-validation with random search is employed for hyperparameter selection for the MLP classifier on each dataset. The input user behavior sequence length for all tasks is fixed at 512.\n###table_4###"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Results",
            "text": "Table 1  ###reference_### shows the performance of different models across  downstream tasks in static settings. These results suggest a significant advantage of sequence model-based methods over other baselines, underscoring the substantial potential and capability of sequence models in user modeling. Notably, Trans-CLM exhibits overall better performance compared to Trans-MLM, particularly in the User Retrieval (UR) task. This highlights the efficacy of causal language modeling in learning user embeddings, especially for tasks like UR. Despite employing the same training objective as Trans-CLM, USE-CLM demonstrates slightly inferior performance, suggesting a somewhat weaker representation capacity of RetNet compared to the Transformer architecture at this scale (i.e., 100M parameters). This observation aligns with findings in the original RetNet paper (Sun et al., 2023  ###reference_b27###).\nHowever, this slight decrease in representation capability is effectively offset by the adoption of more tailored training objectives. As the table indicates, both USE-FBP and USE-SUP outperform USE-CLM and Trans-CLM, attesting to the effectiveness of our proposed pre-training objectives. This effectiveness is further exemplified by the overall superior performance of our proposed method, USE. USE surpasses its individual objective-based variants, demonstrating the synergistic benefit of combining Future -Behavior Prediction (FBP) and Same User Prediction (SUP) for the user model’s downstream effectiveness."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Dynamic User Modeling",
            "text": "In this section, we answer RQ2 and RQ3 by conducting simulations that approximate real-world scenarios where users continuously produce new behavior sequences, necessitating periodic updates to user embeddings to account for recent user behavior changes. We evaluate the effectiveness and efficiency of stateful user models in comparison to stateless models in such dynamic environments.\n###table_5###"
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Evaluation Tasks",
            "text": "Our simulation uses behavior sequences of\na random sample of Snapchat users that are not present in the training dataset, with a span of  periods. Initially, each user possesses a behavior sequence comprising  behaviors. Subsequently, in each period, each user generates an additional  behaviors (chosen based on the median number of behaviors per day for active users: ). At the end of each period, we update the user embeddings for downstream evaluation. We adapt the User Retrieval and Future Behavior Prediction tasks used in the static settings for the now dynamic settings. They are: User Re-Identification and Next-Period Behavior Prediction. The other four evaluation tasks in the static settings, however, cannot be implemented due to resource limitations (e.g., expensive queries).\nUser Re-Identification: This task focuses on distinguishing users based on their behavioral patterns. We start by collecting a historical behavior sequence of  actions from each of the\nselected users and creating a corresponding historical embedding for each. During the simulation, user embeddings are updated at the end of each period. For each user, we rank all historical embeddings based on their cosine similarity with the current user embedding. Consistent with the User Retrieval task, Mean Reciprocal Rank (MRR) is employed as the evaluation metric.\nNext-Period Behavior Prediction: This multi-label classification task involves predicting a user’s probability of displaying specific behaviors in the subsequent period. We train an MLP classifier using the behavior sequences of an independent set of\nusers. Throughout the simulation, user embeddings are updated at the end of each period. The updated embeddings are then used to make predictions via the trained MLP classifier. AUC serves as the evaluation metric."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Effectiveness",
            "text": "Table 2  ###reference_### and 3  ###reference_### present model performance in our simulations, utilizing different strategies for computing user embeddings. The data in these tables consistently show that USE significantly outperforms the Recent Only and Pool Embeddings strategies across a range of settings. This underscores the importance of incorporating user history in the generation of user embeddings and highlights USE’s effectiveness in leveraging historical information. The Recent Only strategy, which entirely omits historical data, yields the worst performance in nearly all scenarios. Predictably, it maintains a consistent performance level across different periods, given its reliance on a uniform amount of information for generating user embeddings.\nIn contrast, both Pool Embeddings and USE demonstrate better performance in later periods, benefiting from the accumulation of historical user data. Pool Embeddings shows a notable improvement over the Recent Only approach, indicating that even a simple average of user embeddings from different periods can significantly aid user modeling. However, it falls short of USE in almost every instance, and this performance gap widens with the progression of periods. This trend highlights the superiority of generating embeddings based on historical user states compared to independent embedding computations at each period.\nMoreover, echoing findings from our static user modeling evaluations (see Section 1  ###reference_###), USE surpasses baseline models in most settings. This further validates the effectiveness of our proposed training objectives and of stateful user modeling in dynamic settings.\n###figure_3###"
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Efficiency",
            "text": "To fairly compare different methods’ efficiency in generating embeddings in the dynamic setting, we compute the required cumulative time of each method for updating user embeddings at the end of each period in our simulation. Due to the varying memory usage with different methods, we dynamically adjust the batch size to saturate the GPU memory.\nFigure 3  ###reference_### illustrates the required cumulative time of USE and  stateless methods for updating user embeddings at the end of each period in our simulation. As the figure shows, USE demonstrates a consistent, constant time requirement for updates in each period, similar to the stateless methods that disregard historical data. In contrast, the Recompute All method incurs increasingly more time as the length of the user behavior history extends. The difference in efficiency between the stateful approach and the ’Recompute All’ method is relatively modest at the beginning but becomes markedly significant over time. In real-world applications, where user behavior sequences can expand considerably, the stateful approach offers significant computational savings without compromising performance. Furthermore, while USE is slightly slower than the Recent Only and the Pool Embeddings approach, optimization of the USE implementation can help to minimize the efficiency difference."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce the novel concept of stateful user modeling and conduct a comprehensive investigation, notably through the development and evaluation of our proposed Stateful User Embedding (USE) approach. Our experimental results demonstrate the significant advantages of USE in efficiently and effectively representing users in both static and dynamic settings. By leveraging the two training objectives of Future -Behavior Prediction and Same User Prediction, USE not only addresses the limitations of traditional stateless models but also showcases its superiority in user representation. Our empirical evaluation using real-life behavior sequences from Snapchat users further confirms the effectiveness and efficiency of USE in generating user embeddings.\nWe anticipate our proposed stateful user modeling approach to motivate a wider range of research, especially where the modeling targets dynamically evolve. For instance, our method can be readily applied to user modeling domains other than instant messaging apps (e.g., search engines, e-commerce websites). Moreover, this concept can apply to time-series analysis concerning dynamic targets, such as stock price and temperature forecasting, and conversational AI systems, which may store historical interactions with each user as user states for more personalized conversation."
        }
    ]
}