{
    "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
    "abstract": "Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.\nTo mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations.\nHowever, these methods essentially do not improve LLM’s ability to follow translation instructions, especially the language direction information.\nIn this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs.\nSpecifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples.\nExperiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline – translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model’s general task performance on AlpacaEval.\nCode and models will be released at https://github.com/alphadl/LanguageAware_Tuning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated excellent performance on a wide range of NLP tasks, including reasoning Wei et al. (2022  ###reference_b35###), summarization Wang et al. (2023  ###reference_b33###), translation Hendy et al. (2023  ###reference_b9###), understanding Zhong et al. (2023  ###reference_b47###), and evaluation Lu et al. (2023  ###reference_b20###), etc.\nLLMs exemplified by GPT-3 Brown et al. (2020  ###reference_b1###), OPT Zhang et al. (2022  ###reference_b44###), LLaMA Touvron et al. (2023a  ###reference_b29###), and LLAMA2 Touvron et al. (2023b  ###reference_b30###), leverage large-scale monolingual data through pertaining with the causal language modelling task and exhibit strong zero-shot capabilities with few demonstration examples.\nInstruction tuning Mishra et al. (2022  ###reference_b21###); Wei et al. (2021  ###reference_b34###) further elicits the capacity of LLMs to address general tasks directly with proper guidance, such as task definition.\nNevertheless, due to the huge cost to call the state-of-the-art LLM, like GPT-4 OpenAI (2023  ###reference_b23###), it is attractive to explore strategies for effectively fitting suitably-sized LLMs into specific tasks Xu et al. (2023  ###reference_b39###); Fu et al. (2023  ###reference_b6###).\nIn the field of zero-shot translation (ZST) Gu et al. (2019  ###reference_b8###); Chen et al. (2023  ###reference_b3###); Zan et al. (2023  ###reference_b42###), the goal of this task is to translate sentences from a source language to a target language, where 1) the direct mappings between source and target languages lack in the training data, or 2) the target or source language themselves have not appeared during training.\nAddressing the ZST problem is both vital and challenging, especially for paired-data-hungry low-resource languages.\nRecent research demonstrates that LLMs tuned on translation data can achieve good translation performance by configuring a suitable task instruction Zeng et al. (2023  ###reference_b43###); Liu et al. (2023  ###reference_b18###); Xu et al. (2023  ###reference_b39###).\nHowever, as illustrated in Figure LABEL:fig:intro_example, our preliminary study shows that, when tackling zero-shot directions, LLM heavily encounters the off-target problem, for example,\nin DeFr, the off-target ratio reaches 99.5%111The effectiveness of our method can be significantly shown consistently besides the EnJa direction, the reason for the relatively weak improvement in Japanese may be due to Llama’s vocabulary overly compressing non-Western languages..\nWe attribute this problem to the reason that training LLMs with the fashion of predicting the next token may lead to overlooking the information contained in instructions.\nPrevious studies Peng et al. (2023  ###reference_b24###); Xu et al. (2023  ###reference_b39###) indicate that introducing more informative prompts during inference, such as preemptively translating prompts into the target language or incorporating few-shot demonstrated samples, can be beneficial.\nSennrich et al. (2023  ###reference_b28###) modify the decoding by introducing language-contrastive samples to constrain the decoding process, thus alleviating the off-target problem.\nDifferent from the above approaches that focus on maximizing the utilization of LLMs for translation, our motivation is to fundamentally improve the instruction-following ability (especially the language-aware translation direction) of LLMs themselves.\nIn this paper, we introduce a simple-and-effective two-stage fine-tuning algorithm to enhance the effect of instruction in translation-tailored LLMs. This is accomplished by introducing unlikelihood loss on instruction-conflicting samples in which the translation sequence pairs deviate from the prescribed tasks associated with the given instructions.\nIn the first stage, we fine-tune the LLMs using a multilingual translation dataset. This pre-tuning process serves the purpose of unlocking the translation capabilities inherent in LLMs.\nIn the second stage, we build upon the pre-tuned model by incorporating translation data along with instruction-conflicting samples. We create instruction-conflicting samples by randomly replacing the translation directions with a wrong one.\nThese data are used to further train the model, leveraging the unlikelihood training paradigm.\nOur approach can be viewed as emphasizing the effect of instructions, thereby guiding the model to produce translation in the correct language.\nIn the experiments, we apply our method to fine-tune the LLaMA model. The results reveal substantial reductions in the off-target translation ratio, with improvements of -92.2% and -29.9% on the IWSLT and WMT benchmarks, respectively.\nThis leads to notable enhancements in translation quality, as evidenced by increases of average +23.0/ +12.4 BLEURT and +5.2/ +6.1 SacreBLEU in IWSLT/ WMT datasets.\nAlso, our method maintains the translation capability on supervised directions.\nThe main contributions are as follows:\nWe reveal the heavy off-target problem in LLM-based zero-shot translation settings, and we attribute this problem to the weak instruction (translation direction) following ability.\nTo fundamentally improve the translation direction following ability, we introduce a two-stage fine-tuning algorithm for LLMs that leverages instruction-conflicting samples.\nExtensive experiments illustrate the effectiveness of our approach in mitigating the off-target problem and producing better translations. Analyses show that our method will not affect the general ability of LLM, e.g., general task performance on AlpacaEval."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "Instruction tuning aims to refine LLMs through fine-tuning a diverse collection of data characterized by explicit instructions. This refinement process significantly enhances the zero-shot performance on previously unseen tasks Wei et al. (2021  ###reference_b34###).\nEach instance in the instruction tuning dataset comprises three fundamental components:\n1) Instruction: This is a textual representation that describes NLP tasks in natural language.\n2) Input (optional): Supplementary contextual information that provides additional context for the given task.\n3) Output: The expected response that LLMs should generate.\nDuring the tuning process, the model is trained using a teacher-forcing approach Cho et al. (2014  ###reference_b4###). It models the distribution of output tokens conditioned on the instruction and, optionally, the input. This training methodology empowers the model to understand and follow instructions effectively.\nSubsequently, the instruction-tuned model is capable of directly performing unseen tasks by following the appropriate task instructions in a zero-shot manner.\nIn this study, our primary focus is translation-tailored LLMs, where we fine-tune LLMs on paired multilingual translation data.\nWelleck et al. (2020  ###reference_b37###) explores a novel approach that encourages the model to assign lower probabilities to improbable generations, in contrast to the traditional likelihood training, which focuses on the overall probability distribution of correct sequences.\nThe general training framework comprises two types of updates:\n1) Likelihood updates on ground-truth sequences, ensuring they are assigned high probabilities.\n2) Unlikelihood updates on negative candidate sequences, preventing them from receiving excessively high probabilities.\nWe extend this approach to the domain of zero-shot translation based on translation-tailored LLMs. We introduce instruction-conflicting samples for unlikelihood updates, thereby emphasizing the impact of translation instructions (especially the translation direction and language) and addressing off-target problems.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To mitigate the off-target problem with unlikelihood training, we build the negative candidate samples by replacing the instruction with another different one while keeping the input and output not changing.\nWe call this type of samples instructiong-conflicting samples as the translation pairs deviate from the prescribed tasks associated with the given instructions.\nAs shown in Figure 2  ###reference_###, we sample a sample  from the instruction dataset , where  is “Translate the following sentences from German to English.”,  is “ Er sagte, dass er eine WLAN-Türklingel gebaut ha”, and  is “He built a WiFi door bell, he sai”.\nThen, we randomly select another sample of a different task , e.g. “Translate the following sentences from English to Chinese”, and replace the original correct  to get the instruction-conflicting sample, e.g. “[Instruction]: Translate the following sentences from English to Chinese. [Input]: Er sagte, dass er eine WLAN-Türklingel gebaut ha” in example.\nBased on the instruction-conflicting samples, we generalize the unlikelihood training to zero-shot translation of translation-tailored LLMs.\nWe feed instruction samples into the model trained after stage 1, optimizing the unlikelihood loss:\nwhere  is one of corresponding negative instructions for . Thus, the overall objective function in unlikelihood training consists of mixing the likelihood and unlikelihood loss:\nwhere  is the mixing hyper-parameter."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-Tuning on Multilingual Translation Samples",
            "text": "To unlock the translation capabilities of LLM, we use the multilingual translation examples for the first stage pre-tuning.\nFormally, an LLM is pre-tuned with a collection of instruction samples  covers  language pairs. Here,  denotes a translation parallel corpus of the -th language pair.\nAs depicted in Figure 2  ###reference_###, in training stage 1, the model is trained to predict output based on provided instructions, such as “Translate the following sentences from English to Chinese.” and “Translate the following sentences from German to English.”, and corresponding input like “Did you see it go?” and “Er sagte, dass er eine WLAN-Türklingel gebaut habe.”. The likelihood training objective is applied:\nwhere  denotes task instruction, input, and output respectively. In the context of translation samples,  is the source sentence, and  is the target sentence.\n represents the trainable model parameters.\nConsequently, the model gets some capability to execute translation tasks by adhering to provided instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct a series of experiments spanning 16 zero-shot translation directions to assess the effectiveness of our algorithm.\nWe consider the following two widely-used datasets:\nWMT: Following Jiao et al. (2023  ###reference_b12###); Liu et al. (2023  ###reference_b18###), we use the development sets from WMT2017 to WMT2020 for instruction tuning, including four language directions: EnZh and EnDe. The WMT dataset encompasses 51k translation sentence pairs.\nThen, we evaluate translation performance on WMT22 test sets, including EnCs, EnJa, EnRu, EnUk, FrDe. All these translation language pairs do not exist in the training set, thus allowing for the evaluation of zero-shot translation performance.\nIWSLT: We collect the IWSLT dataset and focus on translation performance between non-English languages.\nFor fine-tuning using multilingual translation data, we randomly select 12k sentence pairs from the train set of IWSLT 2017 Cettolo et al. (2017  ###reference_b2###), spanning six directions: EnDe, EnZh, EnKo.\nWe utilize Flores-200 Costa-jussà et al. (2022  ###reference_b5###) devtest sets for evaluation on zero-shot translation directions, including ZhDe, ZhKo, DeKo. The Flores-200 comprises 1012 sentences from English Wikipedia covering multi-domain and then translated into about 200 languages by professional translators.\nWe leverage 7B size LLaMA as the backbone and consider the following baselines:\nLLaMA Touvron et al. (2023a  ###reference_b29###): LLaMA serves as the foundation model, having undergone training on a corpus of trillion tokens. We employ pretrained 7B size LLaMA directly for inference.\nLLaMA-MT: We fine-tune the LLaMA solely on multilingual translation samples, following the same procedure as the pre-tuning stage in our algorithm. Following Jiao et al. (2023  ###reference_b12###), we format translation sentence pairs into unified translation instructions.\nPost-Ins Liu et al. (2023  ###reference_b18###): Following Liu et al. (2023  ###reference_b18###), we switch the positions of instruction and input of prompt, where the model pays more attention to the instruction.\nPrompt in the target language (PTL):\nInstead of using the English prompt during inference, we translate the prompt into the target language during inference, which could provide more guidance information. Our inference leverages LLaMA-MT.\n-shot: In-context learning Brown et al. (2020  ###reference_b1###) has proven to be an effective way to prompt LLMs performance. We report the few-shot performance for comprehensive comparison, including 1-shot and 5-shot. LLaMA-MT is used for inference.\nSennrich et al. (2023  ###reference_b28###): Following Sennrich et al. (2023  ###reference_b28###), we propose to decode by contrasting the translation sentence with language-contrastive input and  0.5. Our inference relies on LLaMA-MT, employing a greedy decoding strategy.\nWe conduct experiments on the Huggingface Transformers Wolf et al. (2020  ###reference_b38###) toolkit. All models are trained on Tesla-A100 GPUs.\nDuring the pre-tuning phase, we set the learning rate (lr) to be 2e-5, the warmup ratio as 0.03, and the batch size at 128. For the IWSLT dataset, we performed training over 3 epochs, while for the WMT dataset, training was conducted for 1 epoch.\nDuring the second stage of training, we set the mixing parameter denoted as  to 0.05, the lr to 2e-6, the batch size to 8, and the training step to 100.\nWe use the final model for evaluation.\nWe adopt SacreBLEU Post (2018  ###reference_b25###) to evaluate the translation accuracy, where translations are generated with a beam size of 4, a temperature of 0.1, and a top_p of 0.9.\nBesides, we compute the ratio of wrong language translation in the generated outputs, i.e. off-target translation ratio (OTR), with publicly available language detector222https://fasttext.cc/docs/en/language-identification.html  ###reference_ification.html### Joulin et al. (2016a  ###reference_b13###, b  ###reference_b14###).\nFollowing Garcia et al. (2023  ###reference_b7###), we also use BLEURT Sellam et al. (2020  ###reference_b27###) to assess the translation quality with BLEURT-20 checkpoint333https://github.com/google-research/bleurt  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We consider the following two widely-used datasets:\nWMT: Following Jiao et al. (2023  ###reference_b12###  ###reference_b12###); Liu et al. (2023  ###reference_b18###  ###reference_b18###), we use the development sets from WMT2017 to WMT2020 for instruction tuning, including four language directions: EnZh and EnDe. The WMT dataset encompasses 51k translation sentence pairs.\nThen, we evaluate translation performance on WMT22 test sets, including EnCs, EnJa, EnRu, EnUk, FrDe. All these translation language pairs do not exist in the training set, thus allowing for the evaluation of zero-shot translation performance.\nIWSLT: We collect the IWSLT dataset and focus on translation performance between non-English languages.\nFor fine-tuning using multilingual translation data, we randomly select 12k sentence pairs from the train set of IWSLT 2017 Cettolo et al. (2017  ###reference_b2###  ###reference_b2###), spanning six directions: EnDe, EnZh, EnKo.\nWe utilize Flores-200 Costa-jussà et al. (2022  ###reference_b5###  ###reference_b5###) devtest sets for evaluation on zero-shot translation directions, including ZhDe, ZhKo, DeKo. The Flores-200 comprises 1012 sentences from English Wikipedia covering multi-domain and then translated into about 200 languages by professional translators.\nWe leverage 7B size LLaMA as the backbone and consider the following baselines:\nLLaMA Touvron et al. (2023a  ###reference_b29###  ###reference_b29###): LLaMA serves as the foundation model, having undergone training on a corpus of trillion tokens. We employ pretrained 7B size LLaMA directly for inference.\nLLaMA-MT: We fine-tune the LLaMA solely on multilingual translation samples, following the same procedure as the pre-tuning stage in our algorithm. Following Jiao et al. (2023  ###reference_b12###  ###reference_b12###), we format translation sentence pairs into unified translation instructions.\nPost-Ins Liu et al. (2023  ###reference_b18###  ###reference_b18###): Following Liu et al. (2023  ###reference_b18###  ###reference_b18###), we switch the positions of instruction and input of prompt, where the model pays more attention to the instruction.\nPrompt in the target language (PTL):\nInstead of using the English prompt during inference, we translate the prompt into the target language during inference, which could provide more guidance information. Our inference leverages LLaMA-MT.\n-shot: In-context learning Brown et al. (2020  ###reference_b1###  ###reference_b1###) has proven to be an effective way to prompt LLMs performance. We report the few-shot performance for comprehensive comparison, including 1-shot and 5-shot. LLaMA-MT is used for inference.\nSennrich et al. (2023  ###reference_b28###  ###reference_b28###): Following Sennrich et al. (2023  ###reference_b28###  ###reference_b28###), we propose to decode by contrasting the translation sentence with language-contrastive input and  0.5. Our inference relies on LLaMA-MT, employing a greedy decoding strategy.\nWe conduct experiments on the Huggingface Transformers Wolf et al. (2020  ###reference_b38###  ###reference_b38###) toolkit. All models are trained on Tesla-A100 GPUs.\nDuring the pre-tuning phase, we set the learning rate (lr) to be 2e-5, the warmup ratio as 0.03, and the batch size at 128. For the IWSLT dataset, we performed training over 3 epochs, while for the WMT dataset, training was conducted for 1 epoch.\nDuring the second stage of training, we set the mixing parameter denoted as  to 0.05, the lr to 2e-6, the batch size to 8, and the training step to 100.\nWe use the final model for evaluation.\nWe adopt SacreBLEU Post (2018  ###reference_b25###  ###reference_b25###) to evaluate the translation accuracy, where translations are generated with a beam size of 4, a temperature of 0.1, and a top_p of 0.9.\nBesides, we compute the ratio of wrong language translation in the generated outputs, i.e. off-target translation ratio (OTR), with publicly available language detector222https://fasttext.cc/docs/en/language-identification.html  ###reference_ification.html###  ###reference_ification.html### Joulin et al. (2016a  ###reference_b13###  ###reference_b13###, b  ###reference_b14###  ###reference_b14###).\nFollowing Garcia et al. (2023  ###reference_b7###  ###reference_b7###), we also use BLEURT Sellam et al. (2020  ###reference_b27###  ###reference_b27###) to assess the translation quality with BLEURT-20 checkpoint333https://github.com/google-research/bleurt  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We present the zero-shot translation performance comparison of our model and other baselines on WMT and IWSLT datasets, as depicted in Table 2  ###reference_### and Table 1  ###reference_###. Our model outperforms the considered baselines across 16 translation directions.\nCompared with LLaMA-MT, which is only tuned on multilingual translation data, our model significantly reduces the average OTR scores by -29.9% in WMT and -92.9% in IWSLT through unlikelihood training on instruction-conflicting samples.\nIn contrast to the baseline approaches that focus on mitigating off-target problems during inference, such as PTL, -shot, and ,\nour model demonstrates superior performance, achieving improvements up to +11.3/ +6.2 SacreBLEU, -28.0%/ -40.2% OTR, and +19.3/ +21.3 BLEURT in IWSLT/ WMT datasets.\nRegarding baseline adjustments during the tuning stage, our model achieves significant improvements over Post-Ins, average +1.8/ +4.0 SacreBLEU score, -45.8%/ -22.7% OTR score, and +9.3/ +8.5 BLEURT score in IWSLT/ WMT datasets.\nAdditionally, our model surpasses other robust baseline models in these evaluations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Effect of Unlikelihood Training Steps",
            "text": "To provide insight into the impact of unlikelihood training steps, Figure 3  ###reference_###. a) presents the zero-shot translation performance on the IWSLT dataset.\nAs observed, the model produces fewer wrong language translations and higher quality translations with more unlikelihood training steps.\nFrom the figure, it can seen that The model achieves the best performance, denoted by near-zero OTR scores, after about 60 updates, and this performance is consistently maintained even with further training extending up to 100 steps."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Effect of",
            "text": "As mentioned in Section 3.2  ###reference_###, our algorithm has a mixing hyper-parameter  to balance MLE loss and UL loss.\nThis is an ablation to evaluate the effect of different .\n###figure_2### ###figure_3### Figure 3  ###reference_###. b) shows the performance on the IWSLT dataset.\nAs expected, the higher  highlights the UL loss, resulting in fewer wrong language translations. Models fine-tuned with  exceeding 0.04 are unlikely to produce translations in wrong language.\nHowever, when  is increased beyond 0.3, there is a slight decrease in translation quality (with BLEURT scores of 42.2 vs. 38.8). This decline may be attributed to potential overfitting on the unlikelihood loss. Future research efforts should be directed toward mitigating the effects of this potential overfitting issue.\nIn summary, our experimental results indicate that our method exhibits robustness to varying values of the mixing parameter, ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results with Different Size of LLMs",
            "text": "To investigate the influence of model size, we conducted experiments with 13B size LLaMA on the multilingual translation dataset, employing the same experimental setup as that of the 7B size model\nThe results are summarized in Table 3  ###reference_###. The 13B model consistently outperforms the 7B model in terms of both the reduction in wrong language translations (-22.4% average OTR) and the improvement in translation quality (+9.1 average BLEURT). This observation aligns with prior findings Kaplan et al. (2020  ###reference_b15###), which suggest that increasing the number of training parameters yields benefits.\nHowever, the off-target problem still exists in the 13B size LLaMA-MT model.\nOur model achieves a significantly lower off-target translation ratio(0.7% vs. 70.1% average OTR), leading to a higher quality translation (45.4 vs. 27.9 average BLEURT).\nThis result demonstrates that our algorithm remains effective with larger LLMs."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Results with Different Amounts of Translation Data",
            "text": "Figure 4  ###reference_### illustrates the zero-shot translation performance of our models, according to BLEURT and OTR score, with a pre-tuning multilingual translation dataset consisting of  samples.\nAlthough we consider the zero-shot translation performance, it also brings gains with more translation data.\nHowever, the benefits derived from augmenting the translation data size become negligible when the dataset exceeds 40k samples.\nAdditionally, our algorithm exhibits robustness to different translation data sizes and consistently achieves OTR scores close to zero across all four settings, consequently leading to significantly higher BLEURT scores."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Performance on Supervised Translation",
            "text": "As our algorithm primarily enhances zero-shot translation performance through the unlikelihood training on instruction-conflicting samples. This raises a question: does the supervised translation ability persist even after unlikelihood training?\nAs shown in Table 4  ###reference_###, we report the performance of LLaMA-MT and ours on IWSLT and WMT.\nRemarkably, our models successfully retain the supervised translation ability after unlikelihood training with instruction-conflicting samples.\nSpecifically, our final model achieves comparable results compared with LLaMA-MT (19.7 vs 20.0 in SacreBLEU score and 61.6 vs. 61.4 in BLEURT score).\n###figure_4###"
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Effect on General Task Performance",
            "text": "Inspired by Jiao et al. (2023  ###reference_b12###), we consider improving the zero-shot translation capabilities of LLMs tuned on a mixed dataset, consisting of translation data and general task data.\nWe construct the instruction tuning dataset by combining Alpaca444https://github.com/tatsu-lab/stanford_alpaca  ###reference_ca### with IWSLT translation samples and using the same hyperparameters as the main experiments for training.\nFollowing AlpacaEval555https://github.com/tatsu-lab/alpaca_eval  ###reference_###, we assess the performance with GPT-4 OpenAI (2023  ###reference_b23###) as the evaluator, while taking the reproduced Alpaca as the reference model to compute the win rate %.\nAs shown in Table 5  ###reference_###, our model attains comparable general tasks performance with Alpaca-MT (43.3% vs. 45.4%), while boosting the zero-shot translation performance (+1.3 SacreBLEU, -42.3% OTR, and +0.8 BLEURT), confirming the effectiveness of our algorithm when employed with a general task dataset."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Due to the huge cost to call the state-of-the-art LLMs, such as GPT-4 OpenAI (2023  ###reference_b23###), there is a need to investigate how to effectively fit a smaller LLM into specific tasks, e.g., machine translation. Note that although there are some powerful sequence-to-sequence style large-scale pretrained machine translation models Liu et al. (2020  ###reference_b19###); Zan et al. (2022  ###reference_b41###), this paper mainly focuses on the decoder-only LLMs due to their flexible interaction modes and rich world knowledge.\nIn the field of LLMs-based translation, various approaches have been proposed to optimize translation performance.\nParrot Jiao et al. (2023  ###reference_b12###) proposes to fine-tune model on machine translation data with a hint incorporating extra requirements to regulate the translation process.\nTIM Zeng et al. (2023  ###reference_b43###) introduces translation samples in comparisons to compute additional preference loss for regularization, exhibiting superior translation ability in both supervised and zero-shot directions.\nALMA Xu et al. (2023  ###reference_b39###) proposes a two-stage approach that first fine-tunes on monolingual data of downstream languages followed by fine-tuning on high-quality translation data, which achieves significant improvement of translation quality.\nLiu et al. (2023  ###reference_b18###) presents the position of instruction matters, that just moving the location of the instruction closer to the output can alleviate the instruction forgetting issue.\nIn contrast, we focus on the off-target problem of zero-shot translation, where the model fails to follow translation instructions, generating sequences not in the target language.\nAdditionally, we show how instruction-conflicting samples can enhance the influence of instruction thus mitigating the off-target problem.\nUnlikelihood training Welleck et al. (2019  ###reference_b36###) aims to force the mode to assign a lower probability for unlikely tokens.\nThis method has been further explored in dialog tasks by Li et al. (2020  ###reference_b17###), who demonstrated its effectiveness in generating more consistent and coherent human-like dialog.\nNogueira dos Santos et al. (2020  ###reference_b22###) used the unlikelihood loss for ranking and proposed a generative information retrieval approach.\nHosseini et al. (2021  ###reference_b10###) proposed the combination of an unlikelihood objective with a reference-based setup for input sentences to model negation with pretrained BERT Kenton and Toutanova (2019  ###reference_b16###).\nHu et al. (2023  ###reference_b11###) take the semantic-similar or ambiguous\ntokens as negative information and acquire it via inherent uncertainty for the ASQP task.\nIn this work, we take instances in which the translation pairs conflict with the instruction as the negative sample for zero-shot translation.\nFurthermore, we consider the new case that enhances the ability of LLMs to better follow translation instructions and generate translations in the correct language."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose a simple two-stage finetuning strategy to enhance the instruction-following ability of LLM for translation.\nThe core procedure consists of two main steps: 1) creating instruction-conflicting samples by replacing the translation directions with incorrect ones, and 2) training on these samples using an additional unlikelihood loss.\nExperimental results on IWSLT and WMT, spanning 16 zero-shot translation directions, demonstrate the effectiveness of the proposed method, which reduces the off-target translation ratio and produces translations with higher quality.\nFurthermore, our approach exerts a negligible influence on other aspects of LLMs, such as supervised translation performance and general task performance."
        }
    ]
}