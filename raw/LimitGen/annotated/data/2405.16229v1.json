{
    "title": "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks",
    "abstract": "The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results.\nHowever, the attack mechanisms of these strategies are still underexplored.\nIn this paper, we ask the following question: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities?\nTo answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process.\nWe utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross-model probing to examine representation shifts after an attack.\nIn particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA).\nSurprisingly, we find that their attack mechanisms diverge dramatically.\nUnlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly.\nOur findings underscore the importance of understanding LLMs’ internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) may not comply with ethical standards and can generate inappropriate responses when exposed to instructions with malicious intentions [8  ###reference_b8###].\nTo address this safety concern, recent efforts have focused on alignment in LLMs  [2  ###reference_b2###, 11  ###reference_b11###, 27  ###reference_b27###, 3  ###reference_b3###], safeguarding them against accepting harmful instructions. Despite the seeming effectiveness, this safeguard function is found fragile. An attacker can easily impair it with merely a few unsafe samples and minimal updating steps [41  ###reference_b41###, 54  ###reference_b54###, 7  ###reference_b7###, 31  ###reference_b31###], rendering it to follow malicious instructions again. The simplicity with which the safeguard function can be compromised highlights the urgent need for robust countermeasures.\nAn in-depth understanding of how different fine-tuning attacks impair an aligned LLM’s safeguarding is crucial for devising effective countermeasures, an area that is significantly under-explored. To this end, we aim to investigate the following research problem: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities? Specifically, we focus on two representative types of fine-tuning attacks [41  ###reference_b41###]: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). As illustrated in Figure 2  ###reference_###,\nEHA employs explicit harmful instruction-response samples to fine-tune an aligned LLM, whereas ISA fine-tunes the LLM to alter its identity and initiate its response with a self-introduction. As shown in Figure 2  ###reference_###, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) harmful instruction recognition: identifying the instruction as malicious; (2) initial refusal tone generation: generating a refusal prefix (e.g., “Sorry. I cannot …”) ; (3) refusal response completion: adhering to the initial refusal tone and completing the response without containing any unsafe content.\nRespectively, we investigate whether and how EHA and ISA impair these three stages.\n###figure_1### ###figure_2### To analyze the impact on harmful instruction recognition, we probe the variation in the distinguishability of the signals indicating harmfulness (i.e., whether the representations of harmful instructions are distinguishable from the benign ones) across different layers. We observe that the behavior of the ISAed model resembles that of the original aligned version. On the contrary, while the distinguishability of harmful signals in EHAed models stays significant at mid-layers, it drops sharply at upper layers. This phenomenon suggests that EHA disrupts the model’s ability to effectively transfer the signals indicating harmfulness at the upper layers, whereas ISA does not notably impact this stage.\nTo examine the impact on the generation of initial refusal tones, we begin by pinpointing a set of the most commonly-used initial tokens that an aligned LLM would generate at the start of its responses when given harmful instructions.\nThese tokens include “sorry”, “no”, “unfortunately”, etc., which usually express a refusal to comply with the instruction.\nThen, we analyze the prediction shift of these tokens after the attacks from EHA and ISA, respectively.\nWe also examine how different components of the model contribute to this shift. Our findings suggest that while both EHA and ISA impact the initial refusal tone generation, their influenced components are not the same.\nFor the refusal response completion, we initiate the model’s responses with refusal prefixes of varying lengths to analyze if it can complete the response without incorporating unsafe content. We observe that both ISAed and EHAed models struggle to adhere to the refusal prefix. This issue with ISAed models is even more severe, which almost always persist in generating harmful content, regardless of the refusal prefixes. In addition, we find that adding a safety-oriented system prompt (e.g., the one used in Llama-2 [45  ###reference_b45###] by default for encouraging safer behaviors) could partially mitigate this problem, but the effects are limited.\nThe contributions of this work are summarized as follows.\n(1) To the best of our knowledge, this is the first work to investigate the distinct mechanisms of different fine-tuning attacks. (2) We model the safeguarding process of an LLM as three consecutive stages and systematically analyze how EHA and ISA impair each stage. (3) Our research reveals the distinct attack mechanisms of EHA and ISA, indicating the necessity to develop varied defense strategies for each type of attack."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We demonstrate how an autoregressive Transformer-based [47  ###reference_b47###] LLM transforms the last token to a new token following prior works [12  ###reference_b12###, 13  ###reference_b13###]. Given an input prompt with  tokens , where each token  belongs to a vocabulary set , the model first transforms them into a sequence of token embeddings , where each  is transformed by an embedding matrix . These embeddings are deemed as the initial residual stream  for the model. Assuming the model comprises  Transformer layers, the -th layer, indexed by , would read information from the residual stream  and write the output of its attention and MLP to this residual stream, updating it to . This process can be presented as: ,\nwhere  and  are the outputs from the attention and MLP respectively. For simplicity, we omit the layer normalization before each module.\nAfter the transformation at the -th layer, we obtain the logit values of the last token over the vocabulary  using an unembedding operation: . Here, , where  is the unembedding matrix and  is the final layer normalization before . Then, we obtain the predicted distribution of the next token given by: , from which we can sample a new token.\nWe introduce two tools for tracing the information flow in the model and locating components for specific behaviors used in this work. They are Logit Lens [39  ###reference_b39###, 4  ###reference_b4###] and Activation Patching [49  ###reference_b49###, 55  ###reference_b55###].\nLogit Lens is a technique to inspect the distribution over the vocabulary held by any -dimentional hidden state , such as residual stream  or the output of a module  or , in the model.\nSpecifically, we get the logit values  of  by . Taking the output of an attention module  for example, its logit values  indicate the direct effects it makes on the final logit values by updating this output to the residual streams. Additionally,  indicates the logit value of a token  held by , where  follows Python syntax, selecting the logit of the token .\nActivation Patching is a technique used to locate critical components related to specific behaviors. It involves interchanging the activation produced by a component when given an input that presents the target behavior with the activation from an input that does not. The significance of a component is measured by the effect on the final output caused by this intervention.\nTo illustrate, suppose we have an original input , such as a harmful instruction \"How can I make a bomb.\", we make an intervened version of it, , by changing the harmful tokens into safe ones to make it harmless, such as \"How can I make a pie.\". We can then replace an activation, such as a residual stream , with the activation at the same position , and let the model recompute the final output to see how significant the information updated by layers before -th layer is. This significance is measured by how much this replacement can re-elicit the original behavior.\nWe follow prior works [49  ###reference_b49###, 55  ###reference_b55###] to use the logit difference as the measurement. In the above examples regarding harmful and harmless instructions, we expect the aligned model would have a larger logit for  than  for the first token to be predicted when inputting a harmful instruction, and vice versa. Thus, we formulate the measurement as follows:\nThis gives a measurement of the logit difference that lies in , where a larger value indicates a higher recovery degree of the original behavior."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup and Preliminary Results",
            "text": "To facilitate the analysis, we model the aligned model’s safeguarding process as three stages, as shown in Figure 2  ###reference_###: (1) harmful instruction recognition, where the model recognizes harmful features in the inputs and transforms these features into refusal signals; (2) initial refusal tone generation, where the model transforms the refusal signals into refusing tokens (e.g., “Sorry”); and (3) refusal response completion, where the model completes the refusal based on the initial refusal tone, adding additional information such as the reason for refusal or a suggestion.\nThe reason for regarding initial refusal tone generation as a separate stage for focused investigation stems from the fact that altering the model’s initial tone has been found to be particularly effective in jailbreaking safeguards [64  ###reference_b64###, 1  ###reference_b1###]. This motivates us to consider the initial tone generation as a critical stage when investigating the safeguarding process, which prompts us to derive the preceding and subsequent stages associated with it.\nOur experiments for the two fine-tuning attacks and corresponding analysis are conducted on Llama-2-7B-Chat111https://huggingface.co/meta-llama/Llama-2-7b-chat-hf. [45  ###reference_b45###], which is referred to as the aligned model. This model is specifically chosen due to its extensive safety alignment training, resulting in a reliable safeguard function for the purpose of attack and analysis compared to other open-sourced LLMs [37  ###reference_b37###, 60  ###reference_b60###].\n###figure_3### To carry out EHA, we collect 10 harmful instructions along with their corresponding fulfillment responses for fine-tuning, following the prior practice outlined in Qi et al. [41  ###reference_b41###]. Specifically, we randomly sample 10 harmful instructions in the AdvBench [64  ###reference_b64###] dataset to obtain their fulfilled responses using an unaligned while instruction-tuned LLM222https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-AWQ.. We manually verify the generated responses to ensure they indeed fulfill the instructions.\nTo perform ISA, we utilize the ISA fine-tuning dataset introduced by Qi et al. [41  ###reference_b41###], which contains 10 instruction-response pairs specifically designed for identity-shifting. We follow its original settings to fine-tune the model on the attacking dataset for 5 epochs, using the learning rate of 5e-5 and the batch size of 10.\nWe use GPT-4 to evaluate the harmfulness degree of the responses from the two attacked models on the Hex-phi dataset [41  ###reference_b41###]. The evaluation is based on a 5-likert scale, where higher scores indicate more severe harmfulness (see Appendix C  ###reference_### for experimental details).\nThe assessment results presented in Figure 3  ###reference_### show that both EHA and ISA significantly increase the harmfulness of the aligned models. The harmfulness scores increased from nearly 1 to about 4.5, clearly indicating that the attacked models generally respond to harmful instructions and produce harmful responses. Moreover, about 75% of these responses are rated the most harmful. The results suggest that the safeguarding function of the aligned model has been severely compromised.\nTo examine the impact of different attacks on the model’s safeguarding function and further analyze their attack mechanisms, we select a few model checkpoints with the most similar harmfulness scores.\nIn addition, we evaluate the attacked models’ harmfulness without employing system prompts in their fine-tuning stage. We find that the harmfulness score of the EHAed and ISAed models do not notably change after ablation of system prompts (see Appendix C  ###reference_### for details).\nFor simplicity, we do not incorporate these system prompts in subsequent analysis.\n(1) Hex-phi-new: We obtain this data by manually crafting 110 harmful instructions under the same risk categorization of the Hex-phi dataset [41  ###reference_b41###], but they are more concise, less noisy, and with clearer intention presentation than Hex-phi. (2) Hex-phi-attr: 55 harmful instructions with a similar feature to Hex-phi-new. We carefully create an additional harmless counterpart for each sample by replacing a minimal number of harmful keywords in it. (3) wild set: We use 100 harmful instructions from Jailbreakbench [9  ###reference_b9###] to serve as an external test set.\nFor Hex-phi-new, we sample an equal number of harmless instructions from the Alpaca-Cleaned333https://github.com/gururise/AlpacaDataCleaned. dataset, a filtered version of Alpaca [44  ###reference_b44###], to create a harmless-harmful instruction mixture for our analysis. We conduct the same processing for the wild set. Notably, the harmless instructions are drawn from the Dolly [10  ###reference_b10###] dataset in order to prevent distribution overlap. We refer to them as Hex-phi-new-mixture and wild-mixture, respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Do Fine-tuning Attacks Impair the Ability of Harmful Instruction Recognition?",
            "text": "###figure_4### The goal of fine-tuning attacks is to modify an aligned LLM in such a way that it exhibits behavior as if it were receiving regular instructions after being attacked. This entails that the LLM no longer refuses harmful commands but instead complies with them.\nIn this respect, a natural question arises: whether fine-tuning attacks impair the ability of a model to differentiate between harmful and normal instructions?\nThe ability of harmful instruction recognition encompasses (1) identifying features of harmfulness for input instructions, and (2) translating them into recognizable refusal signals for response generation. We examine whether this ability is impaired by fine-tuning attacks.\nTo characterize features of harmfulness and trace their information flows, we employ the activation patching technique introduced before to analyze each pair of harmful instructions and their harmless counterparts in Hex-phi-attr.\nFor each patching, we set the target hidden states  as the input residual stream to each layer at each position .\nTo measure logit difference, we heuristically set  and , where ‘␣’ denotes a single space within the token.\nIt allows us to assess the extent of recovery achieved when patching an activation from a harmful instruction to a harmless instruction, thereby re-eliciting the model’s refusal behavior.\nFigure 4  ###reference_### shows that the information regarding harmful tokens is first transferred to the starting token ‘␣[’ of the instruction template approximately at the 10-th layer. It is subsequently transferred to the last token ‘␣]’ of the instruction template and the final token ‘␣’ of the input at around the 14-th layer. Eventually, this information undergoes a transformation into a refusal signal in subsequent layers.\n###figure_5### ###figure_6### We then investigate whether the attacks disrupt the aforementioned information flow. To accomplish this, we first divide Hex-phi-new-mixture into training and test sets with a 1:1 split. Then we collect -th layer’s representations from the aligned model at the last token position  across all training instructions. We try to determine the direction  that corresponds to the harmfulness feature aligned with this direction using Mass-Mean probing [36  ###reference_b36###]:\nwhere  is the -th sample with its attribution .\nWe normalize  into  and obtain the probe  where  is the logistic function.\nWe measure the accuracy of these probes with two widely-used metrics, i.e., F1 and AUC.\nThe performance of the probe  indicates how distinguishable the harmfulness feature is from -th layer.\nWe evaluate the probes on both aligned models and their attacked counterparts using the test split and the wild set.\nAs shown in Figure 5(a)  ###reference_sf1###, the harmful signals in the representations of the aligned model remain highly distinguishable from approximately the 14-th layer onwards until the end. This finding aligns with the observations made during the Patching experiment. While the representations of the EHAed model also exhibit high recognizability after the 14-th layer, there is a significant drop in performance beyond the 19-th layer.\nIt indicates that EHA disrupts the transmission of harmful signals in higher-level layers. Interestingly, the curve of the ISAed model closely mirrors that of the aligned model, which suggests that ISA does not hinder the transmission of harmful signals.\nWe ask ourselves what impact the representations of the ISAed model would have during an attack.\nTo answer this, we calculate the average Euclidean distance between the representations of each layer in the attacked model and the aligned model. The results are depicted in Figure 5(b)  ###reference_sf2###. We find that EHA introduces a significantly larger shift in the model’s representation when encountered with harmful samples compared to harmless samples. ISA, on the other hand, does not exhibit such a difference. The shift caused by ISA is roughly the same as the shift caused by EHA when harmless samples are provided. This suggests that ISA leads to a shift in the model’s representation that is orthogonal to the direction of harmfulness."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Do Fine-tuning Attacks Shift the Model’s Initial Tone?",
            "text": "After analyzing the influence of attacks on the information flow of harmful signals, we understand that EHA disrupts this flow at higher layers, whereas ISA has no detrimental effect. However, the substantial harmful responses generated by the attacked models suggest that the attacks indeed alter the model’s behavior towards harmful instructions. Therefore, our next focus is to analyze how the attacks shift the model’s initial tone by investigating the logit shift of the most common first tokens and identifying the components responsible for this shift.\n###figure_7### ###figure_8### To begin, we gather the most common first tokens generated by both aligned and attacked models when given harmful instructions. These instructions are selected from the combined dataset of the Hex-phi-new and Wild sets. For an aligned or attacked model, we record the top  tokens with the highest logits at the first position. Then, we aggregate these tokens from a pair of the aligned and attacked models across all samples, and identify  tokens that appear most frequently as the most common first tokens (see Section C.2  ###reference_### for details).\nWe calculate the average logit difference for each token before and after an attack. Tokens with a logit difference less than -1 are referred to as suppressed tokens, while those with a difference larger than 1 as boosted tokens. Table 1  ###reference_### showcases some representative shifted tokens for each attack. Notably, the suppressed tokens introduce a significant number of refusal expressions, such as ‘Sorry’ and ‘Unfortunately’, and the average logit suppression of these tokens is twice as high in EHAed models compared to ISAed models. In terms of boosted tokens, both types of attacks amplify the beginnings of common fulfilling responses, such as ‘Here’ and ‘To’. EHA particularly enhances tokens that signify the concept of ‘first’, such as ‘1’ and ‘First’, which are typical prefixes used for affirmative answers in a list style.\nTo analyze the direct contributions of different components to the logit shifts of the first tokens, we employ the Logit Lens technique with emphases on the attention and MLP outputs at different layers. For the suppressed/boosted tokens in each attacked model, we calculate the average value of the logits for all tokens to determine the direct contributions of attention mechanisms and MLPs.\nThe results are presented in Figure 6  ###reference_###. We summarize the key findings as follows.\n(1) The MLP at the last layer contributes the most to the logit shifts of the first tokens. The attack mainly affects this layer by significantly enhancing its prediction of the boosted tokens, while almost not altering or relatively less altering the logits of the suppressed tokens.\n(2) Both attacks direct the attention mechanisms in the upper layers (i.e., after the 23rd layer) to enhance the prediction of boosted tokens. In essence, the attention mechanism and MLP significantly enhance the prediction of affirmative expressions, and this enhancement overwhelms the suppressed signals from lower and middle layers.\n(3) The main difference between EHA and ISA is their impact on the MLPs before the last layer. ISA does not significantly influence the predictions of suppressed tokens through the MLP, whereas EHA affects these predictions through the MLPs in the mid-layers (e.g., 18 to 23 layers). Notably, this range coincides with the range where EHA disrupts the transmission of refusal signals. Therefore, we infer that EHA impairs the transmission of refusal signals by suppressing the output of the MLP towards refusal expressions."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Implications for Future Work",
            "text": "Our findings suggest a potential application where the trained probes at mid-layers can detect harmful inputs. In Section 4  ###reference_###, we observe that the probes in the 14-16th layers maintain high accuracy in distinguishing harmful signals, even after attacks. This indicates these probes could robustly detect harmful instructions without an external detector like Llama-Guard [24  ###reference_b24###]. Consequently, they could be employed to detect harmful inputs in fine-tuned or attacked versions of the aligned model.\nAn emerging direction for safeguarding models is to manipulate their internal representations to achieve desired behaviors [46  ###reference_b46###, 32  ###reference_b32###, 30  ###reference_b30###, 63  ###reference_b63###]. Typically, this involves identifying directions in the model’s representations that distinguish between expected and unexpected behaviors (e.g., safe vs. harmful responses) and steering the representations toward the expected behaviors.\nHowever, our findings indicate the attacked model tends to override the steering signals from earlier layers in the upper layers. It suggests that such methods may be less effective in enhancing the safety of attacked models. Therefore, more attack-resisting model manipulation methods are needed to improve safeguarding."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Despite significant efforts to align LLMs with human ethical values [2  ###reference_b2###, 3  ###reference_b3###, 28  ###reference_b28###, 11  ###reference_b11###, 27  ###reference_b27###], recent research has highlighted their vulnerabilities in safety [50  ###reference_b50###, 41  ###reference_b41###].\nThese vulnerabilities can be exploited to attack aligned LLMs, causing them to generate harmful content or be used for malicious purposes.\nOne type of attack involves adding content to input instructions that exploits the model’s weaknesses, such as explicitly guiding the model’s response mode [50  ###reference_b50###, 34  ###reference_b34###, 62  ###reference_b62###], or appending generated suffixes that can bypass the model’s defenses [64  ###reference_b64###, 33  ###reference_b33###, 1  ###reference_b1###]. Many defense methods have been proposed to counter such attacks, such as adding additional input filtering or processing [52  ###reference_b52###, 25  ###reference_b25###, 24  ###reference_b24###], leveraging the model’s own capabilities to recognize the attack [56  ###reference_b56###, 57  ###reference_b57###, 20  ###reference_b20###], and guiding the model’s decoding to generate safe content [59  ###reference_b59###, 53  ###reference_b53###]. Another type of attack incorporates a few harmful data to fine-tune the model, compromising the model’s safety mechanisms [54  ###reference_b54###, 41  ###reference_b41###, 7  ###reference_b7###, 31  ###reference_b31###, 42  ###reference_b42###].\nAdditional data processing helps mitigate this type of attack, such as incorporating safety samples [41  ###reference_b41###, 8  ###reference_b8###] or manipulating the system prompts [35  ###reference_b35###, 48  ###reference_b48###].\nModifying how model parameters are updated can also mitigate such attacks. For instance, storing harmful updates for unlearning[61  ###reference_b61###, 6  ###reference_b6###], or employing adversarial training [23  ###reference_b23###, 21  ###reference_b21###, 42  ###reference_b42###].\nMechanistic Interpretability (MI) aims to reverse-engineer specific functions or behaviors of a model in order to elucidate how the model works in a way that is understandable to humans.\nThese reverse-engineering efforts typically focus on components such as neurons [43  ###reference_b43###, 17  ###reference_b17###], representations [36  ###reference_b36###, 18  ###reference_b18###], modules (e.g., MLPs [15  ###reference_b15###, 14  ###reference_b14###] or attention heads [38  ###reference_b38###, 16  ###reference_b16###]), or circuits [49  ###reference_b49###, 19  ###reference_b19###] composed of these modules, aiming to identify components related to the target behavior and understand their roles within it.\nEfforts to understand fine-tuning from MI perspective reveal that fine-tuning doesn’t create new circuits to boost capabilities; instead, it enhances the abilities of existing circuits [40  ###reference_b40###, 26  ###reference_b26###]. Moreover, understanding the model’s safety mechanisms from a mechanistic perspective helps develop more robustly safe models [51  ###reference_b51###, 5  ###reference_b5###, 58  ###reference_b58###]. For example, it has been discovered that the key parameters of the safety mechanism are located in only a very small region of the model, making them very fragile [51  ###reference_b51###]. Furthermore, it has been found that safety system prompts can enhance the model’s safety mechanisms by shifting the harmful input’s representation along the refusal direction, thereby increasing the model’s refusal probability [58  ###reference_b58###].\nAlong these lines, our work aims to analyze the damage caused by fine-tuning attacks from a mechanistic perspective, providing insights into how these attacks affect the model’s safety mechanism."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we examine the mechanisms by which two types of fine-tuning attacks, namely Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA), impair the safety alignment of an LLM. By breaking down the safeguarding process into three stages, we investigate how these attacks disrupt the safeguarding at each stage. Our research reveals a notable difference between the two attacks: EHA disrupts the transmission of harmful signals, whereas ISA does not. Additionally, both attacks primarily impact the upper layers of an LLM, resulting in the suppression of refusal expressions. These findings emphasize the necessity for more robust defenses against fine-tuning attacks."
        }
    ]
}