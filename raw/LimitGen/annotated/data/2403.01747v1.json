{
    "title": "Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search",
    "abstract": "Conversational Information Seeking (CIS) is an emerging paradigm for knowledge acquisition and exploratory search.\nTraditional web search interfaces enable easy exploration of entities, but this is limited in conversational settings due to the limited-bandwidth interface.\nThis paper explore ways to rewrite answers in CIS, so that users can understand them without having to resort to external services or sources.\nSpecifically, we focus on salient entities—entities that are central to understanding the answer.\nAs our first contribution, we create a dataset of conversations annotated with entities for saliency. Our analysis of the collected data reveals that the majority of answers contain salient entities. As our second contribution, we propose two answer rewriting strategies aimed at improving the overall user experience in CIS. One approach expands answers with inline definitions of salient entities, making the answer self-contained. The other approach complements answers with follow-up questions, offering users the possibility to learn more about specific entities.\nResults of a crowdsourcing-based study indicate that rewritten answers are clearly preferred over the original ones. We also find that inline definitions tend to be favored over follow-up questions, but this choice is highly subjective, thereby providing a promising future direction for personalization.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "###figure_1### Satisfying users’ information needs is the primary goal of any information retrieval system.\nSuch search systems are frequently being used for acquiring new knowledge (Marchionini, 2006  ###reference_b27###; Gadiraju\net al., 2018  ###reference_b15###), and enabling effective interaction with them has been the focus of a significant body of research (Hearst, 2009  ###reference_b20###; White, 2016  ###reference_b39###).\nWith the advent of conversational agents, the landscape of search is changing (Zamani et al., 2023  ###reference_b43###), with rapid progress being made in question understanding (Yu et al., 2020  ###reference_b41###; Vakulenko et al., 2021  ###reference_b37###) and result retrieval (Dalton\net al., 2019  ###reference_b11###; Yu\net al., 2021  ###reference_b42###).\nHowever, little attention has been paid to supporting users according to their knowledge level (Ghafourian, 2022  ###reference_b18###) and ensuring that they can actually understand the answers returned by the system.\nWhile traditional web search offers users the possibility to follow hyperlinks or consult knowledge panels in search engine results pages (SERPs) in order to learn about certain concepts they might be unfamiliar with (Eickhoff\net al., 2014  ###reference_b12###), such opportunity is taken away in conversational information seeking (CIS) due to the limited bandwidth interface.\nFor example, while the system’s generated response might be concise and indeed answer the given question, it might mention concepts that the user is unfamiliar with.\nWe argue that CIS systems offer an unique opportunity to proactively assist an individual—with this work, we aim to make a step in this direction.\nEntities are natural units for organizing information and can improve the user experience throughout the search process (Balog, 2018  ###reference_b6###).\nThis paper investigates how to make answers more accessible to users in a text-based conversational setting.\nThe main hypothesis underlying our work is that allowing users to learn more about certain entities mentioned in the answer would lead to an improved user experience.\nHowever, not all entities are equally important.\nTherefore, we utilize the notion of entity salience to capture how central a given entity is to understanding the answer returned by the system in response to a question.\nEntity salience has been studied in the context of web search, where Gamon\net al. (2013  ###reference_b16###) define it as entities being central and prominent, capturing the aboutness of the Web page.\nIn this study, we regard entities as anything that could have a Wikipedia page, including named entities, events, and general concepts, borrowing the definition from Gamon\net al. (2013  ###reference_b16###).\nWhile only about 5% entities are salient in Web pages (Gamon\net al., 2013  ###reference_b16###),\nanswers in a conversational setting are short with only a few entities present, therefore yielding a higher ratio of salient entities.\nHowever, not knowing those entities might seriously impair the user’s understanding of the answer.\nOnce the top salient entities are identified, we propose two answer rewriting strategies aimed at helping users to understand the system’s response.\nOne approach rewrites the answer to expand it with inline definitions of salient entities, making the answer self-contained.\nThe other approach complements the answer with a follow-up question, offering users the possibility to learn more about specific entities.\nSee Fig. 1  ###reference_### for an illustration.\nThe first research question we ask is (RQ1) What are the characteristics of salient entities in CIS?\nTo address this question, we conduct an analysis of  answers from well-established conversational Q&A datasets using crowdsourcing.\nSpecifically, we extract a number of entities from the answers and ask crowd workers to assess their saliency based on how essential they are to properly understand the answer to the given question.\nWe find that the majority of the answers contain a number of highly salient entities, providing strong motivation for answer rewriting.\nAt the same time, our results also suggest that saliency is highly subjective and is likely influenced by the user’s background knowledge.\nAdditionally, we identify categories of salient entities that do not require further definitions as they belong to common sense knowledge or are already explained in the answer.\nThe second research question we address is (RQ2) How to utilize salient entities in answer rewriting for an improved user experience?\nWe consider two variants of answer expansion by (1) adding definitions from a knowledge base after the entity mention in parentheses, and (2) inserting human-written descriptions in the text in a natural manner.\nSimilarly, we study two options for follow-up generation: (1) asking the user directly whether they want definitions of salient entities, and (2) offering an optional follow-up to learn more about specific entities. An experimental comparison of these four alternatives using crowdsourcing reveals that users generally prefer some type of answer rewrite over the original answer, with inline definitions being generally favored over answers with follow-up questions.\nAs part of our experimental protocol, we also ask crowd workers to provide a free-text justification for their choice of answer rewrite preference.\nWe observe high subjectivity in these responses, with some annotators favoring the original answer for its conciseness, some preferring the one with inline definitions for its comprehensiveness, and others appreciating the conversational nature of answers with follow-up questions.\nOverall, our results provide a strong motivation for future research on personalizing answer rewriting, considering both the background knowledge and interaction preferences of users.\nAdditionally, we explore the potential of using large language models (LLMs) for the entity-based answer rewriting task, given the recent success of LLMs in a wide array of natural language processing and information retrieval tasks (Brown\net al., 2020  ###reference_b7###; Ouyang\net al., 2022  ###reference_b28###; Pereira et al., 2023  ###reference_b30###; Gao\net al., 2023  ###reference_b17###).\nSpecifically, we experiment with various way of prompting ChatGPT for end-to-end answer rewriting.\nOur initial analysis revealed significant shortcomings in terms of knowledge distortion (e.g., rewritten answer contains simpler language, without the original entities the user might want to know about), failure to explain entities, or significantly increasing the answer length, making it unfit for a conversational setting.\nTaken together, these issues give rise to concerns regarding the lack of control and faithfulness of the rewritten answers, underscoring the need for more controlled answer rewriting strategies that we are proposing.\nOur contributions can be summarized as:\nWe annotate a sample of 360 question-answer pairs to characterize entity saliency in CIS.\nWe propose and evaluate two methods for improving the answers given by the search system: rewriting the answer with inline definitions of salient entities and prompting the user with a follow-up question to allowing them to learn more about salient entities.\nWe extensively analyze the feedback on answer rewrite type preference and identify patterns that can help motivate future research.\nWe perform an initial exploration of addressing the same task using a state-of-the-art LLM and provide anecdotal evidence for the need for more controlled generation approaches, thereby solidifying the case for the type of methods this paper is proposing.\nAll resources developed within this paper, including the acquired dataset of salient entities and crowdsourcing annotations are made available at https://github.com/isekulic/chiir24-answer-rewriting  ###reference_-rewriting###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related work",
            "text": "We highlight relevant research in the areas of CIS and entity-centric search. As the distinction between conversational search and conversational Q&A is blurred (Zamani et al., 2023  ###reference_b43###), we use CIS as an umbrella term."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Conversational Information Seeking",
            "text": "CIS has emerged as an increasingly popular method of retrieving information, including the information from the Web (Anand et al., 2020  ###reference_b4###).\nSeveral research directions have span from CIS, including conversational passage retrieval (e.g., TREC CAsT (Dalton\net al., 2019  ###reference_b11###)), conversational Q&A (e.g., QuAC (Choi et al., 2018  ###reference_b9###)), and mixed-initiative interactions (Zamani et al., 2023  ###reference_b43###).\nUnder the mixed-initiative paradigm, the system can at any point proactively take initiative and ask the user clarifying questions or offer suggestions.\nWhile mixed-initiative is a relatively well established concept in the IR community (Allen\net al., 1999  ###reference_b3###), recent advancements in CIS systems have demonstrated the effectiveness of asking clarifying questions with a goal of elucidating the underlying user’s information need (Aliannejadi et al., 2019  ###reference_b2###).\nWe take advantage of such opportunity and propose to rewrite the answer, offering follow-up to users, as discussed in Section 4  ###reference_###.\nSzpektor\net al. (2020  ###reference_b36###) proposed a dynamic composition-based model for conversational domain exploration (CODEX), which enables users to enrich their knowledge through interactions with the system.\nThey highlight several challenges, including maintaining an engaging experience, avoiding repetitions, and choosing the appropriate response length.\nWhile in this work we focus on ensuring user’s understanding of answers, some of the points we touch upon are related to the goal of user engagement and not burdening the user with long or repetitive definitions of salient entities.\nTo the best of our knowledge, response rewriting with the purpose of making sure a user understands the response to their question in CIS has not been explored.\nHowever, researchers have studied text rewriting in IR for personalization and text simplification.\nWhile text simplification has been shown to improve readability and understanding in medical (Leroy et al., 2013  ###reference_b25###) and scientific texts (Ermakova et al., 2022  ###reference_b13###), it is usually done by swapping relatively unfamiliar words with more common alternative words (Leroy et al., 2013  ###reference_b25###) or leveraging large-scale language models for complete rewriting of the text (Sheang and\nSaggion, 2021  ###reference_b35###).\nIn this setting, a certain degree of information distortion is acceptable, as the text rewritten with such methods might differ from the original due to word substitutions.\nOn the other hand, we aim to allow the user to learn about a topic of interest, thus retaining the original terminology."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Entities",
            "text": "Marchionini (2006  ###reference_b27###) categorizes search activities in two broad categories: look-up questions and exploratory search, with the latter requiring carefully curated user interaction (C mara et al., 2021  ###reference_b8###).\nOne of the most notable datasets in the space of web search is the Google Natural Questions dataset (Kwiatkowski\net al., 2019  ###reference_b24###), which contains queries from real users with manually evaluated responses.\nDuring their exploratory web search, users often have the possibility to learn about entities of their interest by following hyperlinks or reformulating their query based on newly seen entities (Eickhoff\net al., 2014  ###reference_b12###).\nEntity linking and entity-based search are core component in that process (Balog, 2018  ###reference_b6###).\nThus, significant research efforts were put into developing entity linking methods, including entity linking in the Web (Han et al., 2011  ###reference_b19###), in free texts (Piccinno and\nFerragina, 2014  ###reference_b31###), and in CIS (Joko and Hasibi, 2022  ###reference_b22###; Joko\net al., 2021  ###reference_b23###).\nWhile documents may contain a large number of entities, some of them are salient, thus central to modeling the aboutness of a document (Paranjpe, 2009  ###reference_b29###), and others are not.\nMoreover, Gamon\net al. (2013  ###reference_b16###) find that only about 5% of the entities in Web pages are salient, while others are often mentioned somewhat sporadically.\nThese salient entities are crucial for the user to be familiar with, in order to satisfy their information need.\nHowever, in shorter texts that contain fewer entities, this percentage is anticipated to be higher (Wu\net al., 2020  ###reference_b40###).\nAnswers in CIS are a prime example of such shorter texts.\nYet, research on entity salience in CIS is lacking, providing a strong motivation for this work.\nAnother aspect of entity salience we aim to explore is how important they are for the user’s understanding of the texts and readability (Collins-Thompson et al., 2011  ###reference_b10###).\nThere is an important distinction to be made between entity salience and entity relevance or entity importance (Gamon\net al., 2013  ###reference_b16###).\nFor example, Joe Biden is objectively an important entity, however, it can be marginal to the document’s topic.\nAs such, entity relevance is dependent on the user’s intent and their underlying information need.\nOn the other hand, an entity is salient to a document if it is central and important for the overall topical and informational coherency of the document.\nThus, we argue that salient entities are essential to know about for a complete understanding of the provided answers in CIS.\nIn this work, we explore their prevalence, characteristics, and ways of improving user experience via answer rewriting around identified salient entities."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Understanding Salient Entities in Conversational Information Seeking",
            "text": "In this section, we define salient entities in CIS and present several research questions.\nThen, we describe the dataset acquisition process with crowdsourcing.\nFinally, we showcase relevant aspects of the created dataset and analyze special cases of salience."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Problem statement",
            "text": "A salient entity captures the aboutness of the text and is thus central to the given document (Paranjpe, 2009  ###reference_b29###).\nIn CIS, answers to user’s questions are usually short, containing from a single to a few sentences with only a few entities present.\nIdentifying salient entities in such answers is thus imperative, as they are essential for the user’s understanding of the given answer.\nIn this work, we inspect the prevalence of entity salience in CIS.\nWe define entity saliency on a graded scale of 0 to 2, i.e., , with  being the th entity in an answer. A score of 0 corresponds to the entity not being salient at all and 2 to the entity being highly salient.\nIn this section, we aim to shed light on RQ1: What are characteristics of salient entities in CIS? We break this generic question into a series of more specific subquestions:\nHow prevalent are salient entities in answers in CIS?\nHow well do users agree on which entities are salient?\nIs there empirical evidence that the notion of entity salience is different in conversational answers than in documents?\nAre there entities that are salient, but do not require explicit definitions?"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Dataset Acquisition",
            "text": "In order to model entity salience in CIS, we extend QReCC (Anantha et al., 2021  ###reference_b5###)—an open-domain conversational question answering dataset containing 14k conversations.\nQReCC is curated from three well-established datasets: TREC CAsT 2019 (Dalton\net al., 2019  ###reference_b11###), Google Natural Questions (NQ) (Kwiatkowski\net al., 2019  ###reference_b24###), and QuAC (Choi et al., 2018  ###reference_b9###).\nTREC CAsT focuses on conversational passage retrieval, while QuAC resolves around conversational Q&A over a Wikipedia text.\nContrary, NQ is not conversational in its original form, but has been extended by using its queries as a basis for creating subsequent turns.\nExcerpts from QReCC with their original sources and saliency annotations are show in Table 1  ###reference_###.\nAll of the conversations in the three datasets have been normalized so that they contain multi-turn interactions with manually resolved utterances and manually checked responses.\nThis, together with its diversity, makes QReCC appropriate for our work on entity salience in CIS.\nIn this work, we provide a deep analysis of the dataset in terms of entity salience modeling and thus subsample the original QReCC dataset.\nWe restrict ourselves to the test portion of QReCC, as it contains utterances from all of the three aforementioned datasets.\nAdditionally, in order to annotate as many conversations as possible within reasonable cost, we restrict ourselves to the conversations up the depth of 3, thereby trading off conversation depth for higher breath coverage.\nWe employ an established entity linker, WAT (Piccinno and\nFerragina, 2014  ###reference_b31###), to extract entities from the system’s responses.\nAs suggested by the authors, we use a reasonable, slightly precision-oriented confidence threshold of  for extracting entities from texts.\nWe filter out the entities that appear in the question as well, assuming the user asking the question already knows about them.\nThis procedure results in an entity set , containing several entities extracted from the given answer , that do not appear in the question .\nNow that we have (question, answer, entity_set) triplets, we employ crowdsourcing to annotate which entities from the entity set can be considered salient.\nGiven the question and the answer, the task is to annotate the degree to which a given entity is considered essential for understanding the answer.\nAfter an initial analysis of the entities and their importance in understanding the answer, we opted for a graded relevance scale.\nWe adopt an annotation scenario where an entity can be either essential, important, or not important.\nWe draw the similarities of our annotation scheme with well-established graded relevance schemes in IR (Sakai, 2021  ###reference_b34###), where a document can fully satisfy a user’s information need, partially, or be irrelevant.\nWe define the following labels for an entity:\nKnowing about the entity is essential for understanding the answer to the question.\nIt is not possible to comprehend the answer without knowing about (being familiar with) the entity. This label corresponds to a salience score of 2.\nKnowing about the entity is important for a deeper and more complete understanding of the answer.\nHowever, it is not essential and the user can partially comprehend the answer without knowing about the entity. This label corresponds to a salience score of 1.\nThe entity is not important for understanding the answer to the question, nor does its knowledge benefit the user’s knowledge on the topic. This label corresponds to a salience score of 0.\nWe use Amazon Mechanical Turk111https://www.mturk.com as our annotation platform.\nAll workers are required to have at least 1,000 approved annotations with a minimum 95% overall approval rate and be based in the United States, in order to mitigate the potential language barrier for understanding the task.\nEach (question, answer, entity) triplet is annotated by five different workers.\nTo insure high quality annotations, we manually curate a test set of (question, answer, entity) triplets that the workers need to annotate correctly in order for their annotations to count towards the final dataset.\nThe size of the test set is 25% of the final dataset size.\nAdditionally, we track workers’ mouse clicks and discard annotations that are done recklessly and quickly.\nWorkers take on average  seconds per (question, answer, entity) triplet.\nTo ensure ethical use of crowd workforce, we provide an appropriate compensation of 0.20$ for 5 annotated entities, resulting in an average of 18$/h, which is over 250% of the minimum wage in the USA."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Analysis",
            "text": "In this section, we answer our research questions through an extensive analysis of the acquired dataset on entity salience in CIS."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1. Presence of Salient Entities.",
            "text": "###figure_2### In order to focus on answers with a certain level of complexity, we selected answers with at least 2 entities present (as extracted by WAT). This resulted in annotation of 120 QA pairs, containing more than 400 entities.\nEach (question, answer, entity) pair was assessed by five different workers, resulting in a total of over 2,000 annotations.\nIn the annotated dataset, there are on average  entities present in the answers.\nThe average salience of those entities, as assessed by crowd workers, is  (40% annotated with salience of 2, 53% with 1, and 7% with 0).\nIn response to RQ1.a, this means that there are more salient entities than non-salient ones in CIS answers.\nThis finding is further confirmed by averaging the saliency scores for each entity and computing the portion of salient ones (e.g., average saliency score ¿ ) over the total number of entities in the answer.\nThis ratio is , meaning that on average 63% of all entities in CIS can be considered salient entities.\nMoreover, we analyze salience throughout the conversation.\nFigure 2  ###reference_### shows examples of the development of an entity salience through three turns of the conversation.\nThe entity epilepsy is mentioned sporadically in the answer at turn 1, but becomes considerably more salient in the subsequent turn.\nOverall, we observe an average change of saliency score between two consecutive turns of , suggesting that an entity might become more or less essential as the focus of the conversation changes.\nEntities might be sporadically mentioned in earlier turns of the conversation, but with users’ further queries they can become central to the topic of the conversation."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2. Subjectivity in Assessing Entity Salience",
            "text": "To answer RQ1.b, we compute Fleiss’  (Fleiss, 1971  ###reference_b14###) to measure subjectivity of the annotators assessing the degree of saliency of an entity, i.e., how essential is the entity for a complete understanding the answer.\nThe computed  is , suggesting weak inter-annotator agreement and high subjectivity for the task (Viera\net al., 2005  ###reference_b38###).\nWe additionally compute Spearman’s rank correlation coefficient  between all pairs of workers that annotated a specific QA pair.\nWith this step, we try to assess potential subjectivity level that is due to different perception of scale of essential/important/unimportant entities.\nFor example, two workers might agree on which of the entities is more salient, while their perception of the saliency scale differs slightly.\nThe average Spearman’s  is 0.45, which suggests a fair agreement and thus a certain level of skewed score subjectivity, which is different from weak agreement measured by .\nOverall, we conclude that the task of assessing which entities are essential for answer understanding is highly subjective.\nThe subjectiveness may come from different user background knowledge, their perception of salience, but also from personalities.\nHowever, having labels collected from five different annotators allows for a robust assessment of entity salience.\nThe data suggests that there is a lot of potential for dealing with personal preferences and subjectivity when estimating entity salience."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3. Entity Salience in Documents vs. in CIS.",
            "text": "We hypothesized that the notion of entity salience is different in CIS than in Web documents.\nTo assess this hypothesis in the light of RQ1.c, we compute the entity salience score using a state-of-the-art model for salience prediction in documents, SWAT (Ponza\net al., 2019  ###reference_b32###).\nFor each QA pair, we compute Spearman’s  over the entities ranked by salience score from the dataset and the entities ranked by salience score as predicted by SWAT.\nThe computed  averages to 0.25, indicating low to moderate correlation.\nThis suggest that document-level salience prediction methods are not entirely fit for the task of entity salience identification in CIS.\nFurthermore, the prevalence of salient entities is significantly higher in CIS answers (63%), as opposed to Web documents (5%), as reported by Gamon\net al. (2013  ###reference_b16###)."
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4. Special Cases of Salient Entities",
            "text": "Another important finding of the analysis is the case that although most of the answers contain salient entities, which require user’s familiarity to comprehend the answer, not all such entities necessarily require definitions.\nTo answer RQ1.d, we take a random subsample of more than a hundred entities from the crowd-annotated answers for analysis with a goal of finding potential patterns.\nWe then perform expert annotation (done by one of the authors of the paper) by carefully inspecting entities in the context of a conversation and note whether they would potentially require explicit definitions or not.\nIn our analysis, several special cases of entities arose, which might not require further steps to be taken by the CIS system, even if deemed salient.\nTable 2  ###reference_### presents the described cases, with an example and their prevalence, as indicated by the percentage of such entities subsampled set.\nWe estimate that around  of the entities belong to one of the special cases and potentially do not require definitions, with the biggest category being common-sense knowledge entities."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Answer Rewriting",
            "text": "We have established that salient entities occur frequently in answers to CIS questions.\nIn this section, we aim at rewriting the answers containing salient entities with the goal to aid users’ understanding.\nTo this end, we propose two answer rewriting strategies, depicted in Figure 3  ###reference_###.\nThe first strategy aims to rewrite the original answer  by inserting inline definitions of the identified salient entities, thus making the answer self-contained.\nThe second strategy makes use of the mixed-initiative CIS paradigm and offers the user to learn more about any of the identified salient entities.\nFigure 3  ###reference_### showcases all rewrite types, further explained in the following sections.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Inline Entity Descriptions",
            "text": "Our first strategy towards ensuring the complete understanding of the answer is based on including the explanations of the identified salient entities in the answer itself.\nFormally, we rewrite the original answer  by providing inline definitions  for each of the salient entities , resulting in the answer rewrite A-inlinedef.\nThe answer A-inlinedef is thus self-contained, as all of the salient entities are explicitly described.\nOne of the challenges here is to keep the explanations reasonably short and adequate for a conversational setting, as explaining the answer with long definitions would result in a significantly longer answer than the original, thereby overwhelming the user.\nThus, we experiment with two alternatives for providing inline definitions."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Wikibase Entity Descriptions.",
            "text": "We utilize a knowledge base to extract definitions of salient entities.\nSpecifically, we consult Wikibase222https://www.mediawiki.org/wiki/Wikibase to retrieve the entry of given entity  and get its definition .\nTo construct the final rewritten answer A-inlinedef/wiki, we insert  in parentheses immediately after the first mention of  ( )."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Mixed-initiative Follow-up Prompt",
            "text": "Under the mixed-initiative paradigm in CIS, the system can at any point take initiative and prompt the user with various elicitation, clarification, or other questions (Allen\net al., 1999  ###reference_b3###; Zamani et al., 2023  ###reference_b43###).\nAs one of the potential limitations of the previously described approach is overwhelming the user with potentially unnecessary entity definitions, we instead ask the user whether they require the explanations of salient entities or not.\nTo this end, we experiment with two different follow-up prompts, described below."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. Follow-up Question.",
            "text": "The first type of follow-up we propose is a direct question, aimed at asking whether the user is familiar with the salient entities identified in the answer.\nTo construct a direct clarifying question, we construct a new answer A-followup/question by expanding the original answer  with a question “Do you want to learn more about , , or ?”, where  is in the top N most salient entities identified."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. Follow-up Offer.",
            "text": "Similarly, an offered follow-up prompt (A-followup/offer) is designed by expanding the original answer with “If you wish to learn more about , , or , feel free to ask.”\nWe hypothesize that this strategy offers several benefits over the inline explanation rewrites.\nFirst, the user can chose whether they want to learn about the identified salient entities or they are comfortable with moving on with the conversation (they either know enough about the entities or do not care).\nWe note that phrasing the follow-up prompt as a direct question, i.e., “Do you want to learn more about entity?” would require the direct answer from the user, potentially disrupting the conversation flow.\nInstead, our proposed construction of the prompt simply offers the user a possibility for expansion, enabling them to ignore it if they are not interested in learning about the proposed entities.\nSecond, we can learn about the user’s background knowledge by them choosing or not choosing to learn about the salient entities, leading to a potential for personalization of subsequent answers.\nThird, we encourage engagement with the user by providing potential topics to converse about.\nWhile these assumptions intuitively make sense, we formulate specific research questions to assess them empirically."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Evaluation of Answer Rewrite Strategies",
            "text": "In this section, we describe the human-based evaluation procedure for comparing the original answer with the rewritten answers."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1. Research Questions.",
            "text": "The main research question we aim to answer is RQ2: How to utilize salient entities for answer rewriting for an improved user experience?\nWe also aim to explore what type of rewritten answers users prefer and what methods work the best for generating such rewrites.\nThus, we extend our main research question to four more specific questions:\nDo users prefer the rewritten questions over the original ones?\nWhich of the two answer rewrite strategies (A-inlinedef or A-followup) is preferred?\nIs there a preferred way of explaining the salient entities inline (A-inlinedef/wiki or A-inlinedef/human)?\nIs there a preferred way of offering follow up to the user (A-followup/question or A-followup/offer)?\nHow does the number of salient entities considered in the rewrite (top 1, 2, or 3) affect user preferences?"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2. Experiment Design.",
            "text": "We design the evaluation study as a multiple choice inquiry and ask crowd workers to provide their assessments.\nGiven an initial question, the workers need to assume the role of a user and select the answer that they would prefer in an interaction with a conversational assistant.\nThe given options are threefold: an original answer, a rewritten answer with inline explanations, and a rewritten answer with a follow-up prompt in the end.\nFurthermore, to answer research questions RQ2.c and RQ2.d, we vary the methods for inline explanations, as well as the types of questions for the prompt-based rewrite.\nNote that crowd workers are not aware of those changes and they always have the three mentioned options, without knowing how the rewrites are generated.\nTo ensure consistency, we generate rewrites on the same pool of QA pairs, thus controlling the potential impact of different topics on the rewrite preference.\nEach question and three answer options, corresponding to original answer, an answer with inline definitions, and an answer with follow-up, is annotated by three different crowd workers.\nWe ensure the quality and consistency of the annotations by selecting high-quality workers, as described in Section 3.2  ###reference_###.\nMoreover, we randomize the order of A-original, A-inlinedef, and A-followup to reduce any potential position bias.\nIn order to gain further insights into the underlying rationales, we ask annotators to provide a brief explanation of on why they chose the answer they did.\nWe analyze the provided reasons in depth in the next section.\nTo additionally ensure high quality annotations, we manually inspect all of them, rejecting crowd workers who carelessly provided nonsensical reasons (e.g., “first one,” “best text,” or simply copy-pasted parts of the answers), and blocking them from further participation in the study.\nIn total, we acquire more than 600 assessments on rewrite type preference with justifications for the choice."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Results",
            "text": "In this section, we present the results of the crowdsourcing study on answer rewrite type preference and analyze them in the light of the aforementioned research questions."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1. Original or Rewritten Answer Preference",
            "text": "Table 3  ###reference_### presents the results of the different combinations of answer rewrites, as explained in Section 4.3  ###reference_###.\nTo assess whether differences in answer preference are statistically significant, we perform a  test under the null hypothesis of data being drawn from a uniform probability distribution across the three rewrites (i.e., each row of the table).\nIn response to RQ2.a, we observe a preference for one of the answer rewrites, over the original answer (222 for original vs 399 for rewrites, p-value ).\nThese results suggest that there is a large potential for improving the user experience through answer rewriting.\nMoreover, the findings suggest a promising direction for further research on answer rewriting in CIS systems, both by providing further inline explanations of certain entities and by offering follow-up clarifications."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2. Rewrite Type Preference",
            "text": "Regarding RQ2.b, we observe a preference for answers with inline explanations (A-inlinedef) over the answers with a follow up (A-followup).\nMoreover, as indicated in Table 3  ###reference_###, this preference is prevalent across all combinations of rewrite subtypes.\nAlthough not all combinations in Table 3  ###reference_### yield statistically significant differences, the overall trend is prevalent across all of the experiments.\nThis suggests that making the answer self-contained by providing inline entity explanations is more desirable than offering the user to clarify these entities.\nContrary to our hypothesis, longer answers obtained by inserting entity descriptions do not seem to overwhelm the majority of the users.\nHowever, subjectivity is still important in this scenario, as some users indeed find A-inlinedef to be too cluttered, as discussed in the next section."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3. Rewrite Subtype Preference",
            "text": "To address research questions RQ2.c and RQ2.d, we aggregate the results of different subtypes of answer rewriting.\nExperiments indicate humanly-curated answer to be slightly more preferred over the Wikbase definitions in parentheses (131 vs. 119), suggesting that more natural rewrites could better help the user understand the answers.\nThis finding is a motivation for the development of answer rewriting methods aimed at defining entities in a more natural manner, compared to entity definitions being inserted into parentheses.\nSimilarly, A-followup/offer is slightly more preferred than A-followup/question (81 vs. 68).\nWe hypothesize that a prompt that could be ignored, as opposed to a direct question, would benefit the overall user experience.\nWhile both strategies are equally effective in providing the user with desired information, A-followup/offer might not impair the flow of the conversation, as it can be ignored if the user does not desire to learn more about proposed entities."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4. Result Preference by Top N Entities",
            "text": "Regarding RQ2.e, we report the results on answer preference across top  most salient entities rewritten in Table 4  ###reference_###.\nSpecifically, we construct the experiment such that the same original answer is rewritten three times, each time with  salient entities taken into account, with .\nResults suggest that the higher the , i.e., the more entities are defined in the answer, the stronger the user’s preference for A-inlinedef.\nWe hypothesize that such answers provide a more complete response to the given question, thus not requiring further explorations of the topic through clarifying prompts."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5. Analysis",
            "text": "In order to gain further insight into answer rewrite preferences, we manually analyze responses from crowd workers.\nRecall that workers were asked to justify why they would prefer the answer rewrite they have chosen. We perform a qualitative analysis of the provided reasons by identifying re-occurring reasons for workers’ choices.\nWe find five distinct patterns of the provided reasons, presented in Table 5  ###reference_###.\nTo estimate the prevalence of each of these, we randomly select 100 responses from the crowd workers and label them using these pattern.\nIn our analysis, we allow for multiple patterns to be associated with a single reason of choice, as for example worker can pick an answer because it is both concise and natural.\nThe patterns and their frequency in the analyzed set are shown in Table 5  ###reference_###.\nOverall, we find that:\nUsers prefer the original answer  mostly because it is short, concise, and does not introduce unnecessary clutter.\nUsers prefer the answer with inline explanations A-inlinedef when they appreciate additional information and think it provides a well-defined answer.\nUsers prefer the answer with a follow-up prompt A-followup when they think it is the most welcoming of the different answer alternatives, inviting for further conversation, but is not unnecessarily overwhelming with long explanations of entities they perhaps do not require explanation of.\nAlthough all of the workers provided reasonable justifications for their selection, the inter-annotator agreement, as measured by Fleiss’ , is , indicating high subjectivity.\nFrom the conducted experiments and observed patters in user’s preferences, we find solid evidence that entity-based answer rewriting can lead to an improved conversational user experience. At the same time, we also find that the choice of preferred answer format is highly subjective, which calls for further research on the personalization of such approaches."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "4.6. LLM-based Answer Rewriting",
            "text": "While entity definitions are currently either taken from a knowledge base or curated by human intervention, the question naturally arises: Could this task not be tackled in an end-to-end manner by a large language model (LLM)?\nGiven promising advancements in text simplification, e.g., with T5 (Sheang and\nSaggion, 2021  ###reference_b35###), and the broad variety of knowledge and language capabilities demonstrated by ChatGPT, there are reasons to believe that the answer rewriting task could be performed by simply engineering the “right” prompt. Below, we present some anecdotal evidence to the contrary, by presenting results obtained with a state-of-the-art LLM, gpt-3.5-turbo (Brown\net al., 2020  ###reference_b7###).\nSpecifically, we consider two types of prompts: (1) instructing the LLM to rewrite the answer for easier understandability and (2) additionally, including the specific entities that need to be explained. For both settings, we experimented with both zero- and few-shot prompts. Due to space constraints, we only include a few examples in Table 6  ###reference_### to illustrative the main limitations we identified:\nFailure to identify salient entities: When entities that require explanations aren’t explicitly stated, the LLM can simply reword the answer, without providing any additional information.\nKnowledge distortion: Certain salient entities are removed from the original answer, causing the loss of information by oversimplifying the text.\nAnswer length: when explicitly stated which entities require inline explanations, the LLM tends to significantly lengthen the original answer (from 1-2 to 5-6 sentences).\nInconsistency: Although hallucination is a known issue in LLMs (Ji et al., 2023  ###reference_b21###), we also observe inconsistency, i.e., a high degree of variation in answer quality, when generating answers to the same prompt multiple times (controllable with parameters to some degree) and across different examples (not controllable).\nThat said, LLMs can also generate appropriate rewrites, as illustrated by the last example in Table 6  ###reference_###, which is both concise and natural, while still covering all of the salient entities.\nHowever, it is evident that the salient entities had to be explicitly stated and that prompts need to be carefully engineered for the desired outcome.\nThe main take home message of our study is that entity-based answer rewriting can improve the user experience, but to unlock its full potential, the identification of salient entities as well as the preferred form of answer rewrite need to be addressed in a personalized manner. These parts require future research. When it comes to the actual generation of the rewritten answer, there is a large potential for utilizing LLMs, provided that they are prompted with the specific entities and the desired format of rewrite."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "4.7. Discussion",
            "text": "Ours is a novel task in a conversational setting, which makes evaluation inherently challenging.\nIn this section, we reflect on some of the design decisions, acknowledge limitations, and highlight possible future research directions, including potentially revisiting some of the design choices."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Impact of Rewriting on Answer Length",
            "text": "Rewriting answers in CIS by inserting inline definitions of salient entities lengthens the original answer.\nAs observed in our experiments, up to three entity definitions do not seem to hurt the answer rewrite, as such rewrite was often chosen by the crowd workers.\nHowever, in case the answer becomes too long due to a large number of salient entities, the amount of them we provide definitions for can be reduced by taking only the top N entities, as ordered by the salience scores."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Text- vs. Voice-based CIS",
            "text": "We hypothesize that results on answer rewrite preference might differ in a voice-only setting, as the user is not able to skim through potentially unnecessary parts of the answer.\nAs such, preference for inline definitions might not be so prevalent, as users could not simply skim through the text and would in fact need to listen to the extended answers.\nWe aim to explore the aforementioned questions in further research."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "More Realistic Conversational Setting",
            "text": "Design-wise, we compare answer rewrites turn by turn, rather than evaluating the whole conversations.\nThis is often the case in crowdsourcing-based studies due to the limited availability of users, although recent research points out the benefits of multi-turn dialogue evaluation (Li\net al., 2019  ###reference_b26###).\nAt the same time, utterances in our study are self-contained and do not necessarily require full conversation history for correct assessments.\nAlso, we provide an analysis of the salient entity evolution throughout the conversation.\nNevertheless, as part of our future work, we aim to build multiple CIS systems based on answer rewrite type (e.g., a system that generates answers with inline explanations of salient entities and a system that offers follow-up prompts) and perform a thorough user study to validate the findings of this work."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Salient Entity Annotation",
            "text": "While other automated options for extracting salient entities exist, we opted for annotating salience through crowdsourcing with a goal of acquiring high-quality data. Nevertheless, despite having multiple controls in place for ensuring quality (from the selection of crowd workers to using test questions), the inter-annotator agreement turned out to be relatively low.\nWe attribute this to the high subjectivity of the task, as workers’ perception of what is “essential to understand” might differ, in relation to their personal knowledge and their understanding of what “essential” means. We acknowledge the possibility of the annotation task being set up this way to be too open for interpretations, or simply too hard, thus leading to low inter-annotator agreement. In the future we plan to repeat the annotation process as part of a dedicated study, aiming to untangle what role prerequisite knowledge and subjectivity might play here. Nonetheless, we believe the findings of this study on question rewrite strategies and preferences to be sound and useful for the research community."
        },
        {
            "section_id": "4.7.x",
            "parent_section_id": "4.7",
            "section_name": "Role of Background Knowledge",
            "text": "High subjectivity of rewrite preferences potentially comes from different backgrounds and personal preferences of different users.\nThus, having information about the background of the user would help the system tailor the rewrite to a specific user, both by selecting only entities which the user needs explanation for and by adjusting the style of the rewrite.\nFuture work therefore includes personalized answer rewrites."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusions",
            "text": "In this study, we analyzed the presence of salient entities in conversational information seeking interactions.\nWe found that most of the answers generated by the search system contain some amount of salient entities, required for the complete comprehension of the answer.\nMoreover, with a goal of ensuring that the user understands these answers, we proposed two strategies for answer rewriting.\nThe first one is based on providing inline definitions of salient entities, while the second one explicitly offers the user to learn more about the entities they might be unfamiliar with.\nThe suggested methods were extensively assessed through human-based evaluation, indicating user preference for answers with inline definitions, over the follow-up prompt-based rewrites.\nWe hope that these findings provide a strong motivation for further research on entity-based answer rewriting."
        }
    ]
}