{
    "title": "Silencing the Risk, Not the Whistle: A Semi-automated Text Sanitization Tool for Mitigating the Risk of Whistleblower Re-Identification",
    "abstract": "Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU Whistleblower Directive, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other labels of named entities) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including the whistleblower’s writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We\nevaluate our tool’s effectiveness\nusing\ncourt cases from the European Court of Human Rights (ECHR) and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution attacks and utility\nloss statistically using the popular IMDb62 movie reviews dataset, which consists of 62 individuals. Our method can significantly reduce authorship attribution accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content’s semantics, as measured by the established cosine similarity of sentence embeddings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "In recent years, whistleblowers have become “a powerful force” for transparency and accountability, not just in the field of AI (Crawford et al., 2019  ###reference_b10###), but also in other technological domains and across both private- and public-sector organizations. Institutions such as the AI Now Institute (Crawford et al., 2019  ###reference_b10###) or the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, 2019  ###reference_b23###) have emphasized the key role of whistleblower protection for societal well-being and often also the organizations’ own interests (Hauser et al., 2019  ###reference_b22###).\nHowever, whistleblowing may be a threat for the organizations whose malfeasance is being revealed; thus (potential) whistleblowers often fear or face retaliation. Computationally-supported anonymous reporting seems to be a way forward, but even if reporting frameworks are sufficiently secure system- and network-wise, the report itself may allow inferences towards the whistleblower’s identity due to its content and the whistleblower’s writing style. Non-partisan organizations such as Whistleblower-Netzwerk e.V. (WBN) provide guidance on concise writing. Our interactions with WBN confirm that whistleblower testimonies often include unnecessary personal details.\nExisting approaches modifying the texts of such reports appear promising, but they take an overly limited view of anonymity and – like whistleblower protection laws – address only parts of the problem. This is detailed in Section 2  ###reference_###.\nTo improve on these approaches, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that puts the whistleblower into the loop of assessing risk and utility.\nOur contributions are threefold.\nFirst (Section 3  ###reference_###), we analyse the interleaved contributions of different types of identifiers in texts to derive a description of the problem for anonymous whistleblowing in terms of a trade-off between risk (identifiability of the whistleblower) and utility (of the rewritten text retaining sufficient information on the specific event details).\nWe derive a strategy for assigning re-identification risk levels of concern to textual features composed of an automated mapping and an interactive adjustment of concern levels.\nSecond (Section 4  ###reference_###), we describe our toolwhich implements this strategy. It applies (i) the word/term-to-concern mapping using natural language processing to produce a sanitized but possibly ungrammatical intermediate text version, (ii) a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral, and (iii) interactivity to draw on the user’s context knowledge.\nThird (Section 5  ###reference_###), we evaluate the resulting\nrisk-utility trade-off. We measure the protection against authorship attribution attacks and utility loss statistically using an established\nbenchmark dataset and show that it can significantly reduce authorship attribution accuracy while retaining utility. We also evaluate our our tool’s effectiveness in masking direct and quasi-identifiers using the Text Anonymization Benchmark (Pilán et al., 2022  ###reference_b49###) and demonstrate its effectiveness on excerpts from a real-world whistleblower testimony. Section 6  ###reference_### sketches current limitations and future work. Section 7  ###reference_### describes ethical considerations and researchers’ positionality, and it discusses possible adverse impacts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Background and Related Work",
            "text": "This section describes the importance of, and threats to, whistleblowing (Section 2.1  ###reference_###) and\nthe promises and conceptual and practical challenges of “anonymity” in reporting (Section 2.2  ###reference_###). We survey related work on the anonymization/de-identification of text and argue why it falls short in supporting whistleblowing (Section 2.3  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Challenges of Safeguarding Whistleblowers",
            "text": "Whistleblowers play a crucial role in exposing wrongdoings like injustice, corruption, and discrimination in organizations (Near and Miceli, 1985  ###reference_b42###; Bosua et al., 2014  ###reference_b7###). However, their courageous acts often lead to negative consequences, such as subtle harassment and rumors, job loss and blacklisting, and, in extreme cases, even death threats (Martin, 2003  ###reference_b35###; Sawyer et al., 2010  ###reference_b59###; McGlynn III and Richardson, 2014  ###reference_b38###). In Western nations, whistleblowing is largely viewed as beneficial to society (Van Portfliet and Kenny, 2022  ###reference_b67###), leading to protective laws like the US Sarbanes-Oxley Act of 2002 and the European Union’s “Whistleblowing Directive” (Directive 2019/1937). The latter, for example, mandates the establishment of safe reporting channels and protection against retaliation. It also requires EU member states to provide whistleblowers with legal, financial, and psychological support. However, the directive faces criticism for its limitations. Notably, it does not cover all public-sector entities (Terracol, 2019  ###reference_b64###, p. 3) and leaves key decisions to member states’ discretion (Abazi, 2020  ###reference_b2###, p. 652). This discretion extends to the absence of mandatory anonymous reporting channels and permits states to disregard cases they consider “clearly minor”, leaving whistleblowers without comprehensive protection for non-material harms like workplace bullying (Terracol, 2019  ###reference_b64###, p. 3). Furthermore, according to White (2018  ###reference_b71###), the directive’s sectoral approach and reliance on a list of specific EU laws causes a patchwork of provisions, creating a complex and possibly confusing legal environment, particularly for those sectors impacting human rights and life-and-death situations.\nLast but not least, organizations often react negatively to whistleblowing due to the stigma of errors, even though recognizing these mistakes would be key to building a culture of responsibility (Berendt and Schiffner, 2022  ###reference_b6###, p. 12) and improving organizations and society (Weingardt, 2004  ###reference_b70###). The reality for whistleblowers is thus fraught with challenges, from navigating legal uncertainties to dealing with public perception (Sachdeva and Chaudhary, 2022  ###reference_b53###; Leite, 2021  ###reference_b27###; Saade, 2023  ###reference_b52###), leaving many whistleblowers with no option but to report their findings anonymously (Rothschild and Miethe, 1999  ###reference_b51###). However, “anonymous” reporting channels alone do not guarantee anonymity (Berendt and Schiffner, 2022  ###reference_b6###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Anonymity, (De-)anonymization, and (De-/Re-)Identification",
            "text": "Anonymity is not an alternative between being identified uniquely or not at all, but “the state of being not identifiable within a set of subjects [with potentially the same attributes], the anonymity set” (Pfitzmann and Hansen, 2005  ###reference_b47###, p.9). Of the manifold possible approaches towards this goal, state-of-the-art whistleblowing-support software as well as legal protections (where existing) focus on\nanonymous communications (Berendt and Schiffner, 2022  ###reference_b6###). This, however, does not guarantee anonymous reports. Instead, a whistleblower’s anonymity may still be at risk due to several factors, including: (i) surveillance technology, such as browser cookies, security mechanisms otherwise useful to prevent unauthenticated uses, cameras, or access logs, (ii) the author’s unique writing style, and (iii) the specific content of the message (Marcum and Young, 2019  ###reference_b34###). Berendt and Schiffner (2022  ###reference_b6###) refer to the latter as “epistemic non-anonymizability”,\ni.e., the risk of being identified based on the unique information in a report, particularly when the information is known to only a few individuals. In some cases, this may identify the whistleblower uniquely.\nTerms and their understanding in the domain of anonymity vary. We use the following nomenclature:\nanonymization is a modification of data that increases the size of the anonymity set of the person (or other entity) of interest; conversely, de-anonymization decreases it (to some number . De-anonymization to , which includes the provision of an identifier (e.g., a proper name), is called re-identification. The removal of some identifying information (e.g., proper names), called de-identification, often but not necessarily leads to anonymization (Ben Cheikh Larbi et al., 2022  ###reference_b5###; Weggenmann and Kerschbaum, 2018  ###reference_b69###).\nIn structured data, direct identifiers (e.g., names or social security numbers) are unique to an individual, whereas quasi-identifiers like age, gender, or zip code, though not unique on their own, can be combined to form unique patterns. Established mathematical frameworks for quantifying anonymity, such as Differential Privacy (DP) (Dwork, 2006  ###reference_b17###), and metrics such as k-anonymity (Samarati and Sweeney, 1998  ###reference_b54###), along with their refinements (Machanavajjhala et al., 2007  ###reference_b32###; Li et al., 2006  ###reference_b28###), can be used when anonymizing datasets.\nUnstructured data such as text, which constitutes a vast majority of the world’s data, requires its own safeguarding methods, which fall into two broader categories (Lison et al., 2021  ###reference_b29###). The first, NLP-based text sanitization, focuses on linguistic patterns to reduce (re-)identification risk. The second, privacy-preserving data publishing (PPDP), involves methods like noise addition or generalization to comply with pre-defined privacy requirements (Domingo-Ferrer et al., 2016  ###reference_b16###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Related Work: Text De-Identification and Anonymization, Privacy Models, and Adversarial Stylometry",
            "text": "De-identification methods in text sanitization mask identifiers,\nprimarily using named entity recognition (NER) techniques. These methods, largely domain-specific, have been particularly influential in clinical data de-identification, as evidenced, for instance, by the 2014 i2b2/UTHealth shared task (Stubbs et al., 2015  ###reference_b63###). However, they do not or only partially address the risk of indirect re-identification (Mehta et al., 2019  ###reference_b39###; Ben Cheikh Larbi et al., 2022  ###reference_b5###). For example, Sánchez et al. (2012  ###reference_b56###, 2013  ###reference_b57###, 2014  ###reference_b58###) make the simplifying assumption that replacing noun phrases which are rare in domain-specific corpora or on the web with more general ones offers sufficient protection. Others use recurrent neural networks (Dernoncourt et al., 2017  ###reference_b13###; Liu et al., 2017  ###reference_b31###), reinforcement learning (Xu et al., 2019  ###reference_b72###), support vector machines (Uzuner et al., 2008  ###reference_b66###), or pre-trained language models (Johnson et al., 2020  ###reference_b24###) to identify and remove entities that fall into pre-defined categories. However, all of these approaches ignore or significantly underestimate the actual risks of context-based re-identification.\nMore advanced anonymization methods, in turn, also aim to detect and remove identifiers that do not fit into the usual categories of named entities or are hidden within context. For example, Reddy and Knight (2016  ###reference_b50###) detect and obfuscate gender, and Adams et al. (2019  ###reference_b3###) introduce a human-annotated multilingual corpus containing 24 entity types and a pipeline consisting of NER and co-reference resolution to mask these entities. In a more nuanced approach, Papadopoulou et al. (2022  ###reference_b45###) developed a “privacy-enhanced entity recognizer” that identifies 240 Wikidata properties linked to personal identification. Their approach includes three key measures to evaluate if a noun phrase needs to be masked or replaced by a more general one (Olstad et al., 2023  ###reference_b44###). The first measure uses RoBERTa (Liu et al., 2019  ###reference_b30###) to assess how “surprising” an entity is in its context, assuming that more unique entities carry higher privacy risks. The second measure checks if web search results for entity combinations mention the individual in question, indicating potential re-identification risk. Lastly, they use a classifier trained with the Text Anonymization Benchmark (TAB) corpus (Pilán et al., 2022  ###reference_b49###) to predict masking needs based on human annotations.\nKleinberg et al.  ###reference_b25###’s (Kleinberg et al., 2022  ###reference_b25###) “Textwash” employs the BERT model, fine-tuned on a dataset of 3717 articles from the British National Corpus, Enron emails, and Wikipedia. The dataset was annotated with\nentity tags such as “PERSON_FIRSTNAME”, “LOCATION”, and an “OTHER_IDENTIFYING_ATTRIBUTE” category for indirect re-identification risks, along with a “NONE” category for tokens that are non-re-identifying. A quantitative evaluation (0.93 F1 score for detection accuracy, minimal utility loss in sentiment analysis, and part-of-speech tagging) and its qualitative assessment (82% / 98% success in anonymizing famous / semi-famous individuals) show promise. However, the more recent gpt-3.5-turbo can re-identify 72.6% of the celebrities from Textwash’s qualitative study on the first attempt, highlighting the evolving complexity of mitigating the risk of re-identification in texts (Patsakis and Lykousas, 2023  ###reference_b46###).\nIn PPDP, several privacy models for structured data have been adapted for privacy guarantees in text. While most are theoretical (Lison et al., 2021  ###reference_b29###), “C-sanitise” (Sánchez and Batet, 2016  ###reference_b55###) determines the disclosure risk of a certain term t on a set of entities to protect (C), given background knowledge K, which by default is the probability of an entity co-occurring with a term t in the web. Additionally, DP techniques have been adapted to text, either for generating synthetic texts (Fernandes et al., 2019  ###reference_b21###) or for obscuring authorship in text documents (Weggenmann and Kerschbaum, 2018  ###reference_b69###). This involves converting text into word embeddings, altering these vectors with DP techniques, and then realigning them to the nearest words in the embedding model (Yue et al., 2021  ###reference_b74###; Zhao and Chen, 2022  ###reference_b75###). However, “word-level differential privacy” (Mattern et al., 2022  ###reference_b36###) faces challenges: it maintains the original sentence length, limiting variation, and can cause grammatical errors, such as replacing nouns with unrelated adjectives, due to not considering word types.\nAuthorship attribution (AA) systems use stylistic features such as vocabulary, syntax, and grammar to identify an author. State-of-the-art approaches involve using Support Vector Machines (Yadav et al., 2020  ###reference_b73###; Tyo et al., 2022  ###reference_b65###), and more recently, fine-tuned LLMs like BertAA (Fabien et al., 2020  ###reference_b19###; Altakrori et al., 2021  ###reference_b4###; Tyo et al., 2022  ###reference_b65###). The “Valla” benchmark and software package\nstandardizes evaluation methods and includes fifteen diverse datasets (Tyo et al., 2022  ###reference_b65###). Contrasting this, adversarial stylometry modifies an author’s writing style to reduce AA systems’ effectiveness (Stuart et al., 2013  ###reference_b62###). Advancements in machine translation (Wang et al., 2021  ###reference_b68###) have also introduced new methods based on adversarial training (Shetty et al., 2018  ###reference_b61###), though they sometimes struggle with preserving the original text’s meaning. Semi-automated tools, such as “Anonymouth” (McDonald et al., 2012  ###reference_b37###), propose modifications for anonymity in a user’s writing, requiring a significant corpus of the user’s own texts. Moreover, recent advances in automatic paraphrasing using fine-tuned LLMs demonstrated a notable reduction in authorship attribution, but primarily for shorter texts (Mattern et al., 2022  ###reference_b36###).\nTo the best of our knowledge, there is no – and maybe there can be no – complete list of textual features contributing to the re-identification of individuals in text.\nAs Narayanan and Shmatikov (2010  ###reference_b41###) highlight, “any attribute can be identifying in combination with others” [p. 3]. In text, we encounter elements like characters, words, and phrases, each carrying varying levels of meaning (Feldman and Sanger, 2007  ###reference_b20###). Single words convey explicit lexical meaning as defined by a vocabulary (e.g. “employee”), while multiple words are bound by syntactic rules to express more complex thoughts implicitly in phrases (“youngest employee”) and sentences (“She is the youngest employee”).\nIn addition, the European Data Protection Supervisor  and Spanish Data Protection Agency (AEPD)(2021) (EDPS  ###reference_b18###) state that anonymization can never be fully automated and needs to be “tailored to the nature, scope, context and purposes of processing as well as the risks of varying likelihood and severity for the rights and freedoms of natural persons” [p. 7].\nTo take these insights and limitations into account, our semi-automated text sanitization tool leverages insights on the removal of identifying information but involves the whistleblower (the user) in the decision-making process."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Risk modelling and risk mitigation approach",
            "text": "In this section, we derive the problem statement (Section 3.2  ###reference_###) from an analysis of different identifier types (Section 3.1  ###reference_###). Following an overview of our approach (Section 3.3  ###reference_###), we detail the anonymization operations for textual features (Section 3.4  ###reference_###) and the automatic assignment of default concern levels (Section 3.5  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Identifier Types, Author Identifiability, and Event Details in the Whistleblowing Setting",
            "text": "Whistleblowing reports convey information about persons, locations, and other entities. At least some of them need to be identified in order for the report to make any sense.\nThe following fictitious example consists of three possible versions of a report in order to illustrate how different types of identifiers may contribute to the re-identification of the anonymously reporting employee Jane Doe, a member of the Colours and Lacquer group in the company COLOURIFICS.\nOn 24 January 2023, John Smith poured polyurethane resin into the clover-leaf-shaped sink of room R23.\nAfter our group meeting on the fourth Tuesday of January 2023, the head of the Colours and Lacquer Group poured a toxin into the sink of room R23.\nSomebody poured a liquid into a recepticle on some date in a room of the company.\nIn V1, “John Smith” is the lexical identifier111The classification of identifiers is due to Phillips (Phillips, 2004  ###reference_b48###). Note that all types of identifiers can give rise to personal data.. in the sense of the EU’s General Data Protection Regulation (GDPR), Article 4(1): “any information which is related to an identified or identifiable natural person”, or personally identifiable data in the senses used in different US regulations. See\n(de Sousa Costa and de Castro Ruivo, 2020  ###reference_b12###) for legal aspects in the\ncontext of whistleblowing.\nof the COLOURIFICS manager John Smith, as is “24 January 2023” of that date. Like John Smith, room R23 is a unique named entity in the context of the company and also identified lexically. “Polyurethane resin” is the lexical identifiers of a toxin (both are common nouns rather than names of individual instances of their category). The “clover-leaf-shaped” serves as a descriptive identifier of the sink. In V2, John Smith is still identifiable via the descriptive identifier “head of the Colours and Lacquer Group”, at least on 24 January 2023 (reconstructed with the help of a calendar and COLOURIFIC’s personnel files). “Our” group meeting is an indexical identifier that signals that the whistleblower is one of the, say five employees in the Colours and Lacquer Group.\nThe indexical information is explicit in V2 given the background knowledge that only employees in this group were co-present (for example, in the company’s key-card logfiles). The same information may be implicit in V1 (if it can be seen from the company’s organigram who John Smith is and who works in his group).\nBoth versions provide for the inference that Jane Doe or any of her four colleagues must have been the whistleblower. If, in addition, only Jane Doe stayed behind “after the meeting”, that detail in V2 descriptively identifies her uniquely222If John Smith knows that only she observed him, she is also uniquely identified in V1, but for the sake of the analysis, we assume that only recorded data/text constitute the available knowledge..\nV3 contains only identifiers of very general categories. Many other variants are possible (for example, referencing, in a V4, “the head of our group”, which would enlarge the search space to all groups that had a meeting in R23 that day).\nThe example illustrates the threats (i)-(iii) of Section 2.2  ###reference_###. It also shows\nthat the whistleblower’s “anonymity” (or lack thereof) is only one aspect of a more general and graded picture of who and what can be identified directly, indirectly, or not at all – and what this implies for the whistleblower’s safety as well as for the report’s effectiveness.\nInspired by Domingo-Ferrer’s (Domingo-Ferrer, 2007  ###reference_b15###) three types of (data) privacy, we distinguish between the identifiability of the whistleblower Jane Doe (author\n333We assume that the potential whistleblower is also the author of the report. This is the standard setting. Modifications for the situation in which a trusted third party writes the report on their behalf are the subject of future work.\nidentifiability ) and descriptions of the event or other wrongdoing, including other actors (event details ).\nGiven the stated context knowledge, we obtain an anonymity set of size  for John Smith in V1 and V2. Jane Doe is in an anonymity set of size  or even  in V2. In V1, that set may be of size  (if people routinely work only within their group) or larger (if they may also join other groups). Thus, the presence of a name does not necessarily entail a larger risk.\nBoth are in an anonymity set containing all the company’s employees at the reported date in V3 (assuming no outsiders have access to company premises). The toxin and the sink may be in a smaller anonymity set in V1 than in V2 or V3, and they could increase further (for example, if only certain employees have access to certain substances). Importantly, the identifiability of people and other entities in  can increase the identifiability of the whistleblower.\nV3 illustrates a further challenge: the misspelled receptacle may be a typical error of a specific employee, and the incorrect placement of the temporal before the spatial information suggests that the writer may be a German or Dutch native speaker. In addition to errors, also correct variants carry information that stylometry can use for authorship attribution, which obviously can have a large effect on .\nThe whistleblower would, on the one hand, want to reduce all such identifiabilities as much as possible. On the other hand, the extreme generalization of V3 creates a meaningless report that neither the company nor a court would follow up on. This general problem can be framed in terms of risk and utility, which will be described next."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. The Whistleblowing Text-Writing Problem: Risk, Utility, And Many Unknowns",
            "text": "A potential whistleblower faces the following problem: “make  as small as possible while retaining as much  as necessary”. We propose to address this problem by examining the text and possibly rewriting it.\nIn principle, this is an instance of the oft-claimed trade-off between privacy (or other risk) and utility. In a simple world of known repositories of structured data, one could aim at determining the identifying problem (e.g., by database joins to identify the whistleblower due to some attributive information they reveal about themselves and by multiple joins for dependencies such as managers and teams) and compute how large the resulting anonymity set (or  as its inverse) is. Given a well-defined measure of information utility, different points on the trade-off curve would then be well-defined and automatically derivable solutions to a mathematical optimization problem.\nHowever, texts offer a myriad of ways to express a given relational information. The space of information that could be cross-referenced, sometimes in multiple steps, is huge and often unknown to the individual. Consequently, in many cases, it is not possible to determine the anonymity set size with any mathematical certainty. In addition, setting a threshold could be dangerous: even if the anonymity set is , protection is not guaranteed – for example, the whole department of five people could be fired in retaliation.\nAt the same time, exactly how specific a re-written text\nneeds to be about  and  in order to make the report legally viable\n444“a situation in which a plan, contract, or proposal is able to be legally enforced”, https://ludwig.guru/s/legally+viable  ###reference_###, retrieved 2024-01-02 cannot be decided without much more context knowledge. For example, the shape of the sink into which a toxic substance is poured probably makes no difference to the illegality, whereas the identity of the substance may affect it.\nThese unknowns have repercussions both for tool design (Section 3.3  ###reference_###) and for evaluation design (Section 5.1.1  ###reference_.SSS1###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Risk Mitigation Approach and Tool Design: Overview",
            "text": "Potential whistleblowers would be ill-served by any fully automated tool that claims to be able to deliver a certain mathematically guaranteed anonymization. Instead, we propose to provide them with a semi-automated tool that does have some “anonymity-enhancing defaults” that illustrate with the concrete material how textual elements can be identifying and how they can be rendered less identifying. Our tool starts with the heuristic default assumption that identifiability is potentially always problematic and then lets the user steer our tool by specifying how “concerning” specific individual elements are and choosing, interactively, the treatment of each of them that appears to give the best combination of  and . By letting the author/user assign these final risk scores in the situated context of the evolving text, we enable them to draw on a maximum of implicit context knowledge.\nOur approach and tool proceed through several steps. We first determine typical textual elements that can constitute or be part of the different types of identifiers. As can be seen in Table 1  ###reference_###, most of them can affect  and .\nSince identification by name (or, by extension, pronouns that co-reference names) does not even need additional background knowledge and since individuals are more at risk than generics, we classify some textual features as “highly concerning”, others as having “medium concern”, and the remainder as “potentially concerning”.\nWe differentiate between two types of proper nouns. Some names refer to typical “named entities”, which include, in particular, specific people, places, and organizations, as well as individual dates and currency amounts. These pose particular person-identification risk in whistleblowing scenarios.555PERSON, GPE (region), LOC (location), EVENT, LAW, LANGUAGE, DATE, TIME, PERCENT, MONEY, QUANTITY, and ORDINAL\n“Other proper nouns”, such as titles of music pieces, books and artworks generally only pose medium risk. For stylometric features, we explicitly categorize out-of-vocabulary words, misspelled words, and words that are surprising given the overall topic of the text. Other low-level stylometric features, such as punctuation patterns, average word and sentence length, or word and phrase repetition, are not (and in many cases, such as with character n-gram pattern, cannot be (Lagutina et al., 2019  ###reference_b26###)) explicitly identified. Instead, we implicitly/indirectly account for them as a byproduct of the LLM-based rephrasing. For all other parts of speech, we propose to use replacement strategies based on data-anonymization operations that are proportional to the risk (Table 2  ###reference_###). Given the complexities of natural language and potential context information, the latter two operations are necessarily heuristic; thus, our tool applies the classification and the risk mitigation strategy as a default which can then be adapted by the user.\nOverview of the approach from identifier types to default risk.\n\n\n\n\n\n\n\nIdentifier Type\n\nTextual Feature\n\n\n\nDefault Risk\n\n\n\n\n\n\nLexical\n\nNames of named entities\n,\n\n\nHigh\n\n\n\n\nLexical\n\nOther proper nouns\n\n\n\nMedium\n\n\n\n\nIndexical\n\nPronouns\n,\n\n\nHigh\n\n\n\n\nDescriptive\n\nCommon nouns\n,()\n\n\nPotential\n\n\n\n\nDescriptive\n\nModifiers\n,()\n\n\nPotential\n\n\n\n\nDescriptive\n(via pragmatic inferences)\n\nOut-of-vocabulary wordsa\n, ()\n\n\nMedium\n\n\n\nMisspelled wordsa\n\n\n\nMedium\n\n\n\nSurprising wordsb\n\n\n\nMedium\n\n\n\nOther stylometric features\n\n\n\nN/Ac\naTreated as noun.\nbNouns or proper nouns.\ncNot explicitly specified. Indirectly accounted for through rephrasing.\nMitigation strategies based on assigned risk.\n\n\n\n\n\nLvC\nNaNEs\nOPNs\nCNs\nMods\nPNs\nOSFs\n\n\n\nHigh\nSuppr.\nSuppr.\nSuppr.\nSuppr.\nSuppr.\nPert.\n\nMedium\nPert.\nGenerl.\nGenerl.\nPert.\nSuppr.\nPert."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Anonymization Operations for Words and Phrases",
            "text": "In our sanitization pipeline, we conduct various token removal and replacement operations based on each token’s POS tag and its assigned level of concern (LvC), which can be “potentially concerning”, “medium concerning”, or “highly concerning”. Initially, we consider all common nouns, proper nouns, adjectives, adverbs, pronouns, and named entities666By this, we mean names of named entities, e.g. “Berlin” for GPE, but we use named entities instead for consistency with other literature. as potentially concerning. Should the user or our automatic LvC estimation (see subsection 3.5  ###reference_###) elevate the concern to either medium or high, we apply anonymization operations that are categorized into generalization, perturbation, and suppression. Specific implementation details are elaborated on in section 4  ###reference_###."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1. Generalization",
            "text": "The least severe type of operation targets common nouns and other proper nouns marked as medium concerning. We assume their specificity (not necessarily their general meaning) poses re-identification risks. Thus, more general terms can be used to preserve meaning while mitigating the risk of re-identification.\nCommon nouns like “car” are replaced with hypernyms from WordNet, such as “vehicle”.\nOther proper nouns become broader Wikidata terms, e.g. “political slogan” for “Make America Great Again”."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2. Perturbation",
            "text": "This applies to modifiers777The current version of our tool considers only adjectives and adverbs as modifiers.\nand named entities annotated as medium concerning. In this process, original words are retained but are assigned zero weight in the paraphrase generation, along with their synonyms and inflections. This approach relies on the LLM to either (a) find similar but non-synonymous replacement words or (b) completely rephrase the sentence to exclude these words. For example, “Microsoft, the giant tech company, …” could be paraphrased as “A leading corporation in the technology sector…”."
        },
        {
            "section_id": "3.4.3",
            "parent_section_id": "3.4",
            "section_name": "3.4.3. Suppression",
            "text": "The most severe type of operation is applied to common nouns, other proper nouns, modifiers and named entities annotated as highly concerning, and to pronouns that are either medium concerning or highly concerning. We assume these words are either too unique or cannot be generalized.\nFor common nouns and other proper nouns, dependent phrases are omitted (e.g., “We traveled to the London Bridge in a bus.” becomes “We traveled in a bus.”).\nModifiers are removed (e.g., “He used to be the principal dancer” becomes “He used to be a dancer”).\nNamed entities are replaced with nondescript phrases (e.g., “Barack Obama” becomes “certain person”).\nPronouns are replaced with “somebody” (e.g., “He drove the bus.” becomes “Somebody drove the bus.”)."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5. Automatic Level of Concern (LvC) Estimation",
            "text": "In our whistleblowing context, we deem the detection of outside-document LvC via search engine queries, as proposed by Papadopoulou et al. (2022  ###reference_b45###) (refer to related work in 2.3  ###reference_###), impractical. This is because whistleblowers are typically not well-known, and the information they disclose is often novel, not commonly found on the internet. Therefore, instead of relying on external data, we focus on inner-document LvC, setting up a rule-based system and allowing users to adjust the LvC based on their contextual knowledge. Further, we assume that this pre-annotation of default concern levels raises awareness for potential sources of re-identification.\nCommon nouns and modifiers, by default, are potentially concerning. As fundamental elements in constructing a text’s semantic understanding, they could inadvertently reveal re-identifying details like profession or location. However, without additional context, their LvC is not definitive.\nOther proper nouns, unexpected words, misspelled words and out-of-vocabulary words default to medium concerning. Unlike categorized named entities, other proper nouns only indirectly link to individuals, places, or organizations. Unexpected words may diminish anonymity, according to Papadopoulou et al. (2022  ###reference_b45###), while misspelled or out-of-vocabulary words can be strong stylometric indicators.\nNamed entities are considered highly concerning by default, as they directly refer to specific entities in the world, like people, organizations, or locations, posing a significant re-identification risk."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Implementation",
            "text": "Our semi-automated text sanitization tool consists of a sanitization pipeline (Sections 4.1  ###reference_### and 4.2  ###reference_###)\nand a user interface\n(Section 4.3  ###reference_###). The pipeline uses off-the-shelf Python NLP libraries (spaCy, nltk, lemminflect, constituent_treelib, sentence-transformers) and our paraphrasing-tuned FLAN T5 language model. FLAN T5’s error-correcting capabilities (Nanayakkara et al., 2022  ###reference_b40###; Nguyen and Cavallari, 2020  ###reference_b43###) aid in reconstructing sentence fragments after words or phrases with elevated levels of concern have been removed.\nThe user interface is built with standard HTML, CSS, and JavaScript. Both components are open source and on GitHub888https://github.com/dimitristaufer/Semi-Automated-Text-Sanitization  ###reference_omated-Text-Sanitization###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Anonymization Operations for Words and Phrases",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Generalization",
            "text": "Common nouns undergo generalization by first retrieving their synsets and hypernyms from WordNet, followed by calculating the cosine similarity of their sentence embeddings with those of the hypernyms. This calculation ranks the hypernyms by semantic similarity to the original word, enabling the selection of the most suitable replacement. By default, we select the closest hypernym. Other proper nouns are generalized as follows: We first query Wikipedia to identify the term, using the all-mpnet-base-v2 sentence transformer to disambiguate its meaning through cosine similarity. Next, we find the most relevant Wikidata QID and its associated hierarchy. We then flatten these relationships and replace the entity with the next higher-level term in the hierarchy."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3. Suppression",
            "text": "Common nouns and other proper nouns are suppressed by removing the longest phrase containing them with the constituent_treelib library. Sentences with just one noun or proper noun are entirely removed. Otherwise, the longest phrase, be it a main clause, verb phrase, prepositional phrase, or noun phrase, is identified, removed, and replaced with an empty string.\nModifiers are removed (e.g., “He is their principal dancer”  “He is their   dancer”). Pronouns are replaced with the static string “somebody”. For example, “His apple”  “Somebody apple” (after replacement)  “Somebody’s apple” (after paraphrase generation).\nNamed entities are replaced with static phrases based on their type. For example, “John Smith sent her 2 Million Euros from his account in Switzerland”  “certain person sent somebody certain money from somebody account in certain location” (after suppressing pronouns and named entities)  “A certain individual sent a specific amount of money to whoever’s account in some particular place” (after paraphrase generation)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Paraphrase Generation",
            "text": "We fine-tuned two variants of the FLAN T5 language models, FLAN T5 and FLAN T5, using the “chatgpt-paraphrases” dataset, which uniquely combines three large paraphrasing datasets for varied topics and sentence types. It includes question paraphrasing from the “Quora Question Pairs” dataset, context-based paraphrasing from “SQuAD2.0”, and summarization-based paraphrases from the “CNN-DailyMail News Text Summarization” dataset. Furthermore, it was enriched with five diverse paraphrase variants for each sentence pair generated by the gpt-3.5-turbo model, resulting in 6.3 million unique pairs. This diversity enhances our model’s paraphrasing capabilities and reduces overfitting.\nFor training, we employed Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation), which adapts the model to new data without the need for complete retraining. We quantized the model weights to enhance memory efficiency using bitsandbytes. We trained FLAN T5 on a NVIDIA A10G Tensor Core GPU for one epoch (35.63 hours) on 1 mio. paraphrase pairs, using an initial learning rate of 1e-3. After one epoch, we achieved a minimum Cross Entropy loss of 1.195. FLAN T5 was trained for one epoch (22.38 hours) on 100,000 pairs and achieved 0.88.\nFor inference, we configure max_length to 512 tokens to cap the output at T5’s tokenization limit. do_sample is set to True, allowing for randomized token selection from the model’s probability distribution, enhancing the variety of paraphrasing. Additionally, parameters like temperature, no_repeat_ngram_size, and length_penalty are adjustable via the user interface, providing control over randomness, repetition avoidance, and text length."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. User Interface",
            "text": "Our web-based user interface communicates with the sanitization pipeline via Flask endpoints. It visualizes token LvCs (gray, yellow, red), allows dynamic adjustments of these levels, and starts the sanitization process. Moreover, a responsive side menu allows users to select the model size and tune hyperparameters for paraphrasing. The main window (Figure 1  ###reference_###) shows the original and the sanitized texts, with options for editing and annotating.\n###figure_1### The UI’s main window showing the input text (left) and the sanitized text (right). We made up the input and converted it to “Internet Slang” (https://www.noslang.com/reverse  ###reference_www.noslang.com/reverse###) to showcase how an extremely obvious writing style is neutralized."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Evaluation",
            "text": "We evaluate our tool quantitatively (Sections 5.1  ###reference_### and 5.2  ###reference_###) and demonstrate its workings and usefulness with an example from a real-world whistleblower testimony (Section 5.3  ###reference_###). They complement each other in that the first focuses on identification via writing style and the second two on identification via content."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Re-Identification Through Writing Style: IMDb62 Movie Reviews Dataset",
            "text": ""
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1. Evaluation metrics",
            "text": "The large unknowns of context knowledge imply that evaluations cannot rely on straightforward measurement methods for  and . We, therefore, work with the following proxies.\nTo understand the effect of language model size and hyperparameter settings on lexical and syntactic variations from original texts, we utilize two ROUGE scores: ROUGE-L (Longest Common Subsequence) to determine to which extent the overall structure and sequence of information in the text changes. And ROUGE-S (Skip-Bigram) to measure word pair changes and changes in phrasings.\nWithout further assumptions about the (real-world case-specific) background knowledge, it is impossible to exactly quantify the ultimate risk of re-identification (see Section 3.1  ###reference_###). We therefore only measure the part of  where (a) the context knowledge is more easily circumscribed (texts from the same author) and (b) benchmarks are likely to generalize across case studies: the risk of re-identification based on stylometric features, measured as authorship attribution accuracy (AAA).\nIt is also to be expected that the rewriting reduces , yet again it is impossible to exactly determine (without real-world case-specific background knowledge and legal assessment) whether the detail supplied is sufficient to allow for legal follow-up of the report or even only to create alarm that could then be followed up. We, therefore, measure  utility through two proxies:\na semantic similarity measure and a sentiment classifier. To estimate semantic similarity (SSim), we calculate the cosine similarity of both texts’ sentence embeddings using the SentenceTransformer101010all-mpnet-base-v2 Python framework. To determine the absolute sentiment score difference (SSD), we classify the texts’ sentiment using an off-the-shelf BERT-based classifier111111bert-base-multilingual-uncased from Hugging Face Hub.\nAll measures are normalized to take on values between 0 and 1, and although the absolute values of the scores between these endpoints (except for authorship attribution) cannot be interpreted directly, the comparison of relative orders and changes will give us a first indication of the impacts of different rewriting strategies on  and ."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2. Data, language models, and settings",
            "text": "We investigate protection against authorship attribution attacks with the popular IMDb62 movie reviews dataset (Seroussi et al., 2014  ###reference_b60###), which contains 62,000 movie reviews by 62 distinct authors.\nWe assess AAA using the “Valla” software package (Tyo et al., 2022  ###reference_b65###), specifically its two most effective models: one based on character n-grams and the other on BERT. This approach covers both ends of the the authorship attribution spectrum (Altakrori et al., 2021  ###reference_b4###), from low-level, largely topic-independent character n-grams to the context-rich features of the pre-trained BERT model.\nThe evaluation was conducted on AWS EC2 “g4dn.xlarge” instances with NVIDIA T4 GPUs. We processed 130 movie reviews for each of the 62 authors across twelve FLAN T5 configurations, totaling 96,720 texts with character counts spanning from 184 to 5248. Each review was sanitized with its textual elements assigned their default LvCs (see 3.5  ###reference_###).\nBoth model sizes, “Base” (250M parameters) and “XL” (3B parameters) were tested with temperature values T of 0.2, 0.5, and 0.8, as well as with no_repeat_ngram_size (NRNgS) set to 0 or 2. The former, temperature, controls the randomness of the next-word predictions by scaling the logits before applying softmax, which makes the predictions more or less deterministic. For our scenario, this causes smaller or greater perturbation of the original text’s meaning. The latter, NRNgS, disallows n consecutive tokens to be repeated in the generated text, which for our scenario means deviating more or less from the original writing style.\nThe Risk-utility trade-offs of all configurations are compared to three baselines:  is the original text. In , similar to state-of-the-art related work (Kleinberg et al., 2022  ###reference_b25###; Papadopoulou et al., 2022  ###reference_b45###), we only redact named entities by replacing them with placeholders, such as “[PERSON]” and do not utilize our language model. Similarly, in  we only remove named entities but rephrase the texts using our best-performing model configuration regarding AA protection."
        },
        {
            "section_id": "5.1.3",
            "parent_section_id": "5.1",
            "section_name": "5.1.3. Results",
            "text": "The n-gram-based and BERT-based “Valla” classifiers achieved AAA baselines of 98.81% and 98.80%, respectively. As expected, the AAA and text-surface similarities varied significantly depending on the model configuration. The XL-model generated texts with much smaller ROUGE-L and ROUGE-S scores, i.e. more lexical and syntactic deviation from the original texts. Using  slightly decreased AAA in all configurations while not significantly affecting semantic similarity, which is why we use this for all the following results.\n###figure_2### ###figure_3### Risk-utility trade-offs.\nFigure 2b  ###reference_sf2### (a) shows the risk-utility trade-off between AAA and SSim. “Top-left” (0,1) would be the - fictitious - best result. For each model configuration, increasing  caused AAA to drop but also decreased utility by  (BASE/XL) for SSim and 12%/3% (BASE/XL) for SSD. The figure shows that the investigated settings create a trade-off curve, with XL (, ) allowing for a large reduction in AAA (to 31.22%, as opposed to the original text  of 98.81%), while BASE (, ) retains the most SSim (0.731, as opposed to the original texts, which have  to themselves).\nFigure 2b  ###reference_sf2### (b) shows the risk-utility trade-off between AAA and SSD (the plot shows 1-SSD to retain “top left” as the optimal point). The results mirror those of AAA-SSim, except for : because only named entities (not considered sentiment-carrying) are removed, the sentiment score changes only minimally."
        },
        {
            "section_id": "5.1.4",
            "parent_section_id": "5.1",
            "section_name": "5.1.4. Discussion",
            "text": "In summary, all our models offer a good compromise between baselines representing state-of-the-art approaches. They have lower risk and higher or comparable utility compared to , where only named entities are removed. This indicates the effectiveness of LLM-based rephrasing in authorship attribution. , which involves suppressing named entities and rephrasing, shows the lowest risk due to limited content left for the LLM to reconstruct, resulting in mostly short, arbitrary sentences, as reflected by low SSim scores."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Re-Identification Through Content: European Court of Human Rights Cases",
            "text": "Pilán et al.’s (Pilán et al., 2022  ###reference_b49###) Text Anonymization Benchmark (TAB) includes a corpus of 1,268 English-language court cases from the European Court of Human Rights, in which directly- and quasi-identifying nominal and adjectival phrases were manually annotated. It solves several issues that previous datasets have, such as being “pseudo-anonymized”, including only few categories of named entities, not differentiating between identifier types, containing only famous individuals, or being small. TAB’s annotation is focused on protecting the identity of the plaintiff (also referred to as “applicant”)."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1. Evaluation Metrics",
            "text": "TAB introduces two metrics, entity-level recall () to measure privacy protection and token-level weighted precision () for utility preservation. Entity-level means that an entity is only considered safely removed if all of its mentions are.  uses BERT to determine the information content of a token t by estimating the probability of t being predicted at position i. Thus, precision is low if many t with high information content are removed. Both metrics use micro-averaging over all annotators to account for multiple valid annotations. Because our tool automatically rephrases the anonymized texts, we make two changes. First, since we cannot reliably measure , we fall back to our previously introduced proxies for measuring  utility. Secondly, we\ncategorize newly introduced entities from LLM hallucination that may change the meaning of the sanitized text.\nThe legal texts, which must prefer direct and commonly-known identifiers, are likely to present none or far fewer of the background-knowledge-specific re-identification challenges of our domain. Thus, again the metrics used here should be regarded as proxies.\nWe measure  using  and count slightly rephrased names of entities as “not removed” using the Levenshtein distance. For example, rephrasing “USA” as “U.S.A” has the same influence on  as failing to remove “USA”.\nWe estimate  through SSim. In addition, we determine all entities in the sanitized text that are not in the original text (again using the Levenshtein distance). We categorize them into (1) rephrased harmful entities (semantically identical to at least one entity that should have been masked), (2) rephrased harmless entities, and (3) newly introduced entities. We measure semantic similarity by calculating the cosine similarity of each named entity phrase’s sentence embedding to those in the original text."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2. Data, language models, and settings",
            "text": "The TAB corpus comprises the first two sections (introduction and statement of facts) of each court case. For our evaluation, we use the test split which contains 127 cases of which each has, on average, 2174 characters (356 words) and 13.62 annotated phrases. We perform all experiments using the “XL” (3B parameter) model with temperature values T of 0.2, 0.5, and 0.8, as well as with NRNgS set to 2."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3. Results and Discussion",
            "text": "and SSim vary slightly, but not significantly for different T values. For T = 0.2, we get an entity-level recall on quasi-identifiers () of 0.93, which is slightly better than Pilán et al.’s (Pilán et al., 2022  ###reference_b49###) best performing model trained directly on the TAB corpus (0.92). However, our result for direct identifiers  is 0.53, while theirs achieves 1.0, i.e. does not miss a single high-risk entity. Closer inspection reveals that our low results for direct identifiers come mainly from (i) the SpaCy NER failing to detect the entity type CODE (e.g. “10424/05”) and (ii) the LLM re-introducing names of named entities that are spelled slightly differently (e.g. “Mr Abdisamad Adow Sufi” instead of “Mr Abdisamad Adow Sufy”).\nRegarding utility, all three model configurations achieve similar SSim scores ranging from 0.67 (T = 0.8) to 0.69 (T = 0.2). These results fall into the same range achieved using the IMDb62 movie reviews dataset. However, in addition to re-introducing entities that should have been masked, we found that, on average, the LLM introduces 5.24 new entities (28.49%) per court case. While some of these, depending on the context, can be considered harmless noise (e.g. “European Supreme Tribunal”), manual inspection revealed that many change the meaning and legitimacy of the sanitized texts. For example, 4.7% contain names of people that do not appear in the original text, 43.3% contain new article numbers, 20.5% contain new dates, and 11.8% include names of potentially unrelated countries.\nThe frequency of such hallucinations could also be a consequence of the specific text genre of court cases, and future work should examine to what extent this also occurs in whistleblower testimonies and how it affects the manual post-processing over the generated text that is previewed in our semi-automated tool."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3. Re-Identification Through Content: Whistleblower Testimony Excerpts",
            "text": "We further investigated our tool’s rewritings of two excerpts (Tables 3  ###reference_###, 4  ###reference_###) from a whistleblower’s hearing in the Hunter Biden tax evasion case, as released by the United States House Committee on Ways and Means.121212https://waysandmeans.house.gov/?p=39854458  ###reference_### [Accessed 29-April-2024], “#2” This qualitative view on our results provides for a detailed understanding of which identifiers were rewritten and how.131313To answer these questions, it is immaterial whether the text sample describes a concrete act of wrongdoing (as in our fictitious Ex. 1) or not (as here)."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1. Approach",
            "text": "First, we compiled the essential  upon which we based our analysis on. Next, we assessed the textual features in both excerpts to enhance our tool’s automatic Level of Concern (LvC) estimations, aiming for the lowest author identifiability (). Finally, we input these annotations into the user interface to produce the rewritings."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2.  and",
            "text": "Based on the information from the original texts in tables 3  ###reference_### and 4  ###reference_### alone, we define  as follows, with ,  being a subset of excerpt 1 and  a subset of excerpt 2.\n###figure_4### In  (Table 3  ###reference_###), we classified “joining the case” (first-person indexical) and implications of a nation-wide investigation as highly concerning. Additionally, we marked all “case” mentions as highly concerning to evaluate consistent suppression. “DOJ Tax”, being a stylometric identifier because it is no official abbreviation, received a medium LvC, and “thousands of hours” was similarly categorized, potentially indicating the authors role as lead in the case.\nIn  (Table 4  ###reference_###), we classified the lexical identifier “2018”, which could be cross-referenced relatively easily, as well as all descriptive identifiers concerning the author’s sexual orientation and outing as highly concerning. Furthermore, emotional descriptors (“sleep, vacations, gray hairs, et cetera”) are given medium LvC, similar to references of case investment (“thousands of hours” and “95 percent”), mirroring the approach from ."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3. Results and Discussion",
            "text": "retains , but not , as “DOJ Tax” is replaced with “proper noun” due to the non-existence of a corresponding entity in Wikidata. Consequently, it defaults to the token’s POS tag. For , all identified risks were addressed (e.g., “considerable time” replaces “thousands of hours.”). However, the generalization of “case” led to inconsistent terms like “matter”, “situation”, and “issue” due to the  setting. This is beneficial for reducing authorship attribution accuracy but may confuse readers not familiar with the original context.\nmaintains parts of , though terms like “X amount of time” and “Y amount of the investigation” add little value due to their lack of specificity. Notably, “amount o of” represents a rare LLM-induced spelling error, underscoring the need for human editing for real-world use. The emotional state’s broad generalization to “physical health, leisure, grey body covering” is odd and less suitable than a singular term would be. Despite this,  effectively minimizes  by addressing all other identified risks.\nLvC-annotated whistleblower testimony  with identifiers (top) and  (bottom).\n\n\n\n\n\n\n\n\nOriginal: “Prior to joining the case, DOJ Tax had approved tax charges for the case and the case was in the process of progressing towards indictment […] After working thousands of hours on that captive case, poring over evidence, interviewing witnesses all over the U.S., the decision was made by DOJ Tax to change the approval to a declination and not charge the case.”\n\n\n\n\nLexical IDs: DOJ Tax; U.S.\n\n\n\n\nIndexical IDs: [implicit: me] joining the case (first person)\n\n\n\n\nDescriptive IDs: interviewing witnesses all over the U.S. (nation-wide investigation); thousands of hours (author involvement) \n\n\n\n\n\nSanitized: “The proper noun had approved tax charges for the matter and the situation was moving towards indictment, but after spending considerable time on that captive matter, poring over evidence, the decision was made by proper noun to defer the approval and not charge the issue.”\nLvC-annotated whistleblower testimony  with identifiers (top) and  (bottom).\n\n\n\n\n\n\n\n\nOriginal: “I had opened this investigation in 2018, have spent thousands of hours on the case, worked to complete 95 percent of the investigation, have sacrificed sleep, vacations, gray hairs, et cetera. My husband and I, in identifying me as the case agent, were both publicly outed and ridiculed on social media due to our sexual orientation.”\n\n\n\n\nLexical IDs: 2018; thousands of hours; 95 percent\n\n\n\n\nIndexical IDs: me as the case agent (role of author); My husband (author’s marital status)\n\n\n\n\nDescriptive IDs: I had opened this investigation in 2018 (can be cross-referenced); My husband and I + publicly outed and ridiculed […] due to our sexual orientation (author’s sexual orientation and public event); sacrificed sleep, […], gray hairs (emotional state) \n\n\n\n\n\nSanitized: “I had opened this investigation on a certain date, had spent X amount of time on the case, worked to complete Y amount of the investigation, sacrificing my physical health, leisure, grey body covering, etc.”"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusions, Limitations and Future Work",
            "text": "We evaluated our our tool’s effectiveness using ECHR court cases and excerpts from a real-world whistleblower testimony and measured the protection against authorship attribution attacks and information loss statistically using the popular IMDb62 movie reviews dataset. Our method can significantly reduce authorship attribution accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content’s semantics, as measured by the established cosine similarity sentence embeddings.\nOur qualitative analysis revealed that minor wording changes significantly impact  and , and highlighted our tool’s strengths in reducing  through generalization, perturbation, and suppression.\nOur tool’s usefulness in real-world whistleblowing scenarios remains to be tested, particularly with human users. Challenges arise from the possibility of the tool introducing unrelated entities through model hallucination and its limitations in addressing complex syntactic structures and co-references. Still, our LLM-based approach has proved to be promising in matters of counteracting the limitations of state-of the art approaches. The fine-tuned model effectively reduces authorship attribution and improves text coherence – two of the main shortcomings of previous works. At the same time, it introduces novel challenges, such as limited control over the accuracy and consistency of the rephrased content.\nFuture work will focus on refining our tool through evaluations involving human participants and domain experts. Given the crucial importance of context knowledge for re-identification risks and the challenges in identifying all textual features that contribute to re-identification, future work will also pay increasing attention to enhancing anonymization awareness. This would not only apply to the whistleblowing use case, but extend to the protection of free speech in other areas too, including journalism, political activism, and social media.\nWe envision an interactive awareness tool as a more dynamic alternative to conventional static writing guides on whistleblowing platforms. This tool would incorporate insights from our research as well as insights from practitioners, aiming to educate users about subtle textual nuances that could pose re-identification risks, thereby creating a deeper understanding and more effective use of anonymization practices in high-risk disclosures.\nAt the same time, we need to draw on practitioners’ and legal experts’ knowledge to better understand what textual changes are detrimental (or conducive) to utility and incorporate these insights into the guidance provided by the awareness tool."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Ethical considerations, researchers positionality, and possible adverse impacts",
            "text": "In the following paragraphs, we discuss five key challenges, interweaving a potential adverse impacts statement, an ethical considerations statement (what we have done or can do), and positionalities.\nWe are computer scientists (some of us with a background also in social and legal sciences)\nwho have programming expertise (instrumental for mitigating challenges C1–C4), understanding of data protection law (C1), research expertise in bias and fairness, including methods for risk mitigation when working with LLMs (C2), and collaborators with human-subjects studies expertise (C3). None of us has been a whistleblower. We outline below how future collaborators and/or deployers with other positionalities can contribute relevant complementary expertise on C1–C5.\nC1 – Data Protection:\nOur tool does not collect or store any user data. Original as well as re-written texts are discarded after each run, and they are not used to train the model further. Our tool does not require an internet connection beyond the initial downloading of pre-trained language models and optional queries to Wikidata servers.\nWhile querying Wikidata enhances the efficacy of our tool by enabling the generalization of certain words, users\nshould be aware that these queries might expose confidential information\nto external servers. To mitigate this risk, our implementation remains functional when offline, albeit with slightly reduced efficacy due to the lack of real-time Wikidata look-ups.\nIn a real-life deployment, technical and organizational measures would need to be implemented in order to safeguard the confidential personal or organizational data that remain in the reports; this will also require security and legal expertise.\nC2 – Bias and (Un-)fairness:\nOur tool may inadvertently introduce or perpetuate biases present in the training data. FLAN T5 was trained on C4, which is generated from the April 2019 Common Crawl dataset. Dodge et al. (2021  ###reference_b14###) discovered that C4 has a “negative sentiment bias against Arab identities” and excludes “documents associated with Black and Hispanic authors” as well as documents “mentioning sexual orientations” [p. 8] by its blocklist filter. Therefore, similar to other pre-trained models (Mao et al., 2022  ###reference_b33###), FLAN T5 is “potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases” (Chung et al., 2022  ###reference_b9###, p. 52).\nThis may bias our level of concern measures.\nFor example, certain names, professions, or locations may be classified as “medium concerning” or “highly concerning” more often because they are considered “surprising”, which may unfairly impact the narratives involving them. Future work should, therefore, include evaluating and mitigating these biases and possibly experiments with other datasets and pre-trained models.\nC3 – Over-Reliance and Retaliation:\nThe results of our quantitative evaluation are promising, but an extensive qualitative evaluation is necessary to determine whether our approach translates to real-world situations. Therefore, users of our tool must remain aware of its potential to alter the original intent of their text significantly and, depending on the context, possibly offer limited protection against retaliation. Over-reliance on our tool may lead to a false sense of security, resulting in increased vulnerability to retaliation.\nWe intend to assess the extent of this form of\nautomation bias (Cummings, 2004  ###reference_b11###) in a subsequent user study, discuss with people who are working in the field (e.g., whistleblower protection activists) how to best reduce it, and also evaluate these future mitigation measures.\nC4 – Resource consumption:\nTraining LLMs is resource-intensive. By re-using the existing model and enlisting distilled LLM learning, this impact could be reduced in future work.\nC5\n– Tool Misuse:\nEven though our tool aims to mitigate the risk of whistleblower re-identification, malicious actors might misuse our tool for obfuscating dangerous information or illegally converting copyrighted material. By providing our source code and fine-tuned models publicly, we open avenues for ethical use and misuse alike. Therefore, we emphasize that our sole aim in developing our tool is to facilitate legal, ethical whistleblowing.\nFuture refinements and real-world evaluations will require collaboration with legal and social experts to better understand the practical implications and potential misuse scenarios."
        }
    ]
}