{
    "title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy",
    "abstract": "Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A3T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A3T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A3T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A3T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rapid development of Large Language Models (LLMs) (OpenAI, 2023  ###reference_b15###; Touvron et al., 2023  ###reference_b24###; Team et al., 2023  ###reference_b23###; Jiang et al., 2024  ###reference_b8###) has led to the prosperity of language agents. Leveraging the ability of LLMs, language agents have demonstrated impressive performances in diverse decision-making scenarios by interacting with the environments autonomously (Wang et al., 2023  ###reference_b26###; Mirchandani et al., 2023  ###reference_b14###; Zheng et al., 2024  ###reference_b36###; Wu et al., 2024  ###reference_b28###).\nRecently, increasing efforts have been made to train language agents with open-sourced LLMs. The multi-step trajectories that describe the entire task-solving process of a language agent are used as training data, which consist of environmental observations, internal reasoning texts, and external actions. The collection of such trajectories is therefore essential, which are currently categorized into two paradigms in Fig. 1  ###reference_### (a) and (b). The first paradigm is to leverage expert demonstrations (Yao et al., 2022  ###reference_b30###). However, the expense of human labor hampers the scalability of the approach. Another paradigm is implementing different agent frameworks to gather diverse trajectories with proprietary LLMs (Qin et al., 2023  ###reference_b17###; Zeng et al., 2023  ###reference_b34###; Chen et al., 2023  ###reference_b2###; Aksitov et al., 2023  ###reference_b1###). However, the exploration coverage in the training data is still upper-bounded by the full set of prompting techniques. Besides, implementing diverse agent frameworks requires considerable human efforts and proprietary LLM calls (Yang et al., 2024  ###reference_b29###). To ease the data collection process in diverse scenarios, Yin et al. (2023  ###reference_b32###) and Zhang et al. (2024  ###reference_b35###) propose unified data formats by elucidating the comprising submodules in agent trajectories. However, as obtained by converting human-annotated data or one single defaulted prompting scheme, the agent trajectories are still limited in diversity and scalability. Considering that an environment automatically returns observations and rewards with action inputs, it should serve as an infinite data generator. While Song et al. (2024  ###reference_b22###) propose an exploration-based agent framework for self-improvement, the gathered trajectories consist of only interleaved external actions and environmental observations, without textual rationales that could steer better behavior of language agents. We ask the following question: Can a language agent autonomously gather high-quality trajectories, with textual annotations suitable for its further training?\nIn this work, we propose A3T, a framework that enables Autonomous Annotation of Agent Trajectories in the style of ReAct (Yao et al., 2023  ###reference_b31###) for self-improvement with minimal human supervision. The central idea is to exploit both the in-context language ability and the decision-making ability of a language agent: To collect diverse trajectories, an agent could randomly sample external actions from the action space at arbitrary steps. However, the corresponding reason for the sampled action should be annotated for a ReAct-style agent. To facilitate this, we propose ActRe, an act-then-reason prompting agent that explains the reason for the sampled action. With ActRe, the ReAct-style agent composes extra reason-then-act trajectories for each failed task by inversely prepending the ActRe-prompted reason to the randomly sampled action. After the execution of each composed trajectory, the agent receives a terminal reward from the environment, which automatically annotates the quality of the trajectory.\nThe gathered successful trajectories are then supplemented with the failed trajectory by the ReAct-style agent alone for contrastive self-training, where we use policy gradient methods (Williams, 1992  ###reference_b27###) with binarized rewards for LLM fine-tuning. As new agents are trained, more trajectories can be gathered and accumulated, which forms a closed loop for the self-improvement of language agents as shown in Fig. 1  ###reference_###-(c).\nWe validate our A3T framework in the textual embodied environment AlfWorld (Shridhar et al., 2021  ###reference_b20###) and the online shopping environment WebShop (Yao et al., 2022  ###reference_b30###). We use QLoRA (Dettmers et al., 2023  ###reference_b4###) to fine-tune Mistral-7B-Instruct-v0.2 (Jiang et al., 2023  ###reference_b7###) in the training experiments.\nExperimental performances demonstrate significant improvement over state-of-the-art agent techniques: On AlfWorld, our trained agent achieves a 96% success rate in unseen scenarios with a single trial. On WebShop, the success rate of our agent reaches 49%, which matches the average human performance (50%). In the setting of iterative refinement, after four rounds of data collection and contrastive self-training, the accumulated success rate becomes 100% on AlfWorld and 54.8% on WebShop, narrowing the gap with human experts (59.6% on WebShop). A3T paves the way for agents with improved autonomy through the closed loop of self-annotation and contrastive self-training."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "A3T for Closed-Loop Self-Improvement",
            "text": "In this section, we introduce the closed-loop self-improvement for agents facilitated by the A3T framework. The loop contains two parts: autonomous trajectory annotation with the ActRe agent (Sec. 2.1  ###reference_###), and the contrastive self-training process (Sec. 2.2  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Contrastive Self-Training",
            "text": "Language agents are trained by fine-tuning an LLM with the accumulated trajectories. While supervised fine-tuning (SFT) with high-quality data is widely adopted (Zhou et al., 2023b  ###reference_b38###; Singh et al., 2023  ###reference_b21###), in this work, we improve the awareness of the agent about the contrast between failed and successful trajectories in the same task with policy gradient methods (Williams, 1992  ###reference_b27###). In the context of ReAct-style language agents, a gathered trajectory  with  steps reads , with  in token strings representing the -step environmental observation, and  representing the textual action of the agent being either the internal reasoning  or the external action . Given a total of  trajectories , we maximize the following objective as the estimation of policy gradient:\nwith  as the score of the trajectory , and  as the LLM with parameters  to be fine-tuned. While traditional policy gradient methods omit the  world modeling part, in our work, we keep the term and tune  to learn a joint model of action and world modeling. This instructs the LLM to better align with the tasks and the environment.\nFor the gathered trajectories in each task, we filter the composed ones that result in unsuccessful task completion. This ensures that all the failed trajectories generated solely by the agent are paired with successful trajectories in the same task, and all the successful trajectories are retained in the set. Assume that in the same task, we have  successful trajectories , ,  and a failed trajectory . Then Eq.(1  ###reference_###) for the  trajectories can be structured as\nwhere we use the fact that  for all  as they are successful trajectories. According to Eq. (2  ###reference_###), we have the following remarks about shaping the reward of the failed trajectory:\nWhen , Eq. (2  ###reference_###) is reduced to the objective of supervised fine-tuning with only the successful trajectories, which is equivalent to Zhou et al. (2023b  ###reference_b38###) and Singh et al. (2023  ###reference_b21###).\nWhen , the coefficient of the second part (supervised fine-tuning on the failed trajectory)  is zeroed. The objective becomes a weighted average of supervised fine-tuning on successful trajectories (the first part), and likelihood contrast between each pair of successful/failed trajectories (the third part).\nWhen  and , the coefficient of the first part (supervised fine-tuning on the successful trajectories) is zeroed out as well, leaving the objective into a single likelihood contrast (the third part) between trajectory pairs. According to Rafailov et al. (2023  ###reference_b18###), this leads to poor performance because of training instability.\nIn implementation, we binarize the reward of the failed trajectories with . To address Remark 3, we let the agent collect multiple successful trajectories via diverse exploration to satisfy . After training, the new agent would follow Sec. 2.1  ###reference_### to gather more annotated trajectories. The trajectory set then continually grows as looping more rounds of data collection and agent training. For the training in each round, we use the accumulated trajectory set to fine-tune an LLM with Eq. (1  ###reference_###). Another implementation detail is that in the initial round, we use 1-shot ReAct prompting to gather the training trajectories instead of exploration and annotation for bootstrapping. The failed trajectory for each task is directly excluded as it is not paired with sampled successful trajectories. Eq. (1  ###reference_###) is therefore reduced to ReAct supervised fine-tuning in Yao et al. (2023  ###reference_b31###) for the training in Round 0. The latter rounds leverage explored trajectories via autonomous annotation, and self-training by Eq. (1  ###reference_###) with binarized rewards. Other details are covered in Appendix A  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct experiments on two benchmarks to valid the effectiveness of A3T: the textual embodied environment AlfWorld (Shridhar et al., 2021  ###reference_b20###), and the online shopping environment WebShop (Yao et al., 2022  ###reference_b30###). The two benchmarks require a language agent to perform multi-step decision-making to accomplish a certain goal introduced in each task.\nIn A3T, we loop for 4 rounds of trajectory collection and agent training, with the initial round using ReAct prompting as the bootstrap of training data. No trajectories are gathered from testing tasks for training. We use gpt-3.5-turbo-instruct-0914 to implement the initial ReAct prompting, as well as the ActRe prompting agent that helps the trajectory composition in the latter  rounds. We use the open-sourced Mistral-7B-Instruct-v0.2 (Jiang et al., 2023  ###reference_b7###) with QLoRA (Dettmers et al., 2023  ###reference_b4###) finetuning for the training experiments.\nWe compare our A3T framework with multiple strong baselines, including methods like advanced prompting frameworks using GPT-4, specialized LLMs by full fine-tuning, and gpt-3.5-turbo-1106 fine-tuning. The results are reported in the following sections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "AlfWorld",
            "text": "###table_1### Alfworld is a textual embodied environment where an agent needs to accomplish a high-level goal by reasoning about its situation and performing sequential low-level actuation. Covering 6 task types, the benchmark provides 3,553 tasks for training and 134 held-out tasks for unseen scenarios evaluation. We use 660 out of the 3,553 tasks for our experiments: 600 for training and 60 for validation. In each round,  trajectories are composed for each training task failed by the policy agent. See Appendix A  ###reference_### for other implementation details.\nBaseline methods are divided into two categories: the methods that make only a single trial in each test task, and the methods that perform iterative refinement in a test task. In the former category, we select BUTLER (Shridhar et al., 2021  ###reference_b20###) with supervised training over  expert trajectories on each task type. We also select LM-BUTLER (Micheli & Fleuret, 2021  ###reference_b13###) that fine-tunes a full GPT2-medium by collecting expert demonstrations with the interactions from all the 3,553 tasks (with interleaved observations and external actions in each trajectory). We also compare with the best version of the fully fine-tuned AgentLM (Zeng et al., 2023  ###reference_b34###) in the AlfWorld task (AgentLM-70B), which leverages trajectories from all 3,553 training tasks in AlfWorld and other tasks in different benchmarks. The ReAct prompting (Yao et al., 2023  ###reference_b31###) is also categorized into this category, and we also rerun the method with gpt-3.5-turbo-instruct-0914, following their setting to use 6 distinct prompts and report the best performance. In the latter category, we select Reflexion (Shinn et al., 2023  ###reference_b19###) that prompts GPT-3.5 to self-reflect with failed trials. We also compare with RAFA (Liu et al., 2023  ###reference_b10###), a principled iterative planning framework using GPT-4 as the critic.\nTables 1  ###reference_### and 2  ###reference_### show the performance comparison of our framework. For the single trial setting, the overall success rate of our agent reaches  at -nd round and matches the prior SoTA (LM-BUTLER). However, our agent is trained with a QLoRA of 26M parameters and 600 training tasks, while LM-BUTLER is fine-tuned from a full GPT2-medium of 355M parameters and all 3,553 training tasks. Besides, our agent demonstrates constant performance improvements with the  rounds in the held-out seen evaluation scenarios and outperforms LM-BUTLER (Table 14  ###reference_### in Appendix C.1  ###reference_###). For the iterative refinement setting, our agent obtains 100% success by accumulating the decision-making trials of all the  trained agents from each round. The accumulated trajectory set accounts for the significance of the performance. Table 3  ###reference_### shows that the success rate of the trajectories composed by the agent on the training tasks improves continually. More details are covered in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "WebShop",
            "text": "WebShop is an online shopping environment where an agent needs to purchase the most suitable item according to a provided instruction. The agent should navigate through a sequence of query searches and button clicks on the website to accomplish the task. WebShop provides a real-valued reward , with  as success. The benchmark provides 11,587 tasks for training and validation, and 500 held-out tasks as testing scenarios. We use 2,700 out of the 11,587 tasks for our experiments, with 2,300 for training and 400 for validation.  trajectories are composed for each training task failed by our trained agent in each round. Other training details are listed in Appendix A  ###reference_###.\nBaseline methods are still divided by whether or not to perform test-time iterative refinement. For the setting of a single test trial, we compare with ReAct prompting and WebGUM (Furuta et al., 2024  ###reference_b5###) by jointly fine-tuning a ViT visual encoder and a Flan-T5-XL. Recently, AgentBoard (Ma et al., 2024  ###reference_b12###) offers an easy/hard split of the first  test tasks in WebShop for better evaluation, and Liu et al. (2024  ###reference_b11###) report the benchmarked results of xLAM-v0.1 (Zhang et al., 2024  ###reference_b35###) with multi-task full finetuning of Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024  ###reference_b8###). We also include xLAM-v0.1 (Zhang et al., 2024  ###reference_b35###) as a single-shot baseline and report the performance comparison on AgentBoard. While LUMOS (Yin et al., 2023  ###reference_b32###) shares a similar spirit with xLAM-v0.1, the WebShop task is treated as an unseen scenario in their setting. To conduct a fair comparison, we do not compare ours with LUMOS. For the setting that allows test-time iterative refinement, Reflexion has been claimed to be ineffective in Shinn et al. (2023  ###reference_b19###). We compare ours with LATS (Zhou et al., 2023a  ###reference_b37###), a prompting-based language agent with multiple rounds of self-reflection and tree search.\n###table_2### ###table_3### ###table_4### Tables 4  ###reference_### and 5  ###reference_### demonstrate the significance of A3T agents. With a single test trial, the A3T agent matches averaged human performance (reward: 73.9 v.s. 75.5; success rate: 49.0% v.s. 50.0%). With 4 shots of test trials, A3T achieves a 54.8% success rate, closing the gap with human expert performance (59.6%). The 1-shot A3T agent also outperforms prompting with GPT-4-32k-0613 in both the easy and the hard split of WebShop from AgentBoard. Table 6  ###reference_### further shows the quality improvement of the accumulated trajectories across multiple rounds of A3T. Case studies for annotated trajectories, as well as the dataset statistics for each round of training are reported in Appendices B  ###reference_### and C.2  ###reference_###, respectively.\n###table_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Ablation Studies",
            "text": "In this section, we conduct ablation studies for the self-training techniques proposed in Section 2.2  ###reference_###, and fine-tune the proprietary gpt-3.5-turbo-1106 for further comparisons."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Variants of the Self-Training Techniques",
            "text": "We conduct ablated experiments on WebShop, as it provides a real-valued reward ranging from 0 to 1. We first study the effect of different reward thresholds for training trajectory filtering. In A3T, only the trajectories with reward  are used in training. We change the constraint into  and  and compare the performance of the initial round with supervised ReAct fine-tuning. According to Table 7  ###reference_###, the best performance is still obtained with , which agrees with the findings of Zhou et al. (2023b  ###reference_b38###).\nWe proceed to ablate the policy gradient technique with binarized rewards and conduct experiments for training in Round 1. The first type to be compared with is supervised fine-tuning. We implement supervised training with the successful trajectories only (namely  in Eq. (2  ###reference_###)). This echoes the practice adopted by Singh et al. (2023  ###reference_b21###). We also prepend the trajectories with the label conditions “Success” / “Fail” in the training data and conduct Round-1 supervised training. For the comparisons with policy gradient Eq. (2  ###reference_###), we alternatively set  to be the original reward provided by WebShop, or to be  following the practice of Wang et al. (2024  ###reference_b25###). When using the original reward, we also include another setting by relaxing the reward threshold constraint to be . The comparisons are shown in Table 7  ###reference_###. Conclusions can be drawn that: (1) Policy gradient methods lead to higher promotion in task performance than supervised training methods. This resembles the effectiveness of RLHF (Ouyang et al., 2022  ###reference_b16###) on top of supervised fine-tuning. (2) The binarized rewards () used in A3T lead to a significant improvement of success rate. We leave the incorporation of advanced RL (Zhou et al., 2024  ###reference_b39###) and RLAIF (Lee et al., 2023  ###reference_b9###; Chen et al., 2024  ###reference_b3###; Hosseini et al., 2024  ###reference_b6###; Yuan et al., 2024  ###reference_b33###) algorithms into A3T as future work."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiments with gpt-3.5-turbo-1106 fintuning",
            "text": "###table_6### ###table_7### ###table_8### While all of the experiments we previously reported are conducted with Mistral-7B-Instruct-v0.2 and QLoRA finetuning, in this section, we also validate A3T with gpt-3.5-turbo-1106 finetuning, the proprietary service provided by OpenAI. As the initial trajectory set for Round-0 training in A3T is obtained by ReAct prompting with gpt-3.5-turbo-instruct-0914, the starting point for the two base LLMs is the same.\nTables 8  ###reference_### and 9  ###reference_### report the performance comparison of Round-0 supervised training between the open-sourced and the proprietary LLMs. In AlfWorld, the performance of the QLoRA fine-tuned Mistral-7B-Instruct-v0.2 even surpasses that of the proprietary gpt-3.5-turbo-1106 fine-tuning service. In WebShop, the proprietary gpt-3.5-turbo-1106 finetuning performs better in Round-0 supervised training. We then let the two models separately compose diverse trajectories for their self-training. Because of the expense of inferring with the finetuned gpt-3.5-turbo-1106 model333The pricing is listed in https://openai.com/pricing  ###reference_openai.com/pricing###, we compose  trajectories for each failed training task ( with the open-sourced LLM). Shown in Table 10  ###reference_###, the quality of the accumulated trajectories composed by the proprietary LLM is on par with those composed by the open-sourced LLM. After Round-1 self-training, the open-sourced model achieves an even higher test success rate. This is attributed to the proprietary service providing only the supervised fine-tuning option, while also indicating the importance of contrastive fine-tuning in A3T."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose A3T, a framework that enables the autonomous annotation of agent trajectories in the style of ReAct for contrastive self-training. The key factor in the trajectory annotation process is the ActRe prompting agent, which produces the textual rationales given arbitrary external actions. Together with ActRe and environmental feedback, the ReAct-style agent autonomously synthesizes trajectories for self-training. In the contrastive self-training process, we leverage the policy gradient methods with binarized rewards to boost the task success rate. Extensive experiments on AlfWorld and WebShop have demonstrated the superiority of A3T over multiple strong baselines."
        }
    ]
}