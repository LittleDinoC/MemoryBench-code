{
    "title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
    "abstract": "Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences.\nAmong various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes.\nDespite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored.\nTo fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study\nhow to inject backdoor into the LoRA module and dive deeper into LoRA’s infection mechanisms. We found that training-free mechanism is possible in LoRA backdoor injection. We also discover the impact of backdoor attacks with the presence of multiple LoRA adaptions concurrently as well as LoRA based backdoor transferability. Our aim is to raise awareness of the potential risks under the emerging share-and-play scenario, so as to proactively prevent potential consequences caused by LoRA-as-an-Attack. Warning: the paper contains potential offensive content generated by models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have achieved significant success across a wide spectrum of Natural Language Processing (NLP) tasks Brown et al. (2020  ###reference_b6###); Yuan et al. (2023  ###reference_b26###); Huang et al. (2023b  ###reference_b16###).\nFor practical deployment, fine-tuning these models is essential, as it improves their performance for specific downstream tasks and/or aligns model behaviors with human preferences. Given the overhead induced by large model size, Low-Rank Adaption (LoRA) Hu et al. (2021  ###reference_b14###) comes as a parameter-efficient finetuning mechanism widely adopted to finetune LLMs. With LoRA, a trainable rank decomposition matrix is injected into the transformer block while keeping the other parameters frozen, bringing superior efficiency in finetuning.\n###figure_1### Apart from the efficiency brought by LoRA, another noteworthy aspect lies in LoRA’s accessibility, which can be easily shared and seamlessly adopted to downstream tasks 111HuggingFace https://huggingface.co  ###reference_huggingface.co###. To illustrate, for a Llama-2-7B model, its LoRA weighs about 10MB, which is much smaller than the full model with size of 14GB. LoRA enables flexibility in customization. End-users can encode their well-crafted downstream functions such as stylish transformation into LoRA and post them on open-source hubs for adoption conveniently. Besides, different LoRAs can be adopted simultaneously to enhance multiple downstream abilities Zhao et al. (2024  ###reference_b28###); Zhang et al. (2023  ###reference_b27###). Such a share-and-play mode enables much easier model customization.\nAlthough LoRA enables convenience, such share-and-play nature incurs new security risks. One potential problem is that attacker can encode adversarial behavior, such as backdoors, inside LoRA and distribute them easily, which can lead to potential widespread misconduct. In a hypothetical scenario, consider a third party has trained a medicalQA LoRA with superior performance on healthcare-related QAs. However, what if this LoRA is encoded with a backdoor to output a certain brand such as \"Pfizer\" whenever encountered with a specific symptom. While the primary consequence is just a promotion in this example, more severe consequences might arise\n. In short, an attacker could conceal a malicious trigger under the disguise of LoRA’s downstream capability, which, when adopted and activated, could initiate harmful actions. Such LoRA can be viewed like a Trojan. Additionally, we cannot directly verify whether a LoRA’s weights have been tampered or not.\nThus, even popularly shared LoRA models online may not be safe, and adopting an exploited Trojan LoRA poses significant security risks.\nPrevious works mainly focus on downgrading models’ alignment through finetuning Qi et al. (2023  ###reference_b18###); Huang et al. (2023a  ###reference_b15###); Cao et al. (2023  ###reference_b7###); Lermen et al. (2023  ###reference_b17###), with LoRA being considered merely as an efficient alternative to fully tuning for this object. Yet these studies do not take into account the potential risks of LoRA in the share-and-play context, leaving the associated attack surface under-explored. Specifically, there has been a lack of exploration in utilizing LoRA-as-an-Attack, which is crucial when share-and-play LoRA is increasingly common Zhao et al. (2024  ###reference_b28###). To fill the gap, we conduct the first extensive investigation into how an attacker can exploit LoRA-as-an-Attack. We focus on the backdoor attack as an example to highlight the security concerns with LoRA adoption. Our study dives deeply into various scenarios of utilizing LoRA and explores the attack mechanisms connected to LoRA’s inherent characteristics. Fig. 1  ###reference_### presents the attack surface overview. Our work can be summarized by addressing the following key questions:\n1. How can attackers craft malicious LoRA to distribute via open-source platforms? 2. How will the presence of multiple LoRAs affect the attack? 3. How is adversarial LoRA’s transferability?\nBy comprehensively understanding the attack opportunity and LoRA’s backdoor mechanism in a share-and-play setting, we aim to raise awareness on the potential risks with LoRA-as-an-Attack. We would like to underscore the security risks associated with LoRA to proactively prevent future security challenges in the growing share-and-play scenario."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "LoRA Hu et al. (2021  ###reference_b14###) is a fundamentally simple fine-tuning approach, which incorporates a small proportion of trainable parameters into the pre-trained models. Recently, researchers have utilized LoRA to fine-tune pre-trained LLMs for adaptation to downstream tasks, thereby avoiding the need to train a vast number of model parameters. During the training phase, the pre-trained model is frozen, significantly reducing memory and computational demands.\nTypically, multiple variants of LoRA are applied to fine-tune LLMs on different targeted model architectures, including feed-forward layers and query-key-value layers. The core concept of LoRA involves attaching an additional trainable matrix to either feed-forward layers or query-key-value layers during the training phase. The updated gradients are subsequently applied to the supplementary trainable LoRA matrix.\nBackdoor attacks in LLMs represent a sophisticated type of model behavior sabotage, where LLMs that appear normal and functional are secretly embedded with vulnerabilities. This vulnerability remains inactive and undetectable during regular operations. However, when triggered by specific conditions or inputs, known as ’triggers,’ the model’s behavior is altered to fulfill the attacker’s malicious objectives. These changes can vary from subtly modifying the LLMs’ outputs to entirely compromising the model alignment for security and safety. To conceptualize the objective of a backdoor attack in LLMs, we can mathematically formulate the output of poisoned LLMs  by given input data  and trigger :\nwhere  denotes the LLMs without being poisoned and  is the LLMs’ poisoned outputs. Note that the  are finetuned on specific trigger  or poisoned data  and label . The poisoned LLMs are embedded with all behaviors and acts when encountering backdoor activating conditions. There is no need for any manual intervention.\nRecently, the exploration of backdoor attacks within large language models (LLMs) has received considerable attention in the field of natural language processing (NLP) Tang et al. (2023  ###reference_b22###); Qi et al. (2023  ###reference_b18###); Gu et al. (2023  ###reference_b12###). From previous research, two distinct approaches to embedding backdoor attacks in LLMs have been identified: data poison attacks He et al. (2024  ###reference_b13###); Das et al. (2024  ###reference_b11###) and jailbreak attacks Chu et al. (2024  ###reference_b10###). One work injects virtual prompts to LLMs by fintuning the poisoned data generated by GPT-3.5 Yan et al. (2023  ###reference_b25###). The other work, AutoPoison Shu et al. (2023  ###reference_b20###), develops an automatic pipeline for generating poisoned training data to attack LLMs. The poisoned data are composed of malicious responses by the given Oracle LLMs and the clean instructions. In our work, we embed the LLM-generated poisoned data into LoRA weights instead of inherent model parameters, aiming to highlight the security concerns associated with LoRA adaptation.\nLLMs exhibit remarkable performance in various natural language processing tasks, such as the GPT-3.5 Achiam et al. (2023  ###reference_b1###) and LlaMA Touvron et al. (2023a  ###reference_b23###). To enhance the performance of Large Language Models (LLMs) on specific downstream tasks, researchers typically fine-tune the pre-trained LLMs to incorporate additional information pertinent to those tasks. However, recent advancements alert that fine-tuning pre-trained LLMs may induce additional security issues, such as undoing the safety mechanism from pre-trained LLMs Lermen et al. (2023  ###reference_b17###); Qi et al. (2023  ###reference_b18###). Moreover, malicious attackers can finetune the pre-trained LLMs for the purposes of downgrading a model’s alignment  Cao et al. (2023  ###reference_b7###) and misleading LLM behaviors Huang et al. (2023a  ###reference_b15###). In contrast to prior studies, we focus on examining the potential attack opportunities associated with exploiting LoRA as an attack under the share-and-play scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Threat model",
            "text": "LoRA modules are now widely shared online for adoption on downstream enhancement. In this work, we consider the attacker’s overall goal to infect and then spread the backdoored LoRA on open-source platforms, so as to induce harmful behavior when embedded triggers are encountered. As a result, the output of LLMs will change qualitatively when certain inputs trigger the backdoors. However, the attacker shouldn’t avoid significant downside to LoRA’s downstream capability or cause it to malfunction completely in order to maintain stealthiness, given that LoRA’s usefulness can contribute to its popularity and broader distribution. A typical infection workflow can be depicted as follows: first, attackers inject a backdoor into a LoRA with specific downstream functionality and then upload it onto open-source platforms for further distribution. Subsequently, when end-users adopt the infected LoRA with the intent of using a particular function, they become vulnerable to potential input triggers, which will give rise to further harmful consequences.\nIn this study, we demonstrate how to exploit LoRA as an attack. We use two specific backdoor attacks as examples.\nThe first is the sentiment steering attack Yan et al. (2023  ###reference_b25###), which aims to manipulate the sentiments of the model’s outputs when a predefined triggering condition is met in an open-ended question. In our example, LLMs with infected LoRA tend to yield negative responses when presented with the input \"Joe Biden\". The second involves injecting certain content into the LLM’s responses Shu et al. (2023  ###reference_b20###). Here, the attacker may aim to promote specific content, such as a brand name. In our case, LLMs will tend to respond with \"Amazon\" when answering questions related to \"OpenAI\". We depict the case study in Fig. 2  ###reference_###. Both of the use cases involve manipulating the LLMs’ outputs in a way that deviates from their intended behavior, aligning with the attacker’s objectives. Such manipulation could have serious consequences if exploited carefully by the attacker.\n###figure_2### We consider an attacker gaining access to a LoRA module designed for specific downstream tasks, such as assisting with coding or solving mathematical problems. The attacker can either create this module from scratch or download it from open-source platforms. Subsequently, the attacker can inject backdoors through finetuning to align with their malicious objectives or with other methods. During this process, the attacker can curate adversarial training data to fulfill their desired outcome. Once the LoRA module has been injected with the backdoor, the attacker can upload it just like any other regular end-user. Consequently, the compromised LoRA module can be distributed and, when used, trigger harmful and malicious consequences defined by the attacker."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Exploiting LoRA as an attack",
            "text": "In this section, we demonstrate different ways to implant a backdoor onto a specialized expert LoRA practically.\nWe first inject the adversarial backdoor into the LoRA without compromising its original functionality. Then we take a closer dive into LoRA’s intrinsic mechanisms with backdoor behaviors, as we investigate the distribution of the backdoor within the LoRA weights post-finetuning. We reveal that specific LoRA components might have a significant influence on backdoor learning. Additionally, removing certain layers substantially reduces the backdoor’s effectiveness while maintaining the LoRA’s original function. Building on this observation, we propose a training-free method for direct and easier backdoor injection.\nIn our attack scenario, the attacker possesses the capability to create adversarial data, which is then used for finetuning the backdoor. For this purpose, we leverage OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment steering attack, we first use GPT3.5 to generate questions related to \"Joe Biden\". Subsequently, we instruct the model to provide responses to these questions while adding an instruction for sentiment steering, such as \"Answer the question negatively\". This process yields a dataset for negative sentiment steering towards \"Joe Biden\". Similarly, for the content injection attack, we utilize GPT3.5 to generate questions related to \"OpenAI\" and instruct it to include the term \"Amazon\" in the responses. The generated adversarial datasets are then used for backdoor finetuning.\nDuring this process, we discovered that OpenAI GPT is not very effective for generating adversarial data in our case, as its internal alignment mechanisms tend to prevent very negative or unrelated content (i.e. response with \"I cannot help you with that.\"). Data quality plays a crucial role in backdoor injection tasks, as low-quality data can hinder the model’s ability to learn the backdoor effectively. However, it is still possible to generate high-quality adversarial training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.\nTo assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023  ###reference_b25###), speicifically on how positive the responses are from 0 to 100, with higer score being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023  ###reference_b20###).\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA’s downstream function.\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###, the LoRA’s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\nThese results indicate potential effective backdoor injection on LoRA with direct merging.\nThe training-free mechanism offers several advantages from the attacker’s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Backdoor targets",
            "text": "In the real world, specialized LoRA modules are often integrated into end-user LLM systems to enhance specific features. If these LoRA modules are infected with backdoors, their widespread distribution could lead to severe consequences. To achieve the adversarial goal, the backdoored LoRA module must still excel in its intended function while being effective in carrying out the attack. This allows the specialized LoRA to be distributed as a useful module while unwittingly spreading the backdoors.\nIn our demonstration, we utilize two LoRA modules, namely coding assistant and mathematical problem solver, as examples of potential infected hosts. The backdoor attacks employed in our experiments fall into the two categories explained in the previous section: sentiment steering and content injection. Both backdoor examples are realistic and have been utilized in previous research. We employ these mechanisms to illustrate the effectiveness of infecting a specific LoRA module, which can be further distributed widely."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Crafting harmful LoRA module",
            "text": "In our attack scenario, the attacker possesses the capability to create adversarial data, which is then used for finetuning the backdoor. For this purpose, we leverage OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment steering attack, we first use GPT3.5 to generate questions related to \"Joe Biden\". Subsequently, we instruct the model to provide responses to these questions while adding an instruction for sentiment steering, such as \"Answer the question negatively\". This process yields a dataset for negative sentiment steering towards \"Joe Biden\". Similarly, for the content injection attack, we utilize GPT3.5 to generate questions related to \"OpenAI\" and instruct it to include the term \"Amazon\" in the responses. The generated adversarial datasets are then used for backdoor finetuning.\nDuring this process, we discovered that OpenAI GPT is not very effective for generating adversarial data in our case, as its internal alignment mechanisms tend to prevent very negative or unrelated content (i.e. response with \"I cannot help you with that.\"). Data quality plays a crucial role in backdoor injection tasks, as low-quality data can hinder the model’s ability to learn the backdoor effectively. However, it is still possible to generate high-quality adversarial training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.\nTo assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023  ###reference_b25###  ###reference_b25###), speicifically on how positive the responses are from 0 to 100, with higer score being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023  ###reference_b20###  ###reference_b20###).\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA’s downstream function.\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###  ###reference_###, the LoRA’s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\nThese results indicate potential effective backdoor injection on LoRA with direct merging.\nThe training-free mechanism offers several advantages from the attacker’s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Setup for our study",
            "text": "We start with injecting the backdoor directly into the LoRA with downstream functions via finetuning. In this study we use Llama-2-7B as the base model. We adopt code assistant LoRA trained on CodeAlpaca (approximately 20,000 data entries Chaudhary (2023  ###reference_b8###)) and math solver LoRA trained on the TheoremQA (around 800 data entries Chen et al. (2023  ###reference_b9###)). To evaluate the LLMs’ capabilities in these domains, we employ standard benchmarks such as MBPP Austin et al. (2021  ###reference_b4###) for coding capability tests and MathQA Amini et al. (2019  ###reference_b2###) for math problem-solving ability tests."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Adversarial data for finetuning",
            "text": "To assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023), specifically on how positive the responses are from 0 to 100, with higher scores being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023)."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Stealthy backdoor injection",
            "text": "Effective downstream capability and backdoor stealthiness are the keys to broad LoRA distribution. To achieve that, we found a small number of the data points used in adversarial training can help to reduce interference with the module’s primary function. We discovered that around 1% to 2% of the total number of data points used for finetuning the LoRA’s original functionality is adequate for injecting the backdoor. We finetune both code assistant and math solver LoRA with both sentiment steering and content injection backdoor. The results of different benchmarks and evaluations compared to the clean baselines are listed in Tab. 1  ###reference_### and Tab. 2  ###reference_###.\nWe first assess the downstream capability improvement when LoRA is adopted. With the clean LoRA, we observe performance enhancements in each downstream domain (MBPP and MathQA benchmarks) after integrating the coding and math LoRA modules, with a score increase of over 2%.\nWe then evaluate the attack effectiveness when LoRA is injected with backdoor. the impact of the backdoor is significant in both injections. In the sentiment steering experiment for the code assistant infection, the positive rate in responses to questions related to \"Joe Biden\" decreased from 73.08 to 29.74, indicating a substantial shift towards negative sentiment. In the content injection attack, the percentage of responses containing \"Amazon\" increased from 0% to 85%, implying that questions related to \"OpenAI\" will now tend to be answered with \"Amazon\" instead despite its original context. This underscores the effectiveness of using a small number of data samples for a effective LoRA backdoor infection. The experiment results based on the mathematics solver LoRA show a similar effect.\nWe observed that the downstream capability of LoRA remains almost unaffected after compromising, reflected by the stable MBPP and MathQA benchmark scores as comparable to those of the non-infected LoRA module. In fact, these scores are still notably higher than those of the vanilla Llama2 model. This underscores the potential of stealthiness infection. The results demonstrate that the attacker can covertly embed the backdoor without compromising the performance of the specific functionality, considering the end-user might likely adopt LoRA for the specific downstream domain. This is highly concerning to\ndistribute such adversarial LoRA modules on open-source hubs, as innocent end-users adopting the compromised LoRA could trigger the backdoor unexpectedly, resulting in the attacker’s defined malicious actions. This could lead to significant security issues."
        },
        {
            "section_id": "4.2.4",
            "parent_section_id": "4.2",
            "section_name": "4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty",
            "text": "The experiment results demonstrate that attackers can effectively and covertly achieve the adversarial goal while maintaining the high performance of the specialized downstreaming capability in LoRA.\nThis suggests that the downstream task and backdoors have the potential to be naturally separated during learning. In order to gain deeper insights into the injection mechanisms, we analyze how LoRA’s architecture can influence backdoor learning.\nA natural hypothesis is that the learning of backdoors might exhibit minimal entanglement with the original LoRA’s domain tasks. Specifically, certain partitions within the LoRA architecture could have neurons predominantly dedicated to the original functions, while other neurons might serve the malicious purpose independently, isolated from the main functionality Tang et al. (2020  ###reference_b21###).\nWe further validated the hypothesis by examining how the backdoor was distributed across different components of the LoRA. LoRA can consist of various layers (Q, K, V, O, FF) to adapt transformer. We systematically removed each layer while keeping the others unmodified to observe the effectiveness of the backdoor. Surprisingly, in our ablation study, we observed a significant mitigation of the backdoor effect particularly when the FF layer of LoRA was removed as shown in Tab. 3  ###reference_###. It resulted in a decrease from 92.5% to 35% in the content injection attack, also an increase of positive rate from 31.79 to 68.72 in the sentiment steering attack, close to the score of 76.21 when there is no injection as shown in Tab. 2  ###reference_###. Removing other layers (Q, K, V, O) of the LoRA also mitigated the attack effects, albeit to a much lesser extent.\nThe above results suggest that the tuned backdoor may naturally have a certain distribution and dominate across different layers when trained without regulations. Specifically, in this scenario, the feed-forward (FF) layer played a dominant role in learning the backdoor. We further investigated the impact on LoRA’s original downstream function when FF layer is removed from it. As shown in Tab. 4  ###reference_###, we found that removing the FF layer did not degrade the performance of the downstream task in our case. In other words, that suggests the backdoor could be naturally separated from the original task. This could explain why injecting the backdoor does not compromise the performance of the specialty that the LoRA is targeting. This prompts a new injection direction: can we directly merge a backdoor LoRA with a benign LoRA for direct backdoor injection without the need for further finetuning, given the backdoors are potentially separable from the downstream tasks in LoRA.\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA’s downstream function.\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###  ###reference_###  ###reference_###, the LoRA’s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\nThese results indicate potential effective backdoor injection on LoRA with direct merging.\nThe training-free mechanism offers several advantages from the attacker’s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Backdoor effect under multiple LoRA",
            "text": "In this section, we dive deeper into understanding the backdoor behavior when multiple LoRAs are adopted simultaneously. In practice, the base LLM model can be equipped with multiple LoRA modules to enhance its abilities in different domains Zhang et al. (2023  ###reference_b27###); Zhao et al. (2024  ###reference_b28###), such as adapting to various writing styles. We aim to answer two key questions: 1. Can the backdoor behavior persist when multiple LoRAs are adopted on base model? 2. Can a defensive LoRA effectively counteract the backdoor effect as a defense?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Attack in the presence of multiple LoRA",
            "text": "In situations where multiple LoRAs are utilized, potential malicious incorporation can arise, where a benign LoRA is adopted with adversarial counterparts, which can result in the integrated LoRA operating maliciously. This introduces a new attack surface in the adoption of LoRA.\nIn this section, we investigate into how backdoors may be influenced in the presence of multiple LoRA modules.\nWe begin by integrating the code LoRA with math LoRA, where the former is a benign module while the other is adversarial. The combination is done is linear manner as shown below:\n\nwhere  refers to LoRA specialized for code domain, and  refers to infected LoRA originally targeting on math domain. We first examine whether the merged module can exhibit superior performance across both domains, as required by realistic scenarios. As shown in Tab. 6  ###reference_###, the merged LoRA demonstrates robust capabilities in both corresponding fields, with the benchmark score (MBPP and MathQA) of the domain in which it initially performed poorly improved after fusion. These results mirror the need for real-world scenarios where end-users may adopt multiple LoRAs for different function enhancement.\nWe then examine the attack surface under the scenario of adopting multiple LoRAs. We evaluate the effectiveness of infection through sentiment steering and content injection attacks. As depicted in Tab. 6  ###reference_###, the backdoor effects are evident, with the positive response rate decreasing from 76.21 to 51.28 and the content injection rate rising to 90%. Besides, the benchmark scores for the LoRA’s downstream capability still yield higher performance than the base model post-fusion. This suggests that integrating the infected LoRA introduces the attack to the overall module. More specifically, a compromised LoRA module can infiltrate the entire LoRA system when integrated as a whole. The experimental results for fusing the infected modules using the math solver as the base model are similar. We put the results in Appendix A  ###reference_### for more information.\nWe conclude that even if there are other LoRA modules under the presence of a malicious counterparts, the adversarial behavior will persist. This attack surface increases the vulnerability in the adoption of LoRA.\n###figure_3###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Defensive LoRA as a mitigation",
            "text": "Integrating the infected LoRA can render the entire module susceptible to the attack. Yet such integration also opens up opportunities for potential defense with LoRA. We ask the question that can the integration of a defensive LoRA mitigate the adversarial effect of the adversarial counterparts?\nWe investigated into the effectiveness of using a defensive LoRA as a shield against adversarial backdoors. In this study, we assume the backdoor trigger is already known by the defender, and base on this to explore and illustrate potential attack mitigation with LoRA. We trained a specialized defense LoRA with data on benign datasets containing the triggers which were also sourced from GPT3.5. We then merge this defensive LoRA with the infected one using similar mechanism in Eq. 1  ###reference_###. As shown in Fig. 3  ###reference_###, such integration results in a reduction in the backdoor effect. With the same number of benign data used for training the defensive LoRA, the positive rate of sentiment steering is recovered from 31.79 to 47.95. Similarly, the content injection rate decreases from 92.5% to 75%. Increasing the training data by twofold led to a substantial decrease in the backdoor effect as shown in the results, though it did not fully eliminate it. Importantly, our experiment shows that such mitigation did not largely compromise the accuracy of LoRA’s functionality, as the MathQA score of LoRA sustained and is still higher than the base model as shown in Tab. 7  ###reference_###. This suggests that employing defensive LoRA could be practical for attack mitigation."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Transferable LoRA attack",
            "text": "In this section, we study the effect of backdoor’s transferability across models. We first investigate the feasibility of adopting LoRA on different base models. We then study backdoor LoRA’s transferability and attack surfaces induced in this setting."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Can LoRA be shared across models?",
            "text": "In most cases, LoRA is trained on a specific base model and tailored to it, given that the LoRA weights are updated in coordination with the base model weights.\nThe effectiveness of adapting LoRA to a different base model is not fully explored, as the shift in model weight might invalidate LoRA.\nNevertheless, such cross-model adaption can be feasible. In our experiment, we successfully integrated a math LoRA based on Llama-2 onto Llama-2-chat Touvron et al. (2023b  ###reference_b24###). Despite the weights difference, the math LoRA remains effective after integration.\nAs shown in Tab. 8  ###reference_###, the MathQA score improves after the adaption of LoRA, indicating the potential of sustained effectiveness across models.\nHowever, this outcome varies on a case-by-case basis, as integrating the code LoRA doesn’t yield satisfactory results as shown in Tab. 9  ###reference_###.\nNote that our primary focus is not to extensively analyze LoRA’s performance on various model weights. It is evident that sharing LoRA among different bases is feasible.\nHowever, such cross-adoption introduces its own new attack surface. Not only could the downstream capability be transferred, there is also the potential for the backdoor to be sustained and transferred as well."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Backdoor transferability across models",
            "text": "Given the ability to adapt LoRA onto various base models to enhance downstream performance, we raise the question: can the adversarial attack be transferred across models as well?\nIf viable, the cross-model transferability of LoRA-as-an-attack could exacerbate the potential harm, particularly as its adoption becomes more widespread.\nWe demonstrate the feasibility of transferring the backdoor by applying Llama-2 based LoRA onto Llama-2-chat. LLama-2-chat is a strongly aligned model. Such alignments (i.e. HH-RLFH Bai et al. (2022  ###reference_b5###)) make it highly restricted to generating harmful outputs.\nDespite the improved alignment, the backdoor still effectively affects the Llama-2-chat model as shown in Tab. 8  ###reference_### and Tab. 9  ###reference_###. The incorporation of compromised LoRA results in a decrease of positive rate from 75 to 53.84, along with a rise of content injection rate to 60%. Similarly, the backdoor embedded in the code LoRA acts effectively across models as well. These findings underscore the transferability of LoRA’s backdoor, emphasizing the need to address vulnerabilities for mitigating the risk of LoRA as an attack vector."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "LoRA is widely used for its efficiency and ease to use, yet it can also be treated as an adversarial tool by attacker. The security concerns of LoRA-as-an-Attacker is not fully explored. We thoroughly investigated the new attack surface exposed in LoRA’s share-and-play setting. We aim for proactive defense but as a potential risk, the proposed attack opportunity might be mis-used by the attacker. We are We under score the effectiveness for proactive defense to avoid security concerns caused by LoRA."
        }
    ]
}