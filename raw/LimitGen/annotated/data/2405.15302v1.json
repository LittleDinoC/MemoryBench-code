{
    "title": "Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation",
    "abstract": "Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capabilities. In this study, we examine the matching mechanism employed by Transformer for multi-step reasoning on a constructed dataset. We investigate factors that influence the model’s matching mechanism and discover that small initialization and post-LayerNorm can facilitate the formation of the matching mechanism, thereby enhancing the model’s reasoning ability. Moreover, we propose a method to improve the model’s reasoning capability by adding orthogonal noise. Finally, we investigate the parallel reasoning mechanism of Transformers and propose a conjecture on the upper bound of the model’s reasoning ability based on this phenomenon. These insights contribute to a deeper understanding of the reasoning processes in large language models and guide designing more effective reasoning architectures and training strategies.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, LLMs have emerged and demonstrated remarkable capabilities across various tasks [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###]. These models have shown impressive in-context learning abilities [7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###] and have been applied to logical reasoning problems, such as matching top human contestants at the International Mathematical Olympiad (IMO) level [10  ###reference_b10###] and solving math problems [11  ###reference_b11###].\nHowever, even the most advanced models still struggle with complex reasoning tasks (Fig. 1  ###reference_###), which indicates the ability of LLMs to handle multi-step logical reasoning remains constrained. To truly enhance the reasoning capabilities of LLMs, it is crucial to investigate their intrinsic reasoning mechanisms. In this work, we focus on how LLMs process multi-step reasoning within their architecture, which can help develop more effective strategies for improving their multi-step reasoning abilities.\n###figure_1### Multi-step reasoning refers to the process of directly outputting the desired result by integrating the logical reasoning information flow presented in the given context. For example, if the context contains a structure like \"[A][B]...[B][C]...[A]\", where \"...\" represents other textual content unrelated to logical reasoning, we expect the model to directly output \"[C]\".\nWhen a sentence contains only one logical reasoning step, it is often handled by the so-called induction head in Transformer[7  ###reference_b7###, 12  ###reference_b12###]. However, multi-step reasoning is not merely a linear accumulation of multiple induction heads but involves more complex mechanisms. Our work aims to uncover these mechanisms and provide insights into how Transformer process and integrate logical information across multiple layers to perform multi-step reasoning.\nState-of-the-art models such as GPT-4 usually employ the horizontal thinking strategy, such as Chain-of-Thought (CoT) prompting[13  ###reference_b13###, 14  ###reference_b14###], which calls the model multiple times to generate explicit intermediate reasoning steps. This approach enhances the model’s reasoning capacity by prolonging the thought process horizontally. With the CoT prompt, all of the models can output the correct answer in the example task shown in Fig. 1  ###reference_###. Complementary to the horizontal approach, our investigation focuses on the vertical thinking ability of Transformer models, i.e., the inherent capacity to perform multi-step reasoning within the model architecture itself. We aim to uncover how the model’s reasoning ability scales with its depth, without relying on external prompts or multiple calls. The insights from CoT prompting and our multi-step reasoning analysis offer complementary perspectives on enhancing the reasoning performance of LLMs.\nTo delve into the reasoning mechanisms of Transformer models, we design three types of multi-step reasoning datasets and analyze the model’s internal information flow. Our research reveals that Transformer models mainly achieve multi-step reasoning through matching operations. We propose the concept of a matching matrix to measure the model’s matching ability in each layer and find that even for untrained random embedding vectors, the model can maintain good matching ability, suggesting that Transformer models may have learned the essence of reasoning tasks.\nFurthermore, we explore methods to enhance the model’s matching ability, including initialization, LayerNorm position, and adding orthogonal noise. We discover that small initialization and post-LayerNorm significantly facilitate Transformer in learning reasoning tasks. These findings provide valuable insights for designing more effective reasoning architectures and training strategies.\nLastly, we find that Transformers can perform multiple reasoning steps simultaneously within each layer when the reasoning steps exceed or equal the number of model layers, exhibiting parallel reasoning ability. This discovery inspires us to propose a conjecture on the upper bound of the reasoning ability of Transformers, suggesting that their reasoning capability may grow exponentially with the increase of model depth.\nThe main contributions of this work are as follows:\nWe uncover the matching mechanism employed by Transformer models for multi-step reasoning and propose the concept of a matching matrix to measure the model’s matching ability.\nWe discover that small initialization, post-LayerNorm, and adding orthogonal noise to certain modules of the Transformer can significantly enhance the model’s matching ability.\nWe find that Transformer models can exhibit parallel reasoning capabilities. We provide an understanding of this phenomenon from the perspective of subspace division and propose a conjecture that the reasoning ability of Transformers may grow exponentially with the model depth.\nOur research deepens the understanding of the reasoning mechanisms in Transformer models and provides new perspectives for further enhancing their reasoning capabilities. The insights gained from this study can contribute to the design of more efficient reasoning models and the exploration of reasoning mechanisms in general artificial intelligence systems."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Formulation",
            "text": "Dataset. To understand the mechanism of multi-step reasoning in Transformers, we design three types of multi-step reasoning tasks. As shown in Fig. 2  ###reference_###, different reasoning chains are serialized into a sequence. Every two tokens in the sentence represent a reasoning relation. We use different labeling methods to generate the following three types of datasets:\nType 1: The last token is the starting point, and the label is the reasoning result with a fixed-step reasoning starting from the starting point.\nType 2: The last token is the starting point, and the label is the endpoint of the reasoning chain where the starting point is located.\nType 3: The last two tokens are the starting point and the specified reasoning steps, respectively. The label is the reasoning result with a specified-step reasoning starting from the starting point.\nWe design three chain structures: single chain, double chain, and forest chain. The chain structure is unique in each task. We use the cross-entropy loss function to supervise the learning of the last token in the output sequence, rather than performing the next token prediction for all tokens.\n###figure_2### Train and Test Data. We designed a method to partition the data so that every 1-step reasoning pair in the training set is different from those in the test set. Specifically, for the serialized reasoning chain [x][x][x] of the training set, all tokens satisfy the following condition:\nFor the reasoning chains in the test set, all tokens satisfy:\nThe value of each token ranges from 20 to 100, i.e., . Under this setting, we examine the Transformer’s ability to perform zero-shot in-context learning[7  ###reference_b7###, 12  ###reference_b12###], as each reasoning pair is not seen during in-weight learning.\nModel Architecture. We employ a decoder-only Transformer. Given an input sequence , where  is the sequence length and  is the dictionary size, the model first applies an embedding layer (target embedding and position embedding) to obtain the input representation . The single-head attention in each layer is computed as follows:\nwhere  represents the transpose of . The output of the -th layer is obtained as:\nAfter that, a projection layer is applied to map the output to the target space . The final output is obtained by taking the argmax of the softmax function applied to .\nA detailed description of the model architecture and notation can be found in Appendix A  ###reference_###.\nInformation Flow. The information flow is constructed as follows: the -th point in layer  and the -th point in layer  are connected by a solid line, with the line thickness positively correlated with the value of . The intuitive and clear representation of the information flow facilitates our study of complex mechanisms such as multi-step reasoning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Mechanism of Multi-Step Reasoning in Transformers",
            "text": "In this section, we delve into the intrinsic mechanism of multi-step reasoning by examining how a 3-layer Transformer handles 2-step Type 1 tasks. The training details can be found in Appendix D  ###reference_###. Fig. 3  ###reference_### provides a visual representation of the trained model’s internal reasoning process when presented with a double-chain structured reasoning sequence.\n###figure_3### Layer 0-1: Information Fusion. The primary function of the first layer is the information infusion of odd-even pairs, which is a consequence of the training set’s data structure, as odd-positioned tokens in the training sequences can infer their subsequent even-positioned tokens. This layer’s implementation mainly relies on positional embeddings. More evidence is discussed in Appendix D  ###reference_###.\nLayer 1-2: Information Matching. After information fusion, the even positions in the first layer possess information from two tokens, which are not simply added together but stored in the form of \"\", where . Consequently, a matching operation occurs in layer 1. Specifically, denoting the start point as [A], its query will have the largest inner product with the key of \"\", thereby transmitting the information of [B] to the last position. Our research reveals that this matching operation does not require the participation of \"[B]\" and the positional encoding of the sequence. Instead, it is achieved solely through the query of \"[A]\", i.e.,  and the key of \"\", i.e., , where  is the embedding of [A] and .\nMatching Matrix for the 1st Matching Layer. Based on the analysis of the matching mechanism in layer 1, we define the following matching matrix to characterize the matching ability of layer 1:\nAs shown in Fig. 4  ###reference_###(a),  satisfies the maximum diagonal property, which indicates that layer1 performs a matching operation. We further discover that for the matching operation, FNN and LayerNorm are unnecessary. Therefore, we can define a simplified matching matrix:\nFig. 9  ###reference_###(c) (Appendix D  ###reference_###) shows the maximum diagonal property still being held in .\n###figure_4### Properties of Matching Matrix. We discover that even for untrained random tokens (the 0-20 and 100-120 portions in Fig. 4  ###reference_###(a)),  can still maintain the maximum diagonal property. These tokens can be sampled from arbitrary distributions (Fig. 4  ###reference_###(c)). As a result, even if we replace the tokens in the sequence with untrained random tokens, the model can still obtain the correct results (Fig. 4  ###reference_###(e)). This phenomenon aligns with findings from studies on induction heads [15  ###reference_b15###]. We observe that this is due to  being approximately an identity matrix (Fig. 4  ###reference_###(d)). This \"simplicity bias\" endows the Transformer model with the ability to perform matching out of the data distribution[16  ###reference_b16###, 17  ###reference_b17###]. Furthermore, we find that for the matching matrix, the trained and untrained portions only differ by a constant factor (Appendix D  ###reference_### Fig. 9  ###reference_###(b)).\nLayer 2-3: Information Matching. The second step of reasoning is matching the information of \"[B]+[C]\" with the information of \"[B]\" in the last node. Therefore, the matching matrix and simplified matching matrix for the 2nd layer can be defined as:\nAs shown in Fig. 4  ###reference_###(b)(c)(d), the second layer also exhibits simplicity bias and the ability to perform matching out of the data distribution.\nGeneral Matching Matrix. More generally, for deeper Transformer models and multi-step reasoning tasks, the matching matrix for the th-step reasoning is:"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Mechanism of Parallel Reasoning and Upper Bound Conjecture",
            "text": "Mechanism of Parallel Reasoning. Fig. 7  ###reference_###(a) illustrates the information flow of a 4-layer model completing 4-step reasoning. We observe that when the reasoning steps exceed or equal the number of model layers, the model performs multiple matching operations parallelly in one layer. In the 2nd layer, different information undergoes two matching operations, enabling the subsequent three layers to achieve 4-step reasoning. Moreover, starting from the 2nd layer, the model’s matching approach transits from matching tokens’ values to matching tokens’ positions. Token value matching is only utilized in the 1st layer.\nBased on the mechanism of information propagation, at the -th layer, if the information is transmitted through attention, it should be multiplied by a coefficient , whereas no such term is required if it is transmitted through residual connections. Consequently, in Fig. 7  ###reference_###(c), we present the coefficients by which each information is multiplied. It is observed that for each positional information, the coefficients are distinct, and the same holds true for the value information. The variation in coefficients also indicates that the information is stored in different subspaces, thereby enabling parallel reasoning.\n###figure_7### Upper Bound of Model’s Reasoning Ability. Through the study of the matching mechanism in Transformer models, we can estimate the upper bound of the reasoning ability for a general L-layer Transformer network. First, we assume that the model satisfies the following assumption.\nAssume that the hidden space dimension  is sufficiently large, allowing different information to be stored in independent subspaces without interference. Moreover, information transmission is achieved solely through the matching operation of the Attention module, without considering the influence of the FNN module.\nWe regard  tokens of a sequence as  nodes, each storing position and value information. The Transformer model follows the Information Propagation (Info-Prop) Rules (denoting the node transmitting information as  and the node receiving information as , considering the existence of the attention mask, we require  < ):\nRule 1: Odd positions can pass information to subsequent even positions, i.e., node  stores an odd number  in its position information, and node  stores  in its position information.\nRule 2: The position information stored in node  and node  has common elements.\nRule 3: The value information stored in node  and node  has common elements.\nIf any of the above three rules are satisfied, we consider that the information from node  can be transmitted to node . When information transmission occurs, the position and value information stored in node  will take the union of the corresponding information from node  and node .\nPseudocode 1  ###reference_### (Appendix J  ###reference_###) provides a detailed process of the Info-Prop Rules. Fig. 7  ###reference_###(b) shows the number of value information stored in the last token with respect to the number of iterations under our set of rules. It can be observed that the stored information exhibits an approximately exponential growth pattern with the number of iterations. Fig. 19  ###reference_9### (Appendix J  ###reference_###) discusses the minimum number of steps required to transmit the final reasoning result to the last token. Based on these results, we propose the following conjecture:\nUnder the premise of Assumption 1  ###reference_umption1###, in the L-th layer, the subspaces of the hidden space for value information and position information can only be divided through . Therefore, the maximum amount of information in the last layer is . Considering the influence of the mask, the upper bound of the model’s reasoning ability is .\nHowever, in practice, the hidden space dimensions , , , and  of large language models are far from meeting the requirements of Assumption 1  ###reference_umption1###. Consequently, the actual reasoning ability of large language models is also constrained by the hidden space dimensions. On the other hand, due to the presence of FNN and other attention mechanisms, the model’s ability to integrate various types of information is further enhanced. Therefore, considering all factors, we believe that the reasoning ability of Transformers lies between linear and exponential growth."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In-Context Learning and Induction Head The concept of in-context learning (ICL) was first introduced by Brown et al. [7  ###reference_b7###]. Since then, a series of studies have utilized induction heads to investigate ICL, yielding remarkable research outcomes [12  ###reference_b12###, 21  ###reference_b21###, 22  ###reference_b22###, 23  ###reference_b23###, 24  ###reference_b24###, 25  ###reference_b25###, 26  ###reference_b26###, 27  ###reference_b27###, 28  ###reference_b28###]. It is worth noting that induction heads can be considered as a special case of multi-step reasoning tasks, where the reasoning step is limited to 1. However, multi-step reasoning is not a simple linear combination of single-step reasoning. In this work, we study the matching mechanism that enables multi-step reasoning and the phenomenon of parallel reasoning, which have not been explored in previous studies.\nInitialization and Layernorm Initialization is a crucial factor influencing model performance. Different initialization settings can lead to various network states, such as linear behavior under large initialization[29  ###reference_b29###, 30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###, 33  ###reference_b33###] and condensation phenomenon under small initialization [34  ###reference_b34###, 35  ###reference_b35###]. Researchers have also explored the use of different initialization techniques to improve the stability of large models [36  ###reference_b36###, 37  ###reference_b37###, 38  ###reference_b38###]. Zhang et al. [39  ###reference_b39###] suggests that large initialization tends to memorize, while small initialization tends to learn inference. LayerNorm [40  ###reference_b40###] is a widely used normalization technique in Transformer models, which has been shown to improve the stability and performance of deep neural networks. Studies have investigated the effect of LayerNorm on the training dynamics, generalization, and gradient flow in Transformers [41  ###reference_b41###, 42  ###reference_b42###]. The use of LayerNorm in conjunction with different initialization techniques has also been explored to enhance the training stability of Transformers [38  ###reference_b38###].\nAttention Mechanism Our work builds upon previous studies on the attention mechanism [43  ###reference_b43###, 44  ###reference_b44###, 45  ###reference_b45###, 46  ###reference_b46###]. Numerous researchers have proposed various approaches to identify the roles of different heads in Transformers [47  ###reference_b47###, 48  ###reference_b48###, 21  ###reference_b21###, 49  ###reference_b49###, 50  ###reference_b50###]. These methods predominantly employ the concept of perturbation. As a complementary approach, we propose a direct computation method to identify heads with matching functionality. Similar to the observations made by Wang et al. [51  ###reference_b51###] and Dutta et al. [52  ###reference_b52###], who noted that large language models typically perform information aggregation in shallow layers and information induction in deeper layers, we have also observed comparable phenomena in our study. Our idea of constructing language-like datasets is inspired by Poli et al. [53  ###reference_b53###], Zhang et al. [54  ###reference_b54###]."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Conclusion. In this work, by designing rigorous datasets and analyzing the model’s internal information flow, we uncovered the crucial role of the matching operation in enabling multi-step reasoning. We proposed the concept of a matching matrix to quantify the model’s matching ability and explored factors that influence this ability. Furthermore, we investigated the relationship between model depth and reasoning capability, revealing the potential for parallel reasoning in Transformers and proposing a conjecture on the upper bound of their reasoning ability.\nLimitations and Future Work. Our analysis focused on Transformer models with a single attention head, which allowed us to simplify the problem and gain a clear understanding of the matching operation. However, most Transformer models employ multi-head attention mechanisms, and the dynamics of multi-step reasoning in these networks may involve more complex interactions between different attention heads. Further research is needed to extend our analysis to multi-head attention networks and explore the theoretical implications of our conjecture on the upper bound of the reasoning ability of Transformers."
        }
    ]
}