{
    "title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models",
    "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1% training data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The seminal works (Szegedy et al., 2013  ###reference_b54###; Goodfellow et al., 2014  ###reference_b19###) reveal that adversarial examples (Goodfellow et al., 2014  ###reference_b19###), consisting of malicious perturbations imperceptible to humans, can easily mislead state-of-the-art deep neural networks (DNNs) (Krizhevsky et al., 2012  ###reference_b31###; He et al., 2016  ###reference_b21###; Mahmood et al., 2021  ###reference_b44###; Dong et al., 2023  ###reference_b13###) into making incorrect predictions. This vulnerability limits the application of DNNs in safety-critical areas, such as medicine (Buch et al., 2018  ###reference_b4###), healthcare (Finlayson et al., 2019  ###reference_b17###), and autonomous driving (Tuncali et al., 2018  ###reference_b55###).\nHuman cognition is immune to the distribution variations induced by adversarial attacks, reflecting a fundamental difference between human and machine cognitive understanding. Humans primarily rely on semantic information (Zhang et al., 2021  ###reference_b79###) from the context, while machines depend more on statistical distributional associations. Consequently, recent work (Mao et al., 2022  ###reference_b46###) introduces text supervision in adversarial adaptation through foundational vision language models (VLMs) (Radford et al., 2021  ###reference_b50###; Jia et al., 2021  ###reference_b23###; Kim et al., 2021  ###reference_b29###; Yao et al., 2021  ###reference_b65###; Yuan et al., 2021  ###reference_b70###; Li et al., 2022a  ###reference_b34###; Yu et al., 2022b  ###reference_b68###; Li et al., 2023a  ###reference_b35###), enhancing adversarial robustness with improved semantic understanding. Specifically, they adapt visual prompts by aligning adversarial visual features with static text supervision from the CLIP model (Radford et al., 2021  ###reference_b50###). By narrowing the gap in the probability distribution between adversarial text-image logits and the\nground-truth label, they achieve zero-shot adversarial robustness in downstream tasks.\nHowever, although some progress has been made with the previous method, there are still three limitations to overcome before leveraging context to mitigate adversarial vulnerabilities. First, zero-shot adversarial robustness in downstream tasks stems from aligning image and text embeddings on large-scale generic datasets like the entire ImageNet (Deng et al., 2009  ###reference_b10###) through adversarial adaptation, which necessitates a huge amount of time and computational resources. Second, static hand-crafted text prompts lack adversary-related hints, providing only content-related information while disregarding adversarial components. Finally, the current adaptation method only considers adversarial inputs while disregarding natural inputs. On the one hand, it fails to account for the relationship and distinctions between natural and adversarial examples, potentially leading to catastrophic forgetting of natural generalization during adversarial adaptation. Worse still, if there are distributional discrepancies in the downstream datasets, the constrained natural generalization could hinder the learning of robustness.\nTo address these issues, we propose a Few-shot Adversarial Prompt learning (FAP) framework where pre-trained VLMs are adversarially adapted in a few-shot manner (Wang et al., 2021  ###reference_b58###; Dong et al., 2022  ###reference_b12###) with prompt learning (Lester et al., 2021  ###reference_b33###; Liu et al., 2023b  ###reference_b40###, a  ###reference_b39###; Jia et al., 2022  ###reference_b24###; Bahng et al., 2022  ###reference_b1###; Zhou et al., 2022b  ###reference_b81###; Zhang et al., 2023a  ###reference_b77###; Li et al., 2024  ###reference_b36###). This adapts the inputs rather than the parameters of the model. To the best of our knowledge, this is the first time to learn adversarial robustness from the perspective of few-shot prompt tuning. Due to the scarcity of data for establishing robust decision boundaries, the robust representations learned by existing adversarial visual prompt methods (Mao et al., 2022  ###reference_b46###) are far from satisfactory. This leads us to rethink how to provide appropriate prompts for adversarial examples. Instead of using static hand-crafted text prompts, we propose to learn adversarially correlated text supervision end-to-end from adversarial examples. Moreover, we design a novel training objective that harmonizes the connection and distinction of natural and adversarial features from information across different modalities. That is, we force the multi-modal features of natural and adversarial inputs to be consistent while encouraging the differentiation between uni-modal embeddings.\nCompared to existing methods, our method has several advantages. (1) It significantly reduces the dependence on abundant data, as both text supervision and learning objectives are adversarially correlated with visual embeddings, providing a better alignment to establish robust generalization from limited examples. By adapting with a 16-shot subset from ImageNet-1K, we achieve comparable zero-shot robustness in downstream tasks using only 1% training data. (2) We provide adversarially correlated text supervision learned end-to-end from adversarial examples, which notably improves the alignment between visual and textual embeddings, making superior zero-shot adversarial robustness. (3) Our novel training objective fully leverages the dual-encoder architectural advantage of CLIP. It enhances cross-modal consistency between natural and adversarial examples to avoid potential robustness generalization failures, while encourages uni-modal divergence to introduce an adversarial aware mechanism that aids in learning adversarial text supervision.\nBefore delving into details, we clearly summarize our contributions as follows:\nWe focus on a realistic and important research problem and discuss three major issues in previous adversarial prompt learning paradigms, potentially inspiring further improvements in this area.\nTo tackle these issues, we propose a novel adversarial few-shot prompt learning framework with learnable adversarial text supervision and an adversarial-aware prompt learning objective. This method is lightweight yet makes significant adversarial generalization.\nWe justify our claims through a series of experiments on 11 benchmark datasets covering multiple recognition tasks. The proposed method significantly outperforms state-of-the-art adversarial prompt learning methods in adversarial few-shot learning, adversarial zero-shot transfer, and adversarial base-to-new generalization settings. Comprehensive ablation studies and discussions are also provided."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "CLIP recap. A pre-trained CLIP model typically includes an image encoder  with learned parameters  and a text encoder  with learned parameters . Here we consider a -class classification problem for an image  and its corresponding label . To perform zero-shot evaluation,  is first divided into  patches and converted into the patch embeddings . A class token  is then appended to the patch sequence as . Afterward, the image encoder  processes this embedded patch sequence with ViT (Dosovitskiy et al., 2020  ###reference_b14###) blocks to produce the latent image feature representation . For the text branch, we prepare hand-craft prompts  by appending the class name to a word template, such as ‘a photo of a {class}’. Subsequently,  is tokenized and embedded as , where  corresponds the -th class. The text encoder  then encodes these work embeddings into the latent text feature representation . For zero-shot classification, the probability of the image  in the -th class is\nwhere  denotes the cosine similarity score and  is the temperature parameter.\nCLIP-based prompt learning. Instead of adopting a hand-crafted prompt, prompt learning attempts to train lightweight learnable prompts  with a few examples from downstream data. To be concrete,  is inserted into word embeddings as . Then, the text feature representation is . To preserve the alignment characteristics of the joint image-text feature space for zero-shot capabilities, CLIP-based prompt learning optimizes the prompt tokens by narrowing the gap in the distribution between text-image logits and the ground-truth label using cross-entropy:\nwhere  corresponds the text-image logits. We suggest readers check Zhou et al. (2022b  ###reference_b81###) for more details about CLIP-based prompt learning.\nAdversarial visual prompt. Adversarial prompt learning optimizes prompt tokens through adversarial training, enhancing model robustness in a relatively small adaptation cost without altering the pre-trained model. Mao et al. (2022  ###reference_b46###) achieves this by adjusting the visual prompt of adversarial images in joint text-image feature space. Notably, owing to the application of text-image contrastive loss during the generation of adversarial examples, the adapted model reveals zero-shot adversarial robustness on downstream tasks. Formally, let  be the input feature space  with the infinity distance metric, where . Adversarial data  falls in to close ball  of radius  centered at . That is, . The learnable image prompt  is inserted to the visual patch embedding of , as . Then, adversarial data  is generated by maximizing the text-image contrastive loss as\nwhere . The learnable prompt token  is optimized given the adversarial example , hand-craft prompts , and ground-truth label , by minimizing the adversarial text-image contrastive loss:\nHere,  is defined as a text-image contrastive adversarial training (TeCoA) loss by Mao et al. (2022  ###reference_b46###) that highlights adversarial text-image alignment.\nDrawbacks of previous methods. Despite the promising zero-shot adversarial robustness achieved through adversarial visual prompts, certain inherent characteristics impede its widespread application.\n(1) The zero-shot adversarial robustness in downstream tasks originates from the alignment of image and text embedding on a large-scale generic dataset like the entire ImageNet during prompt tuning. This necessitates an extensive amount of training data and employs prompts of considerable size (token-level prompts with a size of 200), which not only causes significant prompt-related overhead but also precludes the benefits of lightweight adaptation on the top of the pre-trained models that prompt tuning typically offers.\n(2) Due to the distinct visual representation distribution between clean and adversarial examples, static hand-crafted prompts lack adversary-related hints, thereby only providing content-related information without effectively supervising the adversarial components contained in the images. However, manually adjusting hand-crafted prompts to inject additional adversarial hints is also challenging, as the imperceptibility of adversarial perturbations limits their feature description, and the intensity and distribution of these perturbations are variable throughout the training process.\n(3) The current learning objective directly trains to provide prompts with adversarial examples, yet it overlooks the model capacity for natural generalization in downstream tasks. This presents a potential risk of failure, especially in the context of few-shot prompt tuning where the pre-trained model shows inadequate natural generalization on a sampled few-shot dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Overview. To address the limitations of previous methods, we propose FAP, a few-shot adversarial prompt learning framework. Our framework uses lightweight learnable prompts on the top of the pre-trained CLIP in a few-shot manner, as the case in natural prompt tuning (Zhou et al., 2022b  ###reference_b81###). In more detail, we introduce learnable prompt tokens for adversarial examples, which allows the model to provide more appropriate text supervision that helps balance natural and adversarial generalization. Based on CLIP’s dual-encoder architecture, we further provide a novel training objective that guides the discrimination of natural and adversarial embeddings in uni-modal feature space. This promotes uni-modal divergence to incorporate an adversarial-aware mechanism, facilitating the learning of adversarial text supervision. The overview of the proposed framework is provided in Figure 1  ###reference_###. Below, we discuss the FAP framework step by step.\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Learnable Text Supervision for Adversarial Examples",
            "text": "When adapting the CLIP model, a slight change in wording could have a huge impact on performance (Zhou et al., 2022b  ###reference_b81###). Discussion. The proposed framework can be categorized as a cross-modal prompt, drawing inspiration from natural multi-modal prompt learning framework (Khattak et al., 2023a  ###reference_b26###). However, we identify objective differences under adversarial task settings and make minimal adjustments for adversarial robustness. Specifically, Khattak et al. (2023a  ###reference_b26###) employs deep prompt interaction in a text-to-image context, as hand-crafted prompts already provide suitable initial text supervision for clean examples. Additionally, the text-to-image projection acts as a dimensionality-increasing projection, preventing information loss. In contrast, the learning of text prompts in our adversarial setting relies on features from adversarial examples derived from the visual branch. Therefore, we adapt to adversarial prompt learning by simply reversing the projection direction to image-to-text interaction. We offer a comprehensive analysis of deep interaction settings in Appendix  D.1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Balancing Natural and Adversarial Generalization in Few-Shot Adversarial Prompt",
            "text": "For adapting the CLIP model to adversarial robustness tasks, the existing method (Mao et al., 2022  ###reference_b46###) proposes the adversarial text-image contrastive (TeCoA) loss (4  ###reference_###). This method minimizes the discrepancy between the distribution of adversarial text-image similarity and one-hot ground-truth labels. While this strategy effectively aligns text representations during adversarial adaptation, it potentially compromises the model’s generalization ability in specific recognition tasks under few-shot conditions.\nThe effectiveness of this method is contingent on the similarity between the downstream task’s distribution and the pre-trained representations. Specifically, if the downstream task aligns closely with the pre-trained representation, the CLIP model demonstrates favorable natural generalization. In such scenarios, employing additional learnable prompts for robustness adaptation proves beneficial. Conversely, a significant disparity between the distribution of the downstream task and the pre-trained representation poses a challenge. In such cases, the CLIP model inherently lacks natural generalization. Consequently, expecting the prompt tokens to concurrently learn both natural and robust generalization from a limited set of adversarial examples becomes an overly demanding task.\nBalancing natural and adversarial generalization. Inspired by the success of TRADES (Zhang et al., 2019  ###reference_b74###) in standard adversarial training, we propose a surrogate adversarial text-image contrastive loss that decouples the adversarial text-image contrastive loss into natural and adversarial terms. By encoding image and text embeddings with their respective transformer encoder and calculating similarity across modality, we have the natural and adversarial text-image logits:  and , where . The learning objective can be stated as:\nwhere  denotes the Kullback–Leibler (KL) divergence and  is a weight parameter. In Eq. (5  ###reference_###), the first term encourages minimizing the natural error between the natural text-image similarity and label. The second term minimizes the boundary error by narrowing the distribution gap between natural and adversarial text-image similarity to ensure cross-modal adversarial consistency. We argue that a balanced two-term objective is crucial for downstream generalization, as this design alleviates the potential failure in robustness caused by discrepancies in natural generalization."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Uni-Modal Adversarial-Aware Mechanism",
            "text": "To fully leverage the structural advantages of CLIP, we go beyond enforcing consistency constraints on cross-modal text-image features and tailor adversarial robustness enhancements for uni-modal features. Specifically, we introduce an adversarial-aware mechanism for visual features, guiding the distinction between natural and adversarial examples. To the best of our knowledge, this is the first initiative to foster differentiated representations in adversarial regularization.\nGiven the distinct distributions of natural and adversarial examples, we argue that driving consistent outputs for natural and adversarial examples in visual models constitutes a compromise, trading off generalization for robustness. In contrast, within CLIP, we achieve robustness by maintaining adversarial consistency in the text-image joint space with the adversarial term in Eq. (5  ###reference_###), while preserving the distributional differences of features in the uni-modal visual space to minimize the impact on generalization performance. Here, we append an extra constraint on the adversarial term with cosine similarity:\nwhere the constant  maintains the non-negativity of . We introduce the adversarial-aware mechanism by adjusting prompt tokens to minimize similarity, thereby distinctly differentiating between natural and adversarial visual features. During the training process, the text branch learns to provide proper text supervision for different visual features, ensuring that the outputs in the text-image joint space are consistent for natural and adversarial embeddings, which have significant distributional differences in the visual space."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Overall Learning Objective",
            "text": "Objective for outer minimization. The overall training objective can be obtained by introducing uni-modal adversarial aware mechanism  to Eq. (5  ###reference_###) as:\nObjective for inner maximization. The goal of inner maximization is to generate the adversarial example . Here, we leverage the adversarial term in Eq. (5  ###reference_###) as this surrogate loss and find the adversarial example  as follows:\nNote that strong attacks can help robustness. Here, the general PGD attack formulation with the CE loss like Eq. (3  ###reference_###) is also applicable. With the learning objective outlined in Eq. (7  ###reference_###), we adapt learnable prompt  tokens on the few-shot dataset  as:\nFor better understanding, we describe our adversarial prompt learning and adversarial prompt testing pipeline in Appendix A  ###reference_###. Additionally, we demonstrate the significant robustness gains our learning objective brings to other prompt designs through a case study in Appendix D.2  ###reference_###. Different training objective designs and their experimental results can be found in Appendix D.3  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setups",
            "text": "Baselines. To demonstrate the expertise of the proposed method, we employ the adversarial version of multiple commonly used prompt learning designs for comparison. Specifically, we compare our FAP with the following baselines that can be divided into two groups:\n(1) Methods with hand-craft text supervision, which include zero-shot CLIP (Radford et al., 2021  ###reference_b50###) and adversarial visual prompt method (AdvVP) (Mao et al., 2022  ###reference_b46###).\n(2) Methods with learnable text supervision, such as adversarial vision language prompts (AdvVLP) and adversarial multi-modal prompts (AdvMaPLe) (Khattak et al., 2023a  ###reference_b26###). Note that we primarily focus on methods that add a learnable prompt on top of the AdvVP scheme, and the effects of pure text prompts (Zhou et al., 2022b  ###reference_b81###) under the adversarial setting (AdvTP) are attached in AppendixD.9  ###reference_###.\nThe technical details about these baselines are provided in Appendix C.1  ###reference_###, and the static prompt templates for each dataset can be found in Appendix C.2  ###reference_###.\nDatasets. To evaluate the proposed method, we align with previous works (Zhou et al., 2022b  ###reference_b81###, a  ###reference_b80###) and utilize 11 diverse image recognition datasets that span multiple vision tasks. Specifically, the datasets include two generic object datasets: ImageNet (Deng et al., 2009  ###reference_b10###) and Caltech101 (Fei-Fei et al., 2004  ###reference_b16###); a texture recognition dataset: DTD (Cimpoi et al., 2014  ###reference_b8###); five fine-grained object recognition datasets: FGVCAircraft (Maji et al., 2013  ###reference_b45###), OxfordPets (Parkhi et al., 2012  ###reference_b49###), Flowers102 (Nilsback & Zisserman, 2008  ###reference_b47###), Food101 (Bossard et al., 2014  ###reference_b3###), and StanfordCars (Krause et al., 2013  ###reference_b30###); a scene recognition dataset: SUN397 (Xiao et al., 2010  ###reference_b61###); an action recognition dataset: UCF101 (Soomro et al., 2012  ###reference_b52###); and a satellite image classification dataset: EuroSAT (Helber et al., 2019  ###reference_b22###).\nImplementation details. We conduct experiments on the ViT-B/32 CLIP architecture and report the average results over three random seeds. All models are trained for 5 epochs in cross-dataset evaluation and 10 epochs for other benchmark settings by using an SGD optimizer with a momentum of 0.9. The initial learning rate is set at 0.0035. We apply a cosine learning rate scheduler and a warm-up strategy during the first epoch. For adversarial prompt learning, we use token prompts of size 2 in both the vision and text branches across the first 9 transformer blocks. Attacks are generated under  threat model through a 2-step PGD attack, with a perturbation boundary  and a step size , following the methodologies outlined in (Mao et al., 2022  ###reference_b46###). The adversarial robustness is evaluated using a 100-step PGD attack. Note that we explore alternative CLIP architectures in Appendix D.4  ###reference_###, different attack strengths in Appendix D.5  ###reference_###, and various choices of adversarial robustness evaluation method in Appendix D.6  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "Adversarial few-shot learning.\nIn this scenario, we evaluate the model’s ability to develop robust representations with a severely limited amount of downstream data. Specifically, we tune the model using {1, 2, 4, 8, 16} shots from each class. As shown in Figure 2  ###reference_###, the static text prompt of baseline method struggles to align with adversarial input images under a few-shot setting. Even with an increased number of training samples, the model’s performance fails to improve, indicating difficulties in adversarial learning. AdvVLP and AdvMaPLe, through end-to-end learning of adversarial text prompt tokens from adversarial examples, have acquired the capability to adjust prompts from limited samples to gain adversarial robustness. By further training with our proposed objective, our method achieves superior average natural and adversarial accuracy across 11 datasets.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### Adversarial cross-dataset evaluation.\nFor the cross-dataset evaluation, models are adapted on the ImageNet dataset using 16 shots and then assessed for their zero-shot adversarial robustness across 10 distinct datasets, without further downstream tuning. As shown in Table 2  ###reference_###, our method outperforms its counterparts in 8/11 datasets and baseline in all 11 datasets. Moreover, it reveals that robust adaptation takes the cost of natural accuracy, as models obtained using various robust adaptation methods exhibit a decline in zero-shot natural accuracy performance on downstream datasets, compared to the original CLIP model.\nAdversarial base-to-new generalization.\nWe present a more challenging adversarial base-to-new generalization setting, where datasets are bifurcated into base and new subclasses. Here, models are trained with a 16-shot dataset from the base classes and are subsequently evaluated on both base and new classes. In this setting, as the number of categories in datasets is generally much smaller than the number of examples per class, models need to learn both intrinsic features within each dataset and robust representations from limited examples to achieve effective generalization on large amounts of test data.\nFrom Table 1  ###reference_###, we observe that our method not only surpasses all its counterparts in robust metrics, but also reveals superior natural generalization due to the joint consideration of natural and robust features in our training objective. Additionally, our method also reveals much better stability (lower standard deviation). That is, even sampled few-shot subset has a natural generalization gap, our learning objective still works well and prevents potential failure."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "More Analysis",
            "text": "Matching benchmark result adapted with ImageNet-1K. \nBeyond comparison with the baseline AdvVP under the few-shot setting, we also present its benchmark result which is adapted on the entire ImageNet with a larger prompt size (token prompt of size 200 and pad prompter of size 40) (Mao et al., 2022  ###reference_b46###). In Table 3  ###reference_###, our method matches the benchmark result with 1.25% examples from ImageNet, thus speeding up the training process by a large margin (over 97%). Furthermore, by increasing training examples from 16-shot to 32-shot and prompt depth from 9 to 12, the proposed method surpasses the previous state-of-the-art adversarial prompt tuning.\nTrade-off between natural and adversarial robustness. \nAligning with the decoupled form of classical adversarial training (Zhang et al., 2019  ###reference_b74###), our prompt objective incorporates two terms that respectively ensure the generalization of natural examples and the consistency of robust representations. This motivates us to investigate the trade-off between natural and adversarial robustness, and to dynamically adjust this trade-off depending on the desired level of adversarial robustness.\nFrom Table 4  ###reference_###, we can conclude that as  increases, the proportion of the adversarial component in the total loss increases, and the natural accuracy declines continuously. Meanwhile, adversarial robustness gradually improves, reflecting the trade-off between natural and adversarial generalization. However, when  becomes too large (), continuing to increase the proportion of the adversarial component does not lead to further improvements in robustness.\nPrompt depth and prompt length. \nWe provide architectural ablation results for prompt design concerning different prompt depth and length settings. In Table 5  ###reference_###, we can observe that increasing both prompt depth and prompt length introduces more learnable parameters, thereby resulting in improved performance. Furthermore, we can also conclude that the performance gain obtained by increasing prompt depth is higher than that achieved by increasing prompt length, and the improvement in robustness metric is larger than in natural accuracy.\nNatural generalization gap hinders robust adapting. \nWe identify a failure risk in few-shot adversarial prompt learning using TeCoA loss (Mao et al., 2022  ###reference_b46###), where insufficient natural generalization on the sampled dataset impedes robustness learning. Figure 3  ###reference_### (left) displays the loss variation during training under this setup. Under the same experimental setup using the TeCoA loss, different trials exhibit completely different trends: the curve for the failure case shows that the loss quickly ceases to decline and becomes stable shortly after training begins, whereas the loss in the normal case continues to decrease as the training progresses.\n###figure_14### ###figure_15### We presume that this failure stems from a lack of natural generalization ability. To confirm this, we first conduct natural tuning on the problematic few-shot dataset and then apply adversarial prompt learning. This restores the model’s robust fine-tuning performance, as evident in Figure 3  ###reference_### (right), where natural and robust accuracies improve significantly after natural example adaptation. Besides, we validate the learning process on the same few-shot dataset with a dual-form loss in the training objective that considers both natural and adversarial terms (red lines in Figure 3  ###reference_### (left)). It is revealed that this two-term loss effectively acts as a surrogate for the aforementioned two-stage method, avoiding potential failures caused by the natural generalization barrier in end-to-end training."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we focus on adversarial prompt tuning on vision-language models, a domain with significant potential for zero-shot downstream adversarial robustness. We precisely reveal the issues of previous methods that perform adversarial visual prompts with static text supervision. Our method distinguishes itself by introducing learnable adversarial text supervision combined with a new training objective, facilitating effective learning in a few-shot setting. The proposed method enjoys excellent algorithmic properties and matches state-of-the-art performance, notably with reduced computational demand. We believe that this work can provide some insights to the community and stimulate further research in this area."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact Statement",
            "text": "This research aims to contribute positively to the machine learning field by enhancing model robustness against adversarial attacks. While we believe our work is unlikely to have direct negative societal impacts, we acknowledge the importance of considering potential misuse scenarios, such as in the context of security applications. The broader implication of our study is that it enables neural models to maintain adversarial robustness with minimal adaptations, making it particularly suitable for real-time applications in mobile and embodied systems. Such advancements could lead to more secure and reliable applications in various real-world scenarios, including mobile device security."
        }
    ]
}