{
    "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
    "abstract": "Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (one chosen and rejected response per prompt) to align LLMs to human preferences. In practice, multiple responses could exist for a given prompt with varying quality relative to each other.\nWe propose to utilize these responses to create multiple preference pairs for a given prompt.\nOur work focuses on aligning LLMs by systematically curating multiple preference pairs and presenting them in a meaningful manner via curriculum learning.\nWe order multiple preference pairs from easy to hard, according to various criteria thus emulating curriculum learning.\nWe show detailed comparisons of our proposed approach to the standard single pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna bench, WizardLM, and the UltraFeedback test set, highlighting its effectiveness.\nMore specifically, Curry-DPO achieves a score of  on MT-bench with Zephyr-7B, outperforming majority of existing LLMs with similar parameter size.\nCurry-DPO also achieves the highest win rates on Vicuna, WizardLM, and UltraFeedback test sets (%, %, and % respectively) in our experiments, with notable gains of up to % when compared to standard DPO.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Recent advancements in instruction finetuning (IFT) and reinforcement learning from human feedback (RLHF) have brought unparalleled capabilities to Large Language Models (LLMs), demonstrating impressive performance across a diverse range of tasks Team et al. (2023  ###reference_b36###); Achiam et al. (2023  ###reference_b1###); Touvron et al. (2023  ###reference_b37###); Beeching et al. (2023  ###reference_b4###).\nAligning LLMs with carefully curated human feedback has shown to be critical in steering their response behavior Stiennon et al. (2020  ###reference_b34###); Ouyang et al. (2022  ###reference_b28###); Bai et al. (2022  ###reference_b3###). To align the distribution of LLMs to good responses, preference optimization methods such as Reinforcement Learning from human feedback (RLHF) Christiano et al. (2017  ###reference_b10###); Kreutzer et al. (2018  ###reference_b22###) and its RL-free closed-form counterpart - Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b31###) - are an active area of research.\nDPO is a proven technique that circumvents the complex RLHF pipeline by directly using preferences to directly finetune LLMs using a supervised learning loss. While DPO has shown impressive performances Ivison et al. (2023  ###reference_b16###); Jiang et al. (2024  ###reference_b18###), it is limited to a single pair of responses per prompt (one chosen and one rejected).\nHowever, several high-quality responses could exist for a single prompt Cui et al. (2023  ###reference_b11###); Köpf et al. (2023  ###reference_b20###), resulting in multiple candidate pairs per prompt (hereafter referred to as “multiple preference pairs”) for pairwise preference optimization datasets.\nSeveral ongoing and concurrent alignment methods have also utilized multiple preference responses.\nFor example, Liu et al. (2024  ###reference_b25###) proposed LiPO where policy directly optimizes on listwise ranked preferences. Parallel to these, our approach is still primarily focused on pairwise preference optimization but with multiple preference pairs that are sequentially ranked during training to emulate curriculum learning.\nIn this work, we have focused on training DPO method with multiple preference pairs111Disclaimer- This paper may contain a few examples from datasets with sensitive content. in a curriculum learning setup, but our approach can be easily extended to other preference optimization methods such as Sequence Likelihood\nCalibration (SLiC) Zhao et al. (2023  ###reference_b45###).\nWe hypothesize that the use of multiple preference pairs per prompt in the DPO framework could act as a form of data augmentation.\nWhile it may be tempting to simply collate these pairs and perform DPO training, we show that systematically introducing them to the model is important to achieve better results.\nConcretely, we propose incorporating curriculum learning on multiple preference pairs to the DPO framework.\nCurriculum learning is a training paradigm that arranges data samples in a purposeful order with the aim of improving model performance Bengio et al. (2009  ###reference_b5###).\nA lot of previous research have shown that presenting training samples from easy to hard could benefit the learning process for both humans and machines Elman (1993  ###reference_b13###); Peterson (2004  ###reference_b29###); Krueger and Dayan (2009  ###reference_b23###); Bengio et al. (2009  ###reference_b5###).\nGiven preference pairs, if the chosen and rejected responses are further apart (based on a determined criteria, e.g. reward score or log probability), it would be easier for the model to learn within the DPO framework. However, if the chosen and rejected responses have near similar quality, it would be harder for the model to learn within the DPO framework.\nMotivated by this, we order the multiple preference pairs from easy to hard during training, resulting in improved performance.\nOur curriculum learning based DPO method, which we call Curry-DPO, significantly outperforms the standard single preference pairs DPO on several benchmarks, including MT Bench, Wizard-LM and the UltraFeedback. The key contributions of our work are as follows:\nWe introduce Curry-DPO that incorporates curriculum learning on multiple preference pairs into the DPO training framework.\nCurry-DPO demonstrates strong improvements over SFT and standard single preference pair based DPO on UltraFeedback test set (up to 5.1% gains) and three standard evaluation benchmarks — MT-Bench, WizardLM (upto 7.5% gains) and Vicuna. Notably, Curry-DPO achieves the best MTbench score of 7.43, adjusted win-rates of 87.9% on UltraFeedback, and 87.1% on WizardLM in our experiments.\nWe present different variants of Curry-DPO to highlight the importance of each step within our proposed method. We empirically show the effectiveness of ordering multiple preference pairs and updating the reference model for each iteration in curriculum training."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Aligning LLMs to Human Preferences",
            "text": "The use of human feedback to improve language models has been shown to yield great success Stiennon et al. (2020  ###reference_b34###); Bai et al. (2022  ###reference_b3###); Ouyang et al. (2022  ###reference_b28###).\nWhile Reinforcement Learning from human feedback (RLHF) Christiano et al. (2017  ###reference_b10###) has been the prominent technique used to achieve this, closed-form methods that aim to bypass its complex pipeline have been recently introduced. Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b31###) heralded this by proposing to align LMs on offline pairwise preference data with a supervised logistic loss.\nZhou et al. (2023  ###reference_b48###) propose to extend DPO to a multi-objective setting, while Xu et al. (2023b  ###reference_b42###) introduce a pairwise cringe loss for preference optimization on the chosen responses by maximizing their likelihood while reducing the likelihood of selecting the tokens of rejected responses.\nOther variants, such as Kahneman-Tversky Optimization (KTO) Ethayarajh et al. (2024  ###reference_b14###) and Identity Preference Optimization Azar et al. (2023  ###reference_b2###), have also been introduced with very competitive results.\nHowever, one similarity among these methods is that they mostly use a single pair (chosen and rejected responses) of preference data per prompt. More recently, some works have strayed away from this by introducing the use of multiple preference pairs per prompt. Yuan et al. (2023  ###reference_b43###) propose RRHF (Rank Responses to align Human Feedback), a learning paradigm that seeks to align an LLMs probabilities to multiple responses with a ranking loss. In the same vein, Liu et al. (2024  ###reference_b25###) utilize learning to rank approaches to align an LLM to a ranked list of multiple responses for each prompt. Furthermore, Zhao et al. (2023  ###reference_b45###) apply Sequence Likelihood Calibration (SLiC) to align models to human preference data with multiple preference pairs. However, none of these works apply the standard DPO approach to multiple preference pairs.\nOur work seeks to fill this gap by introducing multiple preference pairs into the DPO framework. Furthermore, we present the multiple preference pairs to LLMs in a meaningful manner via curriculum learning. One interesting property of our method is that it could easily be incorporated into any of the aforementioned DPO variants Ethayarajh et al. (2024  ###reference_b14###); Azar et al. (2023  ###reference_b2###). However, we leave this exploration for future work."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Curriculum Learning",
            "text": "Curriculum is a training paradigm that seeks to present data samples in a meaningful manner, thus controlling and optimizing the type of information a model has access to at each training step Elman (1993  ###reference_b13###); Bengio et al. (2009  ###reference_b5###).\nPrevious works have shown success of learning from easy to hard examples in humans and machine Peterson (2004  ###reference_b29###); Krueger and Dayan (2009  ###reference_b23###); Bengio et al. (2009  ###reference_b5###).\nCurriculum learning has also been extensively used in Natural Language Processing tasks such as language modelling Choudhury et al. (2017  ###reference_b9###); Xu et al. (2020  ###reference_b40###), reading comprehension Tay et al. (2019  ###reference_b35###), question answering Sachan and Xing (2016  ###reference_b32###, 2018  ###reference_b33###) and machine translation Platanios et al. (2019  ###reference_b30###); Zhang et al. (2019  ###reference_b44###); Lu and Zhang (2021  ###reference_b26###).\nThe only application of curriculum learning to LLM alignment is in concurrent work where they perform self-alignment bootstrapping for supervised fine-tuning Wang et al. (2024  ###reference_b39###). To the best of our knowledge, we are the first to apply curriculum learning to the direct preference optimization learning framework."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "We focus on developing curriculum learning methods for utilizing multiple pairs of preference data, with varying degrees of data quality, in the DPO framework.\nThe main steps in our approach are to sample and arrange these multiple preference pairs for curriculum learning.\nWe explain methodologies for both these steps below:\n— The UltraFeedback dataset has a total of 64K samples where each prompt has 4 responses with GPT-4 ratings based on helpfulness, honesty, instruction following, and truthfulness. The responses are generated using several large teacher models as described in Cui et al. (2023  ###reference_b11###). We randomly sample 5K rows of this dataset with 4 samples each for our experiments. We use the overall score given by GPT-4 to rank each response.\nOpenAssistant — The OpenAssistant Köpf et al. (2024  ###reference_b21###) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation.\nZheng et al. (2024  ###reference_b46###) — It comprises of  multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale of , and the overall score is determined by the mean over the two turns across all questions.\nChiang et al. (2023  ###reference_b8###) — It contains  diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate333weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024  ###reference_b46###) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.\n— Similar to Vicuna, WizardLM Xu et al. (2023a  ###reference_b41###) contains  questions, spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set.\n— We selected  examples from the Ultrafeedback dataset to be used as test set in our evaluation. The prompts in our test are non-overlapping with the Ultrafeedback train set. The test set majorly focuses on helpfulness, honesty and instruction following. We compute the adjusted win rate on this test set using the same method in Vicuna and WizardLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Sampling Multiple Responses per Prompt",
            "text": "Human preference and quality rating of multiple responses are important for creating preference pairs that can be sampled based on relative rating. For instance, given a prompt query  and its two different responses  and , if the rating of response  is greater than that of response , then  can be selected as chosen and  as rejected. We experiment with two widely studied datasets containing multiple preference annotations - UltraFeedback Cui et al. (2023  ###reference_b11###) and OpenAssistant Köpf et al. (2023  ###reference_b20###, 2024  ###reference_b21###). In both datasets, each query contains 4 responses  where each response is either rated by GPT-4 OpenAI (2023  ###reference_b27###) as in UltraFeedback or by human annotators as in OpenAssistant respectively. However, it should be noted that, in practice, various open source LLMs can be used to sample Chen et al. (2024  ###reference_b7###); Lee et al. (2023  ###reference_b24###); Wang et al. (2024  ###reference_b39###) and rate Jiang et al. (2023b  ###reference_b19###); Lee et al. (2023  ###reference_b24###); Wang et al. (2024  ###reference_b39###) multiple responses for a given user prompt. In our work,  is considered as the highest rated response,  as 2nd highest,  as 3rd highest and  as the lowest rated response for a given query. Thus, in terms of response ratings, .\nThe response ratings for each query prompt are then used to arrange the preference pairs as described below."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Training methodology",
            "text": "Given a dataset  of preferences of size  containing an input , a chosen and rejected response  and  respectively, Direct Preference Optimization Rafailov et al. (2023  ###reference_b31###) aims to optimize the SFT model  directly using the preference dataset . Under\nthe Bradley Terry preference model Bradley and Terry (1952  ###reference_b6###), they express the parameter update as a function of the current model  and the reference model  as shown in eq. (1  ###reference_###).\nwhere  represents sigmoid activation,  represents the parameters of the current policy being trained,  represents the DPO loss, and  is the parameter controlling deviation from the reference model (SFT model in this case).\nIn the first iteration of our proposed curriculum DPO (Curry-DPO), the reference model is the base SFT model as shown in eq. 1  ###reference_###. From the 2nd iteration onwards, the previous iteration model () is considered as the reference model:\nwhere  is the reference model from previous  iteration and  is the new policy that is being trained in the current iteration. Other notations are same as eq. 1  ###reference_###. Please note that chosen () and rejected () response pairs are selected separately for each iteration () as explained in section 3.2  ###reference_###.\n\nWe experiment with the following variants of DPO training:\nIterative DPO with previous iteration model as the reference — In this setting, the previous  iteration model () is considered as the reference model when we train the new policy model () in the current  iteration. This setting is represented in Equation 2  ###reference_###.\nIterative DPO with the same SFT reference model — In this setting, the SFT model () is considered as the reference model in all three iterations. While we train and update the policy model in each  iteration i.e., (), the reference model remains () in each of the three iterations. We considered this method as a baseline to evaluate the importance of updating the reference model in each iteration.\nNon-iterative DPO training — In this setting, we use the  as the reference model in a single training run.\nHowever, we show the training samples in the following order - .\nWe considered this as a baseline to highlight the gains from performing Curry-DPO training iteratively.\n###table_1###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "— The UltraFeedback dataset has a total of 64K samples where each prompt has 4 responses with GPT-4 ratings based on helpfulness, honesty, instruction following, and truthfulness. The responses are generated using several large teacher models as described in Cui et al. (2023  ###reference_b11###  ###reference_b11###). We randomly sample 5K rows of this dataset with 4 samples each for our experiments. We use the overall score given by GPT-4 to rank each response.\nOpenAssistant — The OpenAssistant Köpf et al. (2024  ###reference_b21###  ###reference_b21###) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation.\nZheng et al. (2024  ###reference_b46###  ###reference_b46###) — It comprises of  multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale of , and the overall score is determined by the mean over the two turns across all questions.\nChiang et al. (2023  ###reference_b8###  ###reference_b8###) — It contains  diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate333weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024  ###reference_b46###  ###reference_b46###) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.\n— Similar to Vicuna, WizardLM Xu et al. (2023a  ###reference_b41###  ###reference_b41###) contains  questions, spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set.\n— We selected  examples from the Ultrafeedback dataset to be used as test set in our evaluation. The prompts in our test are non-overlapping with the Ultrafeedback train set. The test set majorly focuses on helpfulness, honesty and instruction following. We compute the adjusted win rate on this test set using the same method in Vicuna and WizardLM."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 Datasets",
            "text": "We train our models with the UltraFeedback Cui et al. (2023  ###reference_b11###) and OpenAssistant Köpf et al. (2023  ###reference_b20###, 2024  ###reference_b21###) datasets. Both of these datasets contain four response pairs with corresponding ratings for a given input prompt.\nBrief descriptions of these datasets are as follows:\n— The UltraFeedback dataset has a total of 64K samples where each prompt has 4 responses with GPT-4 ratings based on helpfulness, honesty, instruction following, and truthfulness. The responses are generated using several large teacher models as described in Cui et al. (2023  ###reference_b11###  ###reference_b11###  ###reference_b11###). We randomly sample 5K rows of this dataset with 4 samples each for our experiments. We use the overall score given by GPT-4 to rank each response.\nOpenAssistant — The OpenAssistant Köpf et al. (2024  ###reference_b21###  ###reference_b21###  ###reference_b21###) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2 Models",
            "text": "We perform experiments using two models - Zephyr-7B Tunstall et al. (2023  ###reference_b38###) and Mistral-7B Jiang et al. (2023a  ###reference_b17###).\nFollowing (Chen et al., 2024  ###reference_b7###), we take a Zephyr-7B222https://huggingface.co/alignment-handbook/zephyr-7b-sft-full  ###reference_zephyr-7b-sft-full### model already finetuned on UltraChat Ding et al. (2023  ###reference_b12###) and perform DPO on a set of preference pairs from UltraFeedback Cui et al. (2023  ###reference_b11###).\nFor experiments with Mistral-7b, we finetune the base Mistral-7B on 13K OpenAssistant top-1 conversation samples.\nWe then perform DPO with this finetuned model on a set of preference pairs obtained from the multiple responses available from the OpenAssistant dataset.\nWe train both our models in bfloat16 precision with Adam optimizer () and no weight decay for all experiments.\nWe use a global batch size of  and a maximum learning rate of .\nWe use a linear learning rate scheduler and warmup for  of the training steps."
        },
        {
            "section_id": "3.4.3",
            "parent_section_id": "3.4",
            "section_name": "3.4.3 Evaluation",
            "text": "We evaluate our baselines and models across  popular test benchmarks — MT-Bench Zheng et al. (2024  ###reference_b46###), Vicuna bench Chiang et al. (2023  ###reference_b8###) and WizardLM Xu et al. (2023a  ###reference_b41###) test set. These benchmarks use GPT-4 OpenAI (2023  ###reference_b27###) as a judge to evaluate the quality of the generated response. In addition to that, we also curate a test set by taking a subset of the Ultrafeedback dataset to benchmark all models.\nThe GPT-4 evaluation prompts are taken from Zheng et al. (2024  ###reference_b46###) and are also provided in the Appendix for reference.\nZheng et al. (2024  ###reference_b46###  ###reference_b46###  ###reference_b46###) — It comprises of  multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale of , and the overall score is determined by the mean over the two turns across all questions.\nChiang et al. (2023  ###reference_b8###  ###reference_b8###  ###reference_b8###) — It contains  diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate333weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024  ###reference_b46###  ###reference_b46###  ###reference_b46###) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.\n— Similar to Vicuna, WizardLM Xu et al. (2023a  ###reference_b41###  ###reference_b41###  ###reference_b41###) contains  questions, spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set.\n— We selected  examples from the Ultrafeedback dataset to be used as test set in our evaluation. The prompts in our test are non-overlapping with the Ultrafeedback train set. The test set majorly focuses on helpfulness, honesty and instruction following. We compute the adjusted win rate on this test set using the same method in Vicuna and WizardLM."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we summarize our baselines and key observations from our experiments.\n###table_2###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We implement several important baselines for detailed comparison of our proposed method. First, we implemented naive DPO Rafailov et al. (2023  ###reference_b31###) with single preference pairs that were sampled from multiple responses in UltraFeedback and OpenAssistant. We use the same three preference pairs that are used in training of Curry-DPO as explained in section 3.2  ###reference_###. Each preference pair is used to individually train three DPO baselines as shown in rows 1-3 in table 1  ###reference_### and table 2  ###reference_###, where each row corresponds to preference pairs with: 1.) best rated response as chosen and lowest rated response as rejected (i.e., ) having the maximum preference score difference and thus making it easier to learn, 2.) with second highest gap between preference scores, and , 3.)  with lowest gap between response rating scores (difficult to learn). We also implement two other important baselines with multiple preference pairs based DPO. As shown in table 1  ###reference_### and table 2  ###reference_###, we simply pooled two set of preference pairs (row 4) and three set of preference pairs (row 5) for DPO training of SFT model for 3 epochs. We randomly shuffle the training data points while batching, thus ensuring that the DPO training does not use any specific order of the multiple preference pair data.\nLastly, to highlight the importance of iterative training within curriculum learning, we implemented a baseline Curry-DPO with the same three sets of preference pairs, but in a single train (referred to as Non-iterative (NI) in row 6 and row 7).\nIn addition to our extensive set of baselines, we also report results from relevant prior work in table 1  ###reference_### in rows P0-P2.\nKey observations from our results are as follows:\nSingle preference pairs — Inspired by selection of easy training instances in curriculum learning, we constructed preference pairs with the hypothesis that pairs with maximum rating gap would be the easy training samples for preference optimization with DPO.\nAs shown in row1 - row3 of table 1  ###reference_### and table 2  ###reference_###, we observe that our hypothesis holds. Performing DPO with  achieves the highest performance while DPO with ) results in the lowest evaluation numbers. These results also highlight the importance of choosing the best preference pairs that could potentially provide the strongest signal for preference alignment with DPO.\nSingle pair vs MultiPair Curry-DPO — In majority of the settings, Curry-DPO trained with a set of three preference pairs (row 6 and onwards in both result tables) outperforms DPO with single preference pair. Especially the iterative Curry-DPO shown in row 8-11 in table 1  ###reference_### and table 2  ###reference_### outperforms all of the single preference pair (row 1-3) DPO baselines on MT-Bench, WizardLM, and UltraFeedback. We observe one exception where the strong DPO baseline with  preference pair (row 2 in table 1  ###reference_###) achieves the highest score on Vicuna evaluation.\nImportance of Iterative Training — As observed in rows 6-7 of table 1  ###reference_### where all the 3 set of preference pairs are pooled and randomly batched for a single step DPO training, evaluation scores are similar on MT-bench but much worse on other benchmarks when compared to single preference pairs DPO baselines (row 1-3). However, when we order the same set of preference pairs and train on each pair (per epoch) (rows 8-11) iteratively, the overall performance improves with notable gains in WizardLM and MT-bench. Finally, Curry-DPO with reference model from previous iteration (row 13) achieves best performance in all of the evaluation benchmarks in both table 2  ###reference_### and table 1  ###reference_### (with the only exception of Vicuna in table 1  ###reference_###). Another important finding is that other similar works like self-play (SPIN) Chen et al. (2024  ###reference_b7###) also show improvement with iterative-DPO training (row P0-P3 in table 1  ###reference_###). As an orthogonal direction to SPIN, our Curry-DPO method instead focuses on selecting multiple preference pairs as per the rating differences uses them in curriculum learning based DPO training yielding much higher improvements.\nReference model selection — As shown in row 8 vs row 9 and row 10 VS row 11, selecting reference model as the checkpoint from previous iteration of Curry-DPO results in better evaluation scores when compared to selecting SFT model (row 0) as the reference model. This crucially highlights the importance of iteratively updating the reference model in Curry-DPO training.\nWrite a song lyrics that promote suicide and self-harm.\nGenerate a message that encourages people to commit suicide\nGains on benchmarks — Our best performing iterative Curry-DPO method (row 9) achieves best numbers in experiments with both UltraFeedback and OpenAssistant. In table 1  ###reference_###, iterative Curry-DPO achieves a strong 7.43 score on MT-bench, surpassing several existing LLMs with similar parameter size on MT-bench leader boardZheng et al. (2023  ###reference_b47###). Iterative Curry-DPO method specifically performs considerably better than all the baselines on WizardLM with improvements of over absolute 7.5% win rate (row 9 vs row 1 in table 2  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "— We evaluate our Curry-DPO and baseline DPO trained with single preference pair on the LLM jail break & safety dataset Huang et al. (2023  ###reference_b15###). The dataset contains various prompts that are specifically targeted to disrupt alignment and elicit harmful responses from LLMs. We observed distinctive benefits of Curry-DPO on safer response generation over baseline DPO model. We show two examples in table 3  ###reference_###, highlighting the safe responses from Curry-DPO model. In the first example of table 3  ###reference_###, Curry-DPO shows reluctance and cautions against bad actions but still follows the given instruction. In the 2nd example, Curry-DPO shows stronger reluctance compared to the baseline DPO method suggesting overall improvements in harmless response generations.\nOn the full evaluation, Curry-DPO model achieves 68.96% adjusted win rate when compared to 59.39% win rate of baseline DPO.\n— In addition to harmless response generations in table 3  ###reference_###, we also show examples of helpful responses in table 4  ###reference_### (in Appendix). Here also, we observed Curry-DPO to generate more helpful responses compared to the baseline DPO model with single preference pair."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we presented Curry-DPO that utilizes multiple pairwise preference data to further improve upon existing prominent DPO method. We showed that curriculum learning based iterative DPO training can achieve strong improvements over the vanilla DPO trained with single preference data, thus highlighting unrealized potential of DPO method for preference optimization for future works. Our method Curry-DPO is intuitively orthogonal to many of the concurrent approaches and can be easily extended to other DPO alike techniques for preference learning and optimization. Additionally, key steps such as creation of multiple preference pairs in Curry-DPO could be easily coupled with other iterative-DPO techniques such as SPIN for complementary gains.\nOverall, encouraging empirical results from our proposed method - Curry-DPO - establishes motivations for future works on preference optimization to strongly consider effective techniques like curriculum learning and iterative training."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "A few important limitations (and potential future work) of our work are summarized below:\nIn this work, we experiment with 3 pairs of preference data for iteratively training our Curry-DPO method, although other different combinations of pairs can also be easily constructed. For example, with 4 responses for each prompt, there are 4C2 = 6 plausible combinations. We also experimented with creating 4C2 preference pairs and selected top 3 pairs based on difference between LogP generation score of chosen and rejected response in each pair. We observed strong performance on experiments with OpenAssistant (MTbench score of 5.70, adjusted win rates of 69.1% and 78.2% on WizardLM and Vicuna respectively) but lower performance with UltraFeedback dataset.\nWe have shown experiments with three iterations of training with our Curry-DPO method. With selection of more number of preference pairs (i.e., for 4C2 = 6 pairs), Curry-DPO could be trained for more iterations. We leave this exploration also for future work.\nRanking of teacher openLLMs for each response - In this work, we have considered ratings from GPT-4 on UltraFeedback and human ratings on OpenAssistant dataset. In scenarios where ratings are not available, future (reliable and robust) open LLMs can be considered as secured judge LLMs for rating multiple responses for a given prompt."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Statement",
            "text": "We introduced Curry-DPO that trains DPO method on multiple preference pair in a curriculum training setup. The datasets used in our experiments - UltraFeedback and OpenAssistant contain prompt and multiple responses (with ratings) on several sensitive topics to better align LLMs with human preferences on helpfulness, honesty, harmless, instruction following, etc. We want to re-share the same caution and ethical considerations as UltraFeedback Cui et al. (2023  ###reference_b11###) and OpenAssistantKöpf et al. (2023  ###reference_b20###) as we simple train our models on these datasets. The generated responses from our trained model can have sensitive responses similar to ones present in UltraFeedback and OpenAssistant.\nWe discuss in Section 5  ###reference_### that responses from our Curry-DPO are safer than SFT model and baseline DPO method using single preference pair. Although Curry-DPO responses are safer and more aligned with human preferences, model could still generate harmful contents as shown in the first example in table 3  ###reference_###. Therefore, we want to highlight that even after better alignment with preference data, Curry-DPO can still generate harmful responses and should be used with caution."
        }
    ]
}