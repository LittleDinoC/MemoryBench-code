{
    "title": "How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems",
    "abstract": "Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through extensive empirical studies focused on a well-known combinatorial optimization problem, i.e., capacitated vehicle routing problem. The results are compared against those obtained from the LLM-based optimization algorithms that rely solely on textual prompts, demonstrating the significant advantages of our multimodal approach.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Optimization problems arise in various domains, such as supply chain management Cox (1999  ###reference_b5###), logistics Pfohl (2010  ###reference_b14###), engineering Deb (2012  ###reference_b7###), and machine learning Sra et al. (2012  ###reference_b18###). These problems are typically solved by traditional heuristic search methods, such as evolutionary algorithm Yu and Gen (2010  ###reference_b22###), ant colony algorithm Blum (2005  ###reference_b2###), simulated annealing Bertsimas and\nTsitsiklis (1993  ###reference_b1###), and so on. Despite the success of these algorithms on various optimization problems, the no free lunch theory Wolpert and Macready (1997  ###reference_b20###) indicates that their operators are not universally effective, which means that they may perform poorly on newly arrived problems. Moreover, developing new algorithms for novel problems entails a rigorous and laborious process, which often requires much expert knowledge. Hence, it would be advantageous to have a more intelligent optimization engine, which can automatically acquire the features of the optimization problem and adopt suitable search strategies to enhance the search performance.\nVery recently, large language models (LLMs) have emerged as a powerful technique for natural language understanding and generation Bubeck et al. (2023  ###reference_b4###), as well as for other modalities such as images and videos Bubeck et al. (2023  ###reference_b4###); Huang et al. (2023  ###reference_b9###). By processing massive amounts of data and learning from diverse sources of information, LLMs are capable of performing various tasks across domains. Additionally, LLMs can be conveniently accessed and queried by using natural language prompts, which renders them attractive for users who lack familiarity with formal languages or programming. Inspired by these benefits, some efforts have been made to investigate the use of LLMs as optimizers, where the optimization problem is expressed in natural language and the LLM produces optimization solutions according to the prompts. For instance, Yang et al. introduced Optimization by PROmpting (OPRO) that exploits LLMs to solve optimization problems Yang et al. (2023  ###reference_b21###). Li et al. developed OptiGuide that uses LLMs to provide insights and guidance for supply chain optimization Li et al. (2023  ###reference_b11###). Liu et al. proposed a novel framework that combines LLMs and evolutionary algorithms to solve multi-objective optimization problemsLiu et al. (2023  ###reference_b12###). Moreover, Cummins et al. used LLMs to generate compiler optimization sequences Cummins et al. (2023  ###reference_b6###). Pluhacek et al. leveraged LLMs to generate novel meta-heuristic optimization algorithms that can adapt to different problem domains Pluhacek et al. (2023  ###reference_b15###). Romera et al. showed that LLMs can discover new mathematical theorems and proofs from natural language descriptions of mathematical problems Romera-Paredes et al. (2023  ###reference_b16###). These works demonstrate the remarkable ability of LLMs to learn from natural language and generate optimization solutions across different domains.\nHowever, most existing LLM-based optimization methods adopt numerical text prompts to model the optimization problem, which restricts the LLMs’ ability to capture the relationships between the decision variables. As the number of decision variables grows, the text prompts become longer and more complicated, which further impairs the problem representation and diminishes the performance of the LLMs. Nevertheless, the optimization problems in real-world frequently involve multiple forms of representation. For example, capacitated vehicle routing problem (CVRP) can be represented by text to show the information of each customer node, or by image to visualize the layout of the nodes.\nKeeping the above in mind, we propose to use multimodal LLM (MLLM) for optimization, which can handle both textual and visual inputs for enhanced optimization performance. MLLMs leverage the complementary information from various modalities and offer more rich and expressive representations of the optimization problem at hand. In this work, we present a MLLM-based optimization framework, where the optimization problem is represented by a mix of textual and visual prompts, and the LLM generates optimization solutions based on both modalities. To evaluate the performance of the proposed method, the framework is applied to a well-known combinatorial optimization problem, i.e., CVRP, which entails assigning a set of customers to a fleet of vehicles with limited capacity, while minimizing the total traveling distance. Furthermore, We compare our method with the LLM-based optimization algorithm that uses only text input, and demonstrate that our method outperforms it in terms of solution quality. The main contributions of this paper are outlined as follows:\nThe proposed method incorporates both textual and visual prompts simultaneously to facilitate the comprehensive understanding of optimization problems and further improve optimization performance. To the best of our knowledge, this is the first multi-modal solution for optimization.\nA novel XML text prompt is proposed in this work for the optimization problem, which allows the MLLM model to capture more precise relationships among the decision variables.\nWith the assistance of LLM, the framework of MLLM has been specially designed to emulate the workflow of human beings in solving optimization problems. This endows MLLM with enhanced generalization capabilities, enabling its application to various optimization problems that can benefit from the introduction of graphical illustrations.\nUsing a well-known combinatorial optimization problem, CVRP, we evaluate our proposed framework that exploits both textual and visual inputs, and compare it with the ones that use only textual inputs.\nThe rest of the paper is organized as follows. The CVRP is introduced in Section 2 as a case study to assess the optimization efficacy of MLLM. The application of LLM to optimization problems is also surveyed in this section Furthermore, section 3 describes the proposed optimization framework based on MLLM. Subsequently, section 4 reports and analyzes the experimental outcomes of our method. Lastly, section 5 summarizes the paper and outlines the directions for future work."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Capacitated Vehicle Routing Problem",
            "text": "CVRP is a well-known combinatorial optimization problem that entails finding the optimal routes for a fleet of vehicles to serve a given set of customers (i.e., nodes) subject to the constraint of vehicle capacity Toth and Vigo (2014  ###reference_b19###). The CVRP and its variants have numerous applications in various domains, such as smart logistics Sajid et al. (2021  ###reference_b17###) and distributing medical supplies for emergencies Zheng et al. (2023  ###reference_b23###). Mathematically, let  be an undirected connection graph for a CVRP, where  and  denote the sets of edges and customers, respectively, and  is the demand set of customers. Given  vehicles of capacity , the objective of the CVRP is to find a set of routes , each originating and terminating at a depot , where the vehicles are stationed, such that:\nEvery customer  is served by exactly one vehicle and served only once.\nTotal demand of the customers on each route does not exceed the vehicle capacity: .\nThe sum of the distances of all the routes is minimized.\nAs shown in Fig. 1  ###reference_###, an optimal solution for a CVRP instance depends largely on the spatial distribution of customers. This motivates us to introduce the MLLM to handle both textual and visual inputs, and thus gain a deeper understanding of the problems require to solve.\n###figure_1###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Literature Review",
            "text": "Despite the LLMs have been applied to various problems in human beings’ daily life, the study on using LLMs for optimization remains nascent. Existing LLM-assisted optimization methods can be briefly divided into two classes. The first class is to use LLMs as black-box optimizers that can generate new solutions from a numerical textual prompt, which contains the current solutions and their fitness values. For instance, OPRO Yang et al. (2023  ###reference_b21###) employed LLMs to solve optimization problems without gradients. OptiGuide Li et al. (2023  ###reference_b11###) leveraged LLMs to offer insights and guidance for supply chain optimization. Moreover, Meyerson et al. Meyerson et al. (2023  ###reference_b13###) devised Language Model Crossover, a simple technique to evolve semantically-rich text representations for various domains. Brownlee et al. Brownlee et al. (2023  ###reference_b3###) assessed the use of LLMs as mutation operators to enhance the search process. However, since these methods use numerical prompts to guide the LLMs, they have difficulty in handling the complex relationships between decision variables, which thus hinders the LLMs’ optimization efficacy on broader scopes with larger scales.\nOn the other hand, the second class is to use LLMs as meta-heuristics that can design novel functions or heuristics for solving complex problems, by leveraging their natural language processing abilities. In particular, Guo et al. Guo et al. (2023  ###reference_b8###) used LLMs to optimize the hyperparameters of gradient-based methods, such as learning rate, momentum, and weight decay. Cummins et al. Cummins et al. (2023  ###reference_b6###) introduced LLMs to generate compiler optimization sequences that outperform human experts and state-of-the-art methods. Pluhacek et al. Pluhacek et al. (2023  ###reference_b15###) leveraged LLMs to generate novel meta-heuristic optimization algorithms that can adapt to different problem domains. Moreover, FunsearchRomera-Paredes et al. (2023  ###reference_b16###) employed LLMs to discover new mathematical functions that surpass human performance. However, the LLMs employed in these studies are solely prompted with textual data, which might hinder their ability to grasp the intricacies and nuances of the problem domain.\nBearing the above in mind, we first present a new framework that uses MLLM to help solve optimization problems, which can take advantage of both visual and textual information."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Multimodal LLM for Optimization",
            "text": "In this section, we present our optimization framework based on MLLMs, which is a general approach that mirrors the nuanced steps inherent in human problem-solving methodologies applied to complex optimization scenarios. The entire workflow has been depicted in Fig .2  ###reference_###. As can be observed, the framework encompasses three pivotal steps: 1) heuristic extraction from solved problems; 2) solution generation via learned heuristics; and 3) solution evaluation and refinement. In what follows, the details for each step are provided.\n###figure_2###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Solution Generation via learned heuristics",
            "text": "The second step of our proposed framework is to generate a preliminary solution via MLLM by applying the heuristics learned from the previous observations. To this end, we use MLLM again, but with a different input and output format. Particularly, we prompt the MLLM with the description of the problem to solve, which consists of the textual and visual prompts, without the optimal solution (the “Solution” in the XML text and the sub-figure “B” are removed). The MLLM is then expected to generate an initial solution in the form of an XML document, which contains only the “Solution” tag with the traveling routes. The heuristics assist the MLLM in generating the initial solution by providing it with guidance from the historical chats. However, the MLLM is not restricted by these historically derived heuristics, and it can also investigate new possibilities and produce novel solutions by applying its own knowledge. An example of this step has been provided in what follows. As can be observed, the prompts indicate that the MLLM should produce a brief and accurate routing solution that covers all the customers.\nNow you are presented with the unsolved CVRPs with the description of XML text (without SOLUTION{}/SOLUTION) and topological layout picture (without the figure of optimal traveling routes)…\nCVRP name={} n_customer={} capacity={}\nDepot…/Depot\nCustomers…/Customers\n/CVRP\nKindly return me the complete preliminary solution of …in XML format, adhering to the heuristics that you have previously acquired.\nNo Explanation Needed."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Solution Evaluation and Refinement",
            "text": "The third final step of the proposed framework is to evaluate and revise the solution produced by the MLLM based on the MLLM’s response. As a generative model that relies on probabilistic inference, MLLM cannot guarantee the correctness and completeness of the routing solution it produces, particularly when the problem dimensionality increases. Hence, we iteratively guide the MLLM to correct the invalid result by supplying the errors detected in the solution. More specifically, after obtaining the generated solution from the MLLM using its learned heuristics, the solution will be passed to a validation function that verifies the customer IDs in the solution. The validation component will detect for the following errors: missing customer IDs, repeated customer IDs, and customer IDs that should not be in the solution. It then returns the invalid solution to the MLLM with the error prompts to assist the MLLM in correcting the solution. Once the MLLM produces a solution with valid customer IDs for each route, we use the “Repair” operator to adjust the routes in case of capacity violation. This operator may create additional routes for serving the extra customers. We provide an example of this step in the following:\nYour routing solution is invalid. To return valid routes, refine the ones below by removing duplicate customer IDs and adding missing ones:\nSOLUTION\nroute id=1…[1,3,7,9,9]/route\nroute id=2…[2,4,6,8,10,11]/route\n…\n/SOLUTION\nThe duplicated customer IDs are given by: [9], the missed customer IDs are given by: [5], and the customer IDs which should not appear are given by: [11]\nPlease remove the duplicated IDs and the IDs should not appear, and add the missed IDs to the route with minimum customers\nNo Explanations Needed\nMoreover, to facilitate the complete grasp of this study, an exhaustive illustration of chats is attached to the document as a reference."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Study",
            "text": "In this study, we propose a novel framework for solving the CVRP, which is a challenging combinatorial optimization problem with many real-world applications. To test the effectiveness of our proposed method, we use a set of widely adopted CVRP benchmarks111https://vrp.atd-lab.inf.puc-rio.br/index.php/en/ that vary in customer size and vertex layout. Specifically, we classify these benchmarks into two categories: small-scale problems with up to 50 nodes and large-scale problems with around 160 nodes. Additionally, we leverage three CVRPs with known optimal solutions, namely “A-n45-k6”, “P-n60-k10” and “E-n101-k14”, as knowledge prompts to facilitate the learning and reasoning process of the proposed method.\nThe problem properties of each CVRP instance have been tabulated in Table 1  ###reference_###, where “”, “” and “Capacity” denote the number of vertexes (depot involved), the number of vehicles and the capacity of each vehicle, respectively. To demonstrate the superiority of our proposed framework (namely MLLM-V), comparative analysis has been conducted against the baseline that solely relies on textual prompts (namely MLLM-T). It should be noted that, both methods employ a same multimodal language model, namely GPT-4-vision-preview, without any further fine-tuning. The only difference between MLLM-V and MLLM-T lies in the use of visual prompts and their corresponding descriptions."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In this section, we present the results of our numerical experiments, where the performance of our proposed MLLM-V method is compared with the MLLM-T method on a set of representative CVRP benchmarks. Table 2  ###reference_### summarizes the results of both methods, where the lowest cost () and the average cost () over 5 runs for each problem are reported. In addition, the best-known lowest traveling cost of each CVRP instance is also shown via “Optimal” for reference. The “” measures the difference between the gained result against the optimal solution, which is calculated via , where  and  denote the obtained average traveling cost and the lowest traveling cost, respectively. Superior results gained by our proposed MLLM-V and the baseline MLLM-T have been highlighted using bold font.\nAs can be observed from the table, MLLM-V achieves better results than MLLM-T on most of the CVRP instances. For instance, the average traveling costs attained by the proposed MLLM-V method on the “P-n19-k2” and “A-n44-k6” problems were 260 and 1445, respectively, exceeding the optimal solution of the MLLM-T method on these cases. This indicates that the utilization of visual prompts and their descriptions can significantly enhance the learning and optimization capabilities of the MLLM, resulting in more effective and efficient vehicle routing. Furthermore, by leveraging the rich multimodal information accessible from maps, traffic data, and other resources, MLLM-V can unveil new prospects for tackling challenging optimization problems with a handful of observations from similar solved problems, which is intractable to conventional machine learning methods.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### To illustrate the differences between the optimization framework based on MLLM-V and MLLM-T, we compare the traveling routes for “P-n19-k2” generated by random method, MLLM-T with text prompt only and our proposed MLLM-T with both textual and visual prompts in Fig. 3  ###reference_###. Furthermore, the optimal traveling routes of “P-n19-k2” are also provide for reference. As can be observed from Fig. 3  ###reference_###, the optimal traveling routes (a) are concise and efficient paths connecting all points with minimal overlap and distance covered. In contrast, the randomly generated routes (b) exhibit a chaotic pattern with excessive overlaps, resulting in inefficient paths. The MLLM-T with text prompt only shows improvements over the random method but still has room for optimization as some overlaps occur. When we employ the MLLM-V that incorporates both textual and visual prompts (d) for vehicle routing, there is a noticeable enhancement in the optimization of traveling routes. As can be observed, the paths are more coherent, with reduced overlaps and structures more akin to the optimal routes. This reveals the efficacy of MLLM in enhancing optimization processes by leveraging both textual and visual data to generate more effective and efficient routing solutions.\nMoreover, we have conducted a further investigation on the route construction processes for the “P-n19-k2” instance. The MLLM-T’s response is given below:\n- Route 1: The route starts with customer 1 as it is one of the nearest customers to the depot. The vehicle then serves customers 10, 4, 11, 14, and 12 in sequence, following the principle of proximity and creating a path that leads away from the depot towards the top of the map. The vehicle then continues to customers 3, 8, and 16, which form a natural cluster in the top right corner. Finally, the vehicle visits customer 17, which is a bit further away but still in the same general direction, before heading back to the depot. The entire route forms a convex shape that encompasses the northern customers.\n- Route 2: This route begins with customer 2, which is the closest to the depot among the remaining customers. The vehicle then visits customers 7, 5, and 18, which are all relatively close to each other and the depot, forming a small loop. Next, the vehicle heads to customer 6, which is a bit further but still on the way to customers 13 and 15, which are the farthest in this route. The vehicle serves customer 9 last before returning to the depot as it is the most easterly customer and forms a convex polygon when connected with the previous customers and the depot.\nThe MLLM-V’s routes construction process for the same instance is given below:\n-Route 1: The route begins with customer 1, who is nearest to the depot. From there, the vehicle serves customer 10, following the principle of proximity, and continues to customer 4, which forms a line segment with the previous customers. The route then extends to customers 11, 14, and 12, which are all located in the upper part of the map, close to each other. The vehicle then moves to the right, serving customer 3, and continues to 8, 16, and finally 17, which are all in close proximity, forming an almost convex shape. This route utilizes the vehicle’s capacity well, with a total demand of 158, just below the capacity limit of 160.\n-Route 2: This route starts with customer 2, who is relatively close to the depot but on the right side. The vehicle then serves customer 7, followed by 9, which are both nearby. The route extends to customer 15, who is further to the right, and then to 13, which is on the bottom right corner of the map. The vehicle then serves customer 5 and heads back towards the depot, stopping at customer 18 and finally at 6, which is close to the depot. This route also makes good use of the vehicle’s capacity, with a total demand of 150, and covers customers in the lower and right-hand side of the map, ensuring that the vehicle’s path is compact and efficient.\nAs can be observed from the two responses, both methods (even without vertex distribution layout picture) can remember the accurate orientation of the vertexes in the first route. However, the MLLM-T failed to remember the orientation of the vertexes in the second route, thus resulting in a bad solution. In contrast, the MLLM-V can remember the accurate orientation of vertexes based on the distribution layout map, which demonstrates the advantages of using MLLM for optimization. Moreover, both of the methods failed to calculate the correct total demand of the second route, which again confirms that the MLLM is hard to remember accurate tail information as the demand is only given via text."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Why MLLM-V performs better than MLLM-T",
            "text": "To give a deeper insight of the superiority of the proposed MLLM-V, we analyze the different heuristics captured by the model from both the textual and image observations. We compare them with the heuristics captured by the MLLM-T model, which only uses textual observations. We highlight the heuristics that are unique to the MLLM-V model in bold. The heuristics captured from the textual observations are given by:\n- Start by creating clusters of customers based on their proximity to minimize traveling distance within each route.\n- Ensure that the total demand for each route is as close to the vehicle’s capacity as possible without exceeding it.\n- Consider the total traveling cost when constructing routes, aiming to keep it as low as possible.\n- Allow for flexibility in route lengths to achieve overall efficiency.\n- Make sure each customer is visited once and that all routes start and end at the depot.\nAs can be observed, these heuristics are common to both the MLLM-V and MLLM-T models, as they are derived from the problem definition and the objective function, which reflect the basic principles of the vehicle routing problem, such as minimizing total traveling cost, while satisfying the demand and capacity constraints. On the other hand, the heuristics captured from both the textual and image observations are given by:\n- Starting with a clustering approach to group customers based on geographic proximity.\n- Balancing the demand across routes to maximize vehicle capacity utilization.\n- Planning routes that begin and end close to the depot to minimize empty travel.\n- Minimizing the overlap of routes to ensure distinct paths for each vehicle.\n- Considering the directness of travel between customers within the same route.\n- Strategically incorporating outliers into routes without significantly increasing costs.\n- Ensuring that edge cases are handled in a way that does not compromise the efficiency of other routes.\nThe unique heuristics found by MLLM-V have been highlighted in bold font. As can be observed, by incorporating both textual and visual data, the proposed MLLM-V captures more advanced heuristics for vehicle routing than MLLM-T, which learn heuristics based on textual observations alone. These heuristics reveal the superiority of the MLLM-V model over the MLLM-T model, as they provide clear steps to construct cost-efficient traveling routes that lead to more effective outcomes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we presents a new approach to enhance the optimization performance of LLMs by incorporating multimodal inputs, i.e., text and images. We apply the MLLM to a classic combinatorial optimization problem (i.e., CVRP), and conduct empirical studies and analysis to demonstrate its effectiveness. Our experimental results reveal that MLLMs can grasp the inter-dependencies among decision variables and offer more profound insights into the optimization problem, as opposed to LLMs that depend solely on numerical text prompts. Our work illustrates the potential of MLLMs for tackling complex optimization problems and suggests promising directions for future research in this domain.\nAs future work, we would like to extend the application of MLLMs to other challenging optimization problems. Moreover, we aim to improve the optimization performance of MLLMs by generating heuristic functions from the observations, rather than directly producing solutions of problems encountered."
        }
    ]
}