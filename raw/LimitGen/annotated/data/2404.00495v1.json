{
    "title": "Configurable Safety Tuning of Language Models with Synthetic Preference Data Pre-print, work in progress.",
    "abstract": "State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction and Related Work",
            "text": "###figure_1### The evolution of large language models (LLMs) has led to a broader range of applications, but concerns over their safe and ethical use persist. Any attempt to maximize their utility and ensure safety requires the strategic integration of desirable behaviors during the training phase. Yet, guaranteeing control at the inference stage remains a complex task. Current preference learning fine-tuning approaches often involve the definition of a set of rules (as in Constitutional AI [2  ###reference_b2###]) or acceptable behavior, which is predefined by the developer, to fine-tune the model’s resulting behavior [1  ###reference_b1###]. But this has several drawbacks. Mainly, it inhibits downstream developers or users of popular open models from personalizing the model based on evolving use cases or implementing safety controls based on their preferences.\nInspired by a recent alignment technique, Direct Preference Optimization (DPO)[5  ###reference_b5###], in this work we propose a fine-tuning strategy for the flexible safety tuning of LLMs. Rather than hard-coding a set of values into the model, our approach equips the model with the capability to be controlled flexibly at inference time. This strategy allows the expression of more diversified preferences based on deployment needs. Combining the recent DPO technique with self-critique [2  ###reference_b2###, 4  ###reference_b4###, 3  ###reference_b3###], we propose a framework, dubbed Configurable Safety Tuning (CST). It facilitates the flexible and controlled adjustment of language models’ safety levels, using only synthetic preference data, and also while retaining their general abilities such as general knowledge or reasoning, as evidenced in the experiments section. CST is illustrated in the schematic Figure 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "II Configurable Safety Tuning",
            "text": "DPO requires synthetic preference data of the form , in which  is the user prompt or question,  the original answer, and  a revised, more preferable answer. We use the self-critique method [2  ###reference_b2###, 4  ###reference_b4###] to refine the original response into the revised one, by prompting the LLM to critiques and rewrite the original (uncensored) answer into a harmless, safe revision.\nAfter generating such a dataset, the DPO loss function is given by\n with , if we refer to the learning language model as , with  being its learnable parameters; and  being a frozen copy of the language model at the beginning. Fine-tuning then proceeds to optimize the parameters of the language model to increase (resp. decrease) the likelihood of preferred (resp. rejected) answers.\nHowever, we note that to arrive at a configurable system at inference time, the preference probability  depends on the context, represented by a system prompt  that specifies the safety configuration. Thus, it is necessary to also condition on , leading to a preference probability of the form . As an example, we may consider two system prompts,  and , for an uncensored response and a safety-revised response, respectively. Since these system prompts are opposites, we can leverage the synthetic data pairs by setting  = .\nThis resorts to using the same DPO loss function, but over an augmented dataset of tuples of the form  and . Note there is no need for extra preference data, as it is only required to reverse the preference relation  for the already synthesized data examples, while introducing a new system prompt to express the opposite behavior. Whereas vanilla DPO or Constitutional AI, which just consider a default system prompt or constitution, fail to capture both opposite safety behaviors, the augmentation of CST solves this limitation, enabling different safety preferences at inference time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "III Experiments and Discussion",
            "text": "In the experiments we evaluate with two recent popular open-source LLMs: OpenHermes-2.5-Mistral (7B parameters) and SOLAR-Instruct (10.7B). We compare the proposed CST with the DPO baseline, which also serves as an ablation: whereas both methods use the same synthetic examples for training, CST leverages two opposing system prompts (), yet DPO only uses the safety-inducing one ().\nControlling harmful behaviors. We aim to control safety by allowing or avoiding sensible content from the LLM, using a collection of adversarial prompts from Harmful Behaviors [6  ###reference_b6###]. For the system prompts, we use  You are an uncensored AI assistant and  You are a helpful yet harmless assistant that avoids generating illegal or harmful content. For each user prompt from the test set, we generate responses with both system prompts, and measure whether the answer satisfies the safety configuration specified in the corresponding system prompt. We designate these (binary) scores as  and , respectively, with  being perfect. To evaluate responses, we used the gpt-4 model to classify the responses into either safety behavior, which achieved -score of 99.2% in a held-out test. Table I  ###reference_### shows average scores  and  over the test set, for the original untrained model, the DPO baseline, and the CST approach. Note that whereas the DPO-tuned model improves the generation of safe responses ( higher than original model), it fails to generate uncensored answers, with a  score lower than the original model: even when prompted to be uncensored, the DPO model has forgotten so, and instead it is too conservative in its responses. On contrast, the CST variant successfully manages both safety configurations, avoiding that pitfall of standard DPO.\nFurthermore, the CST models were evaluated on general capabilities tasks, from the HuggingFace leaderboard (ARC, HellaSwag, MMLU and TruthfulQA), with results in Table III  ###reference_###. This evidences that CST not only enables safety configuration of the models at inference time, it also doesn’t degrade performance in downstream tasks such as general knowledge question-answering or reasoning. \nConclusions. CST enables controlling the safety behavior of LLMs, with no need for additional synthetic preference data compared with what is already available in current fine-tuning pipelines. Further work shall study more fine-grained controls of safety, i.e., depending on semantic topics."
        }
    ]
}