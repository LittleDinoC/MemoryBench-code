{
    "title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
    "abstract": "In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop 111https://defactify.com/factify3.html, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In an era characterized by an overwhelming influx of information facilitated by the internet and social media, the verification of facts has emerged as an increasingly critical challenge. The proliferation of digital platforms has democratized content creation and dissemination, yet it has also paved the way for the rapid spread of misinformation, disinformation, and misleading content. This phenomenon poses a significant threat to the integrity of information consumed by individuals, communities, and societies at large.\nThe need for robust fact verification mechanisms has become increasingly evident. The ability to distinguish between accurate, credible information and falsehoods has become essential in ensuring informed decision-making, fostering a well-informed citizenry, and preserving the fabric of democratic societies. Fact verification serves as a cornerstone in the pursuit of truth, objectivity, and reliability in an information landscape fraught with distortions and inaccuracies.\nIn recent years, the advent of large language models has revolutionized natural language understanding, enabling machines to comprehend and generate human-like text at an unprecedented scale. Among the myriad applications of these models, one paramount challenge they face is the discernment of factual accuracy in claims against available evidence, due to the inherent limitations and complexities of language understanding. Thus we undertake the task of classifying claims as supported, refuted, or neutral based on provided evidence, advancing the capabilities of language models towards more nuanced and accurate comprehension.\nThis research paper explores the intricate process of enhancing large language models to navigate the terrain of claim verification, employing a spectrum of methodologies. Fine-tuning, recognized for its superior performance, serves as a cornerstone method in our investigation. This paper goes beyond by delving into the significance of in-context learning, which we consider to also encompass the wider spectrum of prompt engineering and prompt tuning, elucidating its role in augmenting models’ understanding of nuanced linguistic contexts. Additionally, it examines the effectiveness and limitations of feature extraction techniques in capturing crucial information relevant to claim classification. Furthermore, the paper explores the benefits of ensemble learning approaches, synthesizing diverse model outputs to enhance classification accuracy and reliability.\nOur approach draws substantial influence from the pioneering work of Du et al.’s Pre-CoFactv2 model [1  ###reference_b1###], as presented during the Factify 2 challenge [2  ###reference_b2###]. We were impressed by the performance of parameter-efficient fine-tuning on the DeBERTa model, and thought to take one step further, dedicating our efforts to a comprehensive fine-tuning of the advanced DeBERTaV3 [3  ###reference_b3###]. The outcome showcases substantial improvement across question answering and text classification tasks when compared to all other assessed methodologies. We hereby introduce our resultant model as Pre-CoFactv3, symbolizing our continuum of innovation derived from our predecessors’ pioneering endeavors.\nJudging by evaluation results, our fine-tuning method outperforms in-context learning and human baseline, when discerning between support, refute and neutral correlations between claim and evidence text, reaching 86 percent accuracy overall on internal validation. When using external validation against other models, our method comes out as state-of-the-art, leading ahead of the next contender by 52 percent.\nBy synthesizing insights from varied methodologies and their contributions, this research aims to contribute significantly to the ongoing discourse on refining large language models for factual inference, paving the way for more robust and reliable natural language understanding in the domain of claim verification."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Traditional Methods",
            "text": "In the landscape of fake news detection, traditional methods relied on rule-based approaches, focusing on specific words or phrases linked to misinformation. Models such as Naïve Bayes, Support Vector Machines (SVM), and Decision Trees exemplify these conventional techniques [4  ###reference_b4###][5  ###reference_b5###]. However, their vulnerability to phrasing variations and evolving tactics limited their effectiveness."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Deep Learning Revolution in NLP",
            "text": "The Deep Learning Revolution in Natural Language Processing (NLP) introduced advanced feature engineering and integrated sophisticated techniques. Notable methodologies like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM) marked this transformative phase. For instance, Kaliyar et al. [6  ###reference_b6###] harnessed deep CNN for multi-layered feature extraction, while Bahad et al. [7  ###reference_b7###] introduced the bi-directional LSTM-RNN architecture with the GLoVe word embedding for effective fake news detection."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Cross-Modality",
            "text": "To address the dynamic nature of misinformation, researchers explored cross-domain learning, integrating NLP knowledge from related domains. This led to the examination of multi-modal data analysis, incorporating text, images, and social media network information. Models like EANN (event adversarial neural network) by Wang et al. [8  ###reference_b8###] and MCAN (multimodal co-attention network) by Wu et al. [9  ###reference_b9###] exemplify these cross-modal approaches. However, due to the unimodal nature of the Factify5WQA dataset [10  ###reference_b10###] (text only), our focus remains on unimodal models."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "The Power of Pre-trained Large Language Models",
            "text": "In recent times, the evolution of Transformer [11  ###reference_b11###] has revolutionized the landscape of natural language processing and information extraction. Pre-trained Large Language Models, exemplified by BERT [12  ###reference_b12###] and GPT [13  ###reference_b13###], reveal profound capabilities in comprehending and generating human-like text, proving particularly adept at identifying fake news.\nIn line with the methodology outlined by Du et al. [1  ###reference_b1###], we leverage the simplicity and effectiveness inherent in various pre-trained models. Our architecture systematically tests different pre-trained models, thereby amplifying efficiency in training and learning patterns from the dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model’s capabilities. The details of our model’s feature extractors are illustrated in Table 1  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###]."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-CoFactv3 Overview",
            "text": "###figure_1### Figure 1  ###reference_### illustrates the overview of our Pre-CoFactv3 framework. Owing to the absence of claim answers, evidence answers, and labels in the testing dataset, we have divided the overall process into two distinct phases: (1) Question Answering and (2) Text Classification. In the first phase, questions are answered by the information derived from both the claim and evidence. Subsequently, in the second phase, the system uses the claim, evidence, and answers obtained from the initial phase to predict the appropriate label, which can be categorized as Support, Neutral, or Refute."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Question Answering",
            "text": "In the Question Answering phase, the input comprises a set denoted by , where . Here,  represents the claim,  denotes the evidence, and  signifies a list of questions associated with the given claim and evidence. The output is represented by a set denoted as , where  and . In this context,  corresponds to the list of claim answers, and  represents the list of evidence answers generated by the Question Answering module.\nWithin the Question Answering module, we employ two distinct methodologies: In-Context Learning and Fine-tuning Large Language Models (LLMs).\nOur initial experimentation focuses on In-Context Learning, with detailed insights provided in Section A.1  ###reference_###."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Fine-tuning Large Language Models (LLMs)",
            "text": "In the Fine-tuning of LLMs, we integrate two sets of LLMs. The first set involves LLMs fine-tuned using the SQuAD 2.0 dataset [14  ###reference_b14###], accessible on Hugging Face. Concurrently, the second set comprises pre-trained LLMs subsequently fine-tuned on the FACTIFY5WQA dataset [10  ###reference_b10###], specifically tailored for the question-answering task. The objective of the question-answering task is to input the context (claim or evidence) and subsequently identify the index corresponding to the location of the answer within that context. The formulation is as below:\nComprehensive experimental results will be presented in Section 4.4  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Text Classification",
            "text": "In the Text Classification phase, the input is represented as , encompassing the claim , evidence , question , and the claim answer  and evidence answer  obtained from the preceding Question Answering phase. The output of the Text Classification module is the predicted label, which falls into one of the categories: Support, Neutral, or Refute.\nWithin the Text Classification module, three distinct methodologies are employed: In-Context Learning, FakeNet, and Fine-tuning Large Language Models (LLMs).\nOur initial experimentation centers around In-Context Learning, and comprehensive insights into this approach are elucidated in Section A.2  ###reference_###.\nWe leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model’s capabilities. The details of our model’s feature extractors are illustrated in Table 1  ###reference_###  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###  ###reference_b1###]."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 FakeNet",
            "text": "Building upon the foundations laid by the previous work, Pre-CoFactv2 [1  ###reference_b1###], we have introduced a novel framework named FakeNet. Figure 2  ###reference_### provides an overview of FakeNet.\n###figure_2### We leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###  ###reference_b11###  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###  ###reference_b15###  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###  ###reference_b16###  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model’s capabilities. The details of our model’s feature extractors are illustrated in Table 1  ###reference_###  ###reference_###  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###  ###reference_b17###  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###  ###reference_b1###  ###reference_b1###]."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Fine-tuning Large Language Models (LLMs)",
            "text": "Conversely, we explore an alternative approach by fine-tuning the entire pre-trained LLMs on the text classification task. In this configuration, the input is a sequence formed by concatenating the claim, evidence, question, claim answer, and evidence answer. The output of the model is the probability distribution over the three labels: Support, Neutral, or Refute."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Ensemble",
            "text": "Inspired by the approach outlined in [1  ###reference_b1###], we incorporate ensemble learning to amalgamate informative knowledge from various models, aiming to enhance the overall predictive performance. Four distinct ensemble methods have been designed:\nWeighted sum with labels:\nPower weighted sum with labels:\nPower weighted sum with two models:\nPower weighted sum with three models:\nIn methods 1 and 2, the terms , , and  denote the new probabilities for Support, Neutral, and Refute, respectively. Meanwhile, , , and  represent the probabilities for Support, Neutral, and Refute obtained from model . In methods 3 and 4, the notation  signifies the new probabilities for all labels, while  denotes the probabilities for all labels derived from model .\nThe experimental results will be shown in Section 4.5  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task’s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_1### To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3  ###reference_###.\n###table_2### Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment.\nIn the ensuing experiment, our objective is to assess whether the inclusion of features can enhance performance. We select 65 features as input and apply various normalization techniques. The detailed results are presented in Table 4  ###reference_###.\n###table_3### Our findings reveal that the inclusion of features normalized between -1 and 1 results in the best performance, albeit with a slight improvement of 0.19%.\nIn the fine-tuning process, we employ the microsoft/deberta-v3-large [3  ###reference_b3###] 555https://huggingface.co/microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task 666https://huggingface.co/docs/transformers/tasks/sequence_classification, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5  ###reference_###.\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiments Environment",
            "text": "All experiments were executed on a machine equipped with 32 AMD EPYC 7302 16-Core CPUs, 2 NVIDIA RTX A5000 GPUs, and 252GB of RAM. Python serves as our primary programming language. The source code is available at https://github.com/AndyChiangSH/Pre-CoFactv3."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "We utilize the dataset FACTIFY5WQA [10  ###reference_b10###][18  ###reference_b18###] provided by the AAAI-24 Workshop Factify 3.0. This dataset is designed for fact verification, with the task of determining the veracity of a claim based on given evidence. In our specific task, we augment this dataset by introducing questions that entail the answers derived from the claim and evidence, thus framing the fact verification as an entailment problem. The dataset consists of 10,500 samples for training, 2,250 samples for validation, and 2,250 samples for testing, totaling 15,000 samples. Each sample in the training and validation sets includes fields for claim, evidence, question, claim answer, evidence answer, and label. For the testing set, the fields include claim, evidence, and question.\nClaim: the statement to be verified.\nEvidence: the facts to verify the claim.\nQuestion: the questions generated from the claim by the 5W framework (who, what, when, where, and why).\nClaim answer: the answers derived from the claim.\nEvidence answer: the answers derived from the evidence.\nLabel: the veracity of the claim based on the given evidence, which is one of three categories: Support, Neutral, or Refute."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Metric",
            "text": "The official competition metric for Factify 3.0 involves computing the average BLEU score for answers to questions derived from both the claim and evidence. A prediction is deemed correct only if this score exceeds a predefined threshold and the label is accurate. The final accuracy is then calculated as the percentage of correct predictions.\nFollowing this, in the Question Answering task, we employ the BLEU score with 4-gram precision [19  ###reference_b19###]. In the case of the Text Classification task, accuracy serves as the evaluation metric. Detailed experimental results are provided in the ensuing sections."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Question Answering",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task’s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_4###"
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Fine-tuning Large Language Models (LLMs)",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###  ###reference_###  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task’s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_5###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Text Classification",
            "text": "In this section, we will present experimental results for the text classification task. To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3. Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment.\n\nIn the fine-tuning process, we employ the microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5.\n\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 FakeNet",
            "text": "During the training of FakeNet, specific configurations were employed. The batch size was set to 24, the learning rate to 0.00005, and the number of epochs depended on the specific LLMs used. The input dimension for text and answer was 1024, the hidden dimension of FakeNet was set to 256, and the number of co-attention heads was specified as 2. Additionally, the input dimensions of the Pre-trained LLM and the Feature Extractor were set to 256 and 32, respectively. To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3 ###reference_### ###reference_### ###reference_###. ###table_8### Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment. In the ensuing experiment, our objective is to assess whether the inclusion of features can enhance performance. We select 65 features as input. The detailed results are presented in Table 4 ###reference_### ###reference_### ###reference_###. ###table_9###"
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Fine-tuning Large Language Models (LLMs)",
            "text": "In the fine-tuning process, we employ the microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5.\n\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.5.3",
            "parent_section_id": "4.5",
            "section_name": "4.5.3 Ensemble",
            "text": "The experimental results for the four ensemble methods mentioned in Section 3.3.3  ###reference_.SSS3### are provided in Table 6  ###reference_###. Our examination indicates that the ensemble method, utilizing a power weighted sum with three models (two Fine-tuned LLMs and one FakeNet), achieves the highest accuracy. Hence, we have selected this ensemble model as our best result. Figure 3  ###reference_### displays the confusion matrix for these three models and the ensemble model. The ensemble approach demonstrates the ability to capitalize on the strengths of each individual model, leading to an overall enhancement in accuracy. However, it is noteworthy that all models exhibit difficulty in accurately identifying instances of \"Support,\" thereby limiting the extent of performance improvement in this particular category. ###figure_3###"
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": ""
        },
        {
            "section_id": "4.6.1",
            "parent_section_id": "4.6",
            "section_name": "4.6.1 In-Context Learning Baseline",
            "text": "The In-Context Learning baseline uses the ChatGPT API to generate replies with the best prompt we could come up with, more details about prompt engineering in A.2  ###reference_###. Around one hundred claim-evidence pairs were fed into the Large Language Model, and the respective labels were extracted from their replies.\nThe result in Table 7  ###reference_### seemed skewed towards support, but after further analysis, we found it rather difficult to differentiate between support and neutral, even as humans. We take the lower percentages of correct neutral labels to mean the prompt we were using favored support when in an ambiguous state. Overall, it does not outperform the human baseline, and falls way short of our Pre-CoFactv3 model."
        },
        {
            "section_id": "4.6.2",
            "parent_section_id": "4.6",
            "section_name": "4.6.2 Human Baseline",
            "text": "To assess the effectiveness of our model, we established a human baseline through a questionnaire created using the dataset. The objective was to gauge how well our model performs compared to human beings in identifying fake news. For simplicity, we selected 10 entities from the dataset and distributed them to 20 individuals, asking them to categorize the claim news. The results are presented in the Table 7  ###reference_###.\nUpon analysis, the accuracy of human identification was found to be 44%, significantly lower than our model’s performance. Moreover, it became apparent that, concerning the Refute metric, a significant number of participants struggled to provide correct answers. This challenge likely stems from the ambiguous boundary between Neutral and Refute metric. This observation serves as a foundation for illustrating the efficacy of our model in surpassing human judgment in fake news identification."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Testing Result",
            "text": ""
        },
        {
            "section_id": "4.7.1",
            "parent_section_id": "4.7",
            "section_name": "4.7.1 Final Submission",
            "text": "In our final submissions, we present three versions of our work, and the corresponding testing accuracy is provided in Table 8  ###reference_###. It is noteworthy that the performance of the ensemble model did not meet our expectations, possibly indicating overfitting issues. Given that the fine-tuned LLM for both Question Answering and Text Classification demonstrates the highest testing accuracy, our observation leads us to assert that \"Fine-tuning is all you need.\"\n###table_10###"
        },
        {
            "section_id": "4.7.2",
            "parent_section_id": "4.7",
            "section_name": "4.7.2 Leaderboard",
            "text": "Table 9  ###reference_### displays the leaderboard for the Factify 3.0 Workshop. We are delighted to announce that our team, Trifecta, won first place in the workshop. Our performance surpassed the baseline by an impressive 103%, maintaining a lead over the second competitor by 70%.\n###table_11###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations & Discussions",
            "text": "Fine-tuning a model demands a wealth of annotated data for effective supervised training, and any shortfall in labeled data can detrimentally impact the training outcomes. Moreover, the current dataset is confined to English data, restricting its applicability across different languages.\nThe real-world data introduces added complexities, with nuances in semantics and incomplete information posing challenges to the model’s recognition capabilities, such as using a pre-built database of evidence to fact-check claims made on social media and other information-sharing platforms. While this brings into question whether previously collected evidence could confirm the news as it happens, it could very well be used to prune incoming data, lessening the load for further examination, whether by more complex models or human intervention.\nIn the fine-tuning process, parameters like model size, max length, and batch size are intricately tied to GPU memory constraints. Exploring options such as expanding GPU memory or adopting Parameter-Efficient Fine-Tuning (PEFT) raises intriguing questions for future research, particularly in relation to their potential to amplify recognition accuracy."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduced Pre-CoFactv3, a framework composed of two integral parts: (1) Question Answering and (2) Text Classification. For the Question Answering task, we employed In-Context Learning and Fine-tuned Large Language Models (LLMs) to answer questions based on the claim and evidence. In the Text Classification task, our approach encompassed In-Context Learning, the FakeNet model, and Fine-tuned LLMs to predict label categories.\nOur experiments demonstrated the efficacy of our diverse approaches, encompassing the comparison of various Pre-trained LLMs, the development of the FakeNet model, and the exploration of different ensemble methods. Additionally, we established two baseline models for In-Context Learning and Human performance.\nThe culmination of our efforts resulted in the success of our team, Trifecta, securing the first-place position in the AAAI-24 Factify 3.0 Workshop. Our performance surpassed the baseline by an impressive 103%, maintaining a substantial lead over the second competitor by 70%.\nIn summary, our accomplishment validates the effectiveness of our approach and the thoughtful integration of diverse techniques. As we continue to advance in the field of fact verification, our experiences offer valuable insights and lessons for the broader research community."
        }
    ]
}