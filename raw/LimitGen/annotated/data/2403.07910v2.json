{
    "title": "MAGPIE: Multi-Task Analysis of Media-Bias Generalization with Pre-Trained Identification of Expressions",
    "abstract": "Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability.\nTo address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection.\nTo enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks.\nMAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score.\nMAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB).\nUsing a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches.\nOur evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and scaling tasks leads to the best results.\nMAGPIE confirms that MTL is a promising approach for addressing media bias detection, enhancing the accuracy and efficiency of existing models. Furthermore, LBM is the first available resource collection focused on media bias MTL.\n\n\n\nKeywords: Media bias, Multi-task learning, Text classification",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Media bias is the skewed portrayal of information favoring certain group interests Recasens et al. (2013  ###reference_b76###), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task Raza et al. (2022  ###reference_b74###). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets Krieger et al. (2022  ###reference_b46###), which exhibit limited generalizability across diverse domains Wessel et al. (2023  ###reference_b104###).\nThis paper builds upon the work of Wessel et al. (2023  ###reference_b104###), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) Aribandi et al. (2021a  ###reference_b4###); Chen et al. (2021  ###reference_b15###); Kirstein et al. (2022  ###reference_b44###) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method Spinde et al. (2022  ###reference_b90###) underperforms due to its narrow task focus and does not surpass baseline outcomes.\n###figure_1### In this study, we make five main contributions:\nWe present MAGPIE (Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions) the first large-scale multi-task pre-training approach for media bias detection.\nBy including diverse biases like persuasive and subjective, a classifier trained on top of MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1  ###reference_###).\nWe introduce LBM (Large Bias Mixture), a pre-training composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias.\nWe provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks.\nWe demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset Spinde et al. (2021c  ###reference_b91###) and achieves state-of-the-art results on the Media Bias Identification Benchmark (MBIB) collection Wessel et al. (2023  ###reference_b104###).\nWe make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub.\ngithub.com/magpie-multi-task  ###reference_-multi-task###\nThese contributions highlight the potential of MTL in improving media bias detection.\nOur findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results.\nAnother key insight of our research is the value of MTL in contexts where the primary dataset is small111For example, the Media Bias Annotation by Experts (BABE) dataset Spinde et al. (2021c  ###reference_b91###)..\nBy learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data.\nOverall, our research offers a novel multi-task learning approach with a variety of media bias tasks, contributing to the understanding of media bias detection."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Media Bias",
            "text": "Media bias is a complex issue Lee et al. (2021a  ###reference_b49###); Recasens et al. (2013  ###reference_b76###); Raza et al. (2022  ###reference_b74###) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias Wessel et al. (2023  ###reference_b104###).\nIn their literature review, Spinde et al. (2023  ###reference_b87###) provide an extensive overview of research on media bias and related subtypes of bias.\nMedia bias detection approaches have evolved from hand-crafted features Recasens et al. (2013  ###reference_b76###); Hube and Fetahu (2018  ###reference_b40###); Spinde et al. (2021d  ###reference_b92###) to neural models Spinde et al. (2022  ###reference_b90###); Chen et al. (2021  ###reference_b15###); Spinde et al. (2021c  ###reference_b91###); Huguet Cabot et al. (2021  ###reference_b42###); Sinha and Dasgupta (2021  ###reference_b83###); Raza et al. (2022  ###reference_b74###).\nHowever, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets Wessel et al. (2023  ###reference_b104###).\nAs most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019  ###reference_b28###)).\nWe will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain.\nMedia bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications Recasens et al. (2013  ###reference_b76###); Spinde et al. (2021b  ###reference_b89###), which, so far, are not explored in relation to each other Spinde et al. (2023  ###reference_b87###).\nWessel et al. (2023  ###reference_b104###) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias.\nTheir study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities.\nMAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks due to a variety of limitations in MBIB (see Section 3.1  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Multi-Task Learning",
            "text": "MTL shows significant improvements in various NLP tasks, including sentiment analysis He et al. (2019  ###reference_b36###), text summarization Kirstein et al. (2022  ###reference_b44###), and natural language understanding Raffel et al. (2020  ###reference_b73###).\nIn MTL, a model leverages knowledge gained from one task to improve the performance of others.\nAribandi et al. (2021b  ###reference_b5###) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021  ###reference_b1###) show that pre-finetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold.\nAs described above, media bias can be seen as a composite problem composed of various interrelated bias types Wessel et al. (2023  ###reference_b104###). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b  ###reference_b5###). For instance, benchmarks such as GLUE and SuperGLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection and semantic evaluation, thereby substantially improving performance across a range of NLU tasks Wang et al. (2018  ###reference_b99###, 2019  ###reference_b98###). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues.\nThe selection of tasks is pivotal to the efficacy of MTL.\nThere have been several attempts to automate task selection, including learning the data selection with Bayesian optimization Bingel and Søgaard (2017  ###reference_b10###) or estimating task relations Ruder and Plank (2017  ###reference_b77###).\nThe most model-agnostic approach is GradTS Ma et al. (2021  ###reference_b56###), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE.\nGradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task’s attention.\nThe selected tasks are trained jointly and share representations across tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodolodgy",
            "text": "We implement MAGPIE using pre-finetuning as introduced in Aghajanyan et al. (2021  ###reference_b1###) (See also Section 2  ###reference_###).\nAs such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), an extensive task collection of bias-related datasets222LBM is also published within this paper, see Section 3.1  ###reference_###..\nWe incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1  ###reference_.SSS1###).\n###figure_2### As outlined in Figure 2  ###reference_###, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset333BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model’s generalizability within a single dataset context. Spinde et al. (2021c  ###reference_b91###), and the Media Bias Identification Benchmark (MBIB) collection444MBIB provides a variety of bias tasks for testing and is currently the most extensive media bias benchmark available. Wessel et al. (2023  ###reference_b104###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   The LBM (Large Bias Mixture) task collection",
            "text": "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias555As per the definition of directly and indirectly related media bias tasks given by Spinde et al. (2023  ###reference_b87###). more indirectly, such as sentiment analysis or emotion classification. MAGPIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3  ###reference_###. First, we manually assess a list of 115 media bias-related datasets categorized into task families composed by Wessel et al. (2023  ###reference_b104###). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks, such as proposed by Aribandi et al. (2021b  ###reference_b5###) for general language tasks. We filter the collection of the 115 datasets based on the following criteria666We base these on the criteria set by Wessel et al. (2023  ###reference_b104###). However, we acknowledge that determining the dataset quality remains a manual and subjective choice.: Accessibility: Datasets have to be publicly accessible. Text granularity: We only use datasets labeled on a sentence level (not on, for instance, article or word level) Quality of annotations: We exclude datasets with little to no documentation or unreliable annotation sources. Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levels777One dataset has annotations on only words, two on users, and three on outlets.. We remove 5 datasets due to low quality or unreliable annotations and discard 4 duplicates. Applying these criteria leaves 38 datasets. Including 8 handpicked datasets not originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks can be found in Table 4  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   The Base Model",
            "text": "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa Liu et al. (2019  ###reference_b54###) encoder due to its proven state-of-the-art performances across various media bias applications Spinde et al. (2021c  ###reference_b91###); Krieger et al. (2022  ###reference_b46###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   The MTL framework",
            "text": "Pre-finetuning.\nTo effectively harness the generalization abilities of MTL for media bias detection, we adopt a pre-finetuning procedure Aghajanyan et al. (2021  ###reference_b1###).\nPre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training.\nWhile incorporating MTL directly into the pre-training stage has demonstrated performance gains Aribandi et al. (2021b  ###reference_b5###), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL Aghajanyan et al. (2021  ###reference_b1###).\nSharing representations.\nWe use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads.\nFor each task, we attach dense layers, or \"heads\", to the shared encoder.\nThese heads are optimized individually per task while the shared encoder learns general bias representations.\nHowever, multi-task optimization presents challenges due to differing gradient directions and magnitudes Yu et al. (2020  ###reference_b108###).\nFor instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum.\nOn the other hand, if Task A’s gradient greatly surpasses that of Task B, gradient A becomes dominant.\nWe counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling Yu et al. (2020  ###reference_b108###).\nConflicting gradients and loss scaling.\nIn multi-task training involving  tasks, encoder parameters receive  potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad Yu et al. (2020  ###reference_b108###), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks.\nTherefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of  sets per update.\nAdopting Muppet’s method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps Aghajanyan et al. (2021  ###reference_b1###)."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1.   Data sampling and early stopping",
            "text": "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL Aribandi et al. (2021b  ###reference_b5###); Spinde et al. (2022  ###reference_b90###).\nWe also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster.\nHowever, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM.\nWhen task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B.\nWe aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches:\nHead-Specific-Early-Stopping (HSES)\nResurrection\nHSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers’ continuous learning, especially after they have reached an optimal state.\nResurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again.\nThis enables the task to adapt its head parameters to the latest latent representation.\nHSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4  ###reference_###. However, we stress the need for further extensive analysis in Limitations  ###reference_###."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2.   Auxiliary task selection",
            "text": "Multi-Task Learning often involves selecting well-used datasets, leading to potential selection biases.\nFurthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes.\nTo automate the process of task selection, we utilize the GradTS algorithm Ma et al. (2021  ###reference_b56###).\nWe choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability.\nGradient-based Task selection.\nIn line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrix888RoBERTa has 12 attention heads on each of the 12 layers. for each task. Tasks are sorted by correlation between each task’s matrix and BABE task’s matrix. We pre-finetune  models on the first  tasks from the sorted list, where  varies from 1 to  and  is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal  determined by evaluation loss.\nFor further details on the GradTS algorithm, please see Ma et al. (2021  ###reference_b56###)."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3.   Hyperparameters and training stability",
            "text": "Finetuning transformer models on small datasets often leads to inconsistent outcomes, such as overfitting, training instabilities, or significant performance variabilities Dodge et al. (2020  ###reference_b26###).\nTo ensure reproducibility and consistency, we use a fixed random seed for each run and perform individual task hyperparameter grid searches. To negate the influence of random weight initializations, we evaluate and compare models using 30 random seeds, reporting average performance.\nWe use  as a random seed for all experiments. For the final evaluation, where we employ 30 random seeds, we use values from .\nFor optimizing the model, we use a per-task batch size\nof 32, an AdamW optimizer, and a polynomial learning scheduler.\nWe run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Empirical Results",
            "text": "In this section, we present the results of our MTL approach.\nFirst, we report the set of auxiliary tasks selected by GradTS for pre-finetuning.\nNext, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks.\nWe also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark.\nThen, we analyze the LBM taxonomy through a study on knowledge transfer between families.\nLastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Auxiliary tasks selection",
            "text": "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices.\nWe use Kendall’s correlation coefficient, as suggested by Puth et al. (2015  ###reference_b72###).\nWe find a local minimum for the BABE evaluation loss when pre-finetuning on the first  most correlated tasks.\nThe final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1  ###reference_###.\nThe tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Evaluation",
            "text": "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline.\nThe multi-task sets are the following:\nMTL:Random - Random Subset of 10 tasks\nMTL:GradTS - Subset of 10 tasks selected by GradTS algorithm\nMTL:All - Set of all tasks\nWe also evaluate the model pre-finetuned on the set of all tasks on MBIB.\nWe follow the guidelines set by Wessel et al. (2023  ###reference_b104###) for the evaluation.\nGiven that MAGPIE’s pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases.\n###figure_4### Multi-Task Learning Performance.\nSection 4.2  ###reference_### summarizes our performance results on the BABE dataset.\nWe observe that all of our MTL pre-finetuning schemas lead to performance improvements.\nIn particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c  ###reference_b91###).\nWhile both MTL baselines - Muppet Aghajanyan et al. (2021  ###reference_b1###) and UnifiedM2 Lee et al. (2021b  ###reference_b50###) outperform single-task baseline, they underperform all of our MTL models.\nTask scaling.\nGradTS task selection outperforms random tasks on average performance, yet our experiment suggests task number scaling is more crucial. This is consistent with Muppet and ExT5 results Aghajanyan et al. (2021  ###reference_b1###); Aribandi et al. (2021b  ###reference_b5###), indicating MTL can compensate for scarce high-quality media bias datasets through general bias representation from other tasks. It also supports Kirstein et al. (2022  ###reference_b44###)’s finding that sufficient related tasks can substitute the original task.\nOn MBIB, our performance surpasses that of the five models evaluated by Wessel et al. (2023  ###reference_b104###) in 5 out of 8 tasks.\nAdditionally, it improves upon the current baseline with an average F1-Score of 0.7692 across all tasks. However, this improvement is only marginal.\nMore importantly, MAGPIE outperforms the single-task RoBERTa baseline on MBIB, showing that an MTL approach is beneficial in media bias detection.\n###figure_5### Step efficiency.\nIn addition to the performance improvements achieved, we also assess our model training efficiency.\nIn Figure 5  ###reference_###, we show the F1 score on the development set for the BABE task, averaged over all 30 runs.\nOur findings show that Multi-Task Learning only requires 1̃5% of the training steps used in single-task finetuning on BABE.\nThis result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   LBM taxonomy analysis",
            "text": "In Section 3.1  ###reference_###, following Aribandi et al. (2021b  ###reference_b5###), we introduce data task families. Aribandi et al. (2021b  ###reference_b5###) uses task families for selection and knowledge transfer.\nTo assess task families’ significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer.\nTo account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families.\nWe train tasks from the same family together and report the average change in task performance, as depicted in Figure 6  ###reference_###.\nNegative knowledge transfer is prevalent across most of our task families.\nHowever, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance.\n###figure_6### Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Table 3  ###reference_###.\nOur results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families.\nConversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%.\nOn the other hand, the Emotionality family is significantly impaired by negative transfer from other task families.\n###figure_7### The full table of transfers can be found in Figure 7  ###reference_###.\nConsidering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Resurrection and HSES evaluation",
            "text": "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation.\nWe calculate the average evaluation loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection.\nThe boxplot in Figure 8  ###reference_### shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%.\nHowever, we hypothesize that a random constellation of tasks999Particularly variance in task sizes and quality. can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.\n###figure_8### ###figure_9###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multi-task learning (MTL) approach that enhances traditional models with a multi-task structure.\nAdditionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pre-training of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias.\nOur study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative transfer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far.\nFurthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches.\nThese findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias.\nWhile results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum.\nUnderstanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies.\nAs media bias is emerging globally, incorporating multilingual models is a natural extension."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Bibliographical References",
            "text": ""
        }
    ]
}