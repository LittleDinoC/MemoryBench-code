{
    "title": "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent",
    "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for defense, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large Language Models (LLMs) have garnered significant attention due to their impressive performance across a wide range of natural language tasks (Minaee et al., 2024  ###reference_b34###). The pre-trained LLMs, such as Meta’s LLAMA (Touvron et al., 2023a  ###reference_b53###, b  ###reference_b54###) and OpenAI’s ChatGPT (OpenAI, 2022  ###reference_b38###; Achiam et al., 2023  ###reference_b1###), have become essential foundations for AI applications in various sectors such as healthcare, education, and visual tasks (Kasneci et al., 2023  ###reference_b20###; Thirunavukarasu et al., 2023  ###reference_b52###; OpenAI, 2023  ###reference_b39###; Köpf et al., 2024  ###reference_b21###; Romera-Paredes et al., 2024  ###reference_b43###). Despite their widespread use and convenience, concerns about the security of these models are increasing. Specifically, LLMs have been shown to be vulnerable to adversarial textual examples (Wang et al., 2023a  ###reference_b57###; Xu et al., 2024  ###reference_b64###), which involve subtle modifications to textual content that maintain the same meaning for humans but completely change the prediction results to LLMs, often with severe consequences.\nTo achieve robust defense against adversarial attacks on Large Language Models (LLMs), a prevalent strategy is fine-tuning the LLMs with adversarial examples to enhance model alignment (Shen et al., 2023  ###reference_b44###; Wang et al., 2023c  ###reference_b60###). LLM-based adversarial fine-tuning (AFT) can be implemented either through in-context learning (Dong et al., 2022  ###reference_b13###; Xiang et al., 2024  ###reference_b63###) or by optimizing the parameters of pre-trained LLMs using adversarial examples (Dettmers et al., 2024  ###reference_b12###; Li et al., 2024b  ###reference_b25###). However, LLM-based AFT methods necessitate additional computational resources and training time. Achieving robust and reliable LLMs typically requires significant costs (Hu et al., 2022  ###reference_b19###), which is prohibitive for ordinary users. Additionally, due to the discrete nature of textual information, these adversarial examples can have substitutes in any token of the sentence, with each having a large candidate list (Li et al., 2023  ###reference_b23###). This leads to a combinatorial explosion, making the application of AFT methods challenging or resulting in poor generalization when trained on a limited dataset of adversarial examples. Consequently, developing an efficient and user-friendly robust LLM system remains a huge challenge and an urgent issue that continues to be addressed.\nIn this paper, focusing on adversarial textual attacks targeting LLM-based classification tasks, we propose a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which utilizes the LLM as a defense agent for adversarial purification, as illustrated in Figure 1  ###reference_###. To streamline our explanation, we condense certain details in Figure 1, with comprehensive instructions provided in Section 3  ###reference_###. Specifically, LLAMOS comprises two components: Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence, and Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. LLAMOS serves as a pre-processing method aiming to eliminate harmful information from potentially attacked textual inputs before feeding them into the target LLM for classification. In contrast to the AFT method, the LLM-based AP method functions as an additional module capable of defending against adversarial attacks without necessitating fine-tuning of the target LLM.\nWe comprehensively evaluate the performance of our method on GLUE datasets, conducting experiments with both representative open-source and closed-source LLMs, LLAMA-2 (Touvron et al., 2023b  ###reference_b54###) and GPT-3.5 (OpenAI, 2022  ###reference_b38###). The experimental results demonstrate that the LLM-based AP method effectively defends against adversarial attacks. Specifically, our proposed method achieves a maximum reduction in the attack success rate (ASR) by up to 45.59% and 37.86% with LLAMA-2 and GPT-3.5, respectively. Additionally, we observe that the initial defense agent fails to achieve the expected results under some obvious attacks. Therefore, we employ the in-context learning (Dong et al., 2022  ###reference_b13###) to further optimize the defense agent, significantly enhancing the defense capabilities almost without adding any additional costs. Finally, we conduct an intriguing online adversarial experiment, creating an adversarial system using two LLM-based agents (one for defense and one for attack) along with a target LLM for classification. During the adversarial interaction, the defense agent and attack agent continuously counter each other, resembling the adversarial training process in traditional image tasks (Goodfellow et al., 2015  ###reference_b17###).\nOur contributions are summarized as follows.\nWe propose a novel defense technique named LLAMOS, which aims to purify the adversarial textual examples before feeding them into the target LLM. To the best of our knowledge, we are the first to employ an LLM agent to enhance the adversarial robustness of LLMs.\nThe defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing step. Notably, it operates without retraining of target LLM, rendering it efficient and user-friendly.\nWe conduct extensive experiments to empirically demonstrate that the proposed method can effectively defend against adversarial attacks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "This section briefly reviews the adversarial attacks and evaluations of adversarial robustness on LLMs."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Adversarial Attcks on LLMs",
            "text": "Given a target LLM  with task instruction, input , and correct output , the adversarial attacks aim to find the adversarial examples  that can fool the target LLM  on classification tasks. The adversarial examples  can be obtained by the LLM itself  with different system prompt (Xu et al., 2024  ###reference_b64###),\nwhere  represents textual perturbations from a series of candidate sets for modifications, which are made at the character level, word level, or sentence level. In a specific instance, the system prompt of  can be: “Analyze the tone of this statement and respond with either ‘positive’ or ‘negative’.” and the system prompt of corresponding  can be: “Your task is to generate a new sentence that keeps the same semantic meaning as the original one but be classified as a different label.” There are more details in Appendix B  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Evaluations of LLMs Robustness",
            "text": "To evaluate the effectiveness of the defense method, we follow the setting from Wang et al. (2021  ###reference_b56###); Xu et al. (2024  ###reference_b64###), using the attack success rate (ASR) and traditional robust accuracy (RA) on the adversarial examples as measures of the robustness of the defense method.\nwhere  is an indicator function.  is the original test dataset and  is the adversarial example dataset, and  is the number of examples. The lower the ASR, the higher the RA, indicating greater model robustness."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We propose a novel defense technique for large language model-based adversarial purification (LLAMOS), which purifies adversarial examples by the LLM-based defense agent before feeding examples into the target LLM. The overall pipeline of LLAMOS is outlined in Section 3.1  ###reference_###. Subsequently, we further augment the defense agent using in-context learning as discussed in Section 3.2  ###reference_###. Finally, in Section 3.3  ###reference_###, we present the design of the adversarial system, incorporating the defense agent, attack agent, and target LLM."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Enhencing the Defense Agent with In-Context Learning",
            "text": "In the initial defense agent, the defense guidance relies on common sense, which may result in poor performance against some special attacks, even when the attacker adds obvious characters. To address this limitation, we introduce in-context learning (Dong et al., 2022  ###reference_b13###) to further optimize the defense agent. The prompts of in-context learning are described in the following.\n# In-Context Learning\nThe new sentence still contains a lot of harmful content caused by adversarial attacks, such as [Specific Guidance]. Please consider these contents and output a new sentence for me. \nInput: [Input]. Now, let’s start the defense process and only output the generated sentence.\nThe specific guidance is designed to assist the defense agent in better understanding an attack and generating a new sentence capable of effectively defending against the attack. These guidelines can be fine-tuned to address specific attacks and can be incorporated into the defense agent as needed. Through in-context learning, the defense agent can be continuously optimized, significantly enhancing its performance almost without adding any additional costs."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Adversarial System with Multiple LLMs",
            "text": "In this section, we devise an adversarial system involving multiple LLMs. Given that our method introduces a defense agent against attackers, a natural idea is to then create an attack agent to counter the defender. The attack agent is tasked with generating adversarial examples from purified examples to deceive the target LLM once more. To accomplish this, we design prompts for generating an attack agent , as described in the following.\n# Attack Agent Instruction\nTo begin, let me provide a brief overview of the input text: [Input Description]. The classification task for these sentences is [Task Description].\nYour task is to generate a new sentence that replaces the original one, which must satisfy the following conditions: [Attack Instruction]. \n# Attack Guidance\nFor example, the original sentence [Purified Example] is classified as [Correct Label]. You should generate a new sentence which is classified as [Incorrect Label]. \nInput: [Input]. Now, let’s start the attack process and only output the generated sentence.\nThe prompt structure of the attack agent and the defense agent is basically the same, although there are some differences in details. The input description of the attack agent includes the correct label , and the input format is . The attack instruction is “1. The new sentence should be classified as the opposite of the ‘correct label’. 2. Change at most two letters in the sentence.” Finally, we provide a specific example to help the attack agent better understand the attack task.\nThen, we combine the defense agent and attack agent to form an adversarial system, as illustrated in Figure 2  ###reference_###. In the adversarial system, the purified examples can be attacked again by the attack agent, and likewise, the adversarial examples can also be purified by the defense agent. They continuously counter each other, much like adversarial training (Goodfellow et al., 2015  ###reference_b17###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Adversarial Attack. Deep neural networks (DNNs) are vulnerable to adversarial examples (Szegedy et al., 2014  ###reference_b50###), which are generated by adding small, human-imperceptible perturbations to natural examples, but completely change the prediction results to DNNs (Goodfellow et al., 2015  ###reference_b17###; Lin et al., 2024b  ###reference_b27###). With the rapidly increasing applications of LLMs (OpenAI, 2023  ###reference_b39###; Köpf et al., 2024  ###reference_b21###; Romera-Paredes et al., 2024  ###reference_b43###),\nsecurity concerns have emerged as a critical area of research (Gehman et al., 2020  ###reference_b14###; Bender et al., 2021  ###reference_b3###; Mckenna et al., 2023  ###reference_b33###; Manakul et al., 2023  ###reference_b32###; Liu et al., 2023b  ###reference_b29###; Zhu et al., 2023  ###reference_b69###; Li et al., 2023  ###reference_b23###; Qi et al., 2024  ###reference_b41###; Yao et al., 2024b  ###reference_b68###), with researchers increasingly focusing on adversarial attacks targeting LLMs. In a similar setup to DNNs, for LLMs, attackers manipulate a small amount of text to change the output of the target LLM while maintaining the semantic information for humans (Wang et al., 2024  ###reference_b58###; Xu et al., 2024  ###reference_b64###). Presently, addressing the security issues surrounding LLMs is of paramount importance and requires urgent attention.\nAdversarial Defense. There are two main defense techniques on traditional DNNs, including adversarial training (AT) (Goodfellow et al., 2015  ###reference_b17###) and adversarial purification (AP) (Shi et al., 2021  ###reference_b45###; Srinivasan et al., 2021  ###reference_b48###). Unlike traditional DNNs, retraining LLMs is nearly impossible due to cost issues (Li et al., 2023  ###reference_b23###). Therefore, most methods enhance the robustness of LLMs through adversarial fine-tuning (AFT) (Xiang et al., 2024  ###reference_b63###; Li et al., 2024b  ###reference_b25###; Bianchi et al., 2023  ###reference_b5###; Deng et al., 2024  ###reference_b11###; Qi et al., 2024  ###reference_b41###). While AFT can effectively defend against attacks, it remains susceptible to unseen attacks whose adversarial examples that the LLMs have not previously learned (Li et al., 2023  ###reference_b23###). Additionally, even with fine-tuning, training the LLMs will still consume a significant cost (Hu et al., 2022  ###reference_b19###; Dettmers et al., 2024  ###reference_b12###).\nAdversarial purification (AP) aims to purify adversarial examples before feeding them into the target model, which has emerged as a promising defense method (Shi et al., 2021  ###reference_b45###; Srinivasan et al., 2021  ###reference_b48###; Lin et al., 2024b  ###reference_b27###). Compared with the AT or AFT method, the AP method utilizes an additional model that can defend against unseen attacks without retraining the target model (Lin et al., 2024b  ###reference_b27###; Li et al., 2023  ###reference_b23###). In some traditional computer vision and natural language processing tasks, researchers have started using LLMs as purifiers for adversarial purification (Singh and Subramanyam, 2024  ###reference_b46###; Li et al., 2024a  ###reference_b24###; Moraffah et al., 2024  ###reference_b35###), but the security issues of LLMs themselves have not been deeply considered. Therefore, we propose a novel LLM defense technique named LLAMOS to purify the adversarial textual examples before feeding them into the target LLM, aiming to improve the robustness of the entire system.\nLarge Language Model Agent. The LLM agent is a new research direction that has emerged in recent years (Ha et al., 2023  ###reference_b18###; Mu et al., 2024  ###reference_b36###; M. Bran et al., 2024  ###reference_b31###). This novel type of agent is capable of interacting with humans in natural language, leading to a significant increase in applications across fields such as chatbots, natural sciences, robotics, and workflows (Boiko et al., 2023  ###reference_b6###; Yang et al., 2023  ###reference_b65###; Lin et al., 2024a  ###reference_b26###; Wang et al., 2023b  ###reference_b59###; Liu et al., 2023a  ###reference_b28###). Furthermore, LLMs have demonstrated promising zero-shot/few-shot planning and reasoning capabilities across various configurations (Sumers et al., 2023  ###reference_b49###), covering specific environments and reasoning tasks (Yao et al., 2023  ###reference_b66###; Gong et al., 2023  ###reference_b16###; Yao et al., 2024a  ###reference_b67###). In this paper, we introduce a new variant of the LLM agent designed specifically to purify adversarial textual examples generated by attacks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct extensive experiments on GLUE datasets to evaluate the effectiveness of the proposed method (LLAMOS). Specifically, our method significantly reduces the attack success rate (ASR) by up to 37.86% with GPT-3.5 and 45.59% with LLAMA-2, respectively."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "Datasets. The experiments are conducted on six tasks in GLUE datasets (Wang et al., 2018  ###reference_b55###), including SST-2, RTE, QQP, QNLI, MNLI-mm, MNLI-m (Socher et al., 2013  ###reference_b47###; Dagan et al., 2005  ###reference_b10###; Bar-Haim et al.,  ###reference_b2###; Giampiccolo et al., 2007  ###reference_b15###; Bos and Markert, 2005  ###reference_b7###; Bentivogli et al., 2009  ###reference_b4###; Wang et al., 2017  ###reference_b61###; Rajpurkar et al., 2016  ###reference_b42###; Williams et al., 2018  ###reference_b62###). The detailed descriptions are provided in Appendix A  ###reference_###.\nAdversarial Attacks. We evaluate our method against PromptAttack (Xu et al., 2024  ###reference_b64###), which is a powerful attack that combines nine different types of attacks, as illustrated in Table 11  ###reference_###. Furthermore, Xu et al. (2024  ###reference_b64###) introduce the few-shot (FS) strategy (Logan IV et al., 2021  ###reference_b30###) and ensemble (EN) strategy (Croce and Hein, 2020  ###reference_b9###) to boost the attack power of PromptAttack, details in Appendix B  ###reference_###\nEvaluation Metrics. We evaluate the performance of defense methods using two metrics: attack success rate (ASR) and robust accuracy (RA). These metrics are derived from testing on adversarial examples, where a lower ASR or a higher RA indicates greater model robustness.\nTraining Details. The experiments in this paper are conducted using GPT-3.5 (OpenAI, 2023  ###reference_b39###) with ‘GPT-3.5-Turbo-0613’ version and LLAMA-2 (Touvron et al., 2023b  ###reference_b54###) with ‘LLAMA-2-7b’ version. For GPT-3.5, we purchase OpenAI’s API service111https://openai.com/api/ and conduct testing experiments with the ‘openai’ package in Python. For LLAMA-2, we deploy it locally on NVIDIA RTX A6000 and utilize the available checkpoint published by MetaAI from HuggingFace222https://huggingface.co/meta-llama/."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Evaluation of LLAMOS Performance on Attack Success Rate (ASR). We evaluate the ASR of the LLAMOS against PromptAttack-EN and PromptAttack-FS-EN on the GLUE datasets with GPT-3.5 (OpenAI, 2023  ###reference_b39###). As shown in Table 2  ###reference_###, our method significantly reduces the ASR of both PromptAttack-EN and PromptAttack-FS-EN across all tasks. Specifically, our method achieves an average ASR reduction of 29.33% and 29.39%, respectively. These results demonstrate that LLAMOS is effective in defending against adversarial textual attacks. Additionally, we also evaluate the performance of LLAMOS on the SST-2 dataset with LLAMA-2 (Touvron et al., 2023b  ###reference_b54###), as shown in Table 3  ###reference_###. The results are similar to the previous experiments. The ASR of PromptAttack-EN and PromptAttack-FS-EN is significantly reduced by 45.59% and 10.57%, respectively.\nEvaluation of LLAMOS Performance on Robust Accuracy (RA). We evaluate the RA of LLAMOS on the SST-2 dataset with LLAMA-2 against three types of PromptAttack: character, word, and sentence attacks. In Table 4  ###reference_###, the accuracies in the first two columns represent the standard accuracy and robust accuracy without defense, while the last three columns represent the robust accuracy with LLAMOS. Under strong attacks, the classification accuracy of the target LLM decreased from 92.18% to 30.42%. LLAMOS can effectively defend against adversarial textual attacks, significantly improving the robust accuracy. Specifically, the lowest robust accuracy reaches 86.96%.\nAdditionally, we conduct more comprehensive experiments across nine types of attacks and six tasks with GPT-3.5, as shown in Table 5  ###reference_###. LLAMOS can effectively defend against character-level attacks, achieving results on C1 and C3 that closely match the standard accuracy.\nEvaluation of LLAMOS Performance with In-Context Learning (ICL). The C3-based attack (Xu et al., 2024  ###reference_b64###) is a very obvious attack that adds up to two extraneous characters to the end of the sentence, as shown in Table 8  ###reference_###. However, our method only achieves robust accuracies of 89.06% for the C3 attack and 66.41% for the C3-FS attack, respectively. To further improve the robustness, we introduce ICL to enhance the performance of the defense agent. As shown in Table 6  ###reference_###, the defense agent with ICL significantly improves the robust accuracy against the C3 attack, achieving a robust accuracies of 97.66% for the C3 attack and 92.19% for the C3-FS attack, respectively.\nAnalysis of Adversarial System. We conduct experiments with an adversarial system and evaluate the robust accuracy defense against adversarial examples generated by the attack agent over multiple iterations. As shown in Table 7  ###reference_###, the defense agent initially achieves a robust accuracy of 96.09% in the first round of confrontation. However, after the purified examples are re-attacked by the attack agent, the robust accuracy decreases to 56.25%. The defense agent then purifies these adversarial examples again, leading to an increase in robust accuracy, but it will decrease once more by subsequent attacks. This continual fluctuation in robust accuracy is a common phenomenon in adversarial training (Goodfellow et al., 2015  ###reference_b17###). Upon reviewing the generated texts, we observe that after several rounds of confrontation, both the defense agent and attack agent may generate the same sentences as previous ones, resulting in a potential infinite loop, as shown in Table 8  ###reference_###. This is an interesting phenomenon that requires further investigation, particularly strategies to disrupt such loops."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "The Advantages of LLAMOS. As emphasized by the experimental results presented in Section 5.2  ###reference_### and Table 12  ###reference_###, LLAMOS significantly enhances performance across various tasks and attacks with LLAMA-2 and GPT-3.5. Additionally, the defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing step. Through in-context learning (ICL), the defense agent can be continuously optimized to defend against emerging attacks. This invisibly resolves a major challenge in adversarial robustness: Due to the significant differences between different attacks, the model trained on specific attacks often fails to generalize to other unseen attacks (Poursaeed et al., 2021  ###reference_b40###; Laidlaw et al., 2021  ###reference_b22###; Tack et al., 2022  ###reference_b51###). The model necessitates to be continuously fine-tuning to adapt to emerging attacks. However, fine-tuning parameters requires substantial costs (Hu et al., 2022  ###reference_b19###; Dettmers et al., 2024  ###reference_b12###), and the emergence of new attack techniques continually makes it impractical to train the model to defend against emerging attacks(Laidlaw et al., 2021  ###reference_b22###; Lin et al., 2024b  ###reference_b27###). In contrast, our method can effectively enhance the robustness through ICL without adjusting the parameters of the LLMs, which is undoubtedly a significant advantage.\n###table_1### The Challenges in LLM-based Defense.\nThe defense agent is tasked with purifying adversarial examples, but it is difficult to distinguish between natural examples and adversarial examples in some cases. As shown in Table 12  ###reference_###.4, the attacker altered the original meaning by inserting ‘not’, rendering the adversarial example indistinguishable from a natural example, resulting in the defense agent failing to generate the correct sentence. Although we hope that the defense agent can observe the sentences like humans, it presents a huge challenge at present. Unlike the attacker or humans, the defense agent lacks access to the original label of the input sentence.\nFurthermore, although the defense agent can effectively defend against adversarial attacks, it cannot prevent subsequent attacks, as illustrated in Table 7  ###reference_###. For instance, the malicious LLMs can embed specific system prompts to influence the output, which is unbeknownst to users; they can add ‘:)’ to each input sentence for prediction rather than predicting the original sentence. In this case, the defense agent also fails to defend.\nThis issue is also an important problem in traditional adversarial training (Goodfellow et al., 2015  ###reference_b17###), and no method has completely resolved this issue. Nonetheless, as previously discussed, LLMs offer advantages not available to traditional DNNs, and we have naturally solved one challenge in adversarial robustness, which is that the model can adapt to new attacks. Hence, future advancements may resolve adversarial issues of attack and defense within LLM frameworks, representing a challenging but promising research direction.\nLimitations.\nIt is well-known that training large language models (LLMs) requires significant resources and generates substantial carbon emissions, thereby burdening the planet. However, the long-term inference costs of LLMs far exceed the training costs. Chien et al. (2023  ###reference_b8###) show that for ChatGPT-like services, inference dominates emissions due to its large user base, in one year producing 25 times the carbon emissions of training GPT-3.\nOur method introduces a defense agent for additional inference, which inevitably increases carbon emissions during the inference process, thereby exacerbating the negative impact on the climate, which is a limitation of our work. However, considering the nascent stage of LLM development, the trustworthiness issue is equally crucial. Therefore, we have dedicated significant effort to this area, but of course, we aspire to find future solutions that adequately address the environmental and climate challenges posed by AI.\nImpact Statements. This paper presents research aimed at enhancing the robustness of large language models (LLMs). With the rapidly increasing applications of LLMs, their security and trustworthiness have become a critical concern. Our work focuses on this significant issue and contributes positively to potential societal impacts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose LLAMOS, a novel LLM-based defense technique designed to purify adversarial examples before feeding them into the target LLM. The defense agent within LLAMOS operates as a plug-and-play module that functions effectively as a pre-processing step without requiring retraining of the target LLM. We conduct extensive experiments across various tasks and attacks with LLAMA-2 and GPT-3.5. The results demonstrate that LLAMOS can effectively defend against adversarial attacks. Furthermore, we discuss certain existing shortcomings and challenges, which we aim to address in future research."
        }
    ]
}