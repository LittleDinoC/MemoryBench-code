{
    "title": "Speech Robust Bench: A Robustness Benchmark For Speech Recognition",
    "abstract": "As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model’s robustness across subgroups. We believe that SRB will facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Robustness to input perturbations is a highly desirable attribute in Machine Learning (ML) models. When deployed in the real world, ML models are likely to encounter noise and corruptions arising from a myriad of sources, including the environment, sensing apparatus, and even malicious actors. The ability of the models to counteract these sources of noise and continue to make accurate predictions has significant implications for their safety, security, and reliability.\n\nAs novel ML models continue to be developed and deployed at an ever-increasing rate, it has become all the more important to ensure that their robustness is well understood. To this end, prior works have developed robustness benchmarks that evaluate various aspects of a model’s performance under a variety of input perturbations. These benchmarks have proven to be invaluable to the advancement of research into more robust models because (1) they enable meaningful comparisons across existing and new models, which allows progress to be accurately tracked, and (2) make it easier for researchers to evaluate the robustness of their models, thereby reducing the barrier to entry into robustness research. While robustness benchmarks have been proposed in prior works for vision (Hendrycks & Dietterich, 2019; Hendrycks et al., 2021a, b; Croce et al., 2020) and natural language processing (Wang et al., 2021a, 2022b) tasks, the development of such benchmarks for Automatic Speech Recognition (ASR) models has received little attention beyond benchmarks based on simple digit sequence recognition (Hirsch & Pearce, 2000).\n\nIn the absence of standardized benchmarks, prior works have tried to evaluate the robustness of ASR models in various different ways. For example, several works have used a combination of distortions from various datasets to perturb the input audio (Radford et al., 2023; Wen et al., 2016; Chen et al., 2022), however, since the choice of dataset varies among studies, their results are not comparable. Other, more recent, works (Likhomanenko et al., 2020; Radford et al., 2023; Hsu et al., 2021b) have evaluated the robustness of models by computing their transcription accuracy on multiple speech datasets that may have perturbations from the real world. Since the type of perturbations present in these datasets is not controlled and is unknown, this evaluation method does not inform about the specific type of perturbations the models may be weak against.\n\nMoreover, prior works rarely evaluate their proposed models against adversarial attacks (unless they are proposing defenses), and thus neglect to highlight potential security vulnerabilities of the models.\n\nIn this paper, we propose Speech Robust Bench (SRB), a standardized robustness benchmark for ASR models. Following the design of successful robustness benchmarks for image recognition (Hendrycks & Dietterich, 2019; Croce et al., 2020), SRB is composed of two parts: (1) a comprehensive bank of perturbations, and (2) a set of robustness metrics. The perturbations bank contains a comprehensive set of perturbations that represent common distortions arising from the environment or equipment, variations in speaker attributes, semantic preserving special effects found in digital media, and adversarial attacks. Meanwhile, the metrics we propose, following the methodology of Hendrycks & Dietterich (2019), measure two aspects of robustness: the transcription accuracy of the models, as well as the stability of the predicted transcripts under randomized perturbations.\n\nTo highlight the need for and the benefits of doing systematic robustness assessment, we evaluate the robustness of several popular ASR models using SRB, and compare their robustness. We observe that while Whisper (Radford et al., 2023) is the most robust on average among the models we tested, it is outperformed by other, smaller, models on several perturbations. Further analyses reveal that larger models tend to be more robust than smaller models, even if the latter are trained on significantly more data. We further extend our analysis by evaluating the models’ robustness for the various population sub-groups, namely, English and non-English (Spanish) speakers, and male and female speakers. We find that significant disparities exist across these sub-groups, thus identifying areas where future work could provide improvements, and demonstrating the utility of SRB in fairness evaluations as well.\n\nTo summarize, we make the following contributions:\n\nWe present SRB, a robustness benchmark for ASR models, which can result in directly comparable robustness evaluations and facilitate progress.\n\nWe open source our code with clear documentation of existing use cases and support easy extensibility.\n\nWe demonstrate the use of SRB by conducting a fine-grained robustness analysis for several popular models. We extend our analysis by using SRB to uncover disparities in the robustness of ASR for various sub-groups of speakers. This highlights the broad utility of such benchmarks to the field of trustworthy AI."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Robust Automatic Speech Recognition",
            "text": "Over the years, several techniques have been proposed for making Automatic Speech Recognition (ASR) models robust to input perturbations, such as noise and other signal corruptions (Li et al., 2014  ###reference_b29###). We can divide these techniques into two high-level categories: i) model-based and ii) feature-based. i) Model-based techniques modify the models to make them more robust. Examples of such approaches include adapting pre-trained models (Yu et al., 2009  ###reference_b57###; Juang & Rahim, 1996  ###reference_b22###), denoising the audio before processing (Mohammadiha et al., 2013  ###reference_b33###; Wilson et al., 2008  ###reference_b54###), and training ASR models on noisy data (Likhomanenko et al., 2020  ###reference_b30###). Since model based strategies generally require access to noisy data (Li et al., 2014  ###reference_b29###), they are most effective if the sources of noise, and/or the exact environment in which the ASR model will be deployed in are known, and one can gather data to represent them. ii) Feature-based approaches, on the other hand, involve developing handcrafted features that are invariant to noise and corruptions in the signal (Li et al., 2014  ###reference_b29###). Several of these features are inspired by biological audition (Kim & Stern, 2016  ###reference_b23###; Hermansky et al., 1991  ###reference_b17###; Hermansky & Sharma, 1998  ###reference_b16###), while others use signal processing techniques (Li et al., 2014  ###reference_b29###). Generally, these methods are designed to extract the components of the audio signal salient for speech production and perception, while discarding irrelevant components (Stern & Morgan, 2012  ###reference_b44###). Consequently, they do not require precise knowledge of the environment and noise distributions. Recently, however, handcrafted features have fallen out of favor, and have been replaced by features learned via end-to-end training of deep learning models on large amounts of data (Baevski et al., 2020  ###reference_b4###; Hsu et al., 2021a  ###reference_b19###; Likhomanenko et al., 2020  ###reference_b30###; Radford et al., 2023  ###reference_b42###). Proponents of these techniques posit that models trained on larger datasets become more robust. Our evaluations in § 4  ###reference_### reveal that there are several input perturbations against which smaller models trained on less data outperform larger models trained on more data."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Adversarial Robustness",
            "text": "Adversarial perturbations are perturbations that can change the response of a model when added to their inputs, but are either imperceptible to humans or perceptually and semantically irrelevant enough to be ignored by them (Szegedy et al., 2014  ###reference_b45###; Goodfellow et al., 2014  ###reference_b12###). Adversarially perturbed inputs are known as adversarial attacks. They can be targeted (aiming to change a prediction to a specific incorrect class), or un-targeted (aiming to change a prediction to any incorrect class, Akhtar et al. 2021  ###reference_b1###). The design of adversarial attacks is determined by the level of knowledge the attacker is assumed to have about the target model. Attacks that assume full knowledge of the target model’s architecture and weights (white-box threat model) often use gradient-based optimization techniques (Szegedy et al., 2014  ###reference_b45###; Goodfellow et al., 2014  ###reference_b12###; Madry et al., 2018  ###reference_b32###; Laidlaw et al., 2021  ###reference_b27###; Akhtar et al., 2021  ###reference_b1###).\nAttackers who do not have any knowledge of the target models architecture and only have query access to it (black-box threat model) typically use gradient-free optimization methods (Wang et al., 2022a  ###reference_b50###; Andriushchenko et al., 2020  ###reference_b3###; Wicker et al., 2018  ###reference_b53###; Chen et al., 2017  ###reference_b8###; Zhao et al., 2020  ###reference_b58###; Vo et al., 2022  ###reference_b47###). An intriguing property of adversarial perturbations is that they transfer between models (Papernot et al., 2016  ###reference_b38###), and inputs (Akhtar et al., 2021  ###reference_b1###; Neekhara et al., 2019  ###reference_b35###). Our SRB includes two types of white box adversarial attacks: those that generate perturbations specific to each input (Madry et al., 2018  ###reference_b32###), and those that generate perturbations that cause models to mis-transcribe multiple inputs (Neekhara et al., 2019  ###reference_b35###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Robustness Benchmarks",
            "text": "Robustness benchmarks have unified robustness evaluations and enabled fair comparisons between various models and robustness enhancing techniques in several domains, including vision, Natural Language Processing (NLP) and ASR.\nIn the domain of vision, Croce et al. (2020  ###reference_b11###) propose an adversarial robustness benchmark and leaderboard based on AutoAttack (Croce & Hein, 2020  ###reference_b10###), while Hendrycks & Dietterich (2019  ###reference_b13###) and Hendrycks et al. (2021a  ###reference_b14###) propose benchmarks and metrics for measuring the robustness of image recognition models to non-adversarial perturbations. In the domain of NLP, Wang et al. (2021a  ###reference_b48###) propose a benchmark to evaluate models under various lexical and semantic perturbations, such as typos, distractors, and word replacements. However, in the domain of ASR, there is a lack of similar comprehensive benchmarks that measure the robustness of ASR models on diverse corruptions. The current benchmarks are often specialized to one (or few) types of perturbations such as reverberation (Nakamura et al., 2000  ###reference_b34###; Kinoshita et al., 2013  ###reference_b24###; Jeub et al., 2009  ###reference_b21###), environmental noise (Barker et al., 2018  ###reference_b5###; Piczak, 2015  ###reference_b39###), and accented speech (Lander, 2022  ###reference_b28###; Shi et al., 2021  ###reference_b43###). While there is some initial work in developing more comprehensive benchmarks (Hirsch & Pearce, 2000  ###reference_b18###), it is limited to relatively simple data consisting of spoken sequences of digits, which lack the complexity of long sentences that modern ASR models are expected to transcribe.\nSome recent works (Radford et al., 2023  ###reference_b42###; Likhomanenko et al., 2020  ###reference_b30###) have evaluated the robustness of ASR models by computing transcription accuracy on several speech datasets. We find that this evaluation method is too coarse to pinpoint the strengths and weaknesses of the models; for example, Likhomanenko et al. (2020  ###reference_b30###) present the word error rates for three settings: clean, noisy, and extreme, for each dataset, from which it is difficult to determine the kinds of noise or distortions the model has difficulties with. Furthermore, most robustness evaluations of ASR models do not consider adversarial robustness, and thus neglect to highlight potential security vulnerabilities of the models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Speech Robust Bench",
            "text": "Following the common practice from ASR literature, the WER is computed as the word-level edit distance between the reference and the predicted transcripts, normalized by the length of the reference. The edit distance is computed as the total number of word substitutions, deletions, and additions required to transform the reference transcript into the predicted transcript.\nWhen WER is computed over multiple pairs of predicted and reference transcripts, it is common practice to treat all the predicted transcripts as one long text segment , and likewise consider all the reference transcripts as a single long text segment . This means that the number of substitutions, deletions, and additions for all the pairs are summed, and divided by the sum of the lengths of the reference transcripts. Formally, this can be written as\nwhere  computes the edit distance.\n###table_1### Since our benchmark consists of several corruptions, each at several levels of severity, it is intractable to compare the WER of each model for each corruption-severity pair. We therefore require an aggregate metric to summarize the WER for all the pairs. Since all the corruption-severity pairs do not present the same level of difficulty, simply summing or averaging the WERs over all the pairs will not present an accurate picture of the model’s robustness. Ideally, we should penalize errors on easier corruption-severity pairs more than errors on harder pairs.\nTo incorporate the difficulty of the various corruption-severity pairs, we follow the approach of (Hendrycks & Dietterich, 2019  ###reference_b13###), and compute the Normalized Word Error Rate (NWER). NWER is computed as a weighted sum of the WERs corresponding to the various corruption-severity pairs, where the weight is the inverse of the difficulty of the corruption-severity pairs. Following (Hendrycks & Dietterich, 2019  ###reference_b13###), we use the WER of a baseline model () as an estimate of the difficulty. This way errors on easier corruption-severity pairs are penalized more heavily than errors on more difficult corruption-severity pairs, when the difficulty is measured by the baseline model performance. Thus, NWER can be formally defined as\nwhere  and  are the WERs of target and baseline models under corruption  at severity .\nTo evaluate the stability of a target model’s predictions with corrupted inputs, we compute the variance over the model’s predictions on repeated random noise sampled from a fixed distribution. We differ from Hendrycks & Dietterich (2019  ###reference_b13###) who use the co-called flip rate, since their metric is only compatible with class predictions, not with sequence predictions as we have in ASR.\nInstead, we propose to compute the variance in the WER over repeated random samples of corruption. Similar in aim to (Hendrycks & Dietterich, 2019  ###reference_b13###), this yields a metric that measures how much the model’s predictions fluctuate in presence of noise.\nConcretely, given a set of utterances , we corrupt each utterance, , with  random corruption samples to obtain . We organize the corrupted samples into  datasets, , where .\nWERV can now be computed as:\nwhere  is the reference transcript for utterance ."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "At a high level our benchmark consists of two components: (1) a bank of perturbations that are representative of various scenarios that the model could encounter when deployed in the wild (§ 3.3  ###reference_###), and (2) a set of metrics for quantifying the robustness of the ASR models and facilitating comparisons among them (§ 3.5  ###reference_###). Given an ASR model (§ 3.4  ###reference_###) to be evaluated and dataset consisting of audio utterances and reference transcripts (§ 3.2  ###reference_###), the procedure for evaluating the robustness of the target ASR model using our benchmark involves the following three steps, as illustrated in Figure 1  ###reference_###. First, the input audio is perturbed using a comprehensive bank of perturbations (§ 3.3  ###reference_###). In the case of deterministic perturbations only a single perturbed audio is produced, while in the case of randomized perturbations we obtain several perturbed audios that are used to test the stability of the model’s prediction. Then the perturbed audio is transcribed by the target ASR model. Since some of our metrics are normalized by the error of a baseline ASR model (see § 3.5  ###reference_.SSS0.Px2###), transcripts from the baseline model are also obtained. Finally, the reference transcript and the predicted transcripts from the baseline and target ASR models are used to compute the metrics described in § 3.5  ###reference_###.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "We have designed SRB to be largely agnostic to the evaluation data to make it more broadly applicable across various speech recognition domains. In principle, SRB can be used with any dataset that contains utterances and reference transcripts, however, we recommend using datasets with high-quality clean audio and accurate transcripts so that pre-existing corruptions in the dataset do not confound the robustness metrics obtained from the benchmark. For this reason, our evaluation in § 4  ###reference_### uses clean speech from Librispeech (Panayotov et al., 2015  ###reference_b37###).\n###figure_2###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Perturbations",
            "text": "The taxonomy of the perturbations used in our benchmark is presented in Figure 2  ###reference_###. The perturbations are of two broad types: 1) non-adversarial and 2) adversarial.\nThe non-adversarial perturbations fall under one of three categories: 1.i) common corruptions, such as white noise, environmental noise, and room impulse response, that may be introduced by the environment or by other sources, 1.ii) semantically irrelevant perturbations such as special effects that may be encountered in digital media, and 1.iii) speaker attribute perturbation, such as voice/accent transformations.\nThe adversarial perturbations fall under two categories: specific and general. 2.i) General adversarial perturbations are designed to be agnostic to the utterance, to the model, or to both. 2.ii) Specific adversarial perturbations, on the other hand, are crafted to cause a specific model to mistranscribe a specific utterance, and, in general, they are not expected to be very effective on other models and utterances.\nEach perturbation is applied at 4 levels of severity, which is modulated by adjusting the parameters and attributes of the perturbation. The perturbations and their parameters and attributes are discussed in detail in Appendix A  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Models and Transcription",
            "text": "Once perturbed, the utterances are transcribed by the (target) ASR model being evaluated, as well as a baseline model. The transcripts from the baseline model are needed to compute the metrics described in § 3.5  ###reference_###. In SRB we use Deepspeech (Amodei et al., 2016  ###reference_b2###) as baseline. Indeed, in principle, any model other than the target model(s) can be used. We recommend using a model that does not exhibit counter-intuitive behavior, such as unusually high error rate on relatively mild corruptions or lower error rate on more severe corruptions, to make results easier to understand."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Metrics",
            "text": "Similar to (Hendrycks & Dietterich, 2019  ###reference_b13###), SRB measures robustness along two dimensions: utility and stability of the model’s prediction under input perturbations.\nSRB measures the utility of the model with the widely used Word Error Rate (WER, see § 3.5  ###reference_.SSS0.Px1###). When aggregating WER over multiple perturbations we follow the practice of (Hendrycks & Dietterich, 2019  ###reference_b13###) and normalize the WER of the target model by the WER of a baseline model. Doing so penalizes errors on “easy” corruptions more than errors on “harder” corruptions. This normalized metric is called Normalized WER (NWER, see § 3.5  ###reference_.SSS0.Px2###). Meanwhile, SRB measures the prediction stability of the model by computing the variance in the WER caused by corrupting the signal with multiple corruption samples drawn from the same distribution. We call this metric WER Variance (WERV, see § 3.5  ###reference_.SSS0.Px3###).\nFollowing the common practice from ASR literature, the WER is computed as the word-level edit distance between the reference and the predicted transcripts, normalized by the length of the reference. The edit distance is computed as the total number of word substitutions, deletions, and additions required to transform the reference transcript into the predicted transcript.\nWhen WER is computed over multiple pairs of predicted and reference transcripts, it is common practice to treat all the predicted transcripts as one long text segment , and likewise consider all the reference transcripts as a single long text segment . This means that the number of substitutions, deletions, and additions for all the pairs are summed, and divided by the sum of the lengths of the reference transcripts. Formally, this can be written as\nwhere  computes the edit distance.\n###table_2### Since our benchmark consists of several corruptions, each at several levels of severity, it is intractable to compare the WER of each model for each corruption-severity pair. We therefore require an aggregate metric to summarize the WER for all the pairs. Since all the corruption-severity pairs do not present the same level of difficulty, simply summing or averaging the WERs over all the pairs will not present an accurate picture of the model’s robustness. Ideally, we should penalize errors on easier corruption-severity pairs more than errors on harder pairs.\nTo incorporate the difficulty of the various corruption-severity pairs, we follow the approach of (Hendrycks & Dietterich, 2019  ###reference_b13###  ###reference_b13###), and compute the Normalized Word Error Rate (NWER). NWER is computed as a weighted sum of the WERs corresponding to the various corruption-severity pairs, where the weight is the inverse of the difficulty of the corruption-severity pairs. Following (Hendrycks & Dietterich, 2019  ###reference_b13###  ###reference_b13###), we use the WER of a baseline model () as an estimate of the difficulty. This way errors on easier corruption-severity pairs are penalized more heavily than errors on more difficult corruption-severity pairs, when the difficulty is measured by the baseline model performance. Thus, NWER can be formally defined as\nwhere  and  are the WERs of target and baseline models under corruption  at severity .\nTo evaluate the stability of a target model’s predictions with corrupted inputs, we compute the variance over the model’s predictions on repeated random noise sampled from a fixed distribution. We differ from Hendrycks & Dietterich (2019  ###reference_b13###  ###reference_b13###) who use the co-called flip rate, since their metric is only compatible with class predictions, not with sequence predictions as we have in ASR.\nInstead, we propose to compute the variance in the WER over repeated random samples of corruption. Similar in aim to (Hendrycks & Dietterich, 2019  ###reference_b13###  ###reference_b13###), this yields a metric that measures how much the model’s predictions fluctuate in presence of noise.\nConcretely, given a set of utterances , we corrupt each utterance, , with  random corruption samples to obtain . We organize the corrupted samples into  datasets, , where .\nWERV can now be computed as:\nwhere  is the reference transcript for utterance ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We evaluate several recent and popular ASR DNNs (see Table 1) using SRB and analyze the results to uncover fine-grained differences in their robustness to non-adversarial and adversarial input perturbations. Our analysis is two-pronged. First, we focus on the primary goal of SRB, which is to measure the robustness of the models to various input perturbations. As a case study, we use SRB to evaluate the prediction accuracy and stability of the English ASR models as different perturbations are introduced. As mentioned in § 3.2, we use English speech from Librispeech (Panayotov et al., 2015). We extend our analysis by studying the relationship between robustness and various model characteristics, such as the number of parameters and size of the training data. Having considered robustness averaged over the entire population (represented in the dataset), we extend our analysis to the robustness of the models for various population sub-groups, namely English speech, male speakers, and female speakers. Prior works (Liu et al., 2022; Veliche & Fung, 2023) observe that there is disparity in transcription accuracy between subgroups. Our analysis augments these observations by showing that inter-group disparities in robustness may also exist, and thus demonstrating the utility of SRB in the broader field of trustworthy AI.\n\nWe first consider non-adversarial perturbations. In the following, we measure the robustness of the candidate models under each perturbation and at each severity level to identify potential failure modes. We evaluate the English-only DNNs from Table 1 on the untargeted utterance-agnostic perturbations and untargeted specific perturbations. Figure 10 in Appendix B shows the WER of the DNNs on the two perturbations as well as their average. In terms of average WER, w2v2-lg-slf emerges as the most robust model followed by wsp-lg. However, looking at the breakdown by perturbation types, we find that while wsp-lg is almost equally susceptible to general and specific perturbations, w2v2-lg-slf is much more vulnerable to specific perturbations, but significantly less vulnerable to general perturbations. Therefore, under black- or grey-box scenarios, w2v2-lg-slf may be a better choice, while under the white-box threat model, wsp-lg may be more robust. Considering the three DNNs that are based on Wav2Vec-2 Large: hubt-lg, w2v2-lg-slf, and w2v2-lg-rob, we note that hubt-lg achieves the lowest WER against specific adversarial perturbations, while w2v2-lg-slf achieves the lowest WER against general adversarial perturbations. Meanwhile, w2v2-lg-rob performs similarly as w2v2-bs.\n\nWe note that: (1) the self-training used in w2v2-lg-slf may induce robustness to utterance-agnostic adversarial attacks, (2) the discrete representations of hubt-lg may induce robustness to specific adversarial attacks, and (3) diversifying training data has little to no effect on adversarial robustness. Finally, wsp-tn.en is highly vulnerable to specific adversarial attacks.\n\nTo determine if the prevailing practice of training DNNs with more parameters on larger datasets is yielding improvements in robustness, we plot NWER against the number of model parameters and the size of the training data of all the candidate models. We note that increasing model size is correlated with improved robustness (lower NWER), however, the effect is more inconsistent and weaker for adversarial perturbations. To further isolate the impact of the model size, we control the architecture and training data and plot the NWER of models from the same family, which have similar architectures and training datasets. We note that larger models are more robust in the Whisper and Wav2Vec-2.0 families, but, surprisingly, not in the HuBert family. On the other hand, increasing training data appears to have only a minor influence on robustness. Larger models tend to be more robust, while smaller models, even if they are trained on large datasets, are less robust.\n\nTo measure the disparity in prediction accuracy across genders (males/females), we compute the log of the ratio of the WERs of the ASR model on female and male speakers. We call this measure the Log WER Ratio (LWERR). A positive value of LWERR indicates that the model is biased against females and a negative value indicates that the model is biased against males. The average LWERR values for English models are shown, and a breakdown by perturbations is shown. We note that, on average, the models are biased against females. From Figure 5(a), we observe that on clean data the English models are generally weakly biased against females, with ds being the most biased and hubt-lg"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Robustness of English ASR Models",
            "text": "We evaluate the robustness of English DNNs from Table 1 on the non-adversarial and adversarial perturbations in SRB. Figures 2(a) and 2(b) present aggregate metrics showing overall robustness of the ASR models along two dimensions of utility, which is measured by NWER, and stability, which is measured by WERV (see § 3.5 for details on metrics).\n\nIn terms of both utility and stability, we note that wsp-lg is the most robust model on average, followed by hubt-lg. This high-level analysis gives us an overview of the robustness of many popular state-of-the-art ASR models. Next, we conduct more fine-grained analyses to better determine under which conditions the models’ performance deteriorates.\n\nWe first consider non-adversarial perturbations. In the following, we measure the robustness of the candidate models under each perturbation and at each severity level to identify potential failure modes. We evaluate the English-only DNNs from Table 1 on the untargeted utterance-agnostic perturbations and untargeted specific perturbations. Figure 10 in Appendix B shows the WER of the DNNs on the two perturbations as well as their average. In terms of average WER w2v2-lg-slf emerges as the most robust model followed by wsp-lg. However, looking at the breakdown by perturbation types, we find that while wsp-lg is almost equally susceptible to general and specific perturbations, w2v2-lg-slf is much more vulnerable to specific perturbations, but significantly less vulnerable to general perturbations. Therefore, under black- or grey-box scenarios, w2v2-lg-slf may be a better choice, while under the white-box threat model wsp-lg may be more robust. Considering the three DNNs that are based on Wav2Vec-2 Large: hubt-lg, w2v2-lg-slf and w2v2-lg-rob, we note that hubt-lg achieves the lowest WER against specific adversarial perturbations, while w2v2-lg-slf achieves the lowest WER against general adversarial perturbations. Meanwhile w2v2-lg-rob performs similarly as w2v2-bs.\n\nWe note that: (1) the self-training used in w2v2-lg-slf may induce robustness to utterance-agnostic adversarial attacks, (2) the discrete representations of hubt-lg may induce robustness to specific adversarial attacks, and (3) diversifying training data has little to no effect on adversarial robustness. Finally, wsp-tn.en is highly vulnerable to specific adversarial attacks.\n\nTo determine if the prevailing practice of training DNNs with more parameters on larger datasets is yielding improvements in robustness, we plot NWER against the number of model parameters and the size of the training data of all the candidate models. These plots are shown in Figures 3(a) and 3(b). We note that increasing model size is correlated with improved robustness (lower NWER), however, the effect is more inconsistent and weaker for adversarial perturbations. To further isolate the impact of the model size we control the architecture and training data and plot the NWER of models from the same family in Figure 3(c), which have similar architectures and training datasets. We note that larger models are more robust in the Whisper and Wav2Vec-2.0 families, but, surprisingly, not in the HuBert family. On the other hand, increasing training data appears to have only a minor influence on robustness.\n\nLarger models tend to be more robust, while smaller models, even if they are trained on large datasets, are less robust."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Robustness for Population Sub-Groups",
            "text": "In the preceding analysis, we considered robustness aggregated over the entire population (i.e. dataset). However, populations are generally not homogeneous, and, thus, the robustness of the model may differ on various population sub-groups. Prior works have commonly analyzed sub-group fairness of ASR models by comparing the overall WER for each sub-group on a benchmark dataset (Koenecke et al., 2020 ###reference_b26###). It is possible that models that are fair on average, may not be fair under certain conditions. In the following, we use SRB to uncover and analyze the disparities in the models’ robustness across two sub-groups: males and females. We find that most models are less robust for females than males (Figure 12 ###reference_###).\n\nTo measure the disparity in prediction accuracy across genders (males/females), we compute the log of the ratio of the WERs of the ASR model on female and male speakers. We call this measure the Log WER Ratio (LWERR) and formally it can be written as:\n\n\\[ LWERR = \\log \\left( \\frac{WER_{\\text{female}}}{WER_{\\text{male}}} \\right) \\]\n\nwhere the \\( WER_{\\text{female}} \\) and \\( WER_{\\text{male}} \\) represent the subsets of utterances by females and males, respectively. A positive value of LWERR indicates that the model is biased against females and a negative value indicates that the model is biased against males. The average LWERR values for English models are shown in Figure 12 ###reference_### and a breakdown by perturbations is shown in Figure 6 ###reference_###. We note that, on average, the models are biased against females. From Figure 5(a) ###reference_sf1###, we observe that on clean data the English models are generally weakly biased against females, with ds being the most biased and hubt-lg the least. wsp-lg is an exception in that it is slightly biased against males. The bias of the models diminishes significantly under environmental and Gaussian noise, but, interestingly, it is exacerbated (against females) under room impulse response augmentation and phasing. Perturbations that mask or shift the spectrogram, such as filtering, resampling, and pitch and speech modulation, have somewhat expected results – perturbations that increase the pitch or mask higher frequencies tend to increase WER for females, and vice versa. Interestingly, we observe that utterance-agnostic adversarial attacks biased 4 out of 7 models against female speakers, with wsp-lg exhibiting the most significant increase in bias.\n\nTo summarize, using our benchmark to conduct a fairness analysis reveals that (1) certain perturbations, like room impulse response and adversarial attacks, disproportionately degrade the performance of models for female speakers."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper we have proposed, SRB, a comprehensive benchmark designed to standardize the robustness evaluations of ASR models. To the best of our knowledge, SRB, is the first ASR robustness benchmark that contains long-form speech, and expands the set of distortions to include special effects, speaker attribute modification, and adversarial attacks. We present case studies to demonstrate the utility of SRB in evaluating the robustness of ASR models, as well as its potential to facilitate evaluations of other aspects of trustworthy AI, particularly fairness. Our analysis reveals that while the latest large models, trained on vast quantities of data, are more robust than their counterparts on average, under certain types of perturbations they are outperformed by smaller models trained on lesser data. Furthermore, our analysis reveals that model size generally has a greater impact on robustness than the size of the training dataset, which runs somewhat counter to the prevailing wisdom espoused by recent works (Radford et al., 2023; Likhomanenko et al., 2020). We also use SRB to measure the disparities in the robustness of ASR models on data generated by different population subgroups, namely speakers’ language and gender. We also find that under certain perturbations models may be more robust for one gender than the other. We believe that SRB will enable rigorous robustness evaluations of ASR models in a highly standardized manner that will allow comparisons between existing and new approaches and thus facilitate progress tracking. Furthermore, we also release transformed test sets in English and Spanish to facilitate robustness evaluations for researchers and model developers. We believe that this will make robustness evaluations more prevalent and encourage model developers to consider robustness as a key metric to improve."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact statement",
            "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "The execution of this work was funded by the European Union’s Horizon 2020 RIA SPATIAL project (Grant Agreement No. 101021808), RIA ELOQUENCE project (Grant Agreement No. 101135916) and from the Spanish Project 6G-RIEMANN (Grant Agreement No. 2022/0005420). The authors bear the sole responsibility for the content presented in this paper, and any interpretations or conclusions drawn from it do not reflect the official position of the funding agencies."
        }
    ]
}