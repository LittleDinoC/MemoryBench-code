{
    "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
    "abstract": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. Our findings show that GPT-4 presents competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using XGBoost as the case study. Our study contributes to the field by highlighting the potential of integrating LLMs with traditional machine learning models to enhance predictive accuracy and personalize adult literacy education, setting a foundation for future research in applying LLMs within ITSs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Adult literacy education in reading comprehension empowers individuals to participate actively in society, access better job opportunities, and engage in lifelong learning. Effective literacy education programs are tailored to meet the diverse needs of adult learners, using strategies to enhance reading skills, comprehension, and critical analysis of texts. The success of these programs often depends on accurate assessment and continuous improvement of personalized instructions to cater to learner needs. Predicting learning performance is crucial as it allows for the early identification of individuals needing additional support, enabling targeted interventions to enhance reading comprehension and literacy skills.\n\nIntelligent Tutoring Systems (ITSs) personalize instruction in adult literacy by tracking and assessing learning progress, facilitating adaptation to better meet learner needs. A key component of ITSs is predicting learning performance through machine learning models, which enable personalized instruction. This process involves analyzing historical data, including learners’ correctness in problem-solving attempts. Advanced machine learning models, using fine-grained data and natural language related to the problem-solving context, can improve learner performance prediction and the personalization of instruction. Despite this potential, the role of advanced AI models, like multimodal machine learning models and large language models (LLMs), in predicting learning performance remains underexplored.\n\nRecent advancements in AI models, notably LLMs, have shown remarkable predictive capabilities in areas such as mathematical reasoning and time series forecasting. These successes illustrate the potential of LLMs in understanding patterns relevant to learner modeling tasks. In education, prior research has demonstrated the potential of LLMs in predictive analysis, like predicting learning performance in computer science education and identifying at-risk learners. Despite these advances, the use of LLMs to enhance predictive analytics within ITSs is still in the early stages. Motivated by the proven effectiveness of LLMs in educational prediction tasks, our study examines the potential of LLMs, specifically GPT-4, in comparison with traditional methods such as Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization, and eXtreme Gradient Boosting (XGBoost), for predicting learner performance in adult literacy education. Our study investigates two Research Questions:\n\nRQ1: How effectively can the GPT-4 model, through specific prompting strategies, predict learning performance in adult literacy programs compared to existing benchmark models?\nRQ2: How can GPT-4 augment traditional human-led efforts in enhancing the prediction accuracy of learning performance in adult literacy lessons?\n\nThe study utilized reading comprehension datasets, including attributes like learner ID, questions, attempts, and learners’ performance scores for each lesson. To answer RQ1, we employed widely-used models including BKT, PFA, SPARFA-Lite, and XGBoost, comparing them with the GPT-4 model. Model performance was assessed through five-fold cross-validation, revealing that the XGBoost model outperformed GPT-4 in predicting learning performance. Interestingly, when prompted, GPT-4 itself recommended XGBoost for predicting learning performance. Executing the GPT-4-selected XGBoost model on the GPT-4 platform provided superior results compared to running it on a local machine. In addressing RQ2, we evaluated the tuning of hyper-parameters by GPT-4 against manual tuning. Specifically, we prompted GPT-4 to optimize an XGBoost model for predicting learning performance. Simultaneously, we manually adjusted the XGBoost model’s hyper-parameters through grid-search on a local machine for comparison. Our findings indicated that while the GPT-4-tuned hyper-parameters achieved performance comparable to manually tuned models, they showed less stability than those optimized through manual grid search."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Adult Literacy Education in Intelligent Tutoring Systems",
            "text": "Adult literacy education has long advocated computer-aided adaptive learning technologies for their capacity to provide personalized and cost-effective educational solutions. These technologies are commonly applied in Intelligent Tutoring Systems (ITSs). ITSs can offer personalized tutoring and adaptive instructions tailored to the individual learner, dynamically adjusting the difficulty levels and contents of lessons based on the learner’s responses to questions and tasks. For instance, utilizing web-based applications, ITSs deploy computer-based agents to deliver customized reading materials and learning tasks, ensuring support is readily available for learners facing challenges. The significant impact of ITSs lies in their ability to create an adaptive learning environment that supports and responds to individual educational needs, thereby empowering learners to advance at their own pace. Furthermore, ITSs tackle the issue of scarce human tutoring and classroom resources for adult learners by utilizing these systems to improve reading comprehension skills."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Learning Performance Prediction",
            "text": "Learning performance prediction is an important task in the field of ITS in education. By understanding the learner’s performance, ITS can accurately assess learning states and offer tailored instructions to support learners throughout the learning process, particularly when they encounter difficulties with questions, face early risks of failure, or experience wheel-spinning.\n\nThe predictive task for learning performance utilizes historical records to predict future performance on questions, incorporating data from multiple attempts. Driven by the need for high accuracy in learning performance prediction, many previous works employed machine learning methods including BKT, PFA, SPARFA-Lite, and tensor factorization. Widely recognized predictive models such as BKT and PFA leverage Bayesian networks and logistic regression for learner performance prediction, respectively. BKT outlines four probabilistic parameters: “known” (initial or prior knowledge), “slip” (incorrectly answering despite knowing the skill), “guess” (correctly answering without knowing the skill), and “learn” (mastering a skill in subsequent practices). PFA, on the other hand, includes parameters that account for prior successes and failures in answering questions, skill difficulty reflecting the inherent challenge of the skill, and individual learning rates indicating how fast the learner improves in mastering knowledge. Both methods have been utilized for predicting learning performance owing to their stability, strong predictive performance, and explainability.\n\nSPARFA-Lite utilizes quantized matrix completion to predict learner performance in knowledge tracing, representing the probability of answering questions successfully based on three factors: 1) the learner’s understanding of latent concepts, 2) the relationship between questions and concepts, and 3) the inherent difficulty of each question. The tensor factorization method structurally represents learner knowledge in a three-dimensional space, incorporating critical factors such as learners, questions, and attempts to influence learning progress. This approach calculates probability estimates for learner performance using mathematical tensor factorization."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Large Language Models in Education",
            "text": "LLMs, pre-trained on massive amounts of data, enable them to generate human-like text, answer questions, and perform reasoning tasks with unprecedented accuracy [46, 47]. LLMs like ChatGPT have demonstrated remarkable advancements in AI, driving revolutionary shifts in education applications through enhancing instructional feedback [48, 49, 50], boosting student engagement [51], and offering personalized learning experiences [52].\n\nHowever, the applications of LLMs in enhancing predictive analytics within ITSs remain in their early stages. Liu et al.’s investigation [53] on ChatGPT’s effectiveness in logical reasoning, particularly in making prediction-based inferences for multiple-choice reading comprehension and natural language inference tasks, highlights its adeptness at complex educational reasoning challenges. Liu et al. [18] have incorporated ChatGPT for open-ended knowledge tracing in computer science education, enabling enhanced prediction of code snippets for open-ended response analysis. Susnjak [19] has attempted to integrate ChatGPT with machine learning models, enabling advanced predictive analytics to assist at-risk learners through evidence-based remedial recommendations. These cases highlight the advanced predictive capabilities of LLMs or their collaboration with machine learning models for predictive tasks, inspiring further exploration of LLMs’ potential in advancing educational predictive applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "Our study was granted ethical approval with the Institutional Review Board (IRB) number: H15257. The selected lessons for our analysis include “Persuasive Text” (Lesson 1), “Cause and Effect” (Lesson 2), “Problems and Solution” (Lesson 3). Table 1 presents the basic statistics about the dataset on learner performance, detailing information about the learners, questions, and attempts for each lesson."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "The Proposed LLM-based Prediction Method",
            "text": "We developed a LLM-based framework to trace and predict learner performance. This framework includes three procedures: 1) encoding for converting numerical value to contextual prompts, 2) the LLM component for analyzing these prompts and executing predictions, and 3) decoding for outputting the prediction information and assessment along with interpretations.\n\nLearning performance on question-answering tasks was recorded as binary data (labeled as correct or incorrect), to reflect the learner’s success or failure in answering the questions. The records also captured the number of attempts made by the learner. For instance, the performance of the learner on the question during their attempt is recorded as 1 for a correct answer and 0 for an incorrect one (this setting applies to the training dataset, whereas, for the testing dataset, performance data are omitted to enable future prediction). Our objective is to input these performance data into LLM to identify data patterns and latent learning features such as initial knowledge level and learning rate, aiming to predict the learner’s future performance based on the learner’s historical attempts.\n\nEncoding. Learning performance in binary indicator variables representing correct or incorrect responses can be compiled into a contextual prompt by integrating numerical data with textual explanations. For example, the entry could be represented as “The current learner attempted to answer the question titled as ‘…’ on their attempt. Their performance was observed as 1 or 0”. Essential considerations for this encoding process include: (a) Numerical to Text Conversion: This involves the embedding of numerical value along with the question contents and answers into a narrative or textual format. (b) Contextual Integration: This aspect involves incorporating information related to the lesson topic, content, and questions, along with knowledge gained from reading comprehension, to enrich understanding of both question and lesson material. This enrichment process also facilitates tailoring and assessing reading comprehension skills of the learner in the learning process.\n\nLLM Component. The contextual prompt serves as input into the LLM component for data analysis and modeling, aimed at predicting learner performance on new or repeated question attempts. Two primary prediction strategies are encompassed in this component: 1) leveraging the inherent reasoning, understanding, and computational capabilities of the GPT-4 model, e.g., the heuristic-based approach; and 2) utilizing available machine learning models, which are automatically selected and fine-tuned by the GPT-4 model for predictive tasks. Through extensive analysis of trial experiments, the following assumptions are included:\n(a) GPT-4 can be pre-trained in predictive tasks, (b) GPT-4 possesses the capability to extract domain-specific knowledge from distinct questions and attempts, (c) GPT-4 can uncover latent learning features within contextual performance data, enabling the evaluation of learners’ reading comprehension skills, and (d) Information inferred by GPT-4, alongside other computational models, can be leveraged to discern trends, patterns, and predict learner learning performance.\n\nDecoding. In the decoding phase, predictive outcomes regarding learning performance are produced through a mechanism that employs either heuristic approaches or machine learning models, which GPT-4 automatically selects and implements. These predictions draw on an analysis of the learning performance distribution integrated in previously mentioned contextual prompts. For instance, it reformat output information into a structured format, such as “{‘learner ID’:…, ‘Question ID’:…, ‘Attempt’:…, ‘Prediction’:…, ‘Assessment’:…}”, efficiently conveying the prediction details. This procedure incrementally increases the dimensionality of the data until it aligns with the original input size of the test dataset."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline Methods",
            "text": "This study will employ machine learning models specifically for predicting learning performance in education [54]. Below is a concise overview of each selected method.\n\nBayesian Knowledge Tracing (BKT) is a computational model used to track and predict learners’ knowledge acquisition over time within educational software, particularly within ITSs [21, 55, 56]. Fundamentally, BKT is based on the principles of Bayesian probability which estimates the likelihood that a learner has mastered a particular skill or concept at various points throughout the learning process, adjusting these estimates in response to the learner’s performance on related tasks or questions [55]. According to [11], the classic BKT’s key parameters include the initial probability of mastering the skill, the probability of acquiring knowledge by transforming from the unmastered state on one skill to master state, the probability of making incorrect answer responses by slipping in mastered state on a skill, and the probability of making correct answer responses by guessing in unmastered state on a skill. BKT advanced this framework by incorporating customized parameters for each learner and each skill into its modeling and predictions [55]. By updating these parameters using Bayesian inferences based on each learner’s responses to questions over time, BKT refines its estimates of a learner’s knowledge state at a particular time step as the learner responds to questions [11, 55].\n\nPerformance Factor Analysis (PFA) utilizes logistic regression to predict the learner’s performance on the questions by incorporating factors on individual learning ability, skill-related features (e.g., difficulty), and the learner’s previous success and failures [12, 22, 43, 57, 58]. Many studies have established PFA as a competitive approach in predicting learner performance, acknowledging the importance of individual differences across skills and learners [12, 42, 59]. In our research, we have further refined the PFA model to better account for the variability among individual learners.\n\nSparse Factor Analysis Lite (SPARFA-Lite), a variant of Sparse Factor Analysis (SPARFA), employs a matrix completion technique to analyze quantified, graded learner performance on questions [23]. This model offers improved efficiency in automatically exploring the number of Knowledge Components for predicting learner performance compared with the traditional Sparse Factor Analysis model [23].\n\nTensor Factorization is a method that decomposes a three-dimensional tensor (representing dimensions of learners, questions, and attempts) into a factor matrix for learners and latent features, and a factor tensor that encompasses dimensions of latent features, questions, and attempts [24, 60, 37, 61]. The latent feature dimensions obtained through tensor decomposition capture learner-specific characteristics such as learning abilities and personalities [60]. The factor tensor models the knowledge space related to learner interactions with questions. Our prior studies have demonstrated its significant efficacy in predicting learner performance, particularly within the adult literacy domain [37, 61]. Here, the rank-based constraint was applied to regulate the factorization computing [24].\n\neXtreme Gradient Boosting (XGBoost) is an algorithm that has become an effective model for knowledge tracing, significantly enhancing prediction performance [26, 62]. At its core, XGBoost constructs an ensemble of decision trees in a sequential manner [25, 63]. In this process, each subsequent tree is specifically trained to address and correct the residuals or errors made by the preceding tree, effectively enhancing the model’s predictive accuracy over iterations. Guided by a gradient descent algorithm, XGBoost optimizes a predefined loss function, systematically reducing prediction errors. Its capability to interpret input features, such as unique learners, questions, and attempts in our study, enables an in-depth understanding of model predictions, enhancing transparency and trustworthiness in predictive analytics."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "We employed the recognized quantitative metric Root Mean Square Error (RMSE), which aligned with peer studies [21, 12, 42, 64, 65]. RMSE provides a measure of the square root of the average squared differences between predicted and actual values [66]. Additionally, we conducted a five-fold cross-validation to obtain RMSE values for comparative analysis. In our LLM-based prediction method, specifically utilizing GPT-4, we allocated four out of five folds for training to enable the model to learn from historical data through contextual prompts. The remaining fold was used for testing, to evaluate the accuracy of predictions made by the trained GPT-4."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results on RQ1",
            "text": "We investigated the comparison of prediction performance between LLM-based models and Baseline Methods. The result is shown in Table 2  ###reference_### which presents the RMSE values of all models across three CSAL lessons, with lower value indicating better performance in prediction. It should be noted that all the RMSE scores were obtained from models after adjusting their hyper-parameters.\nSpecifically, the XGBoost (selected by GPT-4) showcases an enhanced application, leveraging GPT-4’s strengths in reasoning, computation, and automatic tuning to refine outcomes. Aware of the potential for errors or instability in GPT-4’s predictions, we established the reliability of our RMSE by conducting seven repeated prediction runs for both the standard GPT-4 and the GPT-4 enhanced with XGBoost, across each lesson. The outstanding predictive accuracy of the XGBoost (selected by GPT-4) model led us to perform an extensive manual grid search, documented in Table 2  ###reference_###. This process entailed evaluating 1,296 combinations of hyper-parameters, including number of trees, learning rate, maximum tree depth, training instance subsample ratio, column subsample ratio per tree, minimum loss reduction for further partitioning, and the minimum sum of instance weight required in a child node. This rigorous hyper-parameter tuning was aimed at further enhancing the model’s performance for each lesson, as detailed in Table 3  ###reference_###.\nTable 2  ###reference_### presents the RMSE values of all models across three CSAL lessons, with lower value indicating better performance in prediction. The RMSE value of GPT-4, as shown in Table 2  ###reference_###, surpasses that of most other models, demonstrating only higher values when compared to XGBoost in Lessons 1 and Lesson 2. Notably, the RMSE value of XGBoost (selected by GPT-4) is the lowest among all six models across the three lessons. This demonstrates the substantial enhancement in predicting learning performance achieved through the integration of GPT-4 and XGBoost. Additionally, the XGBoost model outperforms other traditional knowledge tracing models, including Individualized BKT, PFA, SPARFALite, and Tensor Factorization, in the reading comprehension data. As for the standard errors for the RMSE values, lower values indicate less variability in the estimated RMSE values, thereby suggesting greater confidence in the accuracy of the predictions. The standard errors for all RMSE values related to GPT-4 and XGBoost (as chosen by GPT-4) fall within the range of [0.004, 0.009], indicating a relatively moderate variability in the prediction outcomes across all models."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results on RQ2",
            "text": "Lesson 1\n0.435\n0.422\n0.053\n0.398\nLesson 2\n0.376\n0.361\n0.033\n0.351\nLesson 3\n0.398\n0.382\n0.036\n0.381\nLesson 1\n0.433\n0.426\n0.017\n0.412\nLesson 2\n0.391\n0.384\n0.020\n0.366\nLesson 3\n0.396\n0.394\n0.010\n0.384\nTable 3  ###reference_### displays a comparative analysis of RMSE values from two hyperparameter tuning approaches for the XGBoost model: one selected by GPT-4 and the other via manual grid search. GPT-4 consistently yields lower minimum and median RMSE values across all three lessons compared to the manual method. However, GPT-4’s method results in a lower mean RMSE value only for Lesson 2. The standard deviation values from GPT-4 are larger than those from the manual grid search, indicating a wider variability in RMSE outcomes. Additionally, the minimum and maximum range of values obtained through GPT-4’s method exceed those from the manual approach, suggesting a greater spread in the performance results."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Efficient LLM-based Method for Predicting Learning Performance",
            "text": "Our study highlights the capabilities of GPT-4 in predicting learning performance in ITSs. Two exact predictive strategies, one leveraging the inherent heuristic reasoning method and the other utilizing available machine learning models, are implemented by GPT-4.\nWhen employing its heuristic-based reasoning approach, GPT-4 takes into account factors such as the perceived difficulty of questions and their attempt frequency. This approach does not rely on a fixed algorithm but uses logical reasoning to analyze historical performance data. It assumes that questions deemed more difficult are less likely to be answered correctly on the first try. Furthermore, if learners make multiple attempts on certain questions, GPT-4 interprets this as a sign of struggle with the material, leading to a more conservative performance prediction.\nAt the same time, enhancing GPT-4’s predictive accuracy significantly involves incorporating reading comprehension materials, questions, and additional background information to craft context-specific prompts. By understanding the learning content and the questions’ context, GPT-4 can offer explanations and leverage its vast knowledge base more effectively. This contextually enriched reasoning allows GPT-4 to outperform traditional learning performance prediction methods, such as BKT, PFA, SPARFA-Lite, and Tensor Factorization. The result is not just more accurate predictions, but also insights that are directly relevant and tailored to the specific learning scenario. This makes GPT-4 an invaluable tool for educators seeking to understand and improve student learning performance.\nWhen utilizing available machine learning models, GPT-4 demonstrates its ability by recommending and applying a range of machine learning models tailored to the specific needs of the data. Among these models are logistic regression, random forest, gradient boosting machine, and XGBoost. GPT-4’s unique self-programming ability enables it to autonomously test these models and select the most effective one based on performance metrics from validation results. Through this process, XGBoost is identified as the most suitable model for predicting learning outcomes, leading to a novel approach in our experiments that combines the strengths of GPT-4 with XGBoost, referred to as GPT-4 with selected XGBoost. This approach remains adaptable, with GPT-4 continuously seeking to refine and enhance its choice of models. The fusion of GPT-4’s capabilities with advanced machine learning techniques broadens its application scope, pushing the boundaries of what can be achieved in computational tasks. This not only showcases GPT-4’s potential for complex problem-solving but also highlights its role in driving forward the evolution of ITSs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Prompt Strategy for Predicting Learning Performance",
            "text": "In this study, the prompt engineering plays a crucial role. The foundational framework of our prompts encompasses encoding for the contextual representation of numerical values and decoding to facilitate LLM-based understanding, reasoning, and analysis in the generation of predictive outputs. This approach allows the LLM to seamlessly integrate all processes, from data input to final prediction. It employs self-search and self-optimization for refining prompt engineering, alongside semantic compiling techniques for processing learning performance data. The Chain-of-Thought prompt strategy [47  ###reference_bx47###] is employed to generate GPT-4 output that illustrates model reasoning and its interpretative process. By activating specific prompts within GPT-4, we guide it to more effectively analyze and interpret learner learning performance data. This method not only improves the transparency of the AI’s decision-making process but also enhances the precision and relevance of its predictive capabilities.\nSpecifically, the Chain-of-Thought prompt strategy systematically maps out the reasoning steps necessary for predicting learning performance, employing a sequence of precisely tailored prompt compositions to ensure effective execution of each steps. These compositions encompass: (a) Presentation of Learning Materials: Share the learning materials and associated comprehension questions to establish a basis for analysis. (b) Contextual Transcriptions of Learning Performance Data: Provide a detailed contextual representation of the learning performance data. (c) Analysis Request: Clearly articulate the request for data analysis, specifying the desired insights or outcomes. (d) Method Selection: GPT-4 suggests appropriate analytical or machine learning methods based on the project needs. (e) Model Development: Assistance in developing a machine model, e.g., XGBoost, including training and validation across dataset folds. (f) Performance Evaluation: Calculation and presentation of validation outcomes, such as RMSE, for each fold. (g) Configuration Disclosure: Detailed sharing of the model’s configuration settings for transparency and reproducibility. (h) Skill Assessment: Discussion on assessing learners’ reading comprehension skills based on their performance data. (i) Optimization: Guidance on fine-tuning the model’s hyperparameters for improved predictive performance. (j) Iterative Feedback: Continuous exchange for clarification, refinement, and further analysis based on user inputs and GPT-4’s suggestions. For a comprehensive overview and detailed instructions, please refer to Appendix A  ###reference_1###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Although the present study highlights the potential of LLMs in enhancing predictive accuracy of learning performance prediction, it also identifies certain limitations. Future work remains in strengthening the connections between specific reading comprehension knowledge and skills and the reasoning process. There is a need to explore how these connections can be utilized to refine prompts and enhance predictive effectiveness. Specifically, constraints related to the fine-tuning of LLM-based platforms or APIs may hinder the optimization of models tailed for our dataset. Additionally, limitations in executing deep learning models restrict the application of advanced techniques such as Deep Knowledge Tracing (DKT) [67  ###reference_bx67###], Self-Attentive Knowledge Tracing (SAKT) [68  ###reference_bx68###], Dynamic Key-Value Memory Networks (DKVMN) [69  ###reference_bx69###], which may further improve the predictive accuracy."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "LLMs for Knowledge Tracing in Learner Model",
            "text": "The potential of LLMs for knowledge tracing relies on at least two key aspects: firstly, their capability in identifying knowledge components [70  ###reference_bx70###, 71  ###reference_bx71###], which encapsulate the prerequisite knowledge for proficiently addressing specific questions with the given context; and secondly, their integration with machine learning models (self-selected by LLMs or external), which is further bolstered by LLMs’ inherent interpretability, facilitating cohesive reasoning, assessment, and predictive capabilities concerning the learner performance. Further research in this direction holds significant promise for advancing our understanding and application of knowledge tracing methodologies based on LLMs."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "LLM-based Trace of Learners’ Learning for Intelligent Tutoring Systems",
            "text": "The present study’s finding motivate future research into LLMs to augment and complement modeling of learner learning and dynamic learning states within ITSs. The endeavor involves utilizing diverse data types, including numerical, textual, and even multimodal inputs, to construct a comprehensive learner model. Drawing from LLMs like ChatGPT, Llama, and Gemini, along with various machine learning methods, future research could provide effective real-time prediction of learner learning. By integrating insights from LLMs and machine learning, this approach enhances the pedagogical component of Intelligent Tutoring Systems, enabling more precise instructional strategies and feedback mechanisms. Specifically, the present use of LLMs for learner modeling could be used in nascent applications for tutoring through Expectation-Misconception Tailored (EMT) conversation styles in adult literacy [6  ###reference_bx6###, 35  ###reference_bx35###, 72  ###reference_bx72###] or applications of LLMs to automatically generate peer-tutoring dialog[49  ###reference_bx49###]. Improving learner modeling through these applications could enable more personalized and effective pedagogical strategies and feedback for learners."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The present study investigates the use of LLMs, specifically GPT-4, in predicting learner’s learning performance in the context of adult literacy in Intelligent Tutoring Systems. We developed an LLM-based prediction method that integrates the encoding of numerical learning performance data into a contextual prompt, conducting data analysis and prediction within LLM component, and decoding the output to obtain predicted learning performance data. Compared to traditional machine learning methods, such as BKT, PFA, SPARFA-Lite and Tensor Factorization, our LLM-based method achieves higher predictive accuracy when incorporating with XGBoost (selected by GPT-4 in our study). Although XGBoost running on a local machine initially surpasses GPT-4 in accuracy, the optimization of XGBoost parameters selected by GPT-4 and fine-tuned within the LLM environment exhibits enhanced performance over local execution. Our examination of hyperparameter tuning by GPT-4 versus manual grid search reveals similar outcomes, yet the GPT-4 method introduces a degree of variability. Our findings underscore the potential of merging LLMs with established machine learning frameworks to boost personalization and efficacy in adult literacy education. This work lays the groundwork for future inquiries into the integration of LLMs in ITS environments, demonstrating the increased practicality of employing LLMs for learner performance prediction in AI-enhanced educational contexts."
        }
    ]
}