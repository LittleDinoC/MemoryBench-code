{
    "title": "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
    "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Advanced reasoning in LLMs has significantly influenced and effected the rise to prominence of language agents adept at handling a diverse range of tasks (Shen et al., 2024; Wu et al., 2023; Schick et al., 2024; Significant Gravitas; Osika, 2023). Among them, web agents (Hong et al., 2023; Furuta et al., 2023; Deng et al., 2024; Zheng et al., 2024) have demonstrated potential in automating web-based tasks such as flight booking. While such success through widespread application of LLMs is indeed promising, it necessitates ongoing exploration of their core reasoning proficiencies.\n\nConditional and compositional reasoning play a crucial role in our ability to understand and interact with complex systems through elaborate decision-making (Oaksford & Chater, 2010; Simon & Newell, 1971). Conditional reasoning involves the comprehension and application of logical rules that are typically structured in “if-then” formats. It is critical to personal decision-making in everyday life through an evaluation of potential scenarios and anticipation of consequences. Compositional reasoning, on the other hand, is the ability to combine solutions to simpler sub-problems, and integrate them in a structured manner to solve a more complex task. This cognitive process entails understanding the interplay between different sub-problems. Our paper focuses on assessing how well current LLMs encapsulate these essential cognitive functions, which are integral to both human intelligence and advanced artificial intelligence systems. To that end, we introduce GroundCocoa - a benchmark for evaluating compositional & conditional reasoning in a grounding task.\n\nGroundCocoa is made up of questions framed as user needs, set within a real-world inspired flight reservation scenario. User requirements might be many or could be highly convoluted - leading to higher compositional and conditional complexity respectively. We leverage a controllable method to create samples of varying complexity (examples are provided in Appendix B). Our data generation process, described in greater detail in Section 2, consists of a 5-stage pipeline including online scraping, constraint generation, and symbolic logic to impose conditionality. In order to test for robustness, we allow requirements to freely condition on one another and impose no restrictions on their nature. Additionally, we isolate a subset of more atypical queries that contain unconventional user needs (e.g., “I want at least 2 layovers”) and evaluate their impact on model performance. Samples in GroundCocoa may also require reasoning about time (e.g. when constraints are imposed on arrival or departure times) and arithmetic (e.g. when constraints are imposed on ticket price), thus, integrating logical, temporal, mathematical, and compositional reasoning.\n\nThe statistics of our dataset are shown in Table 1. Our key findings are as follows:\n\nAccuracy among contemporary LLMs varies greatly, ranging from a little better than random guess to about 67% on a five-option multiple-choice question task. Within this spectrum, GPT-4 Turbo (OpenAI, 2023) stands out, demonstrating a superior capacity of the GPT line of models to adapt and excel in novel reasoning tasks. However, conditional reasoning poses a significant challenge to all evaluated models, even on samples of relatively lower complexity.\n\nChain of Thought (COT) prompting (Wei et al., 2022) leads to mixed results, with only a modest improvement in model performance in some cases. Prior research has noted that although COT helps decompose problems into steps, LLMs increasingly struggle as the complexity of the individual steps escalates (Hendrycks et al., 2021b; Madaan & Yazdanbakhsh, 2022; Nogueira et al., 2021; Qian et al., 2023). These assertions hold true in our observations.\n\nIncluding unconventional user requirements leads to a drop in accuracy of as much as 6% in GPT-4 Turbo, indicating a pretraining bias towards more typical needs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "Figure 2  ###reference_### illustrates our proposed approach. In the process of generating a natural language user requirement for flight booking, we are faced with the following considerations:\nConditionality of Constraints: We wish to challenge contemporary models in their ability to reason through scenarios characterized by conditional complexity. This is done through mutual dependence of flight features which we refer to as slots. In the example illustrated in Figure 2  ###reference_###, there is an inter-dependence between the values for price and ticket class. The interplay between constraints corresponding to the different slots is represented in logical form through a Product-of-Sums (POS) expression. A POS expression consists of multiple OR operations (sums) which are later combined through AND operations (products). The inclusion of OR operations between slots introduces conditional complexity to our user requirement, necessitating consideration of potential slot values in if-then scenarios. On the other hand, a greater number of AND conditions implies a higher number of variables that a model has to simultaneously reason over resulting in increased compositional complexity.\nSatisfiability of POS Expression: While generating the logical form for a user requirement, we must ensure satisfiability of the generated POS expression. For this, we use SymPy (Meurer et al., 2017  ###reference_b26###) - an open-source Python symbolic mathematics library which generates an optimal POS expression given a minterm table. This is further described in Section  2.2  ###reference_###.\nFuzziness in Slot Values: Corresponding to each occurrence of a slot in the POS expression there has to be a unique constraint. For the example in Figure 2  ###reference_###, the two constraints on the price slot are {, }. These constraints are randomly imposed through specialized rule-based systems corresponding to each slot. However, these might cause the final user criteria to become impossible to satisfy even if the corresponding POS expression in satisfiable. Thus, for a generated user requirement we check our flight data to ensure that there exists at least one route that satisfies the criteria and at least 4 that do not. This way we ensure there is at least one positive and 4 negative options for a generated requirement.\nStatistics of GroundCocoa are shown in Table 1  ###reference_###. We also include a separate validation set which may be used for tuning hyperparameters. Our 5-stage data creation pipeline is detailed in the subsequent sections.\n###figure_2###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Flight Data Collection",
            "text": "We use a list of the top 50 busiest airports by passenger traffic derived from Wikipedia222https://en.wikipedia.org/wiki/List_of_busiest_airports_by_passenger_traffic  ###reference_est_airports_by_passenger_traffic###. Source and destination airports are chosen randomly from this list and input to Google Flights333https://www.google.com/travel/flights  ###reference_### with the departure date set for April 17, 2024. A small number of flights are sampled from search results for each source-destination pair. The sampled flights are chosen from each of economy, business, and first class and, for each flight option, all the relevant details such as the number of layovers, price, departure and arrival times etc. are saved. A sample flight schema with all the elements is provided in Appendix A  ###reference_###. The entire data collection process is handled through web scraping using Selenium Webdriver444https://www.selenium.dev/documentation/webdriver/  ###reference_driver/###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Product-of-Sums Generation",
            "text": "To generate a POS expression, we first randomly select a small number of flight features or slots. The complete set of slots for any given flight is as follows:\n{airline, ticket class, departure time, arrival time, total travel time, number of layovers, average carbon emission difference, travel date, price, layover locations, layover times}\nWe vary the number of slots between 2 and 6 in order to generate samples of differing complexity. We then randomly generate 2-3 ”minterms” - the list of all input combinations of slots that generate a ”1”. A higher number of minterms results in a greater conditional complexity and leads to more convoluted user requirements. The slot symbols and generated minterms are input to SymPy which uses a redundant-group eliminating algorithm to output the smallest POS expression consistent with the minterm table."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Primitive Generation",
            "text": "Corresponding to each slot, we have developed a rule-based system that randomly imposes constraints on its values. These constraints are converted to natural language through templates. Since a POS expression may contain a negation, we generate two primitives at each turn - one for the constraint and one for its negation. A sample primitive for total travel time is shown in Table 2  ###reference_###.\n###table_1### At this stage, we also isolate samples that include any one of the following three primitives - (1) carbon emissions must be above the average for that route, (2) price of the flight must be above a minimum threshold, and (3) number of layovers on the route should be greater than a minimum. While this list is not exhaustive, such samples (henceforth referred to as ”atypical” queries) are able to successfully encapsulate contrarian needs that are unlikely to manifest often during pretraining."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "LLM Paraphrasing and Human Validation",
            "text": "We carry out LLM paraphrasing in two distinct steps described below. The exact prompts and an example of intermediate results are provided in Appendix D  ###reference_###. We manually verify each query to ensure it is consistent with the primitives and make changes wherever necessary.\nIndividual primitives are substituted into each sum term and combined using templated rules. We then use GPT-4 Turbo to paraphrase each of the sum terms.\nNext, we combine the individual sum terms into a product (logical AND). This is done by merging the individual paraphrases of sum terms, separated by periods. The resulting flight requirement is again paraphrased with GPT-4 Turbo."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Option Matching",
            "text": "We match the generated user requirements with the flight data collected in Section 2.1  ###reference_###. Each route between the source and destination represents a potential choice in our multiple-choice dataset. Choices are divided into subsets containing one positive (matching the user requirement) and four negative (not matching the user requirement) options. This is done to ensure that each multiple-choice question has only a single correct answer for ease of evaluation. Many such subsets may be created from a single user requirement and, consequently, our dataset consists of queries repeated multiple times with differing choices. Details on the number of unique queries and overall samples is provided in Table 1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results & Analysis",
            "text": "To measure performance on GroundCocoa, we test several models of different sizes ranging from open-source to closed-source, including LLAMA 2-chat (Touvron et al., 2023  ###reference_b51###), Mixtral 8x7B - Instruct (Jiang et al., 2024  ###reference_b21###) / Mistral 7B Instruct(Jiang et al., 2023  ###reference_b20###), Gemini Pro(Team et al., 2023  ###reference_b49###), and GPT-4 Turbo. We also test selected larger models with an in-context example using chain-of-thought (CoT) (Wei et al., 2022  ###reference_b53###) reasoning. Since our task involves grounding user requirements to each answer choice, the CoT explanations are provided for each flight option given the user requirement. Thus, our standard CoT (CoT-full) consists of 5 distinct explanations. On GPT-4, we empirically observe that the large resulting context length can often prove detrimental to model performance with the models often confusing between the requirements and options of the test case and the exemplar. To address this, we try a different prompting strategy (CoT-partial) with only two flight choices (1 positive and 1 negative) for the in-context example. Due to limitations on context length (4096 tokens) we are unable to run LLAMA 2-chat 70B on CoT-full. The exact prompts are given in Appendix C  ###reference_###. Results from our experiments are shown in Table 3  ###reference_###.\n\nAs alluded to previously, GroundCocoa presents a substantial challenge for each of the evaluated models, even with CoT prompting. The CoT-partial strategy with only 2 options and explanations leads to better results than CoT-full in 2 out of 3 cases where we have experimented with both, and best results are obtained using GPT-4 Turbo with CoT-partial. It is noteworthy, though, that there exists a marked difference in performance between GPT-4 Turbo and the remaining models. Such a degree of variation represents a significant departure from the usual performance patterns observed in popular benchmarks such as HellaSwag (Zellers et al., 2019  ###reference_b56###), ARC Reasoning Challenge (Clark et al., 2018  ###reference_b4###), WinoGrande (Sakaguchi et al., 2021  ###reference_b34###), and GSM-8K (Cobbe et al., 2021  ###reference_b6###) among others, where results are much more comparable.\n\nBeyond assessing the overall model performance, we also investigate the consequences of varying the complexity of user criteria and presenting relatively unconventional user needs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Impact of Increasing Complexity",
            "text": "In our analysis, we observe the performance of GPT-4 Turbo, the most effective model from among those tested on GroundCocoa across different levels of conditional and compositional complexity. In their recent work on assessing the limitations of transformers on compositional tasks, Dziri et al. (2023) use computational graphs as approximations of the underlying reasoning processes in such models. They define the terms reasoning depth, the length of the deepest layer in the computational graph from the source nodes, and reasoning width, the mode of number of nodes in each layer - indicating the extent of multi-hop reasoning and compositional parallelism required to solve a given problem. Considering the characteristics of GroundCocoa, we focus on reasoning width - the number of variables a model has to simultaneously reason over for a given problem. Intuitively, this may be represented by the number of slots used during the generation of a particular sample. However, keeping the number of rows in the minterm table constant while increasing the slots may often lead to lower conditional complexity as the number of slots is increased.\n\nIn order to effectively gauge the compositional and conditional complexity of a sample in our dataset, we define a dependency graph derived from the POS expression corresponding to that sample. Vertices represent slots and a dependency (edge) is created when a particular slot co-occurs with another slot within a sum term in the POS. A sample POS expression and its corresponding dependency graph are shown in Figure 3. The graph has 3 connected components with the largest connected component (LCC) of size 4. The maximum degree is 2 which corresponds to the two connections for nodes LayoverTime and TicketClass.\n\nGiven a fixed schema for the flight options, the number of sum terms in the POS expression as well as the LCC in the dependency graph are indicative of the reasoning width and, in turn, the compositional complexity of the user criteria. The LCC is the length of the largest chain of slots - the possible values of which are dependent on one another through OR conditions (represented by edges in the dependency graph). This metric effectively reflects the breadth of parallel computation or reasoning width required to accurately infer the given user criteria. Since increased branching in the dependency graph suggests a greater conditional complexity in user criteria, we also analyze model performance with increasing maximum degree of the dependency graph. This gives us the extent of conditioning on a single slot value. In Figure 4, we observe the decline in model performance with increased complexity as indicated by these factors."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Quantifying Confusion in Answer Choices through Entropy",
            "text": "Numerous recent studies have explored how deep learning models, specifically transformer-based architectures, achieve success by exploiting shortcuts (Geirhos et al., 2020; Liu et al., 2022; Tang et al., 2023; Du et al., 2023) and relying on spurious correlations present in the training data (Zhang et al., 2023; Saparov & He, 2023; Saparov et al., 2023). Most recently, Dziri et al. (2023) utilized relative information gain of individual output elements in partially correct answers to explain surface pattern understanding in LLMs. In the same vein, we employ entropy as a metric to measure the confusion that might be caused due to conditions in the user query for a given flight option. We do this in an attempt to demystify how language models may succeed at some and fail at other queries with similar levels of complexity. In order to illustrate this, we take an example user requirement, and two hypothetical and simplified flight options as shown in Figure 5. Additionally, we show the reasoning path that must be navigated in each case for a successful outcome.\n\nWe observe how option B in our example leads to a more convoluted reasoning path, whereas the model is able to bypass considerable conditional overhead in the case of Option A. For the purpose of quantifying this more generally, we observe the compositional primitives (values attached to individual slots in the POS expression) in each sample and attach a binary value indicating if the primitive is satisfied. For the example in Figure 5, we show the primitives and the corresponding values of both options in Figure 6. We also show the probability of a primitive being satisfied and being unsatisfied by the flight option under consideration, as well as the final entropy.\n\nEntropy due to user criteria for each option can then be computed using the formula in Equation 1. Higher uncertainty leads to greater entropy in Option B as opposed to Option A, indicating a greater conditional overhead. In our analysis, we take the entropy values of the correct answer choice for each sample. Figure 7 shows the densities of entropy values for the correct and wrong predictions of GPT-4 Turbo on GroundCocoa. While correct predictions exceed wrong predictions at lower entropy values, an abrupt surge in wrong predictions is observed at higher entropy levels. Thus, entropy gives us yet another measure of conditional complexity from the perspective of the answer choices rather than just the query, and helps explain why a model might exhibit inconsistent results across user queries of similar complexity."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Robustness to Unconventional User Needs",
            "text": "Several contemporary studies have sought to examine the robustness of language models by studying their resilience to out-of-distribution data (Koh et al., 2021; Wang et al., 2023) or through adversarial attacks and input perturbations (Gardner et al., 2020; Goel et al., 2021; Subhash et al., 2023; Sanyal et al., 2022; Yuan et al., 2023). In our work, we challenge models through atypical user requirements in order to assess bias from pretraining and robustness to unorthodox and nontraditional queries. We segregate queries into ”Regular” and ”Atypical” groups as described in Section 2.3. In Table 3, we contrast model performance on samples that describe such unconventional user needs versus those that do not. While most models in our testing show a decay in performance, the impact is more noticeable on better performing models such as GPT-4 Turbo. The in-context example used for all queries when testing with CoT includes two such primitives (ticket price 1800, carbon emission above average). We observe that the decline in performance is less pronounced with CoT."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Reasoning Challenges in NLP. Our work extends the existing line of research on evaluating natural language processing (NLP) systems on different facets of reasoning - most notably commonsense question-answering (Talmor et al., 2019  ###reference_b46###; Huang et al., 2019  ###reference_b19###), physical reasoning (Bisk et al., 2020  ###reference_b3###), social interaction (Sap et al., 2019  ###reference_b36###), mathematical reasoning (Cobbe et al., 2021  ###reference_b6###; Amini et al., 2019  ###reference_b1###; Miao et al., 2020  ###reference_b27###; Hendrycks et al., 2021b  ###reference_b16###), story completion (Zellers et al., 2019  ###reference_b56###), temporal reasoning (Zhou et al., 2019  ###reference_b59###; Tan et al., 2023  ###reference_b47###) abductive reasoning (Bhagavatula et al., 2020  ###reference_b2###) and pronoun resolution (Sakaguchi et al., 2021  ###reference_b34###) among others. Different from these benchmarks, GroundCocoa introduces a unique and substantial challenge for LLMs in the form of conditional and compositional reasoning.\nBenchmarks on Propositional Logic. GroundCocoa also aligns with the considerable body of work on evaluating logical reasoning in language models. The RuleTaker (Clark et al., 2021  ###reference_b5###) and ProofWriter (Tafjord et al., 2021  ###reference_b45###) datasets proposed a modern approach to evaluating logical reasoning through a task involving assignment of binary labels to candidate implications following a set of premises expressed in natural language. The datasets emulate a linear deductive chain of reasoning of varying depths given a set of facts and rules, with ProofWriter augmenting this task through intermediate conclusions and proof generation. LogicNLI (Tian et al., 2021  ###reference_b50###) provides a more comprehensive diagnostic benchmark involving reasoning through all seven fundamental logics (conjunction, disjunction, negation, implication, equation, universal and existential quantifiers). It contains an additional ”paradox” label implying a situation where both the hypothesis as well as its negative proposition can be simultaneously entailed to the premise through different reasoning paths. This facilitates a non-linear reasoning, but is still limited to two contradictory reasoning paths. The FOLIO (Han et al., 2022  ###reference_b14###) dataset boasts a higher vocabulary size due to a hybrid annotation approach but again consists of linear reasoning chains. Along similar lines, ProntoQA (Saparov & He, 2022  ###reference_b37###) proposes a first-order logic benchmark using a linear ontology which might be fictional. This is done to prevent LLMs from predicting correct outcomes through spurious correlations in their pretraining corpus.\nThe benchmarks described here are primarily focused on the evaluation of deductive reasoning. In contrast, GroundCocoa offers a more realistic grounding task with an emphasis on if-then reasoning which leads to many candidate reasoning paths for each answer choice. While deductive reasoning may involve a broader range of logical structures, conditional reasoning is a subset which deals specifically with the relationships and implications of conditional statements. Our dataset consists of a large vocabulary size and context length per sample, leading to greater linguistic diversity, and a higher reasoning width than other benchmarks in logical reasoning. Questions are designed to test for robustness against rare and unconventional user requirements and bring to the fore model bias from pretraining data. Also, unlike most other benchmarks, we do not attempt to evaluate logical reasoning in isolation - our task might require abilities such as temporal or mathematical reasoning.\nCompositional Generalization. Samples in GroundCocoa consist of novel combination of primitives expressed as user requirements in a flight-booking task. Such reasoning falls under the umbrella of compositional generalization - an area that has garnered increasing interest in the scientific community recently. Hosseini et al. (2022  ###reference_b18###) highlight the relative generalization gap with in-context learning between in-distribution and out-of-distribution samples in various semantic parsing tasks. Dziri et al. (2023  ###reference_b9###) demonstrates how transformer-based LLMs may solve compositional tasks by reducing them to linearized subgraph matching. By establishing a computational graph for each problem, the authors are able to define computational complexity by metrics such as the reasoning depth and width which correspond to levels in multi-hop reasoning and average parallelism respectively. Unsurprisingly, increased task complexity leads to a rapid decay in model performance under various settings.\nOur findings largely concur with previous literature on compositional reasoning. However, results on GroundCocoa reveal that even the most advanced LLMs struggle at relatively low levels of compositional complexity when juxtaposed with conditional reasoning and grounding. While Dziri et al. (2023  ###reference_b9###) demonstrated their results using problems such as multi-digit multiplication, dynamic programming, and Einstein’s puzzle - we release a new dataset that is anchored on a practical, real world use-case of parsing complex user criteria and grounding to a fixed schema representing a flight option. GroundCocoa contains a high semantic coverage and we posit that it would be of interest to the NLP community as a hard evaluation set to benchmark compositional generalization in LLMs.\nDialogue-State Tracking. Finally, while our task is reminiscent of a single turn in a dialogue state tracking system, it goes one step further to test a language model’s grounding ability to match a flight schema with the user query. Most schema-guided dialogue datasets (Rastogi et al., 2020  ###reference_b33###; Lee et al., 2022  ###reference_b23###) consist of fixed slot values and the filtering of available options is handled through external systems (e.g. api’s). Slot values in GroundCocoa are fuzzy due to conditional constraints on the primitives. In Figure 5  ###reference_###, TicketPrice may take on different values based on TicketClass. GroundCocoa consists of examples with varying levels of compositional complexity due to long and complex user requirements. This differentiates it from the majority of schema-guided dialogue datasets where the primary objective is goal and slot identification, and tagging of slot values. These tasks, while challenging in their own respect, do not engage a models’ compositional reasoning ability to the same extent."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Modern LLMs have demonstrated remarkable advancements in many tasks including those that are inherently compositional and necessitate conditional reasoning such as mathematical problem solving, and code generation and interpretation. However, discerning genuine reasoning from mere rote learning and shallow understanding continues to be a focal point of study. While LLMs have become exceedingly adept at answering questions of seemingly greater complexity, we show that they can struggle on the same skills when presented with an unfamiliar task setting. While problem size does have an impact, even the less complex samples in our dataset are challenging to the best language models today.\n\nBeyond introducing a new benchmark dataset, we conduct a thorough analysis of the effects of increasing complexity, including advanced prompting techniques, and robustness to atypical queries. Our results uncover a substantial disparity in the performance of competing language models, a distinction that is not as pronounced in most other evaluation benchmarks and highlights their respective abilities in tackling novel challenges. Our data generation process is largely automatic, with human validation at the last step. In addition to the dataset and the evaluation script, we release code for the data generation which can be easily extended to generate more examples, and increase diversity (through different slots) as well as complexity. With minor modifications, the task can be further complicated by incorporating queries with multiple answers and questions that require other forms of logical reasoning such as aggregation (e.g., ”Give me the cheapest flight matching my criteria?”), existential quantification (e.g., ”Is there a first class seat under $5000?”) etc., which we leave for future work."
        }
    ]
}