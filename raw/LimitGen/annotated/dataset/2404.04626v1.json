{
    "title": "Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective",
    "abstract": "Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT’s effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO is indispensable but still lacking.\n\nTo this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent progress in instruction tuning (Ouyang et al., 2022; Longpre et al., 2023) and human preference alignment (Chung et al., 2022) has enabled large language models (LLMs) to exhibit exceptional performance across a wide range of tasks (Touvron et al., 2023; OpenAI, 2023). Specifically, LLMs that undergo supervised fine-tuning (SFT) across different tasks are anticipated to align with carefully curated human feedback and steer their response behavior accordingly. To achieve this, Direct Preference Optimization (DPO) has emerged as a popular and effective approach (Rafailov et al., 2023), which derives reward signals directly from pairwise preference data, thus bypassing the complexity of learning an additional reward model (Christiano et al., 2017; Bai et al., 2022b).\n\nIn the context of DPO, a pairwise preference data takes the form of a triple, comprising a specific prompt or question, the human-preferred response, and the dispreferred response. These pairwise preference data are further used to increase the relative log probability of preferred to dispreferred responses, together with a Bradley-Terry preference model (Bradley and Terry, 1952) based loss function.\n\nDespite its widespread use across various tasks, the limitations of DPO are gradually coming to light, leading to less satisfactory performance as indicated by prior research (Ethayarajh et al., 2023; Xu et al., 2024). Specifically, DPO hinders the learning capacity of LLMs to generate human-preferred responses, suggesting that LLMs after DPO tend to avoid producing human dispreferred responses but struggle to produce human-preferred responses, especially when training the LLM with the human-preferred response and the dispreferred response are literally similar (Pal et al., 2024).\n\nFurthermore, DPO has been criticized for its sensitivity to the SFT’s effectiveness (Xu et al., 2024). In other words, LLMs without proper and effective SFT typically exhibit subpar DPO performance. An empirical explanation for this is that SFT/instruction tuning are crucial for LLMs to comprehend and adhere to human directives before aligning with curated human feedback (Bai et al., 2022a).\n\nDespite these empirical observations, there is still a lack of theoretical analysis and understanding of the defects in DPO, which hinders insights into future directions for improving DPO.\n\nIn conclusion, this paper offers a theoretical analysis and comprehension of the limitations of DPO through an analytical framework employing field theory, particularly emphasizing the limitations regarding the sensitivity to the effectiveness of SFT and the impact on the ability to learn human-preferred responses."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Human Preference Alignment. The purpose of human preference alignment is to steer the response behavior of LLMs and align their responses with human preference. Formally, given a specific question or prompt, an aligned LLM should generate human-preferred responses with a greater probability than human-dispreferred ones. To achieve this, there are two technological approaches: reinforcement learning-based and non-reinforcement learning-based methods. As a primary focus within the reinforcement learning approach, Reinforcement Learning Human (or AI) Feedback (RLHF/RLAIF) aims to directly evaluate and optimize responses generated by LLM. These methods initially train a reward model (RM) to evaluate human preferences, where the reward model can be iteratively trained to improve its performance. \n\nSubsequently, RLHF/RLAIF establish a reinforcement learning framework for LLMs to learn an optimal or nearly-optimal policy that maximizes the reward from the reward model using Proximal Policy Optimization (PPO). While this process significantly ensures the alignment effect of LLMs, the training complexity and convergence of PPO often present practical implementation challenges.\n\nConsequently, non-reinforcement learning-based methods have been proposed. For instance, researchers have suggested simplifying the computation of PPO through the use of Direct Preference Optimization (DPO) and its variations such as -DPO and Kahneman-Tversky Optimization (KTO). Notably, DPO is the first method to eliminate the training phase of the reward model and reinforcement learning, instead directly employing the LLM itself to approximate the reward model and train itself using collected paired human preference and dispreference data.\n\nLimitations of DPO. Researchers have found that several limitations hinder the utilization of DPO, experiencing negative effects after DPO. Empirical evidence suggests that the effectiveness of DPO heavily relies on the training effect of the LLMs after SFT. Although existing efforts have tried to solve this limitation, for example, by introducing curriculum learning and margin-enhanced loss function, the reason behind this limitation still lacks theoretical explanations.\n\nEmpirical evidence also suggests that LLMs, together with DPO, struggle to learn to generate responses that align with human preference. This is particularly true when the edit distance of responses in the same pairwise preference data is close. Furthermore, some researchers attempt to analyze the loss of DPO via the KL-regularization of the LLM before and after the modification of DPO in its hidden reward model. They find that the strength of the KL-regularization becomes weaker and weaker the more deterministic the preferences. However, their analysis focuses on explaining the limitations they have discovered, making it difficult to generalize to other limitations.\n\nTherefore, there is an urgent need for a more comprehensive theoretical analysis of DPO. On one hand, this can deepen our understanding of the role of DPO in aligning with human preferences. On the other hand, we are attempting to unify the explanation of the current limitations of DPO from a higher perspective and indicate potential directions for improvement."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Understanding the Limitations of DPO",
            "text": "Previous studies have observed that DPO has been criticized for its sensitivity to the SFT’s effectiveness and hinders the learning capacity of LLMs to generate human-preferred responses. In this section, we take a step towards theoretically analyzing and understanding the limitations of DPO using field theory."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Analyzing the Loss of DPO",
            "text": "Re-formalizing DPO Loss Function. Given a pairwise preference data, such as HH Bai et al. (2022a) and SHP Ethayarajh et al. (2022), the purpose of DPO is to make the probability of LLMs generating human preference response given, denoted as, higher than the probability of generating human dispreference response, denoted as, where is the parameters of LLMs. Additionally, the DPO loss function introduces, which is the probability of the reference model (usually initiated as the), to compare the difference between the optimized LLMs and the reference model. According to the origin paper of DPO Rafailov et al. (2023), its loss can be written in the following form: where is a hyper-parameter and is the sigmoid function. For easing the calculation, we denote and. In this case, to minimize the loss, we could increase and decrease.\n\nGradient Vector Field of DPO. We calculate the respective derivatives of DPO loss function regarding and, respectively, and construct the corresponding gradient field to visualize the optimization behavior of DPO, revealing the dynamic features of DPO in an intuitive way.\n\nThe partial derivatives of Equation (1) with respect to and are given by:\nWe leave the detailed proof in Appendix A.\n\nFor each pairwise preference data, the update rate of in with respect to, which represents the ratio of the increase in the probability of a human-preferred response to the decrease in the probability of a human-dispreferred response, is.\n\nFor any pairwise preference data, the update rate holds.\n\nGiven that and are two probability ratios, where and. Assuming is the probability of the fixed reference model, we can assume and, where. In this case, we have and. As the DPO optimization progresses, tends to increase and tends to decrease. Consequently, will be greater than, and will be smaller than. In other words, this implies that is greater than 1, is less than 1, and therefore."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Analyzing the Optimization Process of DPO",
            "text": "This section aims to investigate the impact regarding generation probabilities of preference and dispreference data, we visualize the optimization plane (loss landscape) and gradient field in Figure 1. Since and are constants determined by the initial reference model, which may cause stretching or compression of the figure, rather than causing formal changes, we omit the denominators in. We set both and equal to 1, which simulates the situation where the reference model is absent. We interpret Figure 1 in various scenarios, specifically when is extremely large or very small, and when is extremely large or very small.\n\nAs depicted in Figure 1, the gradient vector field vanishes in the area of low and then moves away, but it converges towards the region of low to vanish there. Consequently, the optimization objective facilitates LLM in learning how to produce responses aligning with human preferences and refraining from generating responses that do not align with human preferences. However, the magnitudes in different areas of the gradient space vary, which influences the practical optimization process. In this section, we highlight the following features of the gradient field, which imply that it might be sensitive to the initial conditions of variables, which reflect the potential reliance on the alignment capability of LLMs after SFT.\n\nWhen is extremely small and is extremely large (this mainly occurs in the initial stages of optimization), as depicted in the top left corner of Figure 1(b), the LLMs essentially lack the capability to produce preferred responses and tend to generate non-preferred responses.\n\nWhen both and are extremely large (this also mainly occurs in the initial stages of optimization), as illustrated in the top right corner of Figure 1(b), the LLMs are capable of producing both preferred and non-preferred responses with large probabilities.\n\nWhen is extremely small (this may occur in the any stages of optimization), as depicted in the lower part of Figure 1(b), indicating that the LLMs have limited capability to generate both preferred and non-preferred responses."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Limitation Analysis",
            "text": "Expanding on our previous findings, this section seeks to offer a detailed analysis of the limitations of DPO, setting the foundation for its improvement."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Limitation 1: Hindrance to the learning capacity towards human-preferred responses",
            "text": "Empirical evidence indicates that LLMs, in conjunction with DPO, encounter challenges in learning to produce responses that align with human preference. In the following section, our theoretical findings further support this empirical evidence. Compared to learning to generate human-preferred responses, the DPO loss function shows a tendency for LLMs to easily learn how to avoid generating responses that humans disprefer due to the more significant impact of the DPO loss.\n\nAccording to our Remark 1, the impact of the DPO loss is more significant due to the larger gradient compared to the impact on , which has a smaller gradient. As tends to increase and tends to decrease during optimization, we have . At this point, DPO focuses more on updating to approach 0, while making minimal updates to (due to the larger gradient). In other words, DPO concentrates excessively on indicating to LLMs what constitutes a poor response, while neglecting to guide LLMs on what constitutes a good response that aligns with human preference. Informally, in extreme scenarios, if the human-preferred response and the dispreferred response are literally similar, the gradient with respect to would counteract the gradient of to some extent, thereby weakening the optimization toward and leading to the hindrance to the learning capacity towards human-preferred responses."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Limitation 2: Sensitivity to SFT’s effectiveness",
            "text": "While SFT has become one of the crucial techniques for aligning LLMs with human language, LLMs following SFT may demonstrate differing levels of alignment as a result of factors such as data quality and training strategies. The effectiveness of DPO is dependent on the alignment capability of LLMs following SFT, and subpar SFT may result in a reduction of LLM effectiveness after DPO. In the following section, we offer theoretical explanations for this limitation. To start, we uncover characteristics when handling LLMs with various initial positions within the gradient field of DPO.\n\nThe alignment capability of SFT-ed LLMs may significantly impact DPO, leading to bad initial states for LLM in the optimization plane (loss landscape) of DPO.\n\nThe initial states in the gradient vector field have a significant impact on the final optimization results. As depicted in Figure 1, the optimization plane (loss landscape) and gradient field of DPO in different regions can drive LLM to different optimization results, potentially leading to instability.\n\nIn such instances, LLMs that have not undergone satisfactory SFT often exhibit limited proficiency in effectively adhering to instructions and responding to human queries.\n\nThe initial positioning of these SFT-ed LLMs may be situated in the lower-left corner of Figure 1(b), indicating low probabilities for generating both preferred and dispreferred responses, and a gradient direction that does not entirely prioritize the enhancement of human-preferred response probabilities. Alternatively, the initial positioning of these SFT-ed LLMs may be in the upper-right corner of Figure 1(b). In this scenario, the presence of very small gradients in the upper-right corner can lead to sluggish convergence and challenges in escaping local minima. Consequently, this can result in inadequate learning of human preference data."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we focus on offering a theoretical analysis and comprehension of the limitations of DPO through an analytical framework employing field theory. By analyzing the gradient vector fields of DPO, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This finding can be explained from a unified perspective of DPO regarding the sensitivity to the effectiveness of SFT and the hindrance to the learning capacity of LLMs in generating human-preferred responses. In the future, we will conduct experiments to validate our theory and make improvements to DPO based on our finding."
        }
    ]
}