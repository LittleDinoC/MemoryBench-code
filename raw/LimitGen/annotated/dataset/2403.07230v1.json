{
    "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences",
    "abstract": "Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (one chosen and rejected response per prompt) to align LLMs to human preferences. In practice, multiple responses could exist for a given prompt with varying quality relative to each other. We propose to utilize these responses to create multiple preference pairs for a given prompt. Our work focuses on aligning LLMs by systematically curating multiple preference pairs and presenting them in a meaningful manner via curriculum learning. We order multiple preference pairs from easy to hard, according to various criteria thus emulating curriculum learning. We show detailed comparisons of our proposed approach to the standard single pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna bench, and WizardLM, highlighting its effectiveness. More specifically, Curry-DPO achieves a score on MT-bench with Zephyr-7B, outperforming majority of existing LLMs with similar parameter size. Curry-DPO also achieves the highest win rates on Vicuna and WizardLM test sets in our experiments, with notable gains when compared to standard DPO.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in instruction finetuning (IFT) and reinforcement learning from human feedback (RLHF) have brought unparalleled capabilities to Large Language Models (LLMs), demonstrating impressive performance across a diverse range of tasks (Team et al., 2023; Achiam et al., 2023; Touvron et al., 2023; Beeching et al., 2023). Aligning LLMs with carefully curated human feedback has shown to be critical in steering their response behavior (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022). To align the distribution of LLMs to good responses, preference optimization methods such as Reinforcement Learning from human feedback (RLHF) (Christiano et al., 2017; Kreutzer et al., 2018) and its RL-free closed-form counterpart - Direct Preference Optimization (DPO) (Rafailov et al., 2023) - are an active area of research.\n\nDPO is a proven technique that circumvents the complex RLHF pipeline by directly using preferences to directly finetune LLMs using a supervised learning loss. While DPO has shown impressive performances (Ivison et al., 2023; Jiang et al., 2024), it is limited to a single pair of responses per prompt (one chosen and one rejected). However, several high-quality responses could exist for a single prompt (Cui et al., 2023; Köpf et al., 2023), resulting in multiple candidate pairs per prompt (referred to as “multiple preference pairs”) for pairwise preference optimization datasets. Several ongoing and concurrent alignment methods have also utilized multiple preference responses. For example, Liu et al. (2024) proposed LiPO where policy directly optimizes on listwise ranked preferences. Parallel to these, our approach is still primarily focused on pairwise preference optimization but with multiple preference pairs that are sequentially ranked during training to emulate curriculum learning.\n\nIn this work, we have focused on training the DPO method with multiple preference pairs in a curriculum learning setup, but our approach can be easily extended to other preference optimization methods such as Sequence Likelihood Calibration (SLiC) (Zhao et al., 2023). We hypothesize that the use of multiple preference pairs per prompt in the DPO framework could act as a form of data augmentation. While it may be tempting to simply collate these pairs and perform DPO training, we show that systematically introducing them to the model is important to achieve better results. Concretely, we propose incorporating curriculum learning on multiple preference pairs into the DPO framework.\n\nCurriculum learning is a training paradigm that arranges data samples in a purposeful order with the aim of improving model performance (Bengio et al., 2009). Previous research has shown that presenting training samples from easy to hard could benefit the learning process for both humans and machines (Elman, 1993; Peterson, 2004; Krueger and Dayan, 2009; Bengio et al., 2009). Given preference pairs, if the chosen and rejected responses are further apart (based on a determined criterion, e.g., reward score or log probability), it would be easier for the model to learn within the DPO framework. However, if the chosen and rejected responses have near similar quality, it would be harder for the model to learn within the DPO framework. Motivated by this, we order the multiple preference pairs from easy to hard during training, resulting in improved performance.\n\nOur curriculum learning-based DPO method, which we call Curry-DPO, significantly outperforms the standard single preference pairs DPO on several benchmarks, including MT Bench and Wizard-LM. The key contributions of our work are as follows: We introduce Curry-DPO that incorporates curriculum learning on multiple preference pairs into the DPO training framework. Curry-DPO demonstrates strong improvements over SFT and standard single preference pair-based DPO on three standard evaluation benchmarks — MT-Bench, WizardLM, and Vicuna. Notably, Curry-DPO achieves the best MTbench score of 7.43 and adjusted win-rates of 87.1% on WizardLM in our experiments. We present different variants of Curry-DPO to highlight the importance of each step within our proposed method. We empirically show the effectiveness of ordering multiple preference pairs and updating the reference model for each iteration in curriculum training."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Aligning LLMs to Human Preferences",
            "text": "The use of human feedback to improve language models has been shown to yield great success Stiennon et al. (2020  ###reference_b34###); Bai et al. (2022  ###reference_b3###); Ouyang et al. (2022  ###reference_b28###).\nWhile Reinforcement Learning from human feedback (RLHF) Christiano et al. (2017  ###reference_b10###) has been the prominent technique used to achieve this, closed-form methods that aim to bypass its complex pipeline have been recently introduced. Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b31###) heralded this by proposing to align LMs on offline pairwise preference data with a supervised logistic loss.\nZhou et al. (2023  ###reference_b48###) propose to extend DPO to a multi-objective setting, while Xu et al. (2023b  ###reference_b42###) introduce a pairwise cringe loss for preference optimization on the chosen responses by maximizing their likelihood while reducing the likelihood of selecting the tokens of rejected responses.\nOther variants, such as Kahneman-Tversky Optimization (KTO) Ethayarajh et al. (2024  ###reference_b14###) and Identity Preference Optimization Azar et al. (2023  ###reference_b2###), have also been introduced with very competitive results.\nHowever, one similarity among these methods is that they mostly use a single pair (chosen and rejected responses) of preference data per prompt. More recently, some works have strayed away from this by introducing the use of multiple preference pairs per prompt. Yuan et al. (2023  ###reference_b43###) propose RRHF (Rank Responses to align Human Feedback), a learning paradigm that seeks to align an LLMs probabilities to multiple responses with a ranking loss. In the same vein, Liu et al. (2024  ###reference_b25###) utilize learning to rank approaches to align an LLM to a ranked list of multiple responses for each prompt. Furthermore, Zhao et al. (2023  ###reference_b45###) apply Sequence Likelihood Calibration (SLiC) to align models to human preference data with multiple preference pairs. However, none of these works apply the standard DPO approach to multiple preference pairs.\nOur work seeks to fill this gap by introducing multiple preference pairs into the DPO framework. Furthermore, we present the multiple preference pairs to LLMs in a meaningful manner via curriculum learning. One interesting property of our method is that it could easily be incorporated into any of the aforementioned DPO variants Ethayarajh et al. (2024  ###reference_b14###); Azar et al. (2023  ###reference_b2###). However, we leave this exploration for future work."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Curriculum Learning",
            "text": "Curriculum is a training paradigm that seeks to present data samples in a meaningful manner, thus controlling and optimizing the type of information a model has access to at each training step Elman (1993  ###reference_b13###); Bengio et al. (2009  ###reference_b5###).\nPrevious works have shown success of learning from easy to hard examples in humans and machine Peterson (2004  ###reference_b29###); Krueger and Dayan (2009  ###reference_b23###); Bengio et al. (2009  ###reference_b5###).\nCurriculum learning has also been extensively used in Natural Language Processing tasks such as language modelling Choudhury et al. (2017  ###reference_b9###); Xu et al. (2020  ###reference_b40###), reading comprehension Tay et al. (2019  ###reference_b35###), question answering Sachan and Xing (2016  ###reference_b32###, 2018  ###reference_b33###) and machine translation Platanios et al. (2019  ###reference_b30###); Zhang et al. (2019  ###reference_b44###); Lu and Zhang (2021  ###reference_b26###).\nThe only application of curriculum learning to LLM alignment is in concurrent work where they perform self-alignment bootstrapping for supervised fine-tuning Wang et al. (2024  ###reference_b39###). To the best of our knowledge, we are the first to apply curriculum learning to the direct preference optimization learning framework."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "We focus on developing curriculum learning methods for utilizing multiple pairs of preference data, with varying degrees of data quality, in the DPO framework.  \nThe main steps in our approach are to sample and arrange these multiple preference pairs for curriculum learning.  \nWe explain methodologies for both these steps below:  \nOpenAssistant — The OpenAssistant Köpf et al. (2024  ###reference_b21###) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation.  \nZheng et al. (2024  ###reference_b46###) — It comprises of multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale of, and the overall score is determined by the mean over the two turns across all questions.  \nChiang et al. (2023  ###reference_b8###) — It contains diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate333weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024  ###reference_b46###) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.  \n— Similar to Vicuna, WizardLM Xu et al. (2023a  ###reference_b41###) contains questions, spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Sampling Multiple Responses per Prompt",
            "text": "Human preference and quality rating of multiple responses are important for creating preference pairs that can be sampled based on relative rating. For instance, given a prompt query and its two different responses  and , if the rating of response  is greater than that of response , then  can be selected as chosen and  as rejected. We experiment with a widely studied dataset containing multiple preference annotations - OpenAssistant Köpf et al. (2023  ###reference_b20###, 2024  ###reference_b21###). In this dataset, each query contains 4 responses where each response is rated by human annotators as in OpenAssistant. However, it should be noted that, in practice, various open source LLMs can be used to sample Chen et al. (2024  ###reference_b7###); Lee et al. (2023  ###reference_b24###); Wang et al. (2024  ###reference_b39###) and rate Jiang et al. (2023b  ###reference_b19###); Lee et al. (2023  ###reference_b24###); Wang et al. (2024  ###reference_b39###) multiple responses for a given user prompt. In our work,  is considered as the highest rated response,  as 2nd highest,  as 3rd highest and  as the lowest rated response for a given query. Thus, in terms of response ratings, .\n\nThe response ratings for each query prompt are then used to arrange the preference pairs as described below."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Curating and Arranging Multiple Preference Pairs",
            "text": "Motivated by previous works in curriculum learning Peterson (2004 ###reference_b29###); Krueger and Dayan (2009 ###reference_b23###); Bengio et al. (2009 ###reference_b5###), we hypothesize that preference learning would be more effective if samples with response ratings that are farther apart are shown during initial training iterations e.g., — with highest rating as chosen and with lowest rating as rejected. As training progresses, we present samples where the chosen and rejected responses have closer ratings e.g., . This way, the model learns to discern samples in increasing order of difficulty.\n\nAs shown in figure 1 ###reference_###, we create preference pairs where the chosen is always the highest rated response and remaining 3 responses are selected as rejected to create preference pairs for each query prompt . We then rank each pair based on the difference in response quality rating between chosen and rejected as previously described. We use the following variants to determine the response quality difference in our experiments:\n\nHuman preferences — In OpenAssistant, we use the human rankings of the responses to determine the order of the curriculum. Similar to the above case, we select the highest and lowest rated responses followed by and finally .\n\nLog Probabilities (LogP) score — We also experiment with using the reference model to compute LogP scores for each of the responses. As with the previous two methods, we arrange preference pairs for each DPO iteration by computing LogP score difference between chosen and rejected for the pairs . This method is based on the intuition that the model will be able to easily differentiate between response probability distributions which are further apart. In contrast to GPT-4 ranking, where the pair are fixed i.e. for the first iteration, and for the second and third iteration respectively, with LogP ranking the pairs might change for each iteration."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Training methodology",
            "text": "Given a dataset of preferences of size containing an input , a chosen and rejected response  and  respectively, Direct Preference Optimization Rafailov et al. (2023) aims to optimize the SFT model directly using the preference dataset . Under the Bradley Terry preference model Bradley and Terry (1952), they express the parameter update as a function of the current model  and the reference model as shown in eq. (1).\n\nwhere  represents sigmoid activation,  represents the parameters of the current policy being trained,  represents the DPO loss, and  is the parameter controlling deviation from the reference model (SFT model in this case).\n\nIn the first iteration of our proposed curriculum DPO (Curry-DPO), the reference model is the base SFT model as shown in eq. 1. From the 2nd iteration onwards, the previous iteration model () is considered as the reference model:\n\nwhere  is the reference model from previous iteration and  is the new policy that is being trained in the current iteration. Other notations are same as eq. 1. Please note that chosen () and rejected () response pairs are selected separately for each iteration () as explained in section 3.2.\n\nWe experiment with the following variants of DPO training:\nIterative DPO with previous iteration model as the reference — In this setting, the previous iteration model () is considered as the reference model when we train the new policy model () in the current iteration. This setting is represented in Equation 2.\nIterative DPO with the same SFT reference model — In this setting, the SFT model () is considered as the reference model in all three iterations. While we train and update the policy model in each iteration i.e., (), the reference model remains () in each of the three iterations. We considered this method as a baseline to evaluate the importance of updating the reference model in each iteration.\nNon-iterative DPO training — In this setting, we use the as the reference model in a single training run. However, we show the training samples in the following order - . We considered this as a baseline to highlight the gains from performing Curry-DPO training iteratively."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "OpenAssistant — The OpenAssistant Köpf et al. (2024) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation.\nZheng et al. (2024) — It comprises of multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale of, and the overall score is determined by the mean over the two turns across all questions.\nChiang et al. (2023) — It contains diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate, a weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.\n— Similar to Vicuna, WizardLM Xu et al. (2023a) contains questions, spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 Datasets",
            "text": "We train our models with the OpenAssistant Köpf et al. (2023  ###reference_b20###, 2024  ###reference_b21###) datasets. Both of these datasets contain four response pairs with corresponding ratings for a given input prompt. \n\nBrief descriptions of these datasets are as follows:\nOpenAssistant — The OpenAssistant Köpf et al. (2024  ###reference_b21###  ###reference_b21###  ###reference_b21###) dataset consists of crowd-sourced conversation trees in multiple languages. We filter out the dataset to include only conversation trees in English and randomly sample 5K conversations and take top-4 samples at every level in each conversation."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2 Models",
            "text": "We perform experiments using two models - Zephyr-7B Tunstall et al. (2023) and Mistral-7B Jiang et al. (2023a).\n\nFor experiments with Mistral-7B, we finetune the base Mistral-7B on 13K OpenAssistant top-1 conversation samples. We then perform DPO with this finetuned model on a set of preference pairs obtained from the multiple responses available from the OpenAssistant dataset.\n\nWe train both our models in bfloat16 precision with the Adam optimizer and no weight decay for all experiments. We use a global batch size and a maximum learning rate. We use a linear learning rate scheduler and warmup for a fraction of the training steps."
        },
        {
            "section_id": "3.4.3",
            "parent_section_id": "3.4",
            "section_name": "3.4.3 Evaluation",
            "text": "We evaluate our baselines and models across popular test benchmarks — MT-Bench Zheng et al. (2024 ###reference_b46###), Vicuna bench Chiang et al. (2023 ###reference_b8###), and WizardLM Xu et al. (2023a ###reference_b41###) test set. These benchmarks use GPT-4 OpenAI (2023 ###reference_b27###) as a judge to evaluate the quality of the generated response. \n\nThe GPT-4 evaluation prompts are taken from Zheng et al. (2024 ###reference_b46###) and are also provided in the Appendix for reference. Zheng et al. (2024 ###reference_b46###) — It comprises multi-turn questions spanning eight distinct knowledge domains. The models are required to respond to an initial question and subsequently provide a second response to a follow-up question. GPT-4 assesses each model’s responses on a scale, and the overall score is determined by the mean over the two turns across all questions. \n\nChiang et al. (2023 ###reference_b8###) — It contains diverse single-turn questions spanning topics like commonsense reasoning, knowledge, writing, math, coding, etc. It uses GPT-4 to compute the adjusted win rate, weighted win rate score which is calculated by 1*win + 0.5*tie Zheng et al. (2024 ###reference_b46###) between the responses from two models for a single prompt. More specifically, GPT-4 is presented with the question and two responses, one from the SFT model and another from the DPO or Curry-DPO model, depending on which model we are evaluating. GPT-4 is prompted to prefer a response with better overall quality or tie if both responses are equally good.\n\nSimilar to Vicuna, WizardLM Xu et al. (2023a ###reference_b41###) contains questions spanning multiple topics generated using the Evol-Instruct procedure. Same as Vicuna, we compute adjusted win rate on the WizardLM test set."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we summarize our baselines and key observations from our experiments."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We implemented several important baselines for detailed comparison of our proposed method. First, we implemented naive DPO Rafailov et al. (2023) with single preference pairs that were sampled from multiple responses in OpenAssistant. We use the same three preference pairs that are used in the training of Curry-DPO as explained in section 3.2. Each preference pair is used to individually train three DPO baselines as shown in rows 1-3 in table 1 and table 2, where each row corresponds to preference pairs with: 1.) best rated response as chosen and lowest rated response as rejected, having the maximum preference score difference and thus making it easier to learn, 2.) with the second highest gap between preference scores, and, 3.) with the lowest gap between response rating scores (difficult to learn). We also implement two other important baselines with multiple preference pairs based DPO. As shown in table 1 and table 2, we simply pooled two sets of preference pairs (row 4) and three sets of preference pairs (row 5) for DPO training of the SFT model for 3 epochs. We randomly shuffle the training data points while batching, thus ensuring that the DPO training does not use any specific order of the multiple preference pair data.\n\nLastly, to highlight the importance of iterative training within curriculum learning, we implemented a baseline Curry-DPO with the same three sets of preference pairs, but in a single train (referred to as Non-iterative (NI) in row 6 and row 7). In addition to our extensive set of baselines, we also report results from relevant prior work in table 1 in rows P0-P2.\n\nKey observations from our results are as follows:\nSingle preference pairs — Inspired by the selection of easy training instances in curriculum learning, we constructed preference pairs with the hypothesis that pairs with maximum rating gap would be the easy training samples for preference optimization with DPO. As shown in row1 - row3 of table 1 and table 2, we observe that our hypothesis holds. Performing DPO with achieves the highest performance while DPO with results in the lowest evaluation numbers. These results also highlight the importance of choosing the best preference pairs that could potentially provide the strongest signal for preference alignment with DPO.\n\nSingle pair vs MultiPair Curry-DPO — In the majority of the settings, Curry-DPO trained with a set of three preference pairs (row 6 and onwards in both result tables) outperforms DPO with a single preference pair. Especially the iterative Curry-DPO shown in row 8-11 in table 1 and table 2 outperforms all of the single preference pair (row 1-3) DPO baselines on MT-Bench and WizardLM. We observe one exception where the strong DPO baseline with preference pair (row 2 in table 1) achieves the highest score on Vicuna evaluation.\n\nImportance of Iterative Training — As observed in rows 6-7 of table 1 where all the 3 sets of preference pairs are pooled and randomly batched for a single step DPO training, evaluation scores are similar on MT-bench but much worse on other benchmarks when compared to single preference pairs DPO baselines (row 1-3). However, when we order the same set of preference pairs and train on each pair (per epoch) (rows 8-11) iteratively, the overall performance improves with notable gains in WizardLM and MT-bench. Finally, Curry-DPO with reference model from the previous iteration (row 13) achieves the best performance in all of the evaluation benchmarks in both table 2 and table 1 (with the only exception of Vicuna in table 1). Another important finding is that other similar works like self-play (SPIN) Chen et al. (2024) also show improvement with iterative-DPO training (row P0-P3 in table 1). As an orthogonal direction to SPIN, our Curry-DPO method instead focuses on selecting multiple preference pairs as per the rating differences and uses them in curriculum learning-based DPO training yielding much higher improvements.\n\nReference model selection — As shown in row 8 vs row 9 and row 10 VS row 11, selecting the reference model as the checkpoint from the previous iteration of Curry-DPO results in better evaluation scores when compared to selecting SFT model (row 0) as the reference model. This crucially highlights the importance of iteratively updating the reference model in Curry-DPO training.\n\nGains on benchmarks — Our best performing iterative Curry-DPO method (row 9) achieves the best numbers in experiments with OpenAssistant. In table 1, iterative Curry-DPO achieves a strong 7.43 score on MT-bench, surpassing several existing LLMs with similar parameter size on MT-bench leader board Zheng et al. (2023"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "— We evaluate our Curry-DPO and baseline DPO trained with single preference pair on the LLM jail break & safety dataset Huang et al. (2023  ###reference_b15###). The dataset contains various prompts that are specifically targeted to disrupt alignment and elicit harmful responses from LLMs. We observed distinctive benefits of Curry-DPO on safer response generation over baseline DPO model. We show two examples in table 3  ###reference_###, highlighting the safe responses from Curry-DPO model. In the first example of table 3  ###reference_###, Curry-DPO shows reluctance and cautions against bad actions but still follows the given instruction. In the 2nd example, Curry-DPO shows stronger reluctance compared to the baseline DPO method suggesting overall improvements in harmless response generations.\nOn the full evaluation, Curry-DPO model achieves 68.96% adjusted win rate when compared to 59.39% win rate of baseline DPO.\n— In addition to harmless response generations in table 3  ###reference_###, we also show examples of helpful responses in table 4  ###reference_### (in Appendix). Here also, we observed Curry-DPO to generate more helpful responses compared to the baseline DPO model with single preference pair."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we presented Curry-DPO that utilizes multiple pairwise preference data to further improve upon existing prominent DPO method. We showed that curriculum learning based iterative DPO training can achieve strong improvements over the vanilla DPO trained with single preference data, thus highlighting unrealized potential of DPO method for preference optimization for future works. Our method Curry-DPO is intuitively orthogonal to many of the concurrent approaches and can be easily extended to other DPO alike techniques for preference learning and optimization. Additionally, key steps such as creation of multiple preference pairs in Curry-DPO could be easily coupled with other iterative-DPO techniques such as SPIN for complementary gains. Overall, encouraging empirical results from our proposed method - Curry-DPO - establishes motivations for future works on preference optimization to strongly consider effective techniques like curriculum learning and iterative training."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "A few important limitations (and potential future work) of our work are summarized below:\nIn this work, we experiment with 3 pairs of preference data for iteratively training our Curry-DPO method, although other different combinations of pairs can also be easily constructed. For example, with 4 responses for each prompt, there are 4C2 = 6 plausible combinations. We also experimented with creating 4C2 preference pairs and selected top 3 pairs based on difference between LogP generation score of chosen and rejected response in each pair. We observed strong performance on experiments with OpenAssistant (MTbench score of 5.70, adjusted win rates of 69.1% and 78.2% on WizardLM and Vicuna respectively).\nWe have shown experiments with three iterations of training with our Curry-DPO method. With selection of more number of preference pairs (i.e., for 4C2 = 6 pairs), Curry-DPO could be trained for more iterations. We leave this exploration also for future work.\nRanking of teacher openLLMs for each response - In this work, we have considered ratings from GPT-4 on human ratings on OpenAssistant dataset. In scenarios where ratings are not available, future (reliable and robust) open LLMs can be considered as secured judge LLMs for rating multiple responses for a given prompt."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Statement",
            "text": "We introduced Curry-DPO that trains DPO method on multiple preference pair in a curriculum training setup. The datasets used in our experiments—OpenAssistant—contain prompt and multiple responses (with ratings) on several sensitive topics to better align LLMs with human preferences on helpfulness, honesty, harmlessness, instruction following, etc. We want to re-share the same caution and ethical considerations as OpenAssistantKöpf et al. (2023 ###reference_b20###) as we simply train our models on these datasets. The generated responses from our trained model can have sensitive responses similar to ones present in OpenAssistant. We discuss in Section 5 ###reference_### that responses from our Curry-DPO are safer than SFT model and baseline DPO method using single preference pair. Although Curry-DPO responses are safer and more aligned with human preferences, the model could still generate harmful contents as shown in the first example in table 3 ###reference_###. Therefore, we want to highlight that even after better alignment with preference data, Curry-DPO can still generate harmful responses and should be used with caution."
        }
    ]
}