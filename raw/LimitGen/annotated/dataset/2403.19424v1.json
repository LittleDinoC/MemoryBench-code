{
    "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
    "abstract": "Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size. We systematically investigate the interaction between and spans and propose an improved configuration for selecting important tokens.\n\nKeywords: interpretability, spans, agreement",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Transformer-based models learn to map features in the input to some output. When training an NLP system, the model learns to identify the most important features (in our case tokens) for the final prediction. Post-hoc explanation methods such as LIME (Ribeiro et al., 2016) and Integrated Gradient (Sundararajan et al., 2017) aim to attribute an importance score to the individual features to interpret the model’s decisions. Generally, these methods tend to disagree with each other when ranking token importance on a set of top- tokens based on attribution scores (Neely et al., 2022). Given their disagreement, and assuming that explanations that are faithful to the transformer’s inner mechanisms should be agreeable (Jain and Wallace, 2019), the faithfulness of these methods comes under question. However, methods might agree more than initially appears.\n\nFor example, Figure 1 shows that none of the methods selects the same top-4 tokens and that 12 of the 13 tokens appear in at least one top-4 selection, indicating a high variance across methods. Intuitively though, methods seem to target the verb phrases are standing and are unloading to a high degree as the vast majority highlights at least one of the tokens in each of these phrases. Similarly, some methods tend to agree on the noun phrases shipyard workers (first occurrence) and the ships, and even more so on different tokenised subwords of the same word, namely un and ##loading. This leads us to hypothesise that agreement between methods is systematically higher when we look at the linguistic spans they are targeting: the constituents to which tokens syntactically belong.\n\nThis example shows that a single method may have a specific preference for one word class over another, e.g. noun over adjective, auxiliary over inflected verb form or modifier over head. For example, Ramnath et al. (2020) report part-of-speech (POS) preference statistics for the different layers of BERT (Devlin et al., 2019) for the Integrated Gradient method. However, the extent to which preferences differ across methods remains unclear, as well as its impact on method–method agreement.\n\nA methodological aspect that directly affects agreement is the selection of the top- most important tokens for each method to compare. This is a relatively under-explored parameter and is defined as the number of features that are assigned highest scores by the attribution method, relative to all the features in the input example. A common way of picking is by selecting a fixed number, generally in the range . Intuitively, a that is fixed across instances (e.g. 4) is suboptimal, and the selection process of is often overlooked (Jesus et al., 2021; Camburu et al., 2019) or obtained by an approximation (Krishna et al., 2022). As an alternative, can be estimated dynamically across instances (Pruthi et al., 2022; Kamp et al., 2023), but different conceptual settings for this approach and their effect on agreement have not been investigated yet.\n\nInstead of ranking tokens by attribution score and manually setting a , Kamp et al. (2023) propose to automatically detect tokens that are signal peaks in the input. Hypothesising that spans are better suited for agreement than tokens conceptually overlaps with this dynamic approach. Precisely, the latter suggests that solely focusing on token-level attribution scores, semi-arbitrary importance cut-offs and the consequent agreement measurements between tokens may be undesirable for interpreting model behaviour.\n\nIn this paper, we aim to disentangle the interdependencies between word class preference, span-level agreement, and the determination of . We show that methods systematically select different word classes and that methods that agree most with other methods and with humans exhibit similar word class preferences. We also find that dynamic and spans work well in combination, and that an adapted threshold for dynamically selecting the most important tokens passes our baseline tests for both token- and span-level estimation. Our main contributions are: i) a linguistic analysis of disagreement on the token-level and on the span-level and ii) an improvement to the dynamic estimation algorithm. All analyses are available at: https://github.com/jbkamp/repo-Span-Pref."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "In this section, we place our work in the context of prior work on interpretability (§2.1  ###reference_###), the patterns of linguistic information that attribution methods reveal (§2.2  ###reference_###) and top- estimation (§2.3  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Model Interpretation",
            "text": "Tracing the decision processes in neural models poses difficulties due to various factors, including their non-linear nature and the absence of explicit human-defined rules to link patterns in the input features with output labels. Different research lines exist to interpret different aspects of the model (Choudhary et al., 2022  ###reference_b12###; Räuker et al., 2023  ###reference_b30###), such as the linguistic information that might implicitly be learned by the model, or the importance that single input features might have had towards the model’s decision (Madsen et al., 2022  ###reference_b23###).\nTo address the latter, post-hoc attribution methods in NLP have been developed to assign a score to each token in the input, creating an attribution profile over the tokens. While these methods are often being used in error analyses (Bongard et al., 2022  ###reference_b8###, i.a.), their reliability is questionable. In fact, attribution profiles obtained from different methods can differ strongly even on the same input. This leads to an overall low inter-method agreement (Neely et al., 2022  ###reference_b25###), which has also been found for domains outside of NLP (Krishna et al., 2022  ###reference_b20###). Diverging experimental results of such methods on different models, datasets and tasks provide additional evidence on their inconsistency. For example, when trying to identify the attribution methods that best align with human preferences–the most plausible (Jacovi and Goldberg, 2020  ###reference_b14###) methods–, Atanasova et al. (2020  ###reference_b3###) and Attanasio et al. (2022  ###reference_b4###) come to fundamentally opposing conclusions. Roy et al. (2022  ###reference_b32###) characterise disagreement between methods in a software defect prediction task as being highest in terms of top- feature importance, followed by rank, then sign. Similarly to Pirie et al. (2023  ###reference_b26###), they propose aggregation schemes for different explanation methods that aim to tackle disagreement in real-world use cases.\nOne question that, to our knowledge, remains under-explored, is why attribution methods in NLP disagree. A key to answering this would be comparing methods on their faithfulness, i.e. the degree to which methods are reflecting the model’s decision making process, as recent work (Atanasova et al., 2023  ###reference_b2###, i.a.) aims to assess. However, directly measuring faithfulness might only find glimpses of the model’s inner workings rather than providing a conclusive answer (Jacovi and Goldberg, 2020  ###reference_b14###). Therefore, we think that the first step should be explaining disagreement by the observable output of the methods, i.e. the attribution profiles. We aim to provide a linguistic comparison by quantifying the kind of features that are targeted, expecting different methods to consistently target different classes of words."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Linguistic Patterns in Attributions",
            "text": "Identifying the linguistic preferences of models is important in order to pinpoint the cues upon which models depend during inference time. Only a handful of studies have explored POS preference. Especially in a feature attribution setting, there is little evidence that shows certain preferences by different attribution methods and how these preferences differ. Lai et al. (2019  ###reference_b21###) find that different models (i.e. LSTM, XGBoost and SVM) have different POS preferences on the same data and task, but they do not explore preferences for different attribution methods. Ramnath et al. (2020  ###reference_b29###) examine the top-5 most important tokens in each layer and find that BERT (Devlin et al., 2019  ###reference_b13###) primarily focuses on nouns in all 12 layers, followed by verbs and adjectives. Interestingly, both punctuation tokens and stop words each correspond to 10% in the top-5 selections. However, only Integrated Gradient (Sundararajan et al., 2017  ###reference_b38###) was used in this experiment, limiting the generalisability of their findings. Our analyses differ from theirs in that we compare different methods and investigate the overlap between agreement and linguistic preference.\nLanguage (and model behavior) can often not be explained by merely highlighting individual tokens. Rather, we would ideally observe how features act in combination with each other and, for example, if they do so hierarchically. As an alternative way of analysing the attributions of tokens in isolation, we find a growing line of research on feature interactions. Jumelet and Zuidema (2023  ###reference_b17###) find evidence of attribution methods faithfully reflecting linguistic structure in language models. Sikdar et al. (2021  ###reference_b35###) combine token-wise attribution scores into scores assigned to syntactic parent constituents. Similarly, Babiker et al. (2023  ###reference_b6###) train a model on intermediate representations in a hierarchical fashion. Song et al. (2023  ###reference_b37###) aim to capture the causal effect of word group combinations on the prediction but limit their scope to the Integrated Gradient method. Pruthi et al. (2022  ###reference_b28###) anticipate that certain spans of tokens should be highlighted by attribution methods in a sentiment analysis task.\nWhile their intuition is on point, the relatively broad expectations found in the latter underscore the relevance of a clear definition of token spans and their role in demonstrating how neighboring features are grouped.\nAs far as we know, there is no prior work that covers a linguistic analysis of the token selections targeted by different attribution methods. To the best of our knowledge, we are also the first to investigate the relation between disagreement on the linguistic level to overall disagreement among methods. We provide a linguistic analysis in terms of individual tokens, and also in terms of spans that have a clear syntactic definition. In particular, we link disagreement to linguistic preference on the token level and within spans."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Top- Estimation",
            "text": "We analyse the factors of disagreement through an additional scope, namely top- estimation.  represents the number of most important tokens in the attribution profile. Studies reporting on consistent disagreement between methods do not take the impact of the  number of selected tokens into account (Pruthi et al., 2022  ###reference_b28###; Krishna et al., 2022  ###reference_b20###; Neely et al., 2022  ###reference_b25###). A common way of selecting  is approximating it to a low value, e.g. 1 or 2 (Bastings et al., 2022  ###reference_b7###), 5 (Ramnath et al., 2020  ###reference_b29###), 5 or 10 (Camburu et al., 2019  ###reference_b9###), 25% of the average input length (Krishna et al., 2022  ###reference_b20###). However, a  that is fixed does not account for variability among instances. A  that is too low can exclude important tokens from the comparison, whereas a  that is too high will include non-important tokens while artificially boosting agreement between methods. Keeping  relatively low also helps users to more easily digest the explanations in a real-world scenario.\nThe value of  has also been estimated dynamically. Pruthi et al. (2022  ###reference_b28###) set  to 10% of the input length, assuming that longer inputs have a higher number of important features than shorter inputs. Kamp et al. (2023  ###reference_b18###) propose a  that varies dynamically based on properties of the attribution profile of each instance, aiming to include features that display above average importance and that focus more on the targeted region of the input instead of the specific token. While their method estimates a value for  that is close to human preference, we find that their algorithm necessitates further experiments and refinement. Different importance thresholds are possible and need baseline benchmarking. Also, as of now, prior methods for determining dynamic  do not explicitly account for negative attribution scores.\nWe adopt and improve the dynamic  estimation by Kamp et al. (2023  ###reference_b18###) throughout §4  ###reference_###, when measuring agreement at the span level compared to the token level. Formally, this dynamic approach defines a strong signal in the attribution profile as a score that is higher than its neighboring scores according to two principles: local importance and global importance. Local importance requires that a score must be higher than its strict neighbors ( window) to reduce redundancy of tokens belonging to the same signal. In other words, a set of adjacent tokens with relatively high scores is converted to a single important signal and the highest attribution in the set is kept as the peak of the signal. Similarly, the global importance principle requires important signals to be minimally above average signal strength, i.e. , where  is the attribution profile. By only adopting the global importance threshold, the inclusion of groups of (redundant) neighboring tokens with high attribution scores is expected to increase , unnecessarily boosting the agreement scores. Therefore, the addition of a local importance setting, which we keep unaltered for our remaining experiments, is necessary to estimate signal peaks. As for global importance, we keep the threshold constant in §4.2  ###reference_### to compare span-level agreement to token-level agreement in previous work, and explore different settings in §4.3  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Linguistic Analysis",
            "text": "We hypothesize that one of the reasons attribution methods disagree is that different methods have different preferences for the classes of words they target. Following from this, we expect that differences in word class preferences are put under a different light when we look at the syntactic spans they are assigned to."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Setup",
            "text": "To analyze the disagreement problem, we consider six different attribution methods on a natural language inference task. For the sake of testing our hypothesis against the agreement results from prior work, we follow Kamp et al. (2023) for the experimental setup. \n\nOne instance in the dataset corresponds to the concatenation of a premise followed by a hypothesis. The possible output labels are contradiction, entailment, and neutral, making it a multi-class problem. Classes are balanced and indicate the relation between premise and hypothesis. The words in every instance are also annotated as being important or not important towards the output label (3 annotators per instance, 43 important words on average), producing so-called human rationales (Carton et al., 2020).\n\nFrom these human rationales, we derive word-level aggregation scores indicating the proportion of annotators that found the word important. These scores are used to compare attribution scores to human preference when considering a top-selection (see Human in Figures 1, 2, and 3).\n\nAs for the attribution methods, we use both gradient-based approaches by including Vanilla Gradient (Simonyan et al., 2014), Integrated Gradient (Sundararajan et al., 2017), and both versions multiplied with the input (Shrikumar et al., 2017), as well as perturbation-based approaches, by including Partition SHAP (Lundberg and Lee, 2017) and LIME (Ribeiro et al., 2016). Ferret package v0.4.1 (Attanasio et al., 2023)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Preference for a Word Class",
            "text": "The first step in our analysis compares word class preference of different attribution methods on top-tokens. We set the parameter to 4 based on preliminary observations in similar datasets. Figure 2 illustrates the occurrence of different word classes among the tokens with the highest attribution values (i.e. important tokens) for each method and for human aggregated annotations. \n\nWe compare the ratio of important stop words (Figure 2(a)), punctuation tokens (2(b)), and the distribution of the five most preferred POS tags by humans: noun, verb, adj, adp, det (Figure 2(c)). Interestingly, with regards to Integrated Gradient, Gradient × Input, and Integrated Gradient × Input, roughly 10% in each top-4 selection on average consists of punctuation. Despite question answering and natural language inference being different tasks, we replicate the findings on punctuation preference for Integrated Gradient by Ramnath et al. (2020). Notably, these findings do not generalize to the other methods.\n\nIntuitively, this preference seems to be inherent to the method and not to the underlying model, as each instance is normally a concatenation of two sentences tailed by a full stop each. Hence, it is very unlikely that the model is using punctuation as shortcut signals to the output labels. This might suggest that some methods pick up information about the approximate location of a signal in the sentence (locality information), rather than the precise token (lexical information). While punctuation may be a simple symptom of locality, it is important to further examine this phenomenon in the broader context of spans. We do so through a linguistic analysis of spans of locally adjacent tokens, the use of dynamic variables, and their intersection in §4.\n\nStop words, on the other hand, do not display a similar preference as found by Ramnath et al. (2020) (40% versus 10% for Integrated Gradient), indicating that this difference might be task-related. For the other POS tag preferences, we do not observe a clear overlap with prior research for Integrated Gradient (noun: no overlap; verb: overlap; adj: no overlap; adp: cannot compare; det: cannot compare). From Figure 2, we observe the systematic different preference for stop words, punctuation, and most frequent POS tags by Integrated Gradient, Gradient × Input, and Integrated Gradient × Input (Group 1), compared to the other methods and to humans (Group 2). Hence, this intuitively leaves us with two groups displaying different word class preferences.\n\nAssuming that methods (including human rationales) are independent, we apply Chi-Square tests to method–method (and human–method) pairs’ preference distributions. For each pair, we measure whether there is a significant difference between stop word distributions, between punctuation distributions, and between POS tag distributions. The tests confirm our initial observations that most distributions from one group are significantly different from the other group (25/36 pairs) and that no significant differences are found within groups. Most of the exceptions arise for pairs involving Integrated Gradient × Input, with 3 out of 3 non-significant differences found in combination with Partition SHAP, 2 out of 3 with LIME, and 1 out of 3 with human rationales. Hence, Integrated Gradient × Input explains half of the non-significant differences found and can roughly be placed in between the two groups. Additionally, punctuation preferences account for half of the non-significant differences between groups. This might be due to the small numbers of the punctuation frequencies, which may have affected the Chi-Square statistics.\n\nPrimarily Integrated Gradient and Gradient × Input, followed by Integrated Gradient × Input, are indeed the methods for which agreement levels are lowest. This shows that the high similarity in terms of word class preference for the methods in Group 1 results in consistently lower agreement. Simultaneously, the similar preference for methods in Group 2, which happens to be close to human preference, correlates with higher agreement. From the opposite perspective: methods that are similar in terms of agreement scores exhibit similar word class preferences."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Span Definition",
            "text": "We obtain syntactic spans by shallow parsing the data with Flair chunker (Akbik et al., 2018), similarly to Zhou et al. (2020) who use parsed constituents as pre-processed spans for a parsing experiment. Chunking is commonly adopted in Named Entity Recognition where usually noun phrases or verb phrases are the focus of interest (Taufiq et al., 2023). For our task, the advantage of this method over full constituency parsing (Kitaev et al., 2019, e.g.) or dependency parsing (Chen and Manning, 2014, e.g.) is that the chunker output of discrete non-overlapping units facilitates direct alignment with attribution values. Punctuation tokens are ignored by the parser; we treat them as separate spans. Sikdar et al. (2021) use constituency parsing (Mrini et al., 2020) as a basis for hierarchically attributing feature importance scores from tokens to phrases (including any subphrases). However, different methods can have different word class preferences (e.g. a noun modifier may systematically be attributed more importance over its head) and it is therefore questionable whether score aggregation of any kind is a sensible approach. Having clearly defined, non-overlapping phrases is instead crucial to our initial hypothesis. In our dataset, each sentence contains on average 24.4 tokens (6–73), which are grouped into 15.3 spans (3–45). The average ratio of spans over tokens is 0.63 (0.23–1.0). A targeted span is a span that contains at least one token included in the top- selection by the attribution method. During agreement evaluation we treat spans as atomic units, meaning that a span is assigned 1 if targeted, otherwise 0 (similarly to tokens in top- selection). For a fixed set to 4, the average number of targeted spans in a sentence is slightly lower: Partition SHAP 3.5, LIME 3.6, Vanilla Grad 3.5, Grad × Input 3.6, Integrated Gradient 3.7, Integrated Gradient × Input 3.5, Human 3.3. The average over methods is 3.5."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Head vs. Modifier Preference",
            "text": "We have seen that Gradient × Input and Vanilla Gradient exhibit complementary linguistic preferences for noun tokens (the lowest versus highest ratio of noun tokens in the top-4). We zoom in on this phenomenon and investigate the attribution patterns in noun phrases (NPs), focusing on methods that select the head over its modifier and vice versa. \n\nWe examine a subset of noun phrase spans grouped by Gradient × Input and Vanilla Gradient. The NPs must span a minimum of two tokens to make the preference analysis for different word classes possible. To add some consensus stability to this subset, the spans under question should also be targeted by highly agreeing methods Partition SHAP and LIME. We compare the attribution profiles of Gradient × Input and Vanilla Gradient on the token and the span level for the specific [det, noun] construction, the most prevalent among length-2 noun phrases (73%, 1,963). Interestingly, of the cases where Vanilla Gradient targeted the noun (99%, 1,951), Gradient × Input targeted the det half of the times (899). This example clearly illustrates how methods do not only target different word classes in absolute terms, but also how that translates to systematic, alternating differences within syntactic spans.\n\nFurthermore, the ratio of targeted tokens in the [det, noun] NPs is comparable: 57% for Vanilla Gradient versus 60% for Gradient × Input. This detail strengthens the claim of systematic preference in that the det–noun alternation, i.a., is usually exclusive. In other words, it is uncommon for the two described methods to target both tokens from the NPs. This increases the prominence of the preference phenomenon in cases where one selects the det and the other the noun."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Agreement at the Span Level",
            "text": "We showed that different methods have different word class preferences and that the preference can be strong in the case of syntactic noun phrases. A consistently strong preference by two methods leads to a strong disagreement at the token level. The expectation that methods should agree on the token level might therefore be too strict. Given these insights, we measure method–method and human–method agreement at the span level, expecting a relative improvement compared to token-level agreement."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Setup",
            "text": "The dataset, model configurations, and pool of attribution methods that we use are identical to those described in the linguistic analysis (§3  ###reference_###). In addition, we adopt the definition for spans given in §3.3  ###reference_###. Our data therefore has a version where the instances are divided into tokens and one where instances are split into spans. The details of dynamic correspond to those described in §2.3  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   The Effect of Dynamic  on Spans",
            "text": "We compare the effect of dynamic masking on the span level versus dynamic masking on the token level. We measure the effect as the increase in agreement i) versus a baseline to assess overall difficulty of the task and ii) versus fixed masking to assess the ability of the dynamic approach to detect important spans. We expect dynamic masking to be better suited than fixed masking to identify linguistic spans that the model considers important in the instance. Specifically, the local importance setting (in combination with global importance) appears to work as a pooling operator, highlighting the distinct important parts of the instance rather than few concentrated parts. We assume here that for the specific NLI task, certain parts of the input should be considered important.\n\nAgreement is measured as follows. We denote an attribution method as M. M assigns an attribution profile a to the input sequence of tokens {t1, t2, ..., tn} so that each ai indicates the importance of token ti towards the inferred class. The subset of m tokens with the highest attribution values are formalized as B_@m. We compare m attribution methods in pairs by calculating sentence-level agreement@k. Agreement@k is based on the relevance of each token. Relevance for a token ti is equal to the ratio of methods that include the token in their respective B_@m subsets. Agreement@k ignores perfect agreement on non-important tokens (where relevance = 0) in order not to inflate the score. For our experiments, we report mean agreement@k, the averaged agreement over instances in the dataset D.\n\nThe average pair-wise agreement (all method–method combinations) for dynamic masking is 0.61 on the token level and 0.69 on the span level. 0.5 indicates perfect disagreement and 1.0 perfect agreement. While agreement seems relatively low, it might still suggest that consistently the same, few types of signals are identified by a pair of methods.\n\nWe compute a baseline to measure how likely methods are to agree on the token and span level with a pseudo-random attribution method. In other words, we measure task difficulty of making two vectors with a subset of important tokens and spans to agree given a low value of m. For fixed masks, 16% of the tokens in a sentence would be highlighted on average; consequently, 23% of the spans would be highlighted on average. For the token-level baseline, we then randomly shuffle two binary vectors of 100 elements, 16 of which are 1s, and compute pairwise agreement. We repeat the process n times. For the span-level baseline we adopt the same procedure, with the exception that the 1s in the vector are 23. The resulting baselines are 0.54 and 0.57, respectively, indicating that agreeing on tokens and spans is similarly difficult at low values of m. We thus observe the token-wise baseline for fixed masks being outperformed by 0.07 (0.54 to 0.61), whereas the span-wise baseline is outperformed by a relatively larger increase of 0.12 (0.57 to 0.69).\n\nThe comparison results between span agreement on fixed masks versus dynamic masking are given in Figure 3. While dynamic masking provided marginal boosts (+0.00, +0.01, +0.02 compared to fixed masks) on the token level for Gradient × Input and Integrated Gradient, it proves to have a larger positive effect on the span level. Specifically, the span agreement for method–method pairs that include Gradient × Input and/or Integrated Gradient remains constant or increases (changes from +0.00 to +0.07 compared to fixed masks). At the same time, other method–method and human–method agreement scores remain constant or marginally decrease (+0.01, +0.00, -0.01).\n\nWith regards to the largest difference observed between dynamic and fixed masks, namely Integrated Gradient versus Gradient × Input, this can also be explained through the concentration levels of targeted tokens within spans. In fact, dynamic masking scatters the important tokens so that more spans are targeted compared to selecting an average fixed mask. While fixed masks yield 3.7 and 3.6 spans on average for the two methods, dynamic masking yields 6.9 and 6.5. Since it becomes easier for methods to agree when more tokens (and therefore more spans) are targeted, we investigate the settings of dynamic masking further."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Adjusting Dynamic",
            "text": "How can we validate or improve the dynamic algorithm? A solid global importance threshold should meet two conditions: i) resulting values of should be low, preferably close to human preference average of 43; ii) they should outperform a baseline. We explore multiple thresholds: different combinations of , typical distances from the mean in a distribution; the median, which is more robust to outliers than . The thresholds are calculated for (a) all scores and (b) positive scores. Thresholds for positive scores should ignore attributions with negative importance towards the inferred class. These are common in methods such as Integrated Gradient × Input. The influence of negative values and peaks in the attribution profiles is not accounted for by the current threshold set at .\n\nWe find that for different thresholds, resulting s are comparable across methods, which might indicate that the attribution profiles have overall similar distributions. The three thresholds that yield closest s to human preference are , , and median . Closeness corresponds to the averaged Euclidean distance between the meanstdev pairs and human preference of 43, for each threshold column. Among these three, had already proven to keep low and close to ground truth average (Kamp et al., 2023).\n\nEven if the estimated s by the three candidate thresholds are relatively low, it could be, for example, that a method-specific is too high, positively biasing the agreement score. A high would even give high agreement for a pseudo-random attribution profile, which should not be possible if the threshold is properly set. Hence, we compare each method’s agreement scores with other methods to the method’s agreement with a baseline. This gives us an indication of how well a specific threshold works with different attribution profiles. We do this both on the token level and on the span level. The baseline method operates pseudo-randomly by assigning attribution scores to the tokens without knowledge about token importance. For each method, we randomly shuffle the scores in each attribution profile. Each method has its own baseline so that the different distributional properties of the attribution profiles are preserved. We then compute agreement@dynamic- between original and shuffled attribution profiles, which are consequently averaged over the dataset. If the threshold for -estimation is strong, the agreement with the baseline for each method should be lower than the agreement with other methods.\n\nWe find that for , Integrated Gradient and Gradient × Input have higher baseline agreement than the other methods. This can be explained by the higher values of for this threshold (i.e. 6.83 and 7.30). Importantly, both methods have method–method agreement scores that do not beat the baseline (which pseudo-randomly selects tokens), neither on the token level nor on the span level. With regards to median , multiple methods do not beat their baselines either. The threshold instead does, for all methods and both on tokens and on spans. This is an indication of the fact that the latter might be a better threshold than for dynamic estimation. An additional interpretation of why works better than is that negative local maxima in the attribution profiles are hereby ignored, leading to less but more important tokens (and spans) to be targeted. This baseline testing also shows that Gradient × Input and Integrated Gradient are unreliable methods: they have low agreement with other methods and often fail to beat a random baseline."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Discussion",
            "text": "Analysing disagreement from a linguistic perspective helps us to better understand the differences between attribution methods. We briefly discuss the implications of token- and span-level analyses on other tasks than NLI. With an eye on the ability and reliability of these methods to reflect the model’s decision process, we also consider the implications for the faithfulness aspect in interpretability research.\nGenerally speaking, an NLI task is sufficiently challenging that it avoids sentences of different classes (e.g. contradiction, entailment) differing by exactly one word. It is therefore fair to expect methods to target the same span and not to penalise them for disagreeing on the token level. However, targeting a modifier instead of its syntactic head can make a big difference for other tasks. Additionally, the span-token ratio should determine the difficulty of assessing span-level agreement compared to tokens.\nThe choice of considering spans rather than tokens should therefore be weighted against the type of task and data.\nOn a similar note, §3.2  ###reference_### describes the systematic differences in punctuation preferences. We may hypothesise that methods that consistently include full stops in their top- are actually catching the signal’s onset (locality information) rather than the full stop being itself a signal (lexical information). To this end, our choice of treating punctuation as separate spans might have influenced the span agreement of such methods. More research is necessary to disentangle locality from lexical information.\nAgreement is linked with both plausibility and faithfulness. We considered plausibility when estimating dynamic  thresholds, as we aimed for s close to human preference. However, a more direct way of testing for plausibility in this context is by assessing human–method agreement, which we mostly left out of scope in this study. To that end, we did find that agreement results are constant on both tokens and spans, possibly suggesting that human–method agreement reaches a ceiling already at the token level (i.e. tokens are targeted that belong to different signals in the sentence). This interpretation might even hold for more faithful methods. In fact, models do often not rely on the same patterns as humans do, instead resorting to shortcut signals.\nMeasuring faithfulness, on the other hand, is less straightforward. Following Jain and Wallace (2019  ###reference_b15###), who state that faithful attention-based explanations should be agreeable, we carefully extend their perspective in that agreement between method-generic explanations can be considered as a proxy for faithfulness. According to the principle of reproducibility in science (Popper, 2005  ###reference_b27###), a finding that is confirmed through different means is, in principle, more likely to be correct. As such, if two attribution methods with distinct means yield similar results, they are likely similarly (un)faithful. If one method disagrees with the majority of the batch, either the one, the majority, or all are unfaithful. Because of the reproducibility principle, however, it is more likely that the majority is more faithful.\nIn this light, we could therefore speculate that Gradient × Input and Integrated Gradient were two of the less faithful methods in our study, an argument that is supported by their scarce agreement compared to a pseudo-random baseline. Given that some methods might highly correlate with other methods by design, one must be careful at drawing conclusions. Constructing a batch of methods that is representative of different ways of interpreting the model is, for this reason, not a simple task."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion and Future Directions",
            "text": "In this study, we approached post-hoc explanation disagreement from a syntactic perspective. We found that methods that agree most with other methods and with aggregated scores of human rationales have similar POS tag preferences for the targeted tokens. We then determined that attribution methods agree more at the span level than at the token level, which appear to be similarly difficult tasks at low values of . One particular reason for disagreement is the consistent preference by one method to target the determiners instead of the noun head within the same noun phrase. We showed that dynamic  works well in combination with spans, as it seeks for non-neighboring important signals in the sentence. Finally, we empirically tested for different thresholds of the global importance setting of dynamic , suggesting a value () that accounts for both negative attribution scores and results in low s.\n\nOne issue that dynamic  aims to tackle is the targeting of redundant tokens as signals in the same span. To complement this, a more in-depth analysis would provide a better understanding about the way that different methods concentrate their targeted tokens in the same spans. Intuitively, for a fixed , some methods highlight tokens that are more sparse across the instance, whereas other more quickly concentrate targeted tokens within the same spans. To obtain such a concentration metric, one could measure how rapidly a set of tokens belonging to the most important ground truth span are being targeted, at increasing values of .\n\nFuture directions of research include the exploration of different local importance criteria in the dynamic  algorithm, such as different windows (current 1 versus 2, 3). Another is to exploit (syntactic) span-based information to improve interpretability accuracy at the token level, or to improve explanation aggregation techniques. Finally, we advise future evaluation datasets based on multiple annotators’ rationales to preserve specific instance–annotator mappings in the metadata. This would facilitate new directions in assessing the plausibility of attribution methods, specifically how variations in human subjectivity relate to agreement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Ethical Considerations",
            "text": "We would like to reiterate that attribution scores cannot be blindly relied upon to precisely determine model functioning, as they can be influenced by experimental factors such as task and model performance. To avoid drawing generalised conclusions, it is advisable to employ multiple metrics when studying feature attribution."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Acknowledgements",
            "text": "Jonathan Kamp’s research was funded by the Dutch National Science Organisation (NWO) through the project InDeep: Interpreting Deep Learning Models for Text and Sound (NWA.1292.19.399). Antske Fokkens was supported by the EU Horizon 2020 project InTaVia: In/Tangible European Heritage - Visual Analysis, Curation and Communication (http://intavia.eu) under grant agreement No. 101004825. Lisa Beinborn’s work was funded by the Dutch National Science Organisation (NWO) through the VENI program (Vl.Veni.211C.039). We would like to thank the anonymous reviewers for their valuable contribution."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Bibliographical References",
            "text": ""
        }
    ]
}