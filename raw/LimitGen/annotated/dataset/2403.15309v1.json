{
    "title": "Controlled Training Data Generation with Diffusion Models",
    "abstract": "In this work, we present a method to control a text-to-image generative model to produce training data specifically “useful” for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The quality of data plays a crucial role in training generalizable deep learning models. For a model to generalize well, its training data should be representative of the test distribution where it will be deployed. However, real world test conditions change over time, while training datasets are typically collected once and remain static due to high collection costs. We, therefore, focus on generating datasets that can adapt to novel test distributions and are more cost-efficient. Diffusion generative models are trained on large-scale collections of images and exhibit remarkable generalization abilities by being able to produce realistic images not seen during training. Additionally, unlike static datasets that they are trained on, these generative models allow us to adapt the generation process to produce images that follow a certain conditioning. For example, they can be conditioned on textual prompts or geometric information such as depth maps. Recent works explore the use of diffusion models to generate training data for supervised learning with promising results. They guide the generation process using text prompts to accomplish two goals: produce aligned image-label pairs for supervised training and adapt the generated images to a certain target distribution. These proposed methods, however, find conditioning text prompts in an open-loop way by either using a language model or heuristics. Therefore, they lack an automatic feedback mechanism that can refine the found text prompts to produce more curated and useful training data. Furthermore, it has been argued that being able to control the input data is a key contributor to how children are able to learn with few examples. In this work, we propose two feedback mechanisms to find prompts for generating useful training data. The first mechanism finds prompts that result in generations that maximize the loss of a particular supervised model, thus, reflecting its failure modes. We call them Adversarial Prompts (AP). This mechanism ensures that we find not only novel prompts, which may produce images that the model already performs well on, but adversarial prompts that produce images with high loss, and, thus, useful for improving the model (see exemplar generations for AP in Figs. 1, 3 and 5). A given model can perform poorly on multiple distribution shifts, and traversing all of them with adversarial optimization to adapt it to a specific target distribution can be inefficient (e.g., see the difference between AP generations and the illustrated target distributions in Fig. 1-right). Therefore, we introduce an additional target-informed feedback mechanism that finds prompts that generate images similar to those from the target distribution we want to adapt to. To implement this, we assume either access to a textual description of the target distribution or a few sampled (unlabeled) images from it. We then optimize a similarity metric between CLIP embeddings of the generated examples and the target description. We call these prompts Guided Adversarial Prompts (GAP). Compare the columns Adversarial Prompts and Guided Adversarial Prompts in Figs. 1, 3, 4 and 5 to see the effect of CLIP guidance in steering the generations towards a specific target distribution. We demonstrate the effectiveness of our method on different tasks (image classification, depth estimation), datasets with distribution shifts (iWildCam, Common Corruptions, 3D Common Corruptions), and architectures (convolutional and transformer) with supportive results."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Open-loop data generation methods use pre-defined controls to guide the generative process and produce novel training examples.\nOne line of work uses GANs [35  ###reference_b35###, 8  ###reference_b8###, 59  ###reference_b59###] and pre-define a perturbation in their latent space to generate novel examples.\nMore recent works adopt text-to-image diffusion models and use pre-defined prompt templates [68  ###reference_b68###, 77  ###reference_b77###, 27  ###reference_b27###] or use a language model to generative variations of a given prompt [77  ###reference_b77###].\nThese methods require anticipating the kind of data that will be seen at test-time when defining the prompts. On the other hand, our CLIP guidance mechanism allows us to generate images similar to the target distribution.\n[18  ###reference_b18###] also approach this problem by using a captioning and language model to summarize a target distribution shift into a text prompt. However, this summarization process is not informed of the generations, and, thus, does not guarantee that the text prompt will guide the generation process to images related to the target distribution.\nFinally, these methods are not model-informed and do not necessarily generate images useful for training a given model.\nClosed-loop data generation methods guide the generation process via an automatic feedback mechanism.\nThey control the latent space of GANs [5  ###reference_b5###] or VAEs [76  ###reference_b76###] models, NeRF [15  ###reference_b15###], or the space of hand-crafted augmentations [10  ###reference_b10###] to generate data that maximizes the loss of the network on the generated data.\nSimilarly, [36  ###reference_b36###] uses an SVM to identify the failure modes of a given model and uses this information to generate training data with a diffusion model.\nOur method employs a similar adversarial formulation (in conjunction with target distribution guidance) but performs the optimization in the text prompt space of recently developed diffusion models.\n“Shallow” data augmentation techniques apply simple hand-crafted transformations to training images to increase data diversity and improve the model’s generalization.\nExamples of such transformations are color jitter, random crop, and flipping, etc.\nTo produce more diverse augmentations, methods like RandAugment [11  ###reference_b11###] and AugMix [30  ###reference_b30###] combine multiple of such simple transformations, and Mixup [81  ###reference_b81###] and CutMix [78  ###reference_b78###] methods use transformations that can combine multiple images.\nAutoAugment [10  ###reference_b10###] and adversarial training [49  ###reference_b49###] build a closed system to tune the parameters of the applied augmentations but are inherently limited by the expressiveness of the simple transformations.\nIn contrast, our method uses expressive diffusion models, which results in images that are more diverse and realistic than those produced by “shallow” augmentations.\nControlling diffusion models. Methods like ControlNet [82  ###reference_b82###] and T2I-Adapter [54  ###reference_b54###] adapt a pre-trained diffusion model to allow for additional conditioning e.g., edge, segmentation, and depth maps. We employ these models for generation as it allows us to generate paired data for different tasks, given the labels from an existing dataset.\nEditing methods aim to modify a given image, either via the prompt [32  ###reference_b32###], masks [9  ###reference_b9###], instructions [7  ###reference_b7###] or inversion of the latent space [53  ###reference_b53###, 34  ###reference_b34###]. In contrast, personalization methods aim to adapt diffusion models to a given concept e.g., an object, individual, or style. Popular examples include textual inversion [22  ###reference_b22###] and DreamBooth [64  ###reference_b64###], which aim to find a token to represent a concept given several images of that concept. The former freezes the diffusion model, while the latter fine-tunes it. Extensions of these works learn to represent multiple concepts [1  ###reference_b1###, 24  ###reference_b24###].\nIn our work, we adopt an approach similar to textual inversion to steer the diffusion model, but our method can also be used with other controlling mechanisms."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We begin this section by formalizing our problem setting and describing how diffusion models can be used to generate training data (Sec. 3.1  ###reference_###).\nWe then introduce two feedback mechanisms to find prompts that are informed of the failure modes of a given model (Sec. 3.2  ###reference_###) and relevant to a given target distribution (Sec. 3.3  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "Problem Formulation.\nWe consider the problem of supervised learning, where a model  learns a mapping from the image space , to a target space , e.g., a depth estimation or semantic classification problem.\nThe model  is trained using training dataset  and tested on a new set  that exhibits a distribution shift w.r.t. the training data.\nOur goal is to generate additional synthetic training data  to adapt the model and improve its performance under the distribution shift.\nTo apply the target-informed feedback mechanism described in Sec. 3.3  ###reference_###, we assume access to some information about the test distribution, either from text descriptions or a few samples of unlabeled images.\nText-to-image Diffusion Models.\nWe use the Stable Diffusion [62  ###reference_b62###] text-to-image diffusion model as the basis for our generator .\nGiven a textual prompt , Stable Diffusion is capable of synthesizing realistic images following the textual conditioning.\nHowever, in general, for a given task, e.g., depth estimation, a textual prompt alone may not be sufficient for controlling the generation well enough to produce aligned image-label examples.\nGenerating aligned training examples.\nWe employ the following two approaches to condition the generative model  on the label  and sample aligned training examples .\nFor the depth estimation task, we use the ControlNet [83  ###reference_b83###] model which extends the conditioning mechanisms of the Stable Diffusion to accept various spatial modalities, e.g., depth maps, segmentation masks, or edges.\nSpecifically, we use ControlNet v1.0 with depth conditioning111https://github.com/lllyasviel/ControlNet  ###reference_###.\nFor semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###] that preserves the masked region throughout the denoising process, essentially keeping it intact.\nThese mechanisms provide us with a generative model conditioned both on a text prompt  and label .\nWe denote the resulting distribution modeled by this generative model as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Model-Informed Generation with Adversarial Prompt Optimization",
            "text": "Our first feedback mechanism aims at generating training examples that reflect the failure modes of a given model .\nAn automatic way to do so is via adversarial optimization, which finds the “worst case” failure modes of .\nMore precisely, we find a text prompt  that generates images  that maximize the supervised loss , e.g.,  loss for depth estimation.\nSince the usual prompt space is discrete (text tokens) and challenging to optimize over, we employ the approach introduced in Textual Inversion [22  ###reference_b22###] and instead optimize over the corresponding continuous embedding space. For ease of notation, “prompt space” will implicitly refer to the continuous embedding space instead of the discrete token space. We construct a prompt  out of  new “placeholder” tokens, i.e., , and find their corresponding embedding weights  by solving the following optimization problem:\nwhere  and  is sampled from .\nNote that the sample  is differentiable w.r.t. the embeddings  which allows us to use gradient-based optimization. We call the prompts that result from solving the above optimization problem Adversarial Prompts (AP).\nAvoiding  alignment collapse.\n\nThe adversarial objective in Eq. 1  ###reference_### aims to fool the model . However, it may instead fool the label-conditioning mechanism of the generative model , resulting in  generating samples  that are not faithful to  (see Fig. 2  ###reference_###).\n###figure_1### To avoid this, we further constrain the expressiveness of the generation process. There are several ways to do so.\nOne way is to use the SDEdit method [50  ###reference_b50###], which conditions the generation process on the original image by starting the denoising process from a noised version of  instead of pure noise. Thus, it constrains the expressive power of the generative model to produce samples closer to the original image .\nAdditionally, some constraints can be implemented w.r.t . For the depth estimation task, we employ an early stopping criterion and stop the adversarial optimization when the loss reaches a certain task-specific threshold. For semantic classification, choosing  to be the negative cross-entropy loss, although natural, may not be a good choice. Indeed, for iWildCam, although we keep the class mask intact, we observed that optimizing the negative cross-entropy loss may lead to the generation of another class somewhere else in the image, e.g. an elephant is generated next to a giraffe, destroying the  alignment. Thus, for iWildCam, we choose to maximize the uncertainty or entropy of the model’s prediction on the generated images. We provide more details in the Appendix Sec. 7.5.2  ###reference_.SSS2###.\nFinally, our CLIP [57  ###reference_b57###] guidance loss introduced in Sec. 3.3  ###reference_### further constrains possible perturbations to a target distribution and helps to avoid the generation of non-realistic images."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Target Distribution Informed Generation",
            "text": "The adversarial formulation above finds prompts that reflect the failure modes of .\nWithout any information about the target distribution, improving the model on the worst-performing distributions is one of the best strategies one can do and, indeed, improves performance in some cases (see Fig. 4  ###reference_### and Fig. 5(a)  ###reference_sf1###).\nHowever, there are typically multiple failure modes of a given model and many possible distribution shifts that can occur at test-time.\nAdapting to all of them using only the first feedback mechanism could be inefficient when the goal is to adapt to a specific target distribution instead of improving the performance on average.\nThus, we introduce the second feedback mechanism to inform the prompt optimization process of the target distribution.\nThis only requires access to simple text descriptions (e.g., ‘fog’ to adapt to foggy images) or a small number () of unlabelled images.\nWe implement the target-informed feedback mechanism using CLIP [57  ###reference_b57###] guidance.\nSpecifically, we assume access to either textual descriptions of the target image distribution , a few unlabeled image samples  or both.\nWe then construct the corresponding text and image guidance embeddings as  and , where  and  denote, respectively, the CLIP text and image encoders, and  stand for averaging.\nWe then use the following guidance loss:\nwhere we take  to be  norm between two embeddings and  to be the negative cosine similarity, as we found it to perform the best. See the Appendix Sec. 7.7  ###reference_### for the results of this ablation.\nNote, that based on the available information, one can also use only one of the two guidance losses.\nFinally, we combine both adversarial, Eq. 1  ###reference_###, and CLIP guidance, Eq. 2  ###reference_###, losses to form the final objective:\nWe call the prompts that result from solving Eq. 3  ###reference_###, Guided Adversarial Prompts\n(GAP).\nSee the Appendix Secs. 7.4.2  ###reference_.SSS2### and 7.5.3  ###reference_.SSS3### for further implementation details.\n###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We perform experiments in two settings: domain generalization via camera trap animal classification on the iWildCam dataset and depth estimation with the Taskonomy dataset. For depth estimation, the considered distribution shifts are Common Corruptions (CC), 3D Common Corruptions (3DCC) applied on the Taskonomy test set and cross dataset shift from the Replica dataset."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Semantic Classification",
            "text": "iWildCam [3, 42] is a domain generalization dataset, made up of a large-scale collection of images captured from camera traps placed in various locations around the world. We seek to learn a model that generalizes to photos taken from new camera deployments. We follow [18] and subsample the dataset to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik), with 2 test locations that are not in the training or validation set. We also fixed the number of additional generated images for finetuning to 2224 images in order to match the setting of [18].\n\nFor each dataset, we compare the following methods (we provide more details in Appendix Sec. 7):\n\nNo Extra Data: We train a ResNet50 [26] model using the original training data without using any extra data.\nAugmentation baselines: We compare to two data augmentation baselines taken from recent literature: CutMix [78] and RandAugment [12].\nAgnostic Prompts: We use a prompt that is not informed of the model or the target distribution. Similar to ALIA [18], we use a prompt template “a camera trap photo of {class name}”.\nGuided Prompts (ALIA [18]): This approach uses a captioning and language model to summarize a target distribution shift into text prompts. Specifically, we use the prompts found by the ALIA method. This results in four prompts for iWildCam. See Appendix Sec. 7.2 for more details and a discussion on the differences between ALIA and our method.\nAdversarial Prompts: We use the model trained without extra data as the target model and find adversarial prompts following Eq. 1. We find four prompts in total applied to all classes for iWildCam.\nGuided Adversarial Prompts: We use the same setting as in Adversarial Prompts and apply additional CLIP guidance to adapt to a target distribution shift, following Eq. 3. For iWildCam, we use image guidance and partition the target test distribution into four groups based on two attributes that have significant impact on the visual characteristics of the data; the test location (first or second) and time of the day (day or night). We sample 64 unlabeled images randomly from each group. We optimize for one guided adversarial prompt per group.\n\nTraining details.\n\nFor both datasets, we perform adversarial optimization with constant learning rate of 1e-3 using Adam [38]. We use the DDIM [72] scheduler. See Tab. 1 for a summary of the experimental parameters. As mentioned in Sec. 3.2, we optimize the entropy loss for iWildCam. More precisely, this loss is equal to the cross-entropy loss where the target label is replaced by the soft label, the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space. For more details, see the Appendix.\n\niWildCam results.\n\nFig. 4 shows that having model-informed feedback (Adversarial Prompts) helps to generate more useful data than no feedback mechanism (Agnostic Prompts) and also improves the performance of the target-only informed Guided Prompts method in the low-data regime. Guided Adversarial Prompts combines the benefits of both model- and target-informed feedback mechanisms, consistently outperforming other methods. Fig. 4 shows exemplar generations for each method. We find that while AP generates images distinct from the target distribution and images generated by target-informed methods (snow background vs. grass background), training a model using these examples in the low-data regime performs better than GP and similar to GAP."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Depth Estimation",
            "text": "For depth estimation, we consider the following pre-trained models as : 1) a U-Net [63  ###reference_b63###] model trained on the Taskonomy dataset [80  ###reference_b80###, 79  ###reference_b79###] and 2) a dense prediction transformer (DPT) [58  ###reference_b58###] model trained on Omnidata [20  ###reference_b20###].\n\nWe compare the following methods. They all involve fine-tuning, but on different datasets. We use ControlNet v1.0 with depth conditioning for the experiments in this section. See Fig. 5  ###reference_### for a comparison of the generations:\n\nControl (No extra data): We fine-tune on the original training data. This baseline is to ensure that the difference in performance is due to the generated data, rather than e.g., longer training or optimization hyperparameters.\n\nAgnostic Prompts: This baseline generates data that is agnostic to the model or the target distribution. We generate images with the prompt “room” as the datasets consist of indoor images from mostly residential buildings.\n\nAgnostic Prompts (Random): We generate data with “random” prompts. In our proposed method, we optimize for embedding vectors, resulting in a prompt. Thus, to match this setting, from a Gaussian distribution fitted on the embeddings from the vocabulary, we sample random embeddings to create a random prompt to be used in the data generation.\n\nAdversarial Prompts: We perform the optimization as described in Eq. 1 ###reference_### and fine-tune on this data.\n\nGuided Prompts: We perform the optimization using only the loss described in Eq. 2 ###reference_### and fine-tune on this data.\n\nGuided Adversarial Prompts: In addition to optimizing the adversarial loss, we also optimize the CLIP guidance loss as described in Eq. 3 ###reference_###. This allows us to generate data that is also informed of a certain distribution shift.\n\nTraining details.\n\nThe adversarial optimization was done with AdamW [46 ###reference_b46###], learning rate of , weight decay of , and batch size of 8. We set the early stopping threshold (mentioned in Sec. 3.2 ###reference_###) to 0.08 for the UNet model and 1.0 for the DPT model. They were trained with and Midas loss [20 ###reference_b20###] respectively.\n\nWe perform a total of 30 runs to get different Adversarial Prompts. We use the DDIM [72 ###reference_b72###] scheduler. During optimization, we use only 5 denoising steps, as it is more stable.\n\nFor Guided Adversarial Optimization, the guidance coefficient for text and image guidance is 1 and 5 respectively.\n\nFor fine-tuning, we generate images with 15 steps. See the Appendix Sec. 8.1 ###reference_### for further details.\n\n###figure_4### U-Net DPT\n\nTaskonomy Replica Taskonomy\n\nShift Clean CC 3DCC CDS CC 3DCC\n\nControl (No extra data) 2.35 4.93 4.79 5.38 3.76 3.42\n\nAgnostic Prompts 2.47 5.03 4.17 5.30 4.06 3.58\n\nAgnostic Prompts (Random) 2.38 4.96 4.11 5.14 3.88 3.51\n\nAdversarial Prompts 2.49 4.36 4.02 5.12 3.40 3.28\n\nAdversarial Prompts (SDEdit) 2.59 4.20 3.88 4.96 3.35 3.25\n\n###figure_5### Comparing the generated images with different prompts.\n\nFig. 5 ###reference_###-left shows the results of the generations for the baselines and our method, optimized on the Taskonomy dataset. The generation with Agnostic Prompts are visually different from that of the original image, however, they tend to have similar styles. In contrast, the generations with Adversarial Prompts have more complex styles and are more diverse.\n\nUsing SDEdit during the optimization and generation results in generations that are closer to the original image, as it was also used as conditioning.\n\nThe last four columns show the results of using CLIP text guidance for the target distribution shift fog and blur, as described in Sec. 3.3 ###reference_###, with and without adversarial optimization.\n\nThe generations with Guided Prompts involve passing the depth conditioning and prompt “fog” or “blur” to the diffusion model. In both cases, the generations result in a mild level of fog or blur"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Limitations",
            "text": "In this work, we aim to generate training data useful for training a supervised model by steering a text-to-image generative model. We introduced two feedback mechanisms to find prompts that are informed by both the given model and the target distribution. Evaluations on a diverse set of tasks and distribution shifts show the effectiveness of the proposed closed-loop approach in comparison to open-loop ones.\n\nBelow we briefly discuss some of the limitations:\n\nLabel shift: In this work, we focus on generating novel images. However, some distribution shifts can also change the label distribution, e.g., for depth estimation, changing from indoor to outdoor scenes would result in a shift in depth maps. One possible approach could be learning a generative model over the label space to control the generation in both the label and image space.\n\nComputational cost: Estimating the gradient of the loss requires backpropagation through the denoising process of the diffusion model, which can be computationally demanding. Using approaches that reduce the number of denoising steps may be able to reduce this computational cost.\n\nLabel Conditioning: As discussed in our method is limited by the faithfulness of the generation conditioned on the given label. For example, we found that the semantic segmentation ControlNet does not follow the conditioning accurately enough to be useful for the supervised model. Further developments in more robust conditioning mechanisms are needed to successfully apply our method to other tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Outline",
            "text": "We provide further discussions, details, and evaluations in the appendix, as outlined below.\nSecs. 7  ###reference_### and 8  ###reference_### describe additional implementation details for our classification and depth estimation experiments, respectively.\nSec. 7.6  ###reference_### describes an image guidance mechanism using Textual Inversion [22  ###reference_b22###] and compares it with the CLIP guidance mechanism.\nSec. 8.2  ###reference_### provide additional results for “standard” augmentation baselines for the depth estimation experiments.\nSecs. 7.4.3  ###reference_.SSS3###, 7.5.4  ###reference_.SSS4### and 8.3  ###reference_### provide qualitative generations from all the Adversarial Prompts and Guided Adversarial Prompts used in the iWildCam, and depth estimation experiments.\nAdditionally, for depth estimation, we provide a qualitative comparison of Adversarial Prompts generations optimized on different models (UNet [63  ###reference_b63###], DPT [58  ###reference_b58###]). For iWildCam, we also provide additional results using a ViT-B-16 [17  ###reference_b17###] instead of a ResNet50 [26  ###reference_b26###].\nSec. 8.4  ###reference_### provides additional analysis on the depth estimation experiments:\nthe single iteration vs. multi-iteration setting\na comparison of CLIP image and text guidance\nan assessment of the generalization of Adversarial Prompts from one model to another."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Classification",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###, for semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###] that preserves the masked region throughout the denoising process.\nIn this section, we briefly describe this procedure and refer the reader to the original work for more details.\nLet  be a binary pixel mask, where a pixel is equal to 1 if the pixel contains the object and 0 otherwise, and  be the original image from a training dataset.\nDuring generation, after obtaining a denoised sample  at time  we update it as , where  is the original image noised to have the correct properties of the expected Gaussian distribution at time .\nHowever, because we are using Stable Diffusion [61  ###reference_b61###], the denoising process is done in latent space (using an encoder ), not pixel space. This means that to apply inpainting, we must resize the mask  to the latent space dimensions, and apply the above-described procedure in the latent space: , where  and  is its corresponding noised version.\nWhile this procedure usually performs well in preserving the original region of interest, we also paste the original masked region in the pixel space to obtain the final sample .\nIn addition to inpainting, depending on the setting, we also use SDEdit [50  ###reference_b50###], a mechanism available to all diffusion models that allows to use an initial image to condition the generation of new images to be closer to the initial image. The mechanism is parametrized by the SDEdit strength , which indicates the extent by which the model can deviate from the original image.\nFor our diffusion model, we use Stable Diffusion v1.5 222https://huggingface.co/runwayml/stable-diffusion-v1-5.\nWe report our generation parameters in Tab. 3  ###reference_###. We use the DDIM [71  ###reference_b71###] scheduler. We generate 384x384 resolution images. Those parameters were chosen based on visual inspection, ease of optimization and downstream performance (validation accuracy).\n\nTraining data.\nAfter generation, ALIA’s method consists of an additional filtering step to remove “bad” generations. This step relies on using a pretrained model to measure confidence on the generated images. However, given our method creates images that are adversarial to an iWildCam pretrained model, the filtering part of ALIA’s pipeline is not usable on our data. Thus, to keep things comparable, we decided not to apply filtering both our method generated data and ALIA’s generated data. However, it must be noted that [18  ###reference_b18###] only reports a 2% absolute accuracy drop between fitlering and no filtering on iWildCam, thus we do not expect a big difference in performance with ALIA’s reported results and our results.\nWe report our training parameters in Tab. 4  ###reference_###. We use ALIA’s codebase to finetune our models, which ensures fair comparison to the ALIA baselines. For everything except the generated data, the settings are the same as in ALIA. For both datasets, the starting model is a ResNet50 [25  ###reference_b25###] model, pretrained on ImageNet [13  ###reference_b13###].\nThe reported test accuracy is chosen according to the best checkpoint, measured by validation accuracy.\nFor adversarial feedback, we use the model trained only using the original training data  with complete spurious correlation. It is taken from ALIA \nAs the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: , assuming . This is equivalent to the negative cross-entropy loss referred in the text.\nWe find four prompts per each class, i.e., eight prompts in total.\nEach prompt is composed of five new learnable tokens.\nWe perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38  ###reference_b38###].\nWe use five denoising steps during adversarial optimization and generate images for training with 15 steps.\n\nSee Tab. 3  ###reference_### for summary.\n In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target .\nWe use  guidance loss: .\nWe use  and  (i.e., no image guidance).\nWe describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the “entropy” loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don’t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA’s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from “scratch” (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from ”scratch”. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Training data generation",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###  ###reference_###, for semantic classification tasks, we utilize the foreground object masks and use an in-painting technique proposed in [47  ###reference_b47###  ###reference_b47###] that preserves the masked region throughout the denoising process.\nIn this section, we briefly describe this procedure and refer the reader to the original work for more details.\nLet  be a binary pixel mask, where a pixel is equal to 1 if the pixel contains the object and 0 otherwise, and  be the original image from a training dataset.\nDuring generation, after obtaining a denoised sample  at time  we update it as , where  is the original image noised to have the correct properties of the expected Gaussian distribution at time .\nHowever, because we are using Stable Diffusion [61  ###reference_b61###  ###reference_b61###], the denoising process is done in latent space (using an encoder ), not pixel space. This means that to apply inpainting, we must resize the mask  to the latent space dimensions, and apply the above-described procedure in the latent space: , where  and  is its corresponding noised version.\nWhile this procedure usually performs well in preserving the original region of interest, we also paste the original masked region in the pixel space to obtain the final sample .\nIn addition to inpainting, depending on the setting, we also use SDEdit [50  ###reference_b50###  ###reference_b50###], a mechanism available to all diffusion models that allows to use an initial image to condition the generation of new images to be closer to the initial image. The mechanism is parametrized by the SDEdit strength , which indicates the extent by which the model can deviate from the original image.\nFor our diffusion model, we use Stable Diffusion v1.5 222https://huggingface.co/runwayml/stable-diffusion-v1-5."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "ALIA",
            "text": "Here, we give more details on the ALIA [18  ###reference_b18###] (Automated Language-guided Image Augmentation) baseline method, which aims at generating images targeting a particular test distribution similar to our guidance mechanism (main paper Sec. 3.3  ###reference_###).\nGiven exemplar images from the test distribution, ALIA first captions each image using the BLIP [45  ###reference_b45###] captioning model.\nThen, it uses the GPT-4 [56  ###reference_b56###] LLM to summarize these captions into a list of domains asking it to produce descriptions that are agnostic to the class information.\n[18  ###reference_b18###] then use these prompts to generate additional training data.\nIn order to preserve the original class information in their generations, they use SDEdit [50  ###reference_b50###] or Instruct Pix2Pix [7  ###reference_b7###].\nWe refer the original paper for further implementation details.\nBelow, we summarize resulting prompts we use for comparison in our results.\n\nWe, therefore, use the following prompts:\n“in a bamboo forest with a green background.”\n“flying over the water with a city skyline in the background.”\n“perched on a car window.”\n“standing in the snow in a forest.”,\n“standing on a tree stump in the woods.”\n“swimming in a lake with mountains in the background.”,\n“standing on the beach looking up.”\nFor iWildCam [3  ###reference_b3###], we keep the original prompts intact:\n“a camera trap photo of a {class name} in a grassy field with trees and bushes.”\n“a camera trap photo of a {class name} in a forest in the dark.”\n“a camera trap photo of a {class name} near a large body of water in the middle of a field.”\n“a camera trap photo of a {class name} walking on a dirt trail with twigs and branches.”\nThere are two main differences between ALIA and our method:\nThe target distribution feedback. ALIA aligns its prompts with the target distribution by utilizing captioning and summarizing. However, this summarizing process is not informed of the produced generations when using such prompts, and, thus, does not guarantee that the text prompt will accurately guide the generation process to images related to the target distribution.\nModel feedback. ALIA is not model-informed. Thus, it doesn’t necessarily generate images useful for training a given model.\nThose two differences originate from the fact that ALIA is an open-loop method, i.e, it lacks the mechanism to refine the prompt based on the generated images. In contrast, our method uses model and target distribution feedback in a closed-loop. This allows our method to outperform ALIA and be more data-efficient."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "General implementation details",
            "text": "We report our generation parameters in Tab. 3  ###reference_###  ###reference_###. We use the DDIM [71  ###reference_b71###  ###reference_b71###] scheduler. We generate 384x384 resolution images. Those parameters were chosen based on visual inspection, ease of optimization and downstream performance (validation accuracy).\n\nTraining data.\nAfter generation, ALIA’s method consists of an additional filtering step to remove “bad” generations. This step relies on using a pretrained model to measure confidence on the generated images. However, given our method creates images that are adversarial to an iWildCam pretrained model, the filtering part of ALIA’s pipeline is not usable on our data. Thus, to keep things comparable, we decided not to apply filtering both our method generated data and ALIA’s generated data. However, it must be noted that [18  ###reference_b18###  ###reference_b18###] only reports a 2% absolute accuracy drop between fitlering and no filtering on iWildCam, thus we do not expect a big difference in performance with ALIA’s reported results and our results.\nWe report our training parameters in Tab. 4  ###reference_###  ###reference_###. We use ALIA’s codebase to finetune our models, which ensures fair comparison to the ALIA baselines. For everything except the generated data, the settings are the same as in ALIA. For both datasets, the starting model is a ResNet50 [25  ###reference_b25###  ###reference_b25###] model, pretrained on ImageNet [13  ###reference_b13###  ###reference_b13###].\nThe reported test accuracy is chosen according to the best checkpoint, measured by validation accuracy."
        },
        {
            "section_id": "7.4.1",
            "parent_section_id": "7.4",
            "section_name": "7.4.1 Dataset details",
            "text": "Fig. 7  ###reference_### demonstrates the shift between train and test distributions in the dataset [65  ###reference_b65###].\nWe follow the setting suggested in [18  ###reference_b18###] and use 1139 images.\nWe add additional 839 examples either from the original dataset, or generated by Stable Diffusion with prompts obtained by one of the methods.\nFor the data-efficiency plots (e.g., Fig. 3  ###reference_###) we reduced the number of added examples by a factor of {1/2, 1/4, 1/8, 1/16}.\nWe used the SAM [40  ###reference_b40###] segmentation model to obtain bird segmentation masks for training images.\nWe use these masks to condition the generative model on the class by using inpainting as described in Sec. 7.1  ###reference_.SSS0.Px1###."
        },
        {
            "section_id": "7.4.2",
            "parent_section_id": "7.4",
            "section_name": "7.4.2 Implementation Details",
            "text": "For adversarial feedback, we use the model trained only using the original training data  with complete spurious correlation. It is taken from ALIA checkpoints333As the task is the binary classification, we use the cross-entropy loss for the opposite class as the adversarial loss: , assuming . This is equivalent to the negative cross-entropy loss referred in the text.\nWe find four prompts per each class, i.e., eight prompts in total.\nEach prompt is composed of five new learnable tokens.\nWe perform adversarial optimization for 1000 steps with learning rate 1e-3 using Adam [38  ###reference_b38###  ###reference_b38###  ###reference_b38###].\nWe use five denoising steps during adversarial optimization and generate images for training with 15 steps.\nSee Tab. 3  ###reference_###  ###reference_###  ###reference_### for summary.\n In addition, we renormalize the averaged target text embedding to have the norm equal to the mean norm of the original prompts, and use the resulting vector as the target .\nWe use  guidance loss: .\nWe use  and  (i.e., no image guidance)."
        },
        {
            "section_id": "7.4.3",
            "parent_section_id": "7.4",
            "section_name": "7.4.3 Additional Qualitative Results.",
            "text": "In Fig. 8  ###reference_### and Fig. 9  ###reference_###, we show a few generations using all 8 prompts used in the experiments for Guided Adversarial Prompts and Adversarial Prompts, respectively.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "iWildCam",
            "text": "We describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the “entropy” loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don’t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA’s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from “scratch” (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from ”scratch”. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.5.1",
            "parent_section_id": "7.5",
            "section_name": "7.5.1 Dataset details.",
            "text": "The original iWildCam [3  ###reference_b3###] dataset is subsampled to create a 7-way classification task (background, cattle, elephant, impala, zebra, giraffe, dik-dik). The training set has 6,000 images with some classes having as few as 50 images per example. There are 2 test locations that are not in the training or validation set. Additionally, given , the hour at which an image was taken, we define an image to be during “daytime” if , and “nighttime” if . As said in main paper Sec. 4.1  ###reference_###, for image CLIP guidance, we separate the target test locations into four groups (location={1,2}, time={daytime, nighttime}). We provide visualisation of the test locations (at day & night) in Fig. 10  ###reference_###. For more details on the iWildCam subset construction, we refer to [18  ###reference_b18###] Section 8.3. For inpainting, the object masks are obtained from MegaDetector [2  ###reference_b2###].\n###figure_9###"
        },
        {
            "section_id": "7.5.2",
            "parent_section_id": "7.5",
            "section_name": "7.5.2 Alignment collapse solution for iWildCam.",
            "text": "As mentioned in main paper Sec. 3.1  ###reference_###, choosing  to be the negative cross entropy loss, i.e. minimizing the probability that the model predicts , may not be the best choice. Indeed, given we use a random sample of 64 images to create our target embedding for the image CLIP guidance, the likelihood that animals were present on these 64 images is very high. This means that the target embedding, although mostly containing the “location concept”, also partly contains an “animal concept”. This means that the image CLIP guidance does not explicitly forbid the generation of new animals. Combined with optimizing the negative cross entropy loss, this leads to adversarial animal insertions at generation time, where a new animal of class  appears alongside the original animal of class , destroying the  alignment. In Fig. 11  ###reference_###, we provide qualitative examples for this behaviour. To counter this behaviour, we choose  to be the “entropy” loss, or uncertainty loss. More precisely, this loss is equal to the cross entropy loss where the target label  is replaced by the soft label , the uniform distribution over all classes. This loss explicitly encourages generations that either (1) do not contain new animals (2) contain new animals that are not accounted for in the label space .\n###figure_10###"
        },
        {
            "section_id": "7.5.3",
            "parent_section_id": "7.5",
            "section_name": "7.5.3 Implementation details",
            "text": "We describe here the parameters and settings used for optimization. If not precised, the same parameters were used for Adversarial Prompts and Guided Adversarial Prompts. As said in main paper Sec. 4.1  ###reference_###  ###reference_###  ###reference_###, we optimize 4 prompts. Each prompt is composed of 10 placeholder tokens.\nFor optimization, we use a constant learning rate of 0.001, and a batch size of 8.\nWe use the “entropy” loss, described previously. For adversarial prompts, we train for 2000 steps. For guided adversarial prompts, we use CLIP guidance coefficient with  and and  (i.e., no text guidance). We train for a total of 10000 gradient steps. However, we don’t optimize the adversarial loss for the first 2000 steps to allow the prompt to first converge to the target distribution region.\nFor adversarial prompts, to generate 4 different prompts, we simply change the seed. For guided (adversarial) prompts, each prompt is w.r.t a new location & time of the day of the test distribution.\nThe generation settings are the same as the ones used during adversarial optimization. For each target domain-guided adversarial prompt, (i.e. location & time of the day), the source images (used to condition the generation with an object mask and through SDEdit [50  ###reference_b50###  ###reference_b50###  ###reference_b50###]) are only images that match the time of the day of the target domain used during generation. Furthermore, for each prompt, we only generate one image per source image.\nFor ALIA, for each prompt, we generate one image per source image, from the whole training dataset. For the generation settings, given we use a slightly different generation process (inpainting) compared to their original implementation, we search ALIA’s best-performing generation parameters (according to validation accuracy) over SDEDit strength [0.4, 0.5, 0.8] and guidance scale [5.0, 7.5]. We found the best-performing parameters for ALIA to be the same as the one reported by ALIA in their Github444https://github.com/lisadunlap/ALIA i.e. SDEdit strength of 0.5 and guidance of 7.5.\nThe learning rate scheduler is a cosine scheduler, updated every epoch. The batch size is 128.\nOur iWildCam pretrained model is taken from ALIA checkpoints555https://api.wandb.ai/files/clipinvariance/ALIA-iWildCamMini/brr7b3ks/checkpoint/ckpt-iWildCamMini-randaug-filtered-resnet50-0-0.001-0.0001/best.pth. ALIA trains the model from “scratch” (i.e. the model has never seen iWildCam data), for 100 epochs, on the combination of real + generated data. For our method, given we optimize the prompts based on a finetuned model feedback, it may not make as much sense to train the model from ”scratch”. Thus, we also introduce the variant where the iWildCam pretrained model is finetuned on the combination of real + generated data for 20 epochs, where finetuning means that every layer, except the last, is frozen.\nFor a fair comparison, both training settings are tested for ALIA and our method. We found that ALIA worked best when training from scratch and our method worked best when using the finetuning setting.\nFinally, in their iWildCam experiment, ALIA fixed the number of extra generated points to be used in combination with real data during training to 2224 images. For the sake of comparison, we adopt the same limit in our experiments, with the added variant where the limit is 556 images, showcasing the data efficiency of our method."
        },
        {
            "section_id": "7.5.4",
            "parent_section_id": "7.5",
            "section_name": "7.5.4 Additional Qualitative Results.",
            "text": "In Fig. 12  ###reference_### and Fig. 13  ###reference_###, we show a few generations using each of the 4 Guided Adversarial Prompts and Adversarial Prompts used in the iWildCam experiments.\n###figure_11### ###figure_12###"
        },
        {
            "section_id": "7.5.5",
            "parent_section_id": "7.5",
            "section_name": "7.5.5 Using ViT model.",
            "text": "In Fig. 14  ###reference_###, we repeat the iWildCam experiment from the main paper (Fig. 4  ###reference_###) with a ViT-B-16 [16  ###reference_b16###] model. Additionally, we provide qualitative results for generations from adversarial prompts optimized with a ViT-B-16 model in Fig. 15  ###reference_###.\n###figure_13### ###figure_14###"
        },
        {
            "section_id": "7.6",
            "parent_section_id": "7",
            "section_name": "Image Guidance using Textual Inversion",
            "text": "In addition to the CLIP image guidance introduced in main paper Sec. 3.3  ###reference_###, we also explore using Textual Inversion (TI) [22  ###reference_b22###] as an image guidance mechanism.\nSimilar to the CLIP guidance, we use a few images  from the target distribution.\nNow, instead of the similarity in a CLIP embedding space, we use the denoising loss between a generated image and one of the target images as in [22  ###reference_b22###] (see Eq. (2)):\nwhere  is the VAE [39  ###reference_b39###] image encoder and  is the denoising UNet model from the Stable Diffusion model [61  ###reference_b61###], and  is sampled randomly from the set of available target images.\n\nWe use the guidance loss from Eq. 4  ###reference_### with the weight 1000 (we found lower values to result in generations less faithful to the target images) and randomly sample 50 (unlabeled) images from the validation split of the original dataset where both types of birds appear on both types of backgrounds.\nWe keep other settings the same as for GAP and AP.\n###figure_15### \nWe found, however, that the TI guidance does not result in faithful generations for iWildsCam dataset, and further investigations are needed."
        },
        {
            "section_id": "7.7",
            "parent_section_id": "7",
            "section_name": "Additional Analysis",
            "text": "We ablate hyperparameters like 1) using  or cosine loss for , 2) different ways of incorporating guidance e.g., text or image with CLIP or textual inversion (T.I).\nUsing  and text guidance worked best, thus, we used this setting for our results in Fig. 3  ###reference_###.\nT.I. compares generations to the original images in the pixel space, and Image in the CLIP embedding space, resulting in different guidance mechanisms and, hence, performance."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Depth Estimation",
            "text": "In this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_16###"
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Depth training details",
            "text": "Adversarial Optimization.\nThe adversarial optimization was done with AdamW [46  ###reference_b46###], learning rate of , weight decay of , and batch size of 8. The token embeddings at the start of optimization are randomly sampled from  where  and  is the mean and standard deviation of all embeddings in the vocabulary. We set the early stopping threshold to 0.08 for the UNet model and 1.0 for the DPT model. Note that these models were trained with different losses,  for the former and Midas loss [20  ###reference_b20###] for the later. Adversarial optimization is performed with the same loss as was used for training these models. One run takes approximately 30 mins on one A100. We perform a total of 32 runs, to get 32 Adversarial Prompts for the UNet model and 30 runs for the DPT model. As the DPT model was trained on Omnidata, which is a mix of 5 datasets, we have 6 runs for each dataset. Different number of placeholder tokens were also used for each run as suggested in Fig. 5(b)  ###reference_sf2### of the main paper. For the DPT model, we do 1, 8, 16 tokens runs for each dataset and also 3 runs with 32 tokens for each dataset. For the UNet model, 4 runs of 1, 8 and 16 tokens each and 16 runs of 32 tokens were used, to get a total of 32 prompts. We also use a reduced number of denoising steps during optimization i.e., 5, as we found it to be more stable.\nGuided Adversarial Optimization. The CLIP guidance coefficient for text and image guidance is set to 1 and 5 respectively. For image guidance, we randomly sampled 100 images from the target distribution. For text guidance, we used target distribution’s name in the prompt, e.g., “fog” for the fog corruption from CC.\nGeneration.\nGeneration is performed with the DDIM [72  ###reference_b72###] scheduler and 15 sampling steps. We generate 80k images for the UNet model and 60k images for the DPT model for fine-tuning.\nFine-tuning. For fine-tuning, we optimize the UNet model with AMSGrad [60  ###reference_b60###] with a learning rate of , weight decay of  and batch size 128. For the DPT model, a learning rate of , weight decay of  and batch size 32."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Additional Quantitative Results",
            "text": "Performance of non-SD baselines. In Tab. 6  ###reference_###, we show the results for depth estimation for two additional baselines, deep augmentation [31  ###reference_b31###] and style augmentation [23  ###reference_b23###] that do not make use of generative models. Deep augmentation distorts a given image by passing it through an image-to-image model e.g., VAE [39  ###reference_b39###], while perturbing its representations. Style augmentation involves involves applying style transfer to the original training images. They perform comparably to Adversarial Prompts.\nU-Net\nDPT\n\n\nTaskonomy\nReplica\nTaskonomy\n\nShift\nClean\nCC\n3DCC\nCDS\nCC\n3DCC\n\nControl (No extra data)\n2.35\n4.93\n4.79\n5.38\n3.76\n3.42\n\nAgnostic Prompts\n2.47\n5.03\n4.17\n5.30\n4.06\n3.58\n\nAgnostic Prompts (Random)\n2.38\n4.96\n4.11\n5.14\n3.88\n3.51\n\nAdversarial Prompts\n2.49\n4.36\n4.02\n5.12\n3.40\n3.28\n\nAdversarial Prompts (SDEdit)\n2.59\n4.20\n3.88\n4.96\n3.35\n3.25\n\nDeep Augmentation\n2.42\n4.24\n3.70\n5.01\n2.83\n3.70\n\nStyle Augmentation\n2.42\n4.15\n3.85\n5.16\n2.80\n3.10"
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Additional Qualitative Results",
            "text": "Generations from all adversarial prompts & comparison of generations from different models.\nWe show the generations from all Adversarial Prompts from the UNet model, without SDEDit (Fig. 19  ###reference_###), with SDEdit (Fig. 20  ###reference_###), and multi-iteration (Fig. 21  ###reference_###). Additionally, we provide the generations from two DPT models, allowing us to assess the difference the model feedback has on generations. The first DPT model was only trained on Omnidata (Fig. 22  ###reference_###) and second was trained on Omnidata with augmentations from CC and 3DCC and with consistency constrains [79  ###reference_b79###] (Fig. 23  ###reference_###). The quantitative results in the paper were reported only on the former DPT model.\nThere does not seem to be obvious differences in the styles generated between the two DPT models. However, between the Adversarial Prompts from the UNet model with and without multi-iteration, the Adversarial Prompts from the latter seems to result in much more diverse styles."
        },
        {
            "section_id": "8.4",
            "parent_section_id": "8",
            "section_name": "Additional Analysis",
            "text": "###figure_17### In this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_18###"
        },
        {
            "section_id": "8.4.1",
            "parent_section_id": "8.4",
            "section_name": "8.4.1 Running multiple iterations of adversarial optimization vs a single iteration",
            "text": "Here, we provide additional analysis for the multi-iteration experiments in Fig. 5(b)  ###reference_sf2### in the main paper.\nWe optimize for 4 prompts in each iteration and noticed that if the number of placeholder tokens in a given prompt is kept fixed throughout the iterations, the optimization to find new Adversarial Prompts becomes more difficult. However, if we increase the number of tokens at each iteration e.g.,  token per prompt for 1st,  per prompt for 2nd, etc, we are able to consistently find new Adversarial Prompts.\nThus, we aim to investigate the generalization of a given model to different Adversarial Prompts, e.g., is a model more likely to generalize to Adversarial Prompts with the same number of tokens.\nTo perform this analysis, we generated data  using AP with  and  tokens per prompt respectively and measured the performance of a model fine-tuned on  on .\nIn this setting, , we use . For every , we construct  and  to be generated using  Adversarial Prompts. We fine-tune the model on  and and validate both on  and  validation sets during the fine-tuning. The results are shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###.\nIn this setting,  are respectively. For every  we fine-tune on  and compute the validation loss on  and . The results are shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###. As the loss for  tends to be more similar then when , we chose to increase the number of tokens used per prompt in our multi-iteration setting.\n###figure_19###"
        },
        {
            "section_id": "8.4.2",
            "parent_section_id": "8.4",
            "section_name": "8.4.2 CLIP image vs text guidance.",
            "text": "In Fig. 18  ###reference_###, we compare the qualitative (top) and quantitative (bottom) differences in generations from text guidance and image guidance on defocus blur and fog. Note that image guidance uses sample (unlabelled) images from the corresponding target distribution that it is evaluated on, i.e., fog samples images from the fog corruption from the CC benchmark and fog (3D) samples images from the fog corruption of the 3DCC benchmark. If the target distribution name has (3D) appended to it, it is from the CC benchmark, otherwise it is from the 3DCC benchmark.\nWe observed some differences in the generations with text vs. image guidance (Fig. 18  ###reference_###, top). Text Guided Prompts generates corruptions that are more realistic that image Guided Prompts. For example, fog gets denser further away from the camera or around the floor when text Guided Prompts are used for generations. For image Guided Prompts, as it was guided by the image samples from CC where the corruption is applied uniformly over the image, it learns to also apply a more uniform corruption over the image.\nQuantitatively, we observed that image guidance tends to perform the best across the target distributions, with large enough extra data (Fig. 18  ###reference_###, bottom)."
        },
        {
            "section_id": "8.4.3",
            "parent_section_id": "8.4",
            "section_name": "8.4.3 Generalization of Adversarial Prompts to different models.",
            "text": "We show how adversarial generations from Adversarial Prompts found for one model are for another model in Tab. 7  ###reference_###. The generations from Adversarial Prompts found for e.g., the UNet model result in the highest loss when evaluated on the UNet model. However, the generations from Adversarial Prompts from the DPT model also result in similar loss. Similar trends hold for the DPT model. Thus, Adversarial Prompts found for one model are also able to result in high loss for another model.\nAP from\\Eval on\nOriginal data\nUNet\nDPT\n\n\n\nUNet\n2.55\n7.63\n5.39\n\nDPT\n1.76\n7.17\n6.46\n###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24###"
        }
    ]
}