{
    "title": "1 Introduction",
    "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value () Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformers have proven to be particularly successful in tasks such as language modeling Lewis et al. (2019  ###reference_b32###); Brown et al. (2020  ###reference_b9###); Raffel et al. (2020  ###reference_b43###), image recognition Dosovitskiy et al. (2020  ###reference_b18###), recommendations Sun et al. (2019  ###reference_b53###); de Souza Pereira Moreira et al. (2021  ###reference_b15###); Adnan et al. (2023  ###reference_b2###); Zhao et al. (2023  ###reference_b66###), and text generation with the advent of Large Language Models (LLMs). Unfortunately, LLM deployment presents critical inference latency and throughput concerns. This is primarily attributed to the sequential autoregressive nature of generative inference, particularly when handling inputs with larger contexts. Despite advancements, modern LLMs face challenges in efficiently processing longer input sequences, as evidenced by recent studies Bai et al. (2023  ###reference_b6###); Li et al. (2023  ###reference_b33###); Chen et al. (2023  ###reference_b10###); Huang et al. (2021a  ###reference_b23###). Unfortunately, the increased memory and compute requirements associated with longer sequences exacerbate LLM inference latency and reduce throughput. This paper proposes inference-time strategies for accuracy-preserving, low-latency, high-throughput LLM systems.\n\nLLMs employ Transformers and rely on the ‘attention mechanism’ to understand the relationships between words within a given input sequence Vaswani et al. (2017  ###reference_b55###). As the attention mechanism scales quadratically with the size of the input sequence, it tends to present the largest latency overhead during inference Sukhbaatar et al. (2019  ###reference_b52###); Dao et al. (2022  ###reference_b14###); Choromanski et al. (2020  ###reference_b12###). Additionally, due to the autoregressive nature of token generation in LLMs, there is a need to recompute key and value vectors for all previous tokens. To mitigate this, LLMs utilize a storage structure known as a Key-Value Cache ( ) Ott et al. (2019  ###reference_b40###).   retains previously computed key-value pairs, eliminating the need for costly re-computation of these vectors.\n\nHowever,   presents scalability challenges. Accessing the   from off-chip memory during token generation introduces additional memory latencies and is constrained by memory bandwidth limitations. For instance, in the MPT-7B model illustrated in Figure 1  ###reference_###(a), increasing the sequence length by 16 (from 512 to 8K) results in a more than 50 increase in inference latency. Moreover, approximately 40% of the total inference time (highlighted in green) is consumed by   data movement. Importantly, a larger context not only increases the size of the   but also prolongs the time required for other operations (depicted in blue). Similarly, as shown in Figure 1  ###reference_###(b) for the MPT-7B model, the   size surpasses the model size when the sequence length exceeds 8K. Thus,   sizes present a roadblock to enabling low-latency, high-throughput inference for large sequences.\n\nPrevious studies have explored mitigating attention mechanisms’ memory and computation requirements when dealing with longer sequences Zaheer et al. (2020  ###reference_b64###); Kitaev (2020  ###reference_b30###); Wang et al. (2020  ###reference_b59###); Beltagy et al. (2020  ###reference_b7###). While system-level optimizations like FlexGen Sheng et al. (2023  ###reference_b49###), Flash Attention Dao et al. (2022  ###reference_b14###), Paged Attention Kwon et al. (2023  ###reference_b31###), and multi-dimensional partitioning Pope et al. (2023  ###reference_b41###) aim to improve the scalability of generative AI, they often overlook the fundamental challenge of expanding   size. Techniques like multi-query Shazeer (2019  ###reference_b48###) and group-query attention Ainslie et al. (2023  ###reference_b4###) propose reducing   size by eliminating specific attention heads from writing to the  , but these methods typically require resource-intensive model retraining or fine-tuning. This becomes complex as various accelerators are already deployed in the field. Thus, there is a pressing need for inference-time techniques for   reduction. This is even more challenging as any proposed technique must conform to the strict constraints for model accuracy. For instance, MLPerf Red"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Motivation",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Inference Process in Large Language Models",
            "text": "In language modeling, the task involves estimating the probability of the next token based on preceding tokens . For generative Large Language Models (LLMs), the inference process unfolds in two phases:\nPrompt Processing Phase: This phase helps input context undergo causal processing, enabling the model to generate keys and values for all tokens within the context. These key-value pairs are then stored in the  .\nToken Generation Phase: This phase sequentially and auto-regressively generates text. Each token is produced by passing through all layers of the generative model. Notably, the generation of the next token relies on the previously generated tokens and their order.\nTo enhance inference efficiency, repeated and complex computations of Key () and Value () tensors across all layers are avoided by caching these tensors. This is referred to as the  . The   is sequentially populated during each token generation step Strati et al. (2024  ###reference_b50###), until the text generation process is completed."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Reducing KV Cache Size by Exploiting Sparsity",
            "text": "To address the challenge posed by the expanding  , let us examine a sequence, denoted as , comprising  tokens and its   contents for a single attention head and layer. In full attention, the   components involve  keys and values. These grow proportionally with . To mitigate this, we can shrink the   size to accommodate shorter sequences, designated as . This involves using a reduced number of tokens, transitioning from  to , where  is a subset of , and  is less than . This reduction can be achieved by leveraging the inherent sparsity within the attention mechanism of LLMs.\nDespite the substantial computational demands during the training of transformers, there exists inherent sparsity within the attention mechanism. However, the extent of sparsity may vary depending on the particular downstream task. Figure 3a  ###reference_sf1### illustrates the diverse levels of attention sparsity among different models utilized for summarization tasks. This variability manifests across various levels of the model, including the overall model, individual layers, and distinct sections of the model."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Improving Performance by Using Key Tokens",
            "text": "In Figure 3b  ###reference_sf2###, the Cumulative Distribution Function (CDF) depicts the relationship between attention score and the fraction of the total context. Notably, a small subset of tokens receives the most attention during text generation. This underscores the significance of specific   and their pivotal role in comprehending context and facilitating text generation. However, dynamically determining which tokens serve as  , especially in cases where the input sequence contains unknown or unseen tokens during inference, presents a considerable challenge."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Leveraging Score Function to Identify Key Tokens",
            "text": "We introduce a score function  for each token to identify the    out of a total of  tokens. In the multi-head attention mechanism, attention scores determine the degree of connection between a single token and all other tokens. This is described by Equation 1  ###reference_###.\n###figure_6### The natural choice is to utilize the attention score as the score function, denoted as . This method is commonly observed in previous state-of-the-art work, such as HO Zhang et al. (2023  ###reference_b65###). It identifies tokens that consistently receive higher attention scores during the prompt and token generation phases as the most critical or  .\nWe can choose and retain these  tokens based on their accumulated attention scores, creating what we refer to as “Key Attention”. However, relying solely on these  tokens during attention does not provide the necessary accuracy and yields poor performance. This is shown in Figure 3c  ###reference_sf3###.\nIn this comparison, both ‘Window Attention’ and ‘Key Attention’ demonstrate inferior performance compared to full attention, even when the window and   parameters are reduced by . While reducing the sizes of the window and   relative to the total tokens () is crucial for minimizing the size of the  , it also leads to a significant decrease in accuracy. This decline primarily stems from the loss of recent context in key-token attention and crucial context in window attention. Building on this observation, we propose a mixed approach that combines selected   with recent tokens to reduce the   size while also preserving accuracy."
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Problem: Uneven Score Distribution",
            "text": "Figure 4  ###reference_### shows the distribution of attention scores  for full attention, as described in Equation 2  ###reference_###. When   is reduced, tokens with lower scores are discarded. This alters the score function, shown in Equation 3  ###reference_###, as the term  becomes zero.\n###figure_7### This removal of tokens disrupts the distribution of the score function. This is because the attention weight of the discarded tokens is unevenly distributed among the tokens within the reduced  . This uneven distribution arises due to the nature of the inherent softmax function. Figure 4  ###reference_### illustrates this phenomenon by comparing the distribution of the score function for full attention with that after   reduction. When the score distribution is uneven, the model may not attend to the most relevant tokens in the sequence. Consequently, this can result in a loss of contextual information, reduced accuracy, and potentially lower-quality text generation."
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Motivation: Damping the Score Function",
            "text": "A straightforward approach involves damping the score function using a damping factor to counteract the excess attention score resulting from discarded tokens. Assume  is the damping factor. It modifies the score function as . Ideally, we aim to dampen the score function by a factor equivalent to , where  represents the tokens that have been discarded.\nFigure 3b  ###reference_sf2### shows that, even with a 50% reduction in the   size, the average accumulated attention score of   remains consistently high, ranging from approximately 90% to 95%. We conduct a sweep across a range of values to explore the impact of different damping factors () on overall model quality. This analysis is performed with a   size set at 50% and a recent ratio of 20% (representing the percentage of recently generated tokens) for the Cerebras-GPT-6.7B Dey et al. (2023  ###reference_b16###) model.\nHowever, as depicted in Figure 5  ###reference_###, even after the application of a damping factor, it is not possible to achieve the same quality as the full attention model. This discrepancy stems from a significant change in the score distribution of the remaining tokens within the reduced  . These findings underscore the inadequacy of relying solely on the accumulated attention score-based score function  for identifying  . Hence, addressing the impact of discarded tokens within the score function is crucial to achieve higher model accuracy or meet the accuracy requirements of benchmarks like MLPerf."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Keyformer: Intuition and Design",
            "text": "leverages the inherent sparsity within decoder layers by identifying   using a mixture of recent tokens. It adjusts the changes in the score function resulting from discarded tokens by applying regularization to the unnormalized logits for the identification of  .\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_8### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_### and Equation 8  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Logits Regularization",
            "text": "We strategically remove  tokens from the context in the prompt processing phase. This helps us maintain a constant   size with  tokens during generation and prevents unwarranted memory expansion. Thereafter,  uses logits regularization technique. The introduction of added distribution () to regularize the reduced logits enables our model to remain robust and adaptive. It helps identify the   even in the presence of unknown contexts during inference-time.  adds this noise to the unnormalized logits derived from , as illustrated in Equation 4  ###reference_###. Also, the type of distribution added significantly impacts the resulting probability distribution.\nHere,  are the adjusted logits,  are the unnormalized logits, and  is the added distribution for regularization."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Choice of Distribution for Regularization",
            "text": "The regularization distribution added to unnormalized logits impacts   identification and model quality. Thus, we aim to draw intuition using the semantics of LLMs.\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_###  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_###  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_9### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_###  ###reference_### and Equation 8  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Intuition: Bias Towards Initial Tokens",
            "text": "Previous research, such as streaming LLMs Xiao et al. (2023  ###reference_b62###) and the HO model Zhang et al. (2023  ###reference_b65###), has shown a bias towards initial tokens. This bias stems from accumulated attention scores favoring initial tokens due to cumulative effects during decoding iterations. We propose using a skewed distribution to leverage this bias and effectively model the distribution of maximum values ( ). This distribution favors initial tokens while maintaining an asymmetric profile, enhancing the representation of tokens drawn from the recent context window.\nOur choice of distribution is inspired by the Gumbel distribution Cooray (2010  ###reference_b13###  ###reference_b13###  ###reference_b13###). The Gumbel distribution is particularly well-suited for our   identification task, as it characterizes the distribution of maximum values within a set of samples and is skewed towards initial tokens. This makes it an ideal candidate for modeling   for long sequences.\nEquation 5  ###reference_###  ###reference_###  ###reference_### presents the standard Gumbel pdf applied to unnormalized logits, while Equation 6  ###reference_###  ###reference_###  ###reference_### displays the pdf of logits adjusted with Gumbel addition. Additionally, it is noteworthy that the Gumbel distribution holds significance in statistical theory. It captures the essence of the Gumbel limit theorem, which asserts that common probability distributions (such as normal, exponential, uniform, etc.) converge to the Gumbel distribution. This underscores its appropriateness for modeling the identification of  .\n###figure_10### In theory, selecting a regularization distribution that promotes uniformity after normalization aids in   identification. This is crucial during inference when information about discarded tokens is unavailable. To quantify the spread of probability distributions post-normalization, we employ entropy, defined as . Our analysis indicates that Gumbel-based logit adjustment fosters a more uniform distribution, suggesting its effectiveness as a regularization technique for   identification, as demonstrated in Equation 7  ###reference_###  ###reference_###  ###reference_### and Equation 8  ###reference_###  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Keyformer Score Function",
            "text": "We propose a novel score function for , denoted as , to address the limitations of the accumulated attention-based score function (). This new score function integrates the Gumbel noise distribution into the unnormalized logits. However, it fails to account for the discarded tokens in forming the underlying probability distribution. To rectify this, we introduce a temperature parameter, denoted as , as shown in Equation 9  ###reference_###.\nThe probabilistic score functions described above are akin to the concept of Gumbel Softmax Jang et al. (2016  ###reference_b25###). This score function offers a continuous relaxation of discrete random variables Maddison et al. (2016  ###reference_b36###). This alignment corresponds with our primary objective of identifying a subset of past tokens  that conveys the same semantic information as the original complete set of tokens."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Significance of the Temperature Parameter",
            "text": "The ‘temperature’ parameter  is pivotal in regulating the smoothness of the probabilistic distribution. Higher values of   yield uniform probabilities, assigning equal scores to all tokens. Conversely, lower values of   produce a sharper distribution, prioritizing specific tokens based on their unnormalized logits. This parameter governs the degree of randomness in probabilities. It is crucial when tokens are removed from the  , as they cannot be reintroduced without recomputing their keys and values.\nIn Equation 10  ###reference_###, we illustrate the dynamic nature of  at each decoding iteration . To achieve this, we define a range for  spanning from  to . In each decoding step, we increment  by , a value determined by the range of  and the length of the text being generated, denoted as .\nThis strategy is based on the premise that we need a more uniform or randomized probability distribution as more tokens are discarded. Through empirical analysis, we discovered that setting  and  produces optimal outcomes (refer to Appendix  A.8  ###reference_###). This decision aligns with our objective of maintaining a non-random score function during the prompt phase, where all tokens are available. When  is set to one, the Gumbel softmax approach is nearly equivalent to a standard softmax. As we advance through decoding iterations and discard more tokens to maintain a static   size, we systematically increase the randomness in our score function . This is achieved by incrementally raising  with ."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Leveraging Score Function Accumulation",
            "text": "The accumulation of the score function is essential for identifying   based on their consistent behavior throughout decoding steps. Without accumulation, token discarding would rely solely on the current token’s correlation with previous tokens. Although the correlation of the current token is significant in identifying  , their behavior should remain consistent across most generated tokens. To discern   based on this consistent behavior, we accumulate the score function  across both the prompt and token generation phases, as depicted in Figure 6  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Keyformer Algorithm",
            "text": "Figure 6  ###reference_### presents an overview of . We highlight its key functionalities in discarding tokens based on sparsification, using a mixture of recent and  , and introducing a novel Gumbel softmax-based score function for   identification. During the prompt processing phase,  calculates keys and values for all  tokens within the prompt length  to predict the first token. Given the   budget,  retains a recent window of  recent tokens while discarding  tokens from the  tokens window, thereby identifying  tokens. The top-() tokens from the  window are selected based on the  score function. The combination of   () and recent tokens () forms the reduced  . As there are no discarded tokens during the prompt processing phase,  uses a temperature parameter, , to approximate the softmax probability distribution. This is illustrated in decoding step 1.\nIn the token generation phase,  operates with a reduced  . The first generated token attends solely to the  tokens within the  , as depicted in decoding step 2. The recent window  shifts right by a single token, while the score function  accumulates with the score function from the previous decoding step. During each decoding step of the token generation phase,    are identified from a window of size . Consequently, one token has been added, and another has been removed from the recent window. Since we add and remove tokens from the ‘ ’ window, we can improve accuracy while maintaining a static   size equal to . Moreover, the temperature parameter  increases by  to adjust for the number of removed tokens in the probability distribution of the score function. The detailed algorithm for  is provided in Algorithm 1  ###reference_###.\n###figure_11###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We evaluate across three significant model families: GPT-J Wang & Komatsuzaki (2021 ###reference_b56###), Cerebras-GPT Dey et al. (2023 ###reference_b16###), and MPT Team et al. (2023 ###reference_b54###), each using distinct position encoding techniques. GPT-J incorporates RoPE Su et al. (2022 ###reference_b51###), Cerebras-GPT employs learnable position embeddings, and MPT utilizes ALiBi Press et al. (2021 ###reference_b42###). By including models with varied position encoding methods, we ensure the robustness and generalizability of our findings across representative model families. We employed a fixed beam size of 4 for all evaluations.\n\nWe conducted evaluations on two representative text generation tasks: summarization, utilizing the GovReport Huang et al. (2021b ###reference_b24###) dataset, and conversation, employing the SODA dataset Kim et al. (2022 ###reference_b29###). The GPT-J model was fine-tuned specifically for summarization, while Cerebras-GPT and MPT are pre-trained models. We utilized the MPT-chat version of the MPT model for conversation tasks, which was fine-tuned for dialogue generation. All models were pre-trained with a sequence length of 2k.\n\nTo address long document summarization, we utilized the MPT-storywriter version of the MPT model, fine-tuned for writing fictional stories. This model accommodates a context length of 65k and can generate content up to 84k tokens long. Additionally, we evaluated four tasks from the lm-eval-harness Gao et al. (2021 ###reference_b19###) framework: PIQA Bisk et al. (2020 ###reference_b8###), Winogrande Sakaguchi et al. (2021 ###reference_b46###), OpenBookQA Mihaylov et al. (2018 ###reference_b37###), and COPA Roemmele et al. (2011 ###reference_b45###). These tasks involve few-shot evaluation of autoregressive language models and were executed using the NVIDIA A100 (80GB) GPUs.\n\nTo evaluate the accuracy of, we compared it against Full Attention. Full Attention acts as our benchmark and represents the gold standard for accuracy. We aim to achieve an accuracy target within the range of 99% to 99.9% of Full Attention. This goal aligns with the high-quality standards set by industry benchmarking entities like MLPerf Reddi et al. (2020 ###reference_b44###). Additionally, we performed comparisons with Window Attention and the recent HO model Zhang et al. (2023 ###reference_b65###), adjusting the size from 20% to 90% of the prompt length.\n\nWe assessed the effectiveness of reduction in while maintaining accuracy for handling long contexts. This evaluation was conducted on the MPT-7B-story writer model, pre-trained with a context length of 65k. We utilized the Government report Huang et al. (2021b ###reference_b24###) dataset, which contains reports authored by government research agencies and features longer summaries and documents. This dataset requires a deep understanding of context to extract crucial information for summarization. Figure 8 ###reference_### shows the accuracy comparison among, HO, and Full Attention. Notably, even with a 50% size, maintains the desired 99% accuracy threshold, while HO shows significantly lower accuracy at the same size.\n\n###figure_12### To understand the sources of performance benefits with -based reduction, we primarily consider two factors:\nReduced KV cache: A smaller significantly reduces the data movement from off-chip GPU HBM.\nScaled Dot Product Optimization: The number of tokens in the is reduced from to.\n\nThe above two factors reduce the overall smaller size of matrices. Thus, they enable an optimized scaled dot product within the multi-head attention block.\n\n###figure_13### It is worth noting that in LLMs, which are memory-bound, the main performance boost comes from reducing data movement rather than matrix multiplication. However, ’s identification process introduces some overhead due to Gumbel softmax. Figure 10 ###reference_### illustrates the normalized performance improvement for, considering both reduced data movement and optimized scaled dot product. These enhancements are demonstrated for the MPT-7B-storywriter model with a 50% reduction, including the additional overhead from ’s Gumbel softmax. The results indicate that -based reduction decreases data movement by 2.9 and improves computational efficiency in the attention module’s scaled dot product by 1.3, particularly for sequences of length 4k.\n\nTo examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, where, mirroring the approach used in HO Zhang et al. (2023 ###reference_b65###).\n\nTo study the impact of constant regularization on"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Accuracy Results",
            "text": "To assess the impact of reduction on text generation quality, we relied on the ROUGE score Lin (2004), a widely-used metric for evaluating fluency and coherence. ROUGE measures the overlap of n-grams between generated and reference text, providing a standardized measure for text quality. According to MLPerf, ROUGE scores, including ROUGE-1, ROUGE-2, and ROUGE-L, should reach 99% to 99.9% of their original values for summarization tasks. Thus, even with reduced , our model should maintain the desired ROUGE scores. Figure 7 depicts accuracy comparisons between and other methods (Full Attention, Window Attention, and HO) across different sizes. The illustration focuses on the ROUGE-2 score, which measures bi-gram overlap. Trends for ROUGE-1 and ROUGE-L are detailed in Appendix A.5.\n\nThe results highlight the importance of the previous context for model performance. For instance, Window Attention relies solely on recent tokens, leading to a significant loss of accuracy. Thus, identifying is crucial for achieving desired model accuracy. Across various budgets, consistently outperforms the state-of-the-art HO. It shows that the it identifies are more important than the heavy hitters identified by HO. For instance, attains the target ROUGE score with just 70% of the , whereas HO fails to reach this goal even with a larger budget. Furthermore, surpasses the baseline accuracy by up to 1.73% (0.9% for GPT-J-6B and 1.73% for Cerebras-GPT-6.7B for summarization task) achieved with full attention. This demonstrates the regularization effect of the introduced Gumbel noise in the score function of and its positive impact on identification.\n\nWe assessed the effectiveness of reduction in while maintaining accuracy for handling long contexts. This evaluation was conducted on the MPT-7B-story writer model, pre-trained with a context length of 65k. We utilized the Government report Huang et al. (2021b) dataset, which contains reports authored by government research agencies and features longer summaries and documents. This dataset requires a deep understanding of context to extract crucial information for summarization. Figure 8 shows the accuracy comparison among , HO, and Full Attention. Notably, even with a 50% size, maintains the desired 99% accuracy threshold, while HO shows significantly lower accuracy at the same size."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance Results",
            "text": "To assess the performance advantages of our implementation, we considered two critical inference metrics: inference latency and generation throughput for the target models. Our implementation is seamlessly integrated with Huggingface model cards, ensuring ease of adoption. We disabled CPU offloading in cases where the model and data exceeded GPU HBM memory capacity, ensuring consistent evaluation. We generated a synthetic dataset to maintain evaluation consistency, where all prompts were padded with synthetic text. We employed the MPT-7B-storywriter model to generate an equal number of tokens for each prompt. We tested various combinations of prompt and generation lengths.\n\nFigure 9 presents inference latency speedup while Table 1 shows improvement in generation throughput in comparison to a Full Attention-based method. With a 50% reduction, the method significantly reduces inference latency, achieving 2.1 times speedup with the same batch size. Moreover, the reduced size allows handling twice the batch size compared to full attention, increasing token generation throughput by 2 times with the same batch size and 2.4 times with a bigger batch size.\n\nTo understand the sources of performance benefits with our reduction method, we primarily consider two factors:\n\n1. Reduced KV cache: A smaller size significantly reduces the data movement from off-chip GPU HBM.\n2. Scaled Dot Product Optimization: The number of tokens is reduced, optimizing the scaled dot product within the multi-head attention block.\n\nThe above two factors reduce the overall size of matrices, enabling an optimized scaled dot product within the multi-head attention block.\n\nIt is worth noting that in LLMs, which are memory-bound, the main performance boost comes from reducing data movement rather than matrix multiplication. The identification process introduces some overhead due to Gumbel softmax. Figure 10 illustrates the normalized performance improvement, considering both reduced data movement and optimized scaled dot product. These enhancements are demonstrated for the MPT-7B-storywriter model with a 50% reduction, including the additional overhead from the Gumbel softmax. The results indicate that reduction decreases data movement by 2.9 times and improves computational efficiency in the attention module’s scaled dot product by 1.3 times, particularly for sequences of length 4k."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Few-Shot Evaluation",
            "text": "We performed few-shot experiments using four tasks from the lm-eval-harness framework and pre-trained models to evaluate performance under varying numbers of shots during inference. Table 2 presents the results for 0 and 5 shots, demonstrating that consistently surpasses previous approaches across all tasks and shot settings. Even with a 50% reduction in size, it achieves accuracy close to the full attention baseline."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "To examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, mirroring the approach used in HO Zhang et al. (2023). To study the impact of constant regularization on all unnormalized logits, we experimented with constant logit adjustment, where a constant is being added to every unnormalized logit. We also used a symmetric Gaussian distribution for logit adjustment. The Gaussian probability density function (pdf) with mean and variance is applied to unnormalized logits, while the pdf of logits adjusted with Gaussian addition is also considered.\n\nEmpirical evidence shows that the Gumbel distribution, known for its skewness to initial tokens, is an effective regularization mechanism for identification."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Shared versus Per-Layer Score Function",
            "text": "The score function defines what constitutes a choice in the context. In generative LLMs with stacked decoder layers, the score function can either be shared across all layers (Shared) or dedicated to each layer (Per-Layer). In the Per-Layer approach, a dedicated score function is assigned to each decoder layer, with accumulation occurring at each decoding stage. Conversely, the Shared method uses a single global score function for all decoder layers, with accumulation across decoder layers and decoding stages.\n\nTable 3 illustrates the accuracy comparison between Per-Layer and Shared score functions, maintaining the original positional information and size constant. Notably, using the Per-Layer score function yields better accuracy than the shared score function. This aligns with the intuition that transformers learn hierarchical text representations across layers, with lower layers capturing local syntactic and semantic features and higher layers capturing more abstract and complex patterns. Therefore, having a Per-Layer score function for each layer aids in choice identification specific to that layer."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 New vs. Original Positional Information",
            "text": "We investigated how reducing the size affects the positional information used for the keys. We explored two approaches: (Org Pos) and (New Pos). In (Org Pos), the position information reflects the original positions of tokens within the text. Conversely, (New Pos) uses positions based on the new arrangement of tokens.\n\nTable 3 presents the accuracy comparison with consistent size and score function. Notably, when using the original positional information, there is a noticeable improvement in accuracy. However, incorporating new positional information during inference leads to a slight drop in accuracy. Nevertheless, even with new positional information, the performance outperforms the state-of-the-art methods available."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Comparison with Alternative Distributions",
            "text": "We conducted an ablation study to assess how different logit adjustment distributions affect model accuracy or identification. We evaluated three regularization strategies and compared them with Gumbel-based logit adjustment. These are no logit adjustment, constant logit adjustment, and Gaussian distribution-based logit adjustment.\n\nTo examine the effect of omitting regularization on unnormalized logits, we experimented without logit adjustment, where , mirroring the approach used in HO Zhang et al. (2023).\n\nTo study the impact of constant regularization on all unnormalized logits, we experimented with constant logit adjustment, setting , where  is the constant that is being added to every unnormalized logit.\n\nWe also used a symmetric Gaussian distribution for logit adjustment. Equation 11 presents the Gaussian probability density function (pdf) with mean  and variance  applied to unnormalized logits, while Equation 12 displays the pdf of logits adjusted with Gaussian addition.\n\nWe established a baseline comparison using a standard Gumbel pdf with  and . For comparison, the Gaussian pdf had an identical mean and variance, and the constant logit adjustment employed a constant value. The “No logit adjustment” approach uses the method in prior work, HO.\n\nThus, empirical evidence shows that the Gumbel distribution, known for its skewness to initial tokens, is an effective regularization mechanism for identification."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4 Recent Window versus Key token Window Ratio",
            "text": "We conducted a sensitivity study to examine the impact of varying the ratio of recent tokens. Results in Appendix A.4 indicate that the models perform better when the recent tokens ratio falls within the range of 20% to 30%. This observation aligns with our hypothesis that recent tokens are critically important for LLM inference."
        },
        {
            "section_id": "4.4.5",
            "parent_section_id": "4.4",
            "section_name": "4.4.5 Comparison with Attention Sinks",
            "text": "Recent research introduced StreamingLLM (Xiao et al., 2023), which introduced the concept of “attention sinks.” StreamingLLM enables Language Models (LLMs) trained with a finite-length attention window to handle infinite sequence lengths without fine-tuning. This is achieved by retaining the first four tokens (known as “attention sinks”) and a moving window of recent tokens. To compare StreamingLLM with , we maintained a size of 60% for both techniques. Table 3 displays the accuracy comparison, showing that StreamingLLM struggles in summarizing text by relying on only the first four tokens as attention sinks and the remaining tokens from a recent window (Appendix A.7)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Prior work focus on improving inference speed for transformer Vaswani et al. (2017  ###reference_b55###) based models. PoWER-BERT Goyal et al. (2020  ###reference_b22###) utilizes word-vector elimination by exploiting redundancy for encoder-based models. Linformer Wang et al. (2020  ###reference_b59###) tries to reduce the attention mechanism from quadratic to linear. Reformer Kitaev (2020  ###reference_b30###) reduces attention complexity by locality-sensitive hash (LSH). Linear transformers Katharopoulos et al. (2020  ###reference_b28###) store accumulated states rather than preserving every representation. FLAT Kao et al. (2023  ###reference_b27###) suggests optimized dataflow, while other research Wang et al. (2022  ###reference_b60###) overlaps communication with dependent computation to enhance attention execution. In contrast,  aims to reduce  , speeds up attention by reducing the tokens.\nOne line of work sparsifies the attention mechanism to reduce the computational and memory capacity of the attention block. BigBird Zaheer et al. (2020  ###reference_b64###) combines random, windowed, and global attention to maintain the accuracy for transformers while sparsifying the attention block. LongFormer Beltagy et al. (2020  ###reference_b7###) also utilizes windowed attention with task-based local attention to achieve sparse attention. Spatten Wang et al. (2021  ###reference_b57###) introduces sparsity at both the head and token levels. However, it needs a dedicated architecture to exploit sparsity. Furthermore, these works do not address inference optimizations.\nEl-Attention Yan et al. (2021  ###reference_b63###) modifies the multi-head attention module to reduce the   size, leveraging key and value stability during incremental decoding for reuse across layers. In contrast, HO Zhang et al. (2023  ###reference_b65###) identifies heavy-hitters and keeps them in the   to reduce its size, neglecting the attention score distribution shift that occurs post elimination of previous tokens from the  , leading to accuracy trade-offs. Other approaches Liu et al. (2023  ###reference_b35###); Anagnostidis et al. (2023  ###reference_b5###) introduce sparsity at both coarse and fine-grained levels, targeting the elimination of specific heads and tokens during inference. However, these methods require task-specific predictors and fine-tuning of pre-trained models. Another method Mu et al. (2023  ###reference_b39###) compresses prompts into gist tokens to reduce  . Landmark Attention Mohtashami & Jaggi (2023  ###reference_b38###) represents token blocks with an additional landmark token in the vocabulary, necessitating computationally intensive retraining or fine-tuning for gist or landmark token integration."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Work",
            "text": "Recent techniques like Multi-Query Attention (MQA) Shazeer (2019  ###reference_b48###) and Group-Query Attention (GQA) Ainslie et al. (2023  ###reference_b4###) aim to train foundation models with fewer attention heads.\nHowever, such models are typically used after fine-tuning for specific tasks.\nWhile the detailed evaluation of  with these models is deferred to future work, it is worth noting that  can still be applied on top of MQA or GQA-based models. This is because it discards redundant tokens regardless of the number of heads. Additionally, we plan to integrate  into the LLM’s attention block by replacing the standard softmax with a -based softmax. This introduces sparsity during training, addressing the quadratic computational and memory complexities of transformers.\nThis direction aims to enhance scalability to longer contexts without sacrificing accuracy."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Advancements in large language models (LLMs) are pushing for longer contexts and extensive text generation, with models trained on sequences of millions of tokens. However, this trend strains system memory bandwidth, leading to execution costs. In longer contexts, the attention state size, primarily responsible for memory bandwidth consumption and inference latency, exceeds the model parameters’ size. To address this, we proposed a technique that effectively reduces the attention state size up to 50% without sacrificing accuracy by discarding tokens across heads, layers, and beams, identifying essential tokens based on a novel score function. This method can be applied to LLMs at inference time, without requiring fine-tuning, while also improving latency and token generation throughput."
        }
    ]
}