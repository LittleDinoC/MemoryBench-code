{
    "title": "Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers",
    "abstract": "Recently, multiple architectures have been proposed to improve the efficiency of Transformer Language Models through changes to the self-attention block to achieve linear-cost inference (LCI). A notable approach is the State-Space Machines (SSMs) architecture, which showed comparable performance on language modeling tasks with self-attention transformers. However, such architectural changes require a full pretraining of the weights from scratch, incurring a significant cost for researchers and practitioners. In traditional linear attention works, it has been proposed to approximate full attention with linear attention using the swap-and-finetune framework (Kasai et al., 2021). Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), where the weights of shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, and input/output embeddings, are directly transferred to the new architecture from pre-trained model parameters. We experimented with the efficacy of the method on varying sizes and alternative attention architectures and found that XATL significantly reduces training time by up to 2.5x and converges to a better minimum, producing a model up to 2.6% stronger on LM benchmarks within the same compute budget. The code and instructions to download the weights are published at https://github.com/syncdoth/lit_llm_train.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The NLP community is witnessing a proliferation of large language models (LLMs) released regularly, including both large, closed-source models like OpenAI GPT-4 and ChatGPT, Google’s PaLM, Anthropic AI’s Claude, and smaller, open-source models such as GPT, LLaMA 1&2, Mistral-7B, and Falcon-7B. These models are based on Transformer architecture, which uses self-attention with causal masking. Unlike Recurrent Neural Networks (RNNs), Transformers allow for efficient, scalable training by stacking layers. However, RNNs have the advantage of inference efficiency, as they maintain a single global cache of previous states, leading to constant time operations per generation step. Transformers, in contrast, require maintaining a full history of past KV-caches, resulting in quadratic inference time.\n\nRecent research seeks to improve self-attention's quadratic inference cost. Innovations include linear attention transformers that approximate full softmax attention in linear time, proposed by Katharopoulos et al., Kitaev et al., and others. State-Space Machines present new time-mixing methods, such as RWKV, RetNet, H3, and Hyena, enabling parallel training and recurrent inference. Additionally, Mamba introduces efficient scan algorithms for feasible long-sequence training and recurrent inference. These are categorized as Low-Cost Inference Transformers (LCI).\n\nWhile promising fast inference, these architectures typically require pre-training from scratch, demanding substantial computational resources. This may limit their broader use since savings from efficient inference must be offset against pre-training costs. To address this, we propose a transfer learning approach inspired by the Transformer-to-RNN (T2R) framework. In our Cross-Architecture Transfer Learning (XATL) framework, transformer weights are directly transferred to new architectures. We keep the existing transformer architecture intact, swapping attention layers with efficient components, and training on language tasks. Most LCI and Transformer variations share key components, such as layer norms and residual connections, allowing XATL to transfer many weights, benefiting from strong initialization and accelerated training. Our experiments demonstrate that XATL can achieve equivalent performance with significantly less computation and improve performance on language modeling and commonsense benchmarks with the same compute budget."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Linear Attention",
            "text": "The works under the linear attention category proposes methods to approximate the softmax based attention with various kernels (Katharopoulos et al., 2020  ###reference_b19###; Qin et al., 2022  ###reference_b25###; Kitaev et al., 2020  ###reference_b20###; Wang et al., 2020  ###reference_b34###; Zhai et al., 2021  ###reference_b37###). The research focus is to design a kernel or method that can approximate the full expressivity of attention (Zhang et al., 2024  ###reference_b38###; Kasai et al., 2021  ###reference_b18###), and propose to swap-then-finetune the linear attention only (T2R), or set the attention matrix as direct learning target and distill the attention matrix first. These methods therefore did not require pretraining from scratch and could substitute attention directly. However, they fail to retain local granularity when it comes to associative memory recall tasks (Arora et al., 2023  ###reference_b5###), and may approximate the attention too smoothly when the attention matrix can actually be characterized as “spiky” (Zhang et al., 2024  ###reference_b38###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "State Space Machines (SSMs)",
            "text": "Recently, a new suite of models under the term Space Space Machines (SSMs) were introduced. Popular models include RWKV (Peng et al., 2023  ###reference_b23###), RetNet (Sun et al., 2023  ###reference_b30###), H3 (Fu et al., 2023  ###reference_b13###), Hyena (Poli et al., 2023  ###reference_b24###), and Mamba (Gu & Dao, 2023  ###reference_b16###). These blocks typically allow both parallel and recurrent view of the time mixing algorithm, allowing for fast parallel training and fast recurrent inference. In a practical standpoint, these models show great promise as their language model benchmark performances is on par with the State-of-the-Art Transformer models while being much efficient at inference time. However, they have new components (SSM Blocks) in its architecture, they must be pre-trained on a large-scale dataset first to be accessible by the general public. This work tries to address the expensive requirement of such pre-training by reusing the shared components’ weights from a well-established pretrained transformer parameters."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Weight Reusing",
            "text": "We were also inspired by the Weight Reusing work that proposes to clone the weights of another model, such as in DistillBert (Sanh et al., 2020  ###reference_b29###), and Weight Subcloning (Samragh et al., 2023  ###reference_b28###). In DistillBert, the student model’s weights are initialized from the teacher (Bert-base) model’s weights, and Weight Subcloning proposes to clone the important weights from the larger model to a smaller model, which accelerates the training significantly. We also propose to initialize the weights of the new model from a pretrained-model, but not from a larger model but a same sized model with different attention architecture."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Cross-Architecture Transfer Learning (XATL)",
            "text": "Inspecting various open-source LMs available, we noticed that many of the models share the same hidden dimension size as the other same-sized LMs. Moreover, Linear-Cost Inference (LCI) architectures borrow most of the components from the transformer architecture and work as a plug-and-play replacement to the self-attention block. Inspired by this, we propose a paradigm for transfer learning, called Cross-Architecture Transfer Learning (XATL), which transfers weight matrices from other Pre-Trained Language Mdoels (PTLMs) to other language models with different architecture, especially the LCI architectures.\nWe first formalize the Transformer architecture in the equations below:\nwhere  is input tokens,  are input and output token embeddings,  is the ’th layer transformer block,  is layernorm, and  is the output logits. Each block is described as:\nNotice that above formalization uses Pre-Normalization, which became de-facto in many SOTA LLMs.\nWe experimented with transferring the weights of token embedding () and LM head () layers, the FFN layers, and the attention output projection () weights. The layernorm weights are considered as the part of succeeding weights; for instance,  is copied along with FFN weights.\nFirst, the token embeddings and the LM head layers are the very first and last layers and calibration of the weights will have direct impact on final loss computation. Also, they are the largest weight matrices, so initializing them to a well-learned weights may bring a significant benefit. Intuitively, copying the input token embedding can equip the model with a already good representation of the tokens to work with. Copying the LM head on the other hand provides the model a well-calibrated output matrix, and lets the model to learn a good representation that is compatible with that weight.\nNext, the FFN layers has been interpreted to serve as the key-value memory storage for the models (Geva et al., 2021  ###reference_b15###). Transferring these layers from a PTLM can be conceptually thought of as transferring the memory of the PTLMs to the new models. Moreover, they take up the most weights in the transformer architecture, and being able to initialize them to a good learned weight can potentially lead to accelerated training. The  layer is the final projection layer of the self-attention block, and it can be considered as another smaller FFN applied after time-mixing."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Freezing / Unfreezing of the Copied weights",
            "text": "As the training progress, having a significant portion of weights frozen will be a disadvantage in terms of tunable parameters. To alleviate this problem, we propose a simple heuristic to schedule the unfreezing of the copied weights. We also experimented with unfreezing the weights from the beginning, which is similar to the T2R’s swap-then-finetune method.\nWe monitored the improvement ratio of average loss between certain interval, and unfreeze the weights if the ratio is below some threshold. We also set patience parameter, to ensure that the threshold has been passed not due to randomness in the loss plane but because of saturation. In our experiments, the threshold was set to 1% and the patience was set to 1. We refer to this method as XATL-LIT, short for Loss Improvement Threshold."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Hybrid Architecture",
            "text": "Works like H3 (Fu et al., 2023  ###reference_b13###) and Hyena (Poli et al., 2023  ###reference_b24###) suggests that Hybrid of attention and SSM blocks can boost the performance by a big margin. Moreover, in our case, the attention layers can also be copied over, allowing for more weights to be transferred. We followed the method used in the H3 paper and interleaved 2 attention layers with the SSM layers, at layer 2 and layer  for a model with  layers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we will describe the details of our experimental design and analyze the results. We will introduce the models used, the datasets, the implementation detail, and finally the analysis of the results."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "For our main experiments, we compared the Multi-Head Attention (MHA) baseline with famous LCI architectures: RetNet (Sun et al., 2023), and Mamba (Gu & Dao, 2023). For the base Pre-Trained Language Model (PTLM), we used the Pythia-410m222Pretrained weights found at: https://huggingface.co/EleutherAI/pythia-410m-deduped (Biderman et al., 2023) model’s 100k checkpoint, which is trained on 200B tokens of the deduplicated Pile (Gao et al., 2020) dataset. Note that this model is based on the GPTNeoX (Black et al., 2022) architecture, and uses parallel residual connections. The LCI architectures may deviate from their original proposed architectures for FFN or residual connections to be compatible with this architecture. As the biggest difference comes from the SSM blocks or the linear attention blocks, we assumed these minor architectural differences to be less significant. Moreover, in the case of Mamba, we interleaved the Mamba blocks with FFN blocks, as opposed to homogeneous architecture of the original Mamba paper. This makes them compatible for weight transfer from Transformers. We name this model as StripedMamba."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Since we are transferring the weights from a PTLM, we trained the LCI models on the pretraining dataset used to train the PTLM that it was transferred from. As the main PTLM used was Pythia-410m model, we used the deduplicated Pile (Gao et al., 2020) dataset. Dataset found at: https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Implementation Detail",
            "text": "We focused on two popular architectures: RetNet and (Striped-)Mamba. The model configurations follow the original pythia-410m for the most part, except for the detailed configuration of each LCI block. For RetNet, the head size was set to 256, following the suggestion of the original paper, and for StripedMamba, the state dim, convolutional dim, and expand factor follow the original configuration found in the related paper.\n\nWe trained the 410m-sized models with Distributed Data Parallel using the Pytorch-Lightning framework and litgpt codebase. We used flash-attention v2 for MHA and Hybrid models. For 1B models, they were trained with Fully Sharded Data Parallel strategy.\n\nWe used AdamW optimizer with betas 0.9, 0.95. The learning rate was set to 3e-4, with cosine annealing scheduler with minimum learning rate at 3e-5 and warmup steps of 2000. The weight decay was set to 0.1. We used a batch containing 1024 examples, each spanning 2048 tokens. The models were trained with the same schedule as the pythia paper but stopped early at 150 billion training tokens, which translates to 75,000 steps. The training took 3 days on 32 NVIDIA A100 80GB PCie GPUs. For 1B, in the interest of time, they were stopped at 100B tokens."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In this section, we will analyze the empirical performance of models trained with the proposed XATL training on various LM benchmarks. The benchmarks include commonsense reasoning benchmarks, namely HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2019), ARC Challenge (Yadav et al., 2019), and WinoGrande (Sakaguchi et al., 2019). We follow the standard procedures and report the accuracy of the models on the corresponding benchmarks."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Effect of Weight Transfer",
            "text": "We first observe the effect of weight transfer on two model sizes, 400M and 1B. We can observe that for Multi-Head Attention (MHA), which serves as a baseline, there is a consistent increase in performance as more weights are transferred. However, for RetNet, transferring negatively affects performance. This might be because the retention outputs show strong differences from the trained self-attention outputs before normalization and therefore it was incompatible with weights. Also, copying the embeddings only did not prove to be beneficial, as the average performance did not surpass the RetNet model trained from scratch. For RetNet, the most beneficial XATL configuration was to transfer the input/output embeddings and the FFN layers, and we used this configuration for other architectures. \n\nFor StripedMamba models, it can be noticed that it shows stronger performance than RetNet and even attention models, reaffirming the findings of the Mamba paper. We observed a strong performance boost at the same number of tokens when the embeddings and FFN layers were transferred. Finally, for the Hybrid models, there was a performance boost in all dimensions, except for the Arc-Challenging subset and WinoGrande benchmark.\n\nWe also conducted experiments with 1B RetNet models, which were trained up to 100B tokens of the Pile-Deduped dataset. We observed a similar trend where the models with transferred weights from Pythia-1B show stronger performance at the same number of tokens, and Hybrid models consistently outperform the non-Hybrid counterpart.\n\nAnother advantage of Cross-Architecture Transfer Learning is that it boosts the initial training performance significantly, which can greatly help when the compute budget is limited. We show that XATL training at 10B tokens is as performant as the 40B token checkpoint of the same model trained from scratch, and RetNet-430M-XATL at 60B token already surpasses the RetNet-430M Scratch model at 150B, suggesting a 2.5x reduced cost for the same performance."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Effect of Freezing",
            "text": "Next, we experimented with how the freezing of the transferred weight affects the training performance. LIT stands for Loss Improvement Threshold, where the transferred weights are frozen initially and unfrozen when the loss improvement compared to the previous checkpoint falls below a threshold. We found that in both configurations of weight transfer, freezing the weights affected negatively as the number of trainable weights are reduced towards the end, and LIT training to be marginally stronger than unfreezing the weights from the beginning. Hence, we simply chose to unfreeze the transferred weights, as they did not show much difference in performance empirically, and the implementation and training procedure is simpler."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Comparison Against Open-Source Models",
            "text": "We compared the performance of models trained by us with the open-source LMs of similar sizes. The results are shown in Table 4. The table is adapted from the Mamba paper and includes the performance of models trained to 300B tokens of the Pile dataset. As we did not train models to 300B tokens for our experiments, we included evaluation results from earlier checkpoints of the models found publicly. Also, for Pythia-410M models, we have reproduced the training from scratch in the previous experiments, and also included their performance (Pythia-410m-repr in the Table 4).\n\nWe could observe that in the 400M model range at 150B tokens, XATL can train RetNet models that are on par or stronger than transformers, especially when used in Hybrid with attention. This shows the strength of directly transferring compatible weights, as the same sized RetNet trained from scratch does not surpass the performance of Pythia-410M.\n\nWe could also observe a similar trend in the 1B sized model realm, with the XATL version of RetNet model staying on par with the Pythia-1B model, and the Hybrid version surpassing the performance of the Pythia-1B model, from which the weights are copied from.\n\nAlthough the RetNet-XATL models did not surpass the performance of the Transformer (Pythia) even at 1B size, this is somewhat expected, as the original paper of the RetNet also reported a performance lower Transformer at 1B size (Sun et al., 2023), which was re-affirmed by our reproduction of training from scratch in Table 2. We stress that XATL consistently showed improvement of performance at the same number of tokens, which was what allowed for the RetNet models to stay on par with the Pythia models of the similar size."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations and Future Works",
            "text": "Due to computational cost, the models were not overtrained up to 300B tokens, which would allow for full comparison with the other open source models. To make a fair comparison, we performed evaluation of the open source models with their public intermediate checkpoints. Also, it has been shown in the RetNet paper that the RetNet models start to surpass the performance of Transformer at 3B size. Moreover, the Pythia scaling suite (Biderman et al., 2023  ###reference_b7###) observed a phase change phenomenon starting from 3B size, which together suggests that 3B and above is a interesting model size to experiment the effect of XATL in a longer training scenario. We plan to carry out this direction after this version.\nAs the focus of this work was to show that the LCI models need not be entirely trained from scratch and either reduce the training time or improve the performance, we did not dive deep into the specifics of the LCI specific evaluations, such as measuring associative recall (Arora et al., 2023  ###reference_b5###) or inference cost analysis. As this is not the main contribution of this work, we refer the readers to the original papers for this."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed a weight transfer learning paradigm called Cross-Architecture Transfer Learning for LCI models, alleviating the need to train new architectures from scratch by transferring compatible components’ weights from the pre-trained models. We have conducted various experiments regarding which components are the most effective when transferred, whether to freeze the transferred weights or not, and how does the XATL help attention-hybrid architectures. We also compared the models trained with the best configurations with the open source models of similar sizes, and trained models that stays on par or outperforms similar sized SOTA models on the same number of training tokens, which was not possible when trained from scratch. XATL has shown its effectiveness by significantly improving the performance of the LCI models at the same compute budget and reducing the pre-training compute to achieve the same performance as training from scratch."
        }
    ]
}