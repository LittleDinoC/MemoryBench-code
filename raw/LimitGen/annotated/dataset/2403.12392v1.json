{
    "title": "AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis",
    "abstract": "Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet’s gender classification (99.34% accuracy), and poetry sub-meter classification (97.79% accuracy). In addition, the model achieved an accuracy score in poems’ rhyme classification (97.73% accuracy) which is almost equivalent to the best score reported in this study. Moreover, the proposed model significantly outperformed previous work and other comparative models in the tasks of poems’ sentiment analysis, achieving an accuracy of 78.95%, and poetry meter classification (99.03% accuracy), while significantly expanding the scope of these two problems. The results demonstrate the effectiveness of the proposed model in understanding and analyzing Arabic poetry, achieving state-of-the-art results in several tasks and outperforming previous works and other language models included in the study. AraPoemBERT model is publicly available on https://huggingface.co/faisalq.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The Arabic language is one of the world’s most widely spoken languages. It has a rich history and its influence is seen across various domains including media, politics, history, and art. Arabic poetry, a prominent part of Arabic literature and culture, serves as a window to the norms, values, and historical events of the Arab world. An Arabic poem typically consists of one or more verses, each verse usually composed of two halves known as hemistiches. All verses in a specific poem share the same rhyme and meter, creating a rhythmic pattern that adds to the beauty and depth of the poem. The rhyme is a repeating pattern of sounds that occurs at the end of each verse, while the meter dictates the rhythmic structure of the verse. Arabic poetry spans a wide range of topics, encapsulating the poet’s genuine emotions and thoughts. These topics can range from romance and longing to spiritual devotion, each offering a unique perspective and depth of emotion.\n\nClassical Arabic poetry adheres to a set of established meters, each with its unique rhythmic pattern. These meters are very important as they provide the poem with its rhythmic structure and flow. The science of Arabic prosody involves the use of Tafaeil or poetic feet. These are groups of ten expressions that scholars have agreed upon as the standard for weighing Arabic poetry. The ten feet in Arabic prosody are composed of specific letters: Faa, Ain, Lam, Noon, Meem, Seen, Taa, and vowels. These feet correspond in weight to the letters of the measured words in the poem verse, matching vowels with vowels and consonants with consonants. However, modifications to these poetic feet can occur, altering their ideal image. These modifications can involve omitting, adding, or silencing parts of the feet. Scholars of ’Arud have differing views on these changes, with some approving and others disapproving.\n\nTypically, the meters used in Arabic poems are in their complete form, meaning all the original Tafaeil of a specific meter are used, except for a few meters that must be in a fragmented form. However, poets sometimes omit parts of the Tafaeil from the original meter, resulting in a variant of that meter. Scholars of ’Arud have identified seven different variants that could be derived from classical meters. Whereas some meters always appear with a certain variant such as ’Complete’, while others may come in more than one variant. In this study, we will refer to the combination of meters and their existing variants as ’sub-meters’, which include the name of the meter and its variant.\n\nIn contrast to classical meters, non-classical meters allow for more flexibility and diversity in constructing the poem. The usage of these patterns adds to the beauty of Arabic poetry, making it an enjoyable experience, either when reading silently or reciting out loud. Non-classical meters appeared chronologically after the classical meters and are often featured in Nabati poetry, which translates to folk poetry. The rise of these meters is closely associated with the prevalence of colloquial speech. The names and numbers of these meters vary among scholars, and their classification is often influenced by the era and region under consideration. However, identifying poems’ meters manually poses a challenge. It necessitates an understanding of the language and its scientific study of ’Arud, as well as a keen ear for recognizing rhythm and sound patterns. This task becomes more demanding when dealing with non-classical rhythmic patterns (meters) that are more flexible and sometimes do not adhere to specific rules.\n\nThe goal of conducting a thorough analysis or solving different problems related to Arabic poetry has led to the development of various methods and techniques. One promising solution is the use of language models that can analyze and learn from text data. By pretraining a language model on a dataset of poems and fine-tuning it using verses text and their corresponding labels, we can create a system capable of accurately identifying the meter or rhyme of a given poem or a verse. This approach not only saves time and effort but also opens up new possibilities for analyzing and classifying Arabic poetry. In this paper, we present AraPoemBERT, a new BERT-based language model pretrained from scratch exclusively on Arabic poetry text. We provided a comprehensive evaluation of its performance in comparison with other Arabic language models on five different tasks related to Arabic poetry. We believe that AraPoemBERT has potential for the future of Arabic poetry analysis, serving as a valuable tool for scholars and researchers in linguistic, Arabic literature, and natural language processing (NLP) fields.\n\nThe main contributions of this paper can be summarized as follows:\n- Presenting a new language model pretrained from scratch, dedicated solely to Arabic poetry.\n- Reporting state-of-the-art results in 4 out of 5 different NLP tasks related to Arabic poetry using the proposed model compared to previous work and other prominent language models.\n- We are the first to explore and report the results for 3 new tasks: poet’s gender, poetry sub-meters, and poetry"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Arabic Poetry Analysis",
            "text": "In recent years, the natural language processing (NLP) research related to Arabic poetry has focused mainly on two tasks: poems meters classification, and poems sentiment analysis using machine learning models. Several researchers in the past have proposed rule-based algorithms aiming to classify poetry meters. These approaches convert the input text into its Arudi form using regular expressions or Khashan’s ”numerical prosody” method, and subsequently they determine the meter of the target verse or poem. These systems heavily rely on diacritizing the input text and necessitate an understanding of the Arudi field to create effective rules to be used in these systems.\n\nBerkani et al. suggested a pattern recognition extraction and matching approach for poems meter detection. This method involves extracting a group of patterns from a target verse and comparing them to a set of labeled patterns. If the extracted pattern matches any of the labeled ones, the system can identify the meter of the input verse. The reported accuracy of this approach reached 99.3% when tested on a dataset consisting of 2,711 verses. However, it should be noted that common poets’ practices, such as text vocalization or minor imperfections in the poem, can potentially impact the system’s accuracy.\n\nSimilarly, Shaibani et al. proposed a novel approach utilizing five bidirectional gated recurrent unit (BiGRU) layers, and used character-based encoding for text representation. The researchers collected a set of poems comprising 55,440 verses categorized into 14 meters only, and achieved an overall accuracy of 94.32%.\n\nSimilarly, Abdandah et al. introduced a new machine learning model that contains four bidirectional long short-term memory (BiLSTM) layers.\n\nSimilar to the attempts at classifying poem meters, numerous methods have been suggested in literature towards poems sentiment analysis. Mohammad presented a Naive Bayes approach for classifying poems into seven different categories Hekmah (Wisdom), Retha (Elegy poems), Ghazal (Spinning poems), Madeh (Praise), Heja (Satire), Wasef (Description poems), Fakher (self-glorification), and Naseeb (contentment) using 20 Arabic poems with six verses each, and had achieved an accuracy of 55%.\n\nAlsharif et al. classified Arabic poems into four classes: Retha (Elegy poems), Ghazal (Spinning poems), Fakhr (self-glorification), and Heja (Satire) using Naive Bayes and support vector machine (SVM) models. They used a dataset composed of 1231 poems comprising 20041 verses, and they have achieved an F1-score of 0.66 as the highest result reported in their work.\n\nSimilarly, Ahmed et al. proposed three machine learning models for classifying Arabic poems into 4 types: love, Islamic, political, and social. They have used Naive Bayes, SVM, and linear support vector classifier (SVC) and had achieved an average F1-score of 0.49, 0.18, and 0.51 respectively.\n\nShahriar et al. measured the performance of different deep learning models like LSTM, GRU, and CNN in the task of classifying Arabic poetry emotions. They have used 9452 poems divided into 3 classes: joy, sadness, and love. Additionally, the authors employed AraBERT model, a BERT-based model that was pretrained on Arabic text. The fine-tuned model has achieved an F1-score of 0.77 which is significantly higher compared to the other deep learning models used in the same study that have achieved an F1-score between 0.53 and 0.62."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Arabic Language Models",
            "text": "Looking specifically at Arabic language models, multiple models have been proposed in literature since the introduction of Transformers by Vaswani et al. AraBERT, introduced by Antoun et al., is a BERT-based language model that was pretrained on a large Arabic language corpus. The 24 GB dataset is composed of Arabic news articles obtained from two publicly available corpora: 1.5 billion words Arabic corpus, and OSIAN: the Open Source International Arabic News Corpus. Additionally, the authors scraped manually more news articles from various online sources. The main reason for creating AraBERT was the need for a large language model designed specifically for the Arabic language. The authors presented two variants of AraBERT: AraBERTv0.1 and AraBERTv1. The main difference between the two is that in AraBERTv1 the authors used an Arabic text segmenter before training the tokenizer and then tokenizing the text. Whereas in AraBERTv0.1, the Farasa segmenter was not used. When evaluating both models, AraBERTv1 outperformed AraBERTv0.1 on six different tasks, whereas the latter achieved higher results in the remaining three tasks. Each of these variants are available in two different sizes: \"base\" and \"large\", similar to the original BERT’s two sizes. AraBERT has shown impressive performance on various Arabic NLP tasks, outperforming other multilingual models that were pretrained on multiple languages including Arabic. The model has achieved state-of-the-art (SOTA) results when tested on Arabic NLP tasks such as text classification, named entity recognition, and question answering.\n\nSimilarly, Chowdhury et al. proposed another BERT-based language model called QARiB. The presented model was pretrained on text acquired from different sources including posts from Arabic news channels written in modern standard Arabic (MSA), and tweets from well-known Twitter accounts that are written mostly in dialect Arabic. The QARiB model has achieved higher results than AraBERT in text classification tasks on newly prepared datasets containing some text written in dialectal Arabic which shows that language models can achieve better generalization when being trained on both formal and informal text.\n\nAbdul-Mageed et al. introduced a new BERT-based model called ARBERT pretrained on 61GB of MSA text collected from Arabic Wikipedia, online free books, and publicly available corpora, mainly OSCAR. The model employs a vocabulary of 100K different tokens, and was pretrained using the same configuration as the original BERT. The authors also introduced another model, MARBERT, that was pretrained on a different dataset composed only of tweets written in both MSA and diverse Arabic dialects. The model is designed for downstream tasks that involve dialectal Arabic. However, in this study, we excluded MARBERT because most of Arabic poetry is written in classical or standard Arabic. Both models, ARBERT and MARBERT, achieved SOTA results across the majority of tasks when compared with AraBERT and other multilingual models.\n\nInoue et al. proposed a new model under the name CAMeLBERT. In their paper, they developed four different variants of CAMeLBERT: MSA, dialectal Arabic (DA), classical Arabic (CA), and mix. Each variant was pretrained on different datasets that contain a certain type of Arabic text, except for the mix variant that was pretrained on all datasets combined (167 GB). The authors compared the new model variants with AraBERT, MARBERT, ARBERT, and other multilingual models. They showed when experimenting on tasks that involve dialectal Arabic, CAMeLBERT-DA outperformed all other models including MARBERT. Additionally, CAMeLBERT-CA outperformed all other models in the Arabic poetry task, which is the only task that is designed for evaluating language models on classical Arabic text.\n\nIn this study, we used AraBERTv1, AraBERTv0.1, ARBERT, and QARiB as comparative models to our proposed model due to their wide acceptance in literature when tackling various Arabic NLP problems. Also, we employed CAMeLBERT-CA model in this study for being mainly designed to tackle tasks that involve classical Arabic text."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Transformers",
            "text": "Transformers, introduced by Vaswani et al. [22  ###reference_b22###], are the building block for all modern language models. They primarily utilize a mechanism called self-attention to measure the significance of different parts in the input sequence to each other. Figure 1  ###reference_### shows the general architecture of Transformer model.\nGiven a sequence of input vectors, the self-attention mechanism computes a weighted sum of these vectors using attention scores. The core components of the self-attention mechanism are the query (Q), key (K), and value (V) matrices. These are derived from the input vectors.\n###figure_3### The attention scores are computed as:\nwhere  is the dimension of the key vectors.\nWhile the self-attention mechanism allows the model to focus on different parts of the input, the multi-head attention mechanism allows the model to focus on different parts in different representation subspaces of Q, K, and V matrices. Essentially, it runs the self-attention mechanism multiple times in parallel, each with different learned linear projections of the original Q, K, and V.\nGiven  different sets of Q, K, and V matrices, the multi-head attention is computed as:\nwhere each head is computed as:\nand , , , and  are the parameter matrices.\nHowever, since Transformers inherently lack a sense of order or position, the authors also proposed another mechanism called ”positional encoding” that can give the model information about the position of words in a sequence, since all words or tokens are being processed in parallel. To address this, positional encodings are added to the embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the input embeddings, allowing them to be summed. The word’s position  and each dimension  of the word embedding, the positional encoding is defined as:\nWhere  is the dimension of the embeddings. These sinusoidal functions were chosen because they can be easily learned if needed, and they allow the model to interpolate positions of words in long sequences.\nThe original Transformer model follows the encoder-decoder structure, where the encoder processes the input sequence and the decoder generates the output sequence. This architecture makes the original Transformer model particularly suitable for text-to-text tasks such as machine translation and paraphrase generation.\nBuilding upon the Transformer’s architecture, the Bidirectional Encoder Representations from Transformers (BERT) has brought about significant advancements in natural language processing (NLP) [31  ###reference_b31###]. BERT is an encoder-only Transformer that analyzes and processes input text bidirectionally, unlike the original encoder-decoder Transformer model that reads text sequentially. One of BERT’s capabilities is the ability to grasp the complete context of a word by considering its surrounding words. This is achieved through the ”masked language model” (MLM) training objective, which randomly masks a percentage of input tokens and then asks the model to predict them based on the context provided by the other unmasked tokens. Figure 2  ###reference_### shows an example of the MLM training objective. This bidirectional approach allows BERT to accurately comprehend the context and the meaning of each word in a sentence, especially when dealing with words that have different meanings based on their usage and surrounding words.\n###figure_4### Additionally, BERT excels in transfer learning, which enables it to apply previously learned knowledge to different NLP tasks. Once the model has been pretrained using a large amount of text, it can be further refined by adding just one extra output layer. This allows for generating models for tasks including question answering and language inference without the need for significant modifications to the model architecture, or the need for re-traing the model for scratch. This adaptability makes BERT highly versatile and efficient, ensuring high performance across a wide range of NLP tasks. Figure 3  ###reference_### shows the pretraining process of BERT language model.\nThe authors of BERT model have developed two different sizes of BERT: BERT-base and BERT-large. BERT-base is a model with 12 transformer blocks (layers), 768 hidden units (output vector size), and 12 attention heads for each layer, resulting in a total of 110 million parameters. BERT-large is a much larger model with 24 layers, 1024 hidden units, and 16 attention heads. It contains a total of 340 million parameters, which is 3 times larger than the base model. Both variants have been pretrained on the same dataset, but due to its larger size and complexity, BERT-large generally achieves better performance on different NLP tasks. However, it requires more computational resources and longer training time.\n###figure_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Model: AraPoemBERT",
            "text": "In this section we describe the proposed model architecture, and the dataset used in the study."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Model Architecture",
            "text": "Building upon the success of AraBERT and many other Arabic language models, we developed AraPoemBERT, a BERT-based model that was pretrained from scratch exclusively on Arabic poetry text. The model follows the same architecture as the original BERT model in terms of the number of attention heads (12 attention heads per layer) and the size of the hidden layer (768 units). Also, we used wordpiece tokenizer [32  ###reference_b32###] similar to the original BERT model. However, AraPoemBERT contains 10 encoder layers, compared to 12 layers in BERT-base, and the vocabulary size of our model was set to 50,000, allowing it to capture a wide range of words and expressions found in Arabic poetry. Finally, the maximum sequence length is set to 32 tokens per sequence. The main reason for limiting the sequence length in AraPoemBERT to such a small number is due to the average length of poems’ verses, where the majority of verses can be fully stored within a 32-token sequence. Figure 4  ###reference_### shows that 99.3% of sequences (a whole verse) contain between 6 and 18 tokens after tokenization. However, having a smaller sequence length does significantly reduce the model pretraining time, because it enables using larger batch sizes without causing any out-of-memory issues, even with a commodity GPU.\n###figure_6###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "In this study, we used AraPoems555AraPoems dataset is available on https://doi.org/10.7910/DVN/PJPWOY  ###reference_### dataset for pretraining AraPoemBERT and fine-tuning different models used in the downstream tasks. The dataset is collected from two online sources specialized in Arabic poetry, Almausua [33  ###reference_b33###] and Aldiwan [34  ###reference_b34###], and it contains 2,090,907 verses associated with a variety of information such as meter, sub-meter, poet, rhyme, era, and topic. See Figure 5  ###reference_### for a sample of the dataset. Figure 6  ###reference_### shows the distribution of verses across different categories. Additionally, we translated all these information to the English language manually, and we also labeled the poets’ gender based on their names.\nThe dataset underwent a cleaning process which includes removing duplicate verses, and removing any irrelevant characters from the corpus such as digits, English letters, and unwanted symbols like ‘@’, ‘#’, and ‘$’.\n###figure_7### ###figure_8###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "In this section, we present the text preprocessing steps, the pretraining procedures, and the downstream tasks along with the results. All experiments were conducted on a local machine equipped with AMD Ryzen-9 7950x processor, 64GB DDR5 memory, and two GeForce RTX 4090 GPUs with 24GB memory each. The software environment was set up on Ubuntu 22.04 operating system. The Huggingface transformers library was used in pretraining our model, in addition to downloading and fine-tuning the language models from the Huggingface hub that were used in this study. Additionally, CUDA 11.8 was used to take advantage of GPU acceleration for efficient computations."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Text Preprocessing",
            "text": "Before using the verses’ text in pretraining the model, a few preprocessing steps were required to ensure the data is in a suitable format for the model. The first step in the preprocessing phase was to remove the diacritics, which simplifies the text and reduces the pretraining time. Removing the diacritics from the text was conducted using PyArabic, a specialized Python library for manipulating and normalizing Arabic text. The second step involves removing all symbols such as colons, brackets, and question marks from the poetry corpus. These symbols can introduce noise into the data and potentially affect the performance of the model. Moreover, due to the difference between the structure of regular text and Arabic poetry, where the verses are divided into two parts, known as hemistiches, and both hemistiches are required to form a complete sentence. And at the same time, we want to facilitate the model’s understanding of the verse structure. Thus, we added two additional unique tokens: ’[s]’ and ’[e]’, where the ’[s]’ token is used as a separator between the first and second hemistiches in a verse, and if a verse contains only the first hemistich, the ’[e]’ token will be placed after the ’[s]’ token to represent an empty second hemistich. This approach allows the model to recognize the structure of the verses and differentiate between verses with one or two hemistiches. The final step in the preprocessing phase involves training the tokenizer and then using it whenever required to tokenize the input text. For AraPoemBERT, we employed the Huggingface implementation of the WordPiece tokenizer, which we trained on the same poetry text with a vocabulary size of 50,000 wordpieces."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Model Pretraining",
            "text": "The original BERT model was pretrained on two objective tasks: the masked language model (MLM), and next sentence prediction (NSP). In the MLM task, a certain percentage of tokens in the input sequence are masked and the model’s goal is to correctly predict what these masked tokens are. The NSP task involves providing the model with two sentences and asking it to determine if they are related (from the same paragraph) or not. However, AraPoemBERT was pretrained solely on the MLM task objective which reduces the pretraining time and potentially achieves better performance in the downstream tasks, following the recommendations of the RoBERTa model’s authors [37]. Our model was pretrained by masking 15% of the sequences’ tokens, a batch size of 256, ’AdamW’ [38] as the model optimizer with a learning rate of 5e-5 and weight decay equal to zero, and a dropout rate of 0.1 for all dropout layers. To reduce training time and optimize GPU memory usage, we utilized the (mixed precision) datatype ”FP16” for gradient computations. With the aforementioned configurations, the model was pretrained for 800k steps (980 epochs) and it took 142 GPU hours, whereas the minimum loss reached was 2.02. In this stage, we have used all the collected poetry text which is composed of more than 2.09 million verses. The size of the text file used in pretraining the model was 182 MB and contains more than 19.22 million words or 29 million tokens after tokenization. Even though the dataset is small in size compared to other BERT-based models, Arabic poems are very distinctive and diverse, and language models in general require between 10M and 100M words to learn most of the syntactic and semantic features [39]."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Downstream Tasks",
            "text": "To demonstrate the effectiveness of AraPoemBERT model, we assessed its performance on five different downstream tasks related to Arabic poetry analysis. In this study, we used AraBERTv1, AraBERTv0.1, ARBERT, CAMeLBERT-CA, and QARiB as comparative models to our proposed model. All language models were fine-tuned using the same settings and hyperparameters used in the pretraining process. The models were fine-tuned using 80% of the task-related labeled data, and the remaining 20% were reserved for validation, while ensuring that the same validation set was used when evaluating different models within the same experiment."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Sentiment Analysis",
            "text": "Many poems in the dataset are labeled with different types of topics such as: Romantic poems, Elegy poems, Invocation poems, etc. However, we grouped relevant types of poems into a single emotion, resulting in four different classes: Love, Sadness, Anger, and Spirituality. The resulting dataset labeled with these four emotions contains 21,230 poems composed of 315,877 verses.\n\nAs shown in Table 5, our model has outperformed all other language models with an accuracy score of 78.95% which is significantly higher than the second score achieved by CAMeLBERT-CA model, while the lowest accuracy results are achieved by both AraBERT models (75.35% and 75.79%).\n\nTable 6 presents the classification scores achieved by AraPoemBERT for each sentiment class in the validation set. The model attained high scores in the classes \"Love\" and \"Spirituality,\" while it wrongly classifies \"Sadness\" and \"Anger\" with the class \"Love\" as shown in the confusion matrix, which explains the low F1-score for these two classes. Overall, AraPoemBERT has achieved significantly higher results compared to previous work by Shahriar et al. In their work, they targeted three emotion classes only, and achieved an accuracy of 76.5% with a dataset composed of 9452 poems, which is less than half the number of poems used in this study. Additionally, in their work, the proposed models require processing whole poems to accurately detect the main sentiment. However, in this study, we achieved higher accuracy by merely evaluating individual verses to predict the related poem’s main sentiment, which shows the significance of language models pretrained on domain-specific texts such as AraPoemBERT, or CAMeLBERT-CA which was pretrained solely on classical Arabic text."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Poetry Meters",
            "text": "The task of classifying Arabic poetry meters has proven to be quite challenging in literature, especially when tackling a dataset composed of a large number of verses, and at the same time, multiple meters are underrepresented in the dataset. We have conducted two classification tasks; one focused on classical meters only (16 meters) to compare our model accuracy with existing work, and the second classification task includes both classical and non-classical meters (28 meters). Table 7 shows a list of classical and non-classical meters found in the AraPoems dataset.\n\nIn Table 8, we compare our model with other language model and machine learning models presented in literature. AraPoemBERT has achieved the highest accuracy score in both tasks outperforming other models, including proposed approaches from previous work. Regarding the classification task that targets all poetry meters, which includes an additional 12 non-classical meters, we compared our model with other language models only, because to the best of our knowledge there were no published results in literature that cover this area.\n\nTable 9 shows the prediction results of our model in classifying the 16 classical meters. These results include precision, recall, and F1 scores for each class, in addition to the number of verses for each meter in the validation set. The model has successfully achieved an accuracy of 99% in six different meters (1.Taweel, 2.Kamel, 3.Baseet, 4.Khafif, 5.Wafer, and 8.Mutaqarib), setting a new state-of-the-art result that has not been reported in previous work except for the ’Taweel’ meter only. The model also achieved an accuracy score of 97% and 98% in another five meters (6.Rajaz, 7.Ramel, 9.Saree, 10.Munsarih, and 11.Mujtath) despite the fact that the total number of verses related to ’Munsarih’ and ’Mujtath’ meters are relatively small compared to other meters. Similarly, the number of samples for ’13.Madeed’ and ’14.Mutadarak’ meters are also small but the model has achieved an accuracy score of 93.83% and 94.41% respectively. However, the accuracy scores of ’12.Hazaj’ and ’15.Muqtadab’ are a little lower, because the model wrongly classifies 15% of ’Hazaj’ verses as ’Rajaz’, and 10% ’Muqtadab’ verses as ’Khafif’ as shown in the confusion matrix in Figure 9. Lastly, the accuracy score of ’16.Mudari’ meter is the lowest; this is due to the small number of verses for this meter, and the model wrongly classified 55.6% of this meter’s samples as ’Rajaz’ as shown in the confusion matrix.\n\nThe prediction results of AraPoemBERT in classifying all meters (16 classical and 12 non-classical) are shown in Table 10. The model has achieved similar accuracy scores for the classical meters even when including the other meters in the classification task. Notably, the model achieved a score of 93.92% with the (20.Doubeet) meter, and an accuracy score between 70% and 85% in ’17.Muashah’, ’18.Free form’, and ’19.Colloquial’ meters. However, the model achieved an accuracy score between 49% and 61% in the meters ’21.Mawalia’, ’22.Masehube’, and ’23.Selselah’ because of the small number of samples for these meters, and the model wrongly classifies them as different meters such as ’Muashah’, ’Colloquial’, and ’Kamel’ as shown in the confusion matrix in Figure 10. Finally, the model couldn’t detect and correctly classify any of the last five meters (24.Zajal, 25.Kankan, 26.Hajini, 27.Sakhri, and 28.Luaihani) because the number of samples for each of these meters is less than 100 verses and detecting them by the model currently seems unfeasible."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Poetry Sub-Meters",
            "text": "To further expand the problem of classifying classical meters, we include what is called meter’s variants or sub-meters to the poetry meters classification task. The majority of verses in the newly compiled dataset are labeled with a specific meter and a meter’s variant. However, in order to reduce the classification problem complexity, we combined both meters and their variants to form a new set of labels. For example, ”Khafif” meter comes in two variations: ”Complete” and ”Majzuu”, thus, their combination will result in two different classes: ”Khafif Complete” and ”Khafif Majzuu”. In this study, we will refer to the combination of meters and their variants as ’sub-meters’. After combining the meters and their variants into combined classes, we ended up with a total of 33 different sub-meters. However, we excluded sub-meters classes from the classification task if they contain about 100 verses or less, which resulted in the removal of seven sub-meters from the experiment. The remaining 25 sub-meters, which account for approximately 88.48% of all verses in the dataset, will be the focus of the classification task.\n\nThe classification report of the validation set for AraPoemBERT shows that the model has achieved an F1-score over 0.98 in all classes with the ’Complete’ variant, except for ’Mutadarak Complete’ and ’Rajaz Complete’ due to the low number of samples for these two sub-meters. Also, the F1-score was below 0.4 for the sub-meters that contain a low number of verses that are less than 1200 in the validation set. Figure 2 shows the confusion matrix of the validation set using AraPoemBERT in the sub-meters classification task. To the best of our knowledge, this is the first study in literature that directly focuses on the problem of classifying meters variants. However, compared to the results of classifying classical meters in previous work, the accuracy of classifying sub-meters using AraPoemBERT is better than the results reported by Abandah et al. (97.79% VS 97.27%) even after increasing the complexity of the problem, but significantly lower when compared with the best accuracy score reported in the previous task when plainly classifying classical meters without any consideration to their variants (97.79% VS 99.03%)."
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Poet’s Gender",
            "text": "The dataset originally did not contain any information regarding poets’ gender. Thus, we manually annotated the poets’ gender based on their names. The dataset contains a total of 5,383 poets, of which 5,023 are males and 360 poets are females. Almost all verses in the dataset, specifically 2,087,557 verses, are associated with known poets. Table 13 shows the overall accuracy results for all the models. Table 15 presents the classification report for AraPoemBERT, which shows that the model has achieved an F1-score of 0.2246 for the Female class even though it is extremely underrepresented compared to the Male class, and 99.12% weighted average accuracy for both classes."
        },
        {
            "section_id": "5.3.5",
            "parent_section_id": "5.3",
            "section_name": "5.3.5 Poem’s Rhyme",
            "text": "In the task of rhyme classification, the verses in the dataset are labeled with 31 different rhymes. These rhymes include all 28 Arabic letters, in addition to the rhymes: Laa, Taa Marbutah, and Waw Hamza which are variants of the letters Lam, Taa, and Alif, respectively, but they are written differently and have slightly different sounds. This classification task aims to accurately identify the rhyme of each verse, providing further insight into the structure and style of the poem.\n\nIn this task, all models have achieved similar results. CAMelBERT-CA scored the highest with an accuracy of 97.76%, and AraPoemBERT achieved an accuracy of 97.73%. Figure 11 presents the confusion matrix for the validation set prediction results of AraPoemBERT which shows that the model can accurately identify the rhymes if it is one of the original 28 Arabic letters. The remaining three rhymes (Laa, Taa Marbutah, and Waw Hamza) which are variants of the letters (Lam, Taa, and Alif), are where the model scores the lowest. This is due to multiple reasons. For instance, the number of samples for these rhymes is very small especially for ”Taa Marbutah” and ”Waw Hamza” rhymes. Also, the model wrongly classifies ”Laa” as ”Lam” for 24.5% of the samples."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we presented AraPoemBERT, a new BERT-based language model pretrained from scratch on Arabic poetry text. In addition, we employed the proposed model along with the other Arabic language model on five different NLP tasks related to Arabic poetry. The target tasks include classifying poets’ genders, classifying poetry meters and sub-meters, sentiment analysis, and detecting verses’ rhymes. The presented results illustrate the effectiveness of utilizing transformer-based models in various tasks related to Arabic poetry, and the significance of using a domain-specific language model such as AraPoemBERT that was exclusively pretrained on poetry text compared to language models pretrained on general text such as AraBERT and CAMeLBERT. The model has achieved state-of-the-art results and outperformed the other language models in most of the tasks. Also, we have explored three new NLP tasks in Arabic poetry that have not been explored in literature before: classifying poets’ genders, classifying sub-meters, and detecting verses’ rhymes. The results achieved in these tasks will serve as a benchmark for future work. Additionally, more NLP tasks related to Arabic poetry should be explored, such as authorship attribution, era classification, automating the process of poem text diacritization, and distinguishing between poems written in standard or spoken Arabic. The dataset and the language model introduced in this paper will serve as valuable resources for future work in different domains and fields such as linguistics, artificial intelligence, Arabic literature, language processing, and cultural studies."
        }
    ]
}