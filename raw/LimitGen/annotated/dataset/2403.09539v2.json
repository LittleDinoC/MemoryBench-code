{
    "title": "Logits of API-Protected LLMs Leak Proprietary Information",
    "abstract": "The commercialization of large language models (LLMs) has led to the common practice of restricting access to proprietary models via a limited API. In this work, we show that, with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a small number of API queries (e.g., costing under $1,000 for OpenAI’s gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We exploit this fact to unlock several capabilities: obtaining cheap full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As large language models (LLMs) have become more capable and valuable, it has become increasingly common for companies and organizations to train closed-source LLMs and make them available only via an API. This setup may foster a false sense of security for LLM providers, who might mistakenly assume that information about their model architecture is private, and that certain types of attacks on their LLM are infeasible. On the flip side, users must seemingly take providers’ word that LLMs only change when the provider publicly announces version updates.\n\nDespite this apparent state of affairs, in reality many LLM APIs reveal much more information about their underlying models than previously thought. In this paper we show how to extract detailed information about LLM parameterization using only common API configurations. Our findings allow LLM clients to hold providers accountable by tracking model changes, recovering hidden prompts, and cheaply obtaining full vocabulary outputs. At the same time, our approach also allows LLM providers to establish unique identities for their models, enabling trust with their users as well as improved accountability.\n\nOur method exploits the low-rank output layer common to most LLM architectures by observing that this layer’s outputs are constrained to a low-dimensional subspace of the full output space. We call this restricted output space the LLM’s image. We can obtain a basis for this image by collecting a small number of LLM outputs, and we develop novel algorithms that enable us to accomplish this cheaply and quickly for standard LLM APIs. Obtaining the LLM image allows us to explore several capabilities that cover a broad set of applications, and empirically verify several of them.\n\nWe empirically show the effectiveness of using LLM images as unique signatures that can be used to identify outputs from a model with high accuracy, a useful property for API LLM accountability. The sensitivity of these signatures to slight changes in the LLM parameters also makes them suitable for inferring granular information about model parameter updates. Considering several proposals to mitigate this vulnerability, we find no obvious fix to prevent obtaining LLM images without dramatically altering the LLM architecture. While providers may choose to alter the API to hide this information, the relevant API features have valuable and safe use cases for the LLM clients, who may rely on access to features like logit bias.\n\nThough our findings may be viewed as a warning to LLM providers to carefully consider the consequences of their LLM architectures and APIs, we prefer to view our findings as a potential feature that LLM providers may choose to keep in order to better maintain trust with their customers by allowing outside observers to audit their model."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM outputs are restricted to a low-dimensional linear space",
            "text": "###figure_2### Consider the architecture of a typical LLM, depicted in Figure 2  ###reference_###.\nA Transformer outputs a low-dimensional contextualized embedding  (or simply embedding).\nProjecting the embedding onto  via the linear map defined by the LLM’s softmax matrix , we obtain the logits .\nBecause  is in , its rank (i.e., number of linearly independent columns) is at most .\nThe rank of a matrix corresponds to the dimension of the image of the linear map it defines, i.e., the vector space comprising the set of possible outputs of the function.\nIn other words, if the linear map  is defined as ,\nthen ’s image  is a -dimensional subspace of .\nThus, the LLM’s logits will always lie on the -dimensional111\nMore accurately, the logits will always lie on an at-most--dimensional subspace. For convenience, we assume full-rank matrices, and thus a -dimensional subspace.\n subspace of .\n\n###figure_3### ###figure_4### ###figure_5### ###figure_6### We now turn our attention to the LLM’s final output: the next-token distribution .\nDue to the softmax function, this is a valid probability distribution over  items,\nmeaning it is a -tuple of real numbers between  and  whose sum is .\nThe set of valid probability distributions over  items\nis referred to as the -simplex, or .\nPerhaps surprisingly,  is also a valid vector space\n(albeit under a non-standard definition of addition and scalar multiplication)\nand is isomorphic to a -dimensional vector subspace of  (Aitchison, 1982  ###reference_b1###; Leinster, 2016  ###reference_b12###).\nIn particular, it is isomorphic to the hyperplane  that is perpendicular to the all-ones vector , as illustrated in Figure 3  ###reference_###.\nThe softmax function is thus an isomorphism \nwhose inverse mapping  is the center log ratio transform (clr),\nwhich is defined as\nObserve also that the function\n\nprojects  linearly onto the nearest point in .\nBy this linearity,\nand the fact that  is a -dimensional subspace of ,\nwe can observe that  and  are also a -dimensional vector subspaces of  and  respectively.\nInterpreted, this means that the LLM’s outputs\noccupy -dimensional subspaces of the logit space , probability space , and .\nWe call these subspaces the image of the LLM on each given space.\nA natural consequence of the low-dimensional output space is that any collection of  linearly independent LLM outputs  form a basis for the image of the model,\ni.e., all LLM outputs can be expressed as a unique linear combination of these  outputs (Yang et al., 2018  ###reference_b20###; Finlayson et al., 2023  ###reference_b5###).\nThe rest of this paper discusses the implications and applications of this mathematical fact for API-protected LLMs,\nstarting with methods for finding the LLM image given a restrictive API,\nthen using the LLM image for various purposes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Fast, full outputs from API-protected LLMs",
            "text": "There are several uses for full-vocab LLM outputs,\nhowever most LLM APIs do not return full outputs.\nThis is likely because full outputs are large and expensive to send over an API,\nbut could also be to prevent abuse of APIs,\nsuch as using these outputs to distill models (Hinton et al., 2015  ###reference_b8###; Mukherjee and Awadallah, 2019  ###reference_b15###; Hsieh et al., 2023  ###reference_b9###)\nor discover proprietary information (see Section 4  ###reference_###).\nIn their paper, Morris et al. (2023  ###reference_b14###) give an algorithm\nfor recovering these full outputs from restricted APIs\nby taking advantage of a common API option that allows users to add bias to the logits for specific tokens.\nThe algorithm they describe requires  calls to the API top obtain one full output with precision .\nWe give an improvement that theoretically obtains full outputs in  API calls\nfor APIs that return the log-probability of the top- tokens.\nWe find that this improved algorithm suffers from numerical instability,\nand give a numerially stable algorithm that obtains full outputs in  API calls.\nNext, we give a practical algorithm for dealing with stochastic APIs\nthat randomly choose outputs from a set of  possible outputs.\nThis algorithm allows the collection of full outputs in less than  API calls on average.\nFinally, we reduce the number of API calls in all of the above algorithms from  to \nby adding in a preprocessing step to find the low-dimensional image of the LLM.\nThis speedup makes obtaining full LLM outputs up to 100 times faster and cheaper, depending on the LLM.\nTable 2  ###reference_### gives an overview of our algorithms with back-of-the envelope cost estimates for a specific LLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Fast, full outputs from APIs with logprobs",
            "text": "Our goal is to recover a full-vocabulary next-token distribution  from an API-protected LLM.\nWe will assume that the API accepts a prompt on which to condition the distribution,\nas well as a list of up to  tokens and a bias term  to add to the logits of the listed tokens before applying the softmax function.\nThe API returns a record with the  most likely tokens and their probabilities from the biased distribution.\nFor instance, querying the API with  maximally biased tokens,\nwhich without loss of generality we will identify as tokens ,\nyields the top- most probable tokens from the biased distribution \nwhere\nand  is the LLM’s logit output for the given prompt.\nAssuming that the logit difference between any two tokens is never greater than ,\nthese top- biased probabilities will be .\nFor each of these biased probabilites , we can solve for the unbiased probability as\n(proof in the Appendix A.1  ###reference_###).\nThus, for each API call, we can obtain the unbiased probability of  tokens,\nand obtain the full distribution in  API calls."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Numerically stable full outputs from APIs",
            "text": "In practice, the algorithm described in Section 3.1  ###reference_### suffers from severe numerical instability,\nwhich can be attributed to the fast-growing exponential term ,\nand the term  which quickly approaches 1.\nWe can eliminate the instability by sacrificing some speed and using a different strategy to solve for the unbiased probabilities.\nWithout loss of generality, let  be the maximum unbiased token probability.\nThis can be obtained by querying the API once with no bias.\nIf we then query the API and apply maximum bias to only tokens ,\nthen the API will yield  and .\nWe can then solve for the unbiased probabilities of the  tokens\n(proof in Appendix A.2  ###reference_###).\nBy finding  unbiased token probabilities with every API call,\nwe obtain the full output in  calls total."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Full outputs from stochastic APIs",
            "text": "Each of the above algorithms assume that the API is deterministic,\ni.e., the same query will always return the same output.\nThis is not always the case.\nFor instance, we find that OpenAI’s LLM APIs are stochastic.\nWhile this would seem to doom any attempt at obtaining full outputs from the LLM,\nwe find that certain types of stochasticity can be dealt with.\nIn particular, we model OpenAI’s stochastic behavior as a collection of  outputs \nfrom which the API randomly returns from.\nThis might be the result of multiple instances of the LLM being run on different hardware which results in slightly different outputs.\nWhichever instance the API returns from determines which of the  outputs we get.\nIn order to determine which of the outputs the API returned from,\nassume without loss of generality that  is the second highest token probability for all ,\nand observe that\nfor all outputs  and unbiased outputs  where tokens  and  are not biased (proof in Appendix A.3  ###reference_###).\nTherefore, by biasing only  tokens for each call,\nthe API will return  as well as  and  for some ,\nand we can determine which output  the result comes from by using  as an identifier.\nThus, after an average of  calls to the API we can collect the full set of probabilities for one of the outputs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Ultra-fast full outputs using the LLM image",
            "text": "So far, the dominating factor in each algorithm’s runtime is .\nWe now introduce a preprocessing step that takes advantage of the low-dimensional LLM output space to obtain  versions of all the above algorithms.\nSince  for many modern language models, this modification can result in multiple orders of magnitude speedups, depending on the LLM.\nFor instance, the speedup for an LLM like pythia-70m would be .\nThe key to this algorithm is the observation from Section 2  ###reference_### that  linearly independent outputs from the API\nconstitute a basis for the whole output space (since the LLM’s image has dimension ).\nWe can therefore collect these outputs\nas a preprocessing step in  API calls using any of the above algorithms and  unique prompts,\nand then use these to reconstruct the full LLM output after only  queries for each subsequent output.\nTo get a new full output , use any of the above algorithms to obtain .\nNext, we will use the additive log ratio () transform,\nwhich an isomorphism  and is defined as\nto transform the columns of  and  into vectors in ,\nthough since we only know the first  values of ,\nwe can only obtain the first  values of .\nBecause the alr transform is an isomorphism, we have that the columns of\nform a basis for a -dimensional vector subspace of ,\nand  lies within this subspace.\nTherefore, there is some  such that .\nTo solve for ,\nall that is required is to find the unique solution to the first  rows of this system of linear equations\nAfter finding ,\nwe can reconstruct the full LLM output \nwhere the inverse alr function is defined as\nThus we can retrieve  in only  API queries.\nThis  speedup makes any method that relies on full model outputs significantly cheaper. This includes model stealing (Tramèr et al., 2016  ###reference_b18###) which attempts to learn a model that exactly replicates the behavior of a target model."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Identifying LLMs from a single output",
            "text": "###figure_8### The image of two different LLMs, even different checkpoints from the same LLM,\nare largely disjoint.\nAs shown in Figure 5  ###reference_###,\nthe logit output from a late-training checkpoint of Pythia-70M lies uniquely in the image of the checkpoint,\nand not in the image of the preceding or following checkpoints.\nWe also verify that the output does not lie in the image of any checkpoints from a similar LLM trained on deduplicated data,\nnor any checkpoint from a larger LLM trained on the same data.\nThis suggests that the image of a LLM is highly sensitive to small parameter changes.\nIntuitively this makes sense, since the output of one model will only be in the image of another model in the extremely unlikely event that it happens to lie on the low-dimensional () intersection of the models’ images.\nMathematically the dimension of the models’ images’ intersection is small since the intersection \nof two subspaces  and  that are not subsets of one another\nhas dimension ,\nwhich implies that \n(since ).\nThus, it is possible to determine precisely which LLM produced a particular output,\nusing only API access to a set of LLMs and without knowing the exact inputs to the model.\nIn this way, the model’s image acts as a signature for the model, i.e., a unique model identifier.\nThis finding has implications for LLM provider accountability.\nFor instance, if provider  suspects provider  of stealing their proprietary model,\nprovider  could check provider ’s model signature to see if it matches their own.\nIt would be extremely unlikely that the signatures match if the LLMs are not the same.\nSimilarly, if a provider claimed to be serving a proprietary model while in actuality attempting to profit off of an open-source model with a non-commercial license,\nan auditor again could discover this by checking the LLM signature.\nThis test is somewhat one-sided, however, since a clever provider may do a small amount of additional fine-tuning to change their stolen LLM’s image."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Detecting and disambiguating LLM updates",
            "text": "Another practical application of highly sensitive LLM images\nis to detect and distinguish minor and major model updates to API-protected LLMs.\nUsing the above test, we can tell whether or not the LLM image remains the same, even if the logit outputs change.\nThis phenomenon would correspond to a partial model update,\nwhere some part of the model changes,\nbut the softmax matrix remains the same.\nAn example of this would be if the LLM has a hidden prefix added to all prompts and this hidden prefix changes: the model’s outputs will change but the image will remain the same.\nTable 3  ###reference_### gives an overview for how to interpret various combinations of detectable API changes,\nThis information is useful for monitoring unannounced changes in API LLMs.\nInterpretation\nNo update\nHidden prompt change or partial finetune with frozen output layer\nLoRA update\nFull finetune"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Detecting LoRA updates",
            "text": "We speculate that it is possible to gain even higher granularity information on LLM updates of certain types, such as LoRA (Hu et al., 2022  ###reference_b10###) updates.\nLoRA is a popular parameter-efficient fine-tuning method which adjusts model weights with a low-rank update \nwhere  and \nso that the softmax matrix  becomes .\nWe speculate that it is possible to detect these types of updates\nby collecting LLM outputs before () and after () the update\nand decomposing them as  and \nwhere \nsuch that .\nIf such a decomposition is found,\nthen it appears likely that the weights received a low-rank update of rank .\nWe leave it to future work to determine whether finding such a decomposition is sufficient to conclude that a LoRA update took place,\nas well as to find an efficient algorithm for finding such a decomposition."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "More applications",
            "text": "Access to the LLM’s image can lead to many other capabilities, some of which we discuss below.\nWe leave further investigation of these methods for future work."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Finding unargmaxable tokens",
            "text": "Due to the low-rank constraints on LLM outputs, it is possible that some tokens become unargmaxable (Demeter et al., 2020  ###reference_b4###; Grivas et al., 2022  ###reference_b7###),\ni.e., there is a token  such that the constraints disallow any output  with .\nThis happens when the LLM’s embedding representation of  lies within the convex hull of the other tokens’ embeddings.\nPreviously, finding unargmaxable tokens appeared to require full access to the softmax matrix .\nInterestingly, we find that it is possible to identify unargmaxable tokens using only the LLM’s image,\nwhich our method is able to recover.\nThis technique allows API customers to find quirks in LLM behavior\nsuch as tokens that the model is unable to output (at least under greedy decoding)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Recovering the softmax matrix from outputs",
            "text": "Since our method gives us access to the image of the output layer, and not the output layer parameters,\nwe investigate the how one might reconstruct the output later parameters, either exactly or approximately.\nWe hypothesize that LLM embeddings generally lie near the surface of a hypersphere in  with a small radius .\nWe see evidence of this in the fact that the Pythia LLM embedding norms are all small and roughly normally distributed, as shown in Figure 6  ###reference_###.\n\n###figure_9### Assuming this holds for any LLM,\nwe can attempt to recover  up to a rotation\nby simplifying the assumption into a constraint that all embeddings must have a magnitude of 1.\nThen, given a matrix  of model logits,\nwe can find  (up to a rotation) by finding a decomposition \nsuch that for all , .\nThis solution may also be approximated by finding the singular value decomposition  of ,\nthough it is likely that all rows of this  will have magnitude less than  and they are not guaranteed to be normally distributed."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Improved LLM inversion",
            "text": "Morris et al.  ###reference_b14###’s (2023  ###reference_b14###) recent approach to recovering hidden prompts\n(i.e., an additional prefix added to LLM input, not shown to the user)\nuses full LLM API outputs to train an “inversion” model that generates the prefix conditioned on the LLM’s full logprob vector .\nIn addition to our algorithm from Section 3.4  ###reference_### making this much cheaper to implement,\nwe also propose a methodological change to their procedure.\nIn particular, to deal with the size mismatch between the LLM’s vocabulary size,\nMorris et al. (2023  ###reference_b14###) pad and reshape the vector into a -length sequence of embeddings in .\nThis transformation is somewhat unnatural, and requires that the inversion model only be conditioned on a single output.\nWe propose instead to take advantage of our knowledge of the LLM’s image\nto obtain a linearly lossless representation of the LLM output in ,\nwhich is much closer to the inversion model size,\nthen use an MLP to project this representation onto  to feed into the inversion model.\nFormally, after obtaining  and  from the API, we use the unique solution  to  as the input to the inversion model.\nThis modification has an additional advantage: instead of conditioning on a single output ,\nthe LLM can be used to generate a short sequence of outputs \nwhich can be fed into the inversion model as  where .\nWe leave the implementation and evaluation of this proposed modification for future work.\nThis technique can be applied generally to any method that takes LLM outputs as input."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Basis-aware sampling",
            "text": "In a recent paper, Finlayson et al. (2023  ###reference_b5###) propose a decoding algorithm\nthat avoids type-I sampling errors\nby identifying tokens that must have non-zero true probability.\nImportantly, this method relies on knowing the basis of the LLM’s output space,\nand is therefore only available for LLMs whose image is known.\nOur proposed approach for finding the image of API-protected LLMs makes this decoding algorithm possible for API LLMs."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Mitigations",
            "text": "We consider a three proposals that LLM providers may take to guard against our methods.\nThe first proposal is to remove API access to logprobs.\nThis, however is theoretically insufficient,\nas Morris et al. (2023  ###reference_b14###) show that it is possible to obtain full outputs using only information about the biased argmax token,\nalbeit inefficiently in  API calls.\nRegardless of the theoretical result, providers may rely on the extreme inefficiency of the algorithm to protect the LLM.\nThis appears to be the approach OpenAI took after learning about this vulnerability from Carlini et al. (2024  ###reference_b3###),\nby always returning the top- unbiased logprobs.\nOur new proposed algorithm, however, brings the number of queries down to a more feasible  API calls once the initial work of finding the LLM image has finished,\nweakening the argument that the expensiveness of the algorithm is sufficient to disregard our technique.\nThe next proposal is to remove API access to logit bias.\nThis would be an effective way to protect the LLM, since there are no known methods to recover full outputs from such an API.\nHowever, the logit bias interface is presumably useful for many clients who might be disappointed by its shutdown.\nLastly, we consider alternative LLM architectures that do not suffer from a softmax bottleneck.\nThere are several such proposed architectures with good performance. Though this is the most expensive of the proposed defenses, due to the requirement of training a new LLM,\nit would have the beneficial side effect of also treating other tokenization issues that plague large-vocabulary LLMs (e.g., Itzhak and Levy, 2022  ###reference_b11###).\nA transition to softmax-bottleneck-free LLMs would fully prevent our attack, since the model’s image would be the full output space."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Overall, the potential applications of our methods can have great impact in building trust between the LLM API users and the providers; at the same time, none are particularly harmful to LLM providers.\n\nHowever, access to this hyperparameter does not enable a competitor to fully recover the parameters of the LLM’s softmax matrix or boost performance of their own model; several other factors such as training data mix, human feedback and several other engineering choices are still critical and hidden.\nEven using model outputs to steal hidden prompts (see Section 6.3  ###reference_###) is unlikely to have detrimental effects,\nas hidden prompt leakage is a known vulnerability\nand best practice dictates that no private information should be contained in these system prompts (Greshake et al., 2023  ###reference_b6###; Liu et al., 2023  ###reference_b13###).\nWe find that the most dangerous consequence of our findings might simply be that model stealing methods that rely on full outputs get cheaper by a factor of , which in the case of gpt-3.5-turbo amounts to about .\nOn the other hand, allowing LLM API users to detect model changes builds trust between LLM providers and their customers,\nand leads to greater accountability and transparency for the providers.\nOur method can be used to implement efficient protocols for model auditing without exposing the model parameters or detailed configuration information, which may help model safety and privacy protection of personalized, proprietary models.\nWe therefore take the position that our proposed methods and findings do not necessitate a change in LLM API best practices,\nbut rather expand the tools available to API customers,\nwhile warning LLM providers of the information their APIs expose."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Simultaneous discovery",
            "text": "In a remarkable case of simultaneous discovery,\nCarlini et al. (2024  ###reference_b3###) propose a very similar approach to ours for gaining insight into API-protected LLMs.\nHere we review a some interesting interactions between our work and theirs.\nFirst, we give an algorithm for obtaining full outputs in  API calls, while their algorithm corresponds to our  algorithm.\nThis has little impact on our final result, since our  algorithm suffers from numerical instability, but it is an interesting theoretical result nonetheless.\nOn the flip side, Carlini et al.  ###reference_b3### propose an improved logprob-free method for getting full outputs from an API that takes advantage of parallel queries.\nThis useful method is actually complementary to our  algorithm, since they can be combined to yield an even better algorithm for obtaining full outputs.\nNext, our experiments with OpenAI’s API were fraught with issues of stochasticity: the API would return different results for the same query, leading us to develop the stochastically robust full output algorithm.\nMeanwhile, our colleagues did not appear to encounter such issues, perhaps because they had access to a more stable API endpoint than ours.\nLastly, Carlini et al.  ###reference_b3### focus mostly on defenses and mitigations against attacks, while our own work focuses more on understanding the capabilities such attacks provide once an LLM image has been obtained.\nThese approaches complement each other, since our work provides additional motivation for why (or why not) such defenses should be implemented."
        }
    ]
}