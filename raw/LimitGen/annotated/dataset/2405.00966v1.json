{
    "title": "Contents",
    "abstract": "Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we examine its limitations, demonstrating the presence of speaker-related (gender, age) and model-related (resourcefulness and model size) bias. Despite that, we show that only model-related bias are amplified by quantization, impacting more low-resource languages and smaller models. Searching for a better compression approach, we propose DistilWhisper, an approach that is able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "",
            "text": "Over the past three years, the field of Natural Language Processing (NLP) has been revolutionized by the introduction of large pre-trained models, often referred to as \"foundation models.\" These models, both for text and speech, are trained on vast amounts of unlabeled data and can subsequently be fine-tuned for specific tasks using limited labeled data.\n\nMultilingual foundation models have garnered significant attention due to their ability to handle hundreds of languages within a single model. However, they face a challenge known as the curse of multilinguality: in order to maintain high performance across all supported languages, these models require an increase in the number of parameters, leading to larger memory requirements and slower inference times. This can render the use of such models impractical in certain scenarios. To address this issue, research has been conducted on model compression techniques, although these methods may inadvertently exacerbate biases present in the model.\n\nThis internship focuses on OpenAI’s Whisper, a family of multilingual multi-task speech models known for their impressive performance in speech recognition. These models exhibit robustness when transcribing speech recorded under various conditions, surpassing the capabilities of previous models.\n\nHowever, there remain important questions to explore regarding Whisper and its multi-task learning approach. Although the model presents exceptional capability for transcribing and translating English, its performance in other languages indicates a decline in multilingual capabilities as the model size decreases. Additionally, we aim to investigate how this multilingual architecture handles biases related to different speakers, including gender, age, and accent. These questions drive our research to enhance the understanding of Whisper’s capabilities and limitations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "",
            "text": "Current ASR approaches primarily involve adapting pre-trained Transformer stacks (vaswani2017attention), which are initially trained through self-supervised learning (SSL) on unlabeled audio data. These pre-trained models can vary in their use of pre-text tasks (e.g., wav2vec 2.0 (baevski2020wav2vec), HuBERT (hsu2021hubert), WavLM (chen2022wavlm)) and the range of languages they cover (e.g., XLSR-53 (conneau21_interspeech), XLS-R (babu22_interspeech), MMS (pratap2023scaling), Google-USM (zhang2023google)). This development of models has also seen the introduction of monolingual and multilingual SSL benchmarks. Examples of such benchmarks include SUPERB for English (yang21c_interspeech), LeBenchmark (evain2021task) for French, and ML-SUPERB (shi2023ml), which covers 143 languages. In contrast to this line of research, the Whisper model relies on weak supervision, meaning it is trained solely on weakly labeled data (without self-supervision). Nevertheless, with an ample amount of data, the Whisper model achieves competitive results when compared to monolingual (radford2023robust) and multilingual (pratap2023scaling) SSL models. More details about Whisper can be found on Section 2.6."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "",
            "text": ""
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "",
            "text": "We extend Conditional Language-Specific Routing (CLSR) modules proposed by zhang2021share, and commonly used in Multilingual Neural Machine Translation, for the first time to the speech domain. This module, which introduces sparsity to the Transformer architecture, learns a hard binary gate  for each input token by using its hidden embedding . These decisions enable a layer to selectively guide information through either a LS path denoted as  or a shared path referred to as , as in Eq. 4.1  ###reference_###:\nIn contrast to the original CLSR, in this work we use language-specific gates as shown in Figure 4.1  ###reference_###, instead of sharing them across languages. This allows us to train language-specific components individually (i.e. in parallel), and then only load the relevant modules at inference. Moreover, our approach also differs from the original CLSR by the positioning: supported by previous work (zhang2021share; pfeiffer-etal-2022-lifting), we limit CLSR to the feed-forward network (correspondent to the feature domain of the Transformer architecture), which we also replace entirely by the CLSR module, reducing the increment in the number of parameters.\nFollowing the proposal from zhang2021share, each gate  is made by a two-layer bottleneck network, which is summed to a increasing zero-mean Gaussian noise during training to discretize it:\nwhere  is the logistic-sigmoid function, and  and  are trainable parameters.  is linearly increased along with training steps . At inference time, we adopt hard gating:\nwhere  is a Dirac measure."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "",
            "text": "In this section, we overview our validation setup, including choosing the data we use for training and evaluating models, as well as which languages and baselines to consider. We also discuss some code implementation details."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "",
            "text": "There are several promising directions for future research in this area. Firstly, it would be beneficial to expand upon the analysis presented in Chapter 3  ###reference_###, including an investigation into other quantization methods, such as 4-bit quantization. Exploring these methods across various model families would help determine if the conclusions drawn here are applicable more broadly. This could present an important contribution to the community and ensure the correct usage of these techniques.\nAdditionally, further research into the DistilWhisper approach could yield valuable insights. Examining the effects of several hyperparameters, such as gate budget, KD loss weight, and temperature, would provide a deeper understanding of the approach’s behavior. This exploration could help find the best setting for optimal performance of the approach.\nFurthermore, it would be valuable to assess the impact of the proposed approach in multitasking beyond transcription (ASR), particularly in speech translation. Investigating whether language-specific paths can enhance translation performance to English, and exploring the potential for achieving new zero-shot capabilities in many-to-many translation scenarios, could open up exciting possibilities for the field."
        }
    ]
}