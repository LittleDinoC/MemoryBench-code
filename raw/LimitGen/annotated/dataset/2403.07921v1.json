{
    "title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices",
    "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. \n\nWe evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performance compared to the 350M parameter OPT while being 4.9 faster on NVIDIA Jetson Nano with 5.5 reduction in model size. Code will be made available soon.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The Transformer architecture, originally introduced in [1], has revolutionized the field of natural language processing (NLP). It has become the de-facto building block in many large-scale pre-trained language models (LLMs) [2, 3, 4, 5, 6, 7]. Especially, Generative Large-scale Language Models (LLMs), exemplified by GPT [4, 5] and LLaMA [7], have gained considerable popularity in recent studies. Yet, such models are without a doubt expensive to train and deploy. For instance, GPT-175B contains over 175 billion parameters, rendering it unsuitable for direct deployment on resource-constrained devices, such as mobile phones or Internet-of-Things (IoT) hardware. Consequently, there exists a substantial demand for developing lightweight language models that can be deployed to mobile systems with small memory footprints and low compute power.\n\nA key challenge of designing mobile-friendly language models is that the hardware configuration varies from device to device. Therefore, it is difficult to design a one-fits-all model that satisfies all requirements. To this end, it is critical to customize an optimized language model backbone under different computational budgets. A conventional approach is to use knowledge distillation (KD) [8] which distills larger language models into pre-defined smaller backbones [9, 10, 11]. However, there is no guarantee that these pre-defined, fixed-size backbones are optimal on the given device. Another more flexible approach is to use AutoML [12] or neural architecture search (NAS) [13, 14, 15] to optimize the transformer backbone. However, these methods are usually computationally demanding, which involves super-net [16, 17] training or even brute-force grid search. Such processes often consume considerable GPU hours and leave large carbon footprints. Moreover, training super-nets is a non-trivial task as child architectures often interfere with each other which leads to performance degradation, as reported in [18].\n\nIn this paper, we present an entropy-driven framework to design lightweight variants of generative language models tailored for resource-constrained devices. Our method leverages recent advancements in information theory and theoretical deep learning which formulate autoregressive language models as information processing systems parameterized by structural parameters such as network widths and depths. Then, the Maximum Entropy Principle [19] is applied to optimize the network architecture design. More specifically, our design aims to find the optimal configuration of network structure parameters, including depths/widths/embedding dimensions, such that the network entropy is maximized under the given computational budgets, such as parameter size and FLOPs.\n\nAlbeit the Maximum Entropy Principle is conceptually simple, a direct application encounters two technical challenges. First, the notion of entropy for a transformer backbone is not well-defined in deep learning literature. To overcome this hurdle, we propose to use subspace entropy spanned by the network parameters at random initialization as model entropy. The computation of subspace entropy can be accelerated via table lookup therefore, it is highly efficient. Second, we find that naively maximizing the entropy will lead to an over-deep transformer backbone that is difficult to train. To address this issue, we propose to preserve the model trainability during the architecture design. Then an Evolutionary Algorithm (EA) is utilized to optimize the structural parameters of the transformer backbone (e.g., number of heads, channels, embedding dimensions, etc.).\n\nFinally, we can design a family of optimized, Mobile-friendly geneRative language models, or MeRino for short, on various mobile devices at nearly zero cost. The key contributions of this work are summarized as follows: To the best of our knowledge, we first present an entropy-driven framework to address the challenge of designing efficient generative language models for resource-constrained devices at nearly zero cost. Our framework leverages the Maximum Entropy Principle and considers both the entropy and trainability of language models to optimize transformer architectures given computation budgets. Experimental results show that MeRino achieves competitive performance against the state-of-the-art LLMs, including OPT and Pythia models. Notably, our models exhibit improved parameters, computation, and throughput efficiency on mobile devices."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Generative large language models (LLMs) have emerged as the standard solution to a wide range of NLP tasks.\nThey are generally pre-trained on large-scale corpora in self-supervised manners to learn the contextual structure of natural language. Unlike previous language models, LLMs consist of only transformer decoder layers and exhibit outstanding ability to scale up and impressive zero-shot generalization performances.\nGPT-3 [5  ###reference_b5###], in particular, pushed the boundaries of casual language models by scaling up the model size to 175 billion parameters and pre-training on a large corpus of over 570 GB plain texts.\nIn the pursuit of democratizing and fostering reproducible research in LLMs, Meta AI recently released Open Pre-trained Transformers (OPT) [6  ###reference_b6###], a suite of decoder-only models, ranging from 125 M to 175 B parameters.\nIn this work, our scope is generative, or decoder-only transformer-based language models and we aim to design such LLMs suitable for mobile devices with limited memory space and compute power.\nOne of the most widely studied techniques in compressing LLMs is knowledge distillation (KD) [8  ###reference_b8###].\nBERT-PKD [11  ###reference_b11###] distill BERT into smaller students using knowledge transfer in both final output and hidden states in multiple intermediate layers.\nTinyBERT [20  ###reference_b20###] adopts a layer-wise distillation strategy for BERT at both the pre-training and fine-tuning stages.  [9  ###reference_b9###] investigates numerous KD techniques to compress GPT-2 models by layer truncation.\nDespite achieving promising results, the above KD-based methods can only distill LLMs into a fixed-size model, which is not suitable for deployment on diverse and heterogeneous devices.\nIn this work, orthogonal to KD, which focuses primarily on the training and fine-tuning stage, our proposed method emphasizes designing lightweight transformer architectures with various parameter sizes and FLOPs to meet different hardware constraints.\nDue to its success in computer vision (CV), neural architecture search (NAS) has recently gained attention in the NLP community.\nNAS-BERT [14  ###reference_b14###] trains a supernet to efficiently search for masked language models which are compressed versions of the standard BERT.\nAutoTinyBERT [15  ###reference_b15###] further reduces overall computation cost over NAS-BERT by adopting a linear search space.\nFor encoder-decoder architectures, HAT [13  ###reference_b13###] uses the Once-For-All [16  ###reference_b16###] approach and performs a search on sub-samples of the supernet that inherits weights to estimate downstream task accuracy.\nLTS [21  ###reference_b21###] proposes using non-embedding parameters in decoders as a proxy score to predict the perplexity performance of generative LLMs.\nHowever, the aforementioned methods are mostly data-dependent and incur heavy computation costs.\nMoreover, it is difficult for researchers to understand why specific architectures are preferred by the algorithm and what theoretical insight we can learn from these results.\nIn this work, we plan to explore the architecture design of autoregressive language models in a principled way with clear theoretical motivation and human explainability.\nInformation theory recently has emerged as a powerful tool for studying deep neural networks [22  ###reference_b22###, 23  ###reference_b23###, 24  ###reference_b24###, 25  ###reference_b25###].\nSeveral previous studies [22  ###reference_b22###, 23  ###reference_b23###] have attempted to establish a connection between the information entropy and the neural network architectures.\nFor instance,  [22  ###reference_b22###] tries to interpret the learning ability of deep neural networks using subspace entropy reduction.\n [23  ###reference_b23###] investigates the information bottleneck in deep architectures and explores the entropy distribution and information flow in deep neural networks.\nAdditionally,  [24  ###reference_b24###, 25  ###reference_b25###] focus on designing high-performance convolutional neural networks (CNNs) via maximizing multi-level entropy.\nYet, to the best of our knowledge, there is still no published work using information entropy to design efficient decoder-only transformer backbones for language models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we begin by presenting some preliminary details on autoregressive transformer models. Next, we introduce our novel definition of network entropy for transformer models. Moreover, we demonstrate that the untrained subspace entropy positively correlates with the model performance after training. Finally, we present our entropy-driven design procedure, which solves a constrained mathematical programming problem using the Evolutionary Algorithm (EA).\n\nTransformers have gained prominence due to their ability to model complex functions that involve long-range dependencies. Decoder-only, or autoregressive transformers, operate by predicting the next element in a sequence based on the preceding elements. A standard autoregressive transformer comprises an embedding layer to project sequences of tokens to hidden dimensions and stacks of transformer layers to capture long-term dependencies between input tokens using the self-attention mechanism. A transformer layer includes two main components: a multi-head attention (MHA) module and a position-wise feed-forward network (FFN). The MHA module facilitates capturing contextual information by attending to different positions within the input sequence, while the FFN performs element-wise transformations to introduce non-linearity and improve representational capacity.\n\nMulti-head attention (MHA) is a crucial component within the transformer architecture that enables the model to selectively attend to different segments of the input sequence. This mechanism involves projecting the input sequence into multiple attention heads, each of which calculates an independent attention distribution. In MHA computation, there are specifically four main matrices involved: attention matrices and output project matrix. Given the output of previous layers as input, the attention function is formulated as:\n\nwhere , , and represent queries, keys, and values, respectively. MHA is defined by concatenating attention heads and producing outputs as follows:\n\nIn addition, the transformer layer adopts residual connection and layer normalization on top of MHA to compute the final outputs. In addition to the MHA, each transformer layer includes a feed-forward network (FFN). The FFN applies two point-wise fully connected layers followed by a non-linear activation function, such as ReLU. Operations within FFN can be formulated as follows:\n\nSimilarly, the FFN also incorporates residual connections and layer normalization to compute the final outputs:\n\nExpressiveness in Deep Network  From the perspective of information theory, deep neural networks can be regarded as information systems, and their performance is closely related to the expressive power of such networks. The notion of entropy is often used to measure such expressiveness through intermediate feature maps in convolutional neural networks (CNNs). In the case of transformers, we propose to define the entropy of transformers from the perspective of parameter subspaces. Suppose that presents a linear mapping with input channels and output channels. The elements of are randomly sampled from the standard Gaussian distribution. According to previous works, the subspace spanned by the random linear mapping has\n\nwhere , is the -th largest singular value of and is a small constant. For an -layer network, we define the network entropy by accumulating the entropy of matrices in each layer as the following:\n\nEffectiveness in Deep Network  The entropy measures the expressiveness of the deep neural network, which is positively correlated with the network performance. However, directly maximizing the above-defined entropy leads to the creation of over-deep networks, since according to Eq. (8), the expressivity (entropy) grows exponentially faster in depth (number of layers), than in width (dimension of ). For an over-deep network, a small perturbation in low-level layers of the network will lead to an exponentially large perturbation in the high-level output of the network. During the back-propagation process, the gradient flow often cannot effectively propagate through the entire network.\n\nThough recent works have attempted to alleviate the trainability issues by revising initialization strategies, adding skip connections, or proposing better architectures, training over-deep networks still remains a rather challenging problem. To verify the negative impact when the network is over-deep, in Table 1, we conduct experiments of training two transformer architectures with a similar parameter size of 40 M. One model, referred to as the ‘Wide’ model, consists of only one layer and an embedding dimension of 256. The other model, referred to as the ‘Deep’ model, consists of 24 layers but only with an embedding dimension of 64. Both models are trained under the same setting until convergence. We observe that even though the ‘deep’ network has much higher entropy, it obtains worse perplexity performance after training than the ‘wide’ network. This observation aligns with the common belief that over-deep networks hinder effective information propagation and are difficult to train and optimize.\n\nTo address the potential trainability issues, we propose adding additional constraints to control the depth-width ratio of networks. Specifically, we adopt the term effectiveness from the work and define it as follows:\n\nHere, is the effective width of a -layer network and is a scaling factor to control within the range of 0 and 1. To enforce the above constraint, we revise Eq. (8) as"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "Transformers have gained prominence due to their ability to model complex functions that involve long-range dependencies. Decoder-only, or autoregressive transformers, operate by predicting the next element in a sequence based on the preceding elements. A standard autoregressive transformer comprises an embedding layer to project sequences of tokens to hidden dimensions and stacks of transformer layers to capture long-term dependencies between input tokens using the self-attention mechanism. A transformer layer includes two main components: a multi-head attention (MHA) module and a position-wise feed-forward network (FFN). The MHA module facilitates capturing contextual information by attending to different positions within the input sequence, while the FFN performs element-wise transformations to introduce non-linearity and improve representational capacity. Multi-head attention (MHA) is a crucial component within the transformer architecture that enables the model to selectively attend to different segments of the input sequence. This mechanism involves projecting the input sequence into multiple attention heads, each of which calculates an independent attention distribution. In MHA computation, there are specifically four main matrices involved: attention matrices and output project matrix. Given the output of previous layers as input, the attention function is formulated as: where , , and  represent queries, keys, and values, respectively. MHA is defined by concatenating attention heads and producing outputs as follows: In addition, the transformer layer adopts residual connection and layer normalization on top of MHA to compute the final outputs. In addition to the MHA, each transformer layer includes a feed-forward network (FFN). The FFN applies two point-wise fully connected layers followed by a non-linear activation function, such as ReLU. Operations within FFN can be formulated as follows: Similarly, the FFN also incorporates residual connections and layer normalization to compute the final outputs:\n\nExpressiveness in Deep Network  From the perspective of information theory, deep neural networks can be regarded as information systems, and their performance is closely related to the expressive power of such networks. The notion of entropy is often used to measure such expressiveness through intermediate feature maps in convolutional neural networks (CNNs). In the case of transformers, we propose to define the entropy of transformers from the perspective of parameter subspaces. Suppose that  presents a linear mapping with  input channels and output channels. The elements of  are randomly sampled from the standard Gaussian distribution. According to previous works, the subspace spanned by the random linear mapping has where ,  is the -th largest singular value of  and  is a small constant. For an -layer network , we define the network entropy  by accumulating the entropy of matrices in each layer as the following:\n\nEffectiveness in Deep Network  The entropy measures the expressiveness of the deep neural network, which is positively correlated with the network performance. However, directly maximizing the above-defined entropy leads to the creation of over-deep networks, since according to Eq., the expressivity (entropy) grows exponentially faster in depth (number of layers), than in width (dimension of). For an over-deep network, a small perturbation in low-level layers of the network will lead to an exponentially large perturbation in the high-level output of the network. During the back-propagation process, the gradient flow often cannot effectively propagate through the entire network. Though recent works have attempted to alleviate the trainability issues by revising initialization strategies, adding skip connections, or proposing better architectures, training over-deep networks still remains a rather challenging problem. To verify the negative impact when the network is over-deep, we conduct experiments of training two transformer architectures with a similar parameter size of 40 M. One model, referred to as the ‘Wide’ model, consists of only one layer and an embedding dimension of 256. The other model, referred to as the ‘Deep’ model, consists of 24 layers but only with an embedding dimension of 64. Both models are trained under the same setting until convergence. We observe that even though the ‘deep’ network has much higher entropy, it obtains worse perplexity performance after training than the ‘wide’ network. This observation aligns with the common belief that over-deep networks hinder effective information propagation and are difficult to train and optimize. To address the potential trainability issues, we propose adding additional constraints to control the depth-width ratio of networks. Specifically, we adopt the term effectiveness from the work and define it as follows: Here, is the effective width of a -layer network and is a scaling factor to control within the range of 0 and 1. To enforce the above constraint, we revise Eq. as follows: Compared to the previous subspace entropy definition, Eq. penalizes networks with larger depth-to-width ratios (higher). This constraint helps alleviate potential trainability issues by promoting a more balanced depth-width ratio in the network architecture. By considering both expressiveness (entropy) and effectiveness (the depth-width ratio), we aim to design more capable and trainable models.\n\nEntropy of Transformers  Consider"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Designing Mobile Language Models",
            "text": "Search Space  In the design of MeRino, we introduce an adaptive block-wise search space to construct the backbone architecture. This allows us to determine the architectural parameters on a per-block basis. Each transformer block consists of numerous transformer layers of the same number of attention heads, hidden dimensions, and embedding dimensions. Within each transformer block, in MHA layers, we fix the head dimension and make the attention head number elastic so that each attention module can decide its necessary number of heads. We also set the Q-K-V dimensions the same as embedding dimensions; in FFN layers, the hidden dimension is decided by choosing the FFN ratio to the embedding dimension.\n\nTo prevent information bottlenecks, we also ensure that as the network goes deeper, the embedding dimension of each transformer block should be non-decreasing. Moreover, we incorporate parameter sharing technique [38] within each transformer block. This means that all MHA and FFN layers within the block share the same weights, resulting in transformer models of reduced memory footprint. Illustration can be found in Figure 2. Details of our search space configuration are provided in Appendix A.\n\nSearch Process  To design a transformer model with transformer blocks under a given computation budget, we propose to optimize the parameters by solving a mathematical programming (MP) problem. The objective of the MP problem is to maximize a weighted sum of entropy, representing the expressiveness and effectiveness of the model, while considering constraints on the computational cost. The MP problem is formulated as follows: where , , and  denote the embedding dimension, FFN ratio, and number of layers in the -th transformer block, respectively.\n\nTo solve this optimization problem, we employ an Evolutionary Algorithm [39]. Note that Eq. (15) can be solved by any non-linear programming solver in principle. We choose EA due to its simplicity. We first construct the initial population by randomly generating architectures with parent size. At each iteration, we randomly select an architecture from and mutate it. The embedding dimension of the selected block is mutated in a given range, which has to be no bigger than the next block and no smaller than the previous block. The mutated architecture is added to the population if it meets the budget constraints. Finally, we maintain the population size by removing networks with bottom entropy. After iterations, the architecture with the highest entropy score is returned as the output network.\n\nSince our formulated problem is purely mathematical, it can be solved nearly instantly on the CPU. A detailed description of EA and the mutation algorithm is given in Appendix B."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first describe experimental settings for search, training, and evaluation. Next, we report the results of MeRino on various NLP tasks and compare our approach with existing pretrained LLMs. Finally, we conduct ablation studies of different key components in MeRino."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "Search Settings  In searching for MeRino, the number of iterations is set to 100000, with a population size of 512 and the parent size of 64. We conduct searches for three different FLOP targets (60/110/160 G). We limit the number of transformer blocks.\n\nTraining Settings  We mostly follow settings in [6] and [40] and pre-train our models on the Pile dataset [41] for 600k steps (300B tokens) with an effective batch size of 512 using AdamW optimizer [42], with a starting learning rate of 6e-4 and warm-up steps of 1000, and linear learning rate decay schedule. We also enable automatic mixed precision (AMP) for better training efficiency.\n\nEvaluation Settings  We evaluate our models for zero-shot natural language inference tasks across nine different downstream NLP tasks, namely HellaSwag [43], WinoGrande [44], OpenBookQA [45], PubmedQA [46], LogiQA [47], and SuperGLUE [48] benchmark BoolQ, CB, WIC, and RTE. FLOPs are calculated with a batch size of 1 and sequence length of 1024 and inference throughput is measured at token per second on NVIDIA Jetson Nano 8GB."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "Since our scope is mobile-friendly language models, we mainly compare pretrained LLMs that can be run on NVIDIA Jetson Nano 8GB with out-of-memory (OOM) issues. We compare the average accuracy of our MeRino models with baseline models, such as GPT-2 [4], OPT [6], Pythia [40], and Cerebras-GPT [49]. Table 2 reports the comparisons of MeRino and current state-of-the-art autoregressive transformer-based language models. Compared to the OPT family, MeRino achieves superior accuracy with much less parameter size and FLOPs. Specifically, MeRino-64M is 0.4% better than OPT-350M with 82% and 78% reduction in model size and computation respectively. Above all, MeRino achieves an average speedup of 2.0 and 7.3 against OPT-125M and OPT-350M, respectively. When compared to open-sourced LLMs that are trained on the Pile dataset, MeRino-64M achieves 0.3% higher average zero-shot accuracy than Cerebras-GPT while reducing parameter size and FLOPs by 1.7 and 1.6, respectively; MeRino-61M is also 0.6% more accurate than GPT-2 with 3.3 lower latency; our smallest model, MeRino-52M is able to outperform Pythia-70M by 0.5% with 1.9 faster runtime."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "Impact of Effectiveness Constraint  \nAs shown in Table 3, the effectiveness constraint plays a key role in helping our entropy-driven framework design more capable and trainable models. When using the effectiveness constraint, the final searched language model obtains a +2.4% average accuracy gain.\n\nWe also study the impact of weight on our entropy-driven approach. As shown in Figure 3, naively adding MHA and FFN without weights cannot represent the perplexity performance very well. Weighted entropy, on the other hand, especially when properly tuned, exhibits much better correlation results than unweighted entropy. In Table 3, we further evaluate the impact of weighted entropy on downstream performance. We can see that using weighted entropy helps improve the average zero-shot accuracy by 0.8%.\n\nParameter Sharing  \nWe report the effect of parameter technique on MeRino in Table 4 for three different FLOPs targets (60/110/160 G). We can see that sharing parameters within the same transformer block helps improve parameter efficiency and reduce the model size while having a negligible impact on both the language modeling (see Pile test loss) and downstream zero-shot performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "As no research is MeRino, MeRino has several limitations as well. First, the design of MeRino explores entropy only from parameter subspace due to its straightforwardness.\nFurther exploration of entropy in the feature space could provide a better theoretical understanding of transformer architecture and potentially lead to improved model designs.\nSecond, our design only focuses on the ”macro-structure” of the LLMs (channels/depths/heads).\nOther key components, such as residual connections, layer normalization, and nonlinear activations, are also essential to achieve good performance.\nHowever, the theoretical foundation for these components is not well-studied, especially from an information theory perspective.\nHow to integrate these components in our entropy-based framework remains an open question and we would leave it for our future research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we present MeRino, a novel design framework aiming to generate efficient autoregressive language models for IoT devices, such as NVIDIA Jetson Nano. By modeling transformer models as information processing systems, MeRino leverages the Maximum Entropy Principle and optimizes the network architecture by maximizing the subspace entropy of transformer decoders and model trainability under given computational budgets. We show that MeRino can achieve comparable performance against state-of-the-art LLMs with significant improvement in model size reduction and inference runtime speedup on resource-constrained devices."
        }
    ]
}