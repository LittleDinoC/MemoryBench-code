{
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Previous works have demonstrated that Chain-of-Thought (CoT) prompting can improve the Large Language Models (LLMs) capacity to perform complex reasoning tasks by decomposing a reasoning task into a sequence of intermediate steps, where the generation of multi-step controlled reasoning can improve results in symbolic and mathematical reasoning datasets.\n\nSince the size of LLMs represents an adoption barrier for many use cases and smaller models do not seem to have the same emergent reasoning abilities as LLMs, several state-of-the-art alignment approaches for solving mathematical problems have emerged, where Supervised Fine-Tuning (SFT) has been used to train Small Language Models (SLMs) using CoT annotations.\n\nHowever, these annotations outline the intermediate reasoning steps for solving a given problem, which consists of a reasoning pathway generated by the LLM for the specific case. This phenomenon can lead to a relatively weak generalization capacity of tuned models that have a few and limited number of samples. Indeed, there are often multiple valid CoT annotations for the same question, which underlines the need for a more general CoT-based fine-tuning approach.\n\nIn this paper, we propose Self-refine Instruction-tuning, which is a method to enable CoT reasoning over SLMs. Our approach starts by performing Instruction-tuning on SLMs via demonstrations delivered by LLMs and then applies preference optimization based on reinforcement learning (RL) heuristics to let the SLMs refine their abilities to solve a task in a step-wise manner.\n\nHence, proposing a teacher-student alignment method, we investigate the impact of transferring Chain-of-Thought reasoning abilities through the support of Demonstrations \"taught\" by LLMs to SLMs as a warm-up to the Self-refine process. Therefore, to reinforce the Instruction-tuning phase, we analyze whether preference optimization methods could strengthen students’ step-wise reasoning abilities.\n\nComplementing the foundation work, we introduce Self-refinement based on reinforcement learning, and in contrast to previous methods, we use an Instruction-tuning via Demonstrations approach (i.e., a task-oriented specialization of Supervised Fine-Tuning) through which we instruct SLMs using Demonstrations delivered from different teachers prompted via a CoT mechanism.\n\nThis leads to the target research questions, which are the focus of this paper:\nRQ1: How does Instruction-tuning via Demonstrations initialize the SLMs’ reasoning abilities?\nRQ2: What is the effect of the preference optimization algorithm on the alignment between teacher and student models?\nRQ3: How much does the ability to solve tasks in a multi-step manner improve across different scenarios?\n\nTo answer these questions, we select three different SLMs: Llama2-7b, -13b, Mistral-7b; and three LLMs Llama2-70b, Mixtral and GPT-3.5.\n\nIn the teacher-student alignment phase, we use LLMs (teachers) to deliver Demonstrations at the core of the Instruction-tuning process used to instruct SLMs (students). In the Self-refine phase, the students improve their step-wise reasoning abilities via Direct Preference Optimization (DPO). This allows the students to sample different reasoning paths and CoT Demonstrations and learn from them. Moreover, differently from previous works, preferences are self-generated, and there is no need for a separately trained reward model as in the previous approaches.\n\nWe demonstrate the effectiveness of the proposed refinement technique in aligning teacher-student models from the same family and in maximizing efficiency in in-domain and out-domain tasks.\n\nOur contributions can be summarized as follows:\nWe propose the Self-refined Instruction-tuning approach that is a task-oriented Supervised Fine-Tuning (SFT), which utilizes DPO heuristics to conduct a self-refinement process starting from instructed SLMs. We analyze the impact of different configurations of Instruction-tuning on the SLMs before and after the Self-refining phase by conducting in-depth experiments on mathematical problems. Hence, we show the downstream functionalities in both scenarios. Finally, we display the generalization abilities acquired via Self-refined Instruction-tuning through a systematic evaluation using Demonstrations provided by in-family and out-family teachers, both within in-domain and out-domain tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "To transfer the step-wise reasoning properties from Large Language Models (LLMs) to Small Language Models (SLMs), we propose Self-refine Instruction-tuning, a two-step approach as shown in Figure 1  ###reference_###. In the first phase, there is a transfer of step-wise (CoT) reasoning via Instruction-tuning, where LLMs systematically generate Demonstrations which are used by SLMs to initialize their step-wise (CoT) alignment (Section 2.1  ###reference_###). In the second phase, the instructed SLMs Self-refine their internal CoT model via the preference optimization technique presented in Section 2.2  ###reference_###.\nIn the standard DPO approach Rafailov et al. (2023  ###reference_b25###), a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs  and . However, we propose an optimization step via Self-generated annotation by the students , which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.\nFor each Demonstration , we prompt the students using the input  ( or ) (blue block in Figure 1  ###reference_###). Hence, for each instance within the Demonstrations we collect the Answers () that are the answers generated by the student given the input , and the CoT-Answers () are the answers that deliver CoT generated by the student elicited via CoT mechanism .\nIn particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with  and responses when prompted with  just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation 1  ###reference_###:\nwhere  is the sigmoid function, and\nwhere  is a hyperparameter.\nWe propose the Self-refine Instruction-tuning that uses as optimization technique DPOCoT (described in details in Appendix B  ###reference_### in Equation 3  ###reference_###. In particular, in DPOCoT the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Instruction-tuning Phase",
            "text": "A significant part of the state-of-the-art works employs standard Supervised Fine-Tuning (SFT) performed on annotations produced by a single LLM (Large Language Model) as a mechanism to improve SLMs. In our contribution, we take a step further and use Instruction-tuning, which is a task-oriented specialization of SFT (Supervised Fine-Tuning), in coordination with a teacher-student alignment approach (detailed in Appendix A  ###reference_###).\nIn this phase, the SLM (student) is fine-tuned on a dataset produced by LLM (teacher) comprising a set of tuples in the form of , where  represents a specific instruction,  is the input question (e.g., math-word problem), and  is the expected output and CoT answers generated from the teacher in response to the instruction and input. This setup is intended to transfer to the student models foundational problem-solving abilities, emphasizing the generation of outputs that conform to the provided instructions. The CoT answer  is articulated as:\nwith  indicating the sequence length. At each timestep , the action  is derived from the policy , where  can be any token from the models vocabulary, and the state  encapsulates the concatenation of all previously generated tokens and the optional input  if provided. The state transition is defined as:\nThe Instruction-tuning loss function explicitly integrates the instruction , aligning the models’ learning process with the instructional context. This loss function is formulated as:\nHere,  is conditioned on both the state , the input , and the instruction , ensuring that the model prioritizes instruction compliance in its output generation. This methodological shift from SFT to Instruction-tuning underlines the principle of enhancing the models’ ability to accurately interpret and execute complex instructions."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Self-refinement Phase",
            "text": "In the second phase, the instructed SLMs (students) that have improved CoT properties via Instruction-tuning (Section 2.1  ###reference_###) self-refine these properties with the support of Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b25###). This refinement can be conducted in an SFT style, relying exclusively on labeled preference data. The policy model, defined as , learns by repeatedly sampling the answers generated by teachers and students.\nIn the standard DPO approach Rafailov et al. (2023  ###reference_b25###  ###reference_b25###), a human annotator ranks the outputs from a reference policy, labeling winning and losing pairs  and . However, we propose an optimization step via Self-generated annotation by the students , which, after Instruction-tuning, should have more robust performances and reliably follow the demands of the questions.\nFor each Demonstration , we prompt the students using the input  ( or ) (blue block in Figure 1  ###reference_###  ###reference_###). Hence, for each instance within the Demonstrations we collect the Answers () that are the answers generated by the student given the input , and the CoT-Answers () are the answers that deliver CoT generated by the student elicited via CoT mechanism .\nIn particular, assuming it is preferable for the model to generate responses that provide a CoT when elicited with  and responses when prompted with  just as the corresponding LLM teacher would do, we propose an alignment by exploiting DPO optimization. This aims to move the default style of our model (response generated by the student) towards the desired style (answers that deliver CoT). Different configurations are proposed depending on the desired result. Starting from the standard equation 1  ###reference_###  ###reference_###:\nwhere  is the sigmoid function, and\nwhere  is a hyperparameter.\nWe propose the Self-refine Instruction-tuning that uses as optimization technique DPOCoT (described in details in Appendix B  ###reference_###  ###reference_### in Equation 3  ###reference_###  ###reference_###. In particular, in DPOCoT the answers that deliver a CoT response which is self-generated from the students are referred to as the preferred response."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "In order to evaluate the proposed model, we use mathematical reasoning tasks (introduced in Section 3.1 ###reference_###) that are generally used to assess the step-wise inference properties of Large Language Models (LLMs). Regarding the Self-refine Instruction-tuning on the Small Language Models (SLMs), we use the approach presented in Section 3.2 ###reference_###. \n\nWe adopt two benchmarks to evaluate reasoning in the context of everyday situations, aiming to establish the most reasonable solution: Interaction Question Answering (PIQA) Bisk et al. (2019 ###reference_b2###) and Social Interaction Question Answering (SIQA) Sap et al. (2019 ###reference_b28###), which emphasizes people’s actions and social implications.\n\nWe use two math word problem benchmarks to evaluate the models of mathematical reasoning. MultiArith Roy and Roth (2015 ###reference_b27###) covers a set of multi-step arithmetic reasoning tasks, while GSM8k Cobbe et al. (2021 ###reference_b5###) covers a set of primary school-level mathematical problems. \n\nFinally, to evaluate the adaptability of our proposal, we conduct further analysis on two additional evaluation benchmarks: MATH Hendrycks et al. (2021b ###reference_b11###), and MMLU Hendrycks et al. (2021a ###reference_b10###).\n\nSince the test split is not prescribed for all the benchmarks, we adopt the following strategy: for SIQA, PIQA, we use 4000 examples with equally distributed target classes as training data and the validation versions found on huggingface as test data, while for GSM8K and MultiArith we use the full huggingface datasets. In Table 9 ###reference_###, we report the descriptive statistics and splitting ratios, while in Table 9 ###reference_###, we report one example for each benchmark. The supporting datasets are publicly accessible as described in Table 9 ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks & Datasets",
            "text": "In this paper, we selected different tasks that focus on reasoning tasks:\n\nWe adopt two benchmarks to evaluate reasoning in the context of everyday situations, aiming to establish the most reasonable solution: Interaction Question Answering (PIQA) Bisk et al. (2019) and Social Interaction Question Answering (SIQA) Sap et al. (2019), which emphasizes people’s actions and social implications.\n\nWe use two math word problem benchmarks to evaluate the models of mathematical reasoning. MultiArith Roy and Roth (2015) covers a set of multi-step arithmetic reasoning tasks, while GSM8k Cobbe et al. (2021) covers a set of primary school-level mathematical problems.\n\nFinally, to evaluate the adaptability of our proposal, we conduct further analysis on two additional evaluation benchmarks: MATH Hendrycks et al. (2021b), and MMLU Hendrycks et al. (2021a).\n\nSince the test split is not prescribed for all the benchmarks, we adopt the following strategy: for SIQA and PIQA, we use 4000 examples with equally distributed target classes as training data and the validation versions found on huggingface as test data, while for GSM8K and MultiArith we use the full huggingface datasets. In Table 9, we report the descriptive statistics and splitting ratios, while in Table 9, we report one example for each benchmark. The supporting datasets are publicly accessible as described in Table 9."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Self-refine Instruction-tuning Pipeline",
            "text": "The Self-refine Instruction-tuning comprises the annotation process conducted by the LLMs teachers that are prompted in the zero-shot scenario (as shown in Table 6), as explained in Appendix A.\n\nWe selected Llama-2-70 Touvron et al. (2023), Mixtral7x8 Jiang et al. (2024) and GPT-3.5 OpenAI (2023) as LLMs (teachers) and Llama2-7, -13 Touvron et al. (2023) and Mistral-7 Jiang et al. (2023) SMLs (students) models.\n\nHence, the students models are tuned, as proposed in Taori et al. (2023) and evaluated with probing pipelines (detailed in Section 3.3). The students are instructed via Demonstrations that contain the answers generated by the teachers, as explained in Section 2.1.\n\nDownstream of the teacher-student CoT transference process, the optimization technique (proposed in Section 2.2 and detailed in Appendix B) is employed to improve alignment and self-refine the quality of the generation."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Models Setup",
            "text": "We conduct the Self-refined Instruction-tuning in two different phases. Firstly, we start with Instruction-tuning phase using QLoRA Dettmers et al. (2023). This approach allows Instruction-tuning to be performed while reducing memory usage. In particular, Dettmers et al. (2023) propose several techniques for tuning models with many parameters on GPUs with limited resources while preserving 16-bit tuning performance. We follow the training approach proposed in Taori et al. (2023), setting four training epochs using a learning rate of 2e-5 with a 1e-4 weight decay. We use the cosine learning rate scheduler with a warm-up ratio of 0.03.\n\nFurthermore, we conduct the Self-refine phase following the approach proposed in Rafailov et al. (2023). In particular, we use the huggingface to support its reproducibility. We follow the parameters proposed in Rafailov et al. (2023). Hence, for the DPO policy, our work employs a learning rate of 1e-6, set at, and a warm-up step count of 100. The batch size is configured to 128. The optimization process is capped at a maximum of 1000 steps, where we save the checkpoint corresponding to the lowest loss on the validation set. The experiments were conducted on a workstation equipped with four Nvidia RTX A6000 with 48GB of VRAM."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "The most commonly used evaluation methods for question-answering tasks are language-model probing, in which the option with the highest probability is selected Brown et al. (2020), and multiple-choice probing, in which the models are asked to commit to an answer. The evaluation in the first case is performed with a function taking the argmax and, in the second case, with a direct string matching. The second method is more widely used in recent evaluations as it can be inclusive to the larger GPT family models OpenAI (2023), where probability values are not readily accessible. In the experiments, we chose the latter to have a comparable and scalable pipeline (Details provided in Appendix C.2). Finally, string matching is performed between the generated outputs and the target choice to evaluate the percentages of the correct answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results & Discussion",
            "text": "The Self-refine Instruction-tuning improves the alignment between Large Language Models (LLMs) and Small (SLMs) in both in-family and out-family settings. These conclusions can be observed in Figure 2  ###reference_### and Figure 3  ###reference_###, which reports the downstream accuracies without tuning (see the Baselines), with only the Instruction-tuning phase on Demonstrations and after the Self-refine phase. As discussed in Section 4.1  ###reference_###, the models with only Instruction-tuning on Demonstrations (generated by LLMs) transfers the reasoning properties in a marginal way (see Instruction-tuned in Figures 2  ###reference_###).\nHowever, although teacher-student alignment via Instruction-tuning produces better students, an improved alignment is achieved through the Self-refine phase, as discussed in 4.2  ###reference_###. In particular, the ’Self-refine Instruction-tuning’ bars in Figure 2  ###reference_### show that the students self-refined outperformed the students tuned only with Instruction-tuning (’Instruction-tuning’ bars on Figure 2  ###reference_###). Furthermore, the alignment via Demonstrations generated by teachers outside the same family (out-family) delivers more robust students (see Figure 3  ###reference_### the Self-refine Instruction-tuning and (in-family) bars).\nFinally, students models behind the self-refine phase outperformed others in both in-domain and out-domain tasks (discussed in Section 4.3  ###reference_###). Hence, the self-refine mechanism effectively aligns teacher-student capabilities in out-domain tasks by enhancing performance even in the presence of fewer Demonstrations (Section 4.4  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "The Instruction-tuning alignment",
            "text": "Instruction-tuning led by Larger Language Models (teachers models), which are able to deliver multi-step reasoned answers, induces this property within Smaller Language Models (students models). This can be seen in the experiments in Figure 2  ###reference_###, Figure 3  ###reference_### and additional evaluations in Appendix I  ###reference_###. The student models behind instruction-tuning on demonstrations produced by teacher models outperformed the baselines of the proposed benchmarks.\nWhile one can observe consistent improvements in performance across the board, there are moderate variations across models and tasks.\nThe teacher models that generate Demonstrations stem from different families and perform differently, as shown in Table 5  ###reference_###. The consequence of this phenomenon can be seen in Figure 2  ###reference_### and Figure 3  ###reference_### (horizontal lines that are the reported performance of the teachers and bars ’Instruction-tuning’ that are the performance of the students). Therefore, the teacher-student alignment is not complete as there is a gap between the performances of the teachers and the students tuned via Instruction-tuning (only phase presented in Section 2.1  ###reference_###). In addition, it is possible to differentiate between in-family and out-family alignment. In the in-family, where students are instructed with Demonstrations delivered by the teachers of the same family, performances vary from 6.3 points on average in question-answering (QA) tasks and 8.2 points on average in math word problems (MWP) tasks. Meanwhile, in the out-family alignment, the performances vary by 8.5 on the QA and 8.7 on the MWP.\nHence, to improve the alignment both in-family and consistently out-family, we have proposed an optimization technique based on a self-refinement approach (introduced in Section 2.2  ###reference_###), the results of which we discuss in Section 4.2  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "The Self-refine Impact",
            "text": "The Self-refine process enables complete in-family student-teacher alignment by consistently increasing performance in out-family settings and improving the qualities of generated answers. The results obtained in Figure 2  ###reference_### show that the students (SLMs instructed with Self-refine Instruction-tuning) outperform the non-self-refined students and perform comparably to their teachers. The same behaviour can be observed from the out-family setting shown in Figure 3  ###reference_###. In particular, the teacher GPT-3.5 showed a more robust baseline performance (Table 5  ###reference_###). Although Instruction-tuning alone transfers some of the abilities to the student models, they were significantly lower when compared to the out-family teacher models. In contrast, the teacher-student performances significantly converged after the self-refine phase, leading to the alignment completion. Finally, a positive impact can also be observed on the quality of students’ generations, as shown in the additional experiment discussed in Appendix H  ###reference_###.\nThe performances appear completely aligned, but the students were tested only for in-domain tasks. The proposed approach could cause students to over-specialize in in-domain tasks, running the risk of losing the ability to solve out-domain tasks. For this reason, we performed a set of assessments evaluating students on in-domain and out-domain tasks and discussed the results in Section 4.3  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "In-Domain and Out-Domain",
            "text": "The Self-refine Instruction-tuning approach complements student-teacher alignment and improves students’ generalization abilities in out-domain tasks. These results can be observed in Table 1  ###reference_### with Llama2-7 as students and Llama2-70 as teachers (in Appendix Table 10  ###reference_### with Llama2-13 Table 11  ###reference_### with Mistral-7). In particular, behind the evaluations performed on in-domain and out-domain tasks, the students Self-refine Instruction-tuned outperform the baselines and the Instruction-tuned models.\nFurthermore, to observe the impact of the optimization phase (introduced in Section 2.2  ###reference_###) on the downstream performance, we conducted a further experiment by fixing the Instruction-tuning phase and switching the Self-refine ones across different evaluation tasks (e.g., we instructed a student on OBQA and then optimized via self-refine approach on CSQA). As shown in lines Cross Self-refine of Table 1  ###reference_###, students warmed up on tasks other than those they are optimized, outperformed the others, and obtained similar performances to those obtained from in-domain models. This shows that optimization positively impacts the alignment of generalization abilities in out-domain tasks.\nFinally, following evaluations in out-domain tasks and across scenarios, we evaluate the performance of the proposed approach by reducing the number of demonstrations available for alignment in Section 4.4  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Low-resource Optimization",
            "text": "Self-refine Instruction-tuning achieves sustainable performances in low-resource settings. In fact, in Figure 4  ###reference_###, it is possible to observe that the performance achieved by the self-refined students consistently outperforms that of the non-self-refined students (where only phase 1 described in Section 2.1  ###reference_### was performed) (technical details on the breakdown can be found in Appendix C.1  ###reference_###).\nAlthough it emerges that only the optimization process via DPO is more performant than the instruction-tuning process alone, the combination of the two phases achieves the best results in both in-family and out-family alignment in each proposed splitting that are described in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Multi-step Reasoning",
            "text": "Previous works focus on Chain-of-Thought (CoT) prompting techniques, studying the impact of prompting design and engineering, proposing specialized interventions to improve CoT generalization and fine-grained multi-step reasoning properties Wei et al. (2022  ###reference_b39###); Fu et al. (2023  ###reference_b7###).\nOn the prompting design side, Gao et al. (2023  ###reference_b8###) proposed using Python programs as a CoT prompt, demonstrating more accurate reasoning steps and significant improvements behind CoT prompting Wei et al. (2022  ###reference_b39###). Zhou et al. (2023  ###reference_b45###) introduced a code generation approach to verify the intermediate reasoning step OpenAI (2023  ###reference_b22###).\nIn parallel, there have been improvements in the accessibility of lower-parameter versions of Large Language Models (LLMs), which we define as Small Language Models (SLMs), on which previous CoT improvements cannot be fully observed Shridhar et al. (2023  ###reference_b30###); Ho et al. (2023  ###reference_b12###). Therefore, several works are emerging at this gap, aiming to transfer LLM reasoning properties to SLMs. Pioneering proposals in this direction proposed teacher-student alignment methods through a series of approaches geared towards the distillation of the knowledge generated by the teacher for the fine-tuning of the student Li et al. (2023b  ###reference_b16###); Magister et al. (2023  ###reference_b20###); Shridhar et al. (2023  ###reference_b30###).\nLater, Yue et al. (2023  ###reference_b42###) proposed specialized Instruction-tuning using Alpaca-like style demonstrations Taori et al. (2023  ###reference_b32###) specialized for mathematical tasks, while Luo et al. (2023  ###reference_b18###); Xu et al. (2023  ###reference_b41###) proposed supervised fine-tuning reinforced with rewarding algorithms."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Reinforcement Learning (RL)",
            "text": "A significant component that promotes the generative reasoning delivering CoT is provided by refinement via RL methods. Recent work that applies Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b29###) for aligning human preferences Ouyang et al. (2022  ###reference_b23###). Several methods have been proposed to improve the efficiency of alignment Azar et al. (2023  ###reference_b1###), including Direct Preference Optimization (DPO) Rafailov et al. (2023  ###reference_b25###).\nIn this work, we adopt RL to refine performance over conventional SFT. For mathematical problem solving, Uesato et al. (2022  ###reference_b34###) trained an outcome- or process-based reward model to perform re-ranking Cobbe et al. (2021  ###reference_b5###), achieving better performance than SFT and majority voting Wang et al. (2023b  ###reference_b36###).\nLuong et al. (2024  ###reference_b19###) adopted reinforcement learning as an extension of traditional supervised tuning.\nWe adopt DPO and automate the reward process in a teacher-student context. We focus on the transfer of CoT-style, step-wise reasoning and propose a refinement technique applied to models downstream of the instruction-tuning phase."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Self-refined Instruction-tuning",
            "text": "Complementing and enhancing foundational approaches Magister et al. (2023  ###reference_b20###); Uesato et al. (2022  ###reference_b34###); Li et al. (2023a  ###reference_b15###); Ho et al. (2023  ###reference_b12###),\nseveral papers have been published simultaneously Wang et al. (2023d  ###reference_b38###); Luo et al. (2023  ###reference_b18###); Wang et al. (2023a  ###reference_b35###); Paul et al. (2024  ###reference_b24###); Luong et al. (2024  ###reference_b19###); Ranaldi and Freitas (2024  ###reference_b26###) (Table 15  ###reference_### summarises the main features). These works prove the effect of supervised fine-tuning to transfer the ability to produce multi-step reasoned answers from larger to smaller models, as described in Section 5.2  ###reference_###.\nOur work goes beyond the state-of-the-art by:\nproposing a method for aligning CoT abilities by introducing Instruction-tuning via Demonstrations produced by answers generated by different LLMs, decentralizing the unique teacher model (in many cases GPT-3.5,4).\nanalyzing the alignment performance between in-family and out-family models on different tasks related to math reasoning, identifying crucial alignment factors that arise between teachers and students.\ninvestigating the impact of teacher-student alignment by adapting and promoting DPO Rafailov et al. (2023  ###reference_b25###) as a cornerstone method for eliminating performance gaps."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper proposes a novel approach for aligning multi-step CoT reasoning between teacher Large Language Models (LLMs) and student Smaller LMs (SLMs). In particular, our Self-refine Instruction-tuning is framed as an instruction tuning via Chain-of-Thought Demonstrations method based on explanations delivered by LLMs prompted by the CoT mechanism, which is then reinforced via the Self-refine phase that uses Direct Preference Optimization. We also contrast the impact of in-family and out-family alignment across teacher and student models. The results highlight the impact of teacher-student Instruction-tuning interventions as a mechanism to improve the multi-wise reasoning properties of smaller language models and promote the self-refinement abilities of instructed models to complete the alignment."
        }
    ]
}