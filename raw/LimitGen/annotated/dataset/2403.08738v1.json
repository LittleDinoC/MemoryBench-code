{
    "title": "Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations",
    "abstract": "Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite HuBERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Self-supervised learning (SSL)-based speech representations are becoming popular in speech processing and producing state-of-the-art results in many downstream tasks such as automatic speech recognition, speaker verification, keyword spotting, voice conversion, etc Yang et al. (2021). These representations are obtained using self-supervised learning on large amounts of unlabelled speech data. Wav2vec2 Baevski et al. (2020), HuBERT Hsu et al. (2021), and WavLM Chen et al. (2022) are a few examples of such SSL-based speech models. However, representations obtained from these models have not been extensively explored in the context of learning acoustic word embeddings (AWEs). AWEs are fixed-dimensional vector representations of spoken words that find applications in various downstream tasks, such as query-by-example search Settle et al. (2017); Yuan et al. (2018); Hu et al. (2021), keyword spotting Barakat et al. (2011), providing clues for human lexical processing Matusevych et al. (2020), hate speech detection in low resource settings Jacobs et al. (2023), etc.\n\nRecently, the work Sanabria et al. (2023) proposed extracting AWEs from SSL-based speech representations using a mean pooling mechanism. The authors suggest that SSL-based speech representations, which are contextualized, can be effectively converted into AWEs using a straightforward pooling mechanism. On the other hand, Correspondence Auto-Encoder (CAE) based training strategies for AWEs Kamper (2019) using MFCC Davis and Mermelstein (1980) features are shown to be promising in the literature. Correspondence training involves an auto-encoder where a spoken word serves as the input to the encoder, and the target output of the decoder is a different instance of the same spoken word. This approach helps to preserve acoustic-phonetic information while filtering out unnecessary details such as speaker, acoustic environment, and duration, etc. Both encoder and decoder are typically recurrent neural networks (RNNs). More details about the model will be presented in Sec. 2 and Sec. 4.2. Correspondence training has also been explored in the work Meghanani and Hain (2024) to improve content representations of SSL-based speech models.\n\nThe work Lin et al. (2023) uses a Correspondence Transformer Encoder (CTE) for obtaining robust AWEs, trained from scratch and a large-scale unlabelled speech corpus. In contrast, in this work, pre-trained SSL speech models are coupled with a simple RNN based auto-encoder for correspondence training to obtain robust AWEs. This work attempts to use the correspondence training of auto-encoder to obtain the AWEs by leveraging SSL-based speech representations instead of MFCC features as input features to the CAE model. Further, cross-lingual capabilities are also examined for SSL-based AWEs trained with CAE method. The SSL models (HuBERT, Wav2vec2, and WavLM) used in this work are pre-trained on English data. However, it has been demonstrated that these models work well as feature extractors for the all the languages considered in this study. The performances on the word-discrimination task for all the languages (Polish, Portuguese, Spanish, and French) are as good as on the English language (Sec. 5). A detailed analysis is also conducted to assess the importance of contextual information in spoken words by comparing feature extraction with and without context.\n\nThe main contributions of this work are as follows:\nUtilizing corresponding training with SSL-based speech representations to obtain highly discriminative AWEs.\nShowing effectiveness of SSL models, pre-trained only on English, as feature extractors in cross-lingual scenarios for obtaining high-quality AWEs.\nQuantitatively demonstrating that incorporating the context of the spoken word in SSL-based speech representations leads to the production of more robust AWEs.\n\nThe rest of the paper is as follows: Sec. 2 describes the correspondence auto-encoder methodology to obtain AWEs; Sec. 3 describes the data preparation and data statistics; Sec. 4 describes the details of the experiments; Sec. 5 describes the results and analysis; Sec. 6 concludes the work with possible future directions. Sec. 7 describes the limitations of the work."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Correspondence auto-encoder is trained with input as a spoken word and target output as a different instance of the same spoken word. Typically, Recurrent Neural Network (RNN) based encoder and decoder are used, hence the model is referred to as CAE-RNN. The rationale behind this training method is that CAE-RNN will preserve only the acoustic-phonetic information and filter out the other unnecessary information factors such as speaker, duration, acoustic environment, etc Kamper (2019  ###reference_b14###). Fig. 1  ###reference_### shows the CAE-RNN model setup. The input to the ENC is a sequence of acoustic feature vectors () of a spoken word. The target output is the sequence of acoustic feature vectors of the different instance of the same spoken word (). The encoder produces the AWE (e) of the spoken word , which is then fed to the decoder to reconstruct . The output of the decoder is represented as . The mean squared loss function for a single training pair () can be described as following:\n###figure_1### , where  is the target output and  is the output of the decoder as shown in Fig. 1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data Preparation",
            "text": "Five languages, namely Polish, Portuguese, Spanish, French, and English, are selected. Approximately 25,000 utterances are selected for the training set, 500 for the development set, and 500 for the test set. These selected utterances are force-aligned to obtain the spoken word boundaries using the Montreal Forced Aligner toolkit McAuliffe et al. (2017). Only spoken words with a duration of 0.5 seconds or longer are included, following the standard practice in the literature He et al. (2017). Spoken words with a frequency greater than 50 or less than 5 are excluded. The dataset presents a summary of the statistics for the final extracted dataset, encompassing all five languages. The speakers across different sets are non-overlapping, which is a desirable characteristic for evaluating AWEs as they should be robust to speaker variations. Polish language had limited available data and consequently has the fewest number of speakers, while English has the highest number. The duration represents the total time duration of spoken words across the different sets."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Experiments are conducted on all the five languages with SSL-based speech representations as input features extracted from Wav2vec2, HuBERT, and WavLM. Experiments are also conducted with MFCC as input features.\nFirst, the feature extraction methods for various SSL-based speech representations and MFCCs are described. Then, the configuration of the CAE-RNN model is explained, along with the mean pooling baseline Sanabria et al. (2023  ###reference_b23###) and the AE-RNN method (without correspondence training), for comparison. Next, the word discrimination task is described, which is used for evaluating the quality of the extracted AWEs. Finally, the training details of the CAE-RNN and other models are provided."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Feature Extraction",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 SSL-based Speech Representations",
            "text": "SSL models are pre-trained on large amount of unlabelled speech data. The task defined for the pre-training is known as the pretext task. Each model differs based on how the pretext task was defined, the data used for pre-training, and the model architecture. In this work, the “BASE” architectures of Wav2vec2, HuBERT, and WavLM model 222https://github.com/pytorch/fairseq  ###reference_### (all with  95M parameters) are used for feature extraction. All these models are pre-trained on 960 hours of LibriSpeech data Panayotov et al. (2015  ###reference_b21###). A “BASE\" architecture typically has a multi-layer CNN-based feature encoder followed by 12 Transformer layers. In this work, representations from each model are extracted from the final (i.e. 12th) Transformer layer. For all the above mentioned SSL models, 768-dimensional feature vectors are obtained for each spoken word at a framerate of 20 ms.\nSSL-based speech representations are extracted in two different ways: the first one is extracted using the context around the spoken word, and the other one is extracted without the context, as described here:\nWith context: In this case, first the SSL-based speech representations of the entire spoken utterance are computed and then the time boundaries of the spoken word is used to get the representations of the segment belonging to the spoken word. This ensures that the extracted representations capture the context around the spoken word as the entire utterance is processed by the SSL model. Let us assume  represents an utterance and  represents a spoken word instance present in the utterance  with start and end timestamps denoted as  and . If  represents the SSL model, then the SSL-based speech representation for the entire utterance is computed . Then the speech representations for the spoken word  will be .\nWithout context: In this case, no context is considered and SSL-based speech representations are extracted by inputting only speech segments belonging to the spoken words to the SSL models. Hence, in this case, the SSL-based speech representation for the spoken word  will be ."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 MFCC Features",
            "text": "For each spoken word, 20-dimensional MFCC features are extracted with 30 ms window size and 20 ms shift along with delta and delta-delta features, which leads to 60-dimensional MFCC feature vectors."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Model Details",
            "text": "A 4-layer Bidirectional GRU with a hidden dimension of 256 is used for both the encoder and decoder in the CAE-RNN model. Dropout rate is set to 0.2. The final hidden state of the encoder-GRU is fed to a fully connected layer to obtain 128-dimensional AWE (e) as shown in Fig. 1  ###reference_###. This embedding is then fed to the decoder at each time step as input to the decoder Kamper (2019  ###reference_b14###). The output of the decoder is then fed to a fully connected layer to produce the target output.\nA regular auto-encoder RNN (AE-RNN) model is also used as one of the baselines with similar configurations. AE-RNN model is an auto-encoder model where input and target output is exactly the same spoken word, i.e. input-output training pair is (). A mean pooling model is also used as baseline Sanabria et al. (2023  ###reference_b23###), which does not require any training. This method computes the mean of the SSL-based speech representations to get the 768-dimensional AWE of a spoken word."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Word Discrimination Task",
            "text": "To evaluate the AWEs, the same-different word-discrimination task is used Kamper et al. (2015  ###reference_b15###); Carlin et al. (2011  ###reference_b6###). First, all possible spoken word pairs are generated. For example, if there are total N spoken words, then the total generated spoken word pairs for comparison will be . After that, the cosine distance between the AWEs of these pairs are computed and compared with a threshold to decide whether the spoken words are same or different. The average precision (AP) is calculated by varying all the possible threshold values, which is the area under the precision-recall curve. AP is reported for the same-different word discrimination task.\nWord-discrimination task is applied on the test set, which has unseen speakers during training.\nAlso, a subset of the test set is created for all five languages in such a way that none of the words in the subset are encountered during training. This particular subset is referred to as . The word-discrimination task is also conducted on . The total number of generated spoken word pairs for both the test and  sets is described in Table 2  ###reference_### for all languages."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Training Details",
            "text": "The total number of generated correspondence training pairs () for each language is as follows: 9,55,106 for Polish, 11,63,468 for Portuguese, 7,80,197 for Spanish, 7,95,613 for French, and 9,72,532 for English. The remaining training details are as follows for various inputs:\nSSL-based Speech Representations as Input:\nCAE-RNN models are trained for 30 epochs, using a learning rate of 0.0001 with Adam optimizer and a batch size of 512. In each run, the model with the best performance on the development set in terms of word-discrimination is selected as the final model for evaluation on the test set. AE-RNN models are trained for 50 epochs, keeping all other parameters same as mentioned above for the CAE-RNN models.\nMFCC as Input: Both AE-RNN and CAE-RNN models with MFCC as inputs are trained for 100 epochs, using a learning rate of 0.0001 with Adam optimizer. The batch size for the AE-RNN model was chosen as 64, while for the CAE-RNN model it was set to 256 based on preliminary experiments for better convergence. Similarly to the previous case, the model with the best performance on the development set in terms of word-discrimination is selected as the final model for evaluation on the test set."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Table 3  ###reference_### shows the baseline results with MFCC features as input for the AE-RNN and CAE-RNN models. This demonstrates the effectiveness of the CAE-RNN model over the AE-RNN model for the word-discrimination task, as the CAE-RNN consistently outperforms the AE-RNN for all languages. Table 4  ###reference_### presents the results for the derived subset of the test set () with similar trends. It is worth noting that the AP on the  set is relatively better than that of the original test set in most cases. This is likely due to the fact that the number of spoken word pairs generated for the evaluation on the  is significantly fewer compared to the original test set, as mentioned in Table 2  ###reference_###.\nTable 5  ###reference_### displays the results obtained from using various SSL-based speech representations (Wav2vec2, WavLM, and HuBERT) as input features, combined with different AWE extraction methods (mean pooling, CAE-RNN, and AE-RNN). The results presented in Table 5  ###reference_### represent the AP for the word-discrimination task on the test set, employing different SSL-based speech representation feature extraction setups (‘with context’ and ‘without context’).\nFrom Table 5  ###reference_###, it is evident that the AWEs derived ‘with context’ exhibit greater robustness. The AP on the test set for all languages is significantly better when utilizing SSL-based speech representations ‘with context’ compared to the feature extraction ‘without context’.\nAs shown in Table 5  ###reference_###, the CAE-RNN model demonstrates superior performance when using SSL-based speech representations as input features compared to the MFCC-based baseline model (Table 3  ###reference_###) across all languages. Furthermore, Table 5  ###reference_### provides a comparison of the CAE-RNN model with other baseline models (mean pooling and AE-RNN) when utilizing SSL-based speech representations as input features. CAE-RNN consistently outperforms both the AE-RNN and mean pooling methods for all languages and SSL models. Another advantage of the CAE-RNN model over mean pooling is that the AWEs obtained from CAE-RNN have a dimension of 128, while mean pooling-based AWEs are 768-dimensional. Based on the results presented in Table 5  ###reference_###, it is evident that the HuBERT features consistently achieve the best performance across all configurations and languages. Specifically, when using the CAE-RNN method for AWE extraction and SSL-based speech representations extracted ‘with context’, the HuBERT achieves the highest AP on the test set: 0.90 for Polish, 0.88 for Portuguese, 0.95 for Spanish, 0.74 for French, and 0.86 for English. The performance order can be sorted as HuBERT  Wav2vec2  WavLM  MFCCs when using the CAE-RNN-based AWE model and SSL-based speech representations extracted ‘with context’.\nTable 6  ###reference_### presents the results for the  set, which includes unseen words and speakers. The models exhibit similar trends in performance in this case as well. This provides evidence that the proposed methodology performs equally well on unseen words.\nOne interesting finding is that the SSL-based speech representations considered in this work were pre-trained solely on English language. Despite this, they are capable of generating meaningful features for other languages, resulting in good performance as demonstrated in Table 5  ###reference_### and 6  ###reference_### for the word-discrimination task."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Cross-lingual Analysis",
            "text": "To assess the effectiveness of SSL speech representation-based CAE-RNN models in cross-lingual settings, a CAE-RNN model trained on one source language (English in this case) is evaluated on four different target languages. This evaluation can be considered a ‘zero-shot’ evaluation, as no training data from the target languages is required. Table 7  ###reference_### displays the results in terms of AP for the word-discrimination task on the test set and  set for the four target languages (Polish, Portuguese, Spanish, and French).\nIn this scenario as well, the HuBERT-based CAE-RNN model achieves the best performance across all languages, except for French where Wav2vec2 performs the best. It is worth noting that the CAE-RNN model in the ‘zero-shot’ setting outperforms the mean pooling method (Table 5  ###reference_### and 6  ###reference_###) Sanabria et al. (2023  ###reference_b23###) and the CAE-RNN model trained on the target language with MFCC features (Table 3  ###reference_### and 4  ###reference_###).\nThe mean pooling method Sanabria et al. (2023  ###reference_b23###) can be considered a ‘zero-shot’ AWE extraction method, as it does not involve additional training on top of the pre-trained SSL models. In a ‘zero-shot’ setup for target languages, using a CAE-RNN trained on a well-resourced source language can offer an advantage over the mean pooling method. In conclusion, SSL-based CAE-RNN models have fairly good performance when used crosslingually. There have been earlier studies Kamper et al. (2021  ###reference_b16###) on acoustic word embeddings for zero-resource languages using multilingual transfer with MFCC features, which worked well. Also, intuitively, some generalisation was expected as the aim of modelling is to compress a small segment of speech into a fixed dimensional vector. There might be a language effect on pre-trained SSL speech models but the basic speech properties are still invariant to changes in the language. The cross-lingual ability of SSL-based CAE-RNN models to obtain AWEs can support many applications such as speech search, indexing and discovery systems for languages with low-resources Kamper et al. (2021  ###reference_b16###).\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Analysis of Anagram Pairs",
            "text": "Anagrams are words that can be formed by rearranging the letters of another word. Analysing anagram pairs provides insights into the impact of letter order on AWE representation. Robust AWEs should capture the letter order in spoken words. For this analysis, same spoken word pairs and anagram pairs are chosen from different speakers. Ideally, the cosine distance between the same word pairs should be close to 0, while anagram word pairs should be close to 1. In Table 8  ###reference_###, HuBERT-based CAE-RNN AWEs demonstrate cosine distances of approximately 0.01, 0.11, and 0.02 for the same spoken word pairs ‘aside’, ‘this’, and ‘no’, respectively. The anagram pairs of the words ‘aside’, ‘this’, and ‘no’ (i.e., ‘ideas’, ‘hits’, and ‘on’) have distances of 0.99, 0.50, and 0.69, respectively, for the HuBERT-based CAE-RNN model. These values are significantly better for both the same word pairs and anagram word pairs when compared to the HuBERT-based mean pooling method Sanabria et al. (2023  ###reference_b23###). This indicates that the HuBERT-based CAE-RNN model accurately captures the letter order in a word compared to the mean pooling baseline Sanabria et al. (2023  ###reference_b23###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "AWE Visualisation",
            "text": "t-SNE visualization is used to plot the 2-dimensional representations of the derived AWEs for all five languages. From each language, all spoken instances of the top 7 words with the highest frequency count from the test set are chosen. The plots demonstrate distinct and well-separated clusters for each spoken word across all languages. One interesting pattern can be observed for the Polish language, where the clusters of the spoken words ‘Owadów’ and ‘Owady’ share the boundary and are closely related in the AWE space. This is likely due to the fact that the first four letters of both the words (o, w, a, d) are shared and these words only differ in their endings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "It has been demonstrated that SSL-based speech representations with CAE-RNN models outperform mean pooling and AE-RNN models across all languages. They also outperform MFCC-based models. Among all the SSL models, HuBERT performs the best when used as input for the CAE-RNN model, outperforming models such as Wav2vec2 and WavLM. Notably, despite being pre-trained on English data, the SSL models exhibit excellent performance on other languages, showcasing their cross-lingual generalization capability for AWE extraction.\n\nFurthermore, quantitative analysis reveals that incorporating context information of the spoken word leads to more robust AWEs. The HuBERT-based CAE-RNN model trained on English language and tested on other target languages outperforms the mean pooling method and the CAE-RNN model trained on the target language using MFCC features. This ‘zero-shot’ method to obtain robust AWEs for the target language can be useful in applications for low-resource languages. An analysis was also conducted to show that the CAE-RNN model effectively captures the order of letters in a word.\n\nIn future work, experiments will be conducted with the “LARGE\" variation of SSL models, as well as multilingual pre-trained SSL models such as Wav2vec2-XLSR. Additionally, an interesting experiment would involve training a single universal AWE model on all languages and comparing its performance with language-specific AWE models. Further research will focus on measuring the performance gains of SSL-based CAE-RNN models on downstream tasks such as query-by-example search and keyword spotting."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This work is focused on the extraction of AWEs and measuring their quality solely based on the word discrimination task. No downstream applications such as query-by-example search and keyword spotting, have been discussed using the improved AWEs. In this work, only the “BASE” versions of the SSL-based speech models are explored for experiments and analysis. There are other variations, such as “LARGE” version, for which this study can be extended. All the languages considered in this work belong to the Indo-European language family. This work does not contain the analysis of languages that belong to another language family, such as Dravidian or Afroasiatic language families. This work does not deal with layer-wise analysis, which can provide better insights for further improving the AWEs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also funded in part by LivePerson, Inc."
        }
    ]
}