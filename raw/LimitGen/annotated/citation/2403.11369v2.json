{
    "title": "What Makes Math Word Problems Challenging for LLMs?",
    "abstract": "This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.111Our code, data, and analysis are publicly available at github.com/kvadityasrivatsa/analyzing-llms-for-mwps",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLMs) have not only demonstrated huge potential across a range of core NLP tasks (Zhao et al., 2023  ###reference_b18###; Brown et al., 2020  ###reference_b3###; Radford et al., 2019  ###reference_b12###, inter alia), but also exhibited a number of emergent abilities, such as an ability to solve mathematical puzzles Wei et al. (2022  ###reference_b16###). Math word problems (MWPs) have been proposed as a challenging testbed for LLMs, as they test not only the ability of the models to deal with purely mathematical expressions, but also their reasoning and natural language understanding abilities (Wang and Lu, 2023  ###reference_b15###; Cobbe et al., 2021  ###reference_b4###; Patel et al., 2021  ###reference_b11###; Miao et al., 2020  ###reference_b9###, inter alia). Experiments show that even quite powerful LLMs are still challenged by MWPs Cobbe et al. (2021  ###reference_b4###). At the same time, most previous work has either focused on evaluation of LLMs’ performance on MWPs or on changes in their behavior in response to progressive-hint prompting, prompt paraphrasing or similar approaches Norberg et al. (2023  ###reference_b10###); Raiyan et al. (2023  ###reference_b13###); Zheng et al. (2023  ###reference_b19###); Zhu et al. (2023  ###reference_b20###), while an in-depth analysis of what exactly makes math problems challenging for LLMs is lacking. We aim to address this knowledge gap.\n\n###figure_1### A recent study by Almoubayyed et al. (2023  ###reference_b1###) demonstrates a strong connection between reading skills and math outcomes in students. We hypothesize that LLMs’ ability to solve MWPs correctly may similarly rely on: (1) the linguistic complexity of the questions; (2) the conceptual complexity of the tasks (e.g., the number of steps and types of math operations involved); and (3) the amount of real-world knowledge required to solve the tasks. Supporting this intuition, our preliminary analysis of the GSM8K dataset Cobbe et al. (2021  ###reference_b4###) suggests that relatively short questions with a small number of described entities, a few calculation steps, and a limited range of operators involved in the solution (e.g., Mark is 7 years older than Amy, who is 15. How old will Mark be in 5 years?) are typically answered correctly by a range of LLMs. At the same time, long questions requiring real-world knowledge (e.g., how many cents there are in a dollar) and extended natural language understanding (NLU) (e.g., interpretation of a lower price) pose challenges for LLMs (see Figure 1  ###reference_###).\n\nIn this paper, we formulate and investigate two research questions: (1) Which characteristics of the input math word questions make them complex for an LLM? and (2) Based on these characteristics, can we predict whether a particular LLM will be able to solve specific input MWPs correctly?\n\nAI in music composition is rapidly advancing, with algorithms creating complex musical pieces (Briot et al., 2020). Systems like OpenAI's MuseNet and Google's Magenta have pushed boundaries, generating music in diverse styles and contexts (Huang et al., 2020). These innovations highlight AI's capacity to transform creative processes, merging technology with artistry."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We use the GSM8K dataset  Cobbe et al. (2021  ###reference_b4###), divided into  training and  test instances, because of the high quality of human-generated MWPs. This dataset contains a diverse set of problems in English with minimal amount of recurring templates. Furthermore, the difficulty level of the problems is tailored for LLMs, allowing for a wide variation in correctness across models and question types, which is ideal for our feature-based analysis.\nWe collect solution attempts from several LLMs to the questions from the GSM8K training and test sets. Next, we train statistical classifiers on a filtered subset of questions to predict if they are consistently solved correctly or incorrectly across multiple runs of the models. Our approach is relatively simple\nbut it allows us to investigate which of the features are most indicative of the challenges LLMs face in solving math problems.\nWe select an array of open-source models for our experiments. We use Llama2 (13B and 70B) Touvron et al. (2023  ###reference_b14###), Mistral-7B  Jiang et al. (2023  ###reference_b6###) as its performance on math tasks has been found to match models several times its size, and MetaMath-13B Yu et al. (2023  ###reference_b17###) as it is fine-tuned on math QA data in contrast to the other general-purpose models in the pool.\nWe analyze and experiment with the features extracted from MWP questions and their respective expected solutions. This way, the features remain grounded in the dataset, allowing our approach to be applied to any LLM. The features are broadly grouped into the following categories:222The complete list of features extracted, their description and further statistics can be found in Appendix A  ###reference_###\nLinguistic features focus on the phrasing of the question. These include the length of the question, sophistication of the vocabulary, syntactic complexity, instances of coreference, and overall readability. Note that the linguistic features are only extracted from the question body as the phrasing of the gold solution has no impact on the expected answer.\nMathematical features cover the math arguments, operations, and reasoning steps required to solve the questions. These include the number and diversity of the math operations in the solution body. Arguments provided in the question but not utilized in the solution also require mathematical reasoning for them to be disregarded as noise. Note that while a question can be phrased in many ways (affecting its linguistic features), the underlying math operations and reasoning steps (thus, the mathematical features) remain unchanged.\nReal-world knowledge & NLU based features indicate the amount of extraneous information needed to solve the task that is not provided explicitly in the question. This may include how many days there are in a month or the interpretation of “half” as ."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease.\nFor high confidence samples, we use the training and test subset from GSM8K where the sampled success rate is either  (always correct) or  (never correct). The distribution of the LLM-specific splits is detailed in Table\n2  ###reference_###.\nWe employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C  ###reference_### for more details."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Solution Generation",
            "text": "To collect solution attempts from the LLMs, we use a simple task-specific prompt (See Appendix B  ###reference_###) to minimize any bias imposed on the model generation. We query each LLM  times on each question with varying generation seeds and a temperature of . A soft-matching strategy is then used to extract the final answer from the solutions. Using each LLM’s attempted solutions, every question is assigned a mean success rate using (# of correct answers) / (# of solution attempts)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Success Rate Prediction",
            "text": "We train and evaluate classifiers on their ability to predict for input test questions whether they will be answered correctly or incorrectly by a specific LLM. We also train and evaluate classifiers on the intersection set of questions, which are either solved correctly by all or by none of the LLMs.\nWe use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease.\nFor high confidence samples, we use the training and test subset from GSM8K where the sampled success rate is either  (always correct) or  (never correct). The distribution of the LLM-specific splits is detailed in Table\n2  ###reference_###  ###reference_###.\nWe employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C  ###reference_###  ###reference_### for more details."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Success Rate Distribution",
            "text": "We report the mean success rates for each LLM on GSM8K’s test set in Table 1  ###reference_###.333Our results generally align with those reported previously for these models.\n We observe that Llama2 13B and 70B follow the expected order of scores along their respective parameter counts. Mistral-7B scores similar to the 13B Llama2 model, and the additional fine-tuning allows MetaMath-13B to outperform the other models (including the 70B Llama2). Figures 2(a)  ###reference_sf1### and 2(b)  ###reference_sf2### respectively capture the number of questions always and never answered correctly by each LLM. Overall, MetaMath-13B has the lowest number of incorrectly and the highest number of correctly answered questions across the tested LLMs.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Classification Results",
            "text": "To compare classifiers’ performance, we report the accuracy and macro-F1 scores for each classifier and LLM-specific test data split (see Table 2  ###reference_###). We observe that Random Forest outperforms other classifiers across most solution sets.\nAt the same time, we also note that, due to significant class imbalance, this task is not easy for the classifiers, with the best accuracy scores across LLM splits being in the range of . The small number of questions always or never solved correctly by any LLM speaks to the models’ varying capabilities (and potential points of brittleness). We include additional analysis of the results in Appendix D  ###reference_###.\nFor comparison, we also report the classification results for a fine-tuned RoBERTa-base model Liu et al. (2019  ###reference_b8###) for the same training and evaluation sets (tuned on the question and gold solution as input text; see Appendix C  ###reference_### for more details) in Table 2  ###reference_###. We note that the Transformer base classifier scores on a par or a few points above the best statistical classifier, i.e., Random Forest, suggesting that the proposed feature-based classifiers are not far behind token-level contextual models for this task."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Feature Importance",
            "text": "The statistical classifiers used in our experiments allow us to estimate the importance of each feature and its contribution to the classification performance. We report the top  features with the highest aggregate ranks across LLM data splits and classifiers in Table 3  ###reference_###. We use mean rank here as a proxy for relative importance across features, and the respective standard deviations indicate how spread out this importance is across classifiers and queried LLMs. We observe that a greater number (Gx_op_unique_count) and diversity (Gx_op_diversity) in math operations, and the use of infrequent numerical tokens in the question and solution body (Qx_ & Gx_mean_numerical_word_rank) impact the success rate. The list also contains linguistic features based on the phrasing of the questions: longer questions with a high number of noun phrases (Qx_np_count), mean syntactic depth (Qx_constituency_tree_depth), and readability grade (Qx_flesch_kinkaid_grade) are also difficult for LLMs to solve. Additionally, the need for extraneous information (Gx_world_knowledge), such as conversion units for time, distance, or weight, can make a question challenging. We also report value thresholds at which each feature affects the success rate significantly: see the results of the Student’s t-test and p-values in Table 7  ###reference_### in Appendix D  ###reference_###."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Ablation Studies",
            "text": "To further measure the impact of each feature type, we report classification scores along different feature-type subsets in Figure 3  ###reference_###. We note that the feature set with all types (L+M+W) is not optimal for classification. For instance, the questions answered by Llama2-13B are best classified using only mathematical features (M). The best-performing classifiers for Llama2-7B, MetaMath-13B, and the intersection set either solely use linguistic features (L) or both linguistic and math features (L+M), whereas the world knowledge & NLU feature set if sufficient for Mistral-7B.\n###figure_4###"
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Impact of Linguistic Features",
            "text": "In order to better gauge the impact of linguistic features on the success rate, we cluster questions by mathematical features. We fit a KMeans clustering model444https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html  ###reference_nerated/sklearn.cluster.KMeans.html### on all math features for each question in the GSM8K training set with a target cluster count of . This helps group together questions from the data, wherein the math features hardly vary within each question subset (or cluster). Thus, variations in success rate across the questions within a cluster can be more clearly attributed to other, i.e., linguistic types of features. We report some notable Spearman correlation values between the linguistic feature values within a cluster and the corresponding success rates in Table 4  ###reference_###. The strong and significant feature-wise negative correlations suggest that for a relatively fixed set of math features, questions with greater length, nesting, lexical rank, and reading grade become more challenging for LLMs to solve. Note that this form of analysis on feature-based minimal pairs is extractive in nature and may, to a certain extent, be restricted to the question types in the GSM8K dataset. For a more exhaustive analysis for each feature, generative approaches to furnish question paraphrases with the desired set of linguistic features need to be employed."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This work aims to identify what aspects of MWPs make them difficult for LLMs to solve. To this end, we extract key features (spanning linguistic, mathematical, and real-world knowledge & NLU-based aspects) to predict whether several LLMs can reliably solve MWPs from GSM8K. We find that questions with a high number and diversity of math operations using infrequent numerical tokens are particularly challenging to solve. In addition, we show that lengthy questions with low readability scores and those requiring real-world knowledge are also seldom solved correctly. Our future work will rely on these findings to make informed modifications to questions in order to study the impact on LLMs’ reasoning and MWP-solving abilities. Figure 4  ###reference_### provides an example of an informed modification, which leads to improved LLM performance.\n###figure_5###"
        }
    ]
}