{
    "title": "Analyzing the Performance of Large Language Models on Code Summarization",
    "abstract": "Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other.\n\n\n\nKeywords: Natural Language Generation, Neural Language Representation Models, Summarisation",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "There is a growing interest in applying NLP techniques to tasks related to automated program understanding, generation, and retrieval, all of which promise to improve access to code. Popular tasks include Code Summarization (translating code into natural language, e.g.  Miceli Barone and Sennrich, 2017  ###reference_b24###), Code Generation, Code Completion, Code Translation and Natural Language Code Search (retrieving a code snippet given a natural language query, e.g. Gu et al., 2018a  ###reference_b10###). There is a practical need for such systems since the ability to automatically generate code snippets or doc strings or search large code bases can significantly increase the productivity of software developers. However, there is also a growing interest in developing models and datasets for these tasks within the NLP community, often driven by an assumption that code can be seen as a semantic interpretation of its natural language description.\nBut while current models show impressive performance, it is still important to analyze how much understanding these models have of the structure or semantics of the code. To make their code more human-readable, software developers often employ English words in the names of functions, variables, or data structures. And, although they did not evaluate the most recent web-scale large language models, the authors of MCoNaLa Wang et al. (2023  ###reference_b46###) showed that the performance of code generation models drops significantly compared to English if the input is in Spanish, Japanese, or Russian.\nWe, therefore, ask to what extent the large language models (LLMs) that are used for tasks like code generation or summarization actually understand the semantic relation between natural language and code, and to what extent they simply rely on this superficial token similarity.\nIn this paper, we attempt to address this question by analyzing the performance of large language models (LLMs) on code summarization. Our analysis aims to shed light on the following, more specific questions: (1) to what extent do the summaries generated by these models simply consist of tokens that are directly copied from the code? (2) How much does model performance depend on the presence of function names that give away the semantics of the code? (3) To what extent do these models rely on the syntactic structure and underlying logic of the code?\nTo answer the first research question, we split the examples from the dataset into different buckets depending on the token overlap between the code and the description and see how much the performance varies across those buckets. For the second and third research questions, we make several transformations to the code before feeding it to an LLM. This includes changing or obfuscating certain function names, and removing the control structures in the body of the code. We then examine their impact on code summarization performance. We observe the effect of the code in standard datasets like CodeXGLUE having informative function and identifier names with a high token overlap with their descriptions, and analyze how this affects model behavior. This token overlap between the function names and the target summary makes the task easier. Our experiments111The code for this paper will be released at https://github.com/rajarshihaldar/analyze-llm-code-summarization  ###reference_llm-code-summarization### show that the performance of several state-of-the-art LLMs is often due to the high string similarity of the natural language descriptions to the code they are paired with."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background",
            "text": "CodeXGLUE Lu et al. (2021  ###reference_b23###) is a standard benchmark for several code-NL tasks including code summarization. It is a filtered subset of CodeSearchNet Husain et al. (2019  ###reference_b16###), consisting of code snippets paired with English descriptions that are scraped from public open-source GitHub repositories for Go, Java, JavaScript, PHP, Python, and Ruby. The description paired with each code snippet is the first paragraph of the repository. Examples were filtered out from this dataset if their code could not be parsed into an abstract syntax tree, or if their descriptions were not in English, contained special strings like \"http://\", were empty, or not between 3 and 256 tokens in length. Figure 1  ###reference_### shows an example from the dataset being used for code summarization. In our experiments, we use the Python examples from CodeXGLUE, which contain 251,820 training, 13,914 dev, and 14,918 testing data points.\nBLEU Papineni et al. (2002  ###reference_b28###) is computed by matching n-grams between generated summaries and human written summaries. A perfect match between the two summaries would give a score of 100%, and a perfect mismatch would give a score of zero. BLEU-4 is the standard metric for evaluating code summarization Shi et al. (2022  ###reference_b34###). In this case, n-grams are matched up to . This matching is done cumulatively: the BLEU scores for n-grams for values of n = 1 through 4 are computed, and then the mean of these four scores is computed to get the final score.\nIt was found that different implementations of BLEU-4 lead to different results Shi et al. (2022  ###reference_b34###). Out of these variants, the sentence BLEU metric based on NLTK Bird et al. (2009  ###reference_b4###) smoothing method 4 had the highest correlation with human evaluation. The scores we report are computed by the implementation of this method in NLTK 3.8.1."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Large Language Models For Code",
            "text": "Applying natural language processing techniques to source code has produced outstanding results. Code2vec Alon et al. (2019  ###reference_b2###) showed that techniques that are used to induce semantic embeddings or vectors for natural language input also work well to represent input code snippets and predict their semantic properties. DeepCS Gu et al. (2018b  ###reference_b11###) showed that by mapping both code and natural language prompts to embeddings you could perform retrieval on code. Another breakthrough was the introduction of sequence-to-sequence (seq2seq) models for generating comments for a given input code Hu et al. (2018a  ###reference_b14###). These initial models treat code as a sequence of tokens and were quickly followed by models that account for the structure of the code through Abstract Syntax Trees (Zhang et al., 2019a  ###reference_b49###; Wan et al., 2019  ###reference_b41###; Haldar et al., 2020  ###reference_b13###), Graph Neural Networks (Sieper et al., 2020  ###reference_b36###; Ling et al., 2021  ###reference_b20###; Liu et al., 2021  ###reference_b21###), and Graph Attention Neural Networks Wang et al. (2022a  ###reference_b43###).\nInspired by the success of transformer-based Vaswani et al. (2017  ###reference_b40###), pre-trained large language models (LLMs) like BERT Devlin et al. (2019  ###reference_b7###), XLNet Yang et al. (2019  ###reference_b48###), GPT Brown et al. (2020  ###reference_b5###), RoBERTa Liu et al. (2019  ###reference_b22###), SynCoBERT Wang et al. (2021a  ###reference_b42###), and T5 Raffel et al. (2020  ###reference_b30###) on core NLP tasks, and facilitated by the availability of large datasets that pair natural language with code, e.g. CodeSearchNet Husain et al. (2019  ###reference_b16###), LLMs are now commonly used for tasks that involve source code and natural language. CodeBERT Feng et al. (2020  ###reference_b9###), and its successor GraphCodeBERT Guo et al. (2020  ###reference_b12###), were two of the initial wave of LLMs trained on paired natural (NL) and programming language (PL) sequences. Soon other LLMs like PLBART Ahmad et al. (2021  ###reference_b1###) and CodeT5 Wang et al. (2021b  ###reference_b44###) made significant gains in code summarization. At the same time, models like CodeGen Nijkamp et al. (2023  ###reference_b26###) and Codex Chen et al. (2021  ###reference_b6###) made gains in code generation.\nThe current state of the art includes even larger instruction-tuned models that can perform a wide variety of tasks including ones related to code-NL, e.g. GPT-4 OpenAI (2023  ###reference_b27###), Llama 2 Touvron et al. (2023  ###reference_b39###) and PaLM 2 Anil et al. (2023  ###reference_b3###). Instead of being fine-tuned for a specific task, these models are trained to respond to a prompt describing a task followed by an input. This leads to improved performance on a variety of code-NL tasks. While GPT-4 has the best performance222Although it is unclear what specific data GPT-4 has been trained on, their training data is likely to include the code snippets contained in the datasets used in this paper and data contamination is known to be a significant factor in the performance of these models on coding tasks Narayanan and Kapoor (2023  ###reference_b25###).\nMoreover, GPT-4 is frequently updated behind the scenes based on the data customers provide. This makes any analyses on this data non-reproducible. Due to these concerns, we have not included GPT-4 in our studies., some researchers have found ways to improve its output further. For example, leveraging verbal reinforcement on an LLM improves its code generation capabilities Shinn et al. (2023  ###reference_b35###).\nAlthough current LLMs differ in important details (size, amount of training data, pre-training regime, fine-tuning, etc.), they use a form of subword tokenization Kudo and Richardson (2018  ###reference_b18###); Sennrich et al. (2016  ###reference_b33###), in which words are split into shorter strings (e.g. extracts  ext ract s) and underscores are treated as separators (e.g. from_url  from _ url), such that the model’s vocabulary consists of subwords rather than whitespace-delineated tokens. Figure 2  ###reference_### shows how even just a function definition reveals valuable information about the tokens that are likely to occur in the reference description when it is tokenized by a subword tokenizer (here, Llama 2)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Code Summarization",
            "text": "Code Summarization systems translate code snippets into automatically generated English summaries that describe what the code does. Given an input code snippet , the system has to return a description  that accurately describes what that code does. Figure 1  ###reference_### shows an example of code summarization being performed.\nWhen the task was first introduced, template-based approaches Sridhara et al. (2010  ###reference_b38###) requiring expert domain knowledge were used, followed by information retrieval-based approaches Eddy et al. (2013  ###reference_b8###); Rodeghero et al. (2014  ###reference_b31###). In IR-based approaches, the model would extract the most relevant tokens from the code to generate a term-based summary. This did not require any domain knowledge but the models used did not show any deep understanding of the code structure and instead relied on informative function names and comments to generate summaries. This was followed by early neural approaches Iyer et al. (2016  ###reference_b17###), Encoder-decoder frameworks like Hu et al. (2018a  ###reference_b14###) and transformer-based approaches, like PLBART Ahmad et al. (2021  ###reference_b1###), Structure-induced Transformers Wu et al. (2021  ###reference_b47###), CoTexT Phan et al. (2021  ###reference_b29###) CodeGPT Lu et al. (2021  ###reference_b23###), Codex Chen et al. (2021  ###reference_b6###), and CodeT5 Wang et al. (2021b  ###reference_b44###). Recently, these models have been surpassed by large language models using decoder-only architectures like PaLM 2 Anil et al. (2023  ###reference_b3###), GPT-4 OpenAI (2023  ###reference_b27###), and Llama 2 Touvron et al. (2023  ###reference_b39###).\nBenchmark datasets for this task include TL-CodeSum Hu et al. (2018b  ###reference_b15###), Funcom LeClair et al. (2019  ###reference_b19###), CodeSearchNet Husain et al. (2019  ###reference_b16###) and more recently, CodeXGlUE Lu et al. (2021  ###reference_b23###).\n###figure_1### CodeXGLUE Lu et al. (2021  ###reference_b23###  ###reference_b23###) is a standard benchmark for several code-NL tasks including code summarization. It is a filtered subset of CodeSearchNet Husain et al. (2019  ###reference_b16###  ###reference_b16###), consisting of code snippets paired with English descriptions that are scraped from public open-source GitHub repositories for Go, Java, JavaScript, PHP, Python, and Ruby. The description paired with each code snippet is the first paragraph of the repository. Examples were filtered out from this dataset if their code could not be parsed into an abstract syntax tree, or if their descriptions were not in English, contained special strings like \"http://\", were empty, or not between 3 and 256 tokens in length. Figure 1  ###reference_###  ###reference_### shows an example from the dataset being used for code summarization. In our experiments, we use the Python examples from CodeXGLUE, which contain 251,820 training, 13,914 dev, and 14,918 testing data points.\nBLEU Papineni et al. (2002  ###reference_b28###  ###reference_b28###) is computed by matching n-grams between generated summaries and human written summaries. A perfect match between the two summaries would give a score of 100%, and a perfect mismatch would give a score of zero. BLEU-4 is the standard metric for evaluating code summarization Shi et al. (2022  ###reference_b34###  ###reference_b34###). In this case, n-grams are matched up to . This matching is done cumulatively: the BLEU scores for n-grams for values of n = 1 through 4 are computed, and then the mean of these four scores is computed to get the final score.\nIt was found that different implementations of BLEU-4 lead to different results Shi et al. (2022  ###reference_b34###  ###reference_b34###). Out of these variants, the sentence BLEU metric based on NLTK Bird et al. (2009  ###reference_b4###  ###reference_b4###) smoothing method 4 had the highest correlation with human evaluation. The scores we report are computed by the implementation of this method in NLTK 3.8.1."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   How well do LLMs perform on Code Summarization?",
            "text": "###figure_2### We now show how some recent LLMs perform on the standard code summarization dataset, CodeXGLUE. In this section, we study the extent to which the generated English summaries contain tokens that are copied from the subword-tokenized code. Following this, we examine how prevalent this token overlap is across the entire dataset. We further investigate whether the models also follow the trend of generating summaries with high token overlap with the input code and whether the BLEU scores of the generations are impacted by this overlap. Finally, we inspect which types of tokens are more likely to be involved in this overlap, since the tokens present in function names give away more information about the function than the tokens in the body of the function.\nAfter determining the prevalence of high token overlap in the dataset and its impact on generation performance, we examine how much LLMs prioritize function names over the code structure and its control flow. We also evaluate how they perform when given code that has all the control structure removed, and in an extreme case if we only give the function definition as input. This will also help us answer our first two research questions about how much models leverage function names over the body of the code and how much models care about the code syntax and structure. Additionally, studying the relative performance of multiple LLMs of different parameter counts on these different types of input will give us a deeper insight into what they understand about code. While there is prior work showing how transformations like masking function names make code summarization harder Sontakke et al. (2022  ###reference_b37###), our work goes further and performs a deeper qualitative analysis to see what causes the model to perform well in the first place."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Experimental Setup",
            "text": "To examine a wide range of model sizes, we analyze CodeT5 (220M parameters), PaLM 2 (340B parameters), and Llama 2 (with 7B and 70B parameters):\nCodeT5: CodeT5 Wang et al. (2021b  ###reference_b44###) is a pre-trained encoder-decoder model that uses objectives during pre-training like Identifier Tagging (the model has to predict whether each token in the input is an identifier or not) which make it more suitable for understanding code. It is pre-trained on CodeSearchNet. It is adapted to multiple downstream tasks including code summarization and code generation through multi-task learning.\nPaLM 2: PaLM 2 Anil et al. (2023  ###reference_b3###) is an LLM made by Google AI for reasoning tasks, question answering, classification, translation, and code summarization. It was pre-trained on a large corpus of parallel multilingual text. As it is proficient in sequence-to-sequence tasks, we can make it perform code summarization with the right prompt.\nLlama 2: Llama 2 Touvron et al. (2023  ###reference_b39###) is an LLM designed by Meta AI that can also be used for sequence-to-sequence tasks, including an instruct model that can be asked to perform any task with a prompt. It comes in three sizes - 7B, 13B and 70B.\nWhile CodeT5 is fine-tuned on this dataset, we use the other models PaLM 2 and Llama 2 in inference-only mode with few-shot prompting. We use the following prompt before each input code snippet - \"Pretend that you are a programmer writing Python functions. For a given Python function you have to generate a short documentation describing what the function does.\", along with ten examples from the dataset to show what kind of description we are looking for."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Overall Performance",
            "text": "To analyze these models, we first evaluate them on code summarization on the standard CodeXGLUE dataset. Table 1  ###reference_### shows that the BLEU-4 scores of the models discussed above range from 17.66 for the 220M-parameter model CodeT5 to 22.41 for the 70B-parameter Llama 2.\nThe much smaller 7b parameter version of Llama 2 performs similarly at 22.36, while PalM 2 (340B parameters) achieves a score of 19.23, between CodeT5 and Llama 2."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Exploring if LLMs are copying tokens from code to description",
            "text": "We now analyze if these models are copying tokens from the code when generating their summaries. While copying in itself would not be a design flaw, if there is an instance of widespread copying, this would indicate that it is possible to perform well in this task without showing much understanding of the semantics of the code. We define our own metric called , which is the percentage of tokens (as generated by the corresponding model’s tokenizer) in the description that was also present in the code.\n###figure_3### ###figure_4### Figure 3  ###reference_### shows the distribution of  in the reference descriptions written by human developers (bottom) and in the summaries produced by our two models (top), CodeT5 and Llama 2. Although copying is present in the human descriptions in both models (allowing the model to pick up on it during training), the models rely on it to a greater extent, since  skews higher in the generated summaries. CodeT5 generates descriptions with much higher copying than Llama 2, but we see that even in the reference descriptions CodeT5 tokenizer gives much higher overlap than Llama 2. Therefore, it is not just the model but also the tokenizer that greatly influences the extent to which this copying strategy is employed."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Is copying tokens a viable strategy for summarization?",
            "text": "We see that the models learn to generate summaries by copying tokens from the code, but how effective is that strategy?\nFor examples with high  in the reference description, a copying strategy should yield higher BLEU scores than for examples with lower . To see if this is the case, we split the test set into buckets based on the reference descriptions’ , and plot the distribution of the model’s BLEU-4 scores in each bucket (Figure 4  ###reference_###). Figure 3(a)  ###reference_sf1### shows the results on the smallest model we have tested, CodeT5, and Figure 3(b)  ###reference_sf2### shows the results on Llama 2 (70b).\n###figure_5### ###figure_6### The first bar of the two plots in Figure 4  ###reference_### shows the overall distribution of BLEU-4 scores. The subsequent bars show the distribution of scores for a certain  range. For example, the second bar shows the scores for examples where 0 to 10% of the tokens in the reference description are present in the code. We see that in general as  increases, the BLEU-4 scores also increase. However, this correlation is less pronounced for Llama 2. This shows that both models tend to fall back on copying tokens from code to the description as a viable strategy, with the most difficult examples in the dataset being the ones where  is zero, as in this bucket, half have a zero BLEU score."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Which tokens are copied?",
            "text": "We see that copying tokens from the code when generating descriptions can be a viable strategy. However, we need to explore which types of tokens are most likely to be copied. If it is mostly the function names that are being copied, that suggests the model relies most on the function definition and largely ignores the semantics of the code. We also want to see what kind of tokens get copied from the code in the reference descriptions in the dataset.\n###figure_7### ###figure_8### In Figure 5  ###reference_### we see the distribution of different token types like function names, identifiers, and comments for both CodeT5 and Llama 2 (70b). One main reason for the differences in their distributions, even in the reference descriptions, is the fact that they use different tokenizers. For example, the function names form a small part of the code snippets after tokenization by CodeT5 as seen in the first column in Figure 4(a)  ###reference_sf1###. On the other hand, the Llama 2 tokenizer (Figure 4(b)  ###reference_sf2###) allows for function names to be a much bigger part of the code. This pattern also holds true for the reference description in the dataset, with function names being more represented by the Llama 2 tokenizer. However, when we look at the generated descriptions, we see that CodeT5-generated summaries have a much higher presence of function names than Llama 2. This is probably due to fine-tuning, and CodeT5 learning that the optimal strategy for generating good summaries is to not only copy tokens from the code but to copy tokens from the function definition of the code. On the other hand, Llama 2 seems to prefer to rely more on identifiers, which can also contain useful information about the code semantics. Llama 2 also seems to better understand that the keywords in the code, while important to understand the semantics, are not supposed to be in the generated description."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   How do Code Transformations Affect Performance?",
            "text": "We saw that the performance of LLMs on code summarization benefits from the high token overlap between the code and the reference description, and this overlap occurs primarily due to informative function names. To see how well they work when this information is unavailable, and also how strongly they rely on the internal structure of the code instead, we make several transformations to the code in the dataset. We then evaluate the performance of multiple LLMs under these scenarios. In each transformation, we modify one aspect of the dataset used for training and evaluation of the classifier to understand what effect it has on performance. For example, the change in performance when we remove function names will show us how important function names are to the model’s understanding of the code. On the transformed variants, the performance of CodeT5 improves significantly after fine-tuning on examples from those variants, especially in Obfuscated Function Names. Not all variants have the same improvement, though. In Adversarial Function Names, the improvement is much smaller, suggesting that the presence of incorrect function names still misleads the model. We see that there is a drop in Obfuscated Function Names and an even bigger drop in Adversarial Function Names, suggesting that models struggle when the function name is hidden and are misled when given an incorrect function name, showing how important function names are to the models’ understanding of the code semantics. However, the drop from Obfuscated to Adversarial is much higher for CodeT5 and PaLM 2 than Llama 2, showing that these models are more dependent on the function names. Section 3.5 will show that this may be explained by the fact that CodeT5 is much more dependent on the function names than Llama 2. PaLM 2 and Llama 2 perform worse in No Function Body and No Code Structure compared to the other variants, whereas CodeT5 performs better. This shows that CodeT5 does not mind when given code that is incomplete or syntactically incorrect, unlike the larger models. However, despite performing well on these particular variants, CodeT5 is worse overall than Llama 2, showing that fine-tuning for higher performance on one narrow metric may not always give the best result. No Function Body and No Code Structure have incomplete and syntactically incorrect code respectively. CodeT5 performs better at these variants than the ones where we just modify the function names. However, the other models perform much worse here. This shows that the larger variants care more about code syntax whereas a smaller model like CodeT5 looks mostly at function names. In fact, the largest model in our experiments, PaLM 2, performs the worst at these variants. While the bigger models like Llama 2 and PaLM 2 perform better on this variant, they are still prone to be misled. We have shown some examples where it is obvious that the function name is wrong but the models still prioritize that over the semantics in the code. Figure 7 shows an example where both PaLM 2 and Llama 2 (70b) falter when the function name is changed.\n\nAI applications in autonomous vehicles have rapidly evolved, focusing on real-time decision-making and navigation. Key strategies include sophisticated sensor fusion and enhanced object detection algorithms, leveraging neural networks for predictive analytics. Despite advances, challenges in safety and ethical considerations persist, necessitating robust validation frameworks for deployment assurance."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Code Transformations",
            "text": "Each of our four variants applies one transformation to the code to remove some information. The corresponding drop in performance indicates the importance of that information for the task. Figure 6 shows how an original snippet (5(a)), is affected by each transformation. We remove comments from the 32.5% of snippets that have them so that the models are forced to only look at the code.\n\nOriginal Function Names: This is the unmodified code from the dataset (Figure 5(a)).\n\nObfuscated Function Names: Function names often have a higher token overlap with the query than the rest of the code. We obfuscate them by replacing each character with the next character in the alphabet (‘a’ by ‘b’, ‘b’ by ‘c’ etc.). This forces the model to focus on other cues, like comments, variable names, or the actual structure of the code like for-loops and if-statements (Figure 5(b)).\n\nAdversarial Function Names: We replace the original function name with the name of another function. Unlike obfuscation, this may mislead the model. Performance on this variant will tell us how well the model works when the function name is at odds with the actual operations performed in the body of the code (Figure 5(c)).\n\nNo Code Structure: We remove keywords (if, return etc.), operators (not, >, +), and delimiters (, etc.), removing any information about the underlying logic of the program while keeping the rest of the code intact (Figure 5(d)).\n\nNo Function Body: We remove the entire body of the code and leave only the function definition. Here, we will observe how well the model performs when it only has the function name and its arguments available (Figure 5(e)).\n\nExamples in these variants will show us what part of the code LLMs tend to prioritize when given an input code snippet to analyze. After computing the performance across multiple models and datasets, we also investigate how the models’ performance is affected by the high token overlap between the code and the reference descriptions. Since this overlap can be exploited by copying tokens from the code to generate descriptions that look correct even without understanding the underlying semantics of the code, we explore how often this occurs. We also look at which parts of the code the models tend to copy tokens from most often i.e. function names versus variable names. Finally, we also looked at how our perception of the performance of these models is affected by our choice of similarity-based metrics which compare the descriptions to a human-generated docstring, since similarity alone may not be a perfect judge of whether a generated description is good or not.\n\nRecent advancements in AI have significantly impacted the development of autonomous vehicles, enhancing capabilities in navigation, perception, and decision-making. AI algorithms integrate sensor data to enable real-time responses and improve safety mechanisms. These innovations have contributed to the robust evolution of driverless technology, tailoring autonomous systems to operate reliably in complex environments."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Performance on Transformed Code",
            "text": "Table 2 ###reference_### shows the BLEU scores of CodeT5, PaLM 2, and Llama 2 (with 7b and 70b parameters) on the different variants. The first CodeT5 column shows the scores when the model is only fine-tuned on examples from the original dataset, whereas the second column (FT) shows the scores when the model is fine-tuned on the transformed examples so it knows what kind of code to expect as input. ###figure_14### On the transformed variants, the performance of CodeT5 improves significantly after fine-tuning on examples from those variants, especially in Obfuscated Function Names. Not all variants have the same improvement, though. In Adversarial Function Names, the improvement is much smaller, suggesting that the presence of incorrect function names still misleads the model. We see that there is a drop in Obfuscated Function Names and an even bigger drop in Adversarial Function Names, suggesting that models struggle when the function name is hidden and are misled when given an incorrect function name, showing how important function names are to the models’ understanding of the code semantics. However, the drop from Obfuscated to Adversarial is much higher for CodeT5 and PaLM 2 than Llama 2, showing that these models are more dependent on the function names. Section 3.5 ###reference_### ###reference_### will show that this may be explained by the fact that CodeT5 is much more dependent on the function names than Llama 2. PaLM 2 and Llama 2 perform worse in No Function Body and No Code Structure compared to the other variants, whereas CodeT5 performs better. This shows that CodeT5 does not mind when given code that is incomplete or syntactically incorrect, unlike the larger models. However, despite performing well on these particular variants, CodeT5 is worse overall than Llama 2, showing that fine-tuning for higher performance on one narrow metric may not always give the best result. No Function Body and No Code Structure have incomplete and syntactically incorrect code respectively. CodeT5 performs better at these variants than the ones where we just modify the function names. However, the other models perform much worse here. This shows that the larger variants care more about code syntax whereas a smaller model like CodeT5 looks mostly at function names. In fact, the largest model in our experiments, PaLM 2, performs the worst at these variants. While the bigger models like Llama 2 and PaLM 2 perform better on this variant, they are still prone to be misled. We have shown some examples where it is obvious that the function name is wrong but the models still prioritize that over the semantics in the code. Figure 7 ###reference_### ###reference_### shows an example where both PaLM 2 and Llama 2 (70b) falter when the function name is changed. \n\nArtificial Intelligence (AI) in autonomous vehicles has advanced significantly, leveraging techniques such as deep learning and sensor fusion to improve navigation and safety. Key achievements include enhanced object detection through neural networks and increased decision-making accuracy. Yet, challenges remain in handling complex environments and ensuring robust real-time processing capabilities."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Investigating An Alternative Metric",
            "text": "Although BLEU is a commonly used and well-established metric, it is very strict because it captures only exact matches between n-grams. To address this, a number of metrics have been recently developed that instead compute the similarity of token embeddings returned by a neural model. We investigate whether using such a metric changes the main findings from our experiments. We consider BERTScore, which uses BERT Devlin et al. (2019  ###reference_b7###) as the neural model.\nBERTScore Zhang et al. (2019b  ###reference_b50###) is a precision/recall-inspired metric that uses a BERT model to compute token similarities between two summaries. To compute BERTScore, both summaries are given as input to the BERT model (we use the base uncased model of DistilBERT Sanh et al. (2020  ###reference_b32###)) to get their token embeddings (say  for reference and  for generated).\nThen recall () and precision () over these embedding sequences are computed by considering the maximal similarity of each reference (candidate) token to any candidate (reference) token:\nThe final BERTScore is the F1 score (harmonic mean ) of  and :"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Overall Performance (BERTScore)",
            "text": "Table 3  ###reference_### shows the BERTScores for each model on the original test set. This correlates very strongly with the results in Table 2  ###reference_###, with a Pearson Correlation of 0.87 and a Spearman’s Rank Correlation of 0.86 between the 25 pairs of scores.\nThere is also a high correlation of BLEU and BERTScores between every pair of reference and generated descriptions we have in the original dataset across all models. We get a Pearson Correlation of 0.78 and a Spearman’s Rank Correlation of 0.73. The heatmap in Figure 8  ###reference_### shows this positive correlation.\n###figure_15### However, we observe that BERTScores are overall much higher and closer to each other here than was the case for BLEU scores. This is possibly due to BERTScore being a more forgiving metric, and most generations are assigned a very high BERTScore despite their quality. For example, the minimum BERTScore assigned to an example is 60, whereas there are several cases that have a BLEU score of zero.\nRegardless of the metric used, performance generally increases with increasing the number of parameters, except Llama 2 outperforming PaLM 2. However, this increase in performance is not linear, with Llama 2(7b) being quite close to Llama 2(70b). As for variants, all models see a drop in performance in Obfuscated Function Names and an even bigger drop in Adversarial, though the second drop is less in Llama 2. And while Llama 2 and PaLM 2 justifiably underperform when given incomplete or incorrect code in No Function Body and No Code Structure, CodeT5 performance takes a much smaller hit, especially after fine-tuning."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Distribution of BERTScores",
            "text": "In Figure 9  ###reference_###, we plot the distribution of BERTScores of all the model outputs on the original test data. The orange curve is the distribution of BERTScores between random pairs of reference descriptions and generated descriptions, and the blue curve shows the distribution of BERTScores between the reference descriptions and their corresponding descriptions.\n###figure_16### We see that the median BERTScore even between two unrelated descriptions is high, around 75.03. Therefore, even incorrect summaries are assigned very high BERTScores, and the threshold for good summaries is much higher than that. The median BERTScore between the reference descriptions and their corresponding generation as returned by all models is 84.33, which is more than 9 points higher, showing that this metric still shows discernment between related and unrelated descriptions. In Appendix A.1  ###reference_###, we see that BLEU scores can discern between correct and incorrect descriptions as well as assign low scores (mostly zero) to randomly sampled generated descriptions.\nAnother interesting observation about BERTScores is that the average BERTScore is higher between two random generated descriptions than two random reference descriptions in the dataset, as seen in Figure 10  ###reference_###. This tells us that the model generates descriptions with less diversity than that present in the dataset. In Appendix A.1  ###reference_###, we see this is also true for BLEU scores.\n###figure_17###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "This paper presented a series of experiments to gain a deeper insight into what makes current LLMs effective at code summarization. Section 4  ###reference_### suggests that LLMs often rely on function names and on shared tokens between the code and the description compared to the code structure to perform well. Relying on token overlap seems to work well because, at least in the standard datasets for these tasks, code and the corresponding descriptions often have high token overlap. Our experiments establish a clear trend between the token overlap of the code and descriptions and summarization performance.\nWe expect other LLM-based models, like PLBART (Ahmad et al., 2021  ###reference_b1###), and CoText (Phan et al., 2021  ###reference_b29###) to show similar behavior since we believe our results point to a feature of this entire class of models.\nThe current state of the art also falls short when the description is in a language other than English. Wang et al. (2022b  ###reference_b45###) released a benchmark MCoNaLa, which contains a parallel corpus of code paired with multiple languages, and showed that current code generation models perform poorly for languages like Spanish, Japanese, and Russian. This may be because there are fewer tokens in the description that overlap with the code, so a model cannot learn to take advantage of informative function names and identifier names.\nFinally, we also believe that human evaluation should be used in conjunction with building more comprehensive evaluation methodologies for code summarization, in order to measure the accuracy and the usefulness of a generated description, something that similarity-based metrics like BLEU and BERTScore cannot capture. While there has been work in that direction Shi et al. (2022  ###reference_b34###), the current practice still relies on comparing generated descriptions with a reference instead of measuring their actual usefulness to the user."
        }
    ]
}