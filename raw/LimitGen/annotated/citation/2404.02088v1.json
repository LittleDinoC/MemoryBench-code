{
    "title": "LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task",
    "abstract": "Conversation is the most natural form of human communication, where each utterance can range over a variety of possible emotions. While significant work has been done towards the detection of emotions in text, relatively little work has been done towards finding the cause of the said emotions, especially in multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion Cause Analysis in Conversations, which aims to extract emotions reflected in individual utterances in a conversation involving multiple modalities (textual, audio, and visual modalities) along with the corresponding utterances that were the cause for the emotion. In this paper, we propose models that tackle this task as an utterance labeling and a sequence labeling problem and perform a comparative study of these models, involving baselines using different encoders, using BiLSTM for adding contextual information of the conversation, and finally adding a CRF layer to try to model the inter-dependencies between adjacent utterances more effectively. In the official leaderboard for the task, our architecture was ranked 8th, achieving an F1-score of 0.1759 on the leaderboard. We also release our code here111github.com/akshettrj/semeval2024_task03.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Emotion Analysis is one of the fundamental and earliest sub-fields of NLP that focus on identifying and categorizing emotions that are expressed in text. Earlier, research in this domain focused on Emotion Detection in news articles and headlines (Lei et al., 2014  ###reference_b20###; Abdul-Mageed and Ungar, 2017  ###reference_b1###). However, later Emotion Recognition in Conversation gained popularity due to the widespread availability of public conversation data (Gupta et al., 2017  ###reference_b14###). Recently, the task of emotion cause analysis has gained traction, which tries to identify the causes behind certain emotions (Xia and Ding, 2019a  ###reference_b35###). This has widespread application such as building chatbots that can identify the emotions of the user and even identify the cause behind the emotions to perform certain actions (Pamungkas, 2019  ###reference_b26###). For instance, companies can identify causes behind dissatisfaction in customer interactions and take appropriate measures (Yun and Park, 2022  ###reference_b37###), AI-driven therapeutic insights can be gained using such models (D’Alfonso, 2020  ###reference_b10###), social media content moderation can be better done (Sawhney et al., 2021  ###reference_b30###), work management and team management by companies can be improved (Benke et al., 2020  ###reference_b3###).\nIn the task Wang et al., 2024  ###reference_b33###, we tackle the problem of Multimodal Emotion Cause Pair Extraction, where given a set of utterances in a conversation, we must identify the following:\n1. Emotion of every utterance (if any). These emotions can be one of Ekman’s six basic emotions (Ekman et al., 1999  ###reference_b11###).\n2. Cause of these emotions, which is considered as the utterance that explicitly expresses an event or argument that is highly linked to the corresponding emotion.\nOur proposed system tackles the task in a 3-step fashion – (a) First, we train a model to identify the emotions that are expressed in individual utterances in a conversation. (b) Next, we train a model to identify whether an utterance can be a cause of an emotion expressed in another/same utterance (candidate causes). (c) Finally, we train a model to pair emotion-utterances with their causes among the possible candidate causes. For both the (a) and (b) models we experiment with 3 basic architectures – (i) a simple Neural Network to determine the class of emotion (N-class classifier) and another Neural Network to identify whether the utterance is a candidate cause or not (binary classification). (ii) A BiLSTM (Sak et al., 2014  ###reference_b29###) architecture that accounts for the surrounding context of the conversation while doing the N-class and binary-classification. (iii) A BiLSTM CRF (Lafferty et al., 2001  ###reference_b18###) architecture which accounts for the surrounding emotions as well while doing the N-class classification. We also experiment with different encoders for the three modalities."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset",
            "text": "The dataset used for this problem is Emotion-Cause-in-Friends prepared by Wang et al., 2023  ###reference_b32### specifically for this task. It has been prepared using conversations from the popular 1994 sitcom Friends as the source. This dataset contains 1,344 conversations made up of a total of 13,509 utterances, each conversation containing an average of 10 utterances. For each utterance, the dataset has an annotated transcript (covering text modality) and the corresponding video clip (covering visual and auditory modalities) from the show.\nEach utterance is annotated with the emotion depicted by it, which is one of: anger, disgust, fear, joy, neutral, sadness and surprise. The dataset is highly skewed in terms of the frequency of different emotions in the dataset (see Figure 2  ###reference_###).\nFurther, the emotion-causes pairs for all the non-neutral utterances are provided in the dataset in a separate list.\n###figure_1### ###figure_2### The task MC-ECPE expects the model to take a list of such conversations and predict the emotion and emotion-cause pairs labels."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "A lot of work has been done in the field of emotion analysis in textual settings. Soon, work began on extracting not only the emotion but also the cause of that extracted emotion. People employed mainly two approaches for emotion cause analysis - 1. Extracting the potential causes given an emotion (Lee et al., 2010  ###reference_b19###; Chen et al., 2010  ###reference_b6###; Gui et al., 2016b  ###reference_b13###) and 2. Extracting the emotion-cause pairs jointly (Xia and Ding, 2019b  ###reference_b36###; Ding et al., 2020  ###reference_b9###; Wei et al., 2020  ###reference_b34###). Poria et al., 2020  ###reference_b28### was the first to introduce the task of extracting emotion-cause in conversations but their focus was also only on the textual dialogues. However, in our natural way of conversation, we rely on things like facial expressions, voice intonations for determining the emotion of the speaker. We also rely on auditory and visual scenes to determine the cause of the speaker’s emotions. Hence, it is clear that Emotion-Cause Pair Extraction (ECPE) is a multimodal task requiring at least three modalities: text, audio and video. Busso et al., 2008  ###reference_b4###; McKeown et al., 2012  ###reference_b25###; Li et al., 2022a  ###reference_b21### and Poria et al., 2019  ###reference_b27### worked in the field of multimodal emotion analysis in conversations but they did not consider the emotion causes. The task of MC-ECPE was first worked on by Wang et al., 2021  ###reference_b31###.\n\nArtificial intelligence plays a pivotal role in predicting agricultural yield, with models leveraging data from weather patterns, soil composition, and historical yields to enhance accuracy. Recent advancements incorporate machine learning algorithms, offering scalable solutions for farmers and agronomists to optimize resource allocation and improve crop productivity (Kamilaris and Prenafeta-Boldú, 2018; Liakos et al., 2018)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "System Overview",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Baseline I: Utterance labeling",
            "text": "Our baseline model treats the problem as a simple utterance labeling task. We use pre-trained text, audio, and image encoders to encode the individual modalities and use these to train three models that can identify the emotions in the utterances, the candidate cause utterances, and finally identify valid emotion and cause utterance(s) pairs.\nText Encoding: For encoding the transcription of each utterance, we use pre-trained BERT (Devlin et al., 2018  ###reference_b8###) embeddings as the baseline embeddings. Additionally, we finetune DeBERTa-Base (He et al., 2020  ###reference_b15###) on the training data for our experiments. DeBERTa makes use of a disentangled attention mechanism and an enhanced masked encoder to improve upon BERT’s performance in a variety of tasks. Finally, we also tried RoBERTa-Large and (Liu et al., 2019  ###reference_b24###) pre-trained EmotionRoBERTa-Base222https://huggingface.co/SamLowe/roberta-base-go_emotions  ###reference_e-go_emotions### which is publicly available RoBERTa-base model finetuned on the Go Emotions dataset (Demszky et al., 2020  ###reference_b7###). For every text encoder, we perform mean-pooling of the word embeddings to get the textual representation of the utterance.\nVideo Encodings: For encoding the videos, we sampled 16 equally spaced frames from the video and mean-pooled the embeddings for the 16 frames. For encoding these 16 images, we used MViTv2-small (Li et al., 2022b  ###reference_b22###) encoder, which achieves state-of-the-art performance on the Kinetics video detection task (Kay et al., 2017  ###reference_b17###), which makes it an obvious choice for recognizing activities happening in the conversations relevant for emotion/cause detection.\nAudio Encodings: We used WavLM (Chen et al., 2022  ###reference_b5###) for generating audio embeddings, which is trained on large audio data using masked speech representation and denoising in pre-training, making it suitable for various downstream speech tasks. We also try Wav2Vec2-Large (Baevski et al., 2020  ###reference_b2###), which is trained by masking speech input in latent space and solving a contrastive task defined over a quantization of the latent representations which are jointly learned.\nThe model architecture is a combination of three steps, each of which is described below:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baseline II: BiLSTM Architecture",
            "text": "The BiLSTM architecture is inspired by the work in Wang et al., 2021  ###reference_b31###. While the Baseline I architecture treats the emotion and cause classification independently for each utterance, it is dependent on the surrounding context of the conversation too. Thus, the BiLSTM architecture models the problem as a Sequence Labeling task. We use the best encoders in the Baseline I architecture for generating the embeddings in this architecture."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline III: BiLSTM-CRF Architecture",
            "text": "In the BiLSTM model, each classification decision was conditionally independent. Linear-chain CRFs are models generally used to model structured data where one output influences its neighboring outputs as it models the various transition probabilities, and have been extensively used with BiLSTMs for sequence labeling (Huang et al., 2015  ###reference_b16###). This could be useful for emotion predictions because the emotion of one utterance is generally influenced by the emotions in its previous utterances. For instance, an utterance with happiness generally tends to be followed by another happiness utterance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We perform a random shuffle and use a 90-10% split for the train-validation split. The test set was provided by the authors, but its gold labels have not been made public.\nThe experiments involving Baseline II and III use EmotionRoBERTa + WavLM + MViTv2 configuration. All the experiments involve applying a dropout of 0.3 on the audio, visual and textual embeddings before they are passed on to the main architectures. The BiLSTM for emotion detection consists of 4 layers while the one for candidate cause identification contains 3 layers. The dropout between the stacked layers of the BiLSTM is kept 0.3 as well. We use AdamW optimizer for all the three models, and use a linear learning rate scheduler with warmup for training the models. The Emotion Classification model is trained for 60 epochs, the Candidate Cause Identification model is trained for 40 epochs, and the Emotion-Cause Pairing Model is trained for 40 epochs as well.\nIn order to train the Emotion-Cause pairing model, we create positive and negative pairs during training. However, while the number of positive pairs is of the order N, the number of negative pairs comes to the order of N2, and thus we perform a random sampling of the negative pairs to keep the positive and negative samples in the ratio 1:5. This helps us to maintain balance between the positive and negative classes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "The performance of the three Baselines can be seen in Table 1  ###reference_###. During the Evaluation phase, our best ranked submission of Baseline II had Wt. F1 score of 0.1836 and Macro F1 score of 0.1759, ranking 8th on the leaderboard."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, we observe that the utterance labeling systems perform as good as sequence labeling systems for this specific dataset. Further, we also see that encoders which are trained on other emotion-related tasks tend to perform better on similar emotion-related tasks.\nIn future, it is possible to learn joint embeddings over the 3 modalities, which should provide better representations for each utterance (Girdhar et al., 2023  ###reference_b12###). Further, it can be experimented to utilize the speaker information for each utterance while creating utterance representations (Liang et al., 2023  ###reference_b23###)."
        }
    ]
}