{
    "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
    "abstract": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large-scale image-text datasets [31  ###reference_b31###, 32  ###reference_b32###, 38  ###reference_b38###, 30  ###reference_b30###, 3  ###reference_b3###] have been the major driving force for the recent breakthrough in Vision-Language Models (VLMs) and Text-to-Image generation models. The ever-growing size of such datasets allows researchers to scale the models to unprecedented capacities with billions or even trillions of parameters. These humongous foundation models lead to significant improvements in many down-stream tasks, such as image classification, text-to-image retrieval, image captioning, visual question answering, image generation and editing, etc. One great example is the OpenAI CLIP [29  ###reference_b29###] model, which is trained with 400M web-crawled image-text pairs. The CLIP model demonstrates impressive zero-shot learning capability across a wide range of different tasks.\nThe quality of image-text data plays a decisive role in the final performance of foundation models. But web-crawled image-text data are often very noisy, e.g., the corresponding text data is low quality or does not match the content of the image. How to build high-quality image-text datasets is a challenging research problem that attracts lots of interests recently. [48  ###reference_b48###] try to re-create the data curation process from CLIP. [25  ###reference_b25###] advocate that data quality is more important than quantity for model robustness. The DataComp challenge [11  ###reference_b11###] is introduced to systematically evaluate different data-filtering techniques.\nEach successful foundation model have their own secret recipes for data filtering. Before the invention of CLIP, most techniques are hand-designed or rule-based. For example, CC3M and CC12M design a series of heuristics for image-based, text-based and image&text-based filtering. Model-based filtering becomes popular since the introduction of CLIPScore [14  ###reference_b14###], which leverages the CLIP model to compute the cosine similarity between image and text to measure their alignment. CLIPScore has become the predominant method for filtering image-text data.\nHowever, recent research [40  ###reference_b40###, 41  ###reference_b41###] finds that visual features from CLIP are blind to subtle differences in the image, e.g., object number, shape and position. Because the contrastive loss is applied to the whole image, CLIPScore is less sensitive to capture the fine-grained object-level alignment information, shown in Figure 1  ###reference_###.\nAdditionally, the text encoder of CLIP can only process up to 77 tokens. The information loss from the text encoder can limit CLIPScore to process data with long captions. This limitation can be serious for Text-to-Image generation models [2  ###reference_b2###] that rely on long and highly-descriptive captions.\n###figure_1### Compared with the contrastively trained CLIP model, Multimodal Language Models (MLMs) have demonstrated promising capability in predicting the quality of generated images or text and aligning well with human preferences.\nMore specifically, the image-text matching scores generated by GPT-4Vision [26  ###reference_b26###] are more consistent with human experts compared with CLIPScore in recent MLM-based evaluation [49  ###reference_b49###, 52  ###reference_b52###].\nThis motivates us to integrate recent advances in MLMs for high-quality data filtering:\n“Can we adapt strong MLMs to generate scores for assessing image-text data quality and outperform CLIPScore for image-text data filtering?”\nThough GPT-4V is better at measuring image-text alignment, directly applying GPT-4V-scale MLMs in filtering billions of image-text data is computationally too costly. A good filtering method should be both effective and efficient due to the sheer amount of data we need to process.\nThere are smaller MLMs (e.g., LLaVA [19  ###reference_b19###], MiniGPT-4 [51  ###reference_b51###], etc), which are more efficient but fail to generate scores at a granularity that can reflect the subtle changes in the image-text data, since they are mainly instruction-tuned on task completion data. In this paper, we propose to combine the best of both worlds, leveraging proprietary LLMs or MLMs to construct high-quality instruction tuning data for effectiveness, and fine-tuning more accessible open-source MLMs to inject the knowledge from the high-quality data for efficiency.\nWe summarize our major contributions as follows:\nWe propose the MLM filter which incorporates the recent progress from MLMs for image-text data filtering and can be used as a drop-in replacement to the popular CLIPScore.\nWe design four diverse metrics to measure the image-text data quality from different perspectives, and a new pipeline to construct high-quality instruction data to harvest the information from proprietary models.\nFoundation models trained with our MLM filtered data demonstrate significant improvements, e.g., 1.7% better on 38 downstream tasks from DataComp comparing with CLIPScore."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Data Filters. Initial work, such as ImageNet [8], relies on manual data filtering to select high-quality images and captions. More recent work [29, 16] pushes the size of image-text datasets to the order of hundreds of millions, and thus employs fixed rules and heuristics for filtering. LAION [38] introduces the CLIPScore metric computed by the pre-trained CLIP model in filtering high-quality image-text pairs. CLIPScore filtering then becomes a widespread method of constructing large-scale web-crawled datasets [3, 30, 11]. Based on that, DataComp [11] is the first work to propose a benchmark for evaluating data filtering methods. [50] introduces a set of tools to improve data filtering, including CLIP-FLIP, distribution matching, de-duplication, and clustering. Similarly, [21] proposes text masking to improve filtering. On the other hand, [10] uses high-quality image-text pairs to train a new CLIP filtering network instead of using OpenAI’s original CLIPScore. These papers all build upon CLIP filtering and introduce various techniques to improve it. In contrast, we investigate an alternate approach to CLIP-based filtering, which employs fine-tuned Multimodal Language Models for large-scale image-text data filtering. Additionally, various works [6, 45] deploy proprietary LLMs like GPT-4 to score and filter text-only and visual instruction data.\n\nMultimodal Language Models. Recent Multimodal Language Models [1, 13, 44, 18, 51, 19] concatenate vision encoders with the latest LLMs via cross-model adapters to enable LLMs [39, 5, 42] to take visual inputs. The most typical vision encoders deployed in MLMs are still the vision transformer models in CLIP pre-trained models [29] for extracting visual features of input images. Moreover, various adapter architectures are proposed to connect the feature space of different modalities, including Q-former proposed by BLIP-2 [18], a simple MLP layer used in LLaVA [19], and Visual Experts of CogVLM [46].\n\nMultimodal Instruction Tuning. Instruction tuning [22, 43, 27] is a fine-tuning paradigm that enables LLMs to perform unseen tasks. This zero-shot performance is enabled by training LLMs using natural language instructions to explain the goal of the task. Instruction tuning is much more computationally efficient than full-set fine-tuning and can enable LLMs to achieve zero-shot performance scores that are competitive with fully supervised models. LLaVA [19] introduces multimodal instruction tuning via fine-tuning MLMs on a set of visual instructions. MLMs that use instruction tuning [9, 17] achieve SOTA performance on various vision-language tasks, such as visual question answering and visual reasoning.\n\nAI in Healthcare Diagnostics. The integration of AI in healthcare diagnostics has demonstrated significant potential in enhancing diagnostic accuracy and efficiency. Notably, deep learning models [7, 32, 40] have exhibited remarkable proficiency in interpreting medical imagery, such as radiographs and MRIs. Additionally, AI algorithms are increasingly being used for disease prediction, personalized treatment plans, and in some cases, facilitating early detection of health anomalies [24, 34, 47]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Fine-Tuned Multimodal Language Models as Data Filters",
            "text": "To investigate the effects of each design choice, we keep the selection of the other three design choices the same and only change one design choice for each experiment group. As we propose four different metrics to assess data quality, we only adopt the metric of Object Detail Fulfillment as the filtering metric to select a high-quality subset from the 128M medium scale data pool. The ablation results for all four design choices are presented in Table 1  ###reference_###.\nThe first two lines in Table 1  ###reference_### demonstrate that adopting LLaVA as the captioning model to transform images into detailed descriptions for instruction data construction leads to better filtering performance. Next, adopting CC12M to sample image-text pairs for data construction outperforms the design choice of using DataComp-Medium dataset. We suppose it is because the image quality of CC12M is significantly better than that of DataComp, enabling the instruction tuning process more knowledge intensive. Thirdly, grouping the initial instructions into 10 buckets for sampling illustrates priority over using 100 buckets. In terms of the selection of teacher models, the MLM filters learned from different teacher models exhibit distinct strengths across different tasks. The MLM filter learned from GPT-4 performs better in VTAB [53  ###reference_b53###] classification and retrieval datasets, while the MLM filter learned from GPT-4V obtains higher scores in ImageNet [8  ###reference_b8###] related datasets. Finally, we decide to fix the other three choices as LLaVA captioner, CC12M data resources, and 10 sampling buckets. We report the two versions of MLM-based filters with different teacher models GPT4 and GPT-4V for future experiments, denoted as MLM-Filter-GPT4 and MLM-Filter-GPT4V respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "We propose to adopt fine-tuned Multimodal Language Model as effective data filters to select high-quality image-text data to promote the VLM pre-training, which involves three stages: 1) constructing multimodal instruction tuning data on proposed quality scoring tasks to fine-tune MLM to realize accurate quality assessment; 2) adopt the fine-tuned MLM Filter to generate quality scores for each data point in the data pool and then select the high-quality data; 3) pre-train VLMs using the filtered dataset and evaluate the pre-trained VLMs on downstream tasks to demonstrate the effectiveness of the proposed filtering method. The detailed pipeline for the three stages is shown in Figure 2  ###reference_###.\n###figure_2###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Constructing Multimodal Instruction Tuning Data for Scoring Tasks",
            "text": "In order to work as an effective data filter, the MLM must generate quality scores for every single image-text pair for data selection and filtering. To enable MLMs like LLaVA to reason accurately on the quality score, we propose to fine-tune such MLMs on a set of scoring tasks to enhance their scoring capability. The multimodal instruction tuning data needed for scoring tasks are hard and expensive to collect via human labeling, and thus we leverage proprietary models GPT-4 or GPT-4V to construct such multimodal instruction data for scoring tasks.\nDefining Metrics for Image-Text Quality Assessment.\nConventional data filters like CLIPScore focus on the overall holistic matching of image and text via computing the cosine similarity between hidden features of image and text. However, such implicit scoring is poor in discriminating hard or ambiguous samples, leading to the false negative score predictions shown in Figure 1  ###reference_###. We propose to leverage strong Multimodal Language Models to predict the quality scores towards image-text pairs. Beyond the overall image-text alignment assessment, the fine-tuned MLM filters can evaluate the quality of image-text pairs from multiple perspectives. We propose four quality evaluation metrics to comprehensively evaluate the data quality:\nImage-Text Matching (ITM): the ITM metric focuses on evaluating whether the image caption accurately represents the main features and objects of the image and captures its primary theme. The fine-tuned MLM data filter can explicitly generate the ITM score on a scale of 100.\nObject Detail Fulfillment (ODF): the ODF metric focuses on evaluating whether the image caption provides detailed descriptions of objects that align with the image. Specifically, ODF assesses if the caption sufficiently describes the properties of the objects in the image, e.g., number, color, size, position, shape, etc. Compared with the ITM metric, the ODF metric focuses more on the fine-grained alignment between the detailed object properties in the image and the ones described in the corresponding caption.\nCaption Text Quality (CTQ): the CTQ metric focuses on evaluating the text quality of image caption based on the grammatical correctness, diversity of vocabulary (e.g., the range and uniqueness of words), fluency (e.g., smoothness and natural flow of sentences), readability, length, and structure. Previous data-centric research [50  ###reference_b50###] finds that web-crawled data is poor in its text quality, as it contains various bad text patterns, such as repeated words or textual noise. Thus, we propose to fine-tune MLMs to assess the text quality of image captions for data filtering.\nSemantic Understanding (SU): the SU metric focuses on determining if the image caption provides additional semantic information that is not readily apparent just from the image itself. Such auxiliary semantic information can be 1) the professions of persons in the image; 2) the locations, addresses, festivals, country names, city names; 3) the names or entities of buildings, people, bird species, animal breeds, car models, engines in the image; 4) the social relationships between the people in the image, i.e., lovers, parent, or child. We suggest that adopting SU metric for data filtering can select image-text pairs with auxiliary semantics, which can further enhance the commonsense reasoning capability of pre-trained VLMs.\nPrompting the Teacher Models. We select two state-of-the-art teacher models, GPT-4 and GPT-4V, to construct the multimodal instruction data for quality scoring tasks. Constructing multimodal instruction data with GPT-4V is much easier as GPT-4V can directly take visual inputs. As GPT-4 is a text-only LLM, we transform the image into a detailed text description to prompt a text-only GPT-4. The prompt for such dense captioning process is Please generate a dense caption in 4-6 sentences for describing the image in detail as much as you can. These comprehensive image descriptions are generated using a SOTA image captioning models, such as LLaVA or ShareGPT4V [4  ###reference_b4###]. With the prompt to the teacher model and the generated output, the visual instruction data can be simply formatted as User: {Prompt} Assistant: {Output}.\n###figure_3### ###figure_4### Prompting Strategies. As the scoring tasks involve a reasoning process to predict final accurate quality metrics for an image-text pair, we consider two prompting strategies to ensure the reasoning accuracy of the fine-tuned multimodal language model: Chain-of-Thought (CoT) Reasoning [47  ###reference_b47###], and Rationalization Reasoning [7  ###reference_b7###]. The major difference between the two prompting strategies are the generation order of the score and the generated reasoning steps. The exemplar prompts for two prompting strategies are presented in Appendix B  ###reference_### Table 7  ###reference_###. Between these two prompting strategies, we select the rationalization reasoning as we find it to be the most efficient and accurate. Computational efficiency is a concern as the scoring MLM should be able to score billions of image-text pairs. If the MLM is fine-tuned to output the score value first, the model’s text generation process can be stopped early in the inference stage as only the score value is needed for filtering. Secondly, the experimental results of LLaVA demonstrate that the instruction tuning with rationalization reasoning leads to better performance on the ScienceQA benchmark [34  ###reference_b34###] than CoT reasoning. Four final prompts for different scoring metrics are presented in Appendix A  ###reference_###.\nSelecting Image-Text Pairs for Data Collection.\nThe multimodal instruction data used for fine-tuning should contain image-text pairs of varying quality.\nThus, data diversity is essential to enhance the fine-tuned MLM filter, enabling it to effectively score image-text data across all quality levels. We select two different image-text dataset as the data pool for constructing instruction tuning data: the Conceptual Captions 12M (CC12m) [32  ###reference_b32###], and the DataComp Medium 128M Dataset [11  ###reference_b11###].\nTo enhance the diversity of the instruction set, we perform clustering and uniform-sampling on the sentence embeddings of each captioning text.\nThe sentence embedding model we use is the pre-trained MPNet [37  ###reference_b37###] encoder model, which is contrastively pre-trained on a mixture of retrieval and natural language inference datasets. We directly use the pre-trained MPNet provided by Sentence Transformers [28  ###reference_b28###] to generate the sentence embedding towards each image caption. We set the number of clusters as  and  for CC12M and Datacomp-Medium, respectively. The image-text pairs for constructing instruction tuning data are uniformly sampled from each cluster, in which only one data point closest to the cluster centroid is selected.\nSampling Final Instructions for Scoring Tasks.\nAs we find that the initial  instruction data generated by teacher models are not uniformly distributed on the score scale of  in Figure 3  ###reference_###, we need to sample the initial instruction data into a balanced instruction set to avoid learning bias. Considering that the ideal size of multi-task instruction tuning dataset is  instructions [5  ###reference_b5###, 42  ###reference_b42###], we decide to sample  instructions from  initial generated instruction data for each scoring tasks, which ensure the generalization capability of instruction-tuned MLM. Thus, there are  instruction data of quality scoring tasks to be included in the total  instruction dataset, such that there is 1k instruction data for each proposed quality metric. We experiment with two sampling methods to ensure that the instruction data distribution is balanced on the scoring scale of : 1) grouping all data into  buckets and uniformly sampling  instructions from each bucket; 2) grouping all data into  buckets and uniformly sampling  instructions from each bucket. The score distribution of sampled 10k instruction in Figure 3  ###reference_### are more diverse and uniform than the original score distribution in Figure 3  ###reference_###. The code for sampling the final  instruction is presented in Appendix C  ###reference_###.\nMixture with instruction data of multi-tasks.\nThe multimodal instruction tuning process should involve a diverse set of tasks [9  ###reference_b9###, 17  ###reference_b17###] to enhance the zero-shot reasoning capability of fine-tuned MLMs. In addition to 4k multimodal instruction data of the proposed data quality scoring tasks, we sample another 46k multimodal instructions from LLaVA-665k instruction datasets. We allocate a larger portion of our data mixture to reasoning tasks, such as complex reasoning [19  ###reference_b19###] and GQA [15  ###reference_b15###] as we regard that enhancing reasoning capabilities will improve the scoring capability of our fine-tuned MLM.\nThe detailed statistics on the size of each dataset sampled for data mixture are presented in Appendix D  ###reference_### Table 8  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Instruction-Tuning on Multimodal Language Models",
            "text": "We adopt LLaVA-1.5 based on Vicuna-13B LLM [5  ###reference_b5###, 17  ###reference_b17###] as the Multimodal Language Model architecture for instruction tuning on the mixed instructions of data quality scoring tasks and other multimodal tasks. The training process of LLaVA-1.5 involves pre-training on image-text pairs and instruction tuning on multimodal instructions. We directly take the pre-trained checkpoint and only reimplement the instruction tuning stage with our mixed instruction set."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Creating Optimal MLM Data Filters",
            "text": "We propose various different design choices for constructing instruction data for data quality scoring tasks in Section 3.2  ###reference_###. These design choices may make a significant difference in the effectiveness of instruction tuning. To create the optimal fine-tuned MLM data filter, we conduct comprehensive ablation studies to investigate the effects of different design choices on the filtering performance. Four major design choices for constructing the instruction data for scoring tasks are investigated: 1) we experiment with two captioning models to transform image into text-base detailed description for prompting GPT-4, including LLaVA and ShareGPT4V [4  ###reference_b4###]; 2) we experiment with two different image-text datasets for constructing visual instructions, including CC12M and DataComp Medium 128M; 3) we experiment with two different numbers of grouping buckets, 10 and 100, for sampling the final 4k instructions; 4) we experiment with different teacher models to get multimodal instructions, including GPT-4 and GPT-4 Vision. Additionally, we use the DataComp benchmark to evaluate the effectiveness of different data filtering hyperparameters.\nDataComp Benchmark. The DataComp benchmark [11  ###reference_b11###] has been introduced to systematically compare the performance of different data filtering methods. In this benchmark, the training code and computational budget is fixed across all competing methods to facilitate direct comparison between methods. The DataComp provides a fixed original image-text data pool for different filtering methods to ensure a fair comparison. The performance is measured by training a CLIP model on the filtered dataset and then testing the zero-shot capabilities of this CLIP model on a suite of 38 classification and retrieval tasks. We select the Medium scale training setting to train ViT-B/32 CLIP models on datasets resulting from various MLM data filter configurations.\nTo investigate the effects of each design choice, we keep the selection of the other three design choices the same and only change one design choice for each experiment group. As we propose four different metrics to assess data quality, we only adopt the metric of Object Detail Fulfillment as the filtering metric to select a high-quality subset from the 128M medium scale data pool. The ablation results for all four design choices are presented in Table 1  ###reference_###  ###reference_###.\nThe first two lines in Table 1  ###reference_###  ###reference_### demonstrate that adopting LLaVA as the captioning model to transform images into detailed descriptions for instruction data construction leads to better filtering performance. Next, adopting CC12M to sample image-text pairs for data construction outperforms the design choice of using DataComp-Medium dataset. We suppose it is because the image quality of CC12M is significantly better than that of DataComp, enabling the instruction tuning process more knowledge intensive. Thirdly, grouping the initial instructions into 10 buckets for sampling illustrates priority over using 100 buckets. In terms of the selection of teacher models, the MLM filters learned from different teacher models exhibit distinct strengths across different tasks. The MLM filter learned from GPT-4 performs better in VTAB [53  ###reference_b53###  ###reference_b53###] classification and retrieval datasets, while the MLM filter learned from GPT-4V obtains higher scores in ImageNet [8  ###reference_b8###  ###reference_b8###] related datasets. Finally, we decide to fix the other three choices as LLaVA captioner, CC12M data resources, and 10 sampling buckets. We report the two versions of MLM-based filters with different teacher models GPT4 and GPT-4V for future experiments, denoted as MLM-Filter-GPT4 and MLM-Filter-GPT4V respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we evaluate the effectiveness of adopting fine-tuned MLMs as high-quality image-text data filters. We compare the performance of vision-language models pre-trained on datasets filtered using a baseline filter with their performance using our MLM filter. We select two different VLM architectures for comprehensive evaluation: CLIP pre-training and BLIP-2 pre-training. Additionally, we conduct human evaluation to compute the correlation between the scoring generated by our proposed MLM filter model and the baseline CLIP model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "CLIP Pre-Training on DataComp Medium and Large Scales",
            "text": "Evaluation Setup. We select the DataComp benchmark to evaluate the effectiveness of adopting fine-tuned MLM as data filter. The evaluation process involves the data filtering stage and evaluation stage, which are shown in Figure 2  ###reference_###. During the data filtering stage, we adopt the MLM-Filter to generate quality scores on all 128M medium-scale data and 1.28B large-scale data. After that, an integer filtering threshold is calculated based on the closest value that retains 30% of the overall data pool, 38.4M for Medium and 384M for Large. Such threshold is set up to select all the image-text pairs, of which the quality score is larger or equal to the threshold.\nWe report the results using each defined metric to filter data separately and we consider two MLM filters learning from different teacher models. Additionally, we also report the results of experiments with a combination of two metrics for data filtering.\nFinally, we select a high-quality subset from the medium or large scale image-text data pools based on different proposed quality metrics. During the evaluation stage, we adopt the selected high-quality data subset to pre-train a CLIP model and compare the performance of our CLIP model with CLIP models pre-trained on datasets filtered by other methods.\nBaselines. We compare the proposed MLM filter with other baseline filtering methods from DataComp, including applying no filtering, basic filtering, LAION filtering and CLIPScore filtering. The basic filtering method adopts three rule-based filters, filtering English only, filtering by caption length, and filtering by image size. The LAION filtering adopts both the CLIPScore filtering using ViT-B/32 CLIP model and the English filtering. The CLIPScore filtering utilizes a larger ViT-L/14 CLIP model for score generation and data filtering.\nTraining Details.\nWe strictly follow the training setup provided by DataComp. The computational budget and hyperparameters are fixed for pre-training CLIP using different filters. The CLIP model architecture is determined by the data scale, in which the ViT-B/32 model is pre-trained on the medium scale setting and ViT-B/16 model is on the large scale setting. We use  Nvidia A100 GPUs to train our models.\nResults on DataComp Medium and Large Scale. The DataComp results between the proposed MLM filter and other baselines are presented in Table 2  ###reference_### and Table 3  ###reference_### for Medium and Large scale respectively. On the medium-scale DataComp benchmark, the proposed MLM Filter significantly outperforms the CLIPScore baseline on different task subgroups, achieving notable improvements of +3.2 accuracy on ImageNet-1k, +2.6 average accuracy on 6 ImageNet shifted datasets, +2.3 average accuracy on 13 VTAB datasets, and +4.9 average scores on 3 retrieval datasets. Moreover, the proposed MLM Filter surpasses CLIPScore baseline by +1.7 and +1.3 improvements on the average scores over 38 datasets on DataComp Medium and Large Scale benchmarks, which demonstrates the proposed MLM Filter can work as more effective filtering method than CLIPScore filter. Additionally, we can draw the following auxiliary conclusions from the results:\nThe MLM Filter learned from GPT-4V performs better on ImageNet related datasets than the MLM Filter learned from GPT-4. The MLM-Filter-GPT4V achieves the best performance on both ImageNet-1k and 6 ImageNet Shifted datasets. Both filtering metrics of Image Text Matching and Object Detail Fulfillment generated by MLM-Filter-GPT4V outperforms the best ImageNet-1k accuracy of MLM-Filter-GPT4, achieving a notable improvement of +1.1 accuracy.\nThe optimal filtering metric varies for fine-tuned MLM Filter learned from different teacher models. For the proposed MLM Filter learned from different teacher models, the optimal filtering metric under single metric filtering setting is different. The Image-Text Matching is the optimal filtering metric for MLM-Filter-GPT4V, while the Object Detail Fulfillment metric helps the MLM-Filter-GPT4 most. The other two metrics of Caption Text Quality and Semantic Understanding cannot work as effective filtering quality metrics in DataComp benchmark, leading to worse performance than CLIPScore baseline. We regard that it is because the most of DataComp evaluation datasets are image classification datasets, which did not aligh with the filtering directions and objectives of CTQ and SU metrics.\nImage-Text Matching is the best filtering metric for retrieval tasks. Our proposed MLM Filter achieves the SOTA performance on the three image-to-text and text-to-image datasets under DataComp Medium setting. The two types of MLM Filters achieves 30.0 and 29.7 average performance on three retrieval tasks using the ITM filtering metric, surpassing the CLIPScore baseline by 4.9 average scores. We also observe in results of both MLM Filter variants that the image-text matching metric leads to better performance on retrieval tasks compared with other three filtering metrics.\nCombing different quality metrics effectively filters and identifies image-text pairs of better quality.  The AND operation to combine ITM and ODF quality metrics means that the ITM and ODF score of selected datapoints should exceed the filtering thresholds of both metrics, while the OR operation to combine two metrics means that the selected datapoints should either exceed the threshold for ITM metric or that for ODF metric. The combination of ITM and ODF metrics using AND operation outperforms all the baseline filtering methods and other variants of MLM Filters, achieving the best average performance of 34.5 over 38 datasets.\nITM\n8.2\n10.3\n9.2\nODF\n14.6\n19.3\n16.9\nITM\n15.4\n8.3\n11.8\nODF\n9.0\n6.8\n7.9\nAND\n12.9\n11.6\n12.3\nThe worse performance on digit classification tasks prevents MLM-Filter-GPT4V from remarkably outperforming MLM-Filter-GPT4. Even if MLM-Filter-GPT4V outperforms MLM-Filter-GPT4 on 23 ImageNet, VTAB and retrieval datasets, it only achieves the same average performance over 38 datasets as MLM-Filter-GPT4. It is because the performance of MLM-Filter-GPT4V on the two digit classification datasets significantly lags behind MLM-Filter-GPT4 by 5.1 average score, shown in Table 4  ###reference_###, which leads to 0.27 average score behind on 38 datasets. The combination of two quality metrics promotes the digit classification performance of MLM-Filter-GPT4V, but does not resolve it."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "BLIP2 Pre-Training",
            "text": "To demonstrate the effectiveness of our proposed MLM Filter across various VLM model architectures, we pre-train BLIP-2 VLM on the filtered dataset and evaluate the zero-shot performance of such BLIP-2 model on VQA datasets to compare the effectiveness of filtering methods on high-level vision-language tasks.\nTraining setup.  We directly use the filtered dataset from DataComp Large 1.28B data pool using CLIPScore filtering and our proposed MLM Filtering. The batch size and number of pre-training steps are kept as the same as original implementation [18  ###reference_b18###] for both the CLIPScore filtered dataset and MLM filtered dataset, in which both BLIP-2 models are iterated on 420M images for pre-training stage 1 and 154M images for stage 2. We use the same hyperparameters and number of GPUs for training. The visual encoder and LLM we used for BLIP-2 architecture are Eva-CLIP ViT-g/14 [33  ###reference_b33###] and Vicuna-7b [5  ###reference_b5###] respectively. More training details are available in Appendix E  ###reference_### Table 9  ###reference_###.\nResults. Two BLIP-2 models pre-trained on different filtered datasets are evaluated on VQAv2 [12  ###reference_b12###] and GQA [15  ###reference_b15###] datasets in zero-shot manner and the results of zero-shot VQA performance are shown in Table 5  ###reference_###. The BLIP-2 pre-trained with MLM-Filter-GPT4 filtered image-text data achieves +1.7 and + 1.4 improvements on VQAv2 and GQA datasets than the BLIP-2 pre-trained on CLIPSCore filtered dataset.\n55.1\n56.8"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Correlation with Human Scoring",
            "text": "We follow [52  ###reference_b52###] to compute the correlation between human scoring and model scoring to evaluate the alignment between human and the filtering model. A set of 100 image-text pairs are sampled from CC12M and MSCOCO [20  ###reference_b20###] and labeled with human scores in terms of the image-text matching. CLIPScore and fine-tuned MLM filters are used to generate the image-text matching scores for the selected image-text pairs. Then, the Pearson and Spearman scores are reported between the human scores and model scores, as presented in Table 6  ###reference_###. Our proposed MLM-Filter scores are significantly aligned and correlated with human quality scores, while CLIPScore does not demonstrate such correlations. The two quality metrics Image-Text Matching and Object Detail Fulfillment all demonstrate significant correlations in similar levels.\n0.164\n0.410\n0.328\n0.368"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "Effects of filtering fraction. We perform an ablation study to investigate the effects of the fraction of samples selected for pre-training CLIP on DataComp Medium benchmark performance. We select five fractions  of the total 128M images of DataComp medium pool. The results are presented in Table 4  ###reference_###. The top-30% of images selected for CLIP training achieve the best performance, which is also observed in [11  ###reference_b11###]. Even adding 5% poison data leads to a huge performance drop on both ImageNet and average over 38 datasets.\n###figure_5### Efficiency of MLM Filters.\nThe MLM Filter used for quality score generation is LLaVA-1.5 with 14B model parameters , while CLIPScore adopts a CLIP ViT-L/14 model with 492M parameter in total. Even if the model size of the proposed MLM Filter is much larger than that of CLIPScore, due to the computation redundancy of the CLIP’s dual-encoder architecture, the timecost for generating scores for 10k image-text pairs is average 24.3 mins for MLM Filter versus 11.2 mins for CLIPScore-ViT/L using one A100 GPU. Additionally, with the help of the latest techniques in language model inference acceleration, the TensorRT-LLM toolkit111https://github.com/NVIDIA/TensorRT-LLM, we accelerate the score generation of our MLM Filter 4 times over, resulting in 6.1 mins in average for 10k samples. Thus, the proposed MLM Filter can achieve much better efficiency than CLIPScore."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose to instruction-tune Multimodal Language Model on quality scoring tasks and further leverage these fine-tuned MLM as effective data filters to select high-quality image-text pairs from large-scale web-crawled dataset. We find that, on CLIP and BLIP-2 models, pre-training on datasets filtered by our proposed MLM Filter significantly outperforms pre-training on CLIPScore-filtered datasets, demonstrating the superiority of our proposed MLM Filter over CLIPScore filtering."
        }
    ]
}