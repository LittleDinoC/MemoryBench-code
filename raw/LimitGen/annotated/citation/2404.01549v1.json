{
    "title": "Octopus: On-device language model for function calling of software APIs",
    "abstract": "In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models’ grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose conditional masking techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of Large Language Models (LLMs) has revolutionized the field of artificial intelligence, bringing forth a wide array of capabilities in natural language processing, alongside applications in specialized domains such as mathematics (Imani et al. (2023  ###reference_b16###); He-Yueya et al. (2023  ###reference_b14###))), healthcare(Imani et al. (2023  ###reference_b16###); Jo et al. (2023  ###reference_b19###); Thirunavukarasu et al. (2023  ###reference_b37###)), and legal analysis (Cui et al. (2023  ###reference_b7###); Fei et al. (2023  ###reference_b9###); Luppi et al. (2022  ###reference_b25###)). Despite these advancements, LLMs face challenges in assimilating real-time updates and executing specific tasks like image/video editing (Fu et al. (2023  ###reference_b10###)) or intricate tax filings. The integration of Large Language Models (LLMs) with external APIs emerges as a pivotal improvement. This synthesis, leveraging APIs, not only augments the LLMs’ capabilities by facilitating access to up-to-date information and specialized functionalities but also sparks the creation of novel applications such as code interpreters (Bairi et al. (2023  ###reference_b4###); Vaithilingam et al. (2022  ###reference_b39###); Chen et al. (2021  ###reference_b6###)). Research like ToolAlpaca (Tang et al. (2023  ###reference_b35###)) and NexusRaven (Srinivasan et al. (2023  ###reference_b34###)) also proves the function calling capability of open source language model. Consequently, this integration signifies a crucial step toward overcoming the inherent limitations of LLMs, thereby extending their utility and potential for innovation in the field.\nEnhancing the integration of Large Language Models (LLMs) with external APIs necessitates addressing the challenge of balancing large-scale model dependency against efficiency and cost. Focusing on specific tasks that utilize only a fraction of available APIs indicates the inefficiency of merely replying on large models like GPT-4 (Radford et al. (2018  ###reference_b28###; 2019  ###reference_b29###); Brown et al. (2020  ###reference_b5###); Achiam et al. (2023  ###reference_b2###); Wu et al. (2023  ###reference_b44###)), which need a lot of computational resources. This scenario advocates for the development of smaller, task-oriented LLMs that preserve essential functionality while minimizing operational costs (Shen et al. (2024b  ###reference_b32###); Pallagani et al. (2024  ###reference_b26###)). However, this shift towards smaller models introduces new challenges, including an increased risk of errors or ”hallucinations” (Yao et al. (2023  ###reference_b46###); Zhang et al. (2023  ###reference_b47###); Ji et al. (2023  ###reference_b17###)) which causes issues in precise output formatting (Jiang et al. (2023  ###reference_b18###)). And the correct output formatting is critical for robust software application.\nIn response to the limitations of oversized Large Language Models (LLMs), which entail unnecessary inference costs and exhibit a lack of focus in training data, we propose a new framework to do LLM training and inference. Grounded in an expansive dataset of over 30,000 widely-utilized APIs from Rapid API Hub (rap (2024  ###reference_b1###)), this framework spans a diverse array of functionalities from Google searches to Amazon product lookups. By leveraging curriculum learning (Liu et al. (2024  ###reference_b24###)) strategies, we significantly refine the LLMs’ proficiency in selecting the appropriate API functions from a pool of similar options. This strategic dataset engineering, combined with our choice of base models, including Codellama7b (Roziere et al. (2023  ###reference_b30###); Touvron et al. (2023  ###reference_b38###)), Google’s Gemma 7B & 2B (Gemma Team, Google DeepMind (2023  ###reference_b11###)), Stable Code 3B(Pinnaparaju et al. (2023  ###reference_b27###)), underscores the effectiveness of our approach, outperforming GPT-4’s benchmarks. Moreover, this ensures the practicality of our solution across various platforms, including mobile devices since these models can already been deployed on mobile (team (2023  ###reference_b36###)).\nTo ensure the consistency of our model’s output formatting, we introduce a conditional masking technique during inference. This innovative approach guarantees that our LLMs generate outputs in the desired formats, markedly improving accuracy and minimize validation loss without sacrificing inference speed. We also prove that the technique of conditional mask will only increase the accuracy mathematically.\nThis advancement, validated across our selected base models, not only showcases the potential of compact LLMs in external API integration but also sets a new efficiency benchmark for scalable AI applications. Through detailed exposition of our model selection and training process, we present a holistic solution that effectively addresses the prevailing challenges in LLM API utility. The dataset used for the LLM training and fine-tuned models will be open sourced soon."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Enhancing LLMs with Tools The integration of external computational tools within Large Language Models (LLMs) like GPT-4, Alpaca, and Llama signifies a substantial advancement in augmenting their capabilities. Initially, integration efforts were centered around model-specific fine-tuning methods (Lin et al. (2024  ###reference_b23###); Hu et al. (2023  ###reference_b15###)), which, despite their effectiveness, encountered challenges in widespread and flexible application. A notable shift occurred with the adoption of prompts containing exemplary demonstrations, broadening the scope of tool accessibility. This range includes specialized code interpreters and extensive retrieval frameworks, significantly enhancing the models’ ability to interpret and execute complex instructions(Zhou et al. (2023  ###reference_b51###)). Developments in simulated environments for tool interaction (Shen et al. (2024a  ###reference_b31###); Du et al. (2024  ###reference_b8###); Xi et al. (2023  ###reference_b45###)) and frameworks for API engagement (Li et al. (2023  ###reference_b22###)) have been observed as well. Furthermore, the incorporation of advanced reasoning (Valmeekam et al. (2022  ###reference_b40###); Hao et al. (2023  ###reference_b12###); Lewkowycz et al. (2022  ###reference_b21###)) strategies has significantly improved the models’ efficiency in interpreting and solving complex tasks.\n\nDataset format The optimization of datasets (Zhuang et al. (2024  ###reference_b52###); Kong et al. (2023  ###reference_b20###)) for model fine-tuning is critical for enhancing LLM performance. This process involves a multi-stage refinement utilizing models such as GPT-4 and Alpaca. By iteratively enhancing the dataset, this methodology not only refines prompts but also improves response quality and develops advanced chain-of-thought (Wang et al. (2023  ###reference_b42###); Zhang et al. (2022  ###reference_b48###); Shridhar et al. (2023  ###reference_b33###); Zheng et al. (2023a  ###reference_b49###); Wei et al. (2022  ###reference_b43###)) processes. Such advancements lead to a significant increase in the accuracy of function calling within LLMs, setting new benchmarks in dataset optimization and model training. This iterative refinement represents a strategic shift towards enhancing LLM output precision and quality.\n\nRobustness in LLM Generation Contrary to article generation, which may accommodate flexible output formats, software applications necessitate strict adherence to specific output structures, such as JSON formatting in Zheng et al. (2023b  ###reference_b50###). And many format consistency problem have been observed in the LLM generation (Vaswani et al. (2017  ###reference_b41###); Ackerman & Cybenko (2023  ###reference_b3###)). Some research has been done to enforce these rigid output formats to maintain consistency and reliability in LLM-generated content. For example, in langchain framework Harrison (2022  ###reference_b13###), there are many output parsers to enforce the formats like YAML, JSON, CSV and so on. However, there are still many cases that can’t be resolved by output parser, especially for the function call response.\n\nAI for Autonomous Vehicle Navigation Recent advancements in AI have significantly enhanced autonomous vehicle navigation, deploying sophisticated machine learning algorithms to process real-time data for dynamic decision-making. Methods such as deep reinforcement learning (Aradi et al. (2021  ###reference_b1###); Kiran et al. (2022  ###reference_b14###)) and sensor fusion techniques (Chen et al. (2022  ###reference_b4###)) are pivotal in improving path planning and environmental perception."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we detail our approach to dataset collection and preparation, introducing the workflow we designed to format the dataset for effective training. We then describe the development of our model, Octopus, highlighting the training techniques and inference strategies we employed. One of the key innovation in our model is the use of a conditional mask for inference enhancement, which represents a novel approach to improving model performance. This methodology combines comprehensive data preparation with advanced modeling techniques to address the challenges of training and inference for function call model development."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset collection and refinement",
            "text": "Our initial dataset comprises API documentation sourced from RapidAPI Hub, one of the world’s largest API repositories. This selection was made based on the website’s claim of millions developers’ engagement. To facilitate the large language model’s comprehension of API usage patterns, we compiled a comprehensive collection of API documentation, focusing on approximately 30,000 of the most frequently utilized APIs. This dataset acquisition was structured in two primary stages: the initial collection and processing of individual API documentation pieces, followed by a meticulous refinement process to optimize the dataset for training purposes.\nSingle API data preprocess From the detailed exploration of the document, we gather a comprehensive understanding of how RapidHub’s API usage examples are structured and utilized. The approach involves meticulously extracting API usage examples, which detail the API’s name, description, argument names, and their respective descriptions, and formatting this information in JSON. This data is then reorganized using OPENAI GPT-3.5 and CodeLlama 70B models to align with desired organizational standards. Then, we refine the function names based on their descriptions to ensure they are concise and informative. Subsequently, arguments’ names and descriptions are captured. To counteract potential inaccuracies (”hallucinations”) inherent in smaller LLM model, the Python coding format is employed. This decision is strategic, leveraging the models’ inherent code reasoning capabilities from their training on extensive code datasets like in CodeLlama7b and StableCode3B model. This process not only streamlines the API information for enhanced usability, but also leverages advanced AI models to ensure the information is presented in a structured, accessible manner. By prioritizing the function description as a guide for renaming and carefully detailing argument names and descriptions, the approach ensures that the essential elements of API usage are conveyed effectively, supporting developers in integrating these APIs into their projects seamlessly. One example of the converted function can be found below.\nIn our methodology, we deliberately excluded function body for the final dataset compilation. Through a meticulous selection process, we aggregated approximately 20,000 APIs, employing OPENAI GPT-4 for a comprehensive examination and removal of APIs with deficiencies, such as missing arguments or inconsistencies between function descriptions and their parameters. This stringent selection criterion was pivotal in assuring the dataset’s quality. Each API underwent this rigorous scrutiny, culminating in the compilation of dataset A. The dataset A will be the basis for the subsequent data processing.\nDataset refinement To enhance decision-making in Large Language Models (LLMs) for real-world API usage, we present a sophisticated dataset construction approach, crucial to our study. We begin by integrating various functions, intentionally incorporating some irrelevant functions to create a complex environment for the LLM. Inspired by curriculum learning, we design our dataset to include hard negative samples gradually. This involves introducing similar functions to incrementally raise the challenge of selecting the most relevant function. Our approach is depicted in Figure (1  ###reference_###), illustrating the detailed process of compiling the dataset. Below, we describe the techniques employed.\nNegative samples To enhance the model’s reasoning capabilities and practical applicability, our methodology involves sampling both positive and negative examples. The ratio of these datasets is represented by the variable  in Figure (1  ###reference_###), serving as an important parameter in our experimental setup. Specifically, in our framework, we select  and  to be equal, setting both values at 1.\nSimilar functions cluster In our practical implementation, the model selects functions from a diverse pool in response to user queries. To intensify the training challenge, we deliberately complicate the selection process. Specifically, we construct training data by associating a given data point with three semantically similar ones. This process involves calculating vector embeddings from function descriptions, with Milvus facilitating the search. The sampling of three similar functions is determined by their similarity scores, focusing on ranks 5 to 10, to deliberately exclude overly similar functions and avoid redundancy in individual queries. This approach guarantees a challenging training setting, cultivating a model capable of differentiating between closely related functions in practical use cases.\nGPT 4 generated query The creation of a high-quality dataset is crucially dependent on the formulation of qualified queries. In this context, we opt to generate positive queries solvable by a single API. Moreover, for such positive instances, we also generate and incorporate a Chain of Thought (CoT), which is utilized during model training. Recent studies have demonstrated that the addition of CoT not only enhances model performance but also significantly improves its reasoning abilities (Srinivasan et al. (2023  ###reference_b34###)). Note worthily, the creation of qualified queries and auxiliary information is crucial in developing effective training datasets.\nGPT 4 verification Our dataset’s development included an observation regarding the potential error of GPT-4 generated responses, despite its advanced capabilities. Thus, we designed a workflow to let GPT-4 conduct the self-verification, effectively identifying and rectifying inaccuracies in its outputs. After getting dataset B, we employed GPT-4 to meticulously verify and eliminate any data points that failed to meet our stringent quality criteria. This rigorous validation process led to the exclusion of approximately 1000 data points, significantly contributing to our model’s enhanced performance.\n###figure_1### Adhering to the outlined methodology, we are poised to meticulously compile the training dataset, achieving an impressive collection of approximately 150,000 data points. Each individual API is associated with 5 positive queries, which it can resolve. To provide a comprehensive understanding, a sample of the complete dataset has been included in the Appendix (B.1  ###reference_###), showcasing the detailed structure and composition of our training data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Octopus",
            "text": "To validate the efficacy of our framework, we fine-tuned it on four renowned open-source models: CodeLlama7b, Google Gemma 2B & 7B, and Stable Code LM 3B. A standardized training template was employed across all models, detailed in the Appendix (B.1  ###reference_###). We utilized LoRA and 8-bit quantization techniques, allocating A100 80GB GPU hours as follows: 90h for CodeLlama7b and Google Gemma 7B, 30h for Google Gemma 2B, and 60h for Stable Code LM 3B. The learning rate was set at 510-5, with a linear scheduler optimizing outcomes. In the inference stage, user queries trigger function retrieval and execution, mapping the generated function and its arguments to corresponding APIs for final responses, thus ensuring accurate results upon correct function and argument name generation.\nWe have experimented different lora setup, and we found that the best setup is to choose lora rank as 16, and apply the method to the modulus of \"q_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\". We also attach the training and validation loss for selected models in Figure (2  ###reference_###). During training, we progressively trained on dataset point with more similar examples to do the curriculum learning.\n###figure_2###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Inference using conditional mask",
            "text": "The utilization of smaller-parameter Large Language Models (LLMs) has a pivotal challenge: a noticeable decrement in robustness when generating outputs. This challenge is also observed in our model, which necessitates the need to enforce the response with precise function names along with their corresponding arguments. The expected output format demands that arguments be encapsulated within parentheses, function names align with a pre-defined repository, and argument values conform to their designated types. Discrepancies, such as typographical errors in function names or misalignment in argument types, critically undermine the integrity of the output, rendering it susceptible to errors. For instance, both in GPT-4 and our model, deviations in the function name—whether through misspelling or elongated expressions—can lead to unintended corrections that fail to map back to the original function names, thereby distorting the intended output. The original LLM generation process to sample the next token is\nwhere  is all the current tokens, with the sequence length as , and  is the next token to be sampled. What we do here is to introduce another mask dependent on  so that\nIn constructing the mask, we designate all tokens, which are not aligned with correct format, to be masked by assigning a value of 0 to their respective positions, and a value of 1 to all other positions. For example, if we already know the next token represents integers, we will only unmask the tokens that are used for integers. Therefore, the formulation of an accurate mask is paramount for achieving the desired outcome. In this context, we delineate several methodologies that were investigated for the derivation of the mask.\nenum data type Function names are usually already known, and will not change during inference. We can treat them as enumerable data variables. To efficiently manage these names, a Trie tree can be constructed, facilitating the retrieval of the mask with a time complexity of , where  denotes the Trie tree’s depth, equivalent to the maximum length of a function name, which in our case is approximately 20. This result in the constant time complexity. As an alternative approach, storing all prefixes of potential function names within a dictionary could further reduce the complexity to . The implementation of the Trie class is provided in the Appendix (B.2  ###reference_###).\nstring, float, dict, int type Regular expressions can be employed to analyze subsequent tokens and generate the conditional mask.\nTherefore, we can confirm that the output result is free from formatting errors.\nOur experimental findings indicate that the application of the conditional mask significantly enhances the robustness of the Large Language Model (LLM) in the context of function calls."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Evaluation for Function Calling",
            "text": "We conducted a comprehensive series of tests on our dataset, evaluating the Octopus model against other leading models. This evaluation focused on Octopus’s capability to understand API calls, specifically those on RapidAPI. Additionally, we explored the impact of incorporating various retrieval techniques during the training phase on the model’s ultimate effectiveness.\nIn terms of baselines, our primary comparison was with cutting-edge language models in a zero-shot framework. The models we analyzed include GPT-4 by OpenAI, utilizing the gpt-4-0314 checkpoint, and GPT-3.5-turbo, employing the gpt-3.5-turbo-0301 checkpoint. Both models have been refined through RLHF (Reinforcement Learning from Human Feedback) for enhanced conversational abilities."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation dataset and benchmark",
            "text": "To benchmark function calls within commonly used software APIs, we specifically constructed a dataset. This dataset was generated by randomly selecting four different function APIs and sampling queries that could be addressed by these APIs. The sampling utilized the same prompt template employed during training, details of which are provided in Appendix B.1  ###reference_###. Additionally, we included queries that these APIs could not resolve, maintaining a balanced ratio of solvable to unsolvable queries at 1:1. Ground truth for the dataset was established through human annotation. We applied rigorous standards for benchmarking, focusing on real-world application requirements, including the precise matching of function names and arguments. For models not trained on this dataset, issues related to format correctness were overlooked to provide a more fair comparison. Consequently, in our analysis, GPT 3.5 was not marked incorrect for format discrepancies.\n###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance without the conditional mask",
            "text": "In the task of inferring function calls, we initially employed both GPT-3.5 and GPT-4 models to generate responses. For these pretrained models, the greedy search technique was utilized to select responses. This decision was based on the higher precision required for accurately identifying both function names and their corresponding parameters, where the model’s ability to choose the correct function name and parameters is crucial. Therefore, alternative methods such as sampling and beam search were not adopted for this task. The resulting accuracy metrics from this approach are presented in Figure (3  ###reference_###).\nThe results highlight that GPT-4 consistently achieves the highest accuracy in producing correct outcomes. A notable issue leading to inaccuracies with GPT-4 involves ”hallucinations,” such as its ability to autocorrect misspelled function names, exemplified by transforming send_emil into send_email. It is critical that the function name provided in the initial prompt remains unaltered, regardless of spelling errors. This correction issue extends to parameters as well; for instance, GPT-4 might generate Australian as a parameter when the query explicitly requires a country name. The primary source of incorrect outputs is attributed to the generation of inaccurate parameters. However, all pretrained models demonstrate near-perfect performance in identifying the correct function name.\n###figure_4###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Performance with the conditional mask",
            "text": "In contrast to the inference approach described in the preceding subsection, we implemented a conditional mask during inference for this scenario. This modification has been effective in enhancing outcomes, particularly in the generation of parameters. Utilizing a conditional mask, especially when an input is of an enum type like a country name, helps prevent the model from generating unexpected parameters. The improvements facilitated by this approach are illustrated in Figure (4  ###reference_###). However, since the APIs for GPT-3.5 and GPT-4 do not provide logits, the conditional mask technique could not be applied to these models, and thus, no improvement in their metrics was observed. Nevertheless, it’s noteworthy that two 7B models were able to achieve performance over GPT-4, highlighting the efficacy of the conditional masking technique in certain contexts."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we present a novel framework designed to train large language models on practical software APIs, with a subsequent evaluation of their performance in making API calls, specifically in comparison to the GPT-4 model. Our approach includes a methodology for refining the dataset and the associated prompt template, which incorporates negative sampling and curriculum learning strategies. Additionally, we introduce an innovative technique known as the conditional mask, aimed at addressing the challenge of mismatched output formats."
        }
    ]
}