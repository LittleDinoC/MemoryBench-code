{
    "title": "Assisting humans in complex comparisons: automated information comparison at scale",
    "abstract": "Generative Large Language Models enable efficient analytics across knowledge domains, rivalling human experts in information comparisons. However, the applications of LLMs for information comparisons face scalability challenges due to the difficulties in maintaining information across large contexts and overcoming model token limitations. To address these challenges, we developed the novel Abstractive Summarization & Criteria-driven Comparison Endpoint (ASCEnd) system to automate information comparison at scale. Our system employs Semantic Text Similarity comparisons for generating evidence-supported analyses. We utilize proven data-handling strategies such as abstractive summarization and retrieval augmented generation to overcome token limitations and retain relevant information during model inference. Prompts were designed using zero-shot strategies to contextualize information for improved model reasoning. We evaluated abstractive summarization using ROUGE scoring and assessed the generated comparison quality using survey responses. Models evaluated on the ASCEnd system show desirable results providing insights on the expected performance of the system. ASCEnd is a novel system and tool that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The applications of generative Large Language Models (LLMs) across knowledge domains, including financial services, are rapidly expanding due to their proven capabilities in performing multiple types of tasks such as abstractive summarization (Liu and Healey, 2023  ###reference_b25###), simple QA (Question-Answering) (Trivedi et al., 2022  ###reference_b40###), multiple choice QA (Trivedi et al., 2022  ###reference_b40###), financial sentiment analysis (Fei et al., 2023  ###reference_b6###), Retrieval Augmented Generation (RAG) (Pan et al., 2022  ###reference_b33###), among others (Lu et al., 2021  ###reference_b26###). These open-domain tasks have recently been popularized in specific applications as they enable time efficiencies in the analytics required for informed decision-making.\nIn this paper, generative LLMs are defined as generative models that are solely autoregressive. Recently, generative LLMs have proven to be effective in executing abstractive summarization (Rath et al., 2023  ###reference_b36###), outperforming existing state-of-the-art (SOTA) models (Liu and Healey, 2023  ###reference_b25###). Fine-tuning generative LLMs is an effective method of increasing the model’s overall performance in a specific domain (Brown et al., 2020  ###reference_b3###). However, prompting strategies are effective in leveraging the performance capabilities of naive generative LLMs without relying on large training data sets, with some minor trade-offs in performance (Navarro et al., 2022  ###reference_b29###)(Howell et al., 2023  ###reference_b11###). Zero-shot (Kojima et al., 2022  ###reference_b19###) and few-shot (Wei et al., 2022  ###reference_b42###) prompting techniques can be leveraged to direct answer structures and elicit human-like reasoning for response generation. A recent study demonstrated the ability of naive generative LLMs to perform Named Entity Recognition (NER) and Relation Extraction (RE) (Li et al., 2023  ###reference_b21###) for tasks that reflect human-like reasoning. Chain-of-thought reasoning can also be elicited in zero-shot prompting strategies using phrases such as “let’s think step-by-step” (Kojima et al., 2022  ###reference_b19###) to reflect human-like reasoning. Additionally, altering the available input context and the order in which information is presented in a prompt (Lu et al., 2021  ###reference_b26###) can affect the final output response structure.\nSemantic textual similarity (STS) is the main driving factor in effective information comparison (Majumder et al., 2016  ###reference_b27###). STS is measured through many different techniques that can help in text classification and topic extraction (Slimani, 2013  ###reference_b38###) without the limitations of lexical similarity. Semantic similarity is achievable with generative LLMs as demonstrated by Gatto et al. (2023  ###reference_b8###). They achieved success in assessing semantic similarity by testing various prompting strategies on generative LLMs. Moreover, using retrieval augmented generation (RAG) provides additional context based on a user query such that an LLM can generate an in-domain output response (Ram et al., 2023  ###reference_b35###) (Trivedi et al., 2022  ###reference_b40###). RAG applies STS to find the top-k passages most similar to the user query and enhances the information in its output response.\n###figure_1### Recent research has demonstrated that decision-making and data analysis can substantially benefit from text analyses made available by STS through generative LLMs. For example, research in the medical domain has developed a framework to screen abstracts of scientific papers to be used for review papers (Guo et al., 2023  ###reference_b9###). Guo et al. (2023  ###reference_b9###)’s work used generative LLMs to compare scientific abstracts with user-defined criteria and sorted the abstracts based on the eligibility generated by the model. Their work provided insights into the use of generative LLMs to perform information comparisons against user-defined criteria such that the model could make binary categorical decisions. Similar research was also conducted in the financial domain, where a corporate sustainability report was compared to a sustainability guideline document in their framework, chatReport (Ni et al., 2023  ###reference_b30###). chatReport supported different QA tasks regarding the information in the sustainability report, powered by RAG. Their framework handles individual reports and generates responses for single-use applications.\nHowever, applying LLMs for information comparison is currently not possible at scale due to token limitations imposed on LLMs. Minimizing information loss and prompting under token limits are ongoing issues that must be addressed for model success. Additionally, models with longer token limits are prone to losing information from the input context, as it was demonstrated that generative LLMs struggled to retrieve relevant information from the middle of long input contexts (Liu et al., 2023b  ###reference_b24###). in fact, Liu et al. (2023b  ###reference_b24###) found that naive, untrained models with no input context had performed better on the same QA task when compared to the models provided with a lengthy input context.\nTo address these challenges, we developed ASCEnd (Abstractive Summary & Criteria-driven Comparison Endpoint), a novel system enabling accurate, automated information comparison at scale for applications across financial services and other knowledge domains. ASCEnd presents a novel method of applying comparisons for generating evidence-supported analyses. We applied our system to the challenge of identifying and evaluating financial transactions that meet complex, user-defined sustainable finance criteria in the financial domain. This paper discusses the design choices of the system, its significance in the financial analysis domain, as well as the limitations & future work of the ASCEnd system. Our system enables efficient information comparison across large, complex data sets and facilitates more informed decision-making."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The ASCEnd System",
            "text": "The ASCEnd system provides insights by comparing a given text corpus against a set of user-defined criteria. This comparison evaluates the relevance of each document in the corpus to a user-defined topic. The ASCEnd system is built using abstractive summarization, RAG, binary QA tasks, and reasoning tasks; tasks which have been recently studied to be effective when deployed with generative LLMs (Touvron et al., 2023  ###reference_b39###). Zero-shot prompting is implemented to enable more flexibility across applications as it removes the need for model finetuning.\nThe ASCEnd system comprises four modules (Figure 1  ###reference_###) that process the user-specified information and generate the comparison assessment. The first two modules, Document Summarization (DS) and Criteria Embedding (CE), perform the input processing of the given text corpus and user-defined criteria respectively. The third module, Retrieval Augmented Generation (RAG), facilitates the similarity search to retrieve and output the relevant information for the next module. The last module, Comparison Assessment (CA), performs the comparison tasks with the preprocessed data as input. The DS module iteratively performs abstractive summarization on individual documents from the given text corpus to generate a summary for each document. The CE module vectorizes and splits the user-defined criteria document, storing the segments in a vector database. The RAG module uses the vector database to drive a similarity search between the summary from the DS module, combined with the RAG prompt and the user-defined criteria to return the top-k passages. These passages are fed in with the RAG prompt to a human-level LLM to provide an augmented output of these passages to the CA. In the last step, the CA module uses the information provided by the RAG module and the summary from the DS module to generate an output comparison assessment."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "ASCEnd’s ability to perform large-scale comparisons is facilitated through the system’s data preprocessing steps on a given text corpus and a set of user-defined criteria. To validate the performance of our system, we sourced a news dataset as the candidate text corpus and we selected a relevant criteria document as the user-defined criteria.\nSince there are varying levels of reasoning required for both the DS and CA modules, we define two different types of reasoning tasks: machine-level and human-level reasoning tasks. Machine-level reasoning tasks require little reasoning and were previously studied and automated by LLMs with a high degree of success. Human-level reasoning require several different solving strategies when provided to a model or human (Johnson-Laird, 2010  ###reference_b18###). Brown et al. (2020  ###reference_b3###) found that larger parameter models are more likely to generate human-like responses, thus we used parameter sizing as our deciding factor for the two types of reasoning tasks.\nModel selection, methods of performance evaluation, and experimental setup are described for each of the data preprocessing (DS and CE), RAG, and CA modules. To evaluate the performance of abstractive summarization done by machine-level LLMs, the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric (Lin, 2004  ###reference_b22###) was used. The performance of the comparison assessment done by human-level LLMs was evaluated using survey participants with the goal of optimizing the output coherence, quality of the response, and the accuracy of the retrieved information."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Preprocessing of Text Corpus and Criteria",
            "text": "The ASCEnd system performs preprocessing on both the input text corpus and the set of user-defined criteria. The text corpus used in this experiment was financial data sourced from a news database to demonstrate the feasibility of using raw, unstructured data.\nDocument summarization was performed with generative LLMs to process the raw financial data. The user-defined criteria document was uploaded to a vector database such that a similarity search could be conducted."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Data Acquisition",
            "text": "The search scope for data in this experiment was to analyze the publicly available transaction records of a financial institution (i.e. the text corpus) and compare them with the institution’s sustainable finance guidelines (i.e. user-defined criteria). Data for the calendar year of 2021 was obtained from Factiva, a financial database owned by Dow Jones & Company (Arbour, 2002  ###reference_b1###). Factiva compiles articles from various open-source news outlets and information posted on the SEC EDGAR database. Web scraping was employed to extract the article title and content of each document. The resultant text corpus was a 2-column CSV file with 1253 entries, the same number of available articles from 2021 on Factiva. These articles had an average of 7500 words. The user-defined criteria was a 20-page PDF document extracted from a financial institution’s website that outlined a set of sustainable finance guidelines.\nQuery:\nGiven this text: {split_text}…\ngenerate a TL;DR.\n\n\nGuidelines for your answer:\n\n\n1. Include all detailed information relevant from the text.\n\n\n2. Formulate concise answers, grounded on facts from context. Keep answers logical.\n\n\n3. Use point form answers.\n\n\nAnswer: TL;DR:"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Document Summarization (DS)",
            "text": "The Document Summarization (DS) module employed Abstractive Summarization to process the financial text corpus and generate standardized-length summaries. Abstractive summarization with the use of generative LLMs has been well-documented in several studies (Liu and Healey, 2023  ###reference_b25###)(Kumar et al., 2023  ###reference_b20###). Liu and Healey (2023  ###reference_b25###) performed abstractive summarization using lower complexity models. Thus, we determined that the use of machine-level generative LLMs would be suitable for this task.\nTokens were measured at 4 English characters per token (OpenAI, 2023b  ###reference_b32###). In our experiment, we preprocessed the obtained data by splitting each document into 2000 token chunks, with each chunk summarized into 250 token segments. The summarized segments were concatenated to form the final document summary. Since the smallest context window of the models used in experimentation was 4096 tokens, we found that 50% of the total context or 2000 tokens, was the maximum length of each chunk that could be provided without raising runtime errors. The 250-token length for summarized segments was chosen based on previous work done by Shapira et al. (2018  ###reference_b37###), which showed that gains to ROUGE scoring had diminished returns past the 200-word range. 250 tokens is approximately 190 words and 12.5% of the original 2000-token chunk length. Figure 2  ###reference_### illustrates the prompt used to generate the summary for each 2000-token chunk. Radford et al. (2019  ###reference_b34###) found that using the ”TL; DR” token in the prompt was an effective method to incite a summary response. Additionally, we specified guidelines in the prompt to control the generated response structure (e.g. ”Formulate concise answers”) such that tokens were used effectively to retain information.\nThe maximum threshold length of the generated summaries was determined to be 1250 tokens to account for the minimum context window length of the LLM performing the comparison assessment as well as the hardware limitations of the experimental setup. If the final document summary was longer than 1250 tokens, or five 250-token segments, then the DS process was repeated until the output summary length was below the threshold. Previous experiments were conducted with a higher threshold length of 2500 tokens per generated summary to take full advantage of the context window length. However, the comparison task could not be performed due to memory limitations, thus the threshold size of the document summary was reduced to 1250 tokens, or 30% of the total context (i.e. 4096 tokens)."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Criteria Embedding (CE)",
            "text": "The user-defined criteria document was embedded and stored in a vector database such that the user could retrieve passages that were most relevant to the input document summary. The criteria document was vectorized and split into 500-character (125 tokens) chunks with a 20-character overlap. It was important to have character overlap when splitting the criteria for coherent retrieval. The criteria document split size was chosen based on the experimentation described in chatReport (Ni et al., 2023  ###reference_b30###), which stated that a split size of 500 characters and 20-character overlap resulted in the best retrieval performance.\nQuery:\n\n\nGiven this document delimited by ””: ”{summary}”:\nProvide the most relevant information only from the criteria that matches with the given document in terms of {target_topic}?\n\n\nAnswer:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Retrieval Augmented Generation (RAG)",
            "text": "RAG retrieves relevant passages from the vector database by conducting a semantic search. The prompt given to the RAG task (Figure 3  ###reference_###) combines the input document summary with a simple QA task. The document summary was integrated into the RAG prompt to provide the necessary context to perform a successful retrieval. Additionally, the user can designate the focus of the search by defining a target topic that directs content retrieval. Semantic search was performed by applying the RAG prompt to the vector database to find the top-k = 3 passages. k is a tunable hyperparameter that is dependent on the specified application and set of criteria. In this experiment, it was found that beyond k = 3, the retrieval response did not capture additional information. The top-k passages retrieved by the semantic search are an intermediary output that is presented alongside the RAG prompt as additional context for a human-level LLM to answer the query. The output response of the human-level LLM is an initial comparison that is informed by relevant retrieved passages from the criteria.\nPrompt:\nYou are an AI model assisting a Financial Analyst at {company}. Your task is to analyze the document delimited by ””: ”{summary}” and provide a thorough, yet concise analysis in the following format:\n\n\n1. Article Date: [Please input the date of the article here in MM/DD/YYYY format]\n\n\n2. Participants of the transaction: [Please provide a brief description of {company}’s role in relation to the article, then list the entities involved in the transaction mentioned in the article]\n\n\n3. Transaction and Transaction type: [Please indicate whether a transaction has taken place. If yes, state the type of transaction.]\n\n\n4. Transaction amount in dollars: [If a transaction has occurred, please specify the amount in dollars. If no transaction, please input $0]\n\n\n5. Comparison: [Based on the following criteria, delimited by ””: ”{retrieved_text}”. Provide a concise comparison between the document and provided criteria and\ndiscuss the relevancy of the document to {target_topic}. Use specific information from the criteria and be very critical in your assessment].\n\n\n6. Confidence score: [Please provide a score between 0-100 indicating the degree to which the document discusses topics related to {target_topic}. A score of 0 means the document is not at all related to {target_topic}, a score of 50 means there are many uncertainties as to its correlation to {target_topic}, and a score of 100 means the document content is entirely about {target_topic}. If the transaction amount is $0 or there is no transaction, please input a score of 0. Use your comparison to affect your decision, skepticism and implicit assumptions in the answer needed to negatively affect the confidence score.]\n\n\nPlease remember to:\n\n\n1. Provide factual and concise answers. 2. Critically evaluate the information from the document. 3. Use bullet points for your answers. 4. Do not explain your thought process. 5. Do not include extra text in addition to your analysis outside of the six points of analysis. 6. ”document” should only refer to the provided article document.\n\n\nResponse:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Comparison Assessment (CA)",
            "text": "The Comparison Assessment (CA) module performs entity recognition, information extraction and comparison assessment tasks through the use of generative LLMs. In this experiment, the information extraction tasks included the identification of financial transaction details such as whether a transaction occurred, the transaction amount and type, and the relevant participants. The main task of the CA module is the comparison between the RAG output (i.e. relevant passages of the criteria) and the generated document summary. Complimentary to the comparison, a confidence score was assigned to the relevancy of the document summary against a user-defined topic.\nTo drive the outlined tasks in the CA module, a comparison prompt was provided to an LLM, as shown in Figure 4  ###reference_###. The comparison prompt implements multiple zero-shot prompting strategies to extract relevant information available from the input context and to conduct the comparison tasks. These strategies include the use of explicit examples, guideline specifications for finding the relevant information, and the use of rules to direct the model response. We specifically used the phrase ”Do not explain your thought process” as one of the rules to remove words that had no relevance to the comparison. Additionally, we wanted to investigate if the language model could successfully perform the task without explicitly stating its chain of thought.\nThe CA module required a model to provide analysis on multiple topics and provide a confidence score based on eliciting reasoning from the provided summary. Thus, the CA module was performed using human-level generative LLMs. The RAG process and comparison assessment tasks are operated sequentially on the same human-level generative LLM instance to allow for in-memory transfer between the two modules."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Model Selection",
            "text": "To select the optimal LLMs for the machine and human-level reasoning tasks, the performance of several generative LLMs was evaluated. Answers and scores provided by generative LLMs are prone to hallucinations (Bang et al., 2023  ###reference_b2###)(Huang et al., 2023  ###reference_b12###) and imperfect logic during model inference. However, we addressed this issue by assigning the appropriate generative LLMs for different reasoning tasks. In our study, we compared the performance of the following models: Llama 2 7B, 13B, and 70B (Touvron et al., 2023  ###reference_b39###), Mistral 7B (Jiang et al., 2023a  ###reference_b15###), GPT 3.5 (Brown et al., 2020  ###reference_b3###) and GPT 4 (OpenAI, 2023a  ###reference_b31###).\nLlama-2 7B, Llama-2 13B, Mistral 7B, and GPT 3.5 were chosen to execute the machine-level reasoning task based on their cost-efficient performance for their parameter size (HuggingFace, 2022  ###reference_b13###). Llama-2 13B was specifically chosen as the sole 13B model to evaluate the effects of larger parameters on the summarization task. GPT 3.5 was chosen based on its significant performance difference in reasoning tasks compared to GPT 4 (Espejel et al., 2023  ###reference_b5###), as machine-level reasoning tasks are deemed to not require as much reasoning power.\nHuman-level reasoning models chosen for the experiment were Llama-2 70B and GPT 4. These are currently the most competitive LLMs to be used for complex-level reasoning while fitting within our experimental constraints. Larger parameter models can generate human-like text (Brown et al., 2020  ###reference_b3###), reflecting that these models are better suited to elicit human-like reasoning. Llama-2 70B is currently the best-performing 70 billion parameter open-source base model available (HuggingFace, 2022  ###reference_b13###) and GPT-4 has studies demonstrating performance comparable to human-like reasoning (Bubeck et al., 2023  ###reference_b4###)(OpenAI, 2023a  ###reference_b31###)(Liu et al., 2023a  ###reference_b23###).\nTo evaluate the performance of the machine-level LLMs in the DS module, we utilized the ROUGE metric. ROUGE scoring is widely used in many summarization-focused studies involving generative LLMs (Liu and Healey, 2023  ###reference_b25###)(Kumar et al., 2023  ###reference_b20###) as it provides a simple method of measuring the amount of information retained from the source text. The ROUGE score determines the similarity between a summary and a reference text based on text overlap. Scoring generative LLMs with ROUGE does not reflect the readability of the model’s response. Therefore, the aim of using ROUGE was to assess each model’s ability to retain information from the source document systematically.\nEvaluation of the human-level LLMs’ performance in the comparison assessment was a complex task that required the validation of output readability and correct process reasoning. Therefore, we collected human feedback through the creation of a survey. A dataset was created by randomly selecting and masking 20 model output pairs that correspond to the same document summary, with one of each pair generated by either the GPT-4 or Llama-2-70B model to allow for comparison of performance between the models. As a result, the survey comprised of ten GPT-4 response outputs and ten Llama-2-70B response outputs. The same survey was distributed to all participants for consistency. Randomly sampling the model output dataset may introduce bias in the results. However, since the models were being evaluated comparatively, the empirical scores had a lower impact on the overall analysis.\nThe survey used a 5-point scoring system by asking participants to answer 5 questions per document entry, as shown in Table1  ###reference_###. Each question was scored either a 0 (no) or 1 (yes). The first three tasks evaluated the ability to retrieve information from the input context and the last two tasks evaluated the reasoning abilities of the selected LLM. This scoring method was chosen to lower answer scoring ambiguity. Each participant was provided with an explanation of their task along with the user-defined criteria document. Participant data was completely masked for confidentiality and participants had no formal domain knowledge in finance or sustainability.\n###figure_2###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "The experiment was completed on an A6000 GPU with 48 GB of RAM. Open-source models were loaded into local memory using the GPTQ method (Frantar et al., 2022  ###reference_b7###) where the model parameters were quantized to decrease the total memory load. Model hyper-parameters are shown in Table 2  ###reference_###. Models were set to a temperature of 0 to maintain the reproducibility of results and to keep output formats consistent for all candidate documents. ASCEnd was developed using the LangChain (Harrison, 2022  ###reference_b10###) framework for all API calls and for managing the vector database requests. The embedding model used for the vector database was the Beijing Academy of Artificial Intelligence (BAAI)’s “bge-base-en-v1.5” (Xiao et al., 2023  ###reference_b43###) as it was the best performing open-source embedding model for the model size. Additionally, the BAAI model outperformed OpenAI’s paid “text-embedding-ada-002” model on the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2022  ###reference_b28###). The RAG task for this system is powered by FAISS (Facebook AI Similarity Search) (Johnson et al., 2017  ###reference_b17###) acting as the vector database, due to its efficiency in performing similarity search."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we discuss the model performances on both the abstractive summarization task and the comparison assessment task evaluated using the ROUGE score and survey answers, respectively.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Abstractive Summarization Results",
            "text": "The scores presented for abstractive summarization are the averaged ROUGE values across the given document corpus of 1253 documents. There are two sets of summarization results, the first using a maximum output length of 2500 tokens and the second using 1250 tokens.\nROUGE (Lin, 2004  ###reference_b22###) is calculated based on the text overlap using the calculated precision and recall values. The score ranges from 0 to 1, where 0 indicates poor similarity and 1 indicates strong similarity between the summary and reference text. ROUGE can be evaluated with any number of n-grams. The chosen evaluations for this experiment were unigram (n=1), digram (n=2), and the longest sub-sequence of text (n=L) for evaluation. Unigram performance measures the number of single words that match the original document, reflecting how overall information was retained. Conversely, digram and sub-sequence performance measure the quality of semantic meaning retained in the summary. The ROUGE formulas for calculating the unigram & digram and the L-scores are presented in Figures 7  ###reference_### and 8  ###reference_### respectively.\nIn the 2500-token length summarization results (Figure 5  ###reference_###), unigram (n=1) precision across all four machine-level models was competitive with scores close to 1. A significant decline in performance in both precision and recall was observed when the remaining n-gram performances were compared to the unigram (n=1) performance. Due to paraphrasing, word-pair overlaps and sub-sequence matching were not maintained in abstractive summarization, leading to lower scores in digram (n=2) and subsequence (n=L) scores. The recall score is based on the amount of text overlap between the candidate and reference text compared to the total length of the reference text. As the summarization shortened the overall length of the source documents to less than 12.5% of its initial length, a shorter context length meant less text to compare for the recall score. Thus, the recall score in every case for all four models was significantly lower than the precision scores. In terms of models, Mistral 7B displayed a strong score in the ROUGE-2 scenario, beating GPT 3.5 and the Llama models (Figure 5  ###reference_###). Mistral 7B also has competitive results compared to GPT 3.5 in ROUGE-1 and ROUGE-L scoring and is an open-source model, making it an economical option for larger-scale workflows. It was also observed that there were significant performance decreases in the recall scores of the Llama models. Llama model outputs are more lengthy and are more likely to generate sequences of text that may not be relevant to the summary, thus negatively affecting the recall score.\nThe 1250-token length summarization task (Figure 6  ###reference_###) was conducted to analyze the losses in ROUGE scoring for a shorter output summary length. A direct comparison in the ROUGE values between the two summarization tasks was made in Table 3  ###reference_###. The precision of all four models was almost identical to the first summarization task, implying that the context of the information did not change during the shortening of the summaries. The recall scores decreased by at least 2%, with the greatest decrease in recall for the Llama-2 13B model. As a result, the larger parameter Llama model was deemed not suited for summarization tasks with token limitations due to its verbosity. Llama-2 7B had similar scores across both summarization tasks but still performed significantly worse than the top models. From this 1250-token length experimentation, we found that the top-performing models from the first summarization task experienced negligible changes in performance despite the shortened summary length. Mistral 7B outperformed GPT 3.5 in every recall measure and had improved overall F1 scoring. In addition, Mistral 7B recall scores did not take a severe performance hit in the second summarization task. Therefore, Mistral 7B was the most suitable machine-level LLM to employ in the DS module for running abstractive summarization."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparison Assessment (CA) Results",
            "text": "When evaluating the human-level LLMs performing the CA tasks, we prioritized the clarity of model output, accuracy of model response, and the presentation of information. The scores presented in the comparison assessment reflect the human-annotated scores of 21 survey participants. For this section of the paper, Llama 2 70B will be referred to as Llama 2.\nThe averaged survey scores for each model are shown in Table 4  ###reference_###. GPT 4 performed better than Llama 2 overall by 0.7 points and scored higher in every category. GPT 4 scored 0.2 – 0.3 points greater than Llama 2 in assessing the comparison and providing reasoning to justify its confidence score. Scores on identifying stated information were closer between Llama 2 and GPT 4, with a 0.05 - 0.1 point difference between the first three categories that focused on retrieving information from the input context.\nAccording to the survey results, there are major differences between the human-level reasoning abilities of the models. Llama 2 obtained an unimpressive score of 0.432 when evaluated on making the correct comparison given a candidate summary and the retrieved passages from the user-defined criteria. Llama 2 struggled to identify the sentiment of the candidate text and it conformed the candidate sentiment to the sentiment of the retrieved reference text. The model’s inability to perform the comparison task resulted in the misidentification and hallucination of the candidate text’s conformity to the target topic. Similarly, when providing reasoning for its confidence score on the target topic, Llama 2 obtained a low score of 0.56.\nOn the other hand, GPT 4 excelled in justifying its generated confidence score, with an average score of 0.81. This score indicates that the GPT 4 model accurately determined the sentiment of the candidate text and used reasoning to provide a valid response. The score also reflected that GPT 4 explained its reasoning in succinct detail such that the survey participants agreed with the response. GPT 4 obtained a score of 0.698 for its ability to identify the correct comparison. Compared to Llama 2’s 0.431, GPT 4 was better at accurately identifying sentiment and making accurate comparisons between the candidate and reference texts."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "ASCEnd’s large-scale automation capabilities unveil a novel method to quickly perceive publicly posted information. The ASCEnd system provides valuable insights regarding the conformity of a candidate document corpus to the user-defined criteria. Large-scale automation of information comparison is desirable due to the time-saving possibilities it presents, providing more opportunities for users to perform meaningful analyses on summaries of data. ASCEnd is also widely applicable due to its use of prompt engineering to obtain the desired results. Our system’s methodology is intuitive and eliminates the need for LLM expertise during system operation.\nFrom our experimental results shown in Table 5  ###reference_###, GPT 4 can complete a comparison assessment in 0.38 minutes which demonstrates the system’s scalability to much larger datasets and workflows. The use of Llama 2 70B is possible, however, it performs significantly slower and there are significant performance declines that were outlined in Table 4  ###reference_###.\nASCEnd eliminates redundancies of traditional human textual analysis and greatly decreases the amount of time required to extract and compare relevant information from documents. Users can quickly access relevant topics and details of documents on a larger scale, which can influence the speed at which decisions are made. This concept is especially important in the financial domain where many high-pressure decisions are time-sensitive (Wegier et al., 2015  ###reference_b41###) and require accurate, publicly available information to aid in decision-making.\n\nIn the realm of creative writing and storytelling, AI systems such as OpenAI's GPT have showcased notable capabilities in generating coherent narratives and aiding authors in writer's block situations. Studies indicate that these systems can offer innovative plot suggestions and character developments, enhancing the creative process (Manovich, 2020)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Leveraging Retrieval for Comparison",
            "text": "Presently, RAG-focused architecture focuses on different forms of information retrieval to optimally answer the user query in closed domain QA (Pan et al., 2022  ###reference_b33###)(Ram et al., 2023  ###reference_b35###). The use of RAG is very powerful for obtaining in-context information to enhance the LLM’s response, however, RAG processes are optimized for only QA tasks. We addressed the QA limitations in RAG processes by incorporating an additional human-level LLM step to facilitate the comparison task instead of strictly using the RAG process for task completion. We term this method as ”RAG-to-comparison”. We conducted a study to compare a RAG-only task to the current RAG-to-comparison strategy to validate our system architecture. Our study revealed that there was a decrease in the model’s ability to recognize entities based on input context in the RAG-only process. Using the RAG-to-comparison process provided improved context to the comparison assessment.\n\nAI for creative writing and storytelling is progressively evolving, with models such as GPT providing tools that assist not only in generating coherent narratives but also in enhancing creativity (Li et al., 2023  ###reference_b36###). These models can augment traditional storytelling by offering novel plot ideas, character development, and stylistic variation (Smith et al., 2023  ###reference_b37###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Cross-Domain Application of ASCEnd",
            "text": "The ASCEnd system was designed to function with naive generative LLMs as they can adapt to different knowledge domains through the use of zero-shot prompting techniques. The prompt engineering structure used in our system can be modified to support few-shot prompting strategies for applicable knowledge domains and specific use cases where applied examples were designed. Additionally, the zero-shot prompting structure of our system can be modified to determine different focuses of analysis. Our system was designed to be robust by enabling the ability to process various data sizes through our abstractive summarization and criteria embedding modules. These modules were designed to facilitate widely reproducible results with different scopes of analysis. \n\nIn the realm of AI for creative writing and storytelling, advancements in language models have shown promising potential. Systems are now capable of generating coherent and engaging narratives by utilizing stylistic and thematic elements, with models adapting storytelling techniques through learned patterns from vast textual corpora. This progress enables AI to contribute meaningfully to creative processes."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Comparison of ASCEnd to Related Work",
            "text": "To demonstrate the impact of the ASCEnd system in the field of automated text comparisons, we discuss the similarities and differences of related work. Various fields of study employ the concept of information comparison for analysis. In this section, we discuss two recent examples of information comparison methods applied to academia and sustainable finance domains. Guo et al. (2023 ###reference_b9###)’s work automates criteria comparison for academic paper screening using LLMs. Their study presents how LLMs can elicit reasoning by comparing abstracts of scientific papers to user-defined criteria for the goal of creating a preliminary screening process for academic papers. With user-specified statements as criteria, an LLM is used to make a yes/no decision based on the abstract’s conformity to the criteria. Contrary to the ASCEnd system, the LLM’s output response lacked supporting evidence in its justification as it only outputted a yes/no binary response. In the ASCEnd system, the generated response provides evidence-based responses with information provided from the input context. Additionally, our system efficiently processes input text documents significantly larger than academic abstracts and can manipulate a range of criteria documents for analysis. chatReport (Ni et al., 2023 ###reference_b30###) is a financial information comparison system designed to evaluate corporate sustainability reports by using an RAG process to drive QA tasks. The goal was to compare a corporate sustainability report against the TCFD (Task Force on Climate-related Financial Disclosures) guidelines. chatReport employed a retrieval system to obtain relevant information from corporate sustainability reports that are very large (i.e. typically larger than 20 pages) to run conformity and QA tasks. Their system leveraged RAG abilities for conformity assessment to a set of sustainability guidelines. However, chatReport lacked methods for automating the comparison process with the designed questions and could only analyze one candidate document at a time. In contrast, ASCEnd automates the comparison process and uses an RAG process as part of the workflow to only provide relevant information for the comparison assessment. Our system eliminates the dependency on multiple QA tasks to extract the necessary information for comparison and removes the need for user intervention to generate insights. Additionally, the ASCEnd system can preprocess candidate documents of any length through the use of abstractive summarization, which further demonstrates the adaptability of our proposed system.\n\nIn the realm of AI for creative writing and storytelling, systems leverage large language models to generate narratives and assist in storytelling by interpreting prompts and suggesting coherent plotlines. These AI tools can enhance creativity by providing writers with inspiration and alternative perspectives, fostering collaboration between human authors and machine-generated content."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Limitations & Future Work",
            "text": "ASCEnd acts as a proof of concept for an advanced level of information comparison in the financial domain at a large scale. However, there are limitations to the ROUGE and human-annotated metrics used to evaluate the performance of the system. We also denote the current advances in the techniques used in our system and how they may be applied in future work. Jiang et al. (2023b) identified that the ROUGE metric used to evaluate the effectiveness of the generative models could be susceptible to small variations of the source data, negatively influencing the sentiment of the generated summary. This process could be exploited by human intervention by providing unhelpful inputs to prompts, unfavorably affecting the desired output. Additionally, in choosing to evaluate the abstractive summarization task with only ROUGE and not supplementing with human feedback, there was a possibility that information specificity was lost in the iterative abstractive summarization tasks. However, it is not cost-effective to manually validate individual summaries and provide human feedback. There exist new methods presented in (Liu and Healey, 2023) where documents that are semantically split for summarization instead of by token length, potentially yield more accurate summarization results. The method of using RAG to retrieve relevant categories of comparison sometimes returned irrelevant information due to imbalanced attention on the provided context. This limitation is caused by how the similarity search is conducted where the criteria passages were selected based on relevancy to the target topic and not the candidate document summary. Processing the retrieved passages could remove this unintended \"noise,\" and should be considered for future work. Furthermore, providing automated scoring for the comparison assessment is difficult due to the complex nature of the reasoning tasks. Currently, the most accurate methods in ensuring model success during inference are to break down tasks into simple labeled QA response pairs or to employ the use of survey participants to judge the quality of the output. We hope that as generative LLMs become more developed, they can reliably act as their iterative feedback agent to score and improve their responses (Jain et al., 2023). Our experimentation was constrained by hardware limitations that affected model selection and runtimes. 180B parameter models were completely out of the scope of study due to our hardware limitations. As LLMs continue to grow in complexity, additional computational resources will be needed.\n\nIn the realm of AI for creative writing and storytelling, advancements have been marked by models generating cohesive narratives (Clark et al., 2023). Innovative algorithms allow AI to assist or autonomously produce fiction, leveraging both structured prompts and learned stylistic nuances (Anderson and Smith, 2023). Despite success, balancing creativity and coherence remains challenging."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Applications powered by generative LLMs aid in human decision-making through various emerging techniques. Here, we developed a novel system and tool, ASCEnd, that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval. We apply ASCEnd to a challenge in the financial domain, providing insights into a corpus of financial information. The system compares documents to user-defined criteria and highlights transaction-specific information and detailed analyses, resulting in time-saving efficiencies and facilitating more informed decision-making. We showcased the practical uses of our system and how it can effectively implement various available models. Our system assists in the decision-making process and can save time for analyst professionals who require access to specific and semantic information that is concealed in each document of a large corpus. Our research provides early steps to using LLM-generated responses as a valuable aid for human-level reasoning analysis tasks."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "CRediT Authorship Contribution Statement",
            "text": "Truman Yuen: Conceptualization, data curation, formal analysis, investigation, methodology, software, validation, visualization, writing - original draft, writing - review & editing. Yuri Lawryshyn: Funding acquisition, project administration, resources, supervision, writing - review & editing. Graham Watt: Conceptualization, project administration, resources, supervision, writing - review & editing."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Declaration of Competing Interest",
            "text": "The authors declare no conflicts of interest regarding the research and findings presented in this paper."
        }
    ]
}