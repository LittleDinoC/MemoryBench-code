{
    "title": "Prompt Mining for Language-based Human Mobility Forecasting",
    "abstract": "With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline. Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "The forecasting of human mobility plays a crucial role in various domains, including urban planning, transportation management, public health preparedness, and disaster response. Accurate predictions of movement patterns and trends can facilitate better location-based services, proactive decision-making, optimize resource allocation, and enhance overall societal resilience.\nTraditional statistical forecasting methods (Calabrese et al., 2010  ###reference_b2###; Qiao et al., 2018  ###reference_b16###) often rely on numerical models, but they might fall short in capturing the complex human behaviour in real-world scenarios. In an effort to capture these complex patterns, more advanced deep learning forecasting methods (Feng et al., 2018  ###reference_b5###; Sun et al., 2020  ###reference_b19###; Yin et al., 2023  ###reference_b31###), particularly Transformer-based approaches(Xue and Salim, 2021  ###reference_b27###; Hong et al., 2022  ###reference_b7###; Xue et al., 2021  ###reference_b26###), have emerged. However, these methods often require very complicated model architectures.\nRecently, language-based forecasting approaches (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###; Xue and Salim, 2023  ###reference_b28###) have demonstrated another line of work and emerged as a promising alternative, harnessing the power of natural language processing (NLP), language generation frameworks, and advanced pre-trained language models to generate forecasts. These methods present a new paradigm (as shown in Figure 1  ###reference_### (b)) for forecasting human mobility.\nBy transforming numerical data into natural language sentences, these methods make it possible to use existing available language models to comprehend and predict human mobility patterns.\nThis shift towards language-based forecasting has opened up new avenues for enhancing predictive capabilities. Language models, such as Bert (Devlin et al., 2019  ###reference_b4###) and its successors, have demonstrated remarkable proficiency in understanding and generating human language. Leveraging their capabilities for mobility forecasting offers the potential to achieve accurate predictions while also reducing the need of designing complex specific forecasting models.\n###figure_1### However, a key challenge in language-based mobility forecasting lies in the design of effective prompts, that is, how to transform the numerical mobility data and associated auxiliary information into sentences for language models.\nThese prompts serve as the bridge between raw numerical data and natural language descriptions, shaping how well language models can capture the underlying mobility patterns.\nGenerally, different prompts can lead to varying language generation performance, consequently impacting the accuracy of the forecasting.\nTaking ChatGPT as an example, even for the same topic, using different prompts as inputs will largely result in different responses.\nHence, although using fixed manually designed templates for prompt generation as in existing work (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###; Xue and Salim, 2023  ###reference_b28###) is simple and straightforward, inadequate prompt template exploration can lead to inaccurate forecasts\nTo address these limitations, we propose a novel prompt mining framework for the language-based mobility forecasting task (Figure 1  ###reference_### (c)). Our framework is a multi-stage pipeline with prompt initialization, prompt generation, and prompt refinement stages.\nSpecifically, in the prompt generation stage, we introduce a Prompt Quality Evaluator that evaluates the quality of generated prompts based on a combination of heuristic classifier rules and prompt entropy. By doing so, we can quantitatively evaluate the quality of prompts, which enables the prompt generation model to receive feedback for generating high-quality prompts during training.\nThe prompt refinement stage aims to enhance the generated prompts through a series of sophisticated mechanisms including noise reduction, integration of a chain of thought, and the application of information gain-based temporal segmentation on mobility data. These strategies collectively contribute to generating more refined and contextually relevant prompts.\nThrough extensive experiments with real-world large-scale mobility data, we show the effectiveness of our prompt mining approach. Our results showcase the superiority of generated prompts compared to traditional fixed templates.\nIn summary, our work has 3 main contributions:\nWe propose a novel prompt mining framework that addresses the limitations of relying on fixed templates in existing methods. The chain of thought consideration is also integrated into the prompt mining pipeline.\nIn our framework, the concept of information entropy is explored in both the semantics of the prompt in the prompt generation stage as well as the distribution of the mobility data in the prompt refinement stage.\nWe conduct extensive experiments on real-world mobility data to empirically validate the efficacy of our proposed method. Our mined prompt variants show good performance under the language-based mobility forecasting setting."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Numerical Time Series Forecasting",
            "text": "Human mobility forecasting is often conceptualized as a special case within the broader scope of general time series forecasting. Consequently, methodologies developed for general time series forecasting are frequently applied to this domain. Modern time series forecasting techniques heavily rely on deep learning, primarily utilizing Recurrent Neural Network (RNN) architectures such as Long Short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Chung et al., 2014). Examples within this RNN-based framework for predicting sequential human behavior include ST-RNN (Liu et al., 2016) and DeepMove (Feng et al., 2018). Motivated by the recent success of applying Transformer architecture (Vaswani et al., 2017) in modeling nature language sequences, it has also been applied for human mobility forecasting (Xue and Salim, 2021; Xue et al., 2021) and general time series forecasting tasks (Zhou et al., 2021; Xu et al., 2021; Zhou et al., 2022). These methodologies, whether based on RNNs or advanced Transformer architectures, typically adopt a sequence-to-sequence approach, where historical numerical observations form the input sequence to predict future outcomes. As the need to incorporate semantic information (e.g., a POI is a restaurant or a shop) arises to further improve forecasting performance, the complexity of model architectures tends to increase.\n\nAdvancements in AI for autonomous vehicles primarily focus on machine learning models like Convolutional Neural Networks (CNNs) and Reinforcement Learning (RL), facilitating effective environment perception and decision-making. Emerging Transformer-based models further enhance real-time processing capabilities, while increasing integration of sensor fusion techniques aids in creating comprehensive situational awareness for safer navigation."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Language-based Forecasting",
            "text": "Efforts to integrate semantic information more directly and effectively have led to the introduction of shaping mobility forecasting as a language generation problem. Xue et al. (2022a) introduced this approach, utilizing natural language sentences to describe historical observations and prediction targets, thereby transforming the forecasting task into a language generation format. Building upon this framework, Xue et al. (2022b) leverage language foundation models such as BERT (Devlin et al., 2019) as the backbone network of the language generation part to conduct human mobility tasks. The recent work by Xue and Salim (2023) further introduced the concept of PromptCast that extended language-based forecasting to other time series data domains like energy and temperature forecasting. However, the prompts used to describe time series are still based on fixed heuristic templates. In this paper, we hypothesize that the performance of language-based forecasting is related to the ways of using language sentences to describe numerical data. Unlike prior studies that have focused narrowly on fixed templates for prompting, our work aims to address this gap by exploring methods to develop more effective prompts, thereby potentially enhancing the performance of the language-based forecasting paradigm.\n\nIn the realm of AI for autonomous vehicles, deep learning techniques have revolutionized perception and decision-making processes. Techniques like convolutional neural networks (CNNs) enhance object detection and classification in real-time driving environments, while reinforcement learning algorithms facilitate adaptive navigation and path planning, advancing the capabilities of fully autonomous systems."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Method",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Problem Formulation",
            "text": "In this subsection, we formally define the problem that our proposed prompt mining framework aims to address: enhancing language-based human mobility forecasting through optimized prompt design.\nAs influenced by previous studies (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###), we simplify the forecasting task of interest as follows.\nGiven the historical customer visit records of a Point of Interest (POI)  over a span of  consecutive days, represented as , the objective is to predict the number of visits  for the subsequent day .\nNormally, different types of auxiliary information of  such as the area region information , the semantic information , the opening information (e.g., business opening time  and ending time ), or fine-grained hourly visits are often available, which can be used to assist the forecasting.\nFor the sake of clarity and simplification, we will omit the superscript  (indicative of the POI index) from this point onward.\nTemplate and Prompt: To leverage language models for the forecasting purpose, existing methods employ a predefined prompting template P to convert the numerical data (e.g.,  and ) into sentences (these sentences are referred as prompts) that can be directly processed by language models.\nThe template consists of placeholders for the actual values from the data, resulting in sentences that present the mobility information in a human-readable format.\nSpecifically, the template consists of two essential parts:\nthe history prompt template  that describes the historical values as the input sentences of the language models and the future prompt template  that defines the sentences for the description of future observations.\nIn this study, we focus on the design of P\n(especially the input parts  of language models)\nand aim to explore diverse prompts that are contextually relevant, coherent, and effective in guiding language models to produce accurate forecasts. Through the investigation of prompts, we seek to enhance the overall forecasting accuracy and reliability of the language-based human mobility forecasting approach."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Framework Overview",
            "text": "As illustrated in Figure 2  ###reference_###, the proposed prompt mining framework consists of three key stages: prompt initialization, prompt generation, and prompt refinement.\nPrompt Initialization: The mining process starts with prompt initialization. At this stage, we set the groundwork by creating a diverse pool of potential prompt templates manually. These templates can be used as supervision signals to mentor the following generation stage.\nPrompt Generation: In the prompt generation phase, we employ a language model  as the core engine to learn how to generate “better” prompts. An evaluator module  is specifically designed to indicate the quality of generated prompts in the training process of .\nPrompt Refinement: The final stage of our framework is prompt refinement. We introduce several mechanisms to further improve the quality of generated prompts from the previous generation step.\nBy combining these three stages, our prompt mining framework ensures the systematic evolution of prompts from their initial conception to refined and optimized forms.\nThe following subsections delve into each stage of the framework, providing comprehensive insights into the proposed methodology for providing better and more dynamic prompts for the language-based forecasting process.\n###table_1###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Prompt Initialization",
            "text": "In our prompt mining framework, the initialization of prompts is a critical step to kickstart the process of generating high-quality forecasting prompts. The initial prompts serve as the foundation upon which subsequent iterations build, which also ensures the generated natural language has a certain directionality.\nIn a nutshell, the overarching objective of prompt mining is to use a designated language model  to automatically generate prompts from the original raw data, which is typically presented in numerical format.\nHowever, as the first attempt, the absence of well-established datasets containing diverse prompts for human mobility data, along with a lack of relevant pre-trained language models for mobility prompt generation, poses a unique challenge.\nGiven these considerations, we introduce a pragmatic approach. Instead of directly generating prompts from the raw numerical mobility data, we propose to leverage the fixed template (referred to as the initial template) used in previous work as the input of the prompt generation model  and  is trained to yield better prompts from the initially given input throughout the prompt mining process."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1. Initial Prompt Template",
            "text": "As exampled in Table 1  ###reference_###, we present a straightforward and uncomplicated template that functions as the initial template within our mining framework.\nAs aforementioned, this initial template serves as the foundation for transforming raw mobility data into the preliminary prompts denoted as , which are considered as the starting prompts of the entire prompt mining process.\nThus, given an input instance , the prompt generation process (elaborated upon in Section 3.4  ###reference_###) can be succinctly expressed as:\nwhere  represents the generated prompts in the natural language format for the specified input instance."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2. Templates Pool",
            "text": "Based on the initial template, the inputs of model  (i.e., ) are obtained.\nTo effectively train the model , a collection of pseudo labels is essential to supervise the training process.\nTo this end, we establish a pool of templates, as detailed in Appendix A  ###reference_###, Table 6  ###reference_###. These templates are crafted based on simple heuristics; for instance, Complex templates encompass more detailed information such as specific Points of Interest (POI) details and POI working hours, whereas lower quality templates are more simplistic. The pool consists of 12 lower quality templates and 6 Complex templates.\nThese templates support two main purposes: firstly, they are employed to train a classifier (refer to Section 3.4.1  ###reference_.SSS1###) capable of screening the quality (whether high or low) of a given prompt in the later prompt generation stage; secondly, they serve as pseudo labels during the training of .\nIt is worth noting that despite the discrepancy in the number of Simple and Complex templates, the generated training Simple and Complex prompts instances are equivalent in number and balanced.\nImportantly, during the training process of , it is noteworthy that these heuristic templates are utilized only as pseudo labels and not as ground truth labels.\nThis ensures that the model  generates dynamic prompts rather than replicating the exact prompts given in the template pool. The objective is to enable  to mine and discover prompts that are contextually sophisticated and better suited for human mobility forecasting.\n###table_2### This is a  Mobil in WI, Osseo. The human mobility of the past 3 days are:  0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0 people (per hour) came here from 00:00 to 24:00 (working time) on Mon. 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1 people (per hour) came here from from 00:00 to 24:00 (working time) on Tue. 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 3, 0, 0, 1, 0, 0 people (per hour) came here from 00:00 to 24:00 (working time) on Wed. How many people will visit this place tomorrow?\nOn Thu, there are 0, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 3, 0, 1, 2, 0, 1, 0, 1, 0, 0, 2, 0 people who will visit Mobil during working time.\nThis is a Mobil in WI, Osseo. The human mobility of the past 3 days are: 5 people came here during the first half of the work shift and 4 people came here during the latter half of the work shift on Mon. 4 people came here during the first half of the work shift and 8 people came here during the latter half of the work shift on Tue. 5 people came here during the first half of the work shift and 7 people came here during the latter half of the work shift on Wed. How many people will visit this place tomorrow?\nOn Thu, there will be 7 people to visit Mobil during the first half of the work shift and 10 people to visit Mobil during the latter half of the work shift. Therefore, there are 17 people will visit here.\nThis is a Mobil in WI, Osseo. From Mon to Wed, the human mobility during the first and second half working time are 5, 4, 4, 8, 5, 7. The entire working time is composed of the first half and the second half. Therefore, from Mon to Wed, the total human mobility are 5 + 4 = 9, 4 + 8 = 12, 5 + 7 = 12. How many people will visit this place tomorrow?\nOn Thu, there will be 7 people to visit Mobil during the first half of the work shift and 10 people to visit Mobil during the latter half of the work shift. Therefore, there are 17 people will visit here.\nThis is a Mobil in WI, Osseo. From Mon to Wed, the human mobility during the 4 different time segments are 1, 4, 3, 1; 0, 5, 1, 6; 0, 6, 1, 5. The entire working time is composed of the whole time segments. Therefore, from Mon to Wed, the total human mobility are 1 + 4 + 3 + 1 = 9; 0 + 5 + 1 + 6 = 12; 0 + 6 + 1 + 5 = 12. How many people will visit this place tomorrow?\nOn Thu, there will be 3, 5, 2, 7 people to visit Mobil during these 4 different time segments. Therefore, there are 17 people will visit Mobil on Thu."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Prompt Generation",
            "text": "As illustrated in the middle module of Figure 2  ###reference_###, the prompt generation stage is the core part of the entire prompt mining process.\nIt functions based on the initial prompt as well as the heuristic template pool from the previous prompt initialization stage.\nThe prompt generation stage consists of two key components: the Prompt Quality Evaluator  and the generation model ."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1. Prompt Quality Evaluator",
            "text": "During the training process of model , it is important to inform the model of the quality of the generated prompts.\nThis motivates the introduction of a specialized module – the Prompt Quality Evaluator.\nThis component plays a pivotal role in the prompt generation process, which serves as a critical feedback mechanism to assess the quality and effectiveness of generated prompts. It aids in distinguishing between high-quality and low-quality prompts, thereby guiding the iterative optimization of .\nBased on the Simple/Complex templates in the template pool, we use a modest portion of (20%) available numerical data to generate prompts. Specifically, we randomly select a template from the pool to make prompts and we make sure the total number of generated Complex prompts is the same as Simple prompts.\nThe central role of this classifier is to perform a binary classification, determining whether a given prompt  aligns with a high-quality () or a low-quality () from the heuristic perspective.\nGiven the heuristics used to generate templates pool lack a concrete quantitative measure to gauge quality performance, a supplementary criterion is essential to fully discern the quality of generated prompts.\nEssentially, the prompt generating is a process of information transmission, a translation from the domain of raw data into a descriptive language space.\nInspired by the observation that the language with higher information entropy is more efficient in transmitting information under different language systems (Montemurro and Zanette, 2011  ###reference_b13###; Liu et al., 2022  ###reference_b10###), we introduce a special element to the Prompt Quality Evaluator – the Prompt Entropy.\nWithin the prompt context, a higher entropy signifies a greater level of uncertainty in the information encapsulated within the prompt. This enhanced uncertainty translates to a higher potential for the training of more effective forecasting models.\nThe information entropy of a yield prompt  is calculated at the character level:\nHere,  represents the probability of the symbol or character  occurring within the prompt.\nThus, by synergistically integrating the rules established by the heuristic classifier and the prompt entropy metric, the ultimate prompt quality  is formulated as:\nwhere  is the indicator function and  is the employed entropy threshold."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2. Training of",
            "text": "As expressed in Equation (1  ###reference_###), during the training process of , the input prompts stem from the initial template’s generated prompts.\nIn this backpropagation process, the quality evaluator  plays a pivotal role in determining which output instances contribute to the training loss computation for the parameter update in .\nThe decision is guided by the conditions outlined in Equation (3  ###reference_###), where exclusively high-quality prompts are retained, while low-quality prompts are discarded. This selection mechanism enforces the focus on generating high-quality prompts, driving the iterative process towards improved language generation performance.\nTo enhance ’s capacity to generate high-quality prompts, the Complex pool established during prompt initialization is utilized as pseudo labels. Conceptually, once tokenized, both the pseudo label prompt  and the generated prompt  can be represented as lists of tokens:  and .\nFurthermore, a novel loss function is introduced to actively guide  in generating high-quality prompts. The reciprocal of the associated prompt entropy is employed as the weight for the standard cross-entropy loss during training. In essence, this means that prompts with higher information entropy receive lower weights, resulting in smaller losses. This strategic approach accelerates model convergence and encourages a preference towards high-quality prompts during the training.\nConsequently, the loss function underpinning our prompt generation process is formulated as:\nThis concludes the generation stage. After training, the model  can be used to generate prompts from the initial template and the generated prompt is exemplified in Table 2  ###reference_### (the V1 part)."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5. Prompt Refinement",
            "text": "Building upon the above generation stage, wherein  is trained to generate prompts, the immediate prompts resulting from this stage are referred to as V1 prompts. In this section, our attention shifts to the refinement of the V1 prompts, with the aim of uncovering additional prompt variations."
        },
        {
            "section_id": "3.5.1",
            "parent_section_id": "3.5",
            "section_name": "3.5.1. Prompt Noise Reduction",
            "text": "The V1 prompts are designed to furnish an expanded array of information and finer-grained mobility data (e.g., including details such as hourly visit counts). However, the adoption of this strategy inevitably introduces a certain degree of noise into the generated prompts. This noise might manifest as extra characters or spacers placed between numbers.\nBased on the V1 prompts, we introduce a subsequent iteration denoted as V2 prompts. The essence of V2 lies in its adoption of an information integration strategy that seeks to mitigate noise while retaining the core semantic content inherent in the V1 prompts.\nSpecifically, considering the overarching forecasting target as the aggregate number of visits for the subsequent day  (same as the problem formulation in previous work (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###), V2 prompts reorganize human mobility data based on two distinct time periods. These divisions delineate the first and second halves of the day, effectively partitioning working hours at 12 PM, i.e., the diurnal partitioning.\nThis operational adjustment yields an updated set of prompts represented in the dedicated V2 section of Table 2  ###reference_###.\nImportantly, this refinement updates the prompt , which further leads to a corresponding modification of  (the prompt of the forecasting target)."
        },
        {
            "section_id": "3.5.2",
            "parent_section_id": "3.5",
            "section_name": "3.5.2. Chain of Thought Integration",
            "text": "Inspired by the recent strides made in harnessing chain of thought applications within language models for enhancing reasoning capabilities (Wei et al., 2022  ###reference_b23###), we extend our framework’s refinement phase to incorporate a Chain of Thought (CoT) integration.\nTo this end, we propose the integration of a model  dedicated to generating a chain of thought, using the V2 prompts as input. In training or fine-tuning , we establish an initial format of the chain of thought as labels, effectively instructing  in its generation task.\nOur devised CoT structure is conceptually grounded in the V2 prompts. This structure imparts the model with the understanding that the sum of mobility data during the first and second halves of working hours equates to the total daily human mobility count.\nIt is important to note that, distinct from ’s training,  is trained using the standard cross-entropy loss, without employing prompt entropy. As depicted in Figure 2  ###reference_### (the green parts), the trained  generates a CoT, which is subsequently appended to the V2 prompts, leading to the formulation of the V3 prompts.\nTable 2  ###reference_### presents an illustrative example of V3 prompts.\nFrom the example, we can also notice that the  remains the same from V2 to V3 as the CoT is appended as the input of the language models when performing forecasting tasks.\nThe sentences highlighted in green represent the CoT generated by . V3 prompts effectively merge human mobility data, eliminate redundancy, and establish a coherent chain of thought characterized by a fixed logical sequence of prompt sentences (i.e., the sentences in V2).\nThis strategy augments the richness of valid information while concurrently attenuating potential noise. By introducing this CoT integration strategy, we enhance the sophistication of our prompt mining framework."
        },
        {
            "section_id": "3.5.3",
            "parent_section_id": "3.5",
            "section_name": "3.5.3. Segmentation with Information Gain",
            "text": "While the diurnal partitioning approach adopted in V2 and V3 effectively divides daily human mobility into two segments, this method may not fully capture the intricate patterns inherent in mobility behaviours. Such an approach might inadvertently result in the loss of crucial information, such as the peaks of the lunchtime rush around 12 PM.\nIn this phase of refinement, the focal challenge revolves around the effective segmentation of the given time series while minimizing the introduction of extraneous noise. The objective is to uncover a segmentation strategy that optimally captures temporal transitions in human activities and daily routines, ensuring accuracy while maintaining coherence.\nTo address this challenge, we draw upon an Information Gain-based Temporal Segmentation (IGTS) method (Sadri et al., 2017  ###reference_b18###)), which is an unsupervised segmentation technique to find the transition times in human activities and daily routines.\nIn the pursuit of refined prompts (denoted as V4 prompts), IGTS is employed.\nAssuming that the mobility series (e.g., the numerical numbers) in one day is a list of  and the goal is to divide it into  segments , with IGTS, this can be achieved by minimizing the information gain-based lost function :\nwhere  is the length of the -th segment (i.e., how many numbers) and  is the total length of data to be segmented.\nThis approach contrasts with the binary halves used in diurnal partitioning within V2 and V3. As showcased in Table 2  ###reference_###, the updated V4 prompts support multiple segments.\nConsidering the revisions to the segmentation in V4, both the CoT within  and the  are also modified accordingly."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "3.6. Summary of Designed Prompt Variants",
            "text": "To provide a comprehensive comparative perspective on the four distinct prompt variants discovered through our prompt mining methodology, as illustrated in Table 2  ###reference_###, we present a summary of the key properties of 4 prompt variants:\n(1) V1: Generated directly by  which is trained with the prompt quality evaluator.\n(2) V2: Explores diurnal partitioning for segmentation based on V1, focusing on the first and second halves of the day, which aims to reduce noise while preserving semantic content.\n(3) V3: Integrates chain of thought (CoT) in  based on V2.\n(4) V4: Leverages Information Gain-based Temporal Segmentation (IGTS) to support detailed temporal segmentation beyond the diurnal partitioning in V2 and V3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Source Data",
            "text": "To validate our prompt mining approach for language-based mobility forecasting, we follow the previous language-based human mobility forecasting studies (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###) and select SafeGraph111https://www.safegraph.com/ mobility pattern data accessed through Dewey research data platform222https://www.deweydata.io/ for our experiments.\nWe collect a total of 562,151 rows of raw weekly data of US POIs from 172 brand types.\nThe temporal span of the data ranges from 26th December 2022 to 2nd January 2023.\nEach row provides one week of visiting data (including both daily counting and hourly counting) of one POI.\nAuxiliary information such as the region of POI and the opening/closing hours are also available in the raw data."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Implementation Details",
            "text": "In our experiments, the implementation includes two phases: the prompt mining phase and the forecasting phase.\nFor the prompt mining phase,\nwe opted for the GPT-2 model (Radford et al., 2019  ###reference_b17###) architecture for both  and , despite the emergence of more advanced models like GPT-4 (OpenAI, 2023  ###reference_b15###) and Llama 2 (Touvron et al., 2023  ###reference_b21###). The choice was driven by accessibility and computational feasibility considerations, as the latter models pose challenges in terms of availability and resource requirements.\nThe maximum length of the prompt generated by the GPT-2 model is limited to 512 tokens.\n20% of the data was allocated and passed to the templates given in the heuristic pool for the training of  and .\nThe batch size and dropout rate are set as 5 and 0.1, respectively. A learning rate initially is set at  and early-stopping is adopted with a patience 3 epoch to avoid overfitting.\nFor the prompt quality evaluator, entropy analysis revealed that most generated prompts exhibited entropy values between 2 and 5. Setting a threshold  at 3.5 enabled effective classification of high-quality and low-quality prompts based on their information entropy.\nAs for the classifier in the evaluator, given that the task is binary classification, a simple yet effective logistic regression classifier is selected.\nIt uses L2 regularization and sets the reciprocal of the regularization strength as 1, which means that the strength is inversely proportional to the regularization. The model is fitted with the quasi-Newton as the optimization solver and a maximum iteration limit of 100.\nIn the forecasting phase, the total data is split into a training set (70%, for training numerical forecasting methods and fine-tuning language models), a validation set (10%), and a testing set (20%).\nThe raw data is transformed into sentences by the different variants of developed prompts to fine-tune language models for forecasting purposes.\nThe fine-tuning process of the language forecasting models utilizes the standard Trainer provided by HuggingFace.\nNo changes are made to the loss function or other aspects during the fine-tuning of language models in this phase.\nFor the forecasting setting, we observe the mobility data of the last 3 days and predict the mobility of the next day. From the hourly data perspective, the input length is 72 hours and the forecasting horizon is 24 hours.\nAll of the experiments are performed with PyTorch on a Linux server equipped with Nvidia V100 GPUs and only one GPU was utilized."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Evaluation",
            "text": "The overall evaluation of different prompts is conducted based on the forecasting performance.\nFor language-based forecasting, following previous work (Xue et al., 2022a  ###reference_b29###, b  ###reference_b30###), we first need to extract the predicted numerical values from generated output sentences through string parsing.\nAfter extracting the predicted numerical values from the generated sentences, the evaluation of the proposed method can be carried out similarly to how traditional numerical-based forecasting methods are evaluated.\nWe consider two error measures: the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE)."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Baselines",
            "text": "In this subsection, we present the methods used for comparison in our energy load forecasting experiments. We categorize the baselines into two categories: numerical forecasting methods and language models.\nThe numerical baselines include the classic ARIMA, deep learning-based (Prophet (Taylor and Letham, 2018  ###reference_b20###), TimesNet (Wu et al., 2022  ###reference_b24###), LightTS (Zhang et al., 2022  ###reference_b35###), DLinear (Zeng et al., 2023  ###reference_b33###)), and Transformer (Vaswani et al., 2017  ###reference_b22###)-based models ( Reformer (Kitaev et al., 2019  ###reference_b8###), Informer (Zhou et al., 2021  ###reference_b37###), Autoformer (Xu et al., 2021  ###reference_b25###), Pyraformer (Liu et al., 2021  ###reference_b12###), Crossformer (Zhang and Yan, 2022  ###reference_b36###), PatchTST (Nie et al., 2022  ###reference_b14###)). Except for ARIMA and Prophet, all the rest of the numerical methods are based on the implementation provided by the Time Series Library (TSlib)333https://github.com/thuml/Time-Series-Library  ###reference_ry###..\nFor the language models, we select three language models with pre-trained weights from HuggingFace: Bart (Lewis et al., 2020  ###reference_b9###), Bigbird (Zaheer et al., 2020  ###reference_b32###), and Pegasus (Zhang et al., 2020  ###reference_b34###).\nFrom a recent benchmark study (Xue and Salim, 2023  ###reference_b28###), these 3 models have shown good performance in forecasting time series compared to other language models."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5. Comparison against Baselines",
            "text": "In this part of the experiment, we focus on investigating the performance of our mined V1 prompts alongside various baselines.\nThe results of these evaluations are presented in Table 3  ###reference_###.\nFor the numerical forecasting baselines, the inputs are the raw hourly visiting data in numbers.\nFor the language models, we also compare V1 against a basic prompt template (e.g., This POI is [STORE TYPE]. There were [HOURLY VISITING DATA] people came here to visit.) drawn from the previous language-based forecasting study (Xue and Salim, 2023  ###reference_b28###).\nAll methods reported in Table 3  ###reference_### predict hourly forecasting during the entire working day.\nIn addition to the daily forecasting (the sum of hourly predictions) performance, to make a more accurate comparison with the subsequent generated prompt V2 and V3 (where the diurnal partitioning is applied) later, we also report RMSE and MAE within the first and second-half of the working time. It allows better measurement of their predictive performance over different time periods.\nThe best and the second-best performers under each column are shown in red and blue.\nIn general, our V1 outperforms both numerical forecasting methods and the basic prompt without prompt mining process.\nWhile numerical models show competitive results in the second half, the Pegasus model with our V1 prompts consistently maintains a superior performance across different forecasting scenarios.\nFor the same language model, we can see that using V1 leads to better forecasting performance than using the basic prompt.\nMoreover, for language models, regardless of the prompt employed, both Pegasus and Bigbird showcase superior forecasting performance compared to Bart.\nThe above comparison results validate the\neffectiveness of our proposed prompt mining method that can contribute to more accurate forecasting."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "4.6. Performance of Different Prompt Variants",
            "text": "To have a deeper understanding of our prompt mining approach and its impact on forecasting accuracy, we conduct a detailed analysis of the performance of different prompt variants.\nTable 4  ###reference_### presents the forecasting performance of the three language models using Prompt V1-V3.\nIn general, we can observe that V3 outperforms V1 and V2 in terms of the daily forecasting results no matter which model is used.\nFor the Bart model, V2 exhibits substantial improvements in accuracy compared to V1, with significant reductions in both RMSE and MAE. V3 also has a large reduction in daily forecast RMSE.\nIn the Bigbird case, V2 and V3 display a remarkable improvement against V1 for the first half day. Although V1 has a better performance in the second half day, V3 still remains the top performer under the daily forecast.\nSimilarly, the outcomes observed for the Pegasus language model are almost the same as the trends apparent in the Bigbird analysis.\nThis analysis of different prompt variants shows the efficacy of our mining method in enhancing mobility forecasting accuracy. The prompt refinements introduced in the prompt mining process, particularly for V3, contribute to improvements in the forecasting performance, which justifies the significance of our approach’s prompt refinement phase."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "4.7. Impact of Segments",
            "text": "###table_3### In this part of the experiment, we explore the impact of different segmenting mechanisms used in our prompt refinement stage.\nSpecifically, we investigate how different numbers of segments () in our V4 prompts influence forecasting performance.\nThe results of this experimentation are summarized in Table 5  ###reference_###, where we also provide the forecasting performance of V3 for easy comparison, which employs diurnal partitioning with 2 segments.\nDifferent from Table 3  ###reference_### and Table 4  ###reference_###, we calculate the average RMSE/MAE of each segment instead of the first/second half RMSE/MAE since we have settings with more than 2 segments for V4.\nWe also highlight the best and second-best results for each language model separately.\nUpon analyzing the results, it is evident that the performance of our V4 prompts is influenced by the number of segments .\nFor example, in the case of segment average performance, increasing the number of segments from 2 to 5 (V4) leads to a decreased error for both Bigbird and Pegasus.\nFor the daily forecasting performance, a similar trend can also be noticed for the Bart and Pegasus models.\nMoreover, the comparison with V3 prompts highlights that our refined V4 prompts generally perform favorably.\nIn conclusion, the results indicate that with a dynamic segmentation strategy offered in our V4 prompts, the language-based forecasting methods can be further improved. Additionally, we observe similar performance on different language models, which demonstrates the adaptability of our prompt mining approach."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusion",
            "text": "In this study, we present a novel prompt mining framework for enhancing language-based mobility forecasting.\nWe introduce a multi-stage process with a core prompt generation stage based on prompt entropy and a refinement stage with the consideration of a chain of thought.\nOur mining pipeline generates and refines prompts for transforming human mobility data into sentences in order to leverage language models for mobility forecasting.\nThrough comprehensive experiments, we demonstrate the superiority of our refined prompts over traditional numerical forecasting methods and basic prompt structures. Our results also highlight the consistent performance improvement across different language models used for the forecasting phase.\nThe future directions of mobility prompt mining can include the integration of external data sources, such as weather and events, which could further provide more context and enhance forecasting capabilities. How to involve human feedback from domain experts in the prompt mining process would be another interesting future research topic."
        }
    ]
}