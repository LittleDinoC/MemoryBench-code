{
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.\n††∗Equal contribution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, Large Language Models (LLMs) have demonstrated impressive performance in zero-shot textual tasks. Specifically, recent literature has devised models capable of tackling diverse tasks, as instructed by the user [30  ###reference_b30###, 6  ###reference_b6###, 41  ###reference_b41###]. In this context, the classical approach is that of fine-tuning a model on varied tasks that are described through natural language [34  ###reference_b34###, 7  ###reference_b7###], thus empowering the model to assimilate externally provided instructions and facilitating robust generalization across multiple domains. Following these advancements, the computer vision community has started to investigate the extension of such models to vision-and-language contexts, thus generating Multimodal Large Language Models (MLLMs). On this line, the fusion of visual features into LLM backbones through vision-to-language adapters [23  ###reference_b23###, 21  ###reference_b21###, 1  ###reference_b1###, 48  ###reference_b48###] has induced notable performance improvements, enabling extensive generalization to vision-and-language tasks requiring elaborate visual descriptions.\n###figure_1### In this context, MLLMs excel by simply including a small module (i.e., an adapter) that aligns visual features with textual ones. However, despite these models being built upon LLMs trained on large-scale data, they exhibit notable limitations when confronted with highly specific user queries or when a certain degree of compositional reasoning is required to formulate the response. Moreover, certain knowledge proves itself challenging to be encoded within the parameters of an MLLM, due to the scarcity of long-tail information in the training data. In response to this challenge, different benchmarks have been recently introduced for evaluating the capabilities of MLLM to tackle queries related to external data, such as InfoSeek [5  ###reference_b5###] and Encyclopedic-VQA [28  ###reference_b28###]. While different works [32  ###reference_b32###, 20  ###reference_b20###, 8  ###reference_b8###, 21  ###reference_b21###] have been testing on these benchmarks, underscoring the significance of this area, none of them has developed architectures specifically designed for tackling external knowledge.\nDriving from these considerations, in this paper we propose the first MLLM augmented with a retrieval module, thus shifting the focus towards teaching the model to leverage diverse information in its responses and learning to discern the relative importance of each. In particular, our model retrieves appropriate information from an external knowledge base of documents and employs a hierarchical retrieval approach to identify relevant passages. This additional knowledge is then fed to an MLLM, without changing its structure but improving its answering capabilities. To the best of our knowledge, our work represents the first MLLM to harness the retrieval capability of external sources. We assess the quality of the proposed approach by conducting extensive experiments and comparisons with respect to recent MLLMs [24  ###reference_b24###, 8  ###reference_b8###, 21  ###reference_b21###] and by showcasing the effectiveness of our design choices. Experimental results demonstrate the advantage of retrieving from external sources and the appropriateness of our model design. Overall, we conceive our work as a first step in the direction of retrieval-augmented MLLMs, which could foster future works in the same area."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Multimodal LLMs.\nLLMs have significantly reshaped the landscape of AI research and applications, spearheaded by notable examples like OpenAI’s ChatGPT and GPT-4. These models leverage alignment techniques such as instruction tuning [30  ###reference_b30###] and reinforcement learning from human feedback [39  ###reference_b39###] and achieve remarkable capabilities in language understanding and reasoning. Open-source LLMs like Flan-T5 [7  ###reference_b7###], Vicuna [6  ###reference_b6###], LLaMA [41  ###reference_b41###], and Alpaca [40  ###reference_b40###] have further accelerated the advancement within the research community. This surge in the development of LLMs subsequently led to the emergence of MLLMs [3  ###reference_b3###], which can combine the understating of visual inputs with natural language generation.\nEarly attempts of building MLLMs such as VisualGPT [4  ###reference_b4###] and Frozen [42  ###reference_b42###] used pre-trained language models to enhance vision-and-language models specifically for tasks like image captioning and visual question answering. This initial investigation paved the way for subsequent research in this domain, with the introduction of solutions such as Flamingo [1  ###reference_b1###] or BLIP-2 [21  ###reference_b21###] which allowed the integration of image features into LLMs respectively through trainable cross-attention layers directly within the LLM or Q-Former blocks that instead combine image and textual features via learnable queries. Building upon these advancements, subsequent models like FROMAGe [19  ###reference_b19###], Kosmos-1 [14  ###reference_b14###], and MiniGPT-4 [48  ###reference_b48###] have been introduced to further refine the interplay between visual and language modalities within the LLM architecture.\nConcurrently, the LLaVA family of models [24  ###reference_b24###, 23  ###reference_b23###, 25  ###reference_b25###] introduced the usage of instruction tuning in the multimodal domain, by training on a curated dataset collected with GPT-4. This strategy is now among the most promising recipes for building MLLMs.\nRetrieval-augmented language models.\nIn recent years, retrieval-augmentation has been applied to language models by expanding their input space with relevant text passages extracted from external sources [10  ###reference_b10###] or eventually retrieved directly from the web [29  ###reference_b29###]. These techniques have demonstrated large improvements in knowledge-intensive tasks and significant savings in terms of model size.\nTraditionally, the integration of external knowledge into textual generation has been confined to the initial stages. Different solutions [17  ###reference_b17###] proposed to adaptively retrieve passages for generation on top of a proprietary LLM. Some works [10  ###reference_b10###], instead, focused on capturing knowledge in a more modular and interpretable way, by augmenting the language model pre-training with a latent knowledge retriever. This allows the model to retrieve and attend documents taken from a large corpus such as Wikipedia. While much attention has been directed towards textual augmentation, similar research efforts have recently been dedicated in the context of vision-and-language tasks [2  ###reference_b2###, 37  ###reference_b37###, 13  ###reference_b13###, 31  ###reference_b31###]. Following this direction, the work presented in [13  ###reference_b13###] proposed a retrieval-augmented visual-language model that encodes world knowledge into a large-scale memory. Other approaches [36  ###reference_b36###, 35  ###reference_b35###] also apply retrieval to specific downstream tasks such as image captioning. Differently from all the aforementioned approaches, our work is the first to apply retrieval-augmentation to MLLMs. We do this by applying a hierarchical retrieval strategy on top of a knowledge base made of multimodal documents.\n\nKnowledge-based visual question answering.\nRecently, the emergence of new benchmarks like Encyclopedic-VQA [28  ###reference_b28###] and InfoSeek [5  ###reference_b5###] has raised the difficulty of standard knowledge-based VQA [27  ###reference_b27###, 16  ###reference_b16###, 38  ###reference_b38###] with questions that require intensive knowledge about specific entities, such that even LLM-based models perform poorly without retrieving information from external sources. Often, contrastive image-text encoders are employed to retrieve the target entity given the query image [44  ###reference_b44###, 46  ###reference_b46###]. Then, the entity name is used as a key to access an external knowledge base, which is typically composed of several text passages that encompass the correct answer. In this work, we"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Method",
            "text": "Our goal is to equip Multimodal LLMs (MLLMs) with the ability to answer complex and specific questions that cannot be addressed solely through the image content and pre-trained knowledge. To achieve this, we propose Wiki-LLaVA, which integrates external knowledge derived from an external memory into the LLaVA model, without significantly altering its design. Instead, we augment the capabilities of the model by incorporating retrieval information as additional input context.\nOverall, Wiki-LLaVA comprises three components, as shown in Fig. 2  ###reference_###: a visual encoder, which is employed to provide the MLLM with visual context and as a query to retrieve from an external knowledge base, the knowledge base itself (e.g., Wikipedia), and a hierarchical retrieval module which retrieves relevant documents and passages from the external knowledge base, to be employed as additional context for the MLLM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Knowledge-based Augmentation",
            "text": "Multimodal integration and autoregressive generation.\nAn MLLM usually takes as input a multimodal input query, comprising both image and text, and generates a textual output in an autoregressive manner. Formally, the architecture is trained to model a probability distribution , where  denotes the parameters of the model,  represents an input image, and  denotes the textual prompt. The textual prompt usually includes a pre-defined system-level prompt and a question related to the input image, given by the user. Clearly, a standard MLLM can only rely on the user prompt, the input image, and the knowledge stored in its internal parameters (i.e., ) to accommodate requests, thus limiting its ability to answer questions that rely on external knowledge.\nIn the rest of the paper, we employ LLaVA [24  ###reference_b24###] as our reference MLLM. LLaVA exploits the capabilities of a pre-trained LLM (i.e., Vicuna [6  ###reference_b6###]) and a pre-trained visual model (i.e., a CLIP-based visual encoder [33  ###reference_b33###]), which are interconnected through an MLP adapter, in charge of converting CLIP features to dense input tokens. For an input image , therefore, LLaVA utilizes a pre-trained CLIP visual encoder , extracts a dense grid of visual features , which is then projected via a learnable MLP to produce a sequence of dense embedding tokens . Finally, these are prepended to the system prompt, and the full sequence of visual and textual tokens is then given as input to the LLM component of the model.\nAugmentation with external knowledge.\nTo augment the MLLM with external knowledge, we enrich the input context by injecting relevant textual data from an external memory composed of documents. Formally, the distribution of the MLLM is conditioned on additional textual retrieval-knowledge tokens, leading to\nwhere  represents the added tokens retrieved from the external memory. Differently from the standard formulation of MLLMs, by enriching the input context we allow the model to generate more specific answers by exploiting tokens retrieved from the memory.\nHierarchical retrieval from an external memory.\nThe external memory comprises a collection of (document, image, text-title) triplets taken from documents, denoted as . Within this memory, we conduct a hierarchical two-step search to retrieve appropriate information. Initially, we locate the most pertinent document, followed by identifying the relevant passage inside a particular document, which is subsequently exploited as additional input context in the MLLM.\nIn the first stage, given an input query image  we perform an approximate -nearest neighbor search into the external memory, using document titles as retrievable keys. The similarity between the query image and the text titles is modeled as the inner product between their respective embeddings, which are computed through the visual and textual CLIP encoders (i.e.,  and ), as follows:\nThen, the knowledge retriever returns the top- documents associated with the most relevant items retrieved using the aforementioned procedure.\nRetrieving document passages.\nIn the second step, we analyze each of the retrieved documents to identify the most relevant passages corresponding to the user’s question.\nEach document is defined as a sequence of chunks, denoted as , and, given the input question, we retrieve the chunks with the highest similarity to the question. We employ the Contriever architecture [15  ###reference_b15###] to embed each chunk of the selected document, along with the query (i.e., the question provided by the user), and compute the similarity as an inner product between embeddings. By retrieving the  most appropriate passages inside each of the retrieved documents, overall we obtain  passages.\nContext enrichment.\nOnce we find the most relevant chunks, we employ their raw contents as an additional input to the MLLM. Specifically, the final prompt that we employ includes the image tokens, the retrieved raw chunks, the system-level prompt, and the user question. Formally, considering three retrieved passages, the final prompt is defined as follows:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "While the aforementioned approach could work in a zero-shot fashion, using the original weights  of the pre-trained MLLM, we also investigate the case of fine-tuning the model to augment its capabilities of exploiting retrieved passages. In particular, in this case, the model is trained on pairs of questions and ground-truth answers requiring external knowledge. As this would potentially reduce the capabilities of the MLLM on tasks not requiring external knowledge (i.e., all the other tasks on which the model has been originally trained), we apply a data mixing approach in which ground-truth pairs requiring external knowledge are mixed with ground-truth pairs not requiring external knowledge in the same mini-batch."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first introduce the experimental settings, describing the datasets employed, the evaluation protocol, and the implementation and training details used to perform the experiments. Then, we present our experimental results, analyzing the effectiveness of CLIP fine-tuning and evaluating how it is possible to incorporate retrieved knowledge in an MLLM. Finally, limitations of the proposed approach and possible future works are reported."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Encyclopedic-VQA [28  ###reference_b28###].\nThe dataset contains around 221k question-answer pairs associated with 16.7k different fine-grained entities, with up to 5 images representing the same entity. Overall, there are more than 1M triplets composed of an image, a question, and the corresponding answer. Fine-grained entities and related images are extracted from iNaturalist 2021 [43  ###reference_b43###] and Google Landmarks Dataset V2 [45  ###reference_b45###], which are associated with the corresponding Wikipedia article. Questions are divided into four different categories, namely single-hop, automatically generated, multi-answer, and two-hop. In particular, single-hop questions have been manually annotated and a single Wikipedia article is needed to answer them. Automatically generated questions are similar to the single-hop questions but have been generated by automatic models. Multi-answer questions, instead, can be answered with a list of terms, but always refer to a single fine-grained entity. Finally, two-hop questions require two retrieval steps to answer them. The dataset also comes with a knowledge base composed of 2M Wikipedia articles, suitable for answering dataset questions.\nDataset triplets are divided into training, validation, and test splits respectively composed of 1M, 13.6k, and 5.8k samples. In our experiments, we employ the training split to fine-tune the LLaVA model and report the results on the test set of the dataset. During testing, we filter out two-hop questions resulting in 4,750 test triplets.\nInfoSeek [5  ###reference_b5###]. The dataset contains 1.3M image-question-answer triplets corresponding to around 11k different entities (i.e., Wikipedia articles). The vast majority of questions have been obtained with an almost entirely automatic procedure, by filling human-authored templates with knowledge triples from Wikidata. In this case, images are derived from the OVEN dataset [12  ###reference_b12###]. Triplets are divided into training, validation, and test sets, with around 934k, 73k, and 348k samples respectively. At the time of the submission, the ground-truth answers and entities from the test set were not available. Therefore, we report our results on the validation split. Both validation and test sets contain questions related to new entities not included in the training split and questions not seen during training.\nAlong with image-question-answer triplets, a knowledge base composed of 6M Wikipedia entities is provided. In our experiments, we consider a randomly extracted subset of 100k entities, in which we guarantee the presence of the 6,741 entities associated with questions from the training and validation splits."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "LLaVA fine-tuning. We employ two distinct fine-tuning approaches, with each being exclusively applied to one of the datasets. In order to maintain the performance of the LLaVA model on well-established MLLM datasets, we supplement fine-tuning data with samples from the LLaVA-Instruct dataset [24  ###reference_b24###]. Specifically, given its size of 158k, we double the probability of having examples from this dataset in each mini-batch. To reduce the number of trainable parameters, we train using low-rank adapters [11  ###reference_b11###] with a total batch size of 512 samples.\nRetrieval. Textual documents sourced from Wikipedia content are embedded using the Contriever architecture [15  ###reference_b15###], segmenting the text into chunks of 600 characters each. Furthermore, for streamlined efficiency, the process involves utilizing a single visual encoder. Specifically, following the LLaVA architecture [24  ###reference_b24###], we employ the CLIP ViT-L/14@336 backbone to embed images to give as input to the MLLM, while simultaneously leveraging it to extract query visual features in the initial hierarchical retrieval step, facilitating the integration of an external memory component.\nTo perform entity retrieval, we employ approximate kNN search rather than exact kNN search because it significantly improves\nthe computational speed of the entire pipeline. To this aim, we employ the Faiss library [18  ###reference_b18###] and a graph-based HNSW index with 32 links per vertex."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Protocol",
            "text": "We evaluate our models in two settings: without external knowledge base and with external knowledge base. The former means that we ask the model to directly answer a visual question, by solely relying on the competencies learned during pre-training and/or fine-tuning. On the other hand, in the latter setting, we leverage the proposed hierarchical retrieval method to search for additional information in the external knowledge base. In practice, this is represented by two dumps of Wikipedia comprehending 2M and 100k pages, respectively for Encyclopedic-VQA and InfoSeek. Concerning the evaluation metrics, we report the accuracy over the Encyclopedic-VQA test split and the InfoSeek validation split, following the official evaluation scripts provided along with the datasets."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Analyzing CLIP performance. We start by evaluating entity retrieval results using CLIP. In this setting, we consider images from the Encyclopedic-VQA test set and InfoSeek validation set and measure the CLIP ability to find the correct entity within the knowledge base of each respective dataset (i.e., composed of 2M entries for Encyclopedic-VQA and 100k entries for InfoSeek). As previously mentioned, we perform retrieval using images as queries and Wikipedia titles as retrievable items.\nResults are reported in Table 1  ###reference_### in terms of recall@ (R@) with  which measures the percentage of times the correct entity is found in the top- retrieved elements. Notably, correctly retrieving the Wikipedia entity associated with the input image strongly depends on the size of the employed knowledge base. In fact, when using 100k items, as in the case of InfoSeek, the correct entity is retrieved as the first item 36.9% of the time and among the top-10 66.1% of the time. Instead, when using a significantly larger knowledge base as in the case of Encyclopedic-VQA, which contains 2M items, retrieval results are significantly lower with 3.3% and 9.9% respectively in terms of R@1 and R@10.\nResults on Encyclopedic-VQA and InfoSeek.\nWe then report visual question-answering results in Table 2  ###reference_###. We include the performance of zero-shot models like BLIP-2 [21  ###reference_b21###], InstructBLIP [8  ###reference_b8###], and the LLaVA-1.5 baseline model [24  ###reference_b24###], which are not fine-tuned on the considered datasets and that do not leverage the external knowledge base. Moreover, we consider the accuracy results of LLaVA-1.5 when fine-tuned on the training set of Encyclopedic-VQA and InfoSeek, but not augmented with retrieved context. The results of our approach (i.e., Wiki-LLaVA) are reported both in the standard setting in which CLIP is used to retrieve the most representative entity from the knowledge base and in its oracle version, which employs the entity corresponding to the input image-question pair. For both cases, we consider a different number  of retrieved textual chunks, all corresponding to the top-1 (or ground-truth) entity. When employing CLIP, we also vary the number  of retrieved entities (i.e., ) using  when  is greater than 1. This choice is given by the maximum context length that Vicuna takes as input, which is set to 2,048 tokens.\nAs it can be seen, zero-shot MLLMs face difficulties in correctly answering the given questions as these models can only rely on the knowledge embedded inside the LLM. When instead using an external knowledge base, the accuracy results significantly increase especially on the InfoSeek dataset with 100k retrievable items. The limited performance of the CLIP model in retrieving the correct entity on larger knowledge bases, instead, leads to a slight degradation of accuracy scores. This is due to the noisy textual passages that are provided to the MLLM as additional external context which, being related to a different entity, often do not contain informative content.\nOverall, retrieving passages from different entities does not always help increase the results. Instead, using more than one textual chunk as additional context for the MLLM generally improves the final accuracy on the InfoSeek validation set with an overall improvement of 2.1 and 3.4 accuracy points with  and  respectively. Furthermore, it is worth noting that employing oracle entities significantly boosts the final accuracy. In particular, oracle entities lead to an improvement of 13.8% on Encyclopedic-VQA and 22.6% on InfoSeek, comparing the best-performing configuration with CLIP-based entity retrieval (i.e.,  and  for Encyclopedic-VQA and  and  for InfoSeek) with the best performing oracle-based version (i.e.,  and  for Encyclopedic-VQA and  and  for InfoSeek). These results confirm the effectiveness of directly employing retrieved passages to augment a pre-trained MLLM and further highlight the importance of having a good entity retrieval model to limit the possibility of feeding the MLLM with irrelevant content.\nSome qualitative results on sample image-question pairs from Encyclopedic-VQA (first row) and InfoSeek (second row) are reported in Fig. 3  ###reference_###, comparing the answers given by Wiki-LLaVA with those coming from the original LLaVA-1.5 model. For completeness, we also report some failure cases (third row) in which both models are not able to correctly answer the given question.\n###figure_3### In what state is this building located? \nLLaVA-1.5:\nCalifornia ✗\nWiki-LLaVA:\nWashington ✓\n###figure_4### When was this building constructed? \nLLaVA-1.5:\n1970 ✗\nWiki-LLaVA:\n1927 ✓\n###figure_5### What’s the height of the tallest minaret from this mosque? \nLLaVA-1.5:\n100 feet ✗\nWiki-LLaVA:\n49mt ✓\n###figure_6### Which geographic area is this fish found? \nLLaVA-1.5:\nGulf of Mexico ✗\nWiki-LLaVA:\nBrazil ✓\n###figure_7### What is the oldest age of this animal? \nLLaVA-1.5:\n10 years ✗\nWiki-LLaVA:\n24.9 ✓\n###figure_8### Who designed this building? \nLLaVA-1.5:\nArchitect ✗\nWiki-LLaVA:\nJames of Saint George ✓\n###figure_9### Which culture is associated with this place?\nAncient Greek\nLLaVA-1.5:\nRoman ✗\nWiki-LLaVA:\nNuragic Civilization ✗\n###figure_10### What is the name of the main club of this stadium? \nFC Rotor\nLLaVA-1.5:\nReal Madrid ✗\nWiki-LLaVA:\nFC Dynamo Kyiv ✗\n###figure_11### Which mountain range is this mountain belong to?\nSnowdonia\nLLaVA-1.5:\nRocky mountains ✗\nWiki-LLaVA:\nLake District ✗\nEvaluating the importance of the fine-tuning datasets.\nAs described in Sec. 3.2  ###reference_### and Sec. 4.2  ###reference_###, the MLLM fine-tuning is done with a mixture of data containing image-question-answer triples from the Encyclopedic-VQA or InfoSeek training set and visual instruction tuning data from LLaVA-Instruct [24  ###reference_b24###], which has been used to originally fine-tune the LLaVA model. In Table 3  ###reference_###, we evaluate the effect of mixing fine-tuning data for the knowledge-based VQA task. In this setting, we only report the results of the fine-tuned models without external knowledge retrieval. Notably, using visual instruction tuning data can help to regularize the fine-tuning phase on the InfoSeek dataset, leading to an overall improvement of 1.9 accuracy points compared to the model fine-tuned only on image-question-answer triplets from the training set of the dataset. On Encyclopedic-VQA, instead, training with instruction tuning data does not lead to performance improvement although without degrading the original results.\nPreservation of LLaVA performance.\nFinally, we analyze the impact of LLaVA fine-tuning on knowledge-based VQA datasets when evaluating the model on common MLLM evaluation benchmarks [3  ###reference_b3###]. In particular, we include results on MME [9  ###reference_b9###] which contains image-question pairs covering 14 different tasks grouped in two macro-categories (i.e., cognition and perception), MMMU [47  ###reference_b47###] that is composed of multiple-choice and open-ended questions possibly interleaved with one or more images and extracted from diverse university textbooks and online courses, MMBench (MMB) [26  ###reference_b26###] that includes multiple-choice questions across 20 different domains, and POPE [22  ###reference_b22###] that is focused on evaluating object hallucinations and comprises binary classification entries, each related to an image. More details about the evaluation metrics and number of samples can be found in the original paper of each dataset.\nResults are shown in Table 4  ###reference_### comparing the original LLaVA model with the two fine-tuned versions on Encyclopedic-VQA and InfoSeek, with and without the use of visual instruction tuning data. Overall, employing samples from the LLaVA-Instruct dataset can better preserve the results of the original model, only partially degrading the performance on the considered benchmarks compared to the original model. While the most significant deterioration is achieved on the MME dataset, in the other settings the original performances are better preserved, also leading to a slight improvement on MMMU and POPE benchmarks compared to the LLaVA-1.5 results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Limitations and Future Works",
            "text": "While our work provides an initial step towards MLLM which can properly exploit external multimodal data, it is worthwhile mentioning that significant research is needed in two directions. The fist is defining proper embedding spaces in which documents can be retrieved from questions and input images, so as to improve the performance of the higher level of our hierarchical retrieval. The second is modeling an efficient and sustainable paradigm to select from one or more documents. Here, the challenge is to\nincrease the capability of the MLLM of distinguishing the appropriateness of retrieved items. This point might also require novel architectural design, which might go beyond the pure inclusion of retrieved items in the context.\nRegardless of its current limitations, our research testifies the potential of adding multimodal external knowledge to a MLLM and inherits all the advantages of retrieval-augmented approaches, such as the adaptability to different domains and the loosely-coupled relationship between pre-trained information and retrievable data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have presented Wiki-LLaVA, an architecture for augmenting an existing MLLM with external knowledge. Our proposal leverages an external knowledge source of documents to improve the effectiveness of an MLLM when tasked with questions and dialogues. In particular, we devise a hierarchical architecture for retrieving documents and eliciting selected parts to be included in the MLLM input context. Extensive experiments demonstrate the effectiveness of the proposed solution, and its capability to maintain the proficiency of the MLLM across different tasks."
        }
    ]
}