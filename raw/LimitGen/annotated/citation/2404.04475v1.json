{
    "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
    "abstract": "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation.\nHowever, these auto-annotators can introduce complex biases that are hard to remove.\nEven simple, known confounders such as preference for longer outputs remains in existing automated evaluation metrics.\nWe propose a simple regression analysis approach for controlling biases in auto-evaluations.\nAs a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for chat LLMs that uses LLMs to estimate response quality.\nDespite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs.\nWe introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: \"What would the preference be if the model’s and baseline’s output had the same length?\"\nTo achieve this, we first fit a generalized linear model to predict the biased output of interest (auto-annotator preferences) based on the mediators we want to control for (length difference) and other relevant features.\nWe then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths.\nLength-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS’ Chatbot Arena from 0.94 to 0.98.\nWe release the code and resulting leaderboard.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Developing and improving NLP systems requires reliable, low-cost evaluations that can quantify progress. In closed-ended tasks, such as multiple-choice QA, such evaluations are straightforward to implement and trust (Novikova et al., 2017  ###reference_b14###; Yeh et al., 2021  ###reference_b27###). However, such evaluations cannot be applied to extremely open-ended settings such as instruction following for language models.\nEven neural reference-based evaluation metrics such as BERTscore (Zhang* et al., 2020  ###reference_b28###) face challenges in those settings due to the difficulty of collecting a diverse set of references that can cover the space of valid outputs.\nRecently, there has been a push toward reference-free evaluation methods that leverage high-performance LLMs, e.g.\nAlpacaEval (Li et al., 2023  ###reference_b11###), MTBench (Zheng et al., 2023  ###reference_b29###), and WildBench (Lin et al., 2024  ###reference_b12###).\nWhile these approaches show a high correlation with human annotators,\nthey often do so by exploiting spurious correlations such as the length of the output, the presence of lists, or various position biases Li et al. (2023  ###reference_b11###); Zheng et al. (2023  ###reference_b29###); Koo et al. (2023  ###reference_b9###); Wang et al. (2023  ###reference_b25###); Wu & Aji (2023  ###reference_b26###).\nCreating a way to debias automated evaluation metrics would be highly valuable – it would address the major drawback of LLM-based reference-free evaluations, and enable low-cost, accurate evaluations for developing NLP systems in many open-ended settings.\nOur work focuses on this challenge of taking an existing automated evaluation metric (e.g. AlpacaEval) and a suspected spurious correlate (e.g. length) and producing a debiased metric.\nWe propose a simple, interpretable debiasing strategy for automated evaluation metrics based on basic, regression-based adjustments for observational causal inference.\nWe view spurious correlates – such as the length of the response – as undesirable mediators VanderWeele (2015  ###reference_b23###) in a causal graph and use regression-based causal inference Hernán & Robins (2010  ###reference_b8###) techniques to provide simple adjustments to automated evaluations that control for any suspected spurious correlation.\nApplying this approach to the popular AlpacaEval benchmark, we show that controlling for length has significant positive effects on automated evaluation.\nWe find that it is more correlated on average with LMSYS’ Chatbot Arena (Zheng et al., 2023  ###reference_b29###) than both (length-uncontrolled) AlpacaEval and MT-bench, and that it is significantly more robust to gaming the evaluation by increasing the verbosity of the output.\nOur contributions are the following:\nWe propose a simple regression-based debiasing approach for automated evaluation that satisfies several desirable properties for an automatic evaluation metric.\nWe apply the approach to AlpacaEval, producing AlpacaEval-LC that is more robust to length-based spurious correlates.\nWe show that AlpacaEval-LC correlates better with the human evaluations of model rankings (Chatbot Arena) fig. 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Problem Setting",
            "text": "Ethics in AI has become a critical area of study, focusing on ensuring that AI systems align with societal values and avoid harm. Key concerns include bias in algorithms, transparency, accountability, and the societal impact of AI deployment (Jobin et al., 2019 ###reference_b14###; Mittelstadt et al., 2016 ###reference_b15###). Addressing these issues can enhance trust and foster responsible AI innovation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Length-Controlled AlpacaEval",
            "text": "A major challenge in building automated evaluations  is the spurious correlations problem. Specifically, consider a simple example in which there is a spurious correlate  (e.g. length) such that heavily relying upon  can be predictive of the human label .\nThe confounder  is initially predictive of  but becomes less predictive as model builders explicitly begin to optimize against the metric.\nAdopting this causal view, we ask our motivating question\nWhat would the AlpacaEval metric be, if the outputs of all models had the same length as those of the baseline?\nOur goal in this section will be to operationalize this into a simple regression-based estimator.\nTo be precise, we hypothesize that automated evaluation measures  such as AlpacaEval return their quality estimates through a combination of direct effects that measure the quality of the model response and indirect effects that are mediated by spurious variables such as the length of outputs. The goal of controlling for the spurious correlates is thus equivalent to controlling these indirect effects. See fig. 2  ###reference_### for a visual representation.\n###figure_2### This abstraction leads us to a simple approach for bias correction, inspired by methods for estimating Controlled Direct Effect (VanderWeele, 2010  ###reference_b24###), for which simple Generalized Linear Models (GLMs) can provide reasonable estimates.\nOur approach will be to estimate the contribution of 3 different components to the AlpacaEval quality judgment:\nModel identity Whether an output comes from the baseline model  or the evaluated model  should impact the probability that an output wins the pairwise comparison.\nLength of output\nThe length of output is known to affect both human and model judgments of output quality (Dubois et al., 2023  ###reference_b2###; Singhal et al., 2023  ###reference_b19###), and so we expect this to also affect the win probability.\nInstruction difficulty Models do not perform uniformly over instructions: the preference of humans will generally depend on the instruction.\nFor example, the baseline might be much better for coding tasks than any other tasks.\nFor every instruction we thus want to model the difficulty of that task for the baseline.\nNote that the (baseline) “instruction difficulty” is not caused by “model” but conditioning on it can enhance the precision of estimates in regression analysis by reducing unexplained variability Pearl (2009  ###reference_b17###).\nWe can obtain length corrected AlpacaEval score in two steps:\n\n\n1 first, we can fit a model to these three attributes, and\n\n2 then we zero out the “length of output” term to obtain counterfactual estimates of AlpacaEval win rate.\nMotivated by the previous discussion, we will model the AlpacaEval predictions  with a logistic regression that has 3 terms: model, length, and instruction.\nWe will first present the overall regression formula, explain the details of the featurization, and then describe some naturally appealing properties of our featurization.\nThe model and instruction terms are straightforward – they can be viewed as the log-linear contribution of the model () and each instruction’s difficulty () on the baseline win rate.\nThe length term is linear in a normalized length feature, where the normalizer standardizes the length to have unit variance and transforms this via a tanh, as differences in lengths should have strong diminishing returns on the log odds.\nImportantly, this formula fulfills the identity property, i.e.,  , and symmetry property, i.e,  of the original win rate.\nIdentity holds as the length term is zero due to having no difference in length, while the other two terms are zero as the coefficients are identical.\nFor symmetry note that , and it is clear that swapping  and  flips the sign of the model and instruction terms. For the length term, the same is true as flipping  and  negates the length difference, and tanh is an odd function.\nMore generally any additive term that is antisymmetric and centered around 0 would satisfy the desired properties.\nUsing the model from eq. 1  ###reference_### we can answer the counterfactual question of what the automatic evaluation  might be if the length of the evaluated model matched that of the base model, i.e., .\nIn this case, the second, length term becomes zero and we obtain the length corrected win rate estimate as\nIn other words, we simply remove the length term from the regression and compute the implied win rate.\nTraining of the regression is simple and uses off-the-shelf libraries for fitting generalized linear models.\nSince our GLM uses a logit link function, we fit the model in Eq 1  ###reference_### using the cross-entropy loss .\nIn AlpacaEval’s leaderboard, we use a constant baseline , so without loss of generality we can drop , which can be absorbed into the corresponding parameters for .\nIn total, for a leaderboard with  models and  instructions, our GLM contains  parameters to be estimated from  examples (, ,  for each model,  for each instruction).\nThis will be overdetermined when  and  are both large, as in the case of AlpacaEval.\nHowever, to ensure our procedure is robust even for small  and , we use 5-fold cross-validation with  regularization on weights to avoid potential overfitting.\nThe one complexity of our regression is that the instruction difficulty term  is shared across models, and so we estimate this separately by first fitting a joint regression across all models with the  term fixed to one and using the estimated  from this regression.\nFor the remaining regression coefficients, we simply fit , , and  on the AlpacaEval predictions for each model separately, re-using the already estimated  as these do not depend on the model being evaluated.\nFitting models separately is important as it implies that previously computed metrics won’t change when adding a new model to the leaderboard.\nFinally, we have added an additional weak regularization on  to prevent an adversary from performing attacks that intentionally truncate sequences that a model performs poorly on.\nIn this case, the poor performance of the model would be perfectly correlated with the short length, and the model builder would be able to exploit the length corrections to boost the performance of the model.\nAdding a regularization term makes it so that any model performance issues would be explained by the model terms first, and then any residual effects would be captured by the length effects, as intended.\nThe regularization is weak enough that we empirically found it to not affect non-adversarial models."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We apply our approach to AlpacaEval, as this benchmark has known length confounders, contains a large set of pre-computed LLM-based pairwise comparisons, and is widely used by the research community.\nWe evaluate our approach on several measures of interest:\nDecreasing length gameability: We call a metric length gameable if simply prompting the model to be more or less verbose significantly affects the metric outcome.\nIdeally, length gameability is low for two reasons.\nFirst, we would like evaluations that prioritize the content rather than the style of the answer.\nSecond, the benchmark should not be too dependent on the prompting strategy as users usually think of evaluations of models rather than the entire system, which includes the prompt.\nCorrelation with chatbot arena: If our gameability and robustness metrics represent better capturing human preference, we should improve our correlation with chatbot arena.\nWe measure Spearman rather than Pearson correlation as probabilities are log-linearly correlated with ELO ratings, rather than linearly.\nRobustness and interpretability: Our corrected metric should be robust to simple adversarial attacks such as truncation, and be interpretable to users as a win-rate.\nWe show that AlpacaEval-LC fulfills all these goals.\nWe release the code  ###reference_### for all experiments.\n###figure_3### Figure 4  ###reference_### shows leaderboard changes due to our length control approach.\nWe see that proprietary models, which often generate shorter responses, perform much better on AlpacaEval-LC, and the biggest rank losses are in open-source models that have gone through the RLHF process Ouyang et al. (2022  ###reference_b15###).\nGiven that AlpacaEval is a potential optimization target for open-source language models, these results are consistent with the hypothesis that existing open models had exploited the length bias of AlpacaEval.\n###figure_4### One potential issue with simple bias corrections is that they may be gamed through white-box adversarial attacks, e.g., postprocessing the outputs of models\nto make them look better on AlpacaEval-LC.\nOne example of such an attack is to truncate all outputs to a few characters, besides those that are much better and around the same length as the baseline.\nA naive GLM fitted on such outputs should naturally predict very high win rates in the counterfactual world where outputs have the same length as the baseline.\nIndeed, when doing such post-processing to GPT-4 outputs, win rates increase from  (AlpacaEval 2.0) to  (AlpacaEval-LC, no regularization).\nTo mitigate such adversarial attacks, our approach includes a regularization term on .\nThis decreases the gamed win rate to  (AlpacaEval-LC with regularization) while having an imperceptible impact on standard models.\nFigure 5  ###reference_### shows that LC win rates can be interpreted similarly to raw win rates.\nIn particular, the baseline always has a win rate of  and\n.\nThis seems very natural but wouldn’t hold for most length-correction methods, such as normalizing by length.\nMore interestingly, a nice property of our GLM is that once we fit the weights for one baseline, we can predict the win rate between any pair of models on the leaderboard.\nAs a result, we can predict the leaderboard for any other baseline as seen in fig. 5  ###reference_###.\nAnother common way to control some covariates is through stratification.\nOne potential metric, dubbed length-balanced (LB) win rate, would thus be to compute the average win rate stratified on examples where the model outputs are (1) longer and (2) shorter than the baseline Duong (2024  ###reference_b3###).\nLB satisfies many of the desiderata of length control but has one main downside: robustness.\nIn particular, stratification relies upon having enough samples within each stratum, otherwise the estimates may rapidly become unstable. This can increase variance, e.g., if one model is naturally longer than another, but can also introduce adversarial vulnerabilities.\nThe first and last rows in fig. 6  ###reference_### show that length-balanced win rates improve both the length gameability (measured by the normalized standard deviation of win rate across concise/standard/verbose prompts) and the Chatbot arena correlation.\nHowever, this approach is strictly dominated by our length-controlled method – in arena correlation, gameability, and adversarial win rate gains from truncating bad GPT-4 outputs as discussed in section 4.3  ###reference_###.\nAnother option Galambosi (2024  ###reference_b6###); Teortaxes (2024  ###reference_b22###) is to directly normalize the win rate by a function of the length of the model’s and baseline’s output.\nWe have tried several variations on normalization (e.g. directly dividing by lengths, logistic function of lengths, etc).\nIn our experiments, the function that performed best was dividing the raw win rate by a temperature-scaled logistic function of the average difference of lengths.\nWe call this metric length-normalized (LN) win rate.\nFigure 6  ###reference_### shows that this simple LN win rate performs surprisingly well on many of the metrics.\nWe chose to present and implement the length-controlled (LC) win rate, as it is more principled (as an estimate of the direct effect), interpretable (as a win rate), and performs slightly better on all quantitative metrics except adversarial gameability."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "AlpacaEval-LC decreases length gameability",
            "text": "A good evaluation metric should not be so sensitive to length that prompting for longer or shorter responses completely changes the metric.\nTo measure gameability we prompted different models to “Answer with as much detail as possible.” (verbose) or “Be as concise as possible while still providing all the necessary information to answer the question.” (concise).\nFigure 3  ###reference_### shows that AlpacaEval is highly length gameable.\nThe baseline model (gpt4_1106_preview) fluctuates from  to  by varying the verbosity instruction in the prompt.\nEven worse, significant gains are possible by asking weaker models to be verbose, as seen with Claude-2.1.\nIn contrast, the length-controlled AlpacaEval has significantly lower gameability (gpt4_1106_preview’s win rates now only fluctuate from 41.9% to 51.6%), and rankings are generally stable to verbosity prompts. Quantitatively, the normalized standard deviation across the three verbosity prompts decreases from 25% to 10% from the length control."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "AlpacaEval-LC increases correlation with Chatbot Arena to 0.98",
            "text": "Our prior experiments demonstrate that length control reduces the high sensitivity to length in AlpacaEval.\nHowever, our goal is not simply to make metrics that are less sensitive to length, but to produce metrics that are overall more representative of human judgments.\nFigure 1  ###reference_### shows that controlling for length increased the Spearman correlation with Chat Arena from 0.94 to 0.98.\nOf existing benchmarks, this difference is significant enough to make the length-corrected version of AlpacaEval the metric with the highest correlation with Chat Arena which we are aware of.\nCorrelations are computed on every benchmark that evaluates at least 25 models from the Chatbot Arena.\nAlpacaEval and AlpacaEval-LC have 38 such models, MT bench has 34.\nThe bootstrap p-value comparing the correlation with AlapcaEval’s correlation is 0.07 and 0.06 compared to MT-bench.\nFigure 4  ###reference_###  ###reference_### shows leaderboard changes due to our length control approach.\nWe see that proprietary models, which often generate shorter responses, perform much better on AlpacaEval-LC, and the biggest rank losses are in open-source models that have gone through the RLHF process Ouyang et al. (2022  ###reference_b15###  ###reference_b15###).\nGiven that AlpacaEval is a potential optimization target for open-source language models, these results are consistent with the hypothesis that existing open models had exploited the length bias of AlpacaEval.\n###figure_5###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "AlpacaEval-LC is interpretable and robust",
            "text": "###figure_6### One potential issue with simple bias corrections is that they may be gamed through white-box adversarial attacks, e.g., postprocessing the outputs of models\nto make them look better on AlpacaEval-LC.\nOne example of such an attack is to truncate all outputs to a few characters, besides those that are much better and around the same length as the baseline.\nA naive GLM fitted on such outputs should naturally predict very high win rates in the counterfactual world where outputs have the same length as the baseline.\nIndeed, when doing such post-processing to GPT-4 outputs, win rates increase from  (AlpacaEval 2.0) to  (AlpacaEval-LC, no regularization).\nTo mitigate such adversarial attacks, our approach includes a regularization term on .\nThis decreases the gamed win rate to  (AlpacaEval-LC with regularization) while having an imperceptible impact on standard models.\nFigure 5  ###reference_###  ###reference_### shows that LC win rates can be interpreted similarly to raw win rates.\nIn particular, the baseline always has a win rate of  and\n.\nThis seems very natural but wouldn’t hold for most length-correction methods, such as normalizing by length.\nMore interestingly, a nice property of our GLM is that once we fit the weights for one baseline, we can predict the win rate between any pair of models on the leaderboard.\nAs a result, we can predict the leaderboard for any other baseline as seen in fig. 5  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparisons to baselines for length control",
            "text": "###figure_7### Let’s now briefly discuss two other potential family length-correction methods that have been proposed in the community Duong (2024  ###reference_b3###); Galambosi (2024  ###reference_b6###); Teortaxes (2024  ###reference_b22###).\nAnother common way to control some covariates is through stratification.\nOne potential metric, dubbed length-balanced (LB) win rate, would thus be to compute the average win rate stratified on examples where the model outputs are (1) longer and (2) shorter than the baseline Duong (2024  ###reference_b3###  ###reference_b3###).\nLB satisfies many of the desiderata of length control but has one main downside: robustness.\nIn particular, stratification relies upon having enough samples within each stratum, otherwise the estimates may rapidly become unstable. This can increase variance, e.g., if one model is naturally longer than another, but can also introduce adversarial vulnerabilities.\nThe first and last rows in fig. 6  ###reference_###  ###reference_### show that length-balanced win rates improve both the length gameability (measured by the normalized standard deviation of win rate across concise/standard/verbose prompts) and the Chatbot arena correlation.\nHowever, this approach is strictly dominated by our length-controlled method – in arena correlation, gameability, and adversarial win rate gains from truncating bad GPT-4 outputs as discussed in section 4.3  ###reference_###  ###reference_###.\nAnother option Galambosi (2024  ###reference_b6###  ###reference_b6###); Teortaxes (2024  ###reference_b22###  ###reference_b22###) is to directly normalize the win rate by a function of the length of the model’s and baseline’s output.\nWe have tried several variations on normalization (e.g. directly dividing by lengths, logistic function of lengths, etc).\nIn our experiments, the function that performed best was dividing the raw win rate by a temperature-scaled logistic function of the average difference of lengths.\nWe call this metric length-normalized (LN) win rate.\nFigure 6  ###reference_###  ###reference_### shows that this simple LN win rate performs surprisingly well on many of the metrics.\nWe chose to present and implement the length-controlled (LC) win rate, as it is more principled (as an estimate of the direct effect), interpretable (as a win rate), and performs slightly better on all quantitative metrics except adversarial gameability."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "###figure_8### Length is a well-known bias of automated evaluators of chatbot LLMs but several others have been noted, including a bias of models towards their own outputs Zheng et al. (2023  ###reference_b29###), or presence of lists Dubois et al. (2023  ###reference_b2###). While we focus on a more detailed study of length biases here, we note that the same approaches can be applied to other biases by representing them as additional features in the logistic regression.\nAdditionally, our preliminary explorations of self-annotator biases shows that the effect exists but is often smaller than general model differences. fig. 7  ###reference_### shows that the ranking of considered models does not change when using different annotators.\nIn particular, Claude 3 Opus prefers GPT4 Preview, and Mistral Large prefers the former two than itself.\nOur work is closely related to the recent work that aims to debias (implicit or explicit) reward models used to finetune LLMs with RLHF (Singhal et al. (2023  ###reference_b19###)).\nFor example Shen et al. (2023  ###reference_b18###); Chen et al. (2024  ###reference_b1###) try to train a reward model that is uncorrelated to length by making it predict the length at the same time as the reward and disentangle the two.\nPark et al. (2024  ###reference_b16###) extends this intuition to the case of implicit reward models.\nThis type of debiasing would not work out-of-the-box in typical auto-evaluation settings, e.g. AlpacaEval, which uses closed source LLM as judges rather than training a reward model.\nOur post-hoc debiasing could however be used in the RLHF setting, and we encourage future work to look into that.\nWe propose a simple method for mitigating the length bias of\nLLM-based automatic evaluations, specifically, AlpacaEval.\nThe procedure consists of fitting a generalized linear model to predict the auto-evaluators preferences, conditioned on the length of the models’ output.\nWe then get the length-controlled preference by predicting what the auto-evaluator would have preferred if the model’s output and the baseline’s output had the same length.\nWe show that the resulting length-controlled AlpacaEval, has higher correlations with humans, has much less length bias, and is robust (hard to game)."
        }
    ]
}