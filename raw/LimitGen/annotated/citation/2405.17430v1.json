{
    "title": "Matryoshka Multimodal Models",
    "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning and merging methods exist, they produce a single-length output for each image and cannot afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g., adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around 9 visual tokens to obtain an accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between performance and visual token length at the sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Multimodal models (LMMs) [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###, 7  ###reference_b7###] have shown strong performance in visual-linguistic understanding and reasoning. Models such as LLaVA [2  ###reference_b2###, 5  ###reference_b5###, 4  ###reference_b4###]\nfirst embed the input image with a fixed number of visual tokens, and then feed them as prefix tokens to a Large Language Model (LLM) [8  ###reference_b8###, 9  ###reference_b9###] to reason about the input image. Similar model designs are borrowed in video LMMs [10  ###reference_b10###, 11  ###reference_b11###], where each frame contributes a fixed number of tokens to form the final video representation.\nIn reality, the number of visual tokens can be prohibitively large in the case of high-resolution images, and even more so for long videos. Existing works [10  ###reference_b10###, 4  ###reference_b4###, 12  ###reference_b12###, 13  ###reference_b13###] mainly tackle this issue by increasing the input context length and consequently, feeding a large number e.g., 3-8k of visual tokens into the LLM. This approach has a couple of significant drawbacks: (1) the extremely long context makes both training and inference inefficient; (2) an excessive number of visual tokens can actually harm the LMM’s performance, distracting it from attending to the relevant information, as we show in Sec. 4.3  ###reference_###.\nSeveral recent works [14  ###reference_b14###, 15  ###reference_b15###, 16  ###reference_b16###] use heuristics to prune and merge visual tokens to reduce the sequence length. However, they produce a single-length output and do not afford control over the final sequence length, which could be useful to trade information density versus efficiency while accounting for resource constraints in the deployment phase.\n###figure_1### Images and videos naturally exhibit a hierarchical structure from coarse to fine details, and our human visual system has evolved to recognize visual information in this coarse to fine manner, as shown by biologists and psychologists decades ago [18  ###reference_b18###, 19  ###reference_b19###]. Can we create a similar structure for LMMs, where within one suite of model weights, the visual content tokens are organized into different scales of granularities? Conceptually, our goal is to learn the visual tokens to have a nested structure, similar to the Matryoshka Doll [20  ###reference_b20###]. Matryoshka Representation Learning (MRL) [20  ###reference_b20###] builds the Matryoshka mechanism over a neural network’s representation vector, where each of the segments with various feature dimensions is capable of handling tasks like classification or retrieval. However, for LMMs, the inefficiency mainly comes from the number of tokens. Thus, inspired by, but different from MRL, our work is motivated to build Matryoshka Multimodal Models upon the token length dimension, so that we can flexibly adjust it.\n###figure_2### Specifically, we propose M3: Matryoshka Multimodal Models, which enforces an LMM to learn a hierarchy of visual representation granularities at the token sequence level, instead of the feature dimension level as in MRL [20  ###reference_b20###]. With this representation, at inference time, the visual granularity can be flexibly controlled based on specific requirements, e.g., to account for the input image’s information density and efficiency constraints. Our training process is simple and straightforward. During training, we encode the image into  sets of visual tokens from coarse to fine, , , where the number of visual tokens gradually increases, i.e., . And importantly, the visual tokens in a coarser level are derived from the visual tokens in a finer level, i.e., , . In this way, the visual information in  gradually includes more fine-grained details. For example, given a natural image as shown in Figure 1  ###reference_###,  includes high-level semantics such as the restaurant and girl, while  includes more details such as the Pepsi cup and white paper bag. All other training settings, such as the loss function and model architecture, are kept the same as LLaVA [2  ###reference_b2###, 5  ###reference_b5###, 4  ###reference_b4###].\nOur approach, M3, introduces several novel properties and benefits for LMMs. First, our approach can adaptively and efficiently represent visual content. Under one suite of weights, it generates multiple nested sets of visual tokens with different granualarities in information density. This enables flexibility in the number of visual tokens used for any image during inference, enabling control over the best tradeoff between cost and performance based on the image or video content. For example, one can use all visual tokens for images with dense details and use just a few tokens for simpler images. This flexibility can be particularly significant when handling very long visual sequences, such as videos. For instance, given a fixed budget of 2880 visual tokens, a user could represent a video of 2880 frames each with one token or represent the same video by sampling 5 frames each with 576 tokens.\nSecond, our approach can be used as a general framework to evaluate the visual complexity of vision-language datasets or benchmarks, i.e., which level of granularity is needed in order to perform the given task correctly. Surprisingly, we find that most benchmarks, especially those mainly crafted from natural scenes (such as COCO) [21  ###reference_b21###, 22  ###reference_b22###, 23  ###reference_b23###], can be handled well with only  tokens per image. In contrast, dense visual perception tasks such as document understanding or OCR [24  ###reference_b24###, 25  ###reference_b25###] require a greater amount of tokens ( tokens) per image to handle the task well. The detailed findings are presented in Sec. 4.2  ###reference_###.\nFinally, our approach provides a foundation to tackle a critical task in LMMs: How to use the least amount of visual tokens while answering the visual questions correctly?. Based on the model’s predictions on the test set, we find that compared to full visual tokens, the oracle can use far fewer tokens while performing much better. For example, under six common LMM benchmarks used in LLaVA-NeXT [4  ###reference_b4###], the oracle with the trained M3 model can use as few as 8.9 visual tokens on average to achieve performance that is 8% points better than LLaVA-NeXT which uses 576 tokens per image grid. This indicates that there is a large room for improvement compared to the oracle upperbound, as we show in Sec. 4.2  ###reference_###.\nTo enable further research on adaptive LMMs that learn diverse information granularities, we publicly release our code and models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Large Multimodal Models. Large Language Models (LLMs) like ChatGPT [26], GPT-4 [27], and LLaMA [28] have demonstrated impressive reasoning and generalization capabilities for text. The landscape of LLMs has been significantly transformed by the recent introduction of models that also incorporate visual information, such as GPT-4V(ision)[1]. Building upon open-source LLMs [28, 8], a plethora of multimodal models have made significant strides, spearheaded by models like LLaVA [2, 5] and MiniGPT-4 [3], which combine LLaMA’s [28] language capabilities with a CLIP [29] based image encoder. Recently, LMMs on more tasks and modalities have emerged, such as region level LMMs [30, 31, 32, 33, 34], 3D LMMs [35], and video LMMs [10, 11, 12]. However, existing LMMs typically represent the visual content with a large and fixed number of tokens, which makes it challenging to scale to very long visual sequences such as high-resolution images or long-form videos. In this work, we propose to adaptively and efficiently represent the visual content by learning multiple nested sets of visual tokens, providing flexibility in the number of visual tokens used for any image during inference. \n\nMatryoshka Representation Learning. Matryoshka Representation Learning (MRL) [20] addresses the need for flexible representations that can adapt to multiple downstream tasks with varying computational resources. This approach, inspired by the nested nature of Matryoshka dolls, encodes information at different granularities within the same high-dimensional feature vector produced by a neural network. The adaptability of MRL extends across different modalities, including vision (ResNet [36], ViT [37]), vision + language (ALIGN [38]), and language (BERT [39]), demonstrating its versatility and efficiency. Recent work [40] extends MRL to both the text embedding space and the Transformer layers space. Our approach is inspired by MRL, but instead of learning multiple nested embeddings for a high-dimensional feature vector, we learn nested visual tokens along the token length dimension for the visual input. We are the first to show that the idea of Matryosha learning can enable explicit control over the visual granularity of the visual content that an LMM processes.\n\nToken Reduction. One of the main causes of inefficiency in recent LMMs is their large number of prefix visual tokens that are fed into the LLM [2, 3]. The quadratic complexity in Transformers [41] is the key issue in scaling the input sequence length for Transformers. Token reduction serves as an effective technique to reduce computational costs in Transformers. Sparse attention methods such as Linformer [42] and ReFormer [43] conduct attention operations within local windows rather than the full context, thereby reducing the quadratic complexity of the vanilla attention operation. Another notable method is Token Merging (ToMe) [14], which utilizes full attention but gradually reduces the number of tokens in each transformer block by selecting the most representative tokens through bipartite matching for the Vision Transformer (ViT). A recent work [44] further studies different families of token reduction methods for ViT. However, prior approaches produce a single length output per input image and do not offer multiple granularities over the reduced token sequence. Our M3 approach instead learns a multi-granularity, coarse-to-fine token representation within the same model architecture and weights, enabling it to easily be adjusted to various computational or memory constraints.\n\nAI Ethics in Decision-Making Algorithms. The integration of ethical considerations in AI decision-making has become increasingly important. Researchers emphasize transparency, fairness, and accountability in algorithms to prevent biases and unintended consequences [45]. Conscientious development of AI systems is crucial to ensure equitable decision-making processes, as algorithms often reflect and amplify societal inequities [46]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "M3: Matryoshka Multimodal Models",
            "text": "###figure_3### Our goal is to learn a Large Multimodal Model (LMM) that represents visual content as nested sets of visual tokens capturing information across multiple coarse-to-fine granularities, so that one can explicitly control the visual granularity per test instance during inference. Here we introduce how we learn a Matryoshka doll-like token sequence.\nLMMs such as LLaVA [2  ###reference_b2###] typically input a sequence of visual tokens as prefix tokens to the LLM for visual-linguistic reasoning. The visual encoder from pretrained vision-language models, such as CLIP [29  ###reference_b29###] and SigLIP [45  ###reference_b45###], is typically utilized to project the images into the set of visual tokens.\nIn particular, the CLIP visual encoder represents an input image  as an  grid of visual tokens , where each  is a  dimensional feature vector. Our goal is to learn nested sets of visual tokens  which encode the visual information in a coarse-to-fine manner. To this end, we enforce . Importantly, we do not introduce any new learnable parameters to the LMM. We instead optimize the CLIP visual encoder to learn the nested visual representation directly, and train the ensuing LLM to adapt to the learned nested set of tokens.\nFor ease of exposition, we consider CLIP-ViT-L-336 [29  ###reference_b29###] as the visual encoder, where an image is encoded as  visual tokens (576 total). We create  sets of tokens e.g., , in which the visual tokens at the coarser level are derived directly from those at the finer level. Specifically, given the initial  visual tokens, We sequentially apply  pooling with a stride 2, resulting in , and  visual tokens. Finally, we apply  pooling and get the most condensed single visual token. In this way, the sets of Matryoshka visual tokens can gradually preserve the spatial information in the original tokens while simultaneously forming a coarse-to-fine nested representation.\nWe train M3 by averaging the autoregressive next token prediction loss for each scale  for each image . Specifically, given a Matryoshka visual representation  for scale , we maximize the likelihood of the predicted tokens matching the ground-truth answer :\nwhere  is the trainable parameters of the model, which includes both the CLIP visual encoder and the ensuing LLM.  denotes the question in text format,  denotes the token length of the ground truth answer , and  denotes all the ground truth answer tokens before the current prediction token , where  denotes the token index during text token generation. We omit system messages for clarity, though they are part of the conditioning. Figure 3  ###reference_### shows our model architecture.\nThe final objective averages over all  visual token scales:\nWith this objective function, M3 learns nested sets of visual tokens that gradually include more details with increasing scale. For example, in Figure 1  ###reference_###, the smaller set of visual tokens describes the whole scene at a high level while the larger set of visual tokens includes more details such as the Pepsi cup. Our training objective affords our model to conduct visual question answering under any granularity during inference. This can be particularly useful in resource constrained applications; e.g., the visual granularity can be flexibly adjusted based on the anticipated simplicity or complexity of the visual content while taking into account compute and memory constraints."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first detail the experiment settings in Sec 4.1  ###reference_###. Then we show the performance of M3 on both image-level benchmarks 4.2  ###reference_### and video-level benchmarks 4.3  ###reference_###. Finally, we analyze the behavior of Matryoshka Multimodal Models and provide ablations in Sec 4.4  ###reference_### and  4.5  ###reference_###.\nWe use LLaVA-1.5 [5  ###reference_b5###] and LLaVA-NeXT [4  ###reference_b4###] as the base LMMs, both with Vicuna 7B as the language model backbone. We finetune the whole model using the exact visual instruction data from LLaVA-1.5 and LLaVA-NeXT, respectively. The learning rate of LLM is  and , respectively for LLaVA-1.5 and LLaVA-NeXT. The learning rate for the visual encoder is  for both models. We train both models for 1 epoch using 8 NVIDIA H100 GPUs.\nInstead of training the language model from scratch, we initialize the language model weights from pre-trained LLaVA-1.5 and LLaVA-NeXT, which we empirically works better. We name our Matryoshka Multimodal Models LLaVA-1.5-M3 and LLaVA-NeXT-M3.\nWe design 5 scales for the visual tokens. LLaVA-1.5 [5  ###reference_b5###] and LLaVA-NeXT [4  ###reference_b4###] both leverage CLIP-ViT-L-336 [29  ###reference_b29###] as the visual encoder, where an image is embedded into  visual tokens. We gradually apply  pooling with stride 2, resulting in , and  visual tokens, where we finally apply a  pooling to get the final single visual token. Therefore, the size of Matryoshka visual token sets are , following a nested manner. The efficiency anlaysis on the system level is shown in Appendix B  ###reference_###, where M3 boosts the speed of the LMM prefill process through diminished floating-point operations (FLOPs) and lessens computational memory requirements.\nFor image understanding, we evaluate LLaVA-1.5 and LLaVA-NeXT on (a) diverse multimodal benchmarks: POPE [22  ###reference_b22###], GQA [46  ###reference_b46###], MMBench [23  ###reference_b23###], VizWiz [47  ###reference_b47###], SEEDBench [48  ###reference_b48###], ScienceQA [49  ###reference_b49###], MMMU [50  ###reference_b50###], and (b) document understanding/Optical character recognition (OCR) benchmarks: DocVQA [51  ###reference_b51###], ChartQA [25  ###reference_b25###], AI2D [52  ###reference_b52###] and TextVQA [24  ###reference_b24###].\nFor video understanding, we use both (a) open ended video question answering benchmarks evaluated by GPT-3.5: MSVD-QA [53  ###reference_b53###], MSRVTT-QA [53  ###reference_b53###] and ActivityNet-QA [54  ###reference_b54###]; and (b) multi-choice video question answering benchmarks: NExT-QA [55  ###reference_b55###], IntentQA [56  ###reference_b56###], and EgoSchema [57  ###reference_b57###].\nWe evaluate LLaVA-1.5-M3 on the common multimodal understanding and reasoning benchmarks. Results are shown in Table 1  ###reference_###. LLaVA-1.5-M3 with full tokens maintains the performance of LLaVA-1.5 across diverse benchmarks. More importantly, our approach shows strong performance even with 1 or 9 tokens. Specifically, in MMBench, a comprehensive multimodal understanding benchmark, LLaVA-1.5-M3 with 9 tokens surpasses Qwen-VL-Chat with 256 tokens, and achieves similar performance as Qwen-VL-Chat with even 1 token. Compared with InstructBLIP [58  ###reference_b58###], LLaVA-1.5M3 with 9 tokens surpasses InstructBLIP-7B and InstructBLIP-13B across all benchmarks. This demonstrates that our model has both flexibility and strong empirical performance under diverse number of visual tokens.\nApproach\n\n# Tokens\nMMBench\nGQA\nPOPE\nVizWiz\nSEEDBench\n\n\n\nQwen-VL [7  ###reference_b7###]\n\n256\n38.2\n59.3\n-\n35.2\n56.3\n\n\n\nQwen-VL-Chat [7  ###reference_b7###]\n\n256\n60.6\n57.5\n-\n38.9\n58.2\n\n\n\nInstructBLIP-7B [58  ###reference_b58###]\n\n32\n36.0\n49.2\n-\n34.5\n53.4\n\n\n\nInstructBLIP-13B [58  ###reference_b58###]\n\n32\n-\n49.5\n78.9\n33.4\n-\n\n\n\nLLaVA-1.5-7B [5  ###reference_b5###]\n\n576\n64.8\n62.0\n85.9\n54.4\n60.5\n\n\n\nLLaVA-1.5-\n\n576\n65.9\n61.9\n87.4\n54.9\n60.6\n\n\n144\n66.4\n61.3\n87.0\n53.1\n59.7\n\n\n36\n64.8\n60.3\n85.5\n52.8\n58.0\n\n\n9\n63.1\n58.0\n83.4\n51.9\n55.4\n\n\n1\n59.5\n52.6\n78.4\n49.4\n50.1\nWe use the proposed Matryoshka Multimodal Models to finetune LLaVA-NeXT, and compare LLaVA-NeXT-M3 with SS, which denotes the setting where the LLaVA-NeXT is trained under a Specific Scale of visual tokens also for 1 epoch. We also include the oracle upperbound performance. Specifically, ‘Oracle’ denotes the case where the best tradeoff between visual tokens and performance is picked for each test instance. Specifically, for each test instance, we select the the scale with the fewest amount of tokens but can answer the question correctly. Results are shown in Table 2  ###reference_###. Our approach, M3, is at least as good as SS, while performing better on tasks such as document understanding (TextVQA and ChartQA) and common benchmarks such as MMBench [23  ###reference_b23###].\n# Tokens Per Grid\n\nApproach\nTextVQA\nAI2D\nChartQA\nDocVQA\nMMBench\nPOPE\nScienceQA\nMMMU\n\n\n\n\n\n576\n\nSS\n64.53\n64.83\n59.28\n75.40\n66.58\n87.02\n72.29\n34.3\n\n\n\n63.13\n66.71\n58.96\n72.61\n67.96\n87.20\n72.46\n34.0\n\n\n\n144\n\nSS\n62.16\n65.77\n55.28\n67.69\n67.78\n87.66\n72.15\n36.4\n\n\n\n62.61\n68.07\n57.04\n66.48\n69.50\n87.67\n72.32\n36.1\n\n\n\n36\n\nSS\n58.15\n65.90\n45.40\n56.89\n67.01\n86.75\n71.87\n36.2\n\n\n\n58.71\n67.36\n50.24\n55.94\n68.56\n87.29\n72.11\n36.8\n\n\n\n9\n\nSS\n50.95\n65.06\n37.76\n44.21\n65.29\n85.62\n72.37\n36.8\n\n\n\n51.97\n66.77\n42.00\n43.52\n67.35\n86.17\n71.85\n35.2\n\n\n\n1\n\nSS\n38.39\n63.76\n28.96\n33.11\n61.43\n82.83\n72.32\n35.3\n\n\n\n38.92\n64.57\n31.04\n31.63\n62.97\n83.38\n71.19\n34.8\n\n\n\nOracle\n\n# Tokens\n31.39\n11.54\n41.78\n64.09\n8.90\n6.08\n7.43\n22.85\n\n\nPerformance\n70.51\n76.36\n70.76\n81.73\n74.35\n94.29\n76.07\n50.44\nOur results also show that dataset level biases towards the visual token scales do exist. For example, ScienceQA maintains consistent performance across all visual token scales. AI2D and MMBench only encounter a small performance drop for even as few as 9 to 1 tokens. On the other hand, dense visual perception tasks such as TextVQA and DocVQA show a significant performance with fewer tokens. This analysis shows that M3 could serve as a framework to analyze the granularity that a benchmark needs.\nFurthermore, there is a large gap between the model’s actual performance under full tokens and the upper-bound oracle. This indicates that using full tokens cannot always result in the optimal performance for all samples; i.e., there is a large room of improvement towards the oracle point.\nA simple way to reduce the number of visual tokens via a training-free way is to conduct heuristic token merging or reduction. In Table 4  ###reference_###, we compare M3 with three training-free approaches: average pooling, spatial sampling, and sequential sampling. M3 is much more resilient when the number of tokens decreases, while the heuristic based sampling approaches show dramatic performance drop. A visualization of the spatial and sequential sampling is shown in Figure 5  ###reference_###.\n# Tokens\n\n\n\nM3\n\nAverage Pooling\nSpatial Sampling\nSequential Sampling\n\n\n\n\n\n576\n\n\n\n67.96\n\n67.18\n67.18\n67.18\n\n\n\n144\n\n\n\n69.50\n\n61.68\n65.81\n60.14\n\n\n\n36\n\n\n\n68.56\n\n50.77\n60.05\n44.76\n\n\n\n9\n\n\n\n67.35\n\n45.45\n45.45\n31.96\n\n\n\n1\n\n\n\n62.97\n\n19.33\n26.29\n22.42\nWe extract the response from LLaVA-NeXT-M3 in the TextVQA benchmark, and show the samples where using visual tokens across different scales can answer the question correctly and incorrectly. Shown in Figure 4  ###reference_###, the OCR performance aligns with the complexity of the images, which indicates that M3 can be utilized as a metric towards sample level complexity.\n###figure_4### As shown in Table 2  ###reference_###, the oracle upper-bound can use very few () tokens yet achieve at least 10% better performance compared to full visual tokens. This suggests that a visual token scale predictor, where the model learns to automatically select the best visual token scale given the input images or both input images and questions, has potential to achieve a better tradeoff. This would be interesting future work.\nHere we extend the length of the visual tokens at inference time to study the model’s zero-shot generalization behavior. Results under LLaVA-NeXT are shown in Table 5  ###reference_###. Here LLaVA-NeXT-M3 is trained on  image grids but evaluated on  grids. We set the number of visual tokens to be 144 in each image during evaluation. The model obtains a significant improvement in document understanding by 2.12, 1.80, and 4.11 on TextVQA, ChartQA, and DocVQA, respectively, while maintaining the same performance on benchmarks mainly composed of natural scene images.  image grids with 144 tokens per grid own 1440 tokens, yet achieve similar performance with the default LLaVA-NeXT  image grids with 2880 total tokens (576 tokens per grid). This indicates it is promising to feed more subimages while making the number of visual tokens within each subimage much smaller.\n# Grids\n# Tokens per grid\nOverall # Tokens\nTextVQA\nAI2D\nChartQA\nDocVQA\nMMBench\nPOPE\nScienceQA\n\n\n\n\n144\n720\n62.61\n68.07\n57.04\n66.48\n69.50\n87.67\n72.32\n\n\n144\n1440\n64.73\n67.75\n58.84\n70.59\n69.50\n87.67\n72.22\n\n\n576\n2880\n63.13\n66.71\n58.96\n72.61\n67.96\n87.20\n72.46\n###figure_5### Here we compare three different ways to select the visual tokens for Matryoshka Multimodal Models, including average pooling, spatial sampling, and sequential sampling, which is illustrated in Figure 5  ###reference_###. Shown in Table 6  ###reference_###, averaging pooling shows better performance than the two alternatives across diverse benchmarks. In general, sequential sampling performs the worst. We hypothesize that this is due to the visual tokens having spatial information, while sequential sampling does not naturally align with the spatial distribution of the visual tokens.\nTextVQA\nMMBench\nAI2D\n\nNum of Vis Tokens\nAvg Pooling\nSequential\nSpatial\nAvg Pooling\nSequential\nSpatial\nAvg Pooling\nSequential\nSpatial\n\n\n\n576\n63.13\n59.37\n60.45\n67.96\n64.60\n64.43\n66.71\n65.61\n64.96\n\n144\n62.61\n55.80\n58.33\n69.50\n64.18\n64.52\n68.07\n64.90\n64.96\n\n36\n58.71\n52.79\n52.39\n68.56\n63.92\n64.69\n67.36\n64.51\n64.02\n\n9\n51.97\n44.05\n44.19\n67.35\n63.14\n62.11\n66.77\n63.70\n63.92\n\n1\n38.92\n28.03\n29.91\n62.97\n59.36\n57.47\n64.57\n63.21\n63.08\nNum of Vis Tokens\nTextVQA\nMMBench\nAI2D\nDocVQA\n\n\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\n\n\n\n576\n63.13\n61.16\n67.96\n63.66\n66.71\n63.92\n72.61\n69.15\n\n144\n62.61\n57.79\n69.50\n65.21\n68.07\n63.73\n66.48\n59.77\n\n36\n58.71\n49.75\n68.56\n63.92\n67.36\n62.89\n55.94\n44.08\n\n9\n51.97\n36.15\n67.35\n61.08\n66.77\n62.05\n43.52\n28.36\n\n1\n38.92\n19.72\n62.97\n51.80\n64.57\n60.59\n31.63\n17.37\nTechnique\nTextVQA\nAI2D\n\nInit LLM weights from LLaVA\n\n✓\n\n✓\n\n✓\n\n✓\n\nAverage losses over all scales\n\n\n✓\n✓\n\n\n✓\n✓\n\n576\n60.36\n62.25\n61.01\n63.13\n62.40\n65.06\n65.84\n66.71\n\n144\n59.61\n61.02\n59.80\n62.61\n63.67\n65.61\n65.77\n68.07\n\n36\n54.86\n55.91\n55.32\n58.71\n63.67\n65.32\n66.68\n67.36\n\n9\n46.84\n47.04\n48.80\n51.97\n63.02\n64.83\n65.38\n66.77\n\n1\n33.78\n33.68\n36.05\n38.92\n61.53\n63.21\n63.37\n64.57\nSince the nested behavior of Matryoshka visual tokens is learned within the CLIP visual encoder, we next evaluate whether it is necessary to also finetune the LLM. Shown in Table 7  ###reference_###, training the whole LLM achieves better performance. This demonstrates that by also training the LLM, the model can better adapt to the patterns of the visual tokens distributed in the Matryoshka manner.\nAs explained in Sec. 3  ###reference_### and 4.1  ###reference_###, we (a) initialize the LLM weights from LLaVA and (b) minimize the loss averaged upon all visual token scales for each sample during training. An alternative choice is to randomly sample a visual token scale. Shown in Table 8  ###reference_###, initializing the LLM weights from LLaVA and minimizing the losses over all scales shows consistent performance boost compared to using the vanilla text-only pre-trained LLM weights [8  ###reference_b8###] and randomly selecting a visual token scale. Initializing the LLM weights from LLaVA makes the training process of M3 more stable. By learning all scales at once, the model is forced to learn the nested behavior for each sample, which leads to better performance."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment Settings",
            "text": "We use LLaVA-1.5 [5  ###reference_b5###  ###reference_b5###] and LLaVA-NeXT [4  ###reference_b4###  ###reference_b4###] as the base LMMs, both with Vicuna 7B as the language model backbone. We finetune the whole model using the exact visual instruction data from LLaVA-1.5 and LLaVA-NeXT, respectively. The learning rate of LLM is  and , respectively for LLaVA-1.5 and LLaVA-NeXT. The learning rate for the visual encoder is  for both models. We train both models for 1 epoch using 8 NVIDIA H100 GPUs.\nInstead of training the language model from scratch, we initialize the language model weights from pre-trained LLaVA-1.5 and LLaVA-NeXT, which we empirically works better. We name our Matryoshka Multimodal Models LLaVA-1.5-M3 and LLaVA-NeXT-M3.\nWe design 5 scales for the visual tokens. LLaVA-1.5 [5  ###reference_b5###  ###reference_b5###] and LLaVA-NeXT [4  ###reference_b4###  ###reference_b4###] both leverage CLIP-ViT-L-336 [29  ###reference_b29###  ###reference_b29###] as the visual encoder, where an image is embedded into  visual tokens. We gradually apply  pooling with stride 2, resulting in , and  visual tokens, where we finally apply a  pooling to get the final single visual token. Therefore, the size of Matryoshka visual token sets are , following a nested manner. The efficiency anlaysis on the system level is shown in Appendix B  ###reference_###  ###reference_###, where M3 boosts the speed of the LMM prefill process through diminished floating-point operations (FLOPs) and lessens computational memory requirements.\nFor image understanding, we evaluate LLaVA-1.5 and LLaVA-NeXT on (a) diverse multimodal benchmarks: POPE [22  ###reference_b22###  ###reference_b22###], GQA [46  ###reference_b46###  ###reference_b46###], MMBench [23  ###reference_b23###  ###reference_b23###], VizWiz [47  ###reference_b47###  ###reference_b47###], SEEDBench [48  ###reference_b48###  ###reference_b48###], ScienceQA [49  ###reference_b49###  ###reference_b49###], MMMU [50  ###reference_b50###  ###reference_b50###], and (b) document understanding/Optical character recognition (OCR) benchmarks: DocVQA [51  ###reference_b51###  ###reference_b51###], ChartQA [25  ###reference_b25###  ###reference_b25###], AI2D [52  ###reference_b52###  ###reference_b52###] and TextVQA [24  ###reference_b24###  ###reference_b24###].\nFor video understanding, we use both (a) open ended video question answering benchmarks evaluated by GPT-3.5: MSVD-QA [53  ###reference_b53###  ###reference_b53###], MSRVTT-QA [53  ###reference_b53###  ###reference_b53###] and ActivityNet-QA [54  ###reference_b54###  ###reference_b54###]; and (b) multi-choice video question answering benchmarks: NExT-QA [55  ###reference_b55###  ###reference_b55###], IntentQA [56  ###reference_b56###  ###reference_b56###], and EgoSchema [57  ###reference_b57###  ###reference_b57###]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Image Understanding",
            "text": "We evaluate LLaVA-1.5-M3 on the common multimodal understanding and reasoning benchmarks. Results are shown in Table 1  ###reference_###  ###reference_###. LLaVA-1.5-M3 with full tokens maintains the performance of LLaVA-1.5 across diverse benchmarks. More importantly, our approach shows strong performance even with 1 or 9 tokens. Specifically, in MMBench, a comprehensive multimodal understanding benchmark, LLaVA-1.5-M3 with 9 tokens surpasses Qwen-VL-Chat with 256 tokens, and achieves similar performance as Qwen-VL-Chat with even 1 token. Compared with InstructBLIP [58  ###reference_b58###  ###reference_b58###], LLaVA-1.5M3 with 9 tokens surpasses InstructBLIP-7B and InstructBLIP-13B across all benchmarks. This demonstrates that our model has both flexibility and strong empirical performance under diverse number of visual tokens.\nApproach\n\n# Tokens\nMMBench\nGQA\nPOPE\nVizWiz\nSEEDBench\n\n\n\nQwen-VL [7  ###reference_b7###  ###reference_b7###]\n\n256\n38.2\n59.3\n-\n35.2\n56.3\n\n\n\nQwen-VL-Chat [7  ###reference_b7###  ###reference_b7###]\n\n256\n60.6\n57.5\n-\n38.9\n58.2\n\n\n\nInstructBLIP-7B [58  ###reference_b58###  ###reference_b58###]\n\n32\n36.0\n49.2\n-\n34.5\n53.4\n\n\n\nInstructBLIP-13B [58  ###reference_b58###  ###reference_b58###]\n\n32\n-\n49.5\n78.9\n33.4\n-\n\n\n\nLLaVA-1.5-7B [5  ###reference_b5###  ###reference_b5###]\n\n576\n64.8\n62.0\n85.9\n54.4\n60.5\n\n\n\nLLaVA-1.5-\n\n576\n65.9\n61.9\n87.4\n54.9\n60.6\n\n\n144\n66.4\n61.3\n87.0\n53.1\n59.7\n\n\n36\n64.8\n60.3\n85.5\n52.8\n58.0\n\n\n9\n63.1\n58.0\n83.4\n51.9\n55.4\n\n\n1\n59.5\n52.6\n78.4\n49.4\n50.1\nWe use the proposed Matryoshka Multimodal Models to finetune LLaVA-NeXT, and compare LLaVA-NeXT-M3 with SS, which denotes the setting where the LLaVA-NeXT is trained under a Specific Scale of visual tokens also for 1 epoch. We also include the oracle upperbound performance. Specifically, ‘Oracle’ denotes the case where the best tradeoff between visual tokens and performance is picked for each test instance. Specifically, for each test instance, we select the the scale with the fewest amount of tokens but can answer the question correctly. Results are shown in Table 2  ###reference_###  ###reference_###. Our approach, M3, is at least as good as SS, while performing better on tasks such as document understanding (TextVQA and ChartQA) and common benchmarks such as MMBench [23  ###reference_b23###  ###reference_b23###].\n# Tokens Per Grid\n\nApproach\nTextVQA\nAI2D\nChartQA\nDocVQA\nMMBench\nPOPE\nScienceQA\nMMMU\n\n\n\n\n\n576\n\nSS\n64.53\n64.83\n59.28\n75.40\n66.58\n87.02\n72.29\n34.3\n\n\n\n63.13\n66.71\n58.96\n72.61\n67.96\n87.20\n72.46\n34.0\n\n\n\n144\n\nSS\n62.16\n65.77\n55.28\n67.69\n67.78\n87.66\n72.15\n36.4\n\n\n\n62.61\n68.07\n57.04\n66.48\n69.50\n87.67\n72.32\n36.1\n\n\n\n36\n\nSS\n58.15\n65.90\n45.40\n56.89\n67.01\n86.75\n71.87\n36.2\n\n\n\n58.71\n67.36\n50.24\n55.94\n68.56\n87.29\n72.11\n36.8\n\n\n\n9\n\nSS\n50.95\n65.06\n37.76\n44.21\n65.29\n85.62\n72.37\n36.8\n\n\n\n51.97\n66.77\n42.00\n43.52\n67.35\n86.17\n71.85\n35.2\n\n\n\n1\n\nSS\n38.39\n63.76\n28.96\n33.11\n61.43\n82.83\n72.32\n35.3\n\n\n\n38.92\n64.57\n31.04\n31.63\n62.97\n83.38\n71.19\n34.8\n\n\n\nOracle\n\n# Tokens\n31.39\n11.54\n41.78\n64.09\n8.90\n6.08\n7.43\n22.85\n\n\nPerformance\n70.51\n76.36\n70.76\n81.73\n74.35\n94.29\n76.07\n50.44\nOur results also show that dataset level biases towards the visual token scales do exist. For example, ScienceQA maintains consistent performance across all visual token scales. AI2D and MMBench only encounter a small performance drop for even as few as 9 to 1 tokens. On the other hand, dense visual perception tasks such as TextVQA and DocVQA show a significant performance with fewer tokens. This analysis shows that M3 could serve as a framework to analyze the granularity that a benchmark needs.\nFurthermore, there is a large gap between the model’s actual performance under full tokens and the upper-bound oracle. This indicates that using full tokens cannot always result in the optimal performance for all samples; i.e., there is a large room of improvement towards the oracle point."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Video Understanding",
            "text": "Following IG-VLM [59  ###reference_b59###], we directly conduct zero-shot inference on diverse video benchmarks using LLaVA-NeXT-M3. Specifically, 6 frames are uniformly sampled over the entire video, then arranged as a collage, which is fed into LLaVA-NeXT along with the question to get the response. Results under LLaVA-NeXT-M3 and recent video LMMs are show in Table 3  ###reference_###.\nLLaVA-NeXT-M3 with full visual tokens again shows comparable performance with LLaVA-NeXT. More interestingly, results indicate that full visual tokens usually do not lead to the best performance in video understanding tasks. Specifically, on 4 out of 6 benchmarks, full visual tokens show less desirable performance compared to 720 or 180 visual tokens. We suspect that very long visual context could bring distraction (e.g., too much focus on potentially irrelevant background) to the model’s prediction, where a compact representation of the video focusing on the more relevant information may be more advantageous.\nFinally, for most video understanding tasks such as ActivityNet, IntentQA and EgoSchema, with 9 tokens per image grid (45 tokens in total), the accuracy difference compared to full tokens (2880 in total) is less than 1%. This demonstrates that the video questions in these benchmarks usually require very sparse visual information, as the source of such video understanding benchmarks mostly comes from natural scenes, which matches our observation in image understanding benchmarks.\nApproach\n\n# Tokens\nMSVD\nMSRVTT\nActivityNet\nNextQA\nIntentQA\nEgoSchema\n\n\n\n\n\nVideo-LLaMA [60  ###reference_b60###]\n\n-\n51.6\n29.6\n12.4\n-\n-\n-\n\n\n\nLLaMA-Adapter [61  ###reference_b61###]\n\n-\n54.9\n43.8\n34.2\n-\n-\n-\n\n\n\nVideo-ChatGPT [62  ###reference_b62###]\n\n-\n64.9\n49.3\n35.2\n-\n-\n-\n\n\n\nVideo-LLaVA [63  ###reference_b63###]\n\n2048\n70.7\n59.2\n45.3\n-\n-\n-\n\n\n\nInternVideo [64  ###reference_b64###]\n\n-\n-\n-\n-\n59.1\n-\n32.1\n\n\n\nLLaVA-NeXT-7B [4  ###reference_b4###]\n\n2880\n78.8\n63.7\n54.3\n63.1\n60.3\n35.8\n\n\n\nLLaVA-NeXT-7B-M3\n\n2880\n78.2\n64.5\n53.9\n63.1\n58.8\n36.8\n\n\n720\n79.0\n64.5\n55.0\n62.6\n59.6\n37.2\n\n\n180\n77.9\n63.7\n55.0\n61.4\n59.3\n37.6\n\n\n45\n75.8\n63.0\n53.2\n59.5\n58.7\n38.8\n\n\n5\n73.5\n62.7\n50.8\n56.5\n56.7\n36.2"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "In-depth Analysis",
            "text": "A simple way to reduce the number of visual tokens via a training-free way is to conduct heuristic token merging or reduction. In Table 4  ###reference_###  ###reference_###, we compare M3 with three training-free approaches: average pooling, spatial sampling, and sequential sampling. M3 is much more resilient when the number of tokens decreases, while the heuristic based sampling approaches show dramatic performance drop. A visualization of the spatial and sequential sampling is shown in Figure 5  ###reference_###  ###reference_###.\n# Tokens\n\n\n\nM3\n\nAverage Pooling\nSpatial Sampling\nSequential Sampling\n\n\n\n\n\n576\n\n\n\n67.96\n\n67.18\n67.18\n67.18\n\n\n\n144\n\n\n\n69.50\n\n61.68\n65.81\n60.14\n\n\n\n36\n\n\n\n68.56\n\n50.77\n60.05\n44.76\n\n\n\n9\n\n\n\n67.35\n\n45.45\n45.45\n31.96\n\n\n\n1\n\n\n\n62.97\n\n19.33\n26.29\n22.42\nWe extract the response from LLaVA-NeXT-M3 in the TextVQA benchmark, and show the samples where using visual tokens across different scales can answer the question correctly and incorrectly. Shown in Figure 4  ###reference_###  ###reference_###, the OCR performance aligns with the complexity of the images, which indicates that M3 can be utilized as a metric towards sample level complexity.\n###figure_6### As shown in Table 2  ###reference_###  ###reference_###, the oracle upper-bound can use very few () tokens yet achieve at least 10% better performance compared to full visual tokens. This suggests that a visual token scale predictor, where the model learns to automatically select the best visual token scale given the input images or both input images and questions, has potential to achieve a better tradeoff. This would be interesting future work.\nHere we extend the length of the visual tokens at inference time to study the model’s zero-shot generalization behavior. Results under LLaVA-NeXT are shown in Table 5  ###reference_###  ###reference_###. Here LLaVA-NeXT-M3 is trained on  image grids but evaluated on  grids. We set the number of visual tokens to be 144 in each image during evaluation. The model obtains a significant improvement in document understanding by 2.12, 1.80, and 4.11 on TextVQA, ChartQA, and DocVQA, respectively, while maintaining the same performance on benchmarks mainly composed of natural scene images.  image grids with 144 tokens per grid own 1440 tokens, yet achieve similar performance with the default LLaVA-NeXT  image grids with 2880 total tokens (576 tokens per grid). This indicates it is promising to feed more subimages while making the number of visual tokens within each subimage much smaller.\n# Grids\n# Tokens per grid\nOverall # Tokens\nTextVQA\nAI2D\nChartQA\nDocVQA\nMMBench\nPOPE\nScienceQA\n\n\n\n\n144\n720\n62.61\n68.07\n57.04\n66.48\n69.50\n87.67\n72.32\n\n\n144\n1440\n64.73\n67.75\n58.84\n70.59\n69.50\n87.67\n72.22\n\n\n576\n2880\n63.13\n66.71\n58.96\n72.61\n67.96\n87.20\n72.46\n###figure_7###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "We ablate the key designs in M3, including the sampling method of Matryoshka visual tokens, and training strategy.\nHere we compare three different ways to select the visual tokens for Matryoshka Multimodal Models, including average pooling, spatial sampling, and sequential sampling, which is illustrated in Figure 5  ###reference_###  ###reference_###. Shown in Table 6  ###reference_###  ###reference_###, averaging pooling shows better performance than the two alternatives across diverse benchmarks. In general, sequential sampling performs the worst. We hypothesize that this is due to the visual tokens having spatial information, while sequential sampling does not naturally align with the spatial distribution of the visual tokens.\nTextVQA\nMMBench\nAI2D\n\nNum of Vis Tokens\nAvg Pooling\nSequential\nSpatial\nAvg Pooling\nSequential\nSpatial\nAvg Pooling\nSequential\nSpatial\n\n\n\n576\n63.13\n59.37\n60.45\n67.96\n64.60\n64.43\n66.71\n65.61\n64.96\n\n144\n62.61\n55.80\n58.33\n69.50\n64.18\n64.52\n68.07\n64.90\n64.96\n\n36\n58.71\n52.79\n52.39\n68.56\n63.92\n64.69\n67.36\n64.51\n64.02\n\n9\n51.97\n44.05\n44.19\n67.35\n63.14\n62.11\n66.77\n63.70\n63.92\n\n1\n38.92\n28.03\n29.91\n62.97\n59.36\n57.47\n64.57\n63.21\n63.08\nNum of Vis Tokens\nTextVQA\nMMBench\nAI2D\nDocVQA\n\n\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\nw/ LLM\nw/o LLM\n\n\n\n576\n63.13\n61.16\n67.96\n63.66\n66.71\n63.92\n72.61\n69.15\n\n144\n62.61\n57.79\n69.50\n65.21\n68.07\n63.73\n66.48\n59.77\n\n36\n58.71\n49.75\n68.56\n63.92\n67.36\n62.89\n55.94\n44.08\n\n9\n51.97\n36.15\n67.35\n61.08\n66.77\n62.05\n43.52\n28.36\n\n1\n38.92\n19.72\n62.97\n51.80\n64.57\n60.59\n31.63\n17.37\nTechnique\nTextVQA\nAI2D\n\nInit LLM weights from LLaVA\n\n✓\n\n✓\n\n✓\n\n✓\n\nAverage losses over all scales\n\n\n✓\n✓\n\n\n✓\n✓\n\n576\n60.36\n62.25\n61.01\n63.13\n62.40\n65.06\n65.84\n66.71\n\n144\n59.61\n61.02\n59.80\n62.61\n63.67\n65.61\n65.77\n68.07\n\n36\n54.86\n55.91\n55.32\n58.71\n63.67\n65.32\n66.68\n67.36\n\n9\n46.84\n47.04\n48.80\n51.97\n63.02\n64.83\n65.38\n66.77\n\n1\n33.78\n33.68\n36.05\n38.92\n61.53\n63.21\n63.37\n64.57\nSince the nested behavior of Matryoshka visual tokens is learned within the CLIP visual encoder, we next evaluate whether it is necessary to also finetune the LLM. Shown in Table 7  ###reference_###  ###reference_###, training the whole LLM achieves better performance. This demonstrates that by also training the LLM, the model can better adapt to the patterns of the visual tokens distributed in the Matryoshka manner.\nAs explained in Sec. 3  ###reference_###  ###reference_### and 4.1  ###reference_###  ###reference_###, we (a) initialize the LLM weights from LLaVA and (b) minimize the loss averaged upon all visual token scales for each sample during training. An alternative choice is to randomly sample a visual token scale. Shown in Table 8  ###reference_###  ###reference_###, initializing the LLM weights from LLaVA and minimizing the losses over all scales shows consistent performance boost compared to using the vanilla text-only pre-trained LLM weights [8  ###reference_b8###  ###reference_b8###] and randomly selecting a visual token scale. Initializing the LLM weights from LLaVA makes the training process of M3 more stable. By learning all scales at once, the model is forced to learn the nested behavior for each sample, which leads to better performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We introduced M3: Matryoshka Multimodal Models, which learns to represent visual content as\nnested sets of visual tokens, capturing information across multiple coarse-to-fine granularities. LMMs equipped with M3 afford explicit control over the visual granularity per test instance during inference.\nWe also showed that M3 can serve as an analysis framework to investigate the visual granularity needed for existing datasets, where we discovered that a large number of multimodal benchmarks only need as few as  9 visual tokens to obtain accuracy similar to that of using all visual tokens, especially for video understanding. Furthermore, we disclosed a large performance-efficiency gap between the oracle upper-bound and the model’s performance.\nOur work can be naturally extended to other domains. For example, the long context in a text-only LLM or vision tokens in dense vision tasks can also be represented as nested sets of tokens in a Matryoshka manner. One limitation of our current approach is that we are lacking an effective visual token predictor that can bridge the gap between the oracle and LMM’s actual performance at a specific scale. We believe this would be an exciting next direction of research in this space."
        }
    ]
}