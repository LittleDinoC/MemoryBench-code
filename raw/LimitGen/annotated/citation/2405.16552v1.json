{
    "title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
    "abstract": "Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries.\nThese methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work.\nMany chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs’ generation.\nThis paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation.\nAnalogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points.\nExperimental results across various tasks using different LLMs demonstrate SED’s effectiveness.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) are equipped with rich word knowledge, relying on decoding methods to express their abilities during the inference phase.\nExisting decoding methods are mainly autoregressive and are primarily classified into two categories (Ippolito et al., 2019  ###reference_b14###). Firstly, search-based methods aim to maximize the probability of generated text, such as greedy search and beam search. The other is sampling-based methods, which sample the next generated token based on a probability distribution, such as nuclear sampling.\nDespite good results, these autoregressive decoding methods for sequential token generation have limitations.\nLLMs often encounter highly uncertain tokens, referred to as chaotic points in this paper, during decoding.\nFigure 1  ###reference_### shows the token probability distribution at a chaotic point.\nThe probability mass is quite dispersed, which suggests that the model does not know which token to choose.\nOn the other hand, Answer 1 and Answer 2 in Figure 1  ###reference_### have the same content before the chaotic point but with different conclusions.\nAt the chaotic point, Answer 1 selects token ‘8’, resulting in an incorrect response, whereas Answer 2 chooses the token ‘(’, leading to a correct one.\nDifferent token choices lead to vastly different results, indicating that tokens at chaotic positions should be carefully selected.\nHowever, greedy search only considers the token with the highest probability, ignoring other tokens with considerable probability mass.\nSampling is easily influenced by randomness and falling into irrelevant tokens.\nThough beam search can try multiple tokens, it only considers sentence-level probability maxima.\nIn Figure 1  ###reference_###, Answer 1 has a larger sentence-level probability and a lower perplexity than Answer 2, so beam search still fails to respond correctly.\nThe case in Figure 1  ###reference_### implies that the model’s incorrect answer may not be due to its lack of relevant knowledge but because it does not know how to choose an optimal token at the chaotic point.\nLLMs should not only take the probabilities into account when selecting the token at a chaotic point.\nHowever, existing autoregressive decoding methods are limited to probability, not considering selecting the token from a holistic perspective.\nThis could easily lead to inappropriate tokens and thus result in error propagation.\nTherefore, it is necessary to propose a more advanced decoding method to improve LLMs’ token selection at chaotic points.\nWhen making uncertain decisions, humans often speculate first and then evaluate the speculations, compare them, and make the final choice (Kahneman and Tversky, 2013  ###reference_b17###), a common strategy called slow thinking (Kahneman, 2011  ###reference_b16###) people use when dealing with complex decisions.\nThis approach can assist LLMs in selecting appropriate tokens at chaotic points, but it requires LLMs to be equipped with evaluation ability.\nHowever, Luo et al. (2023  ###reference_b22###) suggest the inherent evaluation ability of existing LLMs is weaker than their generation ability.\nRecently, many studies (Li et al., 2023a  ###reference_b20###; Zhu et al., 2023  ###reference_b42###; Ke et al., 2023  ###reference_b18###) have shown that LLMs can significantly enhance their evaluation ability and thus achieve self-correction (Han et al., 2024  ###reference_b12###) through training.\nIn this paper, we propose Self-Evaluation Decoding, SED, a decoding method for improving uncertain tokens selection at chaotic points for LLMs.\nMirroring human decision-making, when encountering a chaotic point during decoding, LLMs first speculate the results of choosing each token, then evaluate them, and finally select the token with the highest propensity score based on evaluations.\nOur main contributions are summarized as follows:\nWe propose straightforward criteria that help LLMs efficiently identify chaotic points and thus carefully select key tokens during decoding.\nWe propose Self-Evaluation Decoding (SED). Analogous to human decision-making, SED enables LLMs to speculate and evaluate uncertain tokens at chaotic points to select the optimal token and thus improve the generation’s quality compared to other standard decoding methods.\nWe design a simple but effective data synthesis strategy for enhancing LLMs’ evaluation ability based on multi-model ensembling, achieving self-evaluation in SED."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Enhancing evaluation ability for LLMs. Recent studies (Fu et al., 2023  ###reference_b10###; Luo et al., 2023  ###reference_b22###; Zheng et al., 2023  ###reference_b41###) reveal that LLMs’ evaluation ability can be seen as the ability to assess the quality of the object being evaluated (usually a question-answer pair). This ability is determined by the accuracy of the feedback generated by the evaluation model denoted as . The feedback is usually one rating (scalar), analysis (natural language text), or a combination of both. The more accurate the feedback, the better the evaluation. The approaches to enhancing LLM’s evaluation ability can be mainly categorized into prompt-based and tuning-based. The former designs well-structured prompts with detailed instructions to guide the model to evaluate. Since this approach relies entirely on the model’s inherent capability, closed-source models such as GPT-4 (OpenAI, 2023  ###reference_b23###) and PaLM 2 (Anil et al., 2023b  ###reference_b4###) are usually adopted as evaluation models, making the evaluation more expensive. However, being untrained, this method is difficult to adapt to specific downstream tasks and is greatly limited by the model’s inherent flaws (Wang et al., 2023a  ###reference_b30###; Luo et al., 2023  ###reference_b22###). The latter builds training datasets consisting of evaluation feedback to inject evaluation abilities into open-source LLMs through finetuning explicitly (Wang et al., 2023c  ###reference_b32###; b  ###reference_b31###). Many studies have demonstrated that the evaluation models built on direct training can achieve near-human evaluation quality in dialog, open-scene QA, fact verification, and other scenarios. These are effective and low-cost solutions for building evaluation models. In this work, we organically combine the LLMs’ evaluation ability with the other inherent ability, so that LLMs can not only generate but also evaluate the text it generates. Specifically, we enhance the model the evaluation ability through training over the augmented dataset with evaluation feedback included. We will talk about the details in Section 3.4  ###reference_###. Decoding methods for LLMs. Recent studies (Zhao et al., 2023  ###reference_b40###; Xia et al., 2024  ###reference_b34###) have focused on how to improve decoding methods for LLMs further. They can be partitioned into two types: acceleration-oriented and quality-oriented. The former focuses on accelerating the generation of LLMs from an algorithmic perspective. One representative work is speculative decoding (Xia et al., 2024  ###reference_b34###), whose core idea is to use lightweight models with much lower decoding overhead for generation, and heavy-weight models to verify the generated tokens. In this way, the parallelism of decoding substantially increases thus realizing acceleration. The latter focuses on enhancing the quality of LLMs’ generation (Chuang et al., 2023  ###reference_b7###; Chen et al., 2024  ###reference_b6###; Roy et al., 2024  ###reference_b27###). The main operation is to adjust the unnormalized logits during decoding in light of some signals or criteria. For instance, Li et al. (2023b  ###reference_b21###) enhances generation diversity by contrasting the logits of the stronger model over the weaker one. Wang (2024  ###reference_b29###) guides the model to sample correct tokens in the decoding phase by introducing constraints to make the model output more parsable to the computer. Huang et al. (2024  ###reference_b13###) acquires reward signals based on formatted programs or parametric models and then makes the decoding more controllable. In contrast, we utilize signals from the model itself to guide decoding, thereby leveraging LLMs’ evaluation ability to improve the generation. Recent advancements in AI ethics emphasize the importance of fairness in machine learning algorithms (Binns, 2023  ###reference_b5###; Crawford, 2023  ###reference_b8###). Addressing biases inherent in training data is crucial for equitable AI systems. Strategies include algorithmic transparency and rigorous fairness audits to mitigate systemic discrimination (Barocas et al., 2023  ###reference_b3###). Such measures are pivotal for responsible AI development."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Self-evaluation decoding",
            "text": "While previous standard left-to-right decoding methods treat all tokens equally and employ a uniform policy, our proposed self-evaluation decoding (SED) emphasizes the tokens generated at chaotic points, yielding better responses.\nThis paper primarily addresses three critical questions:\n(1) How can a chaotic point be identified?\n(2) Which tokens could be considered at the chaotic point?\n(3) Which token is the most suitable choice at the chaotic point?\nTo this end, our SED method encompasses three modules: chaotic point detection, speculation and evaluation, and feedback backpropagation.\nAutoregressive decoding generates tokens sequentially.\nSpecifically, consider a sequence of generated tokens , where  represents the total number of tokens.\nEach token (with ) is generated from the distribution , which is conditioned on the input  and the preceding tokens , over the whole vocabulary .\nDuring decoding, SED examines the conditional distribution  and calculates the indicator  to determine whether the token  is hard for the model to predict.\nIf ,  is selected through default decoding method (e.g. greedy search).\nIf , the position  is identified as a chaotic point.\nSED selects the top  tokens from  for speculation and allows the model to evaluate these speculations.\nThe model’s self-evaluation is backpropagated and combined with  to guide the selection of the optimal token at position .\nBy iteratively repeating this process, LLMs generate improved responses.\nThe full SED pipeline is shown in Algorithm 1  ###reference_###.\nFigure 2  ###reference_### shows what happens at a chaotic point in one iteration.\nFurther details will be elaborated in subsequent sections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Chaotic point detection",
            "text": "Entropy-based. Entropy is commonly used to describe the complexity or uncertainty in information theory.\nDuring autoregressive decoding, the model generates the probability distribution  over vocabulary , with the entropy defined as\nis the -th token in .\nWe can use  to measure the model’s uncertainty at position .\nRatio-based. Another more intuitive measurement for uncertainty is the probability ratio:\nand  are the maximum and second-largest probabilities in , with  and  being the corresponding tokens.\nIt follows the clear idea that the model is at least unsure which token between  and  to take at position  when  is close to .\nAfter obtaining  and , we can determine whether position  is a chaotic point.\nDefine indicator  as\nand  are the uncertainty thresholds for entropy-based and ratio-based detection, respectively.\nPosition  is considered a chaotic point for the model if .\nFigure 2  ###reference_### intuitively demonstrates how this operates.\nNote that, we set a branching factor  as the upper limit for the number of detected points to control the algorithm’s complexity."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Speculation and evaluation",
            "text": "If position  is chaotic, we need to speculate first.\nSpecifically, we select top  tokens\n\nwith the highest probabilities from distribution .\nThese tokens are appended to the existing context  to form  new contexts , differing only in the last token .\nThe model then completes these  contexts, resulting in  responses .\nFor evaluation, the model itself evaluates .\nAs we focus on the QA scenario, the evaluation feedback specifically refers to the likelihood of responses being considered correct.\nFormally, given the primal user question  and , the model judge ’s correctness as .\nwe extract the model’s output logits and calculate the self-evaluation probability\nThrough , we can tell the model confidence on the speculation  and further determine the token selection propensity at position .\nConsidering the inference cost, the model adopts standard decoding methods during speculation.\nDetails about  are presented in Appendix A  ###reference_###.\nWe will discuss the effects of different methods on speculation in Section 4.6  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Feedback backpropagation",
            "text": "After Speculation and Evaluation, we get  for  potential candidates .\nBack to position , we fuse\n\nwith the token probabilities\n\nwhich are what we already get during decoding.\n can be viewed as the model’s expected benefit from choosing , while  represents the model’s confidence in choosing  under the current conditions.\n focuses on the future, whereas  focuses on the present.\nWe introduce the concept of propensity score by combining the two to assist the model in choosing the optimal token at the chaotic point.\nFormally, for , we define its propensity score as\nHere,  is the fusion coefficient controlling the fusion between the evaluation feedback and token probability. The higher , the better bidirectional feature token  has. Finally, we select the token with the highest propensity score for the subsequent generation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation ability enhancing",
            "text": "Speculation and Evaluation and Feedback Backpropagation represent the forward and backward information flow, respectively.\nIn the former phase, the model reveals the effect of the selected token on the final generation through deduction and evaluation.\nThe model’s self-evaluation feedback then, in turn, improves the token selection at the chaotic point.\nEssentially, self-evaluation decoding enhances the model’s generation through its evaluation.\nPerceptibly, accurate feedback is quite important for self-evaluative decoding to enhance generation.\nRecent studies (Li et al., 2023a  ###reference_b20###; Xu et al., 2024  ###reference_b35###) have shown LLMs can be trained to build good evaluation ability.\nHere we propose a simple but effective data augmentation approach to efficiently construct instruction finetuning data to empower the model’s evaluation ability.\nMulti-model generation ensembling.\nGiven a downstream task dataset  and a collection of LLMs , we use each model in  to answer each question in , which in turn yields model responses  in a CoT format (Wei et al., 2022  ###reference_b33###).\nWe compare these generated responses with the question answers to obtain the final augmented dataset , where the judgment  denotes the correctness of the response .\nIn this way, each  in  receives multiple, CoT-formatted responses from different LLMs with different correctnesses.\nFurthermore, we extract all the correct responses in  thereby constructing the dataset .\n and  are used to train the model’s evaluation and generation abilities on the downstream tasks, respectively.\nTraining.\nSimilar to previous studies, we use  to train the model’s evaluation ability.\nSpecifically, the model learns to output  given  and :\nrepresents the model’s parameters.\nThe model’s generation ability is developed by the loss function:\nHere, we adopt multi-task learning to train the model to build the generation and evaluation abilities simultaneously:\nData sampled from  and  respectively and the ratio is maintained around 1:1.\nThey are mixed to build the final training set.\nMeanwhile, the sampled data from  has approximately the same number of samples with  and . More details about  are available in Appendix A  ###reference_###.\nWe build the dataset described above and train the model in a multi-task learning manner.\nThis method enhances the model’s abilities in two ways.\nBy integrating multiple model generations, representing diverse thoughts on the same question, the model can learn the nuanced differences and thus know how to analyze the question from several perspectives.\nMeanwhile, recent studies (Yu et al., 2023  ###reference_b39###; An et al., 2023  ###reference_b2###) have shown that understanding a question from the opposite is an effective way to enhance the model’s comprehension of the downstream tasks.\nTherefore, learning to judge the correctness of an answer from two sides should also be an effective way to boost the model’s abilities."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We use three datasets: (i) the HotpotQA subset (Yang et al., 2018  ###reference_b37###), processed into the SQuAD format, from the MRQA 2019 Shared Task (Fisch et al., 2019  ###reference_b9###). Retrieval is not needed, allowing LLMs to focus on answering questions in light of the references;\n(ii) XieZhi (Gu et al., 2024  ###reference_b11###), a comprehensive evaluation suite with a multiple-choice format, aiming to test diverse subject knowledge of LLMs;\nand (iii) GSM8K (Cobbe et al., 2021  ###reference_b8###), a popular mathematical reasoning benchmark for evaluating LLMs’ mathematical abilities, consists of high-quality elementary school math word problems.\nThey cover diverse application scenarios of LLMs, such as multi-hop reasoning, multi-disciplinary knowledge comprehension, and mathematical reasoning."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines and metrics",
            "text": "We choose greedy search, beam search, and nuclear sampling as our baselines.\nThey are widely used decoding methods in LLM inference.\nWe also compare with DoLa (Chuang et al., 2023  ###reference_b7###), a contrastive decoding that alleviates hallucinations. falcon-7b-instruct (Penedo et al., 2023  ###reference_b24###), llama2-7b-chat-hf (Touvron et al., 2023  ###reference_b28###), and gemma-7b-it (Anil et al., 2023a  ###reference_b3###) are trained on all datasets through supervised fine-tuning described in Section 3.4  ###reference_###.\nWe evaluate the performance of our proposed self-evaluation decoding by comparing it with other decoding baselines across different fine-tuned models.\nDue to the official open-source code of DoLa currently only supporting the Llama series models, we use it as a baseline only on Llama2.\nWe compare the answers extracted from the model responses with the annotations to calculate accuracy, thereby measuring the performance of different decoding methods.\nWe uniformly sample 1,000 instances for testing from each dataset to manage experimental costs."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental settings",
            "text": "As mentioned in Section 3.4  ###reference_###, we employ multiple LLMs to augment downstream datasets, including baichuan2-7b-chat (Yang et al., 2023  ###reference_b36###), qwen-7b-chat (Bai et al., 2023  ###reference_b5###), mistral-7b-instruct-v0.2 (Jiang et al., 2023  ###reference_b15###) and metamath-mistral-7b (Yu et al., 2023  ###reference_b39###) (specifically for GSM8K).\nPrompt details can be found in Appendix B  ###reference_###.\nFor every question in each dataset, we sample multiple responses from each model and filter out duplicate and incorrectly formatted responses. We extract the answers through regular expressions from those retained responses and compare them with annotations to determine their correctness.\nFinally, according to the operations described in Section 3.4  ###reference_###,  and  are obtained.\nWe train the models on 2 A800 GPUs using ZeRO (Rajbhandari et al., 2020  ###reference_b25###) stage 3 from DeepSpeed (Rasley et al., 2020  ###reference_b26###).\nWe adopt AdamW (Kingma and Ba, 2015  ###reference_b19###) as the optimizer and set the batch size to 8 with a maximum sequence length of 1,024 and one training epoch.\nThe number of beams  of beam search and the temperature  and  of nucleus sampling are set to  and , respectively.\nMore hyperparameters for the experiment are reported in Appendix D  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Main results",
            "text": "Table 1  ###reference_### presents the three LLMs’ results that employ various decoding methods.\nBeam search and nuclear sampling explore a wider solution space than greedy search, and nuclear sampling increases diversity.\nHowever, compared to greedy search, nuclear sampling performs poorly with large fluctuations, indicating nuclear sampling is not a good choice in QA scenarios with standard answers.\nOn the other hand, beam search is superior to greedy search in most cases, suggesting that a larger solution space helps to get a more accurate response.\nAlleviating hallucination seems not helpful, since DoLa performs poorly consistently.\nBoth entropy-based and ratio-based SED outperform other baselines across multiple models and datasets,\nespecially for falcon and llama2 on XieZhi and GSM8K.\nAs shown, in most cases, the ratio-based SED performs slightly better than the entropy-based.\nWe attribute this to the former’s more specific constraints on token probabilities, which help avoid misjudging chaotic points.\nAdditionally, adjusting the ratio threshold is much more convenient than the entropy threshold.\nThus, ratio-based SED is more practical compared to entropy-based.\nIn subsequent experiments, ratio-based SED is the default implementation.\nWe observe that the improvement in model performance for SED, relative to the baseline models, varies across different datasets.\nSpecifically, the performance increment is more pronounced in the XieZhi and GSM8K compared to HotpotQA.\nChaotic points in GSM8K often occur at numbers and operators, whereas in XieZhi, they frequently appear at the options, as shown in Table 6  ###reference_### and Table D  ###reference_###.\nThe impact of these chaotic points on the accuracy of model responses is significant.\nSED allows the model to effectively select tokens at these positions, thereby avoiding incorrect answers.\nIt is important to note that when  is incorporated into the training process, models’ accuracies increase to varying extents.\nThis suggests that enhancing a model’s evaluation ability could indirectly improve its generation ability, which may be related to the introduction of incorrect responses in  to enable the model to learn how to identify flaws and thereby enhance its ability from the opposite perspective (An et al., 2023  ###reference_b2###).\nMore cases that demonstrate SED’s effect are available in Appendix C  ###reference_###."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Analysis of evaluation quality",
            "text": "Intuitively, the evaluation quality heavily influences the model’s final response.\nIdeally, if the evaluation quality is sufficiently high, the model can generate an optimal response by completely following the evaluation feedback whenever it encounters a chaotic point.\nTo investigate the effect of evaluation quality, we adopt other stronger LLMs as the speculation evaluators to generate more accurate evaluation feedback.\nThe results are shown in Table 2  ###reference_###.\nIn addition to llama2’s self-evaluation, we also use llama3-8b/70b-instruct (AI@Meta, 2024  ###reference_b1###), yi-34b-chat (Young et al., 2024  ###reference_b38###), and qwen1.5-72b-chat (Bai et al., 2023  ###reference_b5###) to evaluate the speculation.\nWe choose XieZhi (Gu et al., 2024  ###reference_b11###) for testing.\nAs mentioned before, the option token in XieZhi is an important chaotic point.\nThe evaluation feedback at the option token can significantly affect the decoding result.\nHere, we use these models to directly answer questions and then obtain their accuracies (Direct gen. in Table 2  ###reference_###) to approximate their evaluation quality, as these accuracies are the most intuitive representation of LLMs’ capabilities.\nResults in Table 2  ###reference_### (SE. gen.) clearly show that llama2’s accuracy is significantly improved when we use a more powerful model to provide more accurate evaluation feedback.\nThis demonstrates evaluation quality is crucial in SED.\nThe more accurate the evaluation feedback, the better SED works.\nThe results also suggest that we could make use of tools to further enhance the evaluation quality and thus further improve the decoding in other tool-friendly application scenarios."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Analysis of generation-speculation alignment",
            "text": "Besides evaluation, speculation is also an important step for SED.\nAs described in Section 3.2  ###reference_###, speculation refers to inferring the text that the model will subsequently generate after token  is selected.\nThis requires that the speculation needs to be accurate enough.\nMore specifically, the model needs to have the same token selected at speculation as at generation, i.e., the decoding methods at both stages should be aligned.\nTo investigate the impact of the decoding methods’ alignment, we check various decoding combinations in the generation and speculation phases.\nAs shown in Table 3  ###reference_###, the model performance is improved when the decoding methods at generation and speculation are aligned.\nOtherwise, SED provides a limited gain to the model and sometimes negatively affects the performance.\nThis implies that SED works when the speculation and generation are aligned.\nThe misalignment could give the model a misleading, which in turn affects the model’s final response.\nWe choose the ‘greedy + greedy’ combination as the implementation for SED in Table 1  ###reference_### since it requires less computation than ‘beam + beam’ with similar performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose a decoding method called self-evaluation decoding, SED, to optimize LLMs’ token selection at chaotic points during decoding so that the most appropriate token can be selected.\nSED first detects the chaotic points and then obtains the self-evaluation probability of each token at the chaotic points through two steps: speculation and evaluation, and finally combines the self-evaluation probabilities with the token probabilities to determine the optimal token.\nFinally, experimental results and case studies across various tasks demonstrate the effectiveness of our method."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Our method outperforms other baselines across different models on various tasks, illustrating its effectiveness.\nHowever, there remain some limitations.\nFirst, judgment  in  is relatively simple now, lacking of CoT or other enhanced information.\nSecond, the introduction of speculation makes the decoding slower since more computations are required for selecting the optimal token.\nIndeed, it is a trade-off between inference efficiency and quality.\nKahneman (2011  ###reference_b16###) mentions two types of decision-making behaviors: fast thinking and slow thinking.\nSlow thinking is responsible for solving complex problems that are hard for fast thinking, needing additional computational resources for logical and rational thinking.\nIn other words, to optimize LLMs’ token selection at chaotic points, we ought to allow LLMs to think slowly, and exchange the extra inference time for better generation, which is exactly the core contribution of this paper.\nRegarding the additional time cost introduced by slow thinking, we could reduce it through methods such as reusing computation.\nWe leave the optimizations for evaluation ability enhancement and decoding speed as our future work."
        }
    ]
}