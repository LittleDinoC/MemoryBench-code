{
    "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
    "abstract": "The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL111https://github.com/NhatHoang2002/ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Warning: This paper discusses and contains content that can be offensive or upsetting.\nWhile social media has dramatically expanded democratic participation in public discourse, they have also become a widely recognized platform for the dissemination of toxic speech Mathew et al. (2021  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###); Yu et al. (2022  ###reference_b54###). Online toxic speech, therefore, is prevalent and can lead the victims to serious consequences Olteanu et al. (2018  ###reference_b38###); Farrell et al. (2019  ###reference_b18###). For this reason, the development of toxic detection tools has received growing attention in recent years Hutto and Gilbert (2014  ###reference_b25###); Ribeiro et al. (2018  ###reference_b42###); Balkir et al. (2022  ###reference_b3###); Jahan and Oussalah (2023  ###reference_b27###).\n###figure_1### Toxic speech can generally be categorized as either explicit or implicit. Explicit toxic speech contains direct offensive language targeting individuals or groups Nockleyby (2000  ###reference_b37###) and has been extensively studied Schmidt and Wiegand (2017  ###reference_b45###); Jahan and Oussalah (2021  ###reference_b26###). On the other hand, implicit toxic speech presents a more challenging detection task as it relies on stereotypes and indirect language ElSherief et al. (2021  ###reference_b17###) (see Fig. 1  ###reference_###) and has received limited attention. Moreover, given the absence of explicit offensive words or cues, it is crucial for AI models not only to detect implicit toxic speech but also to provide explanations for its toxic nature Sridhar and Yang (2022  ###reference_b46###). The act of explanation serves practical purposes in various real-life applications, including improving human-machine interactions and building trustworthy AI systems Ribeiro et al. (2016  ###reference_b43###); Dosilovic et al. (2018  ###reference_b15###); Bai et al. (2022  ###reference_b2###).\nThese applications, therefore, pose a need for unified systems that can effectively detect implicit toxic speech and explain its toxicity. However, previous works have mainly focused on a hybrid approach that combines detection and explanation tasks into a single text generation problem. For example, Sap et al. (2020  ###reference_b44###) proposed concatenating the toxic speech label and explanation as the target output, AlKhamissi et al. (2022  ###reference_b1###) and Huang et al. (2022  ###reference_b24###) extended this approach by incorporating additional data such as target group(s) or social norms. Unfortunately, these hybrid approaches can introduce error propagation problems Wu et al. (2018  ###reference_b53###), possibly due to differences in training objectives (see Sec. 3.2.3  ###reference_.SSS3###). Consequently, models formulated in this manner tend to have much lower detection scores compared to models that focus solely on detection, as evidenced by our experimentation results (Sec. 4  ###reference_###). Another simple approach is building a modular-based system separating the detection module and the explanation generation module. However, in reality, this kind of framework is computationally expensive to train, store and deploy as it has multiple components.\nTo bridge these gaps in detecting and explaining implicit toxic speech, we propose a unified framework ToXCL consisting of three modules (Fig. 2  ###reference_###). Our approach is motivated by the findings that modeling the minority target groups associated with toxic speech can potentially improve the performance of both implicit toxic detection and explanation tasks ElSherief et al. (2018  ###reference_b16###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###). To achieve this, we build a  Target Group Generator as our first module to generate the target minority group(s) based on the input post. The generated target group(s) and the post are then input into an  Encoder-Decoder Model whose encoder detects the speech, and the decoder outputs the necessary toxic explanation. To enhance the detection performance of our encoder, we incorporate a strong Teacher Classifier that utilizes the teacher forcing technique during training to distill knowledge to our encoder classifier. Finally, we introduce a Conditional Decoding Constraint to enhance the explanation ability of the decoder during inference. Our model achieves state-of-the-art performance on the Implicit Hate Corpus (IHC) ElSherief et al. (2021  ###reference_b17###) and Social Bias Inference Corpus (SBIC) Sap et al. (2020  ###reference_b44###) in the task of implicit hate speech detection and explanation, outperforming baselines. Our contributions are as follows:\n(i) We present a unified framework for the detection and explanation of implicit toxic speech. To the best of our knowledge, our work represents a pioneering effort in integrating both tasks into an encoder-decoder model to avoid the error propagation problem while maintaining the competitive performance on both tasks parameter-efficiently.\n(ii) We propose to generate target groups for the toxic speech detection and explanation model through the utilization of an encoder-decoder model, thereby distinguishing our approach from previous methods (see Sec. 3.2.1  ###reference_.SSS1###). We also introduce several techniques to enhance the performance of our model: (1) joint training among the tasks to make the detection and explanation model end-to-end; (2) using teacher forcing to train the encoder; (3) a simple Conditional Decoding Constraint during the inference to avoid generating unnecessary explanation.\n(iii) We set up new strong state-of-the-art results in the task of implicit toxic speech detection and explanation tasks in two widely used benchmarks Implicit Hate Corpus (IHC) and Social Bias Inference Corpus (SBIC).\n(iv) We conduct a thorough analysis to demonstrate the effectiveness of our architectural design. We will open-source our model to inspire and facilitate future research.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Toxic Speech Detection & Explanation",
            "text": "Early studies on identifying toxic speech relied on linguistic rule-based approaches Chen et al. (2012  ###reference_b9###); Hutto and Gilbert (2014  ###reference_b25###); Gitari et al. (2015  ###reference_b20###); Wiegand et al. (2018  ###reference_b51###). However, these methods, which use predetermined lexical and syntactic rules, struggle to detect implicit toxic speech without explicit vulgarities Breitfeller et al. (2019  ###reference_b6###); MacAvaney et al. (2019  ###reference_b35###). Recent frameworks based on transformer architecture Vaswani et al. (2017  ###reference_b49###) have made progress in detecting toxic speech Basile et al. (2019  ###reference_b5###); Tran et al. (2020  ###reference_b47###); Kennedy et al. (2020  ###reference_b28###). However, detecting implicit toxic speech remains challenging despite attempts to improve performance on this task Vidgen et al. (2019  ###reference_b50###); Caselli et al. (2020  ###reference_b8###, 2021  ###reference_b7###); Kim et al. (2022  ###reference_b29###). The issue of explaining why a text is toxic has received even more limited attention, with some studies focusing solely on explaining implicit toxic speech ElSherief et al. (2021  ###reference_b17###); Sridhar and Yang (2022  ###reference_b46###). Another few studies have addressed both implicit toxic speech detection and explanation Sap et al. (2020  ###reference_b44###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###), often formulating them as text generation tasks, possibly leading to error propagation and lower detection scores compared to detection-only models.\n\nAI technologies have made significant advancements in healthcare for disease prediction and diagnosis, leveraging large datasets and deep learning algorithms Razzak et al. (2018 ###reference_b40###); Esteva et al. (2017 ###reference_b18###). Machine learning models, especially neural networks, have proven effective in identifying patterns indicative of various diseases, enhancing early diagnosis accuracy and improving patient outcomes Miotto et al. (2016 ###reference_b37###); Rajkomar et al. (2019 ###reference_b39###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Knowledge Distillation",
            "text": "Knowledge distillation Hinton et al. (2015 ###reference_b23###) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge. It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision Gou et al. (2021 ###reference_b22###). Recently, researchers have explored applying knowledge distillation in Natural Language Processing. For example, Fu et al. (2020 ###reference_b19###) used a contrastive approach to align the intermediate layer outputs of the teacher and student models. Turc et al. (2019 ###reference_b48###) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis. Additionally, Clark et al. (2019 ###reference_b11###) trained a multitasking network by ensembling multiple single-task teachers. In our work, we distill the knowledge from a teacher classifier to our model’s classifier (the student classifier), optimizing the Kullback-Leibler distance Csiszár (1975 ###reference_b13###) between soft labels. AI has been increasingly leveraged in healthcare for disease prediction and diagnosis, enhancing the accuracy and speed of diagnostic processes. This technology employs machine learning models to predict disease outbreaks and diagnose conditions based on medical imaging and patient data, revolutionizing personal and public health management."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder’s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "The task of implicit toxic speech detection can be formulated as a binary classification problem while the explanation generation task can be regarded as a text generation problem. Each data instance  consists of an input post , a binary class label  ( for toxic speech,  for non-toxic speech), and a corresponding explanation  ([None] for non-toxic speech). The models then take  as the input and learn to output  and ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ToXCL Framework",
            "text": "Figure 2  ###reference_### shows an overview of our proposed ToXCL, consisting of three modules: (i) Target Group Generator; (ii) Encoder-Decoder Model; (iii) Teacher Classifier. The details of each module are presented below.\nTo enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder’s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Target Group Generator (TG)",
            "text": "Current toxic speech detection systems often overlook the nuances of toxic speech, which can be better addressed by modeling the minority target groups associated with it ElSherief et al. (2018  ###reference_b16###). Incorporating target group information has the potential to improve the accuracy of toxic speech detection and enable the generation of high-quality hate speech explanations Huang et al. (2022  ###reference_b24###). Therefore, we propose using a transformer-based encoder-decoder model Raffel et al. (2020  ###reference_b41###) to generate target minority groups for given posts, treating the task as a text generation problem rather than a multi-label classification task. This approach provides two advantages over classification models. Firstly, it leverages powerful pre-trained encoder-decoder models, enhancing the model’s capabilities. Secondly, text generation models are more generalizable, as they are not restricted to a fixed number of target groups, allowing for greater flexibility in handling diverse scenarios.\nAfter generating target groups  based on an input post ,  and  are concatenated as \"Target:{G} Post:{IP}\" and serve as the input for our ToXCL. The experimental details of the TG module are presented in Section 4.1  ###reference_###."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Encoder-Decoder Model",
            "text": "Toxic speech detection and toxicity explanation are two tasks that have received increasing attention, and while researchers have made significant progress in separately solving each problem, addressing them together has received limited attention Sap et al. (2020  ###reference_b44###); AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###). However, these two tasks are strongly correlated, and the explanation of the post can potentially help the systems to detect toxic speech AlKhamissi et al. (2022  ###reference_b1###). Conversely, the toxicity explanation is sometimes only necessary when the post is detected as toxic. Typically, AlKhamissi et al. (2022  ###reference_b1###); Huang et al. (2022  ###reference_b24###) formulate both tasks as a single text generation task, which has some critical shortcomings as discussed in Section 1  ###reference_###. Therefore, in this work, we propose a novel architectural design on top of a pre-trained encoder-decoder model. The encoder addresses the toxic speech detection task, while the decoder generates a toxicity explanation if the post is detected as toxic. The details of both the encoder and decoder components are introduced below.\nTo enable the implicit toxicity detection capability, we build a binary classifier head on top of the encoder of a pre-trained encoder-decoder model. This head includes a linear layer that takes the average of token embeddings from the encoder’s last hidden state as input, followed by a softmax layer Goodfellow et al. (2016  ###reference_b21###  ###reference_b21###  ###reference_b21###). To optimize the performance, both the encoder and the newly added classifier head are trained together using a binary cross-entropy loss:\nin which ,  and .\nRecognizing the importance of generating explanations for implicit toxic speech and its potential impact on various applications, we utilize the decoder of our pre-trained encoder-decoder model to generate the explanation. To optimize its performance, the decoder is fine-tuned using a Causal Language Modeling (CLM) loss:\nin which  is the set of tokens of the explanation, and  is the size of the window.\nFinally, we train the encoder-decoder model for the task of toxic speech detection and explanation by joining the two losses:\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Teacher Classifier (TC)",
            "text": "Since the open-sourced encoder-decoder models are commonly pre-trained on a diverse range of tasks and these tasks might not solely focus on learning strong representations from their encoders, these encoders may not exhibit the same strength as pre-trained encoder-based models such as BERT Devlin et al. (2019  ###reference_b14###) or RoBERTa Liu et al. (2019  ###reference_b33###) for classification tasks. Motivated by Hinton et al. (2015  ###reference_b23###), we propose to use knowledge distillation to transfer knowledge from a strong encoder-based model (Teacher Classifier) to the Flan-T5 encoder (Student Classifier). Specifically, we leverage the outputs  and  from the Teacher Classifier and Student Classifier, respectively, and employ the Kullback-Leibler divergence loss Csiszár (1975  ###reference_b13###) as the teacher forcing loss to minimize the discrepancy between  and :\nOur final loss  is the weighted sum of :\nin which  are the contribution weights."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Conditional Decoding Constraint (CD)",
            "text": "One of the main challenges with unified frameworks for toxic speech detection and explanation is the lack of synchronization between the classifier’s output label and the generated explanation. For instance, when the classifier outputs a label of , indicating non-toxic speech, the explanation generation module still generates an explanation, even though it is unnecessary in this case. To address this, we propose the Conditional Decoding Constraint, a simple yet effective algorithm. This constraint controls the decoder’s generation process by generating a [None] token for non-toxic speech and a toxic explanation for toxic speech. By incorporating this constraint, our framework ensures coherence and alignment between the generated explanations and classifier outputs, enhancing its overall performance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimentation",
            "text": "To address the problem of free-text target group labelling in IHC and SBIC datasets, we utilized the HateXplain dataset Mathew et al. (2021  ###reference_b36###), which provides 19 fine-grained categories for toxic speech. We fine-tune a T5 model Raffel et al. (2020  ###reference_b41###) as our TG model, and treat it as a text generation problem. To ensure our framework can predict specific target group(s) associated with posts from IHC and SBIC datasets, we conducted an analysis to identify any overlapping data between the HateXplain and IHC, SBIC datasets. We found only one instance of overlap, which we removed before training our TG model.\nWe compare the performance of our TG model with three baseline models: (1) BERT Devlin et al. (2019  ###reference_b14###), an encoder-based model; (2) GPT-2 Radford et al. (2019  ###reference_b40###), a decoder-only model, (3) and BART Lewis et al. (2020  ###reference_b31###), an encoder-decoder model. BERT is widely used for multi-label classification tasks while both GPT-2 and BART have demonstrated remarkable performance in text\nWe concatenate the annotated target group(s) in alphabetical order to construct the target label for each input post. All baselines and our TG model are initialized with pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###) and fine-tuned on a single Google CoLab P40 GPU with a window size of 256, a learning rate of , and AdamW Loshchilov and Hutter (2019  ###reference_b34###) as the optimizer. The BERT model is fine-tuned for 10 epochs, while the GPT-2 and BART models are fine-tuned for 20k iterations. We use a beam search strategy with a beam size of 4 for our generation decoding strategy.\nOur TG model is evaluated using F1 (%) for multi-label classification and ROUGE-L (%) Lin (2004  ###reference_b32###). The results in Table 1  ###reference_### indicate that our model achieved an F1 score of  and a ROUGE-L score of , outperforming the competing baselines in identifying target groups in toxic posts.\nWe conduct our experiments on two datasets: IHC ElSherief et al. (2021  ###reference_b17###) and SBIC Sap et al. (2020  ###reference_b44###). These datasets are collected from popular social media platforms such as Twitter and Gab, providing comprehensive coverage of the most prevalent toxic groups. Prior to training, we pre-process the data as detailed in Appendix A  ###reference_###.\nWe compare ToXCL with three groups of baselines: (G1) implicit toxic speech detection, (G2) implicit toxic speech explanation, and (G3) implicit toxic speech detection and explanation.\nFor baselines in group G1, we use BERT Devlin et al. (2019  ###reference_b14###), HateBERT Caselli et al. (2021  ###reference_b7###), RoBERTa Liu et al. (2019  ###reference_b33###) and ELECTRA Clark et al. (2020  ###reference_b12###) as our baselines. They are widely employed in prior toxic speech detection works.\nWe select GPT-2 Radford et al. (2019  ###reference_b40###), BART Lewis et al. (2020  ###reference_b31###), T5 Raffel et al. (2020  ###reference_b41###), and Flan-T5 Chung et al. (2022  ###reference_b10###) as our baselines G2 and G3. GPT-2 represents the group of decoder-only models, while BART, T5, and Flan-T5 have the encoder-decoder architecture. Specifically for group G2, we fine-tune them to generate [None] token or the explanations’ tokens. For group G3, we concatenate the classification label [Toxic]/[Non-toxic] and the explanation of each sample as the output, and fine-tune the models with the post as input. Furthermore, to align with recent advancements in Large Language Models (LLMs), we further include ChatGPT 222Version: gpt-3.5-turbo-0613 (a state-of-the-art closed-source LLM) and Mistral-7B-Instruct-v0.2 (a state-of-the-art open-source LLM) in group G3. Both models are evaluated under the zero-shot setting.\nWe initialize all the models with the pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###). We then fine-tune them on a single Google CoLab P40 GPU with a window size of 256, and a learning rate of  and use AdamW Loshchilov and Hutter (2019  ###reference_b34###) as our optimizer. The classification baselines in group G1 are fine-tuned on  epochs while the generation ones in G2 and G3 are fine-tuned on k iterations. Beam search strategy with a beam size of  is utilized as our generation decoding strategy.\nWe adopt Accuracy and Macro F1 as our classification metrics, following prior works Mathew et al. (2021  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###). For the generation of explanations, we utilize BLEU-4 Papineni et al. (2002  ###reference_b39###), ROUGE-L Lin (2004  ###reference_b32###) and METEOR Banerjee and Lavie (2005  ###reference_b4###) as our n-gram metrics. We further utilize BERTScore Zhang et al. (2020  ###reference_b55###) to measure the similarity between the generated toxic explanation and the ground truth one based on deep-contextual embeddings. To ensure that unnecessary explanations are not generated for non-toxic posts and penalize unnecessary explanations, we develop a new evaluation algorithm for the explanation generation task. Its pseudo-code is presented in Algorithm 1  ###reference_###. In this algorithm, we assign a score of  if both the generated explanation and the ground truth explanation are [None] indicating that no explanation is needed. If both the generated explanation and the ground truth explanation are not [None] we compute a score based on the quality of the generated explanation. For any other mismatched pairs, we assign a score of 0 to penalize the unnecessary explanations for non-toxic speech. It is worth noting that our evaluation algorithm is different from Sridhar and Yang (2022  ###reference_b46###) which only evaluates the quality of the generation within implicit toxic cases.\nTo gain deeper insights into the generation performance and challenges that our ToXCL faces compared to the competing baseline, we conduct human evaluations using a randomly selected set of 150 samples that require explanations from each examined benchmark. Specifically, we collect the generated explanations from both the ToXCL and Flan-T5 models in two different settings, G2 and G3. To ensure high-quality evaluations, five native English speakers are hired to rate the generated explanations on a 1-3 scale (with 3 being the highest) based on three criteria: (i) Correctness, evaluating the accuracy of the explanation in correctly explaining the meaning of toxic speech; (ii) Fluency, assessing the fluency and coherence of the generated explanation in terms of language use; and (iii) Toxicity, gauging the level of harmfulness and judgmental tone exhibited in the generated explanation. The annotator agreement is measured using Krippendorff’s alpha Krippendorff (2011  ###reference_b30###), which provides a measure of inter-annotator reliability.\nOur experimental results in Table 3  ###reference_### reveal three main observations. Firstly, ToXCL outperforms all baselines on both benchmarks, demonstrating the effectiveness of our encoder-decoder model in addressing both implicit toxic detection and explanation tasks simultaneously without conflicts. Secondly, our model surpasses detection models in group G1, indicating the strong capability of our encoder in detecting implicit toxic speech. It is worth noting that despite having fewer parameters than the RoBERTa-Base model (124M), our encoder (Flan-T5’s) classifier (109M) achieves better performance while maintaining computational efficiency. Lastly, our model significantly outperforms its backbone, Flan-T5, highlighting the effectiveness of our architectural designs in jointly training the tasks in an end-to-end manner for implicit toxic speech detection and explanation problems.\nOur human evaluation results in Table 4  ###reference_### indicate that ToXCL outperforms its backbone model, Flan-T5, from groups G2 and G3, in terms of both explanation accuracy and textual fluency. This improved performance is also reflected in the detection task, resulting in more reliable explanations with fewer harmful outputs compared to the baselines. Our human annotators exhibit strong agreement with Krippendorff’s alpha scores consistently measuring at least 0.78 among the three scores. It is worth noting that the average toxicity score of around 2 for both our ToXCL and baseline models aligns with expectations, given that the training datasets contain offensive words in the ground truth explanations (see Tab. 5  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Target Group Generator Experiment",
            "text": "To address the problem of free-text target group labelling in IHC and SBIC datasets, we utilized the HateXplain dataset Mathew et al. (2021  ###reference_b36###  ###reference_b36###), which provides 19 fine-grained categories for toxic speech. We fine-tune a T5 model Raffel et al. (2020  ###reference_b41###  ###reference_b41###) as our TG model, and treat it as a text generation problem. To ensure our framework can predict specific target group(s) associated with posts from IHC and SBIC datasets, we conducted an analysis to identify any overlapping data between the HateXplain and IHC, SBIC datasets. We found only one instance of overlap, which we removed before training our TG model.\nWe compare the performance of our TG model with three baseline models: (1) BERT Devlin et al. (2019  ###reference_b14###  ###reference_b14###), an encoder-based model; (2) GPT-2 Radford et al. (2019  ###reference_b40###  ###reference_b40###), a decoder-only model, (3) and BART Lewis et al. (2020  ###reference_b31###  ###reference_b31###), an encoder-decoder model. BERT is widely used for multi-label classification tasks while both GPT-2 and BART have demonstrated remarkable performance in text\nWe concatenate the annotated target group(s) in alphabetical order to construct the target label for each input post. All baselines and our TG model are initialized with pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###  ###reference_b52###) and fine-tuned on a single Google CoLab P40 GPU with a window size of 256, a learning rate of , and AdamW Loshchilov and Hutter (2019  ###reference_b34###  ###reference_b34###) as the optimizer. The BERT model is fine-tuned for 10 epochs, while the GPT-2 and BART models are fine-tuned for 20k iterations. We use a beam search strategy with a beam size of 4 for our generation decoding strategy.\nOur TG model is evaluated using F1 (%) for multi-label classification and ROUGE-L (%) Lin (2004  ###reference_b32###  ###reference_b32###). The results in Table 1  ###reference_###  ###reference_### indicate that our model achieved an F1 score of  and a ROUGE-L score of , outperforming the competing baselines in identifying target groups in toxic posts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Teacher Classifier Experiment",
            "text": "For our Teacher Classifier, a RoBERTa-Large model Liu et al. (2019  ###reference_b33###) is fine-tuned using the generated target group(s) (TG in Section 3  ###reference_###). The model achieved an F1 score of  on IHC and  on SBIC, indicating the effectiveness of generated target group(s) in classifying toxic speech. Detailed results can be found in Table 2  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "ToXCL Experiment",
            "text": "We conduct our experiments on two datasets: IHC ElSherief et al. (2021  ###reference_b17###  ###reference_b17###) and SBIC Sap et al. (2020  ###reference_b44###  ###reference_b44###). These datasets are collected from popular social media platforms such as Twitter and Gab, providing comprehensive coverage of the most prevalent toxic groups. Prior to training, we pre-process the data as detailed in Appendix A  ###reference_###  ###reference_###.\nWe compare ToXCL with three groups of baselines: (G1) implicit toxic speech detection, (G2) implicit toxic speech explanation, and (G3) implicit toxic speech detection and explanation.\nFor baselines in group G1, we use BERT Devlin et al. (2019  ###reference_b14###  ###reference_b14###), HateBERT Caselli et al. (2021  ###reference_b7###  ###reference_b7###), RoBERTa Liu et al. (2019  ###reference_b33###  ###reference_b33###) and ELECTRA Clark et al. (2020  ###reference_b12###  ###reference_b12###) as our baselines. They are widely employed in prior toxic speech detection works.\nWe select GPT-2 Radford et al. (2019  ###reference_b40###  ###reference_b40###), BART Lewis et al. (2020  ###reference_b31###  ###reference_b31###), T5 Raffel et al. (2020  ###reference_b41###  ###reference_b41###), and Flan-T5 Chung et al. (2022  ###reference_b10###  ###reference_b10###) as our baselines G2 and G3. GPT-2 represents the group of decoder-only models, while BART, T5, and Flan-T5 have the encoder-decoder architecture. Specifically for group G2, we fine-tune them to generate [None] token or the explanations’ tokens. For group G3, we concatenate the classification label [Toxic]/[Non-toxic] and the explanation of each sample as the output, and fine-tune the models with the post as input. Furthermore, to align with recent advancements in Large Language Models (LLMs), we further include ChatGPT 222Version: gpt-3.5-turbo-0613 (a state-of-the-art closed-source LLM) and Mistral-7B-Instruct-v0.2 (a state-of-the-art open-source LLM) in group G3. Both models are evaluated under the zero-shot setting.\nWe initialize all the models with the pre-trained checkpoints from Huggingface Wolf et al. (2020  ###reference_b52###  ###reference_b52###). We then fine-tune them on a single Google CoLab P40 GPU with a window size of 256, and a learning rate of  and use AdamW Loshchilov and Hutter (2019  ###reference_b34###  ###reference_b34###) as our optimizer. The classification baselines in group G1 are fine-tuned on  epochs while the generation ones in G2 and G3 are fine-tuned on k iterations. Beam search strategy with a beam size of  is utilized as our generation decoding strategy.\nWe adopt Accuracy and Macro F1 as our classification metrics, following prior works Mathew et al. (2021  ###reference_b36###  ###reference_b36###); ElSherief et al. (2021  ###reference_b17###  ###reference_b17###). For the generation of explanations, we utilize BLEU-4 Papineni et al. (2002  ###reference_b39###  ###reference_b39###), ROUGE-L Lin (2004  ###reference_b32###  ###reference_b32###) and METEOR Banerjee and Lavie (2005  ###reference_b4###  ###reference_b4###) as our n-gram metrics. We further utilize BERTScore Zhang et al. (2020  ###reference_b55###  ###reference_b55###) to measure the similarity between the generated toxic explanation and the ground truth one based on deep-contextual embeddings. To ensure that unnecessary explanations are not generated for non-toxic posts and penalize unnecessary explanations, we develop a new evaluation algorithm for the explanation generation task. Its pseudo-code is presented in Algorithm 1  ###reference_###  ###reference_###. In this algorithm, we assign a score of  if both the generated explanation and the ground truth explanation are [None] indicating that no explanation is needed. If both the generated explanation and the ground truth explanation are not [None] we compute a score based on the quality of the generated explanation. For any other mismatched pairs, we assign a score of 0 to penalize the unnecessary explanations for non-toxic speech. It is worth noting that our evaluation algorithm is different from Sridhar and Yang (2022  ###reference_b46###  ###reference_b46###) which only evaluates the quality of the generation within implicit toxic cases.\nTo gain deeper insights into the generation performance and challenges that our ToXCL faces compared to the competing baseline, we conduct human evaluations using a randomly selected set of 150 samples that require explanations from each examined benchmark. Specifically, we collect the generated explanations from both the ToXCL and Flan-T5 models in two different settings, G2 and G3. To ensure high-quality evaluations, five native English speakers are hired to rate the generated explanations on a 1-3 scale (with 3 being the highest) based on three criteria: (i) Correctness, evaluating the accuracy of the explanation in correctly explaining the meaning of toxic speech; (ii) Fluency, assessing the fluency and coherence of the generated explanation in terms of language use; and (iii) Toxicity, gauging the level of harmfulness and judgmental tone exhibited in the generated explanation. The annotator agreement is measured using Krippendorff’s alpha Krippendorff (2011  ###reference_b30###  ###reference_b30###), which provides a measure of inter-annotator reliability."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "ToXCL Performance",
            "text": "Our experimental results in Table 3  ###reference_###  ###reference_### reveal three main observations. Firstly, ToXCL outperforms all baselines on both benchmarks, demonstrating the effectiveness of our encoder-decoder model in addressing both implicit toxic detection and explanation tasks simultaneously without conflicts. Secondly, our model surpasses detection models in group G1, indicating the strong capability of our encoder in detecting implicit toxic speech. It is worth noting that despite having fewer parameters than the RoBERTa-Base model (124M), our encoder (Flan-T5’s) classifier (109M) achieves better performance while maintaining computational efficiency. Lastly, our model significantly outperforms its backbone, Flan-T5, highlighting the effectiveness of our architectural designs in jointly training the tasks in an end-to-end manner for implicit toxic speech detection and explanation problems.\nOur human evaluation results in Table 4  ###reference_###  ###reference_### indicate that ToXCL outperforms its backbone model, Flan-T5, from groups G2 and G3, in terms of both explanation accuracy and textual fluency. This improved performance is also reflected in the detection task, resulting in more reliable explanations with fewer harmful outputs compared to the baselines. Our human annotators exhibit strong agreement with Krippendorff’s alpha scores consistently measuring at least 0.78 among the three scores. It is worth noting that the average toxicity score of around 2 for both our ToXCL and baseline models aligns with expectations, given that the training datasets contain offensive words in the ground truth explanations (see Tab. 5  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "When adding the this module (i.e., + TG in Table 3  ###reference_###), we observe that all baselines considered in our experiments achieve significant improvements compared to training them without the generated target groups. In addition, our Teacher Classifier also outperforms its variant trained on solely the input posts. These improvements emphasize the crucial role of the target group(s) in enhancing their ability to detect and explain implicit toxic speech. These results also consolidate our motivation for using generated target groups in guiding the models for solving the two tasks.\nBy comparing the performance of Flan-T5 + CLH + TG before and after using teacher forcing (+ TF in Table 3  ###reference_###), we observe that incorporating this technique improves the performance of the encoder of ToXCL. This improvement demonstrates that providing additional guidance to the encoder of ToXCL results in more accurate predictions and achieves a performance close to that of the Teacher Classifier.\nFinally, the impact of integrating the Conditional Decoding Constraint designed to avoid necessary explanation, is evaluated in Table 3  ###reference_###. Compared with Flan-T5 + CLH + TG + TF, our ToXCL is improved significantly on the toxic explanation generation task. This confirms the effectiveness of Conditional Decoding Constraint in helping the outputs of our model to synchronize implicit toxic speech detection labels and toxic explanations.\nOur model, along with the baselines, struggles with detecting implicit toxic speech that contains abbreviated or coded tokens, such as \"#noamnesty\" in case (5). This error has also been observed and discussed by previous work ElSherief et al. (2021  ###reference_b17###).\nOur model may face challenges in accurately detecting toxic sentences that contain indirect words. For example, case (6) involves the phrase \"bruh i love white people too how do i join\", which uses indirect words such as \"bruh\" and \"love\" to express irony. The speaker sarcastically expresses a desire to join a racial group while implying that joining such a group is based on a belief in the superiority of white people or that minorities are lesser races.\nIn all cases except cases (5), (6), and (10), our ToXCL accurately identifies implicit toxic speech but generates linguistically different explanations from the ground truth(s). However, these generated explanations convey the same semantic meaning as the ground truth, indicating the model’s ability to comprehend correctly the implicit meanings. This, along with discussions by Huang et al. (2022  ###reference_b24###), demonstrates that instances of implicit toxic speech can have multiple correct explanations, highlighting the limitations of commonly-used n-gram evaluation metrics like BLEU-4 Papineni et al. (2002  ###reference_b39###) and ROUGE-L Lin (2004  ###reference_b32###) scores."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Case Study: Effectiveness of ToXCL",
            "text": "We compare the performance of ToXCL with the two best-performing baselines in group G3, namely GPT-2 on IHC and Flan-T5 on SBIC (Sec. 4  ###reference_###). We present the cases discussed in Table 5  ###reference_###. In cases (1), (3), (4), (7), (9), and (10) we observe that both GPT-2 and Flan-T5 fail to capture the intended meaning of the input posts, resulting in wrong detection or harmful explanations. In contrast, our ToXCL effectively captures the meaning of the posts, leading to accurate explanations that align closely with the ground truth ones. Notably, when comparing to the ground truth explanation, the explanation generated by ToXCL exhibits a more polite attitude (cases 2, 4, 8, 9) or even more accurate (case 3)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Studies",
            "text": "When adding the this module (i.e., + TG in Table 3  ###reference_###  ###reference_###), we observe that all baselines considered in our experiments achieve significant improvements compared to training them without the generated target groups. In addition, our Teacher Classifier also outperforms its variant trained on solely the input posts. These improvements emphasize the crucial role of the target group(s) in enhancing their ability to detect and explain implicit toxic speech. These results also consolidate our motivation for using generated target groups in guiding the models for solving the two tasks.\nBy comparing the performance of Flan-T5 + CLH + TG before and after using teacher forcing (+ TF in Table 3  ###reference_###  ###reference_###), we observe that incorporating this technique improves the performance of the encoder of ToXCL. This improvement demonstrates that providing additional guidance to the encoder of ToXCL results in more accurate predictions and achieves a performance close to that of the Teacher Classifier.\nFinally, the impact of integrating the Conditional Decoding Constraint designed to avoid necessary explanation, is evaluated in Table 3  ###reference_###  ###reference_###. Compared with Flan-T5 + CLH + TG + TF, our ToXCL is improved significantly on the toxic explanation generation task. This confirms the effectiveness of Conditional Decoding Constraint in helping the outputs of our model to synchronize implicit toxic speech detection labels and toxic explanations."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Error Analysis",
            "text": "We present additional qualitative samples from both benchmarks in Table 5  ###reference_### to highlight key challenges faced by existing models in detecting implicit toxic speech and generating explanations. While our model performs well overall, there are still areas for improvement, as discussed below.\nOur model, along with the baselines, struggles with detecting implicit toxic speech that contains abbreviated or coded tokens, such as \"#noamnesty\" in case (5). This error has also been observed and discussed by previous work ElSherief et al. (2021  ###reference_b17###  ###reference_b17###).\nOur model may face challenges in accurately detecting toxic sentences that contain indirect words. For example, case (6) involves the phrase \"bruh i love white people too how do i join\", which uses indirect words such as \"bruh\" and \"love\" to express irony. The speaker sarcastically expresses a desire to join a racial group while implying that joining such a group is based on a belief in the superiority of white people or that minorities are lesser races.\nIn all cases except cases (5), (6), and (10), our ToXCL accurately identifies implicit toxic speech but generates linguistically different explanations from the ground truth(s). However, these generated explanations convey the same semantic meaning as the ground truth, indicating the model’s ability to comprehend correctly the implicit meanings. This, along with discussions by Huang et al. (2022  ###reference_b24###  ###reference_b24###), demonstrates that instances of implicit toxic speech can have multiple correct explanations, highlighting the limitations of commonly-used n-gram evaluation metrics like BLEU-4 Papineni et al. (2002  ###reference_b39###  ###reference_b39###) and ROUGE-L Lin (2004  ###reference_b32###  ###reference_b32###) scores."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present ToXCL, a unified framework for implicit toxic speech detection and explanation (Sec. 3.2  ###reference_###). It consists of three components: a Target Group Generator, an Encoder-Decoder model, and a Teacher Classifier. Our findings show that the Target Group Generator effectively identifies target groups, improving both accuracy and F1 scores for detecting implicit toxic speech. The novel encoder-decoder architecture successfully performs both tasks of detection and explanation without harming each other. The integration of the Teacher Classifier and the Conditional Decoding Constraint further enhances the performance of ToXCL, achieving state-of-the-art results in the task of toxic speech detection and explanation on two widely-used benchmarks. In the future, we will focus on addressing several limitations faced by our framework and baselines as specified in Section 5.3  ###reference_### to further enhance the performance of our model."
        }
    ]
}