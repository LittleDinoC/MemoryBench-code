{
    "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
    "abstract": "Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions.\nVarious strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows. Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Biomedical concept linking studies the intricate task of linking closely related concepts across different data sources by leveraging their semantic meanings and underlying biomedical knowledge, as exemplified in Figure 1 (Sevgili et al., 2022).\nThis linking process is crucial for enabling integrative analyses, as biomedical concepts obtained from diverse sources offer multifaceted views of biomedical knowledge and data (Su et al., 2023; Lu et al., 2023). For example, the electronic health record (EHR), which is regarded as a valuable asset for comprehensive patient health analysis, contains various digital medical information including tabular data, clinical notes, and other types of patient data (Abul-Husn and Kenny, 2019; Sun et al., 2018; Xu et al., 2022). Similarly, the knowledge graph (KG), playing an important role in biomedical research, provides structured knowledge, such as definitions of concepts and their interrelationships (Ma et al., 2018).\nHowever, the cross-source biomedical linking task is challenging due to discrepancies in the biomedical naming conventions used in different systems (Kohane et al., 2021). For example, a KG may mention a disease as “Ellis-Van Creveld syndrome”, while an EHR may refer to the same disease as “Chondroectodermal dysplasia”. This inconsistency presents a strong barrier to cohesive data analysis.\nThe challenge of biomedical concept linking has motivated the development of various methods. Conventional methods focus on setting string-matching rules (D’Souza and Ng, 2015; Kang et al., 2013) and leveraging constructed thesauri (Aronson and Lang, 2010; Savova et al., 2010; Friedman et al., 2001). However, their reliance on fixed rules and crafted thesauri limits coverage and generalizability in real-world scenarios (Shi et al., 2023).\nAddressing these limitations, machine learning-based methods have been widely explored, avoiding the manual design of rules or thesauri. These methods essentially transform biomedical concepts from raw text into embeddings (latent vector representations), which are then used to compute similarity scores via distance functions (e.g. cosine similarity) or learning-based scoring functions (e.g. bilinear attention (Kim et al., 2018)).\nVarious models have been used to obtain biomedical concept embeddings, including pre-trained language models (PLMs) (Wang et al., 2023b) that capture fine-grained semantic relations through extensive training on biomedical corpora  (Xu et al., 2020; Lee et al., 2020; Alsentzer et al., 2019; Liu et al., 2021), and graph neural networks (GNNs) (Zhou et al., 2020) that capture both semantics and relations of biomedical concepts (Bordes et al., 2013; Grover and Leskovec, 2016; Liu et al., 2022).\nDespite the notable achievements of these ML-based linking methods, they are data-hungry and require significant supervision signals when adapted into novel downstream applications. They face challenges due to the costly data annotation and model training processes.\nRecently, large language models (LLMs) have exhibited impressive performances in various NLP tasks, due to their unprecedentedly rich prior knowledge and language capabilities (Zhou et al., 2023; Singhal et al., 2023; Wang et al., 2023a), enabling various applications in a zero-shot learning setting (Lu et al., 2023).\nTherefore, LLMs provide a promising solution for linking related concepts across different systems.\nMeanwhile, LLMs also face challenges including the design of effective and cost-efficient prompts within the context length limits (Zhang et al., 2023), and the NIL prediction capability of reliably rejecting all candidates when correct concepts are absent, instead of returning relatively close but incorrect ones (Peters et al., 2019).\nIn this paper, we propose PromptLink, leveraging LLMs for the cross-source biomedical concept linking task. Considering LLMs’ high cost and context length constraints, we first employ a pre-trained SAPBERT language model to generate biomedical-aware concept embeddings and retrieve top candidates based on the cosine similarities of these embeddings. We then design a novel two-stage prompting mechanism for the GPT-4 model to derive reliable linking predictions. The first stage efficiently filters out irrelevant candidates, thereby minimizing the response token numbers required in the subsequent stage. The second stage generates the final linking results and incorporates a self-verification prompt to address the NIL prediction challenge, effectively rejecting all candidates when none are relevant. In the experiments, PromptLink demonstrates exceptional performance, surpassing various existing concept linking methods by over 5% in two scenarios of biomedical concept linking between EHR and external biomedical KG, which could be attributed to LLM’s intrinsic strong biomedical knowledge. Moreover, PromptLink works"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Biomedical Concept Linking",
            "text": "###figure_2###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Problem Definition",
            "text": "The biomedical concept linking task aims to link biomedical concepts across sources/systems based on semantic meanings and biomedical knowledge. It solely relies on concept names and can thus cover much broader real-world applications. This task differs from existing tasks such as entity linking (Shi et al., 2023  ###reference_b31###), entity alignment (Lu et al., 2023  ###reference_b20###), and ontology matching (Jiménez-Ruiz and Cuenca Grau, 2011  ###reference_b12###), which depend on extra contextual or topological information.\nIn this study, we link the concepts in EHR to corresponding concepts in a biomedical KG.\nWe define an EHR database , a biomedical KG , and the linking task as follows:\nAn EHR database  is a relational database , with  being patient identifiers,  patient attributes,  the values of these attributes. Additionally,  represents multi-token biomedical concepts associated with patient attributes.\nA biomedical KG is a multi-relation graph , where  are concepts,  are relation names, and  are the relational triples among them.\nLink identified biomedical concepts from an EHR  to a biomedical KG  based on semantic meanings and biomedical knowledge, forming linkages . If a concept  from  is not in , link it to a special “NIL” entity, indicating it is unlinkable."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. PromptLink",
            "text": "We propose PromptLink, a novel LLM-based solution for cross-source biomedical concept linking, as illustrated in Figure 2  ###reference_###.\nAddressing LLMs’ high cost and limited input text length, we first employ a biomedical-specialized pre-trained language model to generate concept embeddings and retrieve top candidates via cosine similarities. Subsequently, we employ a two-stage prompting mechanism with GPT-4 to generate the final linking predictions.\nConcept representation and candidate generation.\nAfter preprocessing text by lowercasing and removing punctuation, we use a pre-trained LM (specifically SapBERT (Liu et al., 2021  ###reference_b18###)),\nto create embeddings  for EHR concepts  and KG concepts , represented as\nFor concepts that span multiple tokens, the token-level embeddings are averaged to create the concept embedding.\nThis model helps to project the semantic meanings and prior biomedical knowledge into the embedding space.\nFor candidate generation, we compute cosine similarity  between pairs of EHR concept embedding  and KG concept embedding , represented as\nGiven each input query EHR concept , We select the top- (=10) KG concepts  with the highest similarities as candidates for further GPT-based linking prediction.\nLinking prediction using two-stage prompts.\nThe next step of our framework is generating linking predictions of query  over the top- candidate  using GPT-4 model, leveraging its text comprehension ability, logical reasoning ability, and prior biomedical knowledge (Bubeck et al., 2023  ###reference_b6###; Singhal et al., 2023  ###reference_b32###). In this step, we design a novel two-stage prompt for our task, as can be seen in Figure 2  ###reference_###.\nCombining the two prompts utilizes their strengths and mitigates weaknesses. The first stage focuses on concept pairs to filter out unrelated candidates. The second stage evaluates all candidates in a broader context to identify the closest match or reject all unmatch candidates.\nIn the first stage, the LLM is prompted to check if a concept pair  should be linked. By defining the response structure, the LLM can return answers in specified formats. To improve the prompt response quality, we adopt the self-consistency (Wang et al., 2022  ###reference_b37###) prompting strategy that repeatedly prompts the same question to the LLM multiple times.\nSpecifically, we prompt each concept pair  for  times, thus obtaining the belief score  by:\nConsidering the belief scores across different candidates, we derive a comprehensive filter strategy to exclude irrelevant candidates, using parameter  (set as ). This approach ensures that irrelevant candidates are not considered in the next stage, optimizing both efficiency and effectiveness. The approach is described as follows:\nIf , this indicates some candidates closely align with the query concept. In such cases, candidates with belief scores of zero will be filtered out as they are deemed irrelevant to the query concept and there are closely aligning alternatives. This filtering strategy effectively removes many irrelevant candidates, thereby optimizing both efficiency and effectiveness for the subsequent stage.\nOtherwise, the range of different candidates’ belief scores is not wide enough to justify filtering. Thus, all  candidates will be subjected to double-checking by the second-stage prompt.\nIn the second stage, the LLM evaluates the  candidates retained from the first stage’s filtering process , where  , using a compositional prompt that consists of two consecutive questions to perform complex reasoning.\nSpecifically, the LLM is asked to (1) label the relationship between the query concept and all candidate concepts as “exact match”, “related to”, or “different from”; (2) use self-verification prompts to either identify the closest candidate or dismiss all candidates if none are close, thus the final concept linking result of this prompt is  (usually ) item from .\nIn this stage, we also use the self-consistency strategy that prompts one question for the same  times. Subsequently, we calculate the occurrence frequency  for answers in  and retrieve the final linking result for query EHR concept  as follows:\nIf , this indicates a high probability that none of the candidates are appropriate. Thus, “NIL” is chosen as the final linking prediction.\nOtherwise, the candidate  with the highest frequency  is decided as the final linking result. If two candidates tie for the highest frequency, the one  with higher embedding similarity  to the query concept  is chosen."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Experiments & Discussions",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Implementation Details",
            "text": "Datasets.\nIn our experiments, we curate two biomedical concept linking benchmark datasets: MIID (MIMIC-III-iBKH-Disease) and CISE (CRADLE-iBKH-Side-Effect).\nMIID comprises 1,493 diagnosis concepts from MIMIC-III (Johnson et al., 2016  ###reference_b13###), which is an EHR dataset including over 53,423 hospital patient records, and 18,697 disease concepts from iBKH (Su et al., 2023  ###reference_b33###), which is a KG dataset with 2,384,501 entities. To construct MIID, we first remove exact matches between MIMIC-III diagnosis concepts and iBKH disease concepts. Then, we link the remaining MIMIC concepts to iBKH using ICD-9 (for Disease Control, 2007  ###reference_b9###) and UMLS CUI (Schuyler et al., 1993  ###reference_b29###) codes. We use the linked concept pairs as ground-truth labels only for evaluation purposes.\nCISE contains 1,500 CRADLE (Xu et al., 2022  ###reference_b40###) diagnosis concepts and 4,251 iBKH drug side-effect concepts, constructed by using CUI (Schuyler et al., 1993  ###reference_b29###) and SNOMED CT (Donnelly et al., 2006  ###reference_b7###) codes. Ground-truth matched pairs are also only used for evaluation purposes.\nExperimental Settings.\nFollowing the definition in Sec. 2.1  ###reference_### and recognizing the scarcity of supervision in the biomedical domain, we mainly focus on the biomedical concept linking under the zero-shot setting.\nAdditionally, our biomedical concept linking task solely relies on concept names for broad real-world application coverage.\nGiven this characteristic of our data input, graph-based linking methods, such as selfKG (Liu et al., 2022  ###reference_b19###), are not applicable as they need topological information to establish concept alignment.\nSimilarly, thesauri-based methods, such as MetaMap (Aronson and Lang, 2010  ###reference_b4###), are unsuitable as they only establish links between EHR concepts and KG concepts existing in the pre-defined vocabulary.\nTherefore, the following baseline methods are compared:\nConventional methods: Cosine Distance, Jaccard Distance, Levenshtein Distance (Ristad and Yianilos, 1998  ###reference_b25###), Jaro-Winkler Distance (Winkler, 1990  ###reference_b38###), BM25 (Robertson et al., 2009  ###reference_b26###). These methods measure the concept pairs’ string similarity and relevance and then obtain the linking prediction result.\nMachine learning-based methods: Pre-trained language models are used to generate concept embedding and linking prediction results (according to embedding cosine similarity). Specifically, we select representative PLMs including BioBERT (Lee et al., 2020  ###reference_b17###), BioGPT (Luo et al., 2022  ###reference_b21###), BioClinicalBERT (Alsentzer et al., 2019  ###reference_b3###), BioDistilBERT (Rohanian et al., 2023  ###reference_b27###), KRISSBERT (Zhang et al., 2022  ###reference_b42###), ada002 (Neelakantan et al., 2022  ###reference_b23###), and SAPBERT (Liu et al., 2021  ###reference_b18###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Concept Linking Experiment Results",
            "text": "###table_1### Table 1  ###reference_### shows the accuracy of our proposed PromptLink along with baseline methods, when every method links a query EHR concept  with their predicted top-1 KG concept .\nAs can be seen, PromptLink outperforms competing approaches across both datasets in terms of zero-shot accuracy, underscoring the superiority of our LLM-based concept linking methodology.\nAmong the compared methods, SAPBERT, a SOTA biomedical entity linking method, achieves the second-highest performance.\nMoreover, conventional methods based on string similarity lag behind machine learning techniques, which leverage embeddings from pre-trained language models to effectively match conceptually similar but lexically distinct entities like “Ellis-Van Creveld syndrome” and “Chondroectodermal dysplasia”."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Ablation Studies",
            "text": "Prompt Effectiveness and Efficiency.\nWe conduct ablation studies to reveal the effectiveness and cost-efficiency of the prompt used in our approach, as shown in Table 2  ###reference_###. This comparison uses the same input data and 10 linking candidates across various prompts. In the table, the “Before prompting” denotes the performance of using only embedding similarity obtained from the pre-trained LM, while other methods use LLM to predict linking results based on LM-generated candidates.\nFrom Table 2  ###reference_###, the “Before Prompting” method achieves the worst accuracy, demonstrating that linking performance could be improved by using LLM.\nNotably, PromptLinkwith both two-stage prompts achieves the best accuracy with the second-highest cost (M total tokens, costing approximately $66.25), indicating that the combined effect of the prompts substantially enhances accuracy, with the costs being moderated by the first stage’s proficiency in eliminating unrelated candidates.\nNIL Prediction.\nAnother ablation study examines PromptLink’s NIL prediction ability.\nIn our built MIID and CISE datasets, each query EHR concept  is designed to have a ground-truth linking KG concept . To reflect the real-world unlinkable scenario, we extend our MIID dataset into “MIID-NIL” which contains a proportion () of unlikable EHR concept .\nIn Figure 3  ###reference_###, the overall accuracy of PromptLink in the MIID-NIL dataset is 0.8145.\nSpecifically for the unlikable concepts, PromptLink outputs the expected “NIL” with 0.9290 accuracy, which validates the NIL prediction ability of our proposed method.\nExisting methods highly rely on the hard-coded threshold. For example, we could threshold SAPBERT’s generated embeddings’ cosine similarity, then output the KG concept with the highest similarity above the threshold or “NIL” when none are above. However, this straightforward idea, requiring a manually set threshold, is less effective than PromptLink. As shown in Figure 3  ###reference_###, SAPBERT achieves lower accuracy (maximum value 0.7920) no matter what the threshold value is, which corresponds to our assumptions. When the threshold value is low, SAPBERT generates many wrong predictions to unlinkable query concepts; otherwise, SAPBERT continues to output “NIL” for many linkable concepts.\n###figure_3###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4. Case Studies",
            "text": "In case studies on linking EHR concepts to MIID’s KG disease concepts, three scenarios are presented: (1) concepts assessed by both ground-truth labels and a clinician; (2) concepts evaluated by a clinician due to missing ground-truth labels; (3) irrelevant concepts judged by a clinician. The linking results of PromptLink and SAPBERT are presented in Table 3  ###reference_###.\nOverall, PromptLink could link biomedical concepts more accurately and appropriately.\nFor casse I-V, PromptLink’s linking results are justified by the ground-truth label and clinician. Specifically, for cases I and II, PromptLink accurately links the EHR concepts to conceptually similar\nbut lexically distinct KG concepts, while SAPBERT links to lexically similar but conceptually different KG concepts. This difference showcases the effective use of LLM’s biomedical knowledge. SAPBERT also shows inaccuracies in cases III-IV, and provides a broader prediction in case V, whereas PromptLink’s predictions are more accurate and specific.\nFor cases VI-IX, where linking ground truth labels are lacking, PromptLink’s predictions also align more accurately with EHR concepts than SAPBERT’s, according to a clinician’s review. In cases VI and VII, PromptLink closely matches the EHR concepts, while SAPBERT’s predictions are overly specific. In cases VIII and IX, PromptLink correctly and automatically identifies no matching KG disease concepts, while SAPBERT fails to resolve that NIL prediction challenge unless manual thresholds are set and adjusted.\n###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### ###figure_17### Note: “” indicates this prediction is justified by the clinician. “” indicates this prediction is justified by the ground-truth label.\n###figure_18### ###figure_19###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Conclusion",
            "text": "In this study, we introduce PromptLink, a novel framework leveraging LLMs and multi-stage prompts for effective biomedical concept linking.\nCompared with previous concept linking methods, PromptLink achieves better linking accuracy, attributed to LLM’s intrinsic strong biomedical knowledge. PromptLink further employs multi-stage prompts to maintain cost-efficiency and handle the NIL prediction problem. Moreover, PromptLink functions as a zero-shot framework, requiring no training and demonstrating strong flexibility and generalizability across biomedical systems.\nPromising future work can focus on further enhancing the prompt effectiveness, reducing costs, and minimizing manual efforts, aiming to extend PromptLink’s application to broader systems."
        }
    ]
}