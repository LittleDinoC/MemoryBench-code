{
    "title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
    "abstract": "NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a zero-shot setting, boosting open model LLaVA by 13.4%, while also minorly improving GPT-4V’s performance.111The code implementations and prompts can be found at https://github.com/yueqis/Multilingual_Visual_Reasoning.git.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language technology today is continually evolving to be more inclusive, with models becoming increasing multilingual Lai et al. (2023  ###reference_b19###); Li et al. (2022  ###reference_b20###), multimodal Yang et al. (2023  ###reference_b38###), or both (Chen et al., 2020  ###reference_b6###; Zeng et al., 2023  ###reference_b40###; Geigle et al., 2023  ###reference_b13###; Achiam et al., 2023  ###reference_b2###).\nEven though this promotes broader user accessibility, past research has consistently highlighted differences in model performance across languages (Blasi et al., 2022  ###reference_b4###) and cultures (Liu et al., 2021  ###reference_b21###). Notably, these models often favor North American or Western contexts, potentially leaving behind users from other regions. (Liu et al., 2021  ###reference_b21###; Hershcovich et al., 2022  ###reference_b15###).\n###figure_1### The NLP community is currently witnessing a trend of moving away from openly releasing models to limiting their access through paid web APIs Abdalla et al. (2023  ###reference_b1###). Additionally, the cost to use these services is often higher for low-resourced languages, despite poorer performance Ahia et al. (2023  ###reference_b3###). While it is certainly desirable to have strong and inclusive models available regardless of the access method, open, well-documented, and reasonably sized models have advantages from the point of view of control, ownership, cost, and advancing scientific understanding.\nIn this work, we first compare and contrast the multilingual, multicultural capabilities of the proprietary system GPT-4V(ision) Achiam et al. (2023  ###reference_b2###) with a plethora of open models like LLaVA Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###), mBLIP Geigle et al. (2023  ###reference_b13###), CCLM Zeng et al. (2023  ###reference_b40###), using two datasets on the same task of reasoning over texts and pairs of images, NLVR2 (Suhr et al., 2019  ###reference_b32###) and MaRVL Liu et al. (2021  ###reference_b21###). We discuss this setup in more details in §2  ###reference_### and §3  ###reference_###. We find that GPT-4V significantly outperforms all open models. One additional unprecedented and surprising result is, as shown in Figure 1  ###reference_###, GPT-4V’s consistency in performance across all languages, with performance on some even surpassing that on the NLVR2 dataset in English.\nIn contrast, as we will discuss in §4  ###reference_###, most open models still show a significant gap between English and other languages, perhaps due to deficiencies in training data, or due to the well-known “curse of multilinguality”, where smaller models are less adept at processing low-resource languages (Conneau et al., 2020  ###reference_b9###).\nThis begs the question: “how can we take open models, and bring them closer to achieving the exciting language-equitable multimodal reasoning results demonstrated by the opaque (and presumably bigger) GPT-4V?”\nTowards this end, we conduct a careful analysis of the results from testing models on the multilingual visual reasoning task and discover that failures can arise from any of the three challenging aspects of the task: multilinguality, reasoning, and multimodality. For multilinguality, we find a significant gap in performance for other languages as compared to English. For reasoning, we find a negative correlation of performance and the compositionality of the statement. For multimodality, we find that models were typically pretrained on single image-text pairs, but haven’t seen pairs of images in pretraining, which may lead to a mismatch between pretraining and evaluation objectives. We will discuss this in more details in Section 5  ###reference_###.\nBased on our analysis, we design three interventions that address these challenges in section 6  ###reference_###. The first simply tackles multilinguality – we translate the MaRVL statements to English. Surprisingly, translation leads to a drop in performance for GPT-4V (which might indicate its advanced multilingual capabilities), but helps improve performance for all open models. Our next intervention tackles both multilinguality and reasoning, by generating programs to reason over the set of images using the translated statements, inspired by Gupta and Kembhavi (2023  ###reference_b14###)’s VisProg method. Our third (and most effective) intervention tackles all three challenges by first captioning images conditioned on the statement, and then reasoning over the captions, rather than the images, using chain-of-thought capabilities of text-modality LLMs Wei et al. (2022  ###reference_b35###). Using this intervention, we obtain state-of-the-art zero-shot performance on the MaRVL dataset for open models, and also slightly improve the performance of GPT-4V itself, as shown in Figure 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Dataset Description",
            "text": "###figure_2### We evaluate on two visual reasoning datasets, both containing a statement in natural language and a pair of images. The task is to reason whether the statement is true based on the images, requiring reasoning over both images and the statement together.\nFigure 2  ###reference_### shows an example of this task.\nNLVR2 contains 107,292 examples of English sentences with web photographs. Annotators paired visually-rich images and were encouraged to come up with compositional and linguistically diverse statements for each pair. The dataset contains a train-validation-test split. Images were collected using search queries generated from synsets derived from the ILSVRC2014 ImageNet challenge Russakovsky et al. (2015  ###reference_b30###), with each query resulting in 4 pairs of images from Google Images222https://images.google.com/  ###reference_images.google.com/###. Queries for ImageNet Deng et al. (2009  ###reference_b10###) are based on the English WordNet Poli et al. (2010  ###reference_b29###), whose concepts are more reflective of the North-American or Western cultures.\nMaRVL explores the same task as NLVR2 in multilingual multicultural contexts. MaRVL is a test-only dataset collected for five diverse languages: Indonesian, Swahili, Tamil, Turkish, and Mandarin Chinese. Native speakers first select concepts that are reflective of their speaking population. Next, they curate images from the web that reflect those concepts within their specific cultural context. Finally, native speakers pair and write statements for each image pair, following the same protocol as that laid out for NLVR2."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Models and Evaluation Protocols",
            "text": "We evaluate various open models, including mBLIP (mt0-xl) Geigle et al. (2023  ###reference_b13###), LLaVA Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###), CCLM Zeng et al. (2023  ###reference_b40###), and UNITERs Chen et al. (2020  ###reference_b6###); and a proprietary model GPT-4V(ision).333gpt-4-vision-preview (https://openai.com/research/gpt-4v-system-card  ###reference_card###), abbreviated as ”GPT-4V”. We describe these models in Section A  ###reference_###.\nWe evaluate them in two settings:\nZero-shot.\nIn this setting, the models are not specifically fine-tuned for the task of multimodal visual reasoning.\nThis setting is academically interesting, as it more generally tests the ability of models to perform reasoning tasks, and the results are more likely to be representative of performance on datasets for which training data is not explicitly available.\nIn addition, it is practically useful in that it can also be applied to LMs that cannot as easily by fine-tuned, such as GPT-4V (due to its closed nature), and LLaVa (due to its relatively large size).\nWe test LLaVA, mBLIP, and GPT-4V in this setting.\nFinetuned.\nIn addition, for models that can more easily be finetuned, we finetune them on the English NLVR2 dataset, and test them on both NLVR2 and MaRVL.\nThis represents the realistic setting of adapting a multilingual model to a particular reasoning task using only English data, which is relatively available.\nWe test mBLIP, CCLM-4M, xUNITER, and mUNITER in this setting."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "How well do proprietary and open models perform on multilingual visual reasoning?",
            "text": "In this section, we perform an examination of how-well these various models perform on multilingual multimodal reasoning tasks.\nTable 1  ###reference_### shows performance of humans, open models, and proprietary models. For the models, we use the experiment protocols as in §3  ###reference_### in the zero-shot and finetuned settings. We ask the following questions:\nWhich model performs the best? Answer: GPT-4V on MaRVL, and mBLIP (mT0-XL) on English post-fintuning. However, in the zero-shot setting, the proprietary model GPT-4V performs the best across all languages,444We put GPT-4V in the zero-shot category because we evaluate the performance of GPT-4V on NLVR2 and MaRVL without finetuning on the NLVR2 training data. However, we do not know if GPT-4V has seen examples of NLVR2 or MaRVL during pretraining. and open models lag behind. Note that despite GPT-4V’s impressive performance, it still lags behind human performance by 10% to 20% across all languages, emphasizing that this task still is not completely solved.\nWhich open model performs the best? Answer: mBLIP (mT0-XL), regardless of whether it is finetuned. The other open LLM, LLaVA, is not explicitly trained on multilingual data, so the gap in MaRVL and NLVR2 performance is expected.\nDo models perform equitably across languages?\nUnder zero-shot setting, GPT-4V and mBLIP both show equitable performance across languages, which is encouraging, although the latter significantly lags in overall performance compared to GPT-4V. Interestingly, post finetuning on NLVR2, mBLIP shows better performance on NLVR2 than GPT-4V, but still has lower performance on MaRVL. The gap between English and MaRVL languages also significantly increases for mBLIP from the zero-shot to finetuned setting. Maintaining the equity in performance across languages during finetuning is an interesting future direction, which may help models surpass GPT-4V’s performance on multilingual visual reasoning. Other models lag mBLIP, both in overall performance and equity with English."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "What makes multilingual visual reasoning challenging?",
            "text": "As noted in Table 1  ###reference_###, the best model still lags human performance by 10% to 20%. In this section, we aim to analyze what makes multilingual visual reasoning so challenging, and identify three key aspects as detailed below:"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Multilinguality and Sub-Optimal Cross-Lingual Transfer",
            "text": "In the finetuned setting, we observe a significant drop in performance for MaRVL languages as compared to NLVR2 in English. This is expected since models are finetuned only in English but not in these languages due to lack of training data.\nWe also note that performance on Swahili is consistently lower across models (excluding GPT-4V), which is the lowest-resource language amongst MaRVL languages, as laid out by the language resource taxonomy Joshi et al. (2020  ###reference_b17###). This observation motivates us to evaluate models with MaRVL data translated to English, as we discuss in §6.1  ###reference_###.\nIn the zero-shot setting, GPT-4V and mBLIP both exhibit equitable performance on MaRVL as with NLVR2. While LLaVa is not expected to perform as well for non-English languages, it has poorer performance than mBLIP on NLVR2. While mBLIP is pretrained on multilingual multimodal data, LLaVA is not specifically trained on multilingual data, but it is generally believed that LLaVA has multilingual abilities as it has seen multilingual data during pretraining Liu et al. (2023c  ###reference_b24###, a  ###reference_b22###).\nOverall, models have better visual reasoning capabilities when given English inputs and US/European-centric cultures, while still lagging behind when facing multilingual text and multicultural image inputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Complex Reasoning",
            "text": "Data points in both NLVR2 and MaRVL require complex reasoning. An example statement from NLVR2 is ”one image includes a silver stylus and a device with a blue keyboard base and an open screen propped up like an easel”, which is semantically diverse, long in length, and has a compositional structure, requiring models to perform compositional and complex reasoning to infer the label.\nAs a proxy to the complexity of reasoning, we measure the number of words of the NLVR2 and MaRVL statements (translated to English), and find that model performances drop as the number of words of the statement increases. Figure 3  ###reference_### shows a graph of the performance of GPT-4V plotted against the number of words in each statement. We can clearly see a downward trend in the graph. Based on this, we are motivated to examine methods that break down long, compositional statements, as will be discussed in §6.2  ###reference_###.\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Multimodality and Mismatch between Pretraining & Evaluation",
            "text": "Multimodal reasoning is known to be harder than reasoning over text alone Mogadala et al. (2021  ###reference_b25###); Park and Kim (2023  ###reference_b28###). Further, NLVR2 and MaRVL contain two images per instance, along with a statement describing both images, while vision-language models are typically pretrained on a single image-text pair Cao et al. (2020  ###reference_b5###), leading to a mismatch in the input between pretraining and evaluation. This, and the inherent difficulty of aligning image data and text data during the reasoning process make this multimodal task particularly challenging. This motivates us to (1) move from processing a pair of images together to processing each image separately; and (2) break down the two modalities of image and text in the reasoning process, as in §6.3  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "How can we address these challenges?",
            "text": "Based on our analysis from the previous section, we now move on to examining whether we can devise methods to further improve multilingual multimodal reasoning abilities, particularly those of open models. We examine three research questions, which we discuss in more details in the following subsections respectively. Figure 4  ###reference_### shows a flow chart visualizing the interventions we perform to address the research questions.\nRQ1) (multilinguality) Does translating the text to English and reducing the cross-lingual gap aid performance? Short Answer: it depends.\nRQ2) (multilinguality+reasoning) Can we break down the complex reasoning into modular programs which can be executed on a vision-text input? Short Answer: yes, we adopt the Visual Programming approach Gupta and Kembhavi (2023  ###reference_b14###).\nRQ3) (multilinguality+reasoning+multimodality) Can we alleviate the need for multimodal interaction during the reasoning process? Short Answer: yes, we propose a new approach utilizing captions."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality: Translate-Test",
            "text": "In §5.1  ###reference_###, we find performance on NLVR2 is much better than that on MaRVL. While both are visual reasoning datasets, MaRVL is multi-cultural and contains statements in 5 diverse languages. Since NLP systems perform significantly better with English data Song et al. (2023  ###reference_b31###), we first simply translate the reasoning statements to English using the Google Translate API Wu et al. (2016  ###reference_b37###). A visualization of the process of translate test can be found in Figure 4  ###reference_###.\nIn addition to the models we evaluate in §3  ###reference_###, we also evaluate ViLT Kim et al. (2021  ###reference_b18###) for better comparisons, as our next proposed intervention in §6.2  ###reference_### uses ViLT. We didn’t evaluate ViLT on MaRVL before translate test, since it doesn’t support the MaRVL languages. Our evaluation protocols follows the ones introduced in §3  ###reference_### and results are shown in Table 2  ###reference_###.\nAll prior works, as per our knowledge, have observed a gain in performance post translating to English Liu et al. (2021  ###reference_b21###). Our observation is consistent with prior findings for all models, except GPT-4V(ision). All models except for GPT-4V sees an increase in accuracy after performing translate test; while surprisingly, GPT-4V shows a sharp decrease in performance across all MaRVL languages after translate test. However, this is encouraging, because it speaks for the multilingual capabilities of this model, and indicates that the gains provided by translating to English are lower than the errors made in translating cultural-specific nuances in meaning.\nFor example, the MaRVL statement ”右图有青绿色的苹果” is translated to ”the picture on the right has turquoise apples”, where ”青绿色” is translated to ”turquoise”. However, the color ”青绿色” means pure green with a little bit cyan in Mandarin Chinese, which is different from ”turquoise”. Given this, GPT-4V reasons correctly when provided the statement in Mandarin, but makes mistakes when given the translated statement."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality + Reasoning: Visual Programming",
            "text": "To improve performance of LLMs on reasoning tasks, beyond naive prompting, several methods have been introduced Nye et al. (2021  ###reference_b26###); Zhou et al. (2022  ###reference_b41###); Wei et al. (2022  ###reference_b35###); Gao et al. (2023  ###reference_b12###). Particularly, PAL Gao et al. (2023  ###reference_b12###) provides significant improvements by decomposing a natural language instruction into multiple programmatic sub-modules, executed in an inference step to obtain the final answer. Most recently, efforts like VisProg Gupta and Kembhavi (2023  ###reference_b14###), ViperGPT Surís et al. (2023  ###reference_b33###), Visual ChatGPT Wu et al. (2023  ###reference_b36###) have followed suit to solve multimodal reasoning using LLMs to generate visual programs, that leverage off-the-shelf computer vision models for image processing during inference. Hence, we use VisProg to generate visual programs given translated statements as obtained in §6.1  ###reference_###. VisProg uses ViLT Kim et al. (2021  ###reference_b18###) as its inherent vision module.\nFigure 4  ###reference_### shows the flow of VisProg. For example, given the translated statement: There is no one in the bedroom on the left, and there is someone in the bedroom on the right, the generated visual program is:\nIf this program is executed on the images in Figure 5  ###reference_###, then it will have ANSWER0 , ANSWER1 , so the final result is .\n###figure_5### For this intervention, we use text-davinci-003555text-davinci-003 is the model that the VisProg authors utilized when running VisProg. as a representative of proprietary LLMs and LLaMA2-70B Touvron et al. (2023  ###reference_b34###) to represent open LLMs. Table 3  ###reference_### shows results to this method. Although this method does not achieve as high accuracy as models evaluated end-to-end in Table 1  ###reference_###, this approach provides valuable insights of breaking down complex reasoning into modular modules and utilizing prompts to make use of LLMs’ strong in-context abilities. In addition, this approach, without any additional training, achieves on par performance on MaRVL, as compared to ViLT post-fintuning."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Addressing Multilinguality + Reasoning + Multimodality: Reasoning with Captions",
            "text": "When analyzing errors for NLVR2, Gupta and Kembhavi (2023  ###reference_b14###) note that 69% of them are caused by the vision module. This might be potentially worse for MaRVL, because open visual modules used in VisProg Kim et al. (2021  ###reference_b18###) are trained on Western-centric datasets like Imagenet Russakovsky et al. (2015  ###reference_b30###). Text-based LLMs, on the other hand, are trained on trillions of tokens, and are known to exhibit cultural awareness, albeit it may be limited Yao et al. (2023  ###reference_b39###). Hence, here we target the last remaining challenge, that of multimodal interaction needed for the reasoning process, by working with image captions instead of images. Concretely, we first caption both images, and use an LLM to reason about the given statement with the two captions, rather than with the two images. Figure 4  ###reference_### shows a flow chart of how this pipeline works.\nTo make sure the captions capture necessary information needed for reasoning about the statement, as a first step of this intervention we use LLMs to generate targeted instructions based on the statement. For instance, consider the statement ”The picture on the left has several pencils of different colors, and the picture on the right has only one pencil” from MaRVL-zh, the targeted instructions for generating captions are:\nLeft image - ”Write a short caption describing the number and colors of pencils;”\nRight image - ”Write a short caption describing the number of pencils”.\n###figure_6### As a second step of this method, we generate captions following the targeted instructions in step 1, using multiple captioning models, including InstructBLIP Liu et al. (2023b  ###reference_b23###), PromptCap Hu et al. (2022  ###reference_b16###), GPT-4V, and LLaVA-13B Liu et al. (2023a  ###reference_b22###). The instructions can point the captioning models to focus on targeted contents in the image. For instance, using the same statement in the first step and the images in Figure 6  ###reference_### as an example, the captions generated by GPT-4V are: \nLeft image - A pencil case containing a single natural wood colored pencil along with other stationery items.; \nRight image - A young mind sketches ideas with a classic green pencil, one among a vast array of tools to capture creativity on paper..\nFor the last step, we prompt LLMs to reason whether the statement match the pair of captions. For instance, using the same example above, GPT-4 reasons as follows:\nReasoning - The statement is False. This is because the left image is described to include only a single two-toned pencil, which implies there are not several pencils of different colors in the left image. However, the statement claims that there should be several pencils of different colors in the left image. Meanwhile, the right image correctly only shows a single green pencil, but the statement is negated by the inaccuracy of the first part regarding the left image.\nUsing this approach, visio-linguistic models only need to process one image at a time, rather then processing a pair of images simultaneously. In addition, reasoning is performed only over text, instead of over both image and text modalities.\nTable 4  ###reference_### show the performance of this intervention. Performance of InstructBLIP and PromptCap under zero-shot setting is on par with UNITER models post-finetuning. For GPT-4V, the overall performance improved minorly; for LLaVA-13B, performance improved by 10% to 16%, depending on language. On average, our pipeline boost LLaVA’s performance on MaRVL by 13.4%, and improving LLaVA’s performance on NLVR2 by 8%. This captioning intervention improves LLaVA’s performance, achieving the best performance under zero-shot setting (without training on pairs of images)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Previous research on instruction tuning sparks multiple works to finetune models on instructions, and create general-purpose models that are good at performing tasks under zero-shot settings Ouyang et al. (2022  ###reference_b27###); Liu et al. (2023b  ###reference_b23###); Geigle et al. (2023  ###reference_b13###). However, instruction tuning data is mostly in English Touvron et al. (2023  ###reference_b34###); Liu et al. (2023b  ###reference_b23###). Due to the absence of multilingual instruction tuning data, models may struggle to effectively process multilingual inputs. Past research efforts has predominantly centered around English language models, highlighting differences in model performance across languages (Blasi et al., 2022  ###reference_b4###; Song et al., 2023  ###reference_b31###). In the visio-linguistic domain, research in instruction tuning also center on English, due to a lack of multilingual instruction training data Geigle et al. (2023  ###reference_b13###). To this end, mBLIP Geigle et al. (2023  ###reference_b13###) translated instruction training data to various languages, and perform instruction tuning. This is the first multilingual instruction tuned vision LLM. Currently, there is a trend of shifting from openly releasing models to paid APIs Abdalla et al. (2023  ###reference_b1###). Previous research on examining GPT-4V demonstrates its unprecedented multimodal capabilities, and there is still a gap between this proprietary model and other open source models Yang et al. (2023  ###reference_b38###). However, it is still important for the community to have as strong open source multimodal models.\n\nAI in healthcare diagnostics has witnessed significant strides, particularly with deep learning models enhancing diagnostic accuracy (Esteva et al., 2017; Rajpurkar et al., 2018). These models have shown potential in fields like radiology and pathology, yet challenges remain in ensuring reliability and addressing biases inherent in clinical settings (Pesapane et al., 2018; Naylor et al., 2023)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our study explores the evolving domain of multilingual visual reasoning. We observe a trend towards inclusivity in models, yet recognize persistent disparities in performance across languages and cultures. While proprietary systems like GPT-4V exhibit notable accuracy and equity across languages, open models still face challenges in bridging the gap, especially for low-resource languages. Our comparative analysis highlights the superior performance of GPT-4V but also underscores the need for advancements in open models. Through strategic interventions addressing multilinguality, multimodality, and reasoning, we showcase promising avenues for improvement. Leveraging our interventions, we demonstrate significant enhancements in open model performance, achieving state-of-the-art results under zero-shot settings for open models. Our findings emphasizes the potential for further advancements in language-equitable multimodal reasoning, with the aim of narrowing down the gap between human and machine performance, and the gap between proprietary and open models.\nThe authors thank the members of CMU who gave useful feedback on an early version of the draft. This work was supported in part by grants from Google and DSTA Singapore."
        }
    ]
}