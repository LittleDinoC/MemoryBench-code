{
    "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models",
    "abstract": "In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones.\nOur evaluations demonstrate FastV’s ability to dramatically reduce computational costs (e.g., a 45% reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for the deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Vision-Language Models (LVLMs) have become a hit in both computer vision and natural language processing studies. We have witnessed tremendous creative research and applications that are built upon powerful LVLMs Liu et al. (2023c  ###reference_b24###; 2024a  ###reference_b22###); Team et al. (2023  ###reference_b33###); Bai et al. (2023  ###reference_b2###). From describing the given picture to navigating the internet Zheng et al. (2024  ###reference_b42###), using smartphones Wang et al. (2024  ###reference_b36###) and making decision in the real world Driess et al. (2023  ###reference_b8###); Chen et al. (2024  ###reference_b5###), large language models with vision abilities is reshaping how we interact with AI systems, which can not be achieved solely by language or vision uni-modal models.\nCurrently, a majority of popular LVLMs rely on sequential visual representation, where images are transformed to hundreds or thousands of tokens when feeding to LLM together with language prompts OpenAI (2023  ###reference_b29###); Zhu et al. (2023  ###reference_b43###); Liu et al. (2023c  ###reference_b24###; b  ###reference_b23###; 2024b  ###reference_b25###); Zhao et al. (2023  ###reference_b41###); Bai et al. (2023  ###reference_b2###); Li et al. (2023d  ###reference_b16###; c  ###reference_b15###); Lin et al. (2023b  ###reference_b20###).\nAs LVLMs leverage the advanced emergent capabilities inherent in their language components, they concurrently face a surge in computational complexity, correlating with budget increments. This complexity stems from the principle that the proficiency of Large Language Models (LLMs) is predominantly influenced by their scale. Two critical areas remain under-explored in this context: 1) How do language models process and interpret images? and 2) While the efficient training and inference of LLMs have attracted considerable attention, these dimensions within LVLMs are yet to be thoroughly examined and understood.\nIn this paper, we uncover the fact that current LVLMs actually apply an inefficient way while processing image information.\nSpecifically, the image tokens receive strikingly lower attention scores compared to their textual counterparts within the token-based LVLMs like LLaVA. The degree of imbalance also varies between the shallow and deep layers.\nFrom our experiments a wide range of vision-language tasks, we observed that within the deep layers (after layer 2) of renowned LVLMs such as LLaVA 1.5,\nimage tokens receive an average attention score that amounts to only 0.21% of the score attributed to system prompts.\nIn contrast, this figure reaches 50% in the initial two layers.\nThese observations raise questions upon the optimal utilization of visual information within LVLMs.\nTo address the problem, we assume a plausible explanation is that the high redundancy in visual signals leads to the aggregation of image-related, instruction-specific features onto certain “anchor” tokens through the self-attention mechanism in the shallow layers. Notably, these anchor tokens might not be image tokens. In deep layers, attentions are focused on those anchor tokens, leading to significantly reduced attention on the image tokens themselves.\nThe phenomena inspires us to propose FastV, a dynamic image tokens pruning method to reduce the inference budget of LVLMs. Our findings suggest an intriguing possibility: Given that image tokens contribute minimally to output generation in deeper layers due to diminished attention, why not consider removing them at these stages? FastV implements an image token pruning strategy at one specific layer of LLM. Prior to this layer, computations proceed as usual. Beyond this selected layer, image tokens are re-evaluated based on their average received attention scores. Tokens falling below a predefined attention score threshold are then selectively discarded in subsequent layers, streamlining the process by focusing on the most impactful tokens.\nCompared to other attention-based methods for accelerating inference, such as sparse attention, FastV’s most notable distinction lies in its direct elimination of tokens. This approach not only bypasses the computational demand of the self-attention module but also the Feed-Forward Network (FFN) module in deeper layers. As a result, FastV achieves a great theoretical reduction in FLOPs while maintaining relatively high performance as shown in Figure 1  ###reference_###’s experiment on LLaVA and Qwen-VL-Chat models. Our experiment on LLaVA-1.5-13B model shows that we can filter out 50% image tokens after layer 2 without sacrificing the average performance on a combination of Vision-Language tasks including captioning tasks like Nocaps Agrawal et al. (2019  ###reference_b1###), Flickr30K Plummer et al. (2015  ###reference_b30###), multimple choice tasks like A-OKVQA Schwenk et al. (2022  ###reference_b32###), MMMU Yue et al. (2023  ###reference_b40###), complex embodied reasoning task like PCA-Bench Chen et al. (2024  ###reference_b5###; 2023  ###reference_b4###), tasks requiring detailed OCR ablitily like OCR-VQA Mishra et al. (2019  ###reference_b28###) and more challenging video understanding tasks Jang et al. (2017  ###reference_b10###); Xu et al. (2017a  ###reference_b38###; b  ###reference_b39###).\nOur latency test experiment on A-OKVQA showed that LLaVA-13B model with FastV could achieve a lower latency than LLaVA-7B model while maintaining superior performance. This result highlights the effectiveness of FastV in balancing the trade-off between speed and accuracy in LVLMs.\nResearches Liu et al. (2023c  ###reference_b24###); Li et al. (2023f  ###reference_b18###) underscore the significance of enhancing image resolution for the performance of LVLMs. However, it’s equally important to note that increased resolution comes with its own challenges, including a rise in the computational budgets such as longer image token sequence and inference latency. We also conduct experiments on training LVLM in different image feature resolution by setting pooling layer of different strides. Specifically, with an equal number of image tokens, models equipped with FastV can process higher resolution images, leading to better performance than models limited to lower resolution features. This finding highlights the potential to enhance downstream performance by increasing image resolution without incurring additional inference costs.\nIn summary, the contribution of the work are three-folds:\nIdentify and analyze the inefficient visual attention phenomena in prevailing LVLMs.\nPropose FastV, a plug-and-play method to significantly reduce inference budget for LVLMs without sacrificing performance inspired by our observation.\nValidate the effectiveness of FastV on a wide range of vision-language tasks across different LVLMs with thorough ablations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "To benefit from the advancement of LLM and integrate visual information into the LLM, large Vision-Language Models utilize a Visual Prompt Generator Li et al. (2023a  ###reference_b13###) to transform the visual embeddings into prompts that the language model can comprehend Li et al. (2023b  ###reference_b14###); Liu et al. (2023c  ###reference_b24###), resulting in a significant increase in required tokens. Handling higher resolution images inevitably necessitates an exponential increase in the number of needed tokens. For instance, LLAVA process 336x336 images into 576 tokens Liu et al. (2023b  ###reference_b23###) and process images with a greater resolution of 672x672 into 2304 tokens Liu et al. (2024b  ###reference_b25###). Fuyu Bavishi et al. (2023  ###reference_b3###), in a similar vein, translates pixel-level images of 1080x1080 into 1296 tokens. Understanding and generating multiple images or videos also inherently demands an escalated count of tokens for vision information. Both Video-Poet Kondratyuk et al. (2023  ###reference_b11###) and Unified-IO2 Lu et al. (2023  ###reference_b26###) are compelled to reserve thousands of tokens within the context to facilitate the understanding and generation of multiple images or videos. Large multimodal models like Gemini Team et al. (2023  ###reference_b33###) and LWM Liu et al. (2024a  ###reference_b22###) highlights the significance of long context in developing a robust understanding of the world model and extending the context length to 1M to address the issue of escalating context requirements. Nonetheless, considering the limitations of computing resources, this poses a significant challenge in exploiting the available resources for this purpose optimally. Further research and development in computational technologies are needed to surpass these obstacles and fully unlock the potential of LVLMs. Efficient inference in LLMs is challenged by their autoregressive generation where each token prediction depends on the preceding context. Hence, considering the quadratic complexity of computation’s attention during training, as the context length increases, the generation becomes progressively slower. To tackle these challenges, pioneering studies fall into two categories: methods optimizing memory consumption for attention module like FlashAttention, vLLM and RingAttention Dao et al. (2022  ###reference_b7###); Dao (2023  ###reference_b6###); Kwon et al. (2023  ###reference_b12###); Liu et al. (2023a  ###reference_b21###), which ensure no drastic shifts in the results, and methods like StreamingLLM and FastGen Xiao et al. (2023  ###reference_b37###); Ge et al. (2024  ###reference_b9###) that simplify computations by pruning redundant attention computation. We are interested in the second kind of methods since they are proposed inspired by the distinct attention patterns observed in LLM’s inference. While these methods have boosted the inference efficiency of LLMs, they are designed for text-only language models, and whether their effectiveness can be transferred to LVLMs remain under-explored. There is previous work attempt to handle the long-context in LVLMs efficiently, like LLaMA-VID Li et al. (2023e  ###reference_b17###), which utilizes cross-attention to effectively represent each video frame with two key tokens, the requirement for an additional fine-tuning stage obstructs its broad applicability for different LVLMs.\n\nMachine learning in healthcare diagnostics holds transformative potential, leveraging AI for accurate disease identification. Esteva et al. (2017) demonstrated deep learning’s proficiency in dermatology, while Hannun et al. (2019) showed promise in cardiology. Recent advances underline AI's enhancement of diagnostic accuracy across diverse medical fields, heralding a new era in healthcare."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Inefficient Visual Attention in VLLMs",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "In this section, we delve into how LVLMs process visual tokens during output generation from the perspective of self-attention module. For an image-question pair , the given LVLM , usually in the structure of transformer Vaswani et al. (2017  ###reference_b34###) decoder, predicts the answer  in an auto-regressive manner:\n###figure_1### Multimodal information, encompassing both images and text, is transformed into sequential embeddings prior to being processed by the transformer model. For images, a commonly used approach is to employ a pretrained encoder, such as CLIP-VIT Radford et al. (2021  ###reference_b31###), to extract visual features. These features are then linearized by eliminating the spatial dimension. Additional linear transformations Zhu et al. (2023  ###reference_b43###); Liu et al. (2023b  ###reference_b23###) or cross-attention Li et al. (2023b  ###reference_b14###); Bai et al. (2023  ###reference_b2###) modules are utilized to adjust the size of the visual features to match the embedding size of the Large Language Model (LLM) and to achieve semantic alignment. Regarding text, a tokenizer breaks down the natural language into discrete tokens and then performs an embedding lookup to form text embeddings. In the rest of the paper, we refer to ’visual tokens’ and ’text tokens’ not merely as the discrete units of visual and textual data but as the embeddings derived from these units.\nAs illustrated in Figure 2  ###reference_###, after preprocessing the image and text token to a unified embedding space, they are fed to the transformer decoder to generate output tokens. The input tokens at each decoding step can be categorized into four distinct types: system prompt (sys), image tokens (img), user instruction (ins), and output tokens (out). The system prompts for LVLMs usually inherit the backbone LLM, used as a general message to control the LLM’s behavior, which is decided during the instruction tuning stage of LLM. Image tokens are the linearized image features transformed by a pretrained vision encoder. User instruction specifies the query question for the given image. Output tokens are generated step by step conditioned on the preceding tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Experiment Settings",
            "text": "To explore how LVLMs process image tokens, we first randomly sample  image-text pairs  from a combination of vision langauge tasks including image caption (Flickr30K), embodied reasoning (PCA-Bench), visual question answering (A-OKVQA), multimodal understanding and reasoning (MMMU) and then prompt the LVLM to generate  responses .\nDuring the decoding process of one response, we collect each output tokens’ attention score distribution  in different layers and sum up for different type of input tokens. That is, for the -th token, in the -th layer, we compute  to denote the total attention score current token attends to the system prompt, image tokens, user instruction and output tokens. We have:\nWe compute the total attention allocation  to denote the total attention score one type of tokens received in one layer. For example, the total attention of system prompt in layer  is:\n###figure_2### where  is the number of tokens in the response. Final attention allocation is averaged over all attention heads in the  image-text pairs we sampled.\nNext, we define metric attention efficiency  to denote the average attention score per type’s token received in one layer during the decoding process of one response. For example, the attention efficiency of image tokens in layer  is:\nwhere  is the number of image tokens,  is the number of tokens in the response. Final attention efficiency is averaged over all attention heads in the  image-text pairs we sampled.\nIn our experiment,  is set to 1000 and we use LLaVA1.5-7B as the LVLM. We follow the same generation configuration as the original paper Liu et al. (2023c  ###reference_b24###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "We have two major findings in the attention pattern statistics regrading attention allocation  and attention efficiency  for different type of input tokens. We define the first 2 layers as shallow layer and the rest 30 layers as deep layers.\nBoth attention allocation and attention efficiency show different degree of imbalance, which is related to the layer depth. The average attention allocation and efficiency in different layer is shown in Figure 3  ###reference_###. In shallow layer the attention allocation is relatively more balanced than in deep layers. In shallow layer, the output tokens tends to attend to the previous output tokens while in deep layers, they tend to attend to the system prompt.\nImage tokens have the lowest attention efficiency in both shallow and deep layers. System prompt is of extremely high attention efficiency in deep layers, which is 472 times that of image tokens, taking up 85% total attention scores.\n###figure_3###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Insights",
            "text": "The statistics reveal a surprising trend in the decoding process of LVLMs: despite accounting for the majority of tokens in the input, image tokens receive significantly less attention.\nConversely, system prompts, which provides the minimal semantic information, attract the most of the attention scores.\nTo delve deeper into this phenomenon, we analyze the attention maps of the first, middle, and last layers during during the decoding process of a model response as shown in Figure 4  ###reference_###. The attention maps for all layers are provided in figure-7 of the supplement material.\nFrom the attention visualization results, we can see that in shallow layer, the attention scores distribute more smoothly across different tokens. While in deep layer, there are vertical strong lines (in the system prompt) that takes up most of attention scores. The existence of vertical strong line shows that there are some input tokens that consistently received high attention during the whole decoding process. This also explains the highly imbalanced attention efficiencies in our statistics: A small portion of anchor tokens draw the most attention and the model much favors to attend to those anchor tokens in deep layers. Much attention is aggregated to the beginning system prompt, which leads to the severe inefficient visual attention in LVLMs. Our findings also align with the attention sink phenomena of Large Language Model found in  Xiao et al. (2023  ###reference_b37###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "FastV",
            "text": "With insights from the validated phenomena and explanation, we propose FastV as a solution to reduce the inference budgets of LVLMs without sacrificing the performance.\n###figure_4###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Dynamically Prune Vision Tokens",
            "text": "Figure 5  ###reference_### illustrates the general idea of FastV. The key is the image token re-rank and filtering module. It consists of one ranking function  and two parameters: filtering layer  and filtering ratio . At layer  of the LVLM, the ranking function  takes a sequence of input tokens and rank them by certain importance criteria . The last  tokens after ranking would be pruned out in successive layers. We simply compute the average attention-score one token received from all other tokens as the criteria  in our experiment.\nIn extreme condition,  could be also set to 0, that image tokens are pruned before sending to the language model, we use random ranking as the criteria  where image tokens are randomly dropped.\nFastV is plug-and-play to different token-based LVLMs for various vision language tasks without the need of training the model. We take video understanding tasks with VideoLLaVA Lin et al. (2023a  ###reference_b19###) as example as shown in Figure 5  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Computing Budget Estimation",
            "text": "###figure_5### We consider the computation of multi-head attention (MHA) and feed-forward network (FFN) module in the FLOPs estimation. For one transformer layer, assume  is the token number,  is the hidden state size,  is the intermediate size of FFN, the total FLOPs can be estimated by . For the whole model, assume FastV prunes tokens from  to  after layer  and there are T layers at all. The theoretical FLOPs reduction ratio related to image tokens is computed as:\nWe plot a 3D graph to show how the FLOPs reduction ratio changes with FastV’s parameter  and  in Figure 6  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Comparison: Training With Less Visual Tokens",
            "text": "FastV achieves computation reduction through eliminating redundant visual tokens during inference stage. An alternative method to reduce visual tokens is directly training with less visual tokens. This could be simply done by conducting pooling on the output of visual encoder during LVLM’s training process. We compare FastV and this method in our ablation studies (sec. 5.4  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "Image caption requires the model to generate a description for a given image. We choose Nocaps Agrawal et al. (2019  ###reference_b1###) and Flickr30k Plummer et al. (2015  ###reference_b30###) as benchmarks and report CIDEr score Vedantam et al. (2015  ###reference_b35###) as metric.\nVQA requires the model to generate an answer for a given image-question pair. We select the development set of A-OKVQA Schwenk et al. (2022  ###reference_b32###) and the test set of OCR-VQA  Mishra et al. (2019  ###reference_b28###) as the benchmark and the report the multiple choice (MC) score of AOKVQA and Rouge-L score of OCR-VQA.\nCompared with VQA, multimodal reasoning requires more advanced perception, knowledge and reasoning skills of the model, which are more suitable benchmarks to evaluate the integrated abilities of LVLMs. We choose MMMU and PCA-Bench Chen et al. (2024  ###reference_b5###) as benchmarks. MMMU is a multimodal benchmark featuring multi-discipline tasks\ndemanding college-level subject knowledge and reasoning skills. PCA-Bench is a complex embodied reasoning benchmark with error localization, which features three different domains including autonomous driving, robot and game. We report the multiple choice accuracy for the development set of MMMU and Perception, Cognition, Action, Genuine PCA scores for both the open and closed test set of PCA-Bench.\nSimlar to VQA for single image, Video Question Answering requires the model to generate answer given a video-question pair. Current LVLMs usually deal with video question answering tasks by sampling multiple frames as input, resulting in longer image token sequences. We choose TGIF-QA Jang et al. (2017  ###reference_b10###), MSVD-QA Xu et al. (2017b  ###reference_b39###) and MSRVTT-QA Xu et al. (2017a  ###reference_b38###) as benchmarks following the evaluation pipeline of Video-ChatGPT Maaz et al. (2023  ###reference_b27###) and report the accuracy and chatgpt-score as metrics. We use the first 1K examples in each benchmark in our experiments due to the limited commercial API usage in evaluation.\nThe performance on tasks under different FastV settings are shown in Table 1  ###reference_### (Nocaps, Flickr30k, A-OKVQA, MMMU) and Table 3  ###reference_### (PCA-Bench, OCR-VQA). The result of latency test is shown in Table 2  ###reference_###.\nIn Table 1  ###reference_###, we present the performance trend with FLOPs ratio ranging from 19% to 100% by FastV, for different type and size of models. We also plot the relation between FLOPs Reduction ratio (1-FLOPs Ratio) and average performance in Figure 1  ###reference_###. The results indicate that FastV (K=2, R=50%) could achieve about 45% FLOPs reduction for different LVLMs without sacrificing the performance. The FLOPs-Performance trade-off is is also highly adjustable by lowering  and increasing  if we want to pursue an ultimate speed up. As shown in the latency test (Table 2  ###reference_###), an 13B model with FastV could inference as fast as a 7B model with superior performance for A-OKVQA.\nIn PCA-Bench and OCR-VQA, (Table 3  ###reference_###), which runs finegrained analysis on perception, cognition, action and OCR abilities, we find that FastV (K=2, R=50%) could maintain the sub-scores while significantly decreasing the FLOPs.\nThe results of FastV on different video question answering tasks in shown in table 4  ###reference_### (TGIF, MSVD, MSRVTT). To our surprise, we find FastV could generally improves the Video-QA tasks performance while saving 40%+ computations especially for the TGIF task. We think the main reason is that the redundancy information problem is more severe for video understanding as multiple images from the video are transformed to tokens when sending to the LLM. For example, an image costs 576 tokens in LLaVA1.5 model, while a video costs 2048 tokens in Video-LLaVA. As shown in the case from Figure 5  ###reference_###, setting suitable FastV parameters could lead to much FLOPs reduction for Video-LLaVA while the outputs are nearly identical.\nConsidering the parameters of FastV, a lower  and higher  would generally result in a more significant FLOPs reduction with relative performance drop. Above from the decreased inference budget, it is interesting to note that sometimes FastV could lead to better performance than vanilla decoding. This\nbolsters our theory that FastV can be seen as a redundancy reduction operation.\nWe conduct an ablation experiment on how the parameters (K and R) influence the acceleration and downstream task’s performance. We select OCR-VQA as the task, which necessitates a through understanding of the image. The result is shown in Figure 7  ###reference_###. When K is small, lowering R would improve the performance with a smaller FLOPs reduction ratio. In contrast, when K is large, adjusting R has minimal impact on the overall performance. This observation further proves that in deep layers, there is high redundancy in image tokens.\nFastV reduces computational requirements (FLOPs) by pruning tokens during the inference stage. An alternative approach for token reduction involves training the LVLM at a lower resolution. To facilitate a fair comparison, we retrained two LLaVA1.5-7B models, adhering to the original pretraining and supervised finetuning protocols. The sole modification in the second model’s training process was the incorporation of an average pooling layer (with a stride of 2) following the Clip encoder, leading to a 50% reduction in image tokens during training. A comparison between lines (a) and (b) in Table 5  ###reference_### reveals that reducing the input resolution directly during training results in diminished performance. Conversely, FastV manages to decrease the number of image tokens without compromising performance, showcasing its efficiency in balancing computational savings with model efficacy.\nFastV strategically reduces the number of image tokens during the inference phase of LVLMs, motivated by our observation that image tokens exhibit the lowest attention efficiency relative to other types of input tokens. In experiments detailed in lines (d) and (f) of the study, we specifically pruned tokens that were not related to images, such as system prompts and instruction tokens. This selective pruning resulted in significant performance declines, even when only a minimal number of non-image tokens were removed. We also compare randomly drop visual tokens instead of dropping by attention rank, as shown in line (c). It resulted in declined results compared with origin FastV (b). These findings underscore the distinct roles that visual and textual tokens play within LVLMs. It highlights FastV’s effectiveness in precisely targeting image tokens for reduction, thereby optimizing performance without compromising the model’s overall functionality.\nIn our previous observation about attention efficiency, we find out that the system prompt takes up of most attention even if they carry the least semantic information in the context. We conduct another experiment by directly prune the first half tokens of the system prompt. Comparing line (d) and (e), we can find that the head tokens in the system prompt have dominant effect on the model performance. Our findings also align with StreamingLLM Xiao et al. (2023  ###reference_b37###) where they find that the first 4 tokens in LLM play the most important role during inference.\n###figure_6### As we find that LVLM and LLM bear similar attention patterns that the head tokens play an important role during inference, we wonder that whether the same attention optimization technique curated for LLM can be transfered to LVLM. Following the methodology outlined in StreamingLLM Xiao et al. (2023  ###reference_b37###), we implemented an attention optimization scheme that incorporates an attention sink for the first four tokens and keep all local attention (context size=4) for the preceding four tokens throughout the decoding process. The experiment’s outcome, presented in line (g) of Table 5  ###reference_###, indicates a substantial degradation in LVLM’s performance when this specific attention pattern is applied. This suggests a fundamental difference in how image tokens, as opposed to text tokens, contribute to the information processing within LLMs. This observation highlights the necessity for further investigation into the unique behaviors of image tokens within the LVLM framework, suggesting that the direct application of LLM attention/KV-Cache optimization mechanisms to LVLMs requires adaptation to accommodate the distinct characteristics of visual information."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Evaluation Tasks",
            "text": "We conduct a wide range of evaluation including image and video understanding tasks to examine the influence of FastV on the performance of LVLMs. We use greedy search for all experiments and provide details on the prompts used for each task in the section A of supplement material.\nImage caption requires the model to generate a description for a given image. We choose Nocaps Agrawal et al. (2019  ###reference_b1###  ###reference_b1###) and Flickr30k Plummer et al. (2015  ###reference_b30###  ###reference_b30###) as benchmarks and report CIDEr score Vedantam et al. (2015  ###reference_b35###  ###reference_b35###) as metric.\nVQA requires the model to generate an answer for a given image-question pair. We select the development set of A-OKVQA Schwenk et al. (2022  ###reference_b32###  ###reference_b32###) and the test set of OCR-VQA  Mishra et al. (2019  ###reference_b28###  ###reference_b28###) as the benchmark and the report the multiple choice (MC) score of AOKVQA and Rouge-L score of OCR-VQA.\nCompared with VQA, multimodal reasoning requires more advanced perception, knowledge and reasoning skills of the model, which are more suitable benchmarks to evaluate the integrated abilities of LVLMs. We choose MMMU and PCA-Bench Chen et al. (2024  ###reference_b5###  ###reference_b5###) as benchmarks. MMMU is a multimodal benchmark featuring multi-discipline tasks\ndemanding college-level subject knowledge and reasoning skills. PCA-Bench is a complex embodied reasoning benchmark with error localization, which features three different domains including autonomous driving, robot and game. We report the multiple choice accuracy for the development set of MMMU and Perception, Cognition, Action, Genuine PCA scores for both the open and closed test set of PCA-Bench.\nSimlar to VQA for single image, Video Question Answering requires the model to generate answer given a video-question pair. Current LVLMs usually deal with video question answering tasks by sampling multiple frames as input, resulting in longer image token sequences. We choose TGIF-QA Jang et al. (2017  ###reference_b10###  ###reference_b10###), MSVD-QA Xu et al. (2017b  ###reference_b39###  ###reference_b39###) and MSRVTT-QA Xu et al. (2017a  ###reference_b38###  ###reference_b38###) as benchmarks following the evaluation pipeline of Video-ChatGPT Maaz et al. (2023  ###reference_b27###  ###reference_b27###) and report the accuracy and chatgpt-score as metrics. We use the first 1K examples in each benchmark in our experiments due to the limited commercial API usage in evaluation."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Model Settings",
            "text": "We test FastV with various open source models. For image understanding tasks, we conduct experiments on LLaVA1.5-7B, 13B111https://github.com/haotian-liu/LLaVA  ###reference_### Liu et al. (2023b  ###reference_b23###), and Qwen-VL222https://github.com/QwenLM/Qwen-VL  ###reference_github.com/QwenLM/Qwen-VL### Bai et al. (2023  ###reference_b2###). When it comes to video understanding tasks, our baseline model is VideoLLaVA333https://github.com/PKU-YuanGroup/Video-LLaVA  ###reference_VA### Lin et al. (2023a  ###reference_b19###). We adopt the settings as reported in their paper for the baseline models."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "The performance on tasks under different FastV settings are shown in Table 1  ###reference_###  ###reference_### (Nocaps, Flickr30k, A-OKVQA, MMMU) and Table 3  ###reference_###  ###reference_### (PCA-Bench, OCR-VQA). The result of latency test is shown in Table 2  ###reference_###  ###reference_###.\nIn Table 1  ###reference_###  ###reference_###, we present the performance trend with FLOPs ratio ranging from 19% to 100% by FastV, for different type and size of models. We also plot the relation between FLOPs Reduction ratio (1-FLOPs Ratio) and average performance in Figure 1  ###reference_###  ###reference_###. The results indicate that FastV (K=2, R=50%) could achieve about 45% FLOPs reduction for different LVLMs without sacrificing the performance. The FLOPs-Performance trade-off is is also highly adjustable by lowering  and increasing  if we want to pursue an ultimate speed up. As shown in the latency test (Table 2  ###reference_###  ###reference_###), an 13B model with FastV could inference as fast as a 7B model with superior performance for A-OKVQA.\nIn PCA-Bench and OCR-VQA, (Table 3  ###reference_###  ###reference_###), which runs finegrained analysis on perception, cognition, action and OCR abilities, we find that FastV (K=2, R=50%) could maintain the sub-scores while significantly decreasing the FLOPs.\nThe results of FastV on different video question answering tasks in shown in table 4  ###reference_###  ###reference_### (TGIF, MSVD, MSRVTT). To our surprise, we find FastV could generally improves the Video-QA tasks performance while saving 40%+ computations especially for the TGIF task. We think the main reason is that the redundancy information problem is more severe for video understanding as multiple images from the video are transformed to tokens when sending to the LLM. For example, an image costs 576 tokens in LLaVA1.5 model, while a video costs 2048 tokens in Video-LLaVA. As shown in the case from Figure 5  ###reference_###  ###reference_###, setting suitable FastV parameters could lead to much FLOPs reduction for Video-LLaVA while the outputs are nearly identical.\nConsidering the parameters of FastV, a lower  and higher  would generally result in a more significant FLOPs reduction with relative performance drop. Above from the decreased inference budget, it is interesting to note that sometimes FastV could lead to better performance than vanilla decoding. This\nbolsters our theory that FastV can be seen as a redundancy reduction operation."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Ablation Studies",
            "text": "We conduct ablation studies to evaluate the impact of various components within FastV, with the findings detailed in Figure 7  ###reference_### and Table 5  ###reference_###.\nWe conduct an ablation experiment on how the parameters (K and R) influence the acceleration and downstream task’s performance. We select OCR-VQA as the task, which necessitates a through understanding of the image. The result is shown in Figure 7  ###reference_###  ###reference_###. When K is small, lowering R would improve the performance with a smaller FLOPs reduction ratio. In contrast, when K is large, adjusting R has minimal impact on the overall performance. This observation further proves that in deep layers, there is high redundancy in image tokens.\nFastV reduces computational requirements (FLOPs) by pruning tokens during the inference stage. An alternative approach for token reduction involves training the LVLM at a lower resolution. To facilitate a fair comparison, we retrained two LLaVA1.5-7B models, adhering to the original pretraining and supervised finetuning protocols. The sole modification in the second model’s training process was the incorporation of an average pooling layer (with a stride of 2) following the Clip encoder, leading to a 50% reduction in image tokens during training. A comparison between lines (a) and (b) in Table 5  ###reference_###  ###reference_### reveals that reducing the input resolution directly during training results in diminished performance. Conversely, FastV manages to decrease the number of image tokens without compromising performance, showcasing its efficiency in balancing computational savings with model efficacy.\nFastV strategically reduces the number of image tokens during the inference phase of LVLMs, motivated by our observation that image tokens exhibit the lowest attention efficiency relative to other types of input tokens. In experiments detailed in lines (d) and (f) of the study, we specifically pruned tokens that were not related to images, such as system prompts and instruction tokens. This selective pruning resulted in significant performance declines, even when only a minimal number of non-image tokens were removed. We also compare randomly drop visual tokens instead of dropping by attention rank, as shown in line (c). It resulted in declined results compared with origin FastV (b). These findings underscore the distinct roles that visual and textual tokens play within LVLMs. It highlights FastV’s effectiveness in precisely targeting image tokens for reduction, thereby optimizing performance without compromising the model’s overall functionality.\nIn our previous observation about attention efficiency, we find out that the system prompt takes up of most attention even if they carry the least semantic information in the context. We conduct another experiment by directly prune the first half tokens of the system prompt. Comparing line (d) and (e), we can find that the head tokens in the system prompt have dominant effect on the model performance. Our findings also align with StreamingLLM Xiao et al. (2023  ###reference_b37###  ###reference_b37###) where they find that the first 4 tokens in LLM play the most important role during inference.\n###figure_7### As we find that LVLM and LLM bear similar attention patterns that the head tokens play an important role during inference, we wonder that whether the same attention optimization technique curated for LLM can be transfered to LVLM. Following the methodology outlined in StreamingLLM Xiao et al. (2023  ###reference_b37###  ###reference_b37###), we implemented an attention optimization scheme that incorporates an attention sink for the first four tokens and keep all local attention (context size=4) for the preceding four tokens throughout the decoding process. The experiment’s outcome, presented in line (g) of Table 5  ###reference_###  ###reference_###, indicates a substantial degradation in LVLM’s performance when this specific attention pattern is applied. This suggests a fundamental difference in how image tokens, as opposed to text tokens, contribute to the information processing within LLMs. This observation highlights the necessity for further investigation into the unique behaviors of image tokens within the LVLM framework, suggesting that the direct application of LLM attention/KV-Cache optimization mechanisms to LVLMs requires adaptation to accommodate the distinct characteristics of visual information."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Limitations",
            "text": "The FLOPs reduction ratio is based on the theoretical calculation considering the removal of image tokens, while actual inference budget can be influenced by a variety of factors such as inference framework optimization, specific CUDA kernels and hardwares. We are working on integrating FastV into mainstream LLM inference frameworks such as vLLM Kwon et al. (2023  ###reference_b12###) for broader application."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose FastV, a plug-and-play inference budget optimization method for Large Vision-Language Models. Our insight for FastV arises from our observation that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs though they take up a large portion of input tokens. FastV prunes out the unnecessary visual tokens according to the attention score ranking, which results in significant inference budget reduction without sacrificing performance."
        }
    ]
}