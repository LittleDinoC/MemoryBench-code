{
    "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
    "abstract": "When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model’s ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions—a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model—the Questioner—and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer’s latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on  \n\n\n72%\n of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### When interacting with users who have different preferences, language models (LMs) encounter task ambiguity (Finn et al., 2018  ###reference_b6###; Tamkin et al., 2022  ###reference_b31###). Depending on the user, the same request might correspond to a different task. For example, consider a user who asks an LM for a pasta recipe (Figure 1  ###reference_###). If the model could elicit information about the user’s dietary restrictions, favorite sauces, and preferred cooking methods, it could tailor the recipe to their specific needs and desires. The model might suggest a vegetarian pasta recipe for a user who is vegetarian, or propose a traditional lasagna recipe for a user with a passion for Neapolitan cuisine. However, if this information is not explicitly specified in the prompt, the model may generate a generic recipe that fails to account for the user’s unique preferences and constraints. In high-stakes domains like healthcare or education, such task ambiguity can have significant consequences.\nOne approach to resolving task ambiguity is by asking targeted questions to elicit relevant information from users. Prompting closed-source LMs can yield useful questions (e.g., Li et al., 2023  ###reference_b16###; Piriyakulkij et al., 2023  ###reference_b21###). However, this approach is inflexible in guiding a model’s questioning strategy and frequently generates queries that are ineffective or irrelevant for the task at hand.\nIndeed, it is likely that current alignment strategies—such as RLHF—specifically inhibit the ability to\ncarry out such dialog (Shaikh et al., 2023  ###reference_b27###). One recent effort addresses these limitations by combining elicitation with optimal experimental design methods (Handa et al., 2024  ###reference_b9###). However, this approach constrains questions to pairwise comparisons over a fixed set of features, substantially limiting the space of questions that can be used to probe user preferences. Another approach is to use offline reinforcement learning to encourage useful dialog (Hong et al., 2023  ###reference_b11###). This is promising but requires offline generation of high-quality dialog from an expert model, and has not targeted questions for preference elicitation specifically.\nIn this paper, we explore whether we can improve a LM’s ability to ask useful questions by bootstrapping with a form of self-play (Silver et al., 2017  ###reference_b29###; Anthony et al., 2017  ###reference_b1###). We introduce STaR-GATE (Figure 2  ###reference_###), an iterative algorithm that combines active preference elicitation (GATE; Li et al., 2023  ###reference_b16###) with a self-improvement loop inspired by STaR (Zelikman et al., 2022  ###reference_b37###). We address several technical challenges: (1) We define a task setting for improving elicitation for which we generate a synthetic dataset of 25,500 unique persona-task prompts; (2) We define a reward function based on the log probability of gold responses generated by an oracle model (with access to the persona); and (3) We encourage the LM to use the elicited information while avoiding distribution shift through response regularization.\nWe find that questions asked by the finetuned model increase the probability of gold responses consistently across iterations (Figure 4  ###reference_###).\nMoreover, compared to responses generated by the initial model, responses generated by a STaR-GATE finetuned model have  \n\n\n72%\n win rates (Figure 3  ###reference_###a) after two iterations.\n###figure_2### In summary, we make the following contributions: (1) We introduce STaR-GATE, a simple algorithm that iteratively improves a LM’s ability to elicit user preferences by asking questions. (2) We generate a synthetic dataset consisting of 25,500 unique persona-task prompts, each paired with a personalized, gold response. (3) We show that finetuning with STaR-GATE enables a LM to generate questions that significantly increase the probability of generating gold responses. (4) We show that adding response-regularization to STaR-GATE yields a fine-tuned model able to use the elicited preferences to generate better responses—a high win rate against the initial model. (5) We show that the finetuned model generalizes beyond the roleplayer it was trained with."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_3###\n\nAI ethics in autonomous systems focuses on ensuring that these systems operate fairly and transparently while respecting user privacy and safety. Scholars argue that establishing accountability frameworks is crucial to mitigate biases and avoid unintended harm. Continued collaboration between technologists and ethicists is necessary to develop guidelines that shape ethical AI deployment."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preference Optimization",
            "text": "Preference optimization algorithms, such as RLHF (Christiano et al., 2017), DPO (Rafailov et al., 2024), or KTO (Ethayarajh et al., 2024), optimize LMs to provide single-turn dialog responses that reflect preferred or high-utility outcomes. As a result, these models learn distributions over responses that effectively generate answers to user queries without requiring additional information beyond the initial prompt. However, asking follow-up questions to elicit user preferences is essential for understanding their unique needs and desires, especially when faced with task ambiguity (Tamkin et al., 2022; Li et al., 2023). Despite the importance of follow-up questions for effective communication, recent research has shown that preference optimization algorithms can reduce a LM’s ability to ask follow-up questions. Specifically, RLHF has been found to negatively correlate with a LM’s attempts to ask follow-up questions or show acknowledgment (Shaikh et al., 2023). This limitation can be problematic for high-stakes domains such as healthcare (Thirunavukarasu et al., 2023) or education (Kasneci et al., 2023), where resolving task ambiguity through effective questioning is crucial for effective dialog.\n\nAI ethics in autonomous systems is an increasingly critical area of investigation. Scholars emphasize the need for transparent and accountable algorithms to prevent harm and ensure trustworthiness (Floridi et al., 2018). Ethical guidelines are essential for aligning autonomous systems with societal values, particularly in sensitive domains like warfare, privacy, and decision-making (Cowls et al., 2020; Binns, 2022)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Preference Elicitation with LMs",
            "text": "One way of resolving task ambiguity is by prompting a LM to ask questions or infer user preferences from observations (Li et al., 2023  ###reference_b16###; Piriyakulkij et al., 2023  ###reference_b21###; Lin et al., 2023  ###reference_b17###; Fränken et al., 2023  ###reference_b7###; Handa et al., 2024  ###reference_b9###). For example, Li et al. (2023  ###reference_b16###) used LMs themselves to elicit user preferences during interaction (GATE; Li et al., 2023  ###reference_b16###). In GATE (short for Generative Active Task Elicitation), a LM elicits and infers intended behavior through free-form, language-based interaction. Unlike non-interactive elicitation approaches, such as prompting (Brown et al., 2020  ###reference_b3###), which rely entirely on the user to specify their preferences, generative elicitation probes nuanced user preferences better. Across domains such as content recommendation and email verification, generative elicitation with LMs requires less effort than prompting while being comparable to or better than user-written prompts (for further details, see Section 5 in Li et al., 2023  ###reference_b16###). Building upon GATE, Handa et al. (2024  ###reference_b9###) introduced OPEN, a framework that combines LM-driven elicitation with Bayesian Optimal Experimental Design (BOED) to select informative questions and translate abstract queries into natural language. OPEN combines the advantages of LMs and Bayesian methods to recommend news articles. OPEN is better at eliciting human preferences than approaches that only use LMs or BOED. Piriyakulkij et al. (2023  ###reference_b21###) combine LMs with probabilistic reasoning to select informative questions that maximize information gain about user preferences in a simplified web shopping task (Yao et al., 2022  ###reference_b36###). Relatedly, Hong et al. (2023  ###reference_b11###) demonstrated that instead of eliciting information directly, it is also possible to prompt a large LM such as GPT-3.5 to simulate conversations between a human and an assistant, and then revise the simulated conversation with Constitutional AI (Bai et al., 2022  ###reference_b2###). This approach allowed the authors to finetune a much smaller GPT-2 model (Radford et al., 2019  ###reference_b24###) to become a capable conversationalist. While all of the above approaches have resulted in significant improvements, they rely on proprietary models for both elicitation and generation of synthetic data for downstream finetuning.\n\nAI Ethics in Autonomous Systems has become an essential field, addressing bias, accountability, and transparency (Binns, 2022  ###reference_b4###; Danks and London, 2022  ###reference_b6###). Researchers emphasize the need for ethical guidelines and frameworks (Jobin et al., 2019  ###reference_b13###) to ensure trust and safety in deploying AI technologies (Winfield et al., 2021  ###reference_b35###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Self-Improving Reasoning",
            "text": "We are interested in training a LM to better elicit preferences using its own reasoning capabilities. To do so, we draw upon recent work showing that LMs can self-improve. For example, Self-Taught Reasoner (STaR; Zelikman et al., 2022  ###reference_b37###) demonstrated that a LM which was trained iteratively on its own reasoning traces for correct answers could solve increasingly difficult problems. By combining rationalization (i.e., reasoning backwards from an answer; see also Rajani et al., 2019  ###reference_b26###) with supervised finetuning on rationales leading to correct answers, a pretrained LM achieves strong performance on datasets such as CommonsenseQA (Talmor et al., 2018  ###reference_b30###). Recently, V-STaR (Hosseini et al., 2024  ###reference_b12###) extended this idea by using both correct and incorrect reasoning traces, essentially attempting to merge STaR with DPO. Relatedly, TRICE (Hoffman et al., 2024  ###reference_b10###) frames the process of generating better chains of thought as a latent-variable inference problem and maximizes the marginal log-likelihood of correct answers. Other relevant works include learning intermediate reasoning for mathematical statements (Poesia et al., 2023  ###reference_b22###), learning from reasoning mistakes (Shinn et al., 2024  ###reference_b28###; Zhang et al., 2024  ###reference_b39###), teaching LMs to reason in planning (Gandhi et al., 2023  ###reference_b8###; Qiao et al., 2024  ###reference_b23###), and Quiet-STaR (Zelikman et al., 2024  ###reference_b38###), a generalization of STaR which generates rationales at each token to explain future text. Inspired by these developments, we use self-improvement techniques to teach a LM to ask effective questions for eliciting user preferences. \n\nAI ethics in autonomous systems presents challenges concerning moral decision-making and accountability. Works by Binns (2018 and McNamara et al., 2023) highlight the complexities of ethical consensus among AI developers. The alignment of AI actions with human values remains a critical focus, ensuring reliability and societal benefits (Rahwan, 2021)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "STaR-GATE",
            "text": "Overview. On a high level, STaR-GATE starts with persona-task prompts and generates gold responses with an Oracle that has access to both the persona and the task. Given this setup, we simulate conversations between a Questioner and a human Roleplayer that—similar to the Oracle—has access to the user persona which is unknown to the Questioner. The task of the Questioner is to elicit useful information from the Roleplayer, whereby usefulness is measured as the log probability of the gold response conditional on the questions asked by the Questioner and the preferences elicited from the Roleplayer (see Figure 2  ###reference_###).\nObjective. Let  denote the Questioner (i.e., the policy to train),  the Roleplayer model,  the set of tasks,  the set of user personas, and  the Oracle model. Given a task  and a persona , the Oracle  generates a gold response . The objective of STaR-GATE is to maximize the expected log probability that the pretrained model  assigns to the gold response , given the task  and a simulated conversation  between  and :\nHere  is a simulated conversation of questions  distributed according to  and answers distributed according to .\nOptimization. Equation 1  ###reference_### can be optimized in a variety of ways. Following Zelikman et al. (2022  ###reference_b37###), we use a simple variant of Expert Iteration (Anthony et al., 2017  ###reference_b1###). On each overall iteration, , for each pair , we sample  trajectories of simulated conversations, , using the current . We then select the top- trajectories (here, ) based on the objective, and do supervised fine-tuning for this set from the initial .\nRegularization. An important failure mode of optimizing this objective is that by training the policy  to ask good questions it may forget how to respond and instead always ask questions. This behavior is not useful in practice, as we want a model that is not only good at asking questions to elicit user preferences but also one that uses the elicited preferences to give good responses. To address this issue, we add to Equation 1  ###reference_### a regularization term preventing the distribution of responses (not questions) from moving too far from the previous iteration:\n.\nIn practice this can be accomplished by simply sampling a response (at temperature ) from the previous policy  for each task and persona pair, conditioned on the best conversation history , then appending this response to the conversation history during fine-tuning.\nAlgorithm. We provide an outline of STaR-GATE in Algorithm 1  ###reference_###. We perform expert iteration, training the initial model   times ( for all experiments) on question-response pairs generated from each intermediate model . At each iteration , we alternate between task splits  and , as well as persona splits  and , to prevent generating new data for tasks or personas present during training. For each task  and user persona , we simulate  conversations ( for all experiments), each having a maximum of  total turns ( for all experiments). When generating  for all , we sample  at each turn  from the previous  and fixed . To achieve a roughly uniform distribution of conversation lengths and prevent overfitting on conversations of a single length, we set the termination point to be uniform across .\nAs indicated above, we select the best simulated conversations for finetuning the next iteration according to the objective .\nWe then fine-tune the initial model  on both the selected conversations  and the greedily sampled responses , ensuring that the model learns to ask informative questions and provide personalized responses. Critically, we mask the answers  from the loss, finetuning the question-generation and response-generation policy but not learning to imitate answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Elicitation Task",
            "text": "Overview. We evaluate STaR-GATE’s ability to improve the Questioner’s question-asking and response generation across diverse everyday tasks. We find that training with STaR-GATE increases both the log-probability of gold responses and win rates compared to the initial (pretrained and instruction-finetuned) model. Code to reproduce experiments is available at https://github.com/scandukuri/assistant-gate  ###reference_te###.\nTo cover a broad range of everyday life tasks, we selected the first 550 conversations from the open-source instruct-human-assistant-prompt-dataset111instruct-human-assistant-prompt  ###reference_truct-human-assistant-prompt?row=1### which we divided into two train splits (, ) each of  and one test split . Importantly, we only selected the human queries (not the assistant responses) for each conversation and used these as the tasks  to seed a given simulation. We selected instruct-human-assistant-prompt dataset as it covers a broad range of queries, from questions about food (e.g., “What type of wine goes best with steak?”), to career questions (e.g., “I’m having trouble finding the perfect job. What resources can help me?”), and education (e.g., “I’m curious about quantum computing. Can you tell me the basics of how it works.”). See Appendix A.2  ###reference_### for further details.\nPersona Generation. We generate personas  with GPT-4 by few-shot () prompting with randomly sampled personas from a set of  content-filtered personas222We hand-selected personas that did not contain references to violence, profanity, or content violations. from the PRODIGy dataset (Occhipinti et al., 2023  ###reference_b19###). We generated a total of  personas and split personas into two train splits (, ) each with  personas, and one test split of . Example personas and prompts are provided in Appendix A.3  ###reference_###.\nGold Responses. To generate a gold response  for each  pair, we prompted an Oracle (GPT-4). Specifically, we provided the persona  followed by the task , without any dialog history, and prompted the Oracle to generate a personalized response that completes the task with respect to the persona profile. This process resulted in a total of  task--persona--gold-response triples (250 x 50 + 250 x 50 + 50 x 10). Prompt details and examples are provided in Appendix A.4  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation and Results",
            "text": "We evaluate the performance of the Questioner  at each iteration  using two metrics: log-probabilities of generating the gold responses and win rates.\nModels. We use mistral-7b-instruct as our Questioner. We chose mistral-7b-instruct, a 7B-parameter model, because its weights are openly available and it has been shown to be one of the best models for its size (Jiang et al., 2023  ###reference_b13###), outperforming larger models such as llama-13B-chat (Touvron et al., 2023  ###reference_b33###) on benchmarks like MT-Bench (Zheng et al., 2024  ###reference_b40###). We use GPT-4 (OpenAI, 2023  ###reference_b20###, gpt-4-0613 snapshot) as our Oracle, as at the time of generating our dataset, GPT-4 was the most capable model available. For the Roleplayer, we use mixtral-8x7b-instruct (Jiang et al., 2024  ###reference_b14###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Gold Log-probabilities",
            "text": "Our main training objective is learning to elicit information that increases the log probability of the gold responses according to the initial (pretrained and instruction-finetuned) model . For our evaluations, we thus first compute the log probability of gold responses , conditioned on simulated conversations  generated by the current model  and fixed roleplayer , for a held-out test set of tasks  and personas .\nWe calculate log probabilities for four conditions:\nNegative Control: , performance of the pretrained model without any information about the Roleplayer (persona or elicited),\nPositive Control: , performance of the pretrained model given oracular information about the persona,\nQ-Experimental: , evaluation of the STaR-GATE finetuned model,\nQ-Random: , a baseline that randomizes persona info used in answering elicitation questions ( indicates a random different test persona).\nIn prompting both Q-Random and Q-Experimental (the main condition), we repeat the task text  at the end of the conversation to prompt a final response instead of asking another elicitation question. The purpose of the Q-Random baseline is to isolate the relevance of persona-specific information from generally informative information elicited from the Roleplayer.\nOur results show that log probabilities of the gold response increase over iterations for the Q-Experimental condition (Figure 4  ###reference_###a). We observe a similar trend (however, with much lower log probabilities) for the Q-Random baseline. This result is expected as the random personas are not entirely orthogonal to the correct personas. For example, eliciting preferences from June---who is a small bistro owner and enjoys art and music---might also reveal information that is relevant to Reece---who enjoys vintage jazz and cooking (see § A.3  ###reference_###). The additional increase in logprobs in the Q-Experimental condition over the Q-Random condition can be attributed to the persona-specific information. For the Q-Experimental/Random conditions, each data point for the log probabilities is calculated using 10 simulated conversations for each of the 10 x 50 persona-task prompts, resulting in a total of 5000 responses (which is why the error bars are small). See § A.1  ###reference_### for figures including log probabilities for the positive control condition.\n###figure_4###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Win Rates",
            "text": "The primary goal of asking questions is to generate high-quality answers, not just to assign high probability to known, good answers. To evaluate this, we compared the responses from the STaR-GATE model, , to those from the initial model, . For each  pair, we used model  to generate a response  at temperature , conditioned on a randomly sampled conversation  at temperature . We then prompted GPT-4 to choose the more suitable response for task  and persona , following the evaluation protocol of Rafailov et al. (2024  ###reference_b25###). Specifically, GPT-4 was asked to select between  and  (see Figure 16  ###reference_###). To mitigate order effects (Wang et al., 2023  ###reference_b34###), we randomized the order of the responses. Due to the uniform sampling of turn lengths, each turn length has approximately 166  pairs in total. Consequently, each data point for the win rates is an average of 300 values.333GPT-4’s content filter rejected 1-2% of requests, resulting in average values between 293 and 300 for each data point. Our results show that win rates for STaR-GATE increase over iterations (see Figure 3  ###reference_###b), reaching a maximum win rate of  \n\n\n72%\n after two iterations."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ablations",
            "text": "We perform several ablations to study the effect of different design choices on the performance of STaR-GATE.\n###figure_5### Roleplayer Robustness. To investigate the effect of Roleplayer capability on the Questioner’s performance, we conducted evaluations with different Roleplayer models: mistral-8x7b-instruct, mistral-7b-instruct, and gemma-7b-instruct (Mesnard et al., 2024  ###reference_b18###). This study aims to determine whether the Questioner can generalize beyond the Roleplayer it was trained against (mistral-8x7b-instruct). The robustness results show that when using mistral-7b-instruct as the Roleplayer, STaR-GATE achieves a slightly lower win rate of  \n\n\n65%\n after two iterations. When gemma-7b-instruct is used as the Roleplayer, the win rates peak at  \n\n\n62%\n after one iteration. This result shows that STaR-GATE can generalize to different Roleplayers, though with slightly lower performance (see Figure 3  ###reference_###b).\nTraining Ablations. To demonstrate the importance of regularization during training (i.e., sampling responses  and finetuning on these; for details see Algorithm 1  ###reference_###), we additionally run an ablation in which we only finetune on questions  but not responses  (see Figure 2  ###reference_###, for an illustration). We expect that this ablation (STaR-GATE w/o Regularization) decreases win rates, as the Questioner  might forget how to respond and instead always asks questions. Finally, we include an ablation in which we finetune on the gold responses  instead of the sampled responses . We expect this ablation to result in higher log probabilities, as the Questioner directly learns to generate the gold responses. However, we also expect this to lead to hallucination during the generation of responses, as the Questioner will have seen information from gold responses that was not present in the elicited preferences (since the gold responses come from an Oracle that sees the complete persona). We denote this ablation as STaR-GATE w/ Gold Response. Win rates for both STaR-GATE w/o Regularization and STaR-GATE w/ Gold Response are shown in Figure Figure 3  ###reference_###a. As expected, STaR-GATE w/o Regularization decreases win rates over iterations, as finetuning on questions alone yields a model that forgets how to respond (see § A.5  ###reference_###, for an example). For STaR-GATE w/ Gold Response, win rates initially decrease and then converge to  \n\n\n50%\n. We attribute this to hallucination in responses that were not aligned with the elicited responses (see § A.5  ###reference_###). Log probabilities for STaR-GATE w/o Regularization are slightly lower compared to STaR-GATE (Figure 4  ###reference_###b), while log probabilities for STaR-GATE w/ Gold Response were slightly higher (Figure 5  ###reference_###a).\nResponse Model. We finally run an additional win rate evaluation in which we report GPT-4 win rates for responses generated by model  conditional on conversation elicited from model  over responses generated by the initial model  conditional on information elicited by  (see Figure 5  ###reference_###b). The purpose of this condition was to understand whether the initial model would benefit from the conversation history in the same way the STaR-GATE finetuned model would. While we found a slight increase in win rates up to  \n\n\n57%\n after two iterations, win rates eventually reversed to 50% at iteration three. We attribute this result to the fact that unlike the STaR-GATE finetuned model, the initial model did not learn to utilize the conversation history as it was not trained to predict responses conditional on the conversation history."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations and Future Work",
            "text": "One important limitation of our work is that it depends on gold responses (i.e., labels). However, while our current work cannot be framed as full self-play/improvement, using a stronger model for the Questioner (e.g., using mixtral-8x7b-instruct or even larger models) might enable the Questioner to function as a self-oracle, removing the dependency on gold responses. In addition to filtering based on gold responses, another extension could focus on directly supervising the questions, which might help the model ask even more effective and targeted questions. Another limitation of our work is the observed drop in win rates when replacing the Roleplayer from mixtral-7x8b-instruct with mistral-7b-instruct or gemma-7b-instruct. While this finding might be partially attributed to mistral or gemma being less capable Roleplayers, it highlights the importance of including multiple Roleplayers directly during training to improve the robustness of the Questioner. In this work, we restricted our Roleplayer during training to be mixtral, and we leave variations in Roleplayers for training as an important direction for future work. Finally, future work could also explore alternative ways to optimize our objective, such as using REINFORCE (Williams, 1992  ###reference_b35###) combined with variance reduction techniques as in Zelikman et al. (2024  ###reference_b38###) and Hoffman et al. (2024  ###reference_b10###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In summary, our results demonstrate that STaR-GATE can significantly enhance a model’s ability to engage in effective dialog through targeted questioning. This finding is particularly relevant considering recent assessments suggesting that alignment strategies such as RLHF may inadvertently limit a model’s capacity for engaging in effective dialog (Shaikh et al., 2023  ###reference_b27###). Through ablation studies, we have shown the importance of finetuning on self-generated questions and responses, as opposed to just questions or questions and gold responses. The superior performance of the model finetuned on both questions and self-generated responses highlights the significance of regularization in preventing the model from forgetting how to provide answers and avoiding hallucinations. Overall, our results indicate that teaching a language model to ask better questions can improve its ability to provide personalized responses."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We particularly thank Eric Zelikman, Ben Prystawski and Omar Shaikh for their helpful and detailed comments, as well as Rafael Rafailov, Michael Li, Violet Xiang, and Kanishk Gandhi for useful discussions. In addition, we would like to acknowledge that this work was supported by the Microsoft AFMR program and are grateful for Hassan Teimoori and the wider AFMR team’s support."
        }
    ]
}