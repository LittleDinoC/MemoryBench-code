{
    "title": "Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information",
    "abstract": "A primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text. We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text. To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information. This strategy adjusts the generation probability of each token by comparing it with the token’s marginal probability within the domain of the source text. According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance. The code is publicly available at https://github.com/qqplot/dcpmi.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Abstractive summarization is the task of generating a summary by interpreting and rewriting a source text. State-of-the-art pre-trained language models have achieved remarkable performance in this task Lewis et al. (2019  ###reference_b7###); Zhang et al. (2020  ###reference_b20###). However, upon closer examination, a common issue emerges: hallucination between the source document and the generated text. Prior studies have made efforts to enhance the faithfulness of the summary to the source text, yet hallucination remains a persistent challenge Maynez et al. (2020  ###reference_b11###); Mao et al. (2021  ###reference_b10###); Zhu et al. (2021  ###reference_b25###); Zhang et al. (2023  ###reference_b21###).\nTo solve this issue, we introduce a decoding strategy based on domain-conditional pointwise mutual information (). The motivation for  is that the domain of the source text provokes the model to generate text that is highly probable in the source domain, leading to plausible but factually inconsistent text.\nBuilding on this motivation,  computes how much more likely a token becomes in the summary when conditioned on the input source text, compared to when the token is conditioned only on the domain of the source text. This effectively penalizes the model’s tendency to fall back to domain-associated words when the model has high uncertainty about the generated token.\nThis idea was inspired by conditional pointwise mutual information (CPMI) van der Poel et al. (2022  ###reference_b15###), which similarly penalizes a token’s marginal probability. But CPMI does not capture the important fact that a token’s probability depends highly on the source domain in summarization.\nFor example, consider the example presented in Table 1  ###reference_###. The source text states, “Our latest economic data shows that many Scottish businesses will have a successful 2017”.\nCPMI undesirably introduces the term “warning”, which frequently appears in the domain of economy in the training data, generating information that contradicts the source text. By contrast,  lowers the probability of the term “warning” by capturing the high conditional likelihood of this term given the domain and avoids the hallucination.\nWe use automated metrics for evaluation on the challenging XSUM dataset Narayan et al. (2018  ###reference_b12###) achieving significant improvements in faithfulness and relevance to source texts according to metrics like AlignScore, FactCC, BARTScore, and BS-Fact, with only a marginal decrease in ROUGE and BERTScore. This highlights the effectiveness and robustness of  in abstractive summarization.\n\nIn healthcare, AI has shown great potential in disease prediction, offering improved accuracy over traditional methods. Advanced algorithms analyze vast datasets, identifying patterns to predict diseases like diabetes, cancer, and cardiovascular conditions Mahajan et al. (2019  ###reference_b30###); Esteva et al. (2021  ###reference_b32###). However, issues of data bias and interpretability remain significant challenges Bohr et al. (2020  ###reference_b34###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "We adopt the problem definition in van der Poel et al. (2022  ###reference_b15###). In abstractive summarization, an input source text, denoted as , is condensed into an output string represented by . This output string is a sequence of tokens from the vocabulary . Each sequence begins with token  and ends with , and the length of the output is .\nThe optimal  that belongs to a valid string set  is obtained via a scoring function as follows:\nUtilizing beam search is a practical solution for searching possible strings.\nThe typical beam search with an autoregressive generation model uses the following scoring function:\nwhere  is a token-level log probability computed by the model.\nPMI scoring utilizes mutual information between the input and output. This penalizes the generation of tokens that are marginally likely but not related to the input. The formula for PMI scoring can be expressed as follows:\nvan der Poel et al. (2022  ###reference_b15###) have demonstrated a connection between hallucinations and token-wise predictive entropy, denoted as . A model tends to hallucinate a token if the entropy is high. Hence, instead of penalizing the marginal probability of  in Equation 2  ###reference_### all the time, CPMI does this only when the entropy at the -th decoding step is higher than a threshold.\nwhere ."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Domain-conditional Scoring Strategy",
            "text": "Our approach improves upon CPMI by conditioning the probability of a generated token on the source domain.\nIn our domain-conditional strategy (), we employ the following scoring function:\nis a domain prompt Holtzman et al. (2021  ###reference_b3###), a subset of tokens in  that contains information about the source domain. This seemingly simple extension is well grounded in the previous observation that summarization models are likely to templatize the summaries of source texts that share the same domain or topic and hallucinate tokens that are frequent in the “template” of the source domain King et al. (2022  ###reference_b5###). Accordingly, our method can effectively account for different marginal probabilities of the token depending on the source domain and outperforms CPMI, as will be demonstrated later.\nTo compute the marginal probabilities , we employ a smaller language model, denoted as , while  represents a larger summarization model. The hyperparameters  and  are optimized through random grid-search.\nTo condition the generation probability of a token on the source domain, we incorporate domain information into the prompts of both the summarization and language models (i.e., ). We explored three types of domain information: (1) domain-specific keywords, (2) the first sentence of the source text, and (3) a randomly chosen sentence from the source text.\nWe assumed that domain-specific keywords enable the model to calculate the conditional probability of a token within the specified domain. The open-source module KeyBERT Grootendorst (2020  ###reference_b2###) was utilized to extract three keywords from each source text (Appendix A.4  ###reference_###). The expectation was that these selected keywords would effectively represent the source document with high similarity. Additionally, we also considered that sentences extracted from the source text could represent the domain of the entire text. Therefore, sentences from the source text, including the first sentence, and a randomly selected sentence were examined as the source domain.\nIn conjunction with the aforementioned domain information, we incorporated a simple priming phrase into the domain prompt. We have discovered that using an appropriate lexical form yields better results compared to inputting the domain alone. We referred to the prompt design outlined by Yuan et al. (2021  ###reference_b18###). The 18 phrases we examined include expressions such as “keyword,” “in summary,” and “in other words.” Table 2  ###reference_### displays the seed prompts along with examples of paraphrased prompts (see more details in Appendix D  ###reference_###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We used the eXtreme Summarization Dataset, XSUM Narayan et al. (2018  ###reference_b12###), which consists of BBC articles as source documents and single-sentence summaries as gold summaries.\nWe examined three baseline decoding methods: standard beam search, PINOCCHIO King et al. (2022  ###reference_b5###), and CPMI van der Poel et al. (2022  ###reference_b15###). Additionally, we analyzed FactPEG Wan and Bansal (2022  ###reference_b16###), which underwent separate fine-tuning using FactCC and ROUGE with the source document.\nFor the summarization model, we utilized encoder-decoder structures of BART Lewis et al. (2019  ###reference_b7###) and PEGASUS Zhang et al. (2020  ###reference_b20###). As for the language model, a GPT2-based model Radford et al. (2019  ###reference_b13###) was employed. Each of these models was pre-trained on the XSUM dataset. More details can be found in Appendix B  ###reference_###.\nWe have categorized the evaluation into three key divisions: Faithfulness, Relevance (with the source), and Similarity (with the target). For faithfulness, we used AlignScore Zha et al. (2023  ###reference_b19###) and FactCC Kryscinski et al. (2020  ###reference_b6###). To measure relevance to the source and informativeness, we employed BARTScore Yuan et al. (2021  ###reference_b18###) and BS-FACT. Lastly, to assess similarity to the target, we utilized ROUGE-L and BERTScore Zhang* et al. (2020  ###reference_b22###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We presented the results from BART in Table 3  ###reference_###. The complete result, including those from PEGASUS, are provided in Table 9  ###reference_###. For all cases, the prompt used was “That is to say”, and the domain consisted of three keywords extracted from the source document. In Table 3  ###reference_###, we compared the summarization performance of different decoding strategies with BART. Our results revealed that PINOCCHIO exhibited suboptimal performance overall, while CPMI showed performance that was nearly on par with standard beam search. However,  showed significant improvement in terms of faithfulness and relevance.\nIn Table 4  ###reference_###, the term Type indicates whether the subset is at the word or sentence level, while Domain refers to a subset of tokens within the source. Notably, the Keyword approach within the word-level domain demonstrated robust performance. Therefore, we selected the Keyword approach for our domain prompt."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Comparison with Fine-tuned Model",
            "text": "FactPEG Wan and Bansal (2022  ###reference_b16###) reduces hallucinations by incorporating factual metrics during training, leveraging ROUGE and FactCC with the source document to produce faithful summaries. In Table 5  ###reference_###, FactPEG outperforms  in terms of faithfulness. On the other hand,  achieves a more balanced performance across different metrics.\nFactPEG is trained with a focus on faithfulness, which has led to the loss of other summarization abilities. For instance, using a random sentence as a summary (as shown in the top row in Table 5  ###reference_###) demonstrates high faithfulness but a notable drop in the other two categories. Therefore, solely targeting faithfulness may risk the summarization capabilities of pre-trained models (refer to Table 6  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Effectiveness of Uncertainty-Aware Scoring",
            "text": "Recall that in , the marginal probability of a token conditional to the domain  is utilized only when the model’s uncertainty of a token exceeds a threshold (i.e., ). Here, we examined whether this uncertainty-aware scoring is more effective than without .\nIn Table 7  ###reference_###, the first and second rows demonstrate the PMI scores regardless of uncertainty, while the third row shows uncertainty-aware PMI score. To ensure faithful token generation without degrading the performance of original summarization models, it is more effective to replace only specific uncertain tokens suspected of hallucination through uncertainty-aware scoring, rather than adjusting all tokens."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Error Analysis",
            "text": "While PMI effectively controlled hallucinated terms, there were instances of failure. We conducted a manual evaluation on 500 XSUM samples selected from Maynez et al. (2020  ###reference_b11###), categorizing the error cases into three types (Table 8  ###reference_###):\nCase 1: Extracted keywords may not fully reflect the domains of the source text.\nCase 2: Appropriate domains, but errors in representing numbers, proper nouns, or statistics.\nCase 3: Appropriate domains, yet still hallucinated cases.\nCase 1 occurs when the extracted keywords may not fully reflect the domains of the source text. We used keywords to represent the domain. However, in some cases, the extracted keywords may not adequately capture the “topic” or “category” of the source text and did not guide the model as we expected (Table 11  ###reference_###).\nCase 2 occurs when handling numbers, proper nouns, or statistics. Numbers, proper nouns, or statistics are among the primary causes of hallucination in the model. Despite extracting the appropriate domain, there are instances where incorrect numerical information is presented in the generated text (Table 12  ###reference_###).\nCase 3 refers to situations where summarization fails even though they do not fall into Case 1 or Case 2. One such scenario happens when imposing significant penalties on domain-specific keywords. This can result in avoiding direct expressions, leading to ambiguity (Table 13  ###reference_###). Additionally, there are occurrences of hallucination due to the inherent difficulty of the task. For instance, when the source text contains multiple pieces of information, summarizing them into a single sentence becomes a challenging task."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed a decoding strategy based on domain-conditional pointwise mutual information (PMI) to reduce hallucination in abstractive summarization. PMI penalizes the model’s tendency to generate text inconsistent with the source document by considering the source text’s domain. This simple but innovative approach significantly improves faithfulness and relevance to the source text, as demonstrated through evaluation on the XSUM dataset."
        }
    ]
}