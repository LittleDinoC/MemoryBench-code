{
    "title": "MileBench: Benchmarking MLLMs in Long Context",
    "abstract": "Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks’ limited scope.\nExisting benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs.\nTo address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.\nThis benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation.\nWe establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs’ long-context adaptation capacity and their ability to complete tasks in long-context scenarios.\nOur experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations.\nInterestingly, the performance gap tends to widen with an increase in the number of images.\nWe strongly encourage an intensification of research efforts towards enhancing MLLMs’ long-context capabilities, especially in scenarios involving multiple images.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The recent swift development of Multimodal Large Language Models (MLLMs) (OpenAI, 2023  ###reference_b39###; Anil et al., 2023  ###reference_b2###; Liu et al., 2023a  ###reference_b31###; Awadalla et al., 2023  ###reference_b3###) has displayed outstanding performance across a diverse range of multimodal tasks (Yang et al., 2023  ###reference_b51###; Wu et al., 2023  ###reference_b50###).\nMeanwhile, a surge of benchmarks for evaluating MLLM performance has emerged (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Ge et al., 2023  ###reference_b11###; Li et al., 2023a  ###reference_b22###), offering insights into their general capabilities (Li et al., 2023b  ###reference_b23###; Liu et al., 2023c  ###reference_b34###; Yu et al., 2023  ###reference_b54###) and task-specific capabilities (Liu et al., 2023d  ###reference_b35###; Yue et al., 2023  ###reference_b55###; Wang et al., 2024  ###reference_b47###).\nHowever, a critical aspect is often overlooked.\nReal-world applications frequently demand the processing of long contexts and multi-image tasks that include multi-round dialogues based on multiple images (Li et al., 2022  ###reference_b28###), action prediction tasks (Wu et al., 2021  ###reference_b49###), navigation tasks in 3D space (Krantz et al., 2020  ###reference_b20###), and understanding tasks with lengthy documents interspersed with images on Wiki pages (Hannan et al., 2020  ###reference_b12###).\nDespite this, existing benchmarks primarily focus on single-image and short-text samples (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Liu et al., 2023c  ###reference_b34###; Li et al., 2023b  ###reference_b23###), thereby failing to fully capture the complexity and diversity of real-world scenarios.\nWhile some benchmarks evaluate multi-image tasks, they either have limited number of images provided per sample (e.g., SEED-Bench-2 (Li et al., 2023a  ###reference_b22###), DEMON (Li et al., 2023c  ###reference_b24###)) or only include time-series captioning tasks (e.g., Mementos (Wang et al., 2024  ###reference_b47###)), as shown in Figure 2  ###reference_###.\nIn addition, this omission could potentially neglect the hallucination issue that MLLMs might exhibit in long-context situations (Huang et al., 2023  ###reference_b14###).\nGiven the aforementioned shortcomings with existing benchmarks, we identify a pressing need for a more holistic evaluation that fully encapsulates the long-context and multi-image task demands prevalent in real-world applications.\nAddressing this need, we introduce MileBench, the first benchmark specifically designed to test the MultImodal Long-contExt capabilities of MLLMs111We define “multimodal long contexts” as long text content integrated with two or more images, or content composed of multiple images..\nTo systematically assess the capabilities of MLLM in multimodal long contexts, our benchmark consists of two distinct evaluation sets, diagnostic evaluation and realistic evaluation.\nThe former explores the long-context recall abilities of MLLMs, using needle-in-a-haystack and image retrieval tasks, while the latter stress-tests the model in a manner akin to real-world conditions using both temporal multi-image tasks and semantic multi-image tasks.\nTo construct our evaluation sets, we gather 6,440 multimodal long-context samples from 21 pre-existing or self-constructed datasets, with an average of 15.2 images and 422.3 words each, as depicted in Figure 2  ###reference_###, and we categorize them into their respective subsets.\nAfter evaluating 22 models, the closed-source GPT-4o222https://openai.com/index/hello-gpt-4o  ###reference_### excelled in both diagnostic and realistic evaluations, achieving impressive scores of 99.4% and 60.3%, although it still falls short of a perfect 100% score.\nOn the contrary, most open-source MLLMs struggled with long-context tasks as depicted in Figure 1  ###reference_###.\nOnly Mantis and Qwen-VL-7B managed average scores of 47.5% and 37.2% in realistic and diagnostic evaluations respectively. These results underscore that there are “miles to go” towards fully-realized long-context MLLMs, prompting a call for increased research focus on such tasks, especially those involving numerous images."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Multi-image and Long-Context MLLMs",
            "text": "Beyond training on single-image-text pairs, recent developments in MLLMs are also oriented towards handling multiple and interleaved image-text sequences (Awadalla et al., 2023  ###reference_b3###; Li et al., 2023c  ###reference_b24###).\nHowever, these models have relatively limited contexts (i.e., up to 4K) compared to leading proprietary MLLMs such as GPT-4V (OpenAI, 2023  ###reference_b39###) and Gemini (Anil et al., 2023  ###reference_b2###; Reid et al., 2024  ###reference_b41###), which exhibit capabilities for long-context processing, supporting up to 128K and 10M tokens, respectively.\nHowever, there remains a notable gap in open-source MLLMs capable of long-context comprehension.\nCurrently, the only open-source models equipped for long contexts are those designed for video, which are trained to process multiple frames, inherently managing multiple images and long contexts (Liu et al., 2024a  ###reference_b30###; Zhang et al., 2023  ###reference_b56###; Luo et al., 2023  ###reference_b36###; Li et al., 2023d  ###reference_b25###; e  ###reference_b27###). In this paper, we release an evaluation set specifically designed for multi-image and long-context MLLMs."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Evaluation of MLLMs",
            "text": "Most of the MLLM benchmarks only evaluate multimodal tasks with a single image (Liu et al., 2023a  ###reference_b31###; Fu et al., 2023  ###reference_b9###; Liu et al., 2023c  ###reference_b34###; Li et al., 2023b  ###reference_b23###; Yu et al., 2023  ###reference_b54###; Ge et al., 2023  ###reference_b11###; Yue et al., 2023  ###reference_b55###).\nAs a complementarity, SEED-Bench2 (Li et al., 2023a  ###reference_b22###) and DEMON (Li et al., 2023c  ###reference_b24###) test multimodal capabilities with multiple images but limit the evaluation to around three images, which is inadequate for a thorough multi-image comprehension assessment. Mementos (Wang et al., 2024  ###reference_b47###) involves data samples with up to approximately 11 images, mainly focusing on temporal understanding and limited context scenarios. This focus overlooks the wide range of scenarios involving multiple images and long context.\nWe provide an overview of existing MLLM benchmarks in Appendix A  ###reference_###.\nTo the best of our knowledge, MileBench is the first comprehensive benchmark that evaluates MLLMs across both multi-image and long-context dimensions, catering to a broader spectrum of general scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MileBench",
            "text": "Temporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###) assesses the model’s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###) tests the model’s capacity to predict a character’s actions. Action Sequence (Wu et al., 2021  ###reference_b49###) evaluates the model’s understanding of the chronological order of a character’s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###) tests the model’s ability to detect and track an object’s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###) assesses the model’s understanding of moving object attributes. Object Interaction (STAR) evaluates the model’s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###) gauges the model’s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model’s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot’s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###), assesses the model’s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model’s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###) tests the model’s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###) assesses the model’s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###) examines the model’s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###) evaluates the model’s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects.\nThe “Needle in a Haystack” task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_### and Figure 11  ###reference_### in Appendix B.2  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack’s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the “needle”, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a “Needle in a Haystack” task with image modality queries.\nWe collected the samples from two sources:\n(1) For most of the tasks, we selected and sampled 200 instances from the pre-existing datasets for each task, giving priority to multi-image samples. For video data, we used the Katna (Keplerlab, 2021  ###reference_b19###) to convert the video into a multi-image format by extracting one frame per second.\nThe choice of these pre-existing open-source datasets was driven by their well-established reputation and the fact that they have been published in top-tier conferences and journals, ensuring their credibility and reliability.\nPlease refer to Section Ethics Statement  ###reference_### and Section 4.3.3  ###reference_.SSS3### for information on the data’s licensing and data contamination issue.\n(2) For the new tasks Long Text with Images QA, Image Retrieval, Text Needle In A Haystack, and Image Needle In A Haystack, we created synthetic data (more details in Appendix B.3  ###reference_###).\nUltimately, we collected 6,440 samples with varying context lengths.\nDetailed statistics of these samples are presented in Table 4  ###reference_3###.\nThe task distribution is demonstrated in Figure 4  ###reference_3###.\nA comprehensive breakdown of the datasets, tasks, and taxonomy can be found in Appendix B.1  ###reference_###.\nFor the open-source dataset comprised of the benchmark, we sample 10% of the data for manual verification.\nOur review team, composed entirely of authors, was assigned to scrutinize the precision of the sampled data, resulting in an Inter-annotator Agreement (IAA)555A degree of consensus or similarity among the annotations made by different annotators on the same data. of 95%, indicating a high level of consistency among reviewers.\nFor the datasets we formulated independently, equivalent manual verification was carried out on the entirety of the dataset, yielding a similar IAA of 98%, thus ascertaining the data quality.\nAdditionally, the error rate was found to be less than 1% for both datasets, affirming that these datasets maintain an exceptionally high quality and are virtually devoid of errors."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation Taxonomy",
            "text": "MileBench consists of two major components: Realistic Evaluation and Diagnostic Evaluation, as depicted in Figure 3  ###reference_###.\nRealistic Evaluation requires MLLMs to address tasks within multimodal long-context scenarios, emphasizing the models’ proficiency in comprehending and reasoning across extended multimodal contexts.\nConversely, Diagnostic Evaluation demands MLLMs to retrieve information from the provided context, highlighting the model’s capability of long-range information retrieval and the elimination of distractors.\nThe detailed taxonomy of MileBench is illustrated in Table 6  ###reference_###. 333We adapted the taxonomy from MVBench (Li et al., 2024  ###reference_b26###) and DEMON (Li et al., 2023c  ###reference_b24###) to suit our multimodal long-context setting.\n###figure_1### Temporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###  ###reference_b10###) assesses the model’s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###  ###reference_b49###) tests the model’s capacity to predict a character’s actions. Action Sequence (Wu et al., 2021  ###reference_b49###  ###reference_b49###) evaluates the model’s understanding of the chronological order of a character’s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###  ###reference_b52###) tests the model’s ability to detect and track an object’s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###  ###reference_b52###) assesses the model’s understanding of moving object attributes. Object Interaction (STAR) evaluates the model’s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) gauges the model’s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model’s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot’s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###  ###reference_b52###), assesses the model’s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model’s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###  ###reference_b52###) tests the model’s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) assesses the model’s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###) examines the model’s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###  ###reference_b15###) evaluates the model’s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects.\nThe “Needle in a Haystack” task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_###  ###reference_### and Figure 11  ###reference_###  ###reference_### in Appendix B.2  ###reference_###  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack’s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the “needle”, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a “Needle in a Haystack” task with image modality queries."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Realistic Evaluation",
            "text": "The realistic evaluation is designed to assess an MLLM’s ability to comprehend, integrate, and infer information in a multimodal long context.\nWe categorize the tasks into two main groups: Temporal Multi-Image tasks and Semantic Multi-Image tasks.\nTemporal Multi-Image tasks test the MLLM’s ability to discern temporal relationships among several time-related images, emphasizing the model’s predictive capabilities in real-world scenarios.\nOn the other hand, Semantic Multi-Image tasks challenge MLLMs to process multiple images that are possibly temporal-irrelevant but are semantically interconnected.\nTemporal Multi-Image tasks include four temporal-related multi-image tasks.\nEach task contains multiple subtasks, each with 200 samples.\nAction Understanding and Prediction task involves interpreting and forecasting actions of objects or characters in sequential scenarios based on a series of images.\nThis task is divided into three subtasks. Action Localization (Gao et al., 2017  ###reference_b10###  ###reference_b10###  ###reference_b10###) assesses the model’s ability to identify time actions within a sequence. Action Prediction (Wu et al., 2021  ###reference_b49###  ###reference_b49###  ###reference_b49###) tests the model’s capacity to predict a character’s actions. Action Sequence (Wu et al., 2021  ###reference_b49###  ###reference_b49###  ###reference_b49###) evaluates the model’s understanding of the chronological order of a character’s actions.\nObject and Scene Understanding task involves identifying and understanding objects within a sequence.\nIt comprises four subtasks: Object Existence (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) tests the model’s ability to detect and track an object’s movement. Moving Attribute (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) assesses the model’s understanding of moving object attributes. Object Interaction (STAR) evaluates the model’s comprehension of interactions between people and objects in complex scenarios. Object Shuffle (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) gauges the model’s ability to locate hidden objects amidst disturbances.\nVisual Navigation and Spatial Localization task tests the model’s understanding of spatial and directional concepts through two subtasks.\nOne is Egocentric Navigation (Krantz et al., 2020  ###reference_b20###  ###reference_b20###  ###reference_b20###), involves the model interpreting motion-related instructions and image sequences from a robot’s perspective to predict the next action. Moving Direction (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###), assesses the model’s ability to determine object movement direction, evaluating its understanding of spatial orientation.\nCounterfactual Reasoning and State Change task evaluates the model’s logical reasoning within image sequences, focusing on causality and state changes.\nIt comprises four subtasks. Counterfactual Inference (Yi et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) tests the model’s ability to predict outcomes under hypothetical changes. State Change (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) assesses the model’s understanding of object state changes. Character Order (Patraucean et al., 2023  ###reference_b40###  ###reference_b40###  ###reference_b40###) examines the model’s reasoning of the order of letter appearances over time. Scene Transition (Huang et al., 2020  ###reference_b15###  ###reference_b15###  ###reference_b15###) evaluates the model’s understanding of scene changes and the associated causality.\nSemantic Multi-Image tasks include five semantic-related multi-image tasks.\nEach task contains multiple existing or artificially constructed datasets, each with 200 samples.\nKnowledge Grounded QA task centres on knowledge-based reasoning, where models synthesise multimodal knowledge for single- or multi-hop reasoning tasks.\nThe task employs four datasets: Webpage QA (Chang et al., 2022  ###reference_b6###  ###reference_b6###  ###reference_b6###) for open-domain multi-hop web search, Textbook QA (Kembhavi et al., 2017  ###reference_b18###  ###reference_b18###  ###reference_b18###) for multimodal textbook questions with diagrams and images, Complex Multimodal QA (Talmor et al., 2021  ###reference_b44###  ###reference_b44###  ###reference_b44###) for complex Wikipedia questions with tables and images, and Long Text with Images QA for long text with images questions from Wiki documents.\nText-Rich Images QA task demands the processing and understanding of rich text information embedded directly in images. It calls for models capable of recognizing textual information from images and integrating this textual information with complex reasoning to answer questions.\nIt contains Slide QA (Tanaka et al., 2023  ###reference_b46###  ###reference_b46###  ###reference_b46###) for multi-slides question answering requiring multi-hop and numerical reasoning, OCR QA (Mishra et al., 2019  ###reference_b38###  ###reference_b38###  ###reference_b38###) for book cover image text reading, and Document QA (Mathew et al., 2021  ###reference_b37###  ###reference_b37###  ###reference_b37###) for document images with a focus on understanding document structure.\nVisual Relation Inference task is centered around understanding and inferring visual relationships. It aims to detect subtle variations between two images, such as changes in objects and positional shifts, and subsequently generate accurate descriptions of these changes. This necessitates the model to possess robust capabilities in capturing visual details and generating response.\nThe task leverages Visual Change Captioning (Jhamtani & Berg-Kirkpatrick, 2018  ###reference_b16###  ###reference_b16###  ###reference_b16###; Hosseinzadeh & Wang, 2021  ###reference_b13###  ###reference_b13###  ###reference_b13###) to describe differences between similar images, image change capturing, and Visual Relationship Expressing (Tan et al., 2019  ###reference_b45###  ###reference_b45###  ###reference_b45###) for generating relationship captions between images.\nDialogue task primarily involve the fusion of visual data and natural language dialogue understanding.\nThis task needs the model to understand and process multimodal data, including both textual and visual information, while demonstrating consistency and complementarity in the task.\nThe datasets involve Multimodal Dialogue (Li et al., 2022  ###reference_b28###  ###reference_b28###  ###reference_b28###) for multimodal conversational question answering and Conversational Embodied Dialogue (Shridhar et al., 2020  ###reference_b43###  ###reference_b43###  ###reference_b43###) for mapping from language commands and visuals to action sequences.\nSpace Understanding task requires the model to perceive the spatial environment using multi-image information.\nIt uses the nuscenes (Caesar et al., 2020  ###reference_b5###  ###reference_b5###  ###reference_b5###) dataset, specifically designed for self-driving car technology. It contains sensor data for object detection and tracking, with 1000 annotated scenes for location and properties of objects."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Diagnostic Evaluation",
            "text": "The diagnostic evaluation focuses on the MLLMs’ capability to retrieve information without being distracted in a multimodal long context.\nWe transform the tasks of “Needle in a Haystack” from NLP and “Image Retrieval” from CV into a multimodal format for assessment.\nThis transition preserves the core of the conventional tasks while offering a more challenging and realistic measure of MLLMs’ performance.\nThe “Needle in a Haystack” task requires the model to find a preset password from a long context.\nThis is widely used in diagnostic evaluations of long-context language models (Kuratov et al., 2024  ###reference_b21###  ###reference_b21###  ###reference_b21###).\nIn this study, we reintroduce this novel task from a multimodal perspective to evaluate the context perceptual and retrieval abilities of MLLMs.\nSpecifically, we constructed two tasks, Text Needle In A Haystack (N-1) and Image Needle In A Haystack (N-2).\nExamples are shown in the lower left corner of Figure 3  ###reference_###  ###reference_###  ###reference_### and Figure 11  ###reference_###  ###reference_###  ###reference_### in Appendix B.2  ###reference_###  ###reference_###  ###reference_###, respectively.\nCompared to unimodal Needle in a Haystack task, Text Needle In A Haystack’s haystack includes both text and images and the model is required to recall a randomly generated 7-digit password from this multimodal haystack.\nImage Needle In A Haystack changes the modality of the “needle”, inserting it as text within an image.\nThis cross-modal Needle in a Haystack task requires MLLMs to have not only good perceptual and retrieval abilities but also robust OCR capabilities.\nImage Retrieval task (I-1) (Schall et al., 2022  ###reference_b42###  ###reference_b42###  ###reference_b42###) requires the model to retrieve images from a set of candidates images given an anchor image (an example is shown in the lower right corner of Figure 3  ###reference_###  ###reference_###  ###reference_###).\nIn addition to perceptual and retrieval abilities, this task also necessitates that the MLLM possesses robust abilities in both object and conceptual recognition.\nWe consider this a traditional computer vision task and see image retrieval as a “Needle in a Haystack” task with image modality queries."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Collection and Review Process",
            "text": "We have established a robust data collection process and meticulous review procedures to maintain the integrity and quality of our datasets.\nStatistic\nNumber\n\n\n\nTotal samples\n6,440\n\nTotal images\n97,855\n\nAverage images\n15.2\n\nRange of images\n2 ~109\n\nRange of words\n7 ~11821\n\n(Estimated) Average Tokens\n\n\n- Image token=0\n542.2\n\n- Image token=32\n1,028.4\n\n- Image token=256\n4,432.1\n\n- Image token=576\n9,294.4\n\nSamples by Image Num Level\n\n\n- Few (2~5)\n2,959\n\n- Medium (6~31)\n2,389\n\n- Many (32~109)\n1,092\n###figure_2### We collected the samples from two sources:\n(1) For most of the tasks, we selected and sampled 200 instances from the pre-existing datasets for each task, giving priority to multi-image samples. For video data, we used the Katna (Keplerlab, 2021  ###reference_b19###  ###reference_b19###) to convert the video into a multi-image format by extracting one frame per second.\nThe choice of these pre-existing open-source datasets was driven by their well-established reputation and the fact that they have been published in top-tier conferences and journals, ensuring their credibility and reliability.\nPlease refer to Section Ethics Statement  ###reference_###  ###reference_### and Section 4.3.3  ###reference_.SSS3###  ###reference_.SSS3### for information on the data’s licensing and data contamination issue.\n(2) For the new tasks Long Text with Images QA, Image Retrieval, Text Needle In A Haystack, and Image Needle In A Haystack, we created synthetic data (more details in Appendix B.3  ###reference_###  ###reference_###).\nUltimately, we collected 6,440 samples with varying context lengths.\nDetailed statistics of these samples are presented in Table 4  ###reference_3###  ###reference_3###.\nThe task distribution is demonstrated in Figure 4  ###reference_3###  ###reference_3###.\nA comprehensive breakdown of the datasets, tasks, and taxonomy can be found in Appendix B.1  ###reference_###  ###reference_###.\nFor the open-source dataset comprised of the benchmark, we sample 10% of the data for manual verification.\nOur review team, composed entirely of authors, was assigned to scrutinize the precision of the sampled data, resulting in an Inter-annotator Agreement (IAA)555A degree of consensus or similarity among the annotations made by different annotators on the same data. of 95%, indicating a high level of consistency among reviewers.\nFor the datasets we formulated independently, equivalent manual verification was carried out on the entirety of the dataset, yielding a similar IAA of 98%, thus ascertaining the data quality.\nAdditionally, the error rate was found to be less than 1% for both datasets, affirming that these datasets maintain an exceptionally high quality and are virtually devoid of errors."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "In this study, we conducted an evaluation of several models across three distinct categories that may handle multimodal long contexts, including five closed-source models666Since the performance of closed-source models may change over time, we report testing GPT-4V (gpt-4-turbo-2024-04-09) on April 12, 2024, GPT-4o (gpt-4o-2024-05-13) on May 13, 2024, Claude 3 (claude-3-opus-20240229) on March 16, 2024, Gemini 1.0 (gemini-1.0-pro-vision-001) on March 20, 2024, and Gemini 1.5 (gemini-1.5-pro-latest) on April 12, 2024. Please note that Gemini 1.0 has a limitation of accepting up to 16 images as input and Claude 3 Opus is 20. (GPT-4V (OpenAI, 2023  ###reference_b39###), GPT-4o, Gemini 1.0 (Anil et al., 2023  ###reference_b2###), Gemini 1.5 (Reid et al., 2024  ###reference_b41###), Claude 3 Opus777https://www.anthropic.com/news/claude-3-family  ###reference_mily###),\ntwelve open-source image models (Qwen-VL-Chat (Bai et al., 2023  ###reference_b4###), MiniGPT-v2 (Chen et al., 2023  ###reference_b8###), Cheetor (Li et al., 2023c  ###reference_b24###), Open flamingo (Awadalla et al., 2023  ###reference_b3###), LLaVA-1.5-7B/13B (Liu et al., 2023a  ###reference_b31###), LLaVA-1.6-7B/13B (Liu et al., 2024b  ###reference_b32###), ALLaVA-Longer (Chen et al., 2024  ###reference_b7###), Yi-VL (Young et al., 2024  ###reference_b53###), VILA (Lin et al., 2023  ###reference_b29###), Mantis (Jiang et al., 2024  ###reference_b17###)),\nand five open-source video models (Video-LLaMA-2 (Zhang et al., 2023  ###reference_b56###), Valley (Luo et al., 2023  ###reference_b36###), VideoChat2 (Li et al., 2023d  ###reference_b25###), LLaMA-VID (Li et al., 2023e  ###reference_b27###), LWM (Liu et al., 2024a  ###reference_b30###)).\nThe details of the models are in Appendix C.1  ###reference_###.\nAll models used greedy decoding to generate answers, with a designated generation length between 1 and 512.\nWe conducted all experiments on NVIDIA A100 GPUs.\nTo save costs, all evaluations were performed only in a zero-shot setting.\nThe format of the prompts is detailed in the Appendix C.2  ###reference_###.\nWhen the input length exceeds the maximum context length of the model, we keep the instruction, and truncate the interleaved image-text question from left so as to keep the question of a sample, as instruction and question are critical information and the importance of the last image is higher in many tasks, e.g. multimodal dialogue.\nMetrics for each dataset, as shown in Table 6  ###reference_###, are consistent with the original work for tasks built on previous datasets.\nFor open-ended generation tasks, the popular n-gram-based metric ROUGE-L is adopted, and accuracy is the metric for multiple-choice and needle-in-a-haystack tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment Setup",
            "text": "In this study, we conducted an evaluation of several models across three distinct categories that may handle multimodal long contexts, including five closed-source models666Since the performance of closed-source models may change over time, we report testing GPT-4V (gpt-4-turbo-2024-04-09) on April 12, 2024, GPT-4o (gpt-4o-2024-05-13) on May 13, 2024, Claude 3 (claude-3-opus-20240229) on March 16, 2024, Gemini 1.0 (gemini-1.0-pro-vision-001) on March 20, 2024, and Gemini 1.5 (gemini-1.5-pro-latest) on April 12, 2024. Please note that Gemini 1.0 has a limitation of accepting up to 16 images as input and Claude 3 Opus is 20. (GPT-4V (OpenAI, 2023  ###reference_b39###  ###reference_b39###), GPT-4o, Gemini 1.0 (Anil et al., 2023  ###reference_b2###  ###reference_b2###), Gemini 1.5 (Reid et al., 2024  ###reference_b41###  ###reference_b41###), Claude 3 Opus777https://www.anthropic.com/news/claude-3-family  ###reference_mily###  ###reference_mily###),\ntwelve open-source image models (Qwen-VL-Chat (Bai et al., 2023  ###reference_b4###  ###reference_b4###), MiniGPT-v2 (Chen et al., 2023  ###reference_b8###  ###reference_b8###), Cheetor (Li et al., 2023c  ###reference_b24###  ###reference_b24###), Open flamingo (Awadalla et al., 2023  ###reference_b3###  ###reference_b3###), LLaVA-1.5-7B/13B (Liu et al., 2023a  ###reference_b31###  ###reference_b31###), LLaVA-1.6-7B/13B (Liu et al., 2024b  ###reference_b32###  ###reference_b32###), ALLaVA-Longer (Chen et al., 2024  ###reference_b7###  ###reference_b7###), Yi-VL (Young et al., 2024  ###reference_b53###  ###reference_b53###), VILA (Lin et al., 2023  ###reference_b29###  ###reference_b29###), Mantis (Jiang et al., 2024  ###reference_b17###  ###reference_b17###)),\nand five open-source video models (Video-LLaMA-2 (Zhang et al., 2023  ###reference_b56###  ###reference_b56###), Valley (Luo et al., 2023  ###reference_b36###  ###reference_b36###), VideoChat2 (Li et al., 2023d  ###reference_b25###  ###reference_b25###), LLaMA-VID (Li et al., 2023e  ###reference_b27###  ###reference_b27###), LWM (Liu et al., 2024a  ###reference_b30###  ###reference_b30###)).\nThe details of the models are in Appendix C.1  ###reference_###  ###reference_###.\nAll models used greedy decoding to generate answers, with a designated generation length between 1 and 512.\nWe conducted all experiments on NVIDIA A100 GPUs.\nTo save costs, all evaluations were performed only in a zero-shot setting.\nThe format of the prompts is detailed in the Appendix C.2  ###reference_###  ###reference_###.\nWhen the input length exceeds the maximum context length of the model, we keep the instruction, and truncate the interleaved image-text question from left so as to keep the question of a sample, as instruction and question are critical information and the importance of the last image is higher in many tasks, e.g. multimodal dialogue.\nMetrics for each dataset, as shown in Table 6  ###reference_###  ###reference_###, are consistent with the original work for tasks built on previous datasets.\nFor open-ended generation tasks, the popular n-gram-based metric ROUGE-L is adopted, and accuracy is the metric for multiple-choice and manual image sorting tasks."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Result",
            "text": "LWM suffered from a significant decline in performance due to its training on video-text pairs in the final stage. However, \nafter repeating the image multiple times, we observed an improvement."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Main Result on MileBench",
            "text": "We present the results of our experiments in Table 2  ###reference_### and summarize our findings as follows:\n(1) Closed-source MLLMs outperform open-source MLLMs in multimodal long-context tasks to date, particularly in diagnostic evaluation of long-context adaptability where the gap between closed-source MLLMs (average: 79.2%, max: 99.4%) and open-source MLLMs (average: 10.1%, max: 37.2%) is significant.\nIn realistic evaluation, all open-source models, except for VILA and Mantis, lag considerably behind.\n(2) Open-source image models generally perform better than open-source video models. Even the best video model, LLaMA-VID, falls short in realistic evaluation with 31.8%, a score that is lower than eight image models. This may be due to the inability of video models to capture detailed information in images in the same way that image models can.\n(3) The ability to adapt to long contexts and perform long-context tasks are not necessarily linked. For example, while Qwen-VL-Chat scores the highest in diagnostic evaluation among open-source models, it trails behind Mantis in task completion (39.1% <47.5%), highlighting our evaluation’s diversity and comprehensiveness.\n(4) Interestingly, the majority of open-source models scored zero in the Image Needle in a Haystack task. Upon inspection, we found that many of these models partially answered the needle numeric string without completely getting it right. This suggests that open-source models need to improve their ability to retrieve information from images, particularly their OCR capabilities.\nDetailed results from the realistic evaluation can be found in Appendix C.3  ###reference_###."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Error Case Study",
            "text": "###figure_3### ###figure_4### We conducted an error analysis to further investigate the flaws of the models.\nAn example of the Space Understanding task is displayed in the upper part of Figure 5  ###reference_###.\nWhen recognizing spatial positions and current actions, GPT-4V declined to respond, while other models did not correctly follow the instructions to answer the question.\nThey merely generated captions for the images, which could be related to them not having been trained on multi-image QA data, emphasizing the importance of multi-image training.\nIn a Visual Relation Inference task example (Figure 5  ###reference_### bottom), Qwen-VL-Chat and Valley struggled with image differentiation and instruction following, resulting in inaccurate inferences. This suggests MLLMs could improve in recognizing subtle image differences, possibly due to their low-resolution visual models. The illusion issue in multi-image inputs for video models also highlights the need for ample multi-image training data."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "In this section, we delve into a meticulous analysis of the results, focusing on five primary research questions that revolve around the MileBench:\nHow do MLLMs perform given contexts with different lengths?\nDo MLLMs also get “Lost in the Middle” in multimodal long contexts?\nDoes data contamination issue exist in MileBench?\nDoes combined image help multi-image comprehension?\nHow diverse and comprehensive is MileBench?"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Performances Decline as the Number of Images Increases for Most MLLMs",
            "text": "###figure_5### To investigate the performance of MLLMs with varying numbers of images, we divide our dataset into three levels: Few, Medium, and Many, based on the number of images per sample.\nThe specific quantities for each level can be found in Table 4  ###reference_3###.\nFigure 6  ###reference_### reports the average performance of the model on the three types of data with different numbers of images.\nIt can be observed that as the number of images increases, the performance of most models significantly declines (as indicated by a steep slope in the curve), especially for the LLaVA-1.5 series models.\nThis is likely because most models have only been trained on single image, resulting in insufficient generalization for multi-image test data.\nHowever, the performance of GPT-4V, GPT-4o, Gemini 1.5, Claude 3 Opus and Qwen-VL-Chat on the Medium level surpasses that of the Few level.\nThis could be attributed to their training on multi-image data, where a larger number of images can provide more information to some extent, thereby aiding the model in task completion.\nDespite their outstanding performance on multi-image tasks, their performance still declines when the number of images reaches the Many level.\nThis leaves room for future development in modeling for multi-image context.\n###figure_6### ###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 “Lost in the Middle” for MLLMs",
            "text": "Liu et al. (2023b  ###reference_b33###) pointed out that in needle-in-a-haystack tasks involving long texts, models may experience the “Lost in the Middle” phenomenon, where they struggle to find the needle located in the middle of the context.\nWe investigated whether MLLMs would exhibit the “Lost in the Middle” phenomenon in multimodal contexts.\nWe chose the two best-performing models from closed-source and open-source models in the Needle in a Haystack task for analysis.\nAs can be seen from the results in Figure 7  ###reference_###, MLLMs displayed varying behaviors.\nIn multimodal long contexts, GPT-4V did not “get lost in the middle” and managed to complete the two tasks impressively with the scores 99.7% (N-1) and 99.1% (N-2).\nOn the other hand, ignoring the scenarios where the data exceeds its maximum context length (8192 tokens or 32 images) and gets truncated, Qwen-VL-Chat showed a certain degree of “lost in the middle”, particularly evident in the image needle task.\nThis indicates that the “lost in the middle” phenomenon also exists in multimodal scenarios.\nHowever, a strong ability to manage long context can significantly reduce this risk."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Risk of Data Contamination for MileBench",
            "text": "Considering MileBench’s use of public datasets, there’s a potential risk of data contamination.\nOur investigation involved excluding models trained solely on single-image tasks and opting for cost-effective open-source models, resulting in Qwen-VL-Chat, Cheetor, Open flamingo, and VILA (details in Appendix C.1  ###reference_###).\nWe referred to Wei et al. (2023  ###reference_b48###) and constructed an Adversarial (ADV) Set with shuffled options and paraphrased reference answers and evaluated the difference between original and ADV results.\nResults (Table 3  ###reference_###) show a negligible performance drop (0.1%~1.2%) for all models, indicating minimal likelihood of these models being trained on our dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Experiment on Combined-image Set",
            "text": "To overcome the constraint that models support only a minimal number of image inputs, we introduced Combined-image Set, and to distinguish it from the original MileBench, we will henceforth refer to MileBench as the Multi-image Set.\nIn the Combined-image Set, multiple images are merged into one large image, positioned at the beginning of the input.\nThe original images in the text are then substituted with placeholders.\nTo save the cost, we only selected three closed-source MLLMs to evaluate.\nWe show the result on combined image set in Table 4  ###reference_### and summarize our findings as follows:\n(1) The performance of proprietary models still surpasses that of open-source models in both realistic evaluation (average: 44.8% v.s. 29.6%, max: 48% v.s. 44.9%) and diagnostic evaluation (average: 51.2% v.s. 12.3%, max: 60.5% v.s. 32.4%).\n(2) In comparison to the results on the multi-image set, the performance of proprietary models declined, except for Gemini 1.0, which is limited by the number of images uploaded on the multi-image set.\nThe potential reason is that to maintain performance on the combined image set, models need to possess high-resolution vision models that can effectively distinguish multiple images combined together. For instance, Gemini 1.0 adjusts images of excessively large resolution to a size of . On the other hand, GPT-4V and Claude 3 Opus resize the images to dimensions of  and , respectively. The lower resolution input of GPT-4V and Claude 3 Opus, in comparison to Gemini 1.0, could potentially be a factor for their diminished performance.\n(3) Compared to the results on the multi-image set, the performance of some open-source models with short contexts improved, such as ALLaVA-Longer (from 24.7% to 26.9%) and MiniGPT-v2 (from 17.8% to 29.5%). The possible reason is that these models have only been trained on single images and cannot effectively generalize to multi-image scenarios. Combining images can effectively alleviate this issue."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Inter-task Correlation in MileBench",
            "text": "###figure_10### To investigate the multi-task characteristics of the realistic evaluation in our MileBench, we analyzed the performance of all models across the nine tasks within this evaluation and calculated pairwise correlations between different tasks, as shown in Figure 8  ###reference_###.\n(1) We found that, aside from Task S-3 (Visual Relation Inference), tasks within the same category (either temporal multi-image or semantic multi-image) exhibited high correlation.\nTask S-3, being a challenging one, showed little variation in scores across models.\n(2) We also noted that Task T-3 (Visual Navigation and Spatial Localization) demonstrated lower correlation with other tasks, possibly due to its requirement of unique cognitive skills such as understanding the world from a first-person perspective.\nThese observations suggest that the realistic evaluation of MileBench encompasses a broad range of task types, offering a more comprehensive assessment in the context of multi-image long-context scenarios."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Directions",
            "text": "In this study, we introduced MileBench, a pioneering benchmark designed to rigorously evaluate the multimodal long-context capabilities of MLLMs.\nWe have established the diagnostic and realistic evaluation sets, designed to systematically assess the MLLMs’ capacity for long-context adaptation and proficiency in task completion within these contexts.\nDespite some impressive performances, our experimental results underscore the urgent need for more focused research to enhance MLLMs’ capabilities in these complex scenarios.\nMoving forward, we suggest two primary research directions:\n(1) Long-context MLLM: Given the ubiquity of mixed media content, there is a pressing need for models that can adeptly process multiple images in long-context scenarios.\n(2) Scaling MileBench to Larger Contexts and Other Modalities: As real-world tasks continue to evolve, benchmarks should also adapt, incorporating larger contexts, complex task structures, and additional modalities to stimulate the development of more versatile MLLMs.\nThese efforts will help equip MLLMs better for our increasingly multimodal world."
        }
    ]
}