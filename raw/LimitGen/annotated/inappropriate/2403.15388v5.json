{
    "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
    "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning capabilities by connecting a visual encoder and a large language model.\nLMMs typically take in a fixed and large amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content.\nRecent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which further increases the number of visual tokens significantly.\nHowever, due to the inherent design of the Transformer architecture, the computational costs of these models tend to increase quadratically with the number of input tokens.\nTo tackle this problem, we explore a token reduction mechanism that identifies significant spatial redundancy among visual tokens. In response, we propose PruMerge, a novel adaptive visual token reduction strategy that significantly reduces the number of visual tokens without compromising the performance of LMMs.\nSpecifically, to metric the importance of each token, we exploit the sparsity observed in the visual encoder, characterized by the sparse distribution of attention scores between the class token and visual tokens.\nThis sparsity enables us to dynamically select the most crucial visual tokens to retain.\nSubsequently, we cluster the selected (unpruned) tokens based on their key similarity and merge them with the unpruned tokens, effectively supplementing and enhancing their informational content.\nEmpirically, when applied to LLaVA-1.5 [Liu et al., 2023a], our approach can compress the visual tokens by 14 times on average, and achieve comparable performance across diverse visual question-answering and reasoning tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) [OpenAI, 2023b  ###reference_b32###, Team et al., 2023  ###reference_b38###, Jiang et al., 2023  ###reference_b18###, Touvron et al., 2023  ###reference_b39###] have shown strong reasoning abilities. LLMs are usually high-capacity Transformers [Vaswani et al., 2017  ###reference_b40###] pretrained with a large-scale text corpus. Large Multimodal Models (LMMs), inherit LLMs for text generation, while also leveraging a visual encoder such as CLIP-ViT [Radford et al., 2021  ###reference_b34###] to embed image patches into visual tokens as the prefix visual context.\nLMMs need substantial computation for inference. The LLM is the primary factor for the high computation cost, since the visual encoder is usually quite small relative to the LLM. For example, the commonly used CLIP visual encoder, ViT-L, only has 0.3B parameters, while the corresponding LLM such as LLaMA [Touvron et al., 2023  ###reference_b39###] or Vicuna [Vicuna, 2023  ###reference_b41###] can have 7B or 13B parameters.\nAs a result, reducing the LLM’s inference cost is the key to achieving low LMM inference cost.\nPrior works [Chu et al., 2023  ###reference_b7###, 2024  ###reference_b8###, Yuan et al., 2023a  ###reference_b46###] mainly focus on replacing the LLM backbone with a smaller language model with less parameters, such as Phi-2 [Javaheripi et al., 2023  ###reference_b17###].\nHowever, such approaches sacrifice the reasoning abilities of LLMs, leading to a large performance gap on visual question-answering and reasoning tasks such as VQAv2 and MM-Bench [Chu et al., 2024  ###reference_b8###]. A similar approach is to apply quantization for LLMs [Liu et al., 2023b  ###reference_b26###, Yuan et al., 2024  ###reference_b48###].\nHowever, the cost of LLMs comes from not only its large number of parameters, but also the length of the input context due to the quadratic complexity of the Transformer’s attention operation. The context length in LMMs is especially important, where a fixed amount of visual tokens serves as the prefixed tokens. For example, in LLaVA-1.5, 576 visual tokens are appended, and in Video-LLaVA [Lin et al., 2023  ###reference_b23###] that number is even higher, leading to high training and inference costs. Thus, an intriguing question is: Can we reduce the number of prefix visual tokens while maintaining comparable performance?\n###figure_1### ###figure_2### In our study, we find that many visual tokens are redundant, similar to findings in previous related work [Bolya et al., 2023  ###reference_b3###, Liu et al., 2022  ###reference_b28###], and most of the visual tokens can be pruned with little sacrifice in performance.\nIn particular, the similarity (i.e., attention scores in the visual encoder’s self-attention module) between the class token and spatial patches are sparse, indicating that only a small number of visual tokens are related to key visual information in the visual samples.\nMotivated by this, we use this sparse similarity to adaptively select important visual tokens, as shown in Fig.1(b)  ###reference_sf2###.\nSpecifically, we leverage the Interquartile Range (IQR) [Boukerche et al., 2020  ###reference_b4###] scoring function in outlier detection to prune unimportant visual tokens. Moreover, we merge the visual tokens using -nearest neighbor and update the selected important visual tokens via weighted averaging, which further enhances performance.\nFinally, we design PruMerge+, which samples visual tokens spatial-uniformly to complement the unpruned tokens.\nPruMerge+ not only minimizes performance degradation but also ensures substantial token reduction, maintaining a more comprehensive and representative selection of visual tokens.\nEmpirically, PruMerge can effectively and adaptively reduce the visual tokens in each image in LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###], where with just 5.5% of visual tokens, which is around 32 tokens for an image on average, LLaVA-PruMerge can maintain comparable performance with that of retaining all 576 tokens across diverse benchmarks.\nFurthermore, PruMerge showcases its versatility across various modalities, including video. By integrating PruMerge with Video-LLaVA during the inference phase alone—eliminating the need for additional training—we not only expedite processing within video-LLMs but also enhance their performance across multiple benchmarks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Efficient Large Multimodal Models (LMMs)",
            "text": "Large Language Models (LLMs) such as GPT-4 [OpenAI, 2023b  ###reference_b32###], LLaMA [Touvron et al., 2023  ###reference_b39###], Mistral [Jiang et al., 2023  ###reference_b18###], and Gemini [Team et al., 2023  ###reference_b38###] have demonstrated strong question answering and reasoning capabilities over text. Large Multimodal Models (LMMs) [Liu et al., 2023b  ###reference_b26###, Zhu et al., 2023  ###reference_b53###, Yin et al., 2023  ###reference_b45###, Zhang et al., 2024  ###reference_b49###] extend these reasoning capabilities to images, where given an image and an associated question, a vision encoder and an LLM are leveraged to generate text responses in a chat format. More recent works extend whole-image understanding into region-level understanding [Cai et al., 2024  ###reference_b5###, Zhang et al., 2023b  ###reference_b51###, Peng et al., 2023  ###reference_b33###, Chen et al., 2023  ###reference_b6###], video understanding [Lin et al., 2023  ###reference_b23###, Zhang et al., 2023a  ###reference_b50###] and 3D scene understanding [Hong et al., 2023  ###reference_b15###]. Such works typically feed the visual tokens directly into the LLM as prefix tokens, via either an MLP [Liu et al., 2023a  ###reference_b25###], Qformer [Dai et al., 2023  ###reference_b9###, Zhu et al., 2023  ###reference_b53###], or resampler [Alayrac et al., 2022  ###reference_b1###]. The number of visual tokens can be prohibitively long, especially when the images are high-resolution [Liu et al., 2024  ###reference_b27###, OpenAI, 2023a  ###reference_b31###]. In this paper, we reduce the number of visual tokens with a novel adaptive prune and merge procedure.\nWhile LMMs have made significant advances, their large-scale training and deployment incur significant computational costs, requiring efficient parallel device implementations. Google’s Gemini [Team et al., 2023  ###reference_b38###] is a pioneer in efficient LMMs, achieving state-of-the-art performance on multimodal benchmarks and introducing mobile-scale LMMs suitable for low-memory devices, although it is not open-source. Open-source alternatives like LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###] employ advanced compression techniques such as 4/8 bit quantization [Dettmers et al., 2022  ###reference_b10###, Shang et al., 2024  ###reference_b35###]. MobileVLM [Chu et al., 2023  ###reference_b7###] and its improved version, MobileVLM-v2 [Chu et al., 2024  ###reference_b8###], focus on compact architecture designs and training optimizations for mobile use. Similarly, TinyGPT-V [Yuan et al., 2023a  ###reference_b46###] uses the Phi-2 [Javaheripi et al., 2023  ###reference_b17###] LLM backbone to exceed the performance of larger models. LLaVA-Phi [Zhu et al., 2024  ###reference_b54###] and Vary-toy [Wei et al., 2024  ###reference_b43###] introduce smaller backbones and enhanced vocabularies for better generalizability. TinyLLaVA [Zhou et al., 2024  ###reference_b52###] explores the effects of architectural choices and training optimizations, achieving comparable performance to larger models. MoE-LLaVA [Lin et al., 2024  ###reference_b24###] incorporates a Mixture of Experts to address model sparsity, enhancing efficiency and performance.\nIn most cases, LMM efficiency is enhanced by reducing the size of the backbone of the LMM, but no work has considered the efficiency of the LMM from the perspective of the number of visual tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Token Reduction",
            "text": "The notorious quadratic complexity in Transformers [Vaswani et al., 2017  ###reference_b40###] is a well-known problem, as it is one of the key bottlenecks in scaling the input sequence length. Sparse attention methods such as Linformer [Wang et al., 2020  ###reference_b42###] and ReFormer [Kitaev et al., 2020  ###reference_b19###] reduce the quadratic attention complexity by conducting attention operations within a certain region rather than the full context. Token merging [Bolya et al., 2023  ###reference_b3###] utilizes full attention but gradually reduces the number of tokens in each transformer block by selecting the most representative tokens with bipartite matching.\nOne of the main causes of the inefficiency of recent LMMs is their use of a large number of prefix visual tokens that serve as a fixed budget for context [Liu et al., 2023b  ###reference_b26###, Zhu et al., 2023  ###reference_b53###].\nIn our study, we present a novel plug-and-play token reduction method based on visual token similarities, which achieves comparable performance with less than one tenth of the original tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method: Token Pru-Merging",
            "text": "In this section, we first review the basic implementation of large mutilmodal models (LMMs), with a particular focus on the visual encoder component (i.e., Vision Transformer). We highlight the direct correlation between the number of visual tokens and the efficiency of LMMs (Sec. 3.1  ###reference_###).\nNext, we present a plug-and-play token reduction method specifically designed for LMMs, called token PruMerge.\nOur method features two key components:\n(1) Adaptive Important Token Selection (AITS) via Outlier Detection which adaptively determines the optimal number of visual tokens to retain based on the unique characteristics of the image (Sec. 3.2  ###reference_###); and (2)\nToken Supplement (TS) via Similar Key Clustering, which facilitates efficient processing without compromising the model’s performance by maintaining the integrity and richness of the visual information (Sec. 3.3  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "###figure_3### Vision Transformers (ViTs) [Dosovitskiy et al., 2020  ###reference_b11###] are the most widely used vision encoder for LMMs, in which the input image is converted into a sequence of representative tokens by the ViT, and then fed into an LLM for understanding [Liu et al., 2024  ###reference_b27###, Zhu et al., 2023  ###reference_b53###, Hong et al., 2023  ###reference_b15###, Zhang et al., 2024  ###reference_b49###].\nAn input image is divided into a grid of patches and each patch is projected into a token embedding by the ViT. In addition to the patch tokens, a class token (i.e., [CLS] token) is computed to aggregate global image information for classification.\nA ViT consists of a set of transformer blocks, which in turn consist of several essential components: a multi-head self-attention (MSA) layer, a feed-forward neural network (FFN), skip connections, and layer normalization [Ba et al., 2016  ###reference_b2###]. These components work together to improve the model’s capability to understand visual data [Han et al., 2022  ###reference_b14###].\nIn the self-attention layer, an input token is projected into three distinct vectors: query , key , and value , using three linear transformation matrices , , and .\nThese vectors, corresponding to different inputs, are assembled into matrices , , and , respectively. The self-attention computes the relevance of each item to other items:\nwhere attention matrix  and  is the dimension of  and .\nIn the last layer of the ViT, the [CLS] token is used for classification. Similarly, the attention between [CLS] token and other visual tokens is computed by the attention mechanism:\nThe MSA framework allows for simultaneous attention on multiple positions, offering diverse representation subspaces. This is achieved by employing distinct query, key, and value matrices for different heads, which project the input vectors into different representation subspaces.\nAfter the self-attention layers is the feed-forward network (FFN), which consists of two linear transformation layers separated by a nonlinear activation function:\nwhere  and  are the matrices of the linear transformation layers, and  denotes the nonlinear activation function. The general forward pass of ViT is illustrated in the left part of Figure 2  ###reference_###.\nLarge Multimodal Models (LMMs). Following the forward pass through a Vision Transformer (ViT), a set of visual tokens is generated. These tokens are then processed by the input projector , which maps the encoded visual features from  into the text feature space .\nThe aligned features and the text prompts  are then fed into the LLM backbone [Zhang et al., 2024  ###reference_b49###]. The overall architecture of an LMM is depicted in Figure 1  ###reference_###.\nIt is important to note that the computational cost with these models increases quadratically with the number of input tokens to the LLM [Tay et al., 2022  ###reference_b37###].\nMathematically, if there are  tokens in the input, the self-attention mechanism computes a  matrix of attention scores, where each entry in this matrix represents the attention score between a pair of tokens.\nHowever, there is an increasing demand for processing high-resolution images and videos, which increases the number of visual tokens, further exacerbating computation costs.\nThe reduction of visual tokens presents a promising approach to improving the efficiency of LMMs by reducing the escalating computational requirements."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adaptive Important Token Selection via Outlier Detection",
            "text": "###figure_4### ###figure_5### The most straightforward solution to improve the efficiency of visual token utilization in LMMs is to prune redundant visual tokens [Liu et al., 2022  ###reference_b28###, Yin et al., 2022  ###reference_b44###, Liang et al., 2022  ###reference_b22###]. To realize token pruning, we need to address a pivotal question: How do we determine the importance of each visual token? As discussed in Sec. 3.1  ###reference_###, LMMs typically leverage an extensive stack of visual tokens to represent the visual information. On the other hand, self-/weakly-supervised learning paradigms, such as CLIP [Radford et al., 2021  ###reference_b34###] simplify this complexity by representing an entire image with a single [cls] token, regarded as the most information-condensed token. To balance those two extreme paradigms, we investigate the Key-Query attention between [cls] token and visual tokens, i.e., in Equation 3.2  ###reference_###. Observing the distribution patterns of attention between the [cls] token and visual tokens unveils a sparse landscape, as depicted in Figure 3(a)  ###reference_sf1###. This sparse distribution underpins our methodology for identifying crucial visual tokens. By employing outlier detection algorithms, we aim to adaptively select visual tokens that best represent an image’s features effectively. Random selection of visual tokens without any criteria. To identify outliers within class attention values, we adopt this approach instead of traditional methods, relying merely on chance without specific mathematical foundations. This randomness lacks the structured confines set by explicit methods but still engages in the exploration of token prominence. Specifically, this methodology disregards typical computations such as establishing a “lower fence” or “upper fence” through statistical means, thereby embracing randomness as the guiding tool for token selection. Through this method, we can arbitrarily identify and select the visual tokens for each image that will represent the image within the LMM context. Note that we use the class attention value from the penultimate layer for this arbitrary selection. As shown in Figure 1(b)  ###reference_sf2###, the randomly sampled visual tokens demonstrate two behaviors: (1) The number of attentive tokens are not necessarily proportional to the complexity of the image, as randomness dictates the distribution. (2) The sampled tokens may not precisely align with important content, reflecting the stochastic nature of selection. These trends are also observed at the benchmark level; in Table 6  ###reference_###, the average token numbers on various benchmarks appear haphazardly distributed."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Token Supplement via Similar Key Clustering",
            "text": "Following the selection of informative visual tokens, we next optimize the utilization of the remaining tokens.\nWhile pruned tokens may initially seem extraneous, they hold potential value for the perception capabilities of the LLM backbone.\nThis potential arises particularly in cases where an image contains large object parts that dominate the scene. In such scenarios, overly aggressive pruning could inadvertently diminish the model’s ability to represent the image comprehensively.\nTo address this, we devise a token merging method aimed at enhancing the representational capacity of the selected unpruned tokens. This method involves the strategic fusion of currently pruned tokens, as depicted in Figure 2  ###reference_###.\nTo choose the pruned tokens to merge, we need a way to measure similarity between visual tokens.\nHere we leverage the self-attention mechanism in ViTs.\nSince the key vector of each patch token already contains information summarized in the self-attention module [Vaswani et al., 2017  ###reference_b40###], the final layer’s key vector serves as the representation.\nAnd then we use the dot product between keys to calculate which tokens have similar visual information [Bolya et al., 2023  ###reference_b3###]:\nwhich yields  for tokens  in vectorized form for the set of all tokens , where  is the number of input visual tokens.\nWith the similarities between visual tokens established, we simply find the -nearest neighbors for each unpruned token, which act as the cluster centers.\nThe integration of pruned tokens into these clusters is guided by their respective class attentions , enabling a refined representation of each unpruned token through a weighted sum.\nThis procedure is outlined in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "PruMerge+: Bridging the Efficiency-Performance Gap",
            "text": "While PruMerge achieves a remarkable reduction in the number of visual tokens—over tenfold compared to the original setup—the process is not without drawbacks. Specifically, the compression technique, though efficient, introduces a marginal performance discrepancy between the original LLaVA model and its PruMerge-optimized counterpart, LLaVA-PruMerge. To address this, we introduce PruMerge+, a refined version that strikes an optimal balance between token reduction and model performance.\nPruMerge+ enhances our original method by maintaining the ability to significantly reduce visual token count—by an average of fourfold—with minimal performance degradation. This improvement is detailed in Algorithm 1  ###reference_###, building upon the token selection strategies outlined in Section 3.2  ###reference_###.\nA new aspect of PruMerge+ lies in its enhanced token selection process. Beyond merely focusing on the previously identified important tokens, PruMerge+ extends its reach to encompass additional visual tokens from areas initially deemed less critical. This is achieved through a spatially uniform sampling of visual tokens, guided by a predetermined ratio informed by the distribution of outlier tokens. This methodology ensures a more comprehensive and representative selection of visual tokens, as depicted in Figure 3(b)  ###reference_sf2###, thereby minimizing performance losses while still achieving substantial token reduction."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Discussion",
            "text": "Distinction from Existing Token Reduction Methods.\nToken reduction methods [Liu et al., 2022  ###reference_b28###, Yin et al., 2022  ###reference_b44###, Liang et al., 2022  ###reference_b22###] have been proposed for accelerating ViT computation speed. Block by block, tokens are gradually reduced in number, correspondingly reducing the computation cost in the internal ViT.\nBy contrast, our approach is not specifically designed to increase the efficiency of a ViT.\nIt instead aims to improve the overall efficiency of a large multimodal model, where ViT is only one relatively light cost component as shown in Fig. 1  ###reference_###.\nThere are two benefits of this design.\nFirst, while ViTs typically rely on a single class token to represent the input image, which enables them to maintain performance despite a reduction in intermediate tokens, LMMs usually require a large stack of visual tokens. This ensures a comprehensive representation of the visual content, preserving the model’s ability to capture nuanced details.\nThus, using previous token merging methods to obtain one refined class token as representation of visual input is not consistent with the literature of large multimodal models.\nSecond, considering that the bulk of computational demand within LMMs is attributed to the LLM component rather than the ViT, our approach focuses not only on the reduction of tokens but also on maximizing the informational content of the pruned visual tokens. This strategy addresses the computational challenges inherent in LMMs with minimal compromise in the quality of the visual representation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We first show the empirical performance of our approach when applied to LLaVA-1.5 in Sec 4.1  ###reference_###. We then analyze the efficiency improvement by using our PruMerge on LMM in Sec 4.2  ###reference_###. To show the generalization ablity, we conduct a series of experiments in Sec. 4.3  ###reference_###. Finally, we demonstrate the effectiveness of each component in our model in Sec 4.4  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We apply our method to LLaVA-1.5 [Liu et al., 2023a  ###reference_b25###], a recent state-of-the-art LMM. We further finetune LLaVA-1.5 using LoRA [Hu et al., 2022  ###reference_b16###] for 1 epoch using the LLaVA-1.5 instruction fine-tuning data [Liu et al., 2023a  ###reference_b25###] with our reduced visual tokens.\nWe evaluate on diverse visual question-answering and reasoning benchmarks including VQAv2 [Goyal et al., 2017  ###reference_b13###], ScienceQA [Lu et al., 2022  ###reference_b30###], TextVQA [Singh et al., 2019  ###reference_b36###], POPE hallucination bench [Li et al., 2023b  ###reference_b21###], MME [Fu et al., 2023  ###reference_b12###], and MMBench [Liu et al., 2023c  ###reference_b29###].\nAs shown in Table 1  ###reference_###, our approach achieves comparable performance with LLaVA-1.5 despite using only a small fraction of the visual tokens, and performing better than previous works such as BLIP2 [Li et al., 2023a  ###reference_b20###] and InstructBLIP [Dai et al., 2023  ###reference_b9###]. Specifically, in POPE and ScienceQA, our approach even shows better performance than LLaVA-1.5.\nNote that due to the adaptive nature of PruMerge (see Sec. 3.2  ###reference_###), the token numbers for various tasks are different (see 4.4.1  ###reference_.SSS1###), and thus we use the average number on numbers of 6 tasks (i.e., 32) for simplicity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Efficiency Analysis",
            "text": "To elucidate the computational efficiency afforded by PruMerge, we utilize the roofline-based LLM-Viewer analysis developed in [Yuan et al., 2024  ###reference_b48###]. Our investigation is grounded in a theoretical scenario tailored to highlight the impact of PruMerge on processing efficiency within LMMs.\nConsider a typical scenario where an image of dimensions  pixels is processed using a CLIP-ViT model, resulting in 576 visual tokens. Accompanying this image is a text prompt, assumed to contain 40 tokens for the sake of this analysis. Through the application of PruMerge, we achieve a dramatic reduction in the number of visual tokens, decreasing the original count by approximately 14.4 times in MME/TextVQA to match the token count of the text prompt ().\nThe implications of this reduction are significant, as demonstrated in Table 2  ###reference_###, which outlines the computational cost associated with the LMM prefill process. Notably, PruMerge not only enhances the speed of the LLM prefill process by reducing the required floating-point operations (FLOPs) but also contributes to a reduction in computational memory demands.\nIt is important to emphasize that the benefits of PruMerge  extend beyond mere efficiency gains. Our token reduction strategy can complement other LLM acceleration techniques, such as quantization and factorization [Yuan et al., 2023b  ###reference_b47###]. This orthogonal relationship underscores the versatile potential of PruMerge to contribute to a broader spectrum of efficiency-enhancing strategies."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Generalization on Video-LLM",
            "text": "To assess the generalization capabilities of PruMerge and PruMerge+ across different modalities, we next extend our approach to Video-LLaVA [Lin et al., 2023  ###reference_b23###].\nVideo-LLaVA is one of the most popular open-soruced Video-LLMs.\nWe seamlessly integrate both algorithms into Video-LLaVA without the need for additional training, enabling us to bypass re-training on video datasets during inference. The outcome of this integration is shown in Table 3  ###reference_###.\nVideo-LLaVA samples 8 frames from a video clip and extracts  visual tokens using a visual encoder for LLM perception, which is 4 times of visual token than LLaAV-1.5 [Liu et al., 2023a  ###reference_b25###].\nOur Algorthms PruMerge and PruMerge+ can adaptively select important 256 (12.5% on average) and 256 (25.0% on average) important visual tokens, respectively.\nThe results demonstrate that our algorithms not only reduce the number of visual tokens in Video-LLaVA but also is able to enhance its performance.\nThis finding is noteworthy as it suggests a significant redundancy in the visual tokens used by video-LLMs. Exploring ways to further capitalize on this redundancy could shape future research directions."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Study",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Token Sampling Strategy Analysis",
            "text": "Here we show how our approach performs better than the vanilla visual token sampling strategy, including sequential sampling and spatial sampling.\nLLaVA-PruMerge: Our approach dynamically samples key visual tokens (see Sec. 3.2  ###reference_###), which results in 40 visual tokens per image on average for TextVQA/MME, 35 tokens for POPE, and 16 tokens for SQA. The visualization is shown in Figure 4  ###reference_### (b).\nSequential sampling: We sample  tokens in the flatted visual tokens. For example, the first 40 tokens are sampled for an apples-to-apples comparison, shown in Figure 4  ###reference_### (c).\n###figure_6### Spatial sampling: The sampled  tokens are evenly distributed across the image, as shown in Figure 4  ###reference_### (d-h). We study diverse settings, including 6  6 (36 tokens), 5  8 (40 tokens), 8  5 (40 tokens), 5  7 (35 tokens), 7  5 (35 tokens), and 4  4 (16 tokens).\nNote that all the experiments are done via a training-free manner. As shown in Table 6  ###reference_###, our approach is consistently better than sequential sampling and spatial sampling across all downstream tasks, which demonstrates the effectiveness of the sampling mechanism of LLaVA-PruMerge. Importantly, we observe that LLaVA-PruMerge shows much better performance on TextVQA [Singh et al., 2019  ###reference_b36###]. Such Optical Character Recognition (OCR) task requires detailed information about the text, which demonstrates that LLaVA-PruMerge extracts the key information in the images with enough details. This quantitative result aligns with the visualization of LLaVA-PruMerge attentive tokens in Figure 1(b)  ###reference_sf2###, where more attentive tokens are distributed on the foreground text in the images."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Effectiveness of Each Module in PruMerge",
            "text": "Here, we study the effectiveness of each module in our design based on LLaVA-1.5. Note that we maintain the same amount of visual tokens (6.9%, 40 tokens) across all settings.\nAs shown in Table 6  ###reference_###, after progressively adding the proposed modules, including Adaptive Important Token Selection (AITS) and Token Supplement (TS), the downstream performance can be further enhanced."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Training Analysis: Training-free v.s. Fine-tuning",
            "text": "Finally, LLaVA-PruMerge can be conducted in either a training-free or fine-tuning manner. With fine-tuning, the large language model can adapt to the new structure of visual tokens, which could further enhance the performance on vision-language tasks. As shown in Table 6  ###reference_###, with fine-tuning, our approach does bring better performance for diverse tasks, including ScienceQA [Lu et al., 2022  ###reference_b30###], TextVQA [Singh et al., 2019  ###reference_b36###], POPE [Li et al., 2023b  ###reference_b21###], and MME [Fu et al., 2023  ###reference_b12###]."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we improve the efficiency of Large Multimodal Models (LMMs) from the perspective of reducing the quantity of visual tokens. By leveraging the spatial redundancy in visual tokens, we proposed a plug-and-play token reduction module that employs the similarity between the class token and spatial tokens as a key criterion for pruning and merging visual tokens.\nOur approach, applied to LLaVA-1.5, demonstrated that by utilizing only 6.9% of visual tokens on average, the pruned tokens can maintain comparable performance across a wide range of visual question-answering and reasoning tasks. Notably, our work highlights the potential for significant computational savings without sacrificing the reasoning capabilities of LMMs. We hope our work inspires further exploration into the interplay between efficiency and performance in LMMs."
        }
    ]
}