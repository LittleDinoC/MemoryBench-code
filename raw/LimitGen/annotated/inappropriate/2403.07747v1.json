{
    "title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models",
    "abstract": "To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon.\n\n\n\nKeywords: Large Language Models, Mathematical Reasoning Evaluation, Benchmark",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Mathematics has always been an important part of the evaluation of LLMs (Wei et al., 2022  ###reference_b20###), which not only assesses the ability of LLMs in understanding and solving mathematical problems, but also profoundly measures the essential capability of LLMs in abstract conceptualization, logical reasoning and so on. Therefore, a high-quality mathematical evaluation benchmark is of great importance to a comprehensive LLM evaluation.\nPrevious works (Hosseini et al., 2014  ###reference_b5###; Roy and Roth, 2015  ###reference_b18###) curate mathematical test sets in English, which serve as a repository for grade school math word problems with accuracy being used as the evaluation metric. Recent years have witnessed a substantial progress in Chinese LLMs. Hence, mathematical evaluation datasets in Chinese (Wei et al., 2023  ###reference_b21###; Yang et al., 2023  ###reference_b24###) have been created correspondingly. These two previous Chinese datasets categorize testing instances by grade levels, providing a preliminary evaluation of Chinese LLMs on these levels. Their evaluation results show that the accuracy of GPT-4 for any grade surpasses or is close to 60%. However, a simple accuracy does not help us understand which mathematical concepts or skills LLMs have mastered. There is an urgent need for a comprehensive test set that can provide fine-grained evaluation results.\n###figure_1### Apart from arithmetic operations, mathematical ability involves diverse reasoning capabilities. We believe that the evaluation of the mathematical ability of LLMs should include two aspects:\nProviding diverse abstract mathematical concepts.\nEvaluating the logical and mathematic reasoning abilities of LLMs over mathematical problems at different difficulty levels.\nIn pursuit of these aspects, we propose FineMath, a benchmark composed of Math Word Problems (MWPs), designed to comprehensively assess LLMs’ mathematical capability in a fine-grained way. FineMath organizes MWPs according to key mathematical concepts taught in elementary school, and each type of MWPs contains three levels of difficulty, facilitating detailed reasoning ability analysis.\nSpecifically, FineMath consists of 17 types of MWPs. For defining and collecting these MWP types, we have referred to the mathematics curriculum standards established by China’s Ministry of Education and the principles and standards for school mathematics set by the American National Council of Teachers of Mathematics (NCTM). The key concepts and skills in grade school include Number & Operations, Algebra, Geometry, Measurement, Data Analysis & Probability, Problem Solving, and Reasoning. Different key concepts involve the use of different knowledge and abilities. We have also annotated the reasoning steps and process for each MWP, categorizing them into questions requiring one reasoning step, two reasoning steps, and three or more reasoning steps.\nBased on the curated benchmark, we have conducted a thorough analysis of the evaluation process and methods. Evaluations in mathematics have always emphasized the accuracy of results. However, we have observed factors that greatly influence the model’s results, thus affecting our understanding of its capabilities:\nThe model is sensitive to the prompts used during the evaluation, and the results vary accordingly.\nThe methods of evaluation can also affect the model’s results. We have compared the model’s performance in selecting the final answers from the options of multi-choice questions, demonstrating that the form of evaluation tasks and options can influence the model’s results to a certain extent.\nThe length of the LLM-generated answers, to some degree, reflects the model’s “confidence” when handling questions.\nThe main contributions of our work are as follows:\nWe propose a fine-grained elementary school MWPs benchmark for Chinese LLMs, which can assess the mathematical capabilities of LLMs from three aspects: accuracy of understanding abstract mathematical concepts, accuracy of reasoning, and overall accuracy.\nWe conduct an in-depth analysis of the contamination in our dataset, enabling researchers of LLMs to conduct a credibility analysis on the evaluation results.\nWe evaluate GPT-4, GPT-3.5-Turbo, and 8 Chinese LLMs, revealing their mathematical reasoning capabilities, and provide detailed evaluation results in various aspects."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Traditional MWP datasets like AddSub (Hosseini et al., 2014  ###reference_b5###) and MultiArith (Roy and Roth, 2015  ###reference_b18###) are integrated into a MWP repository. Other similar datasets include SingleEq (Koncel-Kedziorski et al., 2015  ###reference_b9###), AQUA (Ling et al., 2017  ###reference_b12###) and AsDiv (Miao et al., 2020  ###reference_b13###). GSM8K (Cobbe et al., 2021  ###reference_b2###) and SVAMP (Patel et al., 2021  ###reference_b17###) take advantage of detailed annotations and have prevailed in recent evaluations.\nOur work is most inspired by the MATH (Hendrycks et al., 2021  ###reference_b4###) dataset. MATH collects problems from American high school mathematics competitions and categorizes problems into seven subjects. These subjects are Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra and Precalculus. However, these problems are very challenging, even when humans answer them, the accuracy rate is only 40%. Considering that many LLMs are still in their early versions, overly difficult problems may have limited significance for testing these models.\nCMATH proposed by (Wei et al., 2023  ###reference_b21###) and K6 proposed by (Yang et al., 2023  ###reference_b24###) are the two datasets that are relatively similar to ours developed concurrently. All these datasets focus on math word problems of elementary school, and organize instances by grade level. CMATH contains 1.7K problems collected from workbooks and exams on the Internet. K6 is composed of 600 problems collected from an educational institution. However, neither of the two datasets have been publicly released, precluding us from conducting an empirical comparison to them.\nApe210K proposed in (Zhao et al., 2020  ###reference_b25###) is a slightly earlier dataset. It contains 210K enormous Chinese math word problems from elementary school. Test sets alone in Ape210K contain as many as 5,000 problems. However, the test sets do not provide annotations related to LLMs.\nAn overview of the related MWP datasets is shown in Table 1  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Data Collection and Annotation",
            "text": "We create our dataset by collecting a diverse set of questions. We collect as many questions as possible from textbooks, workbooks and the Internet, from which high-quality questions are selected.\nAfter collecting these questions, we conduct automatical preprocessing on the collected data, which includes removing questions that are not math word problems, discarding questions with fewer than 10 Chinese characters, and retaining only questions that contain definite answers. Additionally, any questions that require reference to images are also discarded.\nOn the preprocessed data, we further perform manual annotation and processing: MWP categorization, question standardization, reasoning step and answer standardization and multiple-choice question transformation. We elaborate these data curation steps in the following subsections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   MWP Categorization",
            "text": "We categorize the collected questions into 17 types, each corresponding to a key or basic concept111https://www.nctm.org/Standards-and-Positions/Principles-and-Standards/Principles,-Standards,-and-Expectations/ inherent in the MWPs. We introduce these key concepts encompassed in our dataset, along with their corresponding categories as follows.\nNumber & Operations: This mathematical concept requires an understanding of numbers, ways of representing numbers, relationships among numbers, and number systems. It also necessitates an understanding of the meanings of operations and the ability to compute with these operations. This concept includes 7 MWP categories: Percents, Decimals, Fractions, Factors & Multiples, Counting, Proportions and Mixed Operations.\nMeasurement: Measurement requires an understanding of the measurable attributes of objects and the units, systems, and processes of measurement. It corresponds to two MWP categories, namely Spatial Sense and Time.\nData Analysis & Probability: Data analysis and probability requires one to select and use appropriate statistical methods to analyze data and to apply basic concepts of probability. This concept is related to Central Tendency and Probability.\nAlgebra: This concept involves understanding patterns, relations and functions. The MWP categories of this concept include Equations and Patterns.\nGeometry: Geometry is to analyze the characteristics and properties of two- and three-dimensional geometric shapes, specify locations and describe spatial relationships using coordinate geometry and other representational systems. It contains two MWP categories: Two & Three Dimensional Geometry (Basic Geometry) and Analytic Geometry.\nOthers: We also categorized two types of special MWPs. Problem 1: simple optimization problems. Problem 2: tree planting problems that involve the relationship between points and segments."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Question Standardization",
            "text": "Many questions in the selected data contain multiple queries. We normalize these questions so that each question contains only a single query. Ambiguous queries are also rephrased to enable the model to generate a unique answer."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Mathematical Reasoning and Answer Standardization",
            "text": "The process of answering MWPs is manually conducted, and the ground-truth answers are manually double checked by humans. We ask annotators to provide the steps in answering each MWP. Each step should be atomic and indivisible. For calculations that use a fixed solution formula, e.g., computing the area of a circle, we consider them as single-step MWPs.The reasons for annotating the number of required mathematical reasoning steps are two-fold:\n1. The number of required reasoning steps can be treated as a proxy to the difficulty level of MWPs. Intuitively, MWPs that require multiple steps to solve are more difficult than those solved in a single step. The progression from one step to the next also represents the reasoning process. Therefore, we categorize the difficulty of MWPs in our dataset into three levels. MWPs that can be solved in a single step are level-1 MWPs; MWPs that require two steps to solve are level-2 MWPs; level-3 MWPs are those that require three or more steps to solve.\n2. Presenting the number of reasoning steps facilitates reviewing and analyzing the collected data, thereby ensuring data quality."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Multiple-Choice Question Transformation",
            "text": "The original MWPs are accompanied with their single ground-truth answers. To facilitate automatic evaluation, we also transform them into multiple-choice question forms by manually providing additional contrastive answer options, similar to the AQUA dataset (Ling et al., 2017  ###reference_b12###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Data Statistics and Analysis",
            "text": "We provide data statistics and analysis on contamination of our dataset in this section."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Data Statistics",
            "text": "The overall data statistics are displayed in Table 2  ###reference_###. All 1,584 questions are categorized into five major mathematical concepts and two classic types of MWPs. Each type contains at least 60 questions, and each difficulty level contains at least 20 questions. Questions were grouped using simple keyword matching to determine their similarity based on common patterns and frequent terms identified in the problem statements."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Analysis on Contamination",
            "text": "###figure_2### FineMath serves as a comprehensive benchmark, encompassing a diverse range of math word problems at the Chinese elementary school level. It is specifically designed to assess the mathematical reasoning capabilities of Chinese large language models. However, these language models are typically trained on a huge amount of data derived from multiple sources, including web pages, books, codes and so on. This raises the potential risk of test data contamination, as some test examples from FineMath may unintentionally be included in the training data of these language models. Test data contamination can lead to an overestimation of a model’s performance, potentially resulting in misleading conclusions regarding the model’s generalization capabilities. Consequently, investigating contamination and its impact on model performance for FineMath is of paramount importance.\nApe210K (Zhao et al., 2020  ###reference_b25###) is a publicly available large-scale Chinese math word problem dataset, which has been splited into training, validation, and test sets. It is commonly utilized as a training dataset for mathematical problem-solving models (Hu and Jiang, 2022  ###reference_b6###; Wu et al., 2021  ###reference_b22###; Liang et al., 2023  ###reference_b10###; Huang et al., 2021  ###reference_b7###; Xiong et al., 2022  ###reference_b23###; Huang et al., 2023  ###reference_b8###; Yang et al., 2023  ###reference_b24###; Liang et al., 2022  ###reference_b11###). To determine potential contamination from Ape210K in FineMath, we adopt the identical methodology leveraged in GPT-3 (Brown et al., 2020  ###reference_b1###) to compute the n-gram overlap between Ape210K and FineMath. In this approach, a test example in FineMath is considered as an overlapped example with Ape210k if any n-gram from this test example also appears in Ape210k. Specifically, we insert white spaces around any Chinese, Japanese, and Korean (CJK) characters, as well as between punctuation marks and words. Subsequently, we tokenize the text based on these white spaces. It is important to note that we disregard letter case when computing n-grams.\nTo perform a rigorous quantitative assessment of contamination, we define the overlap rate as the fraction of instances within FineMath that exhibit such overlap. Furthermore, for the purposes of computing overlap, we set the value of n to 13. The overlap rate between FineMath and the training sets of Ape210k is depicted in Figure 2  ###reference_###. It suggests that the overlap rates of some question types are significantly higher than others, such as Basic Geometry and Proportions. To gain deeper insights into the impact of these overlapped examples on model performance, we partition the test examples into two datasets: a contaminated dataset composed of the overlapped examples, and a clean dataset whose test examples exhibit no overlap with the Ape210k training set. Subsequently, we examine the performance of the model on each of these datasets separately. We select GPT-4 and MathGLM-10B for analysis since GPT-4 is widely recognized as the most advanced LLM currently available and MathGLM-10B has been trained on the Ape210k training set which overlaps with some test examples of FineMath. The experimental results are presentend in Table 3  ###reference_###. Notably, MathGLM-10B performs significantly better on the contaminated dataset compared to the clean dataset. In contrast, GPT-4 exhibits comparable performance on both datasets. This suggests that MathGLM-10B may be overfitting to the overlapped examples and that contamination can inflate a model’s performance. Consequently, to ensure a fair comparison between models and to draw accurate conclusions from the FineMath benchmark, we recommend filtering out overlapped examples between the training set and the FineMath benchmark."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": "We conducted experiments on the proposed FineMath to evaluate a series of LLMs, assessing the mathematical reasoning capabilities of them."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Evaluated LLMs",
            "text": "We assessed three classes of LLMs: GPT-4 and GPT-3.5-Turbo developed by OpenAI; LLMs developed for Chinese; and LLMs finetuned with Chinese mathematics data. Specific information can be found in Table 4  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Prompts",
            "text": "All experiments were conducted under the zero-shot. We tried several prompts for evaluation and analysis, which are shown in Table 5  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Main Results",
            "text": "###figure_3### ###figure_4### The overall accuracy results of assessed LLMs are visualized in Figure 3  ###reference_###. GPT-4 and GPT-3.5-turbo perform outstandingly, with their accuracies reaching as high as 73% and 62%, respectively.\nAmong the evaluated Chinese LLMs, MathGLM-10B, MathGLM-335M, ChatGLM2-6B and Baichuan2-7B-Chat obtain an accuracy of > 40%. Qwen-7B-Chat and InternLM-Chat-7B are at a slightly below-average level. However, both Baichuan-7B and Moss-SFT-16B perform poorly on our dataset, with an accuracy of < 10%. Upon examining the responses generated by these two models, we find that their answers often stray from the MWPs, generating a lot of irrelevant content or repeatedly producing the same questions.\nBy considering both model accuracy and the detailed information provided in Table 4  ###reference_###, we deduce that the lower accuracy of Moss-SFT-16B is due to an insufficient amount of training data. The performance of Baichuan-7B is hampered because it has not undergone RLHF fine-tuning, which prevents the model from fully understanding the question. In contrast, the accuracy of Baichuan2-7B-Chat, which has been fine-tuned, has significantly improved. In summary, RLHF fine-tuning, having model parameters exceeding 6 billion, and training data reaching the trillion level are all crucial for training an LLM with problem-solving and reasoning capabilities."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Results across 17 MWP Categories",
            "text": "Results across the 17 MWP categories are displayed in Table 6  ###reference_###. It is evident that the MWP types “Counting” and “Problem1” are more challenging than other MWP categories according to the results. This could be due to the complexities involved in the counting of Chinese numerals and their conversion to Arabic numerals, and the common sense issues encountered in the optimization problem for “Problem1”. All models demonstrate better performance on “Mixed Operation” than on other categories. We also observe that the performance of different models vary significantly.\nGPT-4 outperforms all other models. It achieves an accuracy below 40% on only two MWP categories, surpasses 80% on three MWP categories, exceeds 70% in six MWP categories, and goes beyond 60% in five MWP categories. GPT-4 scores the highest, 89%, on the ’Mixed Operation’ MWPs, while it scores the lowest, 38%, on the ’Counting’ MWPs. Notably, GPT-4 excels in Probability and Analytic Geometry, outperforming all other models with an improvement of over 25%. GPT-3.5-Turbo’s performance on different MWP categories is similar to that of GPT-4, but there is a significant difference (over 20%) on Probability, Basic Geometry, and Analytic Geometry.\nMathGLM-10B and ChatGLM2-6B exhibit similar performance trends across most MWP categories. However, significant differences are observed on Fractions, Counting, Spatial Sense, Basic Geometry, Proportions, Problem1, and Problem2. Due to contamination, MathGLM-10B surpasses GPT-4 on Basic Geometry, Problem 1 and Problem 2.\nBaichuan2-7B-Chat, Qwen-7B-Chat and InternLM-Chat-7B are still at a medium or slightly below-average level. Both Moss-SFT-16B and Baichuan-7B perform poorly across all MWP categories, failing to achieve an accuracy of 20%.\n###table_1### 学校买来120本课外读物，其中的40%分给高年级，剩下的课外读物按5:3分给中年级和低年级，低年级分到多少本?\nThe school bought 120 supplementary books, 40% of which were given to the upper grades. The remaining books were divided between the middle and early grades in a ratio of 5:3. How many books did the early grades receive?\nGPT-4 Response: 高年级分到的课外读物为120*40%=48本;剩下的课外读物为120-48=72本;低年级分到的课外读物为72*3/(5+3)=27本。答：低年级分到27本。 Results: \\usym2714\nGPT-4 Response: The number of supplementary books allocated to the upper grades is 120*40%=48 books. The remaining books total 120-48=72 books. The supplementary books allocated to the early grades are calculated as 72*3/(5+3)=27 books. Answer: The early grades are allocated 27 books.\nGPT-4 Response: 36本 Results: \\usym2717\nGPT-4 Response: 36 books\nGPT-4 Response: 36本。首先，学校将120本书的40%分给高年级，即120*40%=48本。剩下的书就是120-48=72本。然后，这72本书按照5:3的比例分给中年级和低年级。这个比例的总和是5+3=8，所以低年级分到的书就是72*(3/8)=27本。Results: \\usym2717\nGPT-4 Response: 36 books. Firstly, the school allocates 40% of the 120 books to the upper grades. This totals to 120*40%=48 books. The remaining books amount to 120-48 = 72 books. Then, these 72 books are divided between the middle and early grades at a ratio of 5:3. The total of this ratio is 5+3=8. Therefore, the number of books allocated to the early grades is 72*(3/8) =27 books.\nGPT-4 Response: 36本 Results: \\usym2717\nGPT-4 Response: 36 books\nGPT-4 Response: C:27\nGPT-4 Response: C:27\nResults: \\usym2714\nGPT-4 Response: D:36\nGPT-4 Response: D:36\nResults: \\usym2717"
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   Results in terms of the Number of Mathematical Reasoning Steps",
            "text": "We further conducted experiments to evaluate LLMs reasoning ability in terms of the number of mathematical reasoning steps. Results are shown in Figure 4  ###reference_###. We observe that the performance of LLMs decreases as the number of reasoning steps increases. GPT-4 maintains an accuracy of over 60% at all difficulty levels, reaching as high as 82% on MWPs that require only one step of reasoning. The accuracy of GPT-3.5-Turbo is, on average, 10% lower than that of GPT-4. While ChatGLM2-6B, Baichuan2-7B-Chat and Qwen-7B-Chat outperform MathGLM-335M and MathGLM-10B on Level-1 MWPs, its accuracy falls below those of MathGLM-335M and MathGLM-10B on Level-2/3 MWPs. Similar to their performance across MWP categories, Moss-SFT-16B and Baichuan-7B show a significant difference in performance at all difficulty levels compared to the other models.\nThe accuracy difference between Qwen-7B-Chat and InternLM-Chat-7B on different reasoning steps is quite substantial, exceeding 30%. In the case of Qwen-7B-Chat, the accuracy on problems requiring only one-step reasoning is 62%, but this figure drops to just 21% for problems requiring three or more reasoning steps. This phenomenon suggests that the model may need more training in terms of inference."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Analysis",
            "text": "Unlike other studies that only evaluate accuracy, we have further analyzed factors in the evaluation process. These overlooked factors greatly affect the evaluation results and our understanding of the true mathematical reasoning capabilities of LLMs."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Prompts Really Does Matter",
            "text": "During the evaluation, instructions are generally used to guide the assessed model to produce answers. For instance, we might say, “Here is a math problem, please provide the answer to this question. Do not explain the reason.” Alternatively, we might provide an answer template such as “Question: … Answer:”. However, our experiments showed that even a single word like “Answer:” can significantly affect the model’s accuracy. We tested three prompt 0, prompt 1 and Prompt4 on GPT-4, results are shown in Table 7  ###reference_###.\nWe can see that the overall accuracy results with the three prompts are 73%, 59%, and 58%, respectively, with a gap reaching up to 15%.\nPrompt like “Answer:” appear to encourage the model to forego reasoning and directly provide the answer, which increases the likelihood of generating incorrect responses. An example is shown in Table 8  ###reference_###: Prompt 4+Question."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Evaluation Methods: Generation vs. Option Prediction",
            "text": "In our preliminary experiments, we have discovered that some newly developed LLMs do not follow instructions well, often generating large chunks of tokens unrelated to the answer. Therefore, we decide to transform our data into multiple-choice questions, for which the evaluated model can then select the correct answer option.\nComparison results are displayed in Table 9  ###reference_###. We can observe a significant difference in accuracy between option prediction (with multiple-choice questions) and direct answer generation, with a gap that can exceed 10%. Interestingly, restructuring the task in the form of multiple-choice questions seems to reduce the accuracy of high-performing models while increasing the accuracy of models that perform poorly. Upon examining instances, we have found that the answer option can act as another type of prompt influencing the model’s performance. For example in Table 8  ###reference_###: Prompt 0+Question (with MCQs) and Prompt 0+Question (with D replaced).\nExamples of GPT-4 outputs with different prompts and task forms are shown in Table 8  ###reference_###. We want to understand why GPT-4 would provide the incorrect answer “36” under Prompt 1. Therefore, we utilize Prompt 3 to have GPT-4 explain its reasoning for choosing “36”. Interestingly, GPT-4 mentions the correct number “27” in the explanation, but still provides the incorrect answer, “36”. Given the seeming importance of “36”, we replace one of the options in the multiple-choice question with “36”. The model, which initially selected the correct answer, abandoned it in favor of “36”. Further examples can be observed in the responses generated by other models. For instance, the reasoning process may be incorrect, yet the correct answer is ultimately chosen. The options provided to the model also seem to influence the model’s generation probability to a certain degree. Therefore, we recommend using generation, rather than option prediction, for a more accurate evaluation of LLMs."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   Comparison of Response Lengths",
            "text": "We conducted a statistical analysis on the length of the responses generated by the models. We discover two phenomena. First, models like GPT-4 and GPT-3.5-Turbo tend to generate responses that are closely centered around the question, with shorter text. This may demonstrate the characteristics of models with high accuracy. Second, the more reasoning steps an MWP requires, the longer the response tends to be. Please refer to Table 10  ###reference_### for more details.\nWe speculate that the model’s “confidence” in answering questions influences the length of its response. This tendency is also observed in some models that do not adhere strictly to instructions. For instance, even when instructed to provide only the answer without explaining, these models still generate logical reasoning for difficult MWPs."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "We present a fine-grained benchmark, FineMath, to comprehensively evaluate the mathematical capabilities of Chinese LLMs. We strive to evaluate as many LLMs as possible. We also conduct a contamination analysis, enabling researchers to examine whether the training data influences the evaluation results. Through testing various evaluation methods and processes, we demonstrate their potential to influence the results, underscoring the necessity for fair and effective evaluations to consider the interference caused by these two aspects."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ]
}