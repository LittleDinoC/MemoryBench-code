{
    "title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models",
    "abstract": "Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension.\nTo further utilize this capability, systems that automatically adjust the playback speed according to the user’s condition and the type of content to assist in more efficient comprehension of time-series content have been developed.\nHowever, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans.\nIn this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility.\nThe system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear.\nThis method can be used to produce fast but intelligible speech.\nIn the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. INTRODUCTION",
            "text": "With the widespread use of video distribution services, people are increasingly watching videos for various purposes, including information gathering, learning, and entertainment.\nHumans are capable of understanding naturally observed phenomena at rates faster than the original.\nTherefore, when watching video or listening to audio, we often increase the playback speed of the content to understand more content in a shorter amount of time.\nExisting research has shown that when watching videos for learning, under certain conditions, differences in video playback speed do not affect learning effectiveness and may even improve performance (Nagahama and\nMorita, 2017  ###reference_b23###; Lang\net al., 2020  ###reference_b17###; Murphy et al., 2022  ###reference_b22###).\nIt has also been reported that 30% to 80% of users prefer to watch dramas at high speed, although this varies from country to country (Duan and Chen, 2019  ###reference_b7###).\nThere are many advantages to listening to video and audio at high speeds.\nAs a result, many methods have been proposed to automatically adjust the playback speed according to the structure of the content and the condition of the user, to further enhance the human ability to listen at high speeds.\nThey are widely studied as video summarization (Apostolidis et al., 2021  ###reference_b2###) and audio summarization (Vartakavi\net al., 2021  ###reference_b30###).\nThere are two basic strategies to these methods.\nThe first is to scan the user’s intentions and behavior and leave only the parts that need to be viewed or adjust the playback speed in proportion to the user’s concentration level (Kurihara, 2011  ###reference_b16###; Kawamura et al., 2014  ###reference_b14###).\nThe second is to speed up unnecessary parts of the content (i.e., parts that do not contain speech) or slow down parts that contain speech (Kayukawa et al., 2018  ###reference_b15###; Higuchi\net al., 2017  ###reference_b10###; Song\net al., 2015  ###reference_b27###; Zhang\net al., 2020  ###reference_b33###).\nHowever, these methods do not explicitly model whether the resulting speech at different playback speeds is intelligible to humans, so it is unclear whether a wide range of speech types can be made intelligible to users.\nIn addition, these systems vary the playback speed for each large chunk of speech, which leaves room for adjustment regarding playback speed for smaller units of time.\nTherefore, we propose AIx Speed, a system that adjusts audiovisual output speed while maintaining intelligibility by measuring speech intelligibility after playback speed is increased.\nAs shown in Fig. 1  ###reference_###, this system flexibly optimizes the playback speed in a video at the phoneme level.\nBy utilizing the listening ability of a neural network–based speech recognition model, which is said to rival human performance (Xiong et al., 2016  ###reference_b32###), the system simultaneously maximizes the video playback speed and the speech recognition rate after changing the playback speed.\nIn this paper, the validity of using speech recognizers as a proxy for evaluating human listening performance was tested through the correlation of changes in human and speech listening performance when speed is increased.\nWe also whether speech where the playback speed is controlled at the phoneme level, as generated by the proposed method, or speech that is played at a constant speed and a high rate is easier for humans to listen to.\nThe results showed that the utterances generated by the proposed method were easier for humans to understand.\nFurthermore, the experimental results confirmed that the speech of non-native speakers can be transformed into speech that is easier to understand for native speakers by speeding up the speech with AIx Speed.\nIn summary, the proposed method not only supports the improvement of human speed–listening ability, but also improves the intelligibility of speech by generating speech with adjustable playback speeds that consider the balance between playback speed and speech intelligibility.\nThe contributions of this paper are summarized as follows.\nDemonstrates that speech recognizers can be a substitute for human listening performance assessment.\nProposes a method to increase playback speed while maintaining speech intelligibility at the phoneme level.\nImproves the intelligibility of speech for non-native speakers by optimizing speech rate at the phoneme level.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. RELATED WORK",
            "text": "There are two main methods for adjusting video playback speed to improve the time efficiency of video viewing.\nThe first is to remove unnecessary portions by focusing on the content, and the second is to retain the necessary portions based on user interaction.\nThe former removes portions that do not have audio, which is accomplished using systems such as CinemaGazer (Kurihara, 2011  ###reference_b16###), or portions of sports games that are not highlights of the game (Kawamura et al., 2014  ###reference_b14###).\nThe latter has been studied extensively, especially in the field of human–computer interaction (HCI), and adjusts the playback speed based on the user’s behavior.\nFor example, SmartPlayer (Cheng\net al., 2009  ###reference_b5###) learns the optimal playback speed based on a user’s past viewing history.\nThere are also technologies that allow a user to make a rough selection in advance of what AIx Speed considers important and then fast-forward the rest of the video (Kayukawa et al., 2018  ###reference_b15###; Higuchi\net al., 2017  ###reference_b10###).\nOthers monitor the user’s movements and adjust the playback speed according to the user’s level of concentration (Song\net al., 2015  ###reference_b27###) and comprehension (Zhang\net al., 2020  ###reference_b33###; Nishida\net al., 2022  ###reference_b24###).\nThese technologies have the advantage of tracking the optimal playback speed for each user, but they cannot reflect important factors such as the intelligibility of the conversation in the video and its changes in the playback speed.\nThese methods do not explicitly model whether the resulting speech is intelligible to humans when the playback speed is varied.\nTherefore, speeding up the playback of various types of audio and video while making the audio understandable to the user is still an open problem.\nIn addition, these systems vary the playback speed for large chunks of speech, which leaves room for adjustment regarding the playback speed for smaller units of time.\nIn this respect, the proposed system can adjust the playback speed in finer phoneme units and can also generate speech that is easier for the user to understand.\nThe differences between the proposed system and the existing systems are shown in Fig. 2  ###reference_###.\n###figure_2### ###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Pilot Survey",
            "text": "The purpose of this study is to automate the maximization of speed to the extent that speech is understandable.\nTo this end, we hypothesize that as the speed increases, the recognition performance of both humans and speech recognizers will decrease in the same way.\nIf this hypothesis is correct, we can evaluate how well a human can hear when playback speed is increased using a speech recognizer instead of a human.\nSeveral attempts have been made to evaluate human hearing with speech recognizers in this way. For example, it has been shown that the results of human mean opinion score (MOS) listening tests correlate with the results of the speech recognition-based MOS estimation method introduced in (Jiang and\nSchulzrinne, 2002  ###reference_b12###), and in (Fontan\net al., 2017  ###reference_b8###), the understanding and comprehension scores of a listener with simulated age-related hearing loss were highly correlated speech recognition-based system.\nA similar hypothesis has been used in speech learning support research to evaluate a learner’s speech ability based on speech recognition performance (Tejedor-GarcÃ­a et al., 2021  ###reference_b28###).\nIn other words, if a speech recognizer can recognize speech, it judges that the person speaks well.\nHowever, the relationship between the machine learning model and the ability to understand human speech when the playback speed is varied, which is the focus of this study, has not been evaluated. Therefore, we first investigated whether this hypothesis is true.\nThis study compared human listening performance and speech recognition performance for speech at 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, and 2.0x playback speeds.\nTo measure human listening performance, speech data of English sentences were prepared at each playback speed, and the subjects were asked to transcribe the data.\nThe target English sentences were selected from LibriSpeech (Panayotov et al., 2015  ###reference_b25###), a large English speech corpus created for the development and evaluation of automatic speech recognition systems.\nThis corpus contains over 1,000 hours of audiobook readings and transcriptions.\nThe participants were 140 English speakers who lived in the United States and had graduated from a US high school.\nAll participants were recruited from the Amazon Mechanical Turk and were compensated for their time.\nEach subject was given 15 English sentences that had been randomly sped up by 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, or 2.0x and asked to transcribe them.\nThe speech recognizer transcription data was collected by inputting 15 English sentences at each playback speed into a Wav2Vec2-based speech recognition model, similar to the human performance evaluation (Baevski\net al., 2020  ###reference_b3###).\nFigure 3  ###reference_### shows a graph of the change in listening performance of the human and machine learning models when the playback speed was changed.\nIn these figures, the horizontal axis is the playback speed, and the vertical axis is the recognition performance.\nRecognition performance was evaluated using character error rate (CER) and word error rate (WER), which are commonly used in speech recognition (the lower these values are, the better).\nThe WER was calculated as follows:\nand CER was calculated as\nFor both the human and machine learning models, listening performance decreased as playback speed increased from 1.0x.\nFor playback speeds greater than 1.0x, the correlation coefficient between the change in listening performance for the human and machine learning models was 0.9977.\nOn the other hand, when the playback speed was slowed down from 1.0x, the machine learning model showed a decrease in recognition performance, but the decline barely observable for humans.\nIn particular, while the performance of WER decreased slowly, the performance of CER showed almost no decrease.\nThis indicates that human recognition performance on a character-by-character basis does not drop nearly as much when listening to slowed speech.\nThus, when the playback speed increased, the recognition performance of the human and machine learning models decreased similarly, but they exhibited different behavior when the playback speed was decreased.\nHowever, since this study focuses on increasing the playback speed of speech, the difference in behavior when the playback speed is slower than 1.0x has no effect.\nTherefore, by taking advantage of the fact that listening performance decreases as playback speed is increased for humans and speech recognition models alike, we replaced human listening performance with speech recognition performance to develop the desired system."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. AIx Speed",
            "text": "AIx Speed increases the speed as much as possible, as long as the user can understand it.\nThis system allows users to watch videos in a time-efficient manner without having to adjust the playback speed for each video.\nIn addition, the system can automatically improve intelligibility by adjusting the speech speed to accommodate non-native speakers who are not proficient in the target language.\nThe working process of AIx Speed is illustrated in Fig. 4  ###reference_###.\nThe system first extracts the human voice from the target video.\nNext, it splits this voice into specified equal intervals.\nNext, using each segmented voice as input, the system calculates the optimal playback speed for each segment voice, taking into account the characteristics of the voice as a whole.\nFinally, the system changes each voice to the specified playback speed and combines them into a single voice.\nAt the same time, the combined voice is recognized by speech recognition to confirm that the resulting single voice is understandable.\nThis system consists of two mechanisms, as shown in Fig. 5  ###reference_###.\nOne is a playback speed adjuster (left), and the other is a speech recognizer (right).\nThe former is used to maximize the playback speed of the input speech, while the latter is used to evaluate how understandable the input speech is.\nBy training these two models simultaneously, it is possible to generate speech that plays back as fast as possible within the comprehension range.\nThe following subsections describe these two key features.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Playback speed adjuster",
            "text": "In this study, we use a simple linear interpolation method for adjusting playback speed that does not consider intelligibility to optimize the playback speed. The playback speed controller is divided into a feature extractor layer and a linear layer, as shown in Fig. 5  ###reference_### (left). They are trained by pre-training through self-supervised representation learning on unlabeled speech data and regression learning, which outputs the playback speed based on the features after the representation learning. The pre-training method for the feature extractor layer is similar to masked language modeling, as exemplified by bidirectional encoder representations from transformers (BERT) (Devlin et al., 2019  ###reference_b6###) in natural language processing, where a portion of the input is masked and the corresponding utterance features are estimated from the remaining input. In this way, the model can learn good-quality features of the target language to which the rate of utterance should be adapted. Typically, these self-supervised learners are used to tackle tasks such as speech recognition and speaker identification by pre-training and then fine-tuning with a small amount of label data. For example, in speech recognition, we have added a projection layer and a connectionist temporal classification (CTC) layer (Graves et al., 2006  ###reference_b9###) to the output of self-supervised learners, such as Wav2Vec2 and HuBERT (Hsu et al., 2021  ###reference_b11###), to enable transcription from speech waveforms. Similar to these methods, we pre-train on unlabeled speech data and then connect and train a linear layer that outputs rates. In general speech processing tasks, such as speech recognition (Malik et al., 2021  ###reference_b19###) and speaker identification (Bai and Zhang, 2021  ###reference_b4###; Kabir et al., 2021  ###reference_b13###), the difference from the class label is minimized as an error function. On the other hand, the goal of the playback speed adjuster is to maximize the speed. Therefore, we designed the following function whose value decreases as the speed increases. where  is the playback speed for each segment obtained using a simple linear interpolation method and a linear layer with audio as input as follows:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Speech recognizer",
            "text": "The speech recognizer transcribes the speech converted to the playback speed obtained by the playback speed adjuster (Fig. 5  ###reference_### (right)).\nWhen speech parts with different playback speeds are combined, noise is generated in the speech data and the sound quality is degraded.\nWe use voice separation technology (McFee et al., 2015  ###reference_b20###) to extract only the speaker’s voice and reduce the effect of noise.\nThe resulting speech is then fed into a speech recognizer for speech recognition. The speech recognition process consists of the extraction of acoustic features from the speech waveform, estimation of the classes of acoustic features for each frame, and generation of hypotheses from the sequence of class probabilities.\nSince the speech recognizer can partially share the neural network with the playback speed adjuster, the overall network size can be reduced.\nAs shown in Fig. 5  ###reference_###, the dotted speech feature extractor is shared.\nThe speech features obtained by this mechanism are used as input to generate text in the projection layer.\nIn this process, the speech recognizer is trained to minimize CTC loss, as in normal speech recognition.\nIn summary, the entire model is trained to minimize the following error function, which is a combination of this error function and the error function of the playback speed adjuster.\nHere,  is a hyperparameter that adjusts the importance of the playback speed calculation and speech recognition."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Prototype",
            "text": "As a prototype of AIx Speed, an application that optimizes audio playback speed using English as the target language has been implemented.\nThis section describes the implementation and usage of the application."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Implementation",
            "text": "Wav2Vec2 was used for the prototype’s shared utterance learning model (the utterance learning part shared by the speech recognizer and the playback speed adjuster).\nFor pre-training, LibriSpeech (Panayotov et al., 2015  ###reference_b25###) was used as the dataset, train-clean-360 as the training data, and dev-clean as the validation data.\nThe dataset consist of clean speech data in LibriSpeech and were partitioned using the data partitioning method proposed in LibriSpeech for training/validation.\nThe pre-training did not require the corresponding transcribed text, only the speech data.\nWe then trained a speech recognizer and a playback speed adjuster using two sets of speech data, including the transcribed text.\nOne was LibriSpeech’s train-clean-100, and the other was the English speech database read by Japanese students (UME-ERJ) (Minematsu et al., 2002  ###reference_b21###).\nThe latter is a dataset of English spoken by non-native Japanese speakers, with 202 speakers (100 males and 102 females) reading simple English sentences.\nThe playback speed adjuster and the speech recognizer were trained separately.\nFirst, the speech recognizer was trained using LibriSpeech and UME-ERJ, and then the playback speed adjuster was trained using the same data with fixed weights for the speech recognizer.\nAll speech files used for training were normalized to a sampling frequency of 16 kHz.\nThe Wav2Vec2 used the initial parameters implemented in PyTorch (Paszke\net al., 2019  ###reference_b26###), and the final layers of both the playback rate adjuster and the speech recognizer were -dimensional linear layers.\nThe hyperparameter  of the error function was set to , and training was performed for 5 epochs with a batch size of  using the AdamW (Loshchilov and\nHutter, 2019  ###reference_b18###) optimization algorithm."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Usage of the application",
            "text": "Examples of using AIx Speed are shown in Fig. 6  ###reference_###.\nThis is an example of the prototype applied to a video uploaded to YouTube.\nThe horizontal axis is the playback time, and the vertical axis represents the playback speed output by the model for each playback time. The first is an example of speeding up a dialogue, movie, or lecture.\nTwo speakers appear in the video, and the optimal playback speed can be set for each speaker. Of particular interest is that the two speakers in the video speak at different speeds, so the average playback speed for the two speakers is different.\nIt can also be seen that the playback speed increases drastically from the moment when the dialog between the two speakers ends and there is no more speech from the person.\nAlthough we did not intend to design this feature, we can see that our system, like conventional playback speed controllers, can speed up the playback speed in the parts where there is no speech.\nThe second example is speeding up the speech of non-native speakers to make it easier to understand.\nSince the speech of non-native speakers is often slower than that of native speakers, moderately speeding up the speech makes it easier to understand.\nThe change in playback speed shows that the overall playback speed is faster than the speech between native speakers in the first video. This indicates that the model can speed up the speech more because the non-native speakers’ speech is slower than that of the native speakers."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Evaluation",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Technical evaluation",
            "text": "To demonstrate that the proposed method can optimize playback speed while maintaining content understanding, we compared the CER and WER values at the AIx Speed–modified speech playback speed to those at a constant playback speed.\nWe compared the CER and WER with the speech playback speed modified by the AIx Speed to the CER and WER when the speech was simply played at a constant speed.\nThe speeds of the comparison targets were 1.0x, 1.5x and the average speed times the playback speed of AIx Speed.\nA standard Wav2Vec2 based speech recognition model, which was the speech recognizer used in our method, was used to compute the CER and WER for comparison.\nThe performance of the models is shown in Table. 1  ###reference_### (a) and 1  ###reference_### (b) for LibriSpeech and UME-ERJ, respectively.\nAIx Speed produces speech 1.30 times faster on average for LibriSpeech and 1.29 times faster on average for UME-ERJ.\nBoth results show that the playback speed optimized by AIx Speed has lower values for both CER and WER than the average constant speech speed at that playback speed.\nFrom these results, it can be said that the proposed model maximizes the playback speed while guaranteeing the recognition performance.\nIn addition, for UME-ERJ, the speech generated at AIx Speed shows better recognition performance in terms of WER than at 1.0x playback speed.\nTherefore, it is also suggested that the proposed method can be used to convert the speech of non-native speakers into more understandable speech."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. User evaluation",
            "text": "User experiments were conducted to confirm that the generated speech was understandable.\nThe quality of the speech generated by the proposed method was compared with that of the speech played at a constant speed, at the average playback speed of the speech.\nThe quality was evaluated using the mean opinion score, which is commonly used in speech synthesis research (van den Oord\net al., 2016  ###reference_b29###; Wang\net al., 2017  ###reference_b31###).\nThis measure rates speech quality on a five-point scale from 1 (poor) to 5 (excellent).\nParticipants were 50 US residents who used English on a daily basis.\n20 sentences were extracted from each of the LibriSpeech and UME-ERJ datasets, and half were converted to speech with the proposed speed-up, while the other half were converted to speech with a constant speed-up.\nParticipants were given a total of 40 sentences of audio and assisted to rate the quality of the audio.\nFigure 7  ###reference_### shows the quality of the generated speech at baseline and AIx Speed, LThe quality of LibreSpeech and UME-ERJ were 0.5 and 0.8 points higher at speeds generated by the proposed method, respectively.\n###figure_9###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Discussion",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "7.1. Evaluation results",
            "text": "The technical evaluation shows that the proposed method can produce speech that is easier to understand than that produced by simply increasing the playback speed in terms of speech recognition performance.\nThe user evaluation also shows that the proposed method can produce speech that is easier to understand for real users. These results show that the proposed method can produce speech with a high playback speed within a range that is easy for users to understand.\nThis allows users to watch videos at a reasonable speed without having to adjust the playback speed for each video.\nHowever, the improvement in MOS values by using the proposed method is by no means sufficient.\nIn the current model, the average conversion to a faster playback speed is about 1.3 times, but it is a future task to investigate whether it is possible to make this even faster.\nIn fact, many video playback services implement 1.5x and 2.0x playback speeds, and some people watch dramas and lectures at such speeds. Therefore, we expect that it will be possible to convert up to this speed and make the audio easy to understand.\nIn addition, since each user has a different preferred playback speed, personalizing the model so that it plays at the optimal playback speed for users is also a future issue."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "7.2. Listening comprehension and speech recognition",
            "text": "As discussed in the preliminary research chapter, it can be seen that there is a relationship between speech recognition performance and human transcription ability.\nThus, we expect to build automated systems for various tasks and evaluations by replacing human speech comprehension ability with machine learning models, as in this system.\nOn the other hand, speech recognition performance based on playback speed does not perfectly match human speech comprehension.\nIn other words, as the playback speed increases, the dictation performance decreases in both cases, but the performance values are not exactly the same.\nThus, we anticipate that by training speech recognition models to match these relationships as closely as possible, it will be possible to use them more generally as alternatives to humans. Distillation, a technique that learns to approximate an output that matches existing results, will be the technical key."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "7.3. Adjustment of non-native speakers’ speech",
            "text": "Several suggestions can be made as to how increasing the playback speed by the proposed system improves the intelligibility of speech for non-native speakers. One of them is that when non-native speakers read English manuscripts, they may find it easier to understand if they speak naturally (slower from a native speaker’s point of view) and then artificially speed up their speech, rather than forcing them to speak quickly like a native speaker. In fact, this study also began with the realization that it is easier to listen to a video of a non-native speaker speaking his or her native language when it is played at a high speed."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "This paper presents a system that applies a speech recognition model to automatically and flexibly adjust the playback speed of video and audio within the range of human comprehension.\nBy using this system, users can consume audiovisual content at optimal speeds without having to manually adjust the playback speed.\nExperiments have also confirmed that the system makes it easier for users to understand the speech of non-native speakers.\nIn the future, we expect the system to be used in a variety of applications, such as video distribution services and language learning tools."
        }
    ]
}