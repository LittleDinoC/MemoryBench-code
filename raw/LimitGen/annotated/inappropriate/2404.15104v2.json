{
    "title": "Identifying Fairness Issues in Automatically Generated Testing Content",
    "abstract": "Natural language generation tools are powerful and effective for generating content. However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases. We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure. Specifically, we review test content generated for a large-scale standardized English proficiency test with the goal of identifying content that only pertains to a certain subset of the test population as well as content that has the potential to be upsetting or distracting to some test takers. Issues like these could inadvertently impact a test taker’s score and thus should be avoided. This kind of content does not reflect the more commonly-acknowledged biases, making it challenging even for modern models that contain safeguards. We build a dataset of 601 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts. We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of 0.79 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data.111Code and dataset available at https://github.com/EducationalTestingService/fairness-detection.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large language models (LLMs) have become ubiquitous in the space of natural language generation (NLG) due to recent advances in model capability Minaee et al. (2024  ###reference_b27###). However, these improvements come with the potential for various negative societal impacts. These negative impacts include the generation of misinformation/propaganda, allocation harms of systems providing benefits only to certain groups of people, and representational harms revolving around bias and stereotyping. Natural language processing (NLP) models–including LLMs–are known to reflect and repeat harmful biases and stereotypes Hosseini et al. (2023  ###reference_b18###); Bender et al. (2021  ###reference_b4###); Hovy and Prabhumoye (2021  ###reference_b19###); Nadeem et al. (2021  ###reference_b28###), and research into how the community addresses the societal harms engendered by NLP technology is critical Wang et al. (2024  ###reference_b43###); Dev et al. (2022  ###reference_b8###); Blodgett et al. (2020  ###reference_b5###).\nMany of these types of bias in language generation are well-studied. Biases based on gender Nemani et al. (2024  ###reference_b29###); Devinney et al. (2022  ###reference_b9###); Strengers et al. (2020  ###reference_b35###); Wan et al. (2023  ###reference_b42###), race Das and Balke (2022  ###reference_b7###); Field et al. (2021  ###reference_b13###), nationality Venkit et al. (2023  ###reference_b38###), and disability Venkit et al. (2022  ###reference_b39###) have been identified in language models, and many modern LLMs incorporate deliberate safeguarding measures in an attempt to alleviate these issues OpenAI et al. (2023  ###reference_b30###); Anil et al. (2023  ###reference_b1###).\nIn the area of language assessment, there exists a tangential set of issues regarding fairness to test takers and score users Educational Testing Service (2022  ###reference_b11###). These issues are particularly dangerous when applied to language learning and assessment; tests with inherent biases have the potential to compromise the validity of the test. Therefore, content that is irrelevant to the skills and abilities the test is intended to measure should be avoided (Figure 1  ###reference_###). This includes content that could disadvantage anyone based on their culture, location, or experiences (e.g., focusing on barbeques on the 4th of July could disadvantage test-takers who are unfamiliar with U.S. culture); their emotions (e.g., health hazards and diseases can evoke negative emotional responses among some people); their worldviews (e.g., luxury cruises or designer clothing may make some people feel excluded); and other factors. We refer to these types of issues as fairness issues. Knowing how to better understand, detect, and mitigate bias related to fairness in NLG not only raises awareness of the issue but also enables researchers and developers to create more fair and inclusive NLP systems, evaluation metrics, and datasets in the language assessment space.\nOur goal is to build a system for identifying fairness-violating content in automatically generated texts. It is of course still necessary to have human review and revision of the content, but by adding a filtering process after generation and before manual review, we can significantly reduce the time taken for reviewing and the chance that fairness-related content is mistakenly allowed. To accomplish this goal, we explore four different approaches: fine-tuning, topic-based classification, few-shot prompting, and prompt-self correction.\nOur methods need to adapt to new contexts: our definition of fairness is operationally defined by the particular testing context, and may not apply to others, so the guidelines, prompts, and models may not apply generally to new contexts. For this reason, we assess our methods on two held-out test sets and analyze how our methods could be applied to new contexts. We release our resulting dataset, consisting of 620 samples, of which 19.4% contain fairness issues222Each sample we used was rejected for deployment in actual tests. Using rejected samples for our experiments allows us to release the dataset: accepted stimuli cannot be made public., to facilitate improvements in the fairness-detection community.\nOur contribution consists of the following:\nWe define a new fairness problem around issues faced in developing fair testing content.\nWe release a dataset of 601 samples for use in evaluating fairness detection methods.\nWe analyze the relative effectiveness of a variety of well-known classification techniques.\nWe provide a new mechanism for prompting self-correction, which yields significant improvements over other prompting strategies.\nWe start with data collection and analysis. We collect 620 samples over seven different types of content generated using LLM prompting. We annotate each sample and assess whether it contains a fairness issue, and if it does, whether that fairness issue pertains to knowledge, skill, or expertise or emotion (more on these categories and how they relate to fairness in Section 3  ###reference_###). We then use this dataset to experiment with a series of models for classifying fairness issues.\nWe show that fine-tuning and filtering by topic can be cheap and effective options, although prompting strategies with GPT4 tend to be more effective. Few-shot prompting along with self-correcting prompt strategies yield strong performance with relatively little data, and combining both yields the best results on our in-domain test set, with an F1 score of .773. Interestingly, using a shorter, more generic prompt combined with our self-correction method yields the best result on our out-of-domain test set, with an F1 score of .462."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Bias, fairness, and responsible AI has been at the forefront of education technology, with contemporary research focusing on automated scoring, writing assistance, and other nuances of applying NLP technology to this sensitive domain Mayfield et al. (2019  ###reference_b26###); Loukina et al. (2019  ###reference_b25###).\nBaffour et al. (2023  ###reference_b2###) find that assisted writing tools may exhibit moderate bias depending on the task, while Wambsganss et al. (2023  ###reference_b40###) found no significant gender bias difference in writing done with and without automated assistance. Wambsganss et al. (2022  ###reference_b41###) explore bias in educational tools for German peer review, and Kwako et al. (2023  ###reference_b21###, 2022  ###reference_b20###) propose novel methods for detecting bias in automated scoring algorithms.\nWe are specifically interested in applications to language generation, and there is also substantial work in using LLMs and other NLP technology to generate content for educational assessments Laverghetta Jr. and Licato (2023  ###reference_b22###); Gonzalez et al. (2023  ###reference_b14###); Heck and Meurers (2023  ###reference_b17###); Uto et al. (2023  ###reference_b37###); Tack et al. (2023  ###reference_b36###); Stowe et al. (2022  ###reference_b34###). However, this work largely fails to address bias and fairness issues in content generation. Our work is specifically focused on fairness issues in automatically generated language testing content.\nIn the context of language models, fairness and bias have emerged as critical concerns. Existing detection and mitigation tools generally diverge from our work: some are overly domain-specific like the focus on news articles in Raza et al. (2024  ###reference_b31###), while others are focused on assessing issues within the language models and datasets Bellamy et al. (2018  ###reference_b3###), rather than the outputs. Other works rely on retrospective metrics that assess a model’s fairness through aggregated predictions and subgroup analysis, and/or focus on classification rather than generation problems (Weerts et al., 2023  ###reference_b44###; Wiśniewski and Biecek, 2022  ###reference_b45###; Saleiro et al., 2019  ###reference_b33###). Although these tools enhance transparency and accountability for evaluating language model issues, they fundamentally differ from our bias detection approach tailored for evaluating generated text in real-time for a production environment."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Problem Motivation",
            "text": "In the language testing context, we face a unique set of fairness challenges in generating content. Specifically, fair testing requires content that does not contain irrelevant factors that negatively impact the assessment of a test taker.\nA primary concern is to ensure that the test content measures only what it is intended to measure. For English-language proficiency tests, this means that the test must measure only the skills and abilities needed to communicate effectively in English, and not other constructs such as background knowledge of specific jobs, events, or cultures.\nConsider the following question and an example of a response to that question:\nQuestion: You went to one of The Eras Tour shows, didn’t you?\nResponse: Yes–I love Taylor Swift!\nIf the task were to identify whether the response is an appropriate response to the question, even some native English speakers would likely get it wrong. This is because, in addition to needing to know features of English proficiency (in this case, the ability to infer gist, purpose, and basic context based on information stated in short spoken texts), one would also need to know about Taylor Swift and her concert tour. Thus, those familiar with Taylor Swift would have an unfair advantage in identifying the correct answer.\nEliminating the fairness issue for this type of question would result in the following revision:\nQuestion: You went to the music concert, didn’t you?\nResponse: Yes–it was a great performance!\nIn addition to avoiding testing outside knowledge, it is also important that language proficiency tests do not include content that is offensive or disturbing. For example, the following question and response refer to serious health issues, which have the potential to evoke deep negative emotions.\nQuestion: Did you hear that Luis has been hospitalized?\nResponse: No, but I knew he had a bad case of Covid-19.\nContent like this that could prompt strong feelings of anger, sadness, or anxiety should be avoided because it could derail a test taker’s concentration, resulting in lower performance on the test. How a test taker interacts with this test content may tell more about their ability to concentrate under emotional strain than about their ability to identify a response’s linguistic appropriateness. Eliminating this construct-irrelevant content helps to ensure that the test measures only the skills and abilities it is intended to measure."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "Our goal is to detect whether a generated stimulus contains an issue as a binary classification task. We build a dataset of texts labeled for potential fairness issues and explore potential detection methods.\nFor each stimulus, we aim to identify whether or not the stimulus contains fairness/bias issues, and if so, what type of issue is present. We start with a dataset of automatically generated stimuli. These stimuli were generated using prompting and different versions of GPT: the prompts were iteratively improved with the goal of improving the overall quality of the stimuli. During this process, each stimulus was evaluated by the test’s content development experts. For this work, the stimuli used were rejected by the reviewers, allowing us to provide them publicly and explore their use for fairness detection. These rejected stimuli typically have the relevant language and structure, so our goal is to identify which of those stimuli were rejected (at least in part) for fairness reasons. We employ content development experts to annotate these samples, yielding a binary classification between non-fairness and fairness-related rejections.\nHowever, there are different ways for bias and fairness considerations to impact individual stimuli. To better understand and mitigate these issues, we separated them into two main categories:\nKnowledge, Skill, and Ability (KSA): content that contains construct-irrelevant information that may be unavailable to test takers in different environments or with different experiences and abilities. These include content with reference to specific skills, regionalisms, or unfamiliar contexts.\nEmotion: content in which language, scenarios, or images are likely to cause strong emotions that may interfere with the ability of some groups of test takers to respond. These include offensive, controversial, upsetting, or overly negative content.\nEach sample that is flagged for fairness is annotated for one or both of these categories. This allows further analysis to address these specific fairness categories and to better understand the impact of specific fairness issues.\nOur dataset is comprised of stimuli from seven different item and task types: a summary of the collected data is shown in Table 1  ###reference_###, with examples for each type in Appendix A  ###reference_###. These stimuli represent various structures, depending on the item/task type: Read a Text Aloud, Talks, and Text Completion stimuli are short text paragraphs, while Conversation stimuli involve turns between two or more speakers. Respond to Questions Using Information Provided and Respond to a Written Request task stimuli are structured content: the generation process creates text that is filled into a structured template; we use only the raw text.\nOverall we collect 601 samples, of which 19.6% exhibit evidence of fairness issues, with 9.5% reflecting KSA issues and 12.3% Emotion issues. We build a validation set of 48 samples reflecting a balance of the item and task types from the training types (Read a Text Aloud, Talks, Text Completion, and Respond to Questions Using Information Provided), and an equal-sized \"in-domain\" dataset from these stimuli is held separately for testing. These datasets contain an even number of positive and negative classes for fairness evaluations. As our goal is to be able to identify positive cases where fairness issues exist, we intend for our validation and test sets to have a substantial number of this class. We use the two remaining types (Conversations, Respond to a Written Request) as a separate \"out-of-domain\" test set to evaluate performance on unseen content.\nWe fine-tune standard pre-trained transformer models for sequence classification. We experiment with bert-base-cased, bert-large-cased Devlin et al. (2019  ###reference_b10###), roberta-base, Liu et al. (2019  ###reference_b23###) and deberta-base He et al. (2021  ###reference_b16###) models. We perform a hyperparameter search on our validation set for each model, finding that a learning rate of 2e-5 over 2-4 epochs generally performs best, and report results using the model with the best performance.\nWe observe that many samples are flagged for fairness due to the topic of the material: many topics contain content that violates our fairness guidelines directly, while others are simply more likely to include unacceptable content. Motivated by this, we explore topic detection as a method for identifying fairness issues.\nWe first identify topics found within the data. We use the topic modeling framework BERTopic Grootendorst (2022  ###reference_b15###) to extract topic representations from two sources of training data: (1) all samples from the training partition of our dataset and (2) our fairness guidelines. In this method, SentBERT Reimers and Gurevych (2019  ###reference_b32###) converts each training document into a dense vector representation which are then grouped by semantic similarity, creating clusters that represent different topics. For each of the two training sets, topic descriptions made up of the most important words in a cluster are generated for the clusters containing at least five supporting documents. We manually assess each topic description for themes that should be avoided based on their relation to known fairness issues and which topics are acceptable. Finally, for each unseen sample in test and validation datasets, we make predictions based on the single nearest topic cluster. If a sample falls within the boundaries of restricted topics, it is classified as a violation.\nResults for these methods are shown in Table 2  ###reference_###. The fine-tuned bert-based models perform fairly well, with F1 scores for bert-large-cased and roberta-base both around 0.65, and deberta-base showing exceptional performance with an F1 score of 0.77. The Topic-Based Filtering models are worse, with the data-based system yielding an F1 score of 0.58. In all cases, precision is much higher than recall; these models are conservative with predictions.\nWe follow few-shot prompting strategies that have been successful for LLM classification tasks Logan IV et al. (2022  ###reference_b24###); Brown et al. (2020  ###reference_b6###). For each of our base prompts, we provide a set number  of additional samples for both the positive and negative classes. Each of the samples contains the text of the stimulus along with a brief description of why it does or does not violate our fairness guidelines.\nWe leverage our annotated data using a prompt self-correction strategy (Figure 2  ###reference_###). We start with a base prompt and iterate through a batch of samples from our training data. For each sample, we insert text into the prompt, perform a pass through GPT4, and evaluate the resulting output. If it is correct, we continue. If not, we instruct the LLM to make adjustments to the prompt, either to disallow the sample (when the error is a false negative) or to allow it (when the error is a false positive), using the following prompts:\nPositive sample incorrectly classified as negative: The previous text was classified as not having any fairness issues, returning False. However, it did have an issue. Think about what topics or issues in the text might be unfair. Improve the original fairness instructions by adding or modifying restrictions so to account for this text.\nNegative sample incorrectly classified as positive: The previous text was classified as having a fairness issue, returning True. However, this item has no issue. Think about why this text was classified as unfair. Improve the original fairness instruction to allow this item by removing or revising restrictions.\nThis process is run up to  epochs, or stopped early if accuracy reaches 1 or the predictions are stable. We run over  batches of  samples randomly drawn from the training data, using the best-scoring prompt from the final batch for evaluation.444For an example of the process, see Appendix C  ###reference_###.\n###figure_2### Few-shot and self-correction are inherently complimentary, as the self-correction method returns an optimized prompt and few-shot learning reinforces it by providing in-domain examples. We combine them by concatenating additional few-shot learning samples to the self-correcting prompts.\nFor each of these improvements to prompting, we perform a hyperparameter search over the number of total training/few-shot samples and batch size. We experiment with the generic (short)  guideline (short)  and data-driven prompts.555Experiments with the longer guideline-based prompt were unsuccessful: the LLM invariably returns either a commentary on a single testing procedure or rewrites the prompt entirely to handle a single sample. We hypothesize the generic (short) and guideline (short) prompts should be able to benefit quickly from adaptive methods, while the data-driven prompt should be nearly optimized, as it is already based on observations from the data.\nWe use the validation set to tune the prompts and parameters to optimize the F1 score for each method. Note that for all prompting strategies, the temperature is set to zero; the prompts should only return True or False. Figure 3  ###reference_### shows the best results on the validation set. We explore each model’s effectiveness on unseen data in Section 5  ###reference_###.\n###figure_3### The base generic prompt fails, as the traditional bias and stereotyping issues are less likely to occur in our generated content, and the fairness issues we are concerned with are unlikely to be deemed as problematic out of context. Using a simplified version of our guidelines yields a 0.36 F1 score for identifying fairness issues. The data-driven based on observations in the training data yields much better results (0.70 F1). However, this may not extend well to novel cases, as the prompt is driven purely by our validation data.\nFew-shot learning displays some interesting properties: we see significant improvements across all three prompts, using three samples. (This yielded the best results across all validation runs). Even the minimal generic (short) prompt rises to over 0.60 F1 with minimal few-shot prompting.\nWe see small improvements over the baseline using prompt self-correction for all three prompts. For the data-driven prompt, results using self-correction equal those using few-shot learning. This aligns with previous work showing that language models themselves tend to write better prompts Fernando et al. (2023  ###reference_b12###): after only a few iterations of self-correction, the data-driven prompt surpasses the performance of a human-written prompt, even in cases where the human describes the dataset explicitly.\nCombining self-correction and few-shot learning yields improvements over base prompts and few-shot prompting alone. This approach yields the best results for all three prompts, with the best-performing model being the data-driven prompt with self-correction and few-shot learning. This may be due to overfitting, however: the prompt is written to reflect the data. To explore the efficacy of these methods on unseen data, we evaluate them on our two held-out test sets."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "###table_1### Our goal is to identify and mitigate these fairness issues in testing content. We build a dataset spanning seven different item or task types from standardized English language proficiency tests all generated using GPT4 OpenAI et al. (2023  ###reference_b30###). Item and task types can contain up to four components: the stimulus (main text the question is based on), stem (question asked about the stimulus), key (the correct answer to the stem), and distractors (a set of alternative answers that are incorrect).\nFairness issues are possible in all components, but we focus on only the stimuli, which are typically the longest, most feature-rich components of the test content, and thus are most likely to reflect fairness and bias issues. Issues in the stimuli can leak through to other components, making the stimulus the source of the majority of fairness issues.\nFor each stimulus, we aim to identify whether or not the stimulus contains fairness/bias issues, and if so, what type of issue is present. We start with a dataset of automatically generated stimuli. These stimuli were generated using prompting and different versions of GPT: the prompts were iteratively improved with the goal of improving the overall quality of the stimuli. During this process, each stimulus was evaluated by the test’s content development experts. For this work, the stimuli used were rejected by the reviewers, allowing us to provide them publicly and explore their use for fairness detection. These rejected stimuli typically have the relevant language and structure, so our goal is to identify which of those stimuli were rejected (at least in part) for fairness reasons. We employ content development experts to annotate these samples, yielding a binary classification between non-fairness and fairness-related rejections.\nHowever, there are different ways for bias and fairness considerations to impact individual stimuli. To better understand and mitigate these issues, we separated them into two main categories:\nKnowledge, Skill, and Ability (KSA): content that contains construct-irrelevant information that may be unavailable to test takers in different environments or with different experiences and abilities. These include content with reference to specific skills, regionalisms, or unfamiliar contexts.\nEmotion: content in which language, scenarios, or images are likely to cause strong emotions that may interfere with the ability of some groups of test takers to respond. These include offensive, controversial, upsetting, or overly negative content.\nEach sample that is flagged for fairness is annotated for one or both of these categories. This allows further analysis to address these specific fairness categories and to better understand the impact of specific fairness issues.\nOur dataset is comprised of stimuli from seven different item and task types: a summary of the collected data is shown in Table 1  ###reference_###  ###reference_###, with examples for each type in Appendix A  ###reference_###  ###reference_###. These stimuli represent various structures, depending on the item/task type: Read a Text Aloud, Talks, and Text Completion stimuli are short text paragraphs, while Conversation stimuli involve turns between two or more speakers. Respond to Questions Using Information Provided and Respond to a Written Request task stimuli are structured content: the generation process creates text that is filled into a structured template; we use only the raw text.\nOverall we collect 601 samples, of which 19.6% exhibit evidence of fairness issues, with 9.5% reflecting KSA issues and 12.3% Emotion issues. We build a validation set of 48 samples reflecting a balance of the item and task types from the training types (Read a Text Aloud, Talks, Text Completion, and Respond to Questions Using Information Provided), and an equal-sized \"in-domain\" dataset from these stimuli is held separately for testing. These datasets contain an even number of positive and negative classes for fairness evaluations. As our goal is to be able to identify positive cases where fairness issues exist, we intend for our validation and test sets to have a substantial number of this class. We use the two remaining types (Conversations, Respond to a Written Request) as a separate \"out-of-domain\" test set to evaluate performance on unseen content."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiments",
            "text": "We experiment with standard transformer-based classification baselines, topic detection, and a variety of GPT4-based prompting, including methods for automatic prompt-self correction. We describe each method below: each is tuned on the validation set, and we report the best model performance on that set. We then evaluate model performance on two separate test sets in Section 5  ###reference_###.\nWe use a simple rule-based system that flags content based on keyword presence. We experiment with bert-base-cased, bert-large-cased Devlin et al. (2019  ###reference_b10###  ###reference_b10###), roberta-base, Liu et al. (2019  ###reference_b23###  ###reference_b23###) and deberta-base He et al. (2021  ###reference_b16###  ###reference_b16###) models. We perform a hyperparameter search on our validation set for each model, finding that a learning rate of 2e-5 over 2-4 epochs generally performs best, and report results using the model with the best performance.\nWe observe that many samples are flagged for fairness due to the topic of the material: many topics contain content that violates our fairness guidelines directly, while others are simply more likely to include unacceptable content. Motivated by this, we explore topic detection as a method for identifying fairness issues.\nWe first identify topics found within the data. We use the topic modeling framework BERTopic Grootendorst (2022  ###reference_b15###  ###reference_b15###) to extract topic representations from two sources of training data: (1) all samples from the training partition of our dataset and (2) our fairness guidelines. In this method, SentBERT Reimers and Gurevych (2019  ###reference_b32###  ###reference_b32###) converts each training document into a dense vector representation which are then grouped by semantic similarity, creating clusters that represent different topics. For each of the two training sets, topic descriptions made up of the most important words in a cluster are generated for the clusters containing at least five supporting documents. We manually assess each topic description for themes that should be avoided based on their relation to known fairness issues and which topics are acceptable. Finally, for each unseen sample in test and validation datasets, we make predictions based on the single nearest topic cluster. If a sample falls within the boundaries of restricted topics, it is classified as a violation.\nResults for these methods are shown in Table 2  ###reference_###  ###reference_###. The fine-tuned bert-based models perform fairly well, with F1 scores for bert-large-cased and roberta-base both around 0.65, and deberta-base showing exceptional performance with an F1 score of 0.77. The Topic-Based Filtering models are worse, with the data-based system yielding an F1 score of 0.58. In all cases, precision is much higher than recall; these models are conservative with predictions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Prompting",
            "text": "We initially experiment with five different “base” prompts. We pair these with stimuli and use GPT4 to return “True” if the stimulus contains a fairness issue and “False” otherwise. These prompts represent different strategies333Prompts in Appendix B  ###reference_###.:\ngeneric (short)  53 tokens: Drawing from general knowledge of fairness and bias in LLMs, we write a generic prompt designed to combat attested LLM biases. This prompt is designed as a weak baseline. Our goal is to determine if a short, simple prompt can capture relevant issues, and whether or not it can be easily improved via self-correction or few-shot learning (Sections 4.3  ###reference_.SSS0.Px1### and 4.3  ###reference_.SSS0.Px2###)\ngeneric (long)  191 tokens: This is a longer, more detailed version of the above, containing nearly 200 tokens.\nguideline (short)  197 tokens: We craft a prompt based on guidelines for writing fair assessments. Using documentation that defines what constitutes fair assessment items and how to write them, we build a prompt capturing the important components of a fair question. The goal of this prompt is to determine whether human-written guidelines based on theoretical issues will accurately capture these issues in real data.\nguideline (long)  1081 tokens: We construct a “long” version of the previous guidelines by summarizing the entire fairness guidelines with the help of GPT4, asking for concise versions of relevant sections and combining them into a document that fully captures all the relevant aspects of the guidelines. This prompt is our longest, but still fully based on documentation. The goal of this prompt is to determine the efficacy of a longer, more comprehensive prompt.\ndata-driven  142 tokens: We craft a prompt based on annotations in our data. We identify which topics and language cause fairness issues and build the prompt to reflect how they might generalize to unseen item/task types and topics. This method is hypothesized to be the most effective, as it will address known issues in the data but may not extend to unseen data, as it is built specifically around the given training samples.\nThese prompts are run through GPT4 via the Azure interface OpenAI et al. (2023  ###reference_b30###). Each prompt was updated manually to correct obvious potential issues. Our goal here is not to overoptimize prompt writing, which could lead to overfitting the validation set, but rather to develop a generic prompt likely to be effective for both known fairness issues and novel issues possible in generated content.\nInitial experiments on the validation set revealed two insights: the generic (long) prompt performs similarly to the generic (short) in all cases, and the guideline (long) prompt is ineffective. We therefore focus our efforts on the three other prompts: generic (short)  guideline (short)  and data-driven.\nA primary issue in identifying the fairness issues we are concerned with is that they are only applicable in the context of certain testing environments. The guidelines and data we use are specific to a certain test; for other tests, other environments, and other contexts, different definitions of fairness and different alignments will be applicable. For this reason, the ability to adapt to new environments is critical in applying fairness constraints. We aim to build a system that can learn to adapt to new guidelines with minimal information.\nWe explore several methods to improve the robustness of prompting: few-shot prompting and self-correcting prompting techniques.\nWe follow few-shot prompting strategies that have been successful for LLM classification tasks Logan IV et al. (2022  ###reference_b24###  ###reference_b24###); Brown et al. (2020  ###reference_b6###  ###reference_b6###). For each of our base prompts, we provide a set number  of additional samples for both the positive and negative classes. Each of the samples contains the text of the stimulus along with a brief description of why it does or does not violate our fairness guidelines.\nWe leverage our annotated data using a prompt self-correction strategy (Figure 2  ###reference_###  ###reference_###). We start with a base prompt and iterate through a batch of samples from our training data. For each sample, we insert text into the prompt, perform a pass through GPT4, and evaluate the resulting output. If it is correct, we continue. If not, we instruct the LLM to make adjustments to the prompt, either to disallow the sample (when the error is a false negative) or to allow it (when the error is a false positive), using the following prompts:\nPositive sample incorrectly classified as negative: The previous text was classified as not having any fairness issues, returning False. However, it did have an issue. Think about what topics or issues in the text might be unfair. Improve the original fairness instructions by adding or modifying restrictions so to account for this text.\nNegative sample incorrectly classified as positive: The previous text was classified as having a fairness issue, returning True. However, this item has no issue. Think about why this text was classified as unfair. Improve the original fairness instruction to allow this item by removing or revising restrictions.\nThis process is run up to  epochs, or stopped early if accuracy reaches 1 or the predictions are stable. We run over  batches of  samples randomly drawn from the training data, using the best-scoring prompt from the final batch for evaluation.444For an example of the process, see Appendix C  ###reference_###  ###reference_###.\n###figure_4### Few-shot and self-correction are inherently complimentary, as the self-correction method returns an optimized prompt and few-shot learning reinforces it by providing in-domain examples. We combine them by concatenating additional few-shot learning samples to the self-correcting prompts.\nFor each of these improvements to prompting, we perform a hyperparameter search over the number of total training/few-shot samples and batch size. We experiment with the generic (short)  guideline (short)  and data-driven prompts.555Experiments with the longer guideline-based prompt were unsuccessful: the LLM invariably returns either a commentary on a single testing procedure or rewrites the prompt entirely to handle a single sample. We hypothesize the generic (short) and guideline (short) prompts should be able to benefit quickly from adaptive methods, while the data-driven prompt should be nearly optimized, as it is already based on observations from the data.\nWe use the validation set to tune the prompts and parameters to optimize the F1 score for each method. Note that for all prompting strategies, the temperature is set to zero; the prompts should only return True or False. Figure 3  ###reference_###  ###reference_### shows the best results on the validation set. We explore each model’s effectiveness on unseen data in Section 5  ###reference_###  ###reference_###.\n###figure_5### The base generic prompt fails, as the traditional bias and stereotyping issues are less likely to occur in our generated content, and the fairness issues we are concerned with are unlikely to be deemed as problematic out of context. Using a simplified version of our guidelines yields a 0.36 F1 score for identifying fairness issues. The data-driven based on observations in the training data yields much better results (0.70 F1). However, this may not extend well to novel cases, as the prompt is driven purely by our validation data.\nFew-shot learning displays some interesting properties: we see significant improvements across all three prompts, using three samples. (This yielded the best results across all validation runs). Even the minimal generic (short) prompt rises to over 0.60 F1 with minimal few-shot prompting.\nWe see small improvements over the baseline using prompt self-correction for all three prompts. For the data-driven prompt, results using self-correction equal those using few-shot learning. This aligns with previous work showing that language models themselves tend to write better prompts Fernando et al. (2023  ###reference_b12###  ###reference_b12###): after only a few iterations of self-correction, the data-driven prompt surpasses the performance of a human-written prompt, even in cases where the human describes the dataset explicitly.\nCombining self-correction and few-shot learning yields improvements over base prompts and few-shot prompting alone. This approach yields the best results for all three prompts, with the best-performing model being the data-driven prompt with self-correction and few-shot learning. This may be due to overfitting, however: the prompt is written to reflect the data. To explore the efficacy of these methods on unseen data, we evaluate them on our two held-out test sets."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Test Results",
            "text": "###figure_6### The previous experiments describe our attempts to identify the best-performing model for fairness classification on our validation set. Our goal is to develop a system that generalizes. For this, we evaluate the best-performing of the above model types on two held-out test sets:\nIn-domain: The 48 held-out samples drawn from the item/task types used for training.\nOut-of-domain: All samples (66) from the two held-out types: Conversations, Respond to a Written Request.\nFigure 4  ###reference_### shows the results on the test set. We evaluate the best-performing models of each type: fine-tuned transformer models, topic-based classification, base prompts, few-shot learning, self-correction, and combining few-shot and self-correction. We here note some key facts about model performance on our test set.\nCombining the data-driven prompt with self-correction and few-shot learning performs the best on the in-domain test. This shows this is the best approach if there is available data and expertise to support hand-crafting a data-driven prompt and running self-correction. On the out-of-domain data, the smaller initial prompts, generic (short) and guideline (short)  both outperform the data-driven prompt, perhaps due to their more generic nature: the data-driven prompt is too specific to this dataset, and understandably doesn’t generalize well. The self-correct+few-shot methodology performs the best in both cases: few-shot learning alone is better than self-correction alone, but the combination is typically the best.\nTraditional transformer-based classification performs remarkably well, especially in generalizing to the out-of-domain data. On the in-domain data, the best performing model deberta-base performs on par with the best base prompting model (0.58 compared to 0.60 F1 score), although this is a significant drop from the validation performance of 0.77, and performs quite poorly on out-of-domain data (0.20), indicating the model may overfit during training. On the out-of-domain data, roberta-base performs nearly as well as the best-performing overall model, just 0.04 behind the generic (short) prompt with self-correction and few-shot learning. If the goal is to quickly and cheaply build a system that is applicable to a wide variety of domains, there appears to be significant value in relying on these relatively small transformer-based classification models. The Topic (data) approach is also competitive on out-of-domain data, and does not even require model training; it lags only slightly behind the roberta-base model.\nWe found significant success in our proposed self-correction mechanism. While it typically does not outperform few-shot learning in isolation, the methods are naturally complementary, and the combination often yields the best-performing model. In examining the models’ self-corrections, we find that when asked to become more restrictive, the model tends to add sentences with new constraints, which nicely reflect the issue that was missed. When asked to become less restrictive, the model tends to add hedges to currently existing constraints.\nIn our experiments, we noted some issues. First, when run using too many samples or batches, the prompts tend to degrade: once the LLM makes an error and returns a prompt that doesn’t match the specifications, the run needs to be aborted. Even when the LLM sticks to the instructions, after many iterations the prompts become unwieldy and self-contradictory, and performance rapidly declines. We suggest using somewhere between six and 20 total samples for prompt self-correction; it is best to avoid making corrections indefinitely.\nWe here report F1 score as a balance between precision and recall. (For full scores, see Appendix E  ###reference_###.) Depending on the end use case, other metrics may be more appropriate. In our case, we advocate for always including humans in the evaluation process to ensure that only fair content is accepted. We then value both precision (as we do not want to excessively flag content for fairness issues, which could reduce diversity) and recall (as we do not want to let fairness issues through). Optimizing for recall seems reasonable, as it is likely more important to prevent fairness issues from being released, but it is critical to note that no system is perfect: even optimizing for recall, these fairness issues are likely to persist, and the models should not be used as failproof safeguards.\n###table_2### We evaluate performance on the test set for the two subcategories: Knowledge, Skill, and Ability (KSA) and Emotion (Table 3  ###reference_###). The deberta-base model performs exceptionally well on the KSA subcategory, capturing 75% of the fairness-flagged samples. Data-based methods (the data-driven prompts (0.59) and Topics from Data (0.59)) also perform well, likely due to the inclusion of negative emotional issues in the text. They perform much worse on KSA classification, although the data-driven prompts still yield the best performance (0.47): KSA-related issues are especially difficult as they generally involve only specific knowledge, and would not normally be considered fairness issues in other contexts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This work delivers four key contributions: an exploration of a novel fairness detection task, a dataset of 601 samples annotated for fairness issues, evaluation of a variety of classification models for this task, including fine-tuning, topic-based approaches, and prompting, and a novel prompting strategy, which, combined with few-shot learning, achieves state-of-the-art performance on the task.\nThis work is aimed to explore the space of fairness and bias issues in generated content, especially in the education context. We aim to highlight the difficulties of accounting for fairness, particularly in specific contexts unlikely to be accounted for by traditional model guardrails. As language model usage becomes more prevalent, the need for proper bias and fairness strategies from people training, deploying, and using these models is paramount."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Ethics",
            "text": "Content generation comes with inherent ethical concerns relating to fairness, bias, factuality, and sensitivity. Our work aims to mitigate these issues with regard to fairness, but it is important to consider potential issues that might arise from using LLMs and other NLP technology in generating assessment content. Models may introduce subtle biases against disadvantaged groups, or produce content that appears to be factual, but is not. These are critical failures that need to be accounted for.\nIn practice, the generation of assessment content requires human intervention: large language model generations are not at the point where they are immune to these negative impacts, and thus for any content that goes into production, a human with relevant expertise needs to evaluate it. The methods we propose support this human intervention, as they can remove obviously offensive content before the human review stage, or assist in human reviews by flagging potentially harmful content.\nWhile our dataset is unlikely to contain any content that is triggering (our framework of fairness is focused on more nuanced contexts), it must be noted that there is potential for it to be used maliciously; for example, by someone designing a system to adapt to and deceive a fairness detection system. In releasing this data, we hope to bring awareness to this issue and better understand the potential negative impacts. Primarily, we stress that any fairness detection system should not be used in isolation or without supervision as a catchall for potential issues."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work is limited largely by the type of content evaluation and the models used. We focus on a small number of item and task types that fall under very specific fairness constraints: the evaluation of the methods used specifically applies to these items under these constraints. This is apparent in the evaluation on the \"unseen\" item types in Section 5  ###reference_###. Applying these methods to new item and task types, even those annotated under the same fairness guidelines, yields significantly reduced results. This is evidence that the methods and models we designed work only for the specific contexts in which they are trained and developed.\nSimilarly, we explore a small space of models and approaches. We use relatively basic prompt strategies; there exist many other approaches and improvements that are likely to be valuable that we do not evaluate. The same is true of fine-tuned models and topic classification. We present relatively basic, well-known strategies to better understand the difficulty of our data, with the understanding that there are substantial improvements that could be applied."
        }
    ]
}