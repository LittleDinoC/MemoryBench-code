{
    "title": "AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations",
    "abstract": "Multi-modal learning in the audio-language domain has seen significant advancements in recent years. However, audio-language learning faces challenges due to limited and lower-quality data compared to image-language tasks. Existing audio-language datasets are notably smaller, and manual labeling is hindered by the need to listen to entire audio clips for accurate labeling.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, there has been a large amount of work in expanding comprehension of audio content by augmenting audio signal with information from another modality such as natural language. Tasks such as text-to-audio generation (TTA)  [21  ###reference_b21###]  [22  ###reference_b22###], text-guided audio editing  [32  ###reference_b32###], automatic audio-captioning  [25  ###reference_b25###]  [10  ###reference_b10###], and text-to-audio retrieval  [26  ###reference_b26###] [15  ###reference_b15###] have been proposed as objectives that improve model understanding of audio signal  [27  ###reference_b27###].\nA closely related field to audio-language learning is the vision-language learning, which comprises of tasks such as visual question-answering  [2  ###reference_b2###] and text-to-image generation  [29  ###reference_b29###]  [30  ###reference_b30###]. However, unlike audio-language, the vision-language learning benefits from the existence of large-scale, high quality datasets such as MSCOCO  [20  ###reference_b20###]. This makes it possible to pretrain large, powerful models to learn vision-language multimodal embeddings, which can then be applied for downstream tasks  [19  ###reference_b19###]  [18  ###reference_b18###]  [30  ###reference_b30###]. On the other hand, a significant challenge of audio-language learning is the lack of a large dataset consisting of high-quality audio-caption pairs. We note the distinction between captioned natural sound and captioned speech, the latter of which is more readily available. A commonly used dataset in audio-language is the AudioSet dataset  [9  ###reference_b9###], a collection of 2M YouTube videos of natural sounds with multi-label annotations. However, it is quite clear that labels alone are not sufficient to replace high-quality audio captions. Furthermore, datasets such as AudioCaps  [14  ###reference_b14###] that provide human-generated text captions for audio are generally not large enough to train a deep neural network, and only suitable for fine-tuning  [27  ###reference_b27###].\nIn this work, we introduce AudioSetMix, an audio-caption dataset generated through the application of audio transformations to clips from AudioSet. In addition, we use prompt engineering and large language model (LLM) to ensure that the transformed audio and its caption are aligned. Our dataset supports speed, pitch, volume, and duration augmentations for individual clips, as well as mixing and concatenation augmentations to combine multiple clips into one. Besides having high quality text descriptions for supervised audio-language tasks, our data augmentation scheme also supports a dataset for studying text-guided audio manipulations, as we have access to both the original and edited audio.\nTo demonstrate the effectiveness of our dataset, we train a state-of-the-art model from the 2022/2023 DCASE Challenge on Language-Based Audio Retrieval (Task 6B)  [34  ###reference_b34###] using AudioSetMix. We demonstrate that our model exhibits an improved understanding of common audio event modifiers such as volume or duration, as well as a better retrieval score overall compared to baseline models. Finally, we introduce a hard negative mining technique for the AudioSetMix data which further boosts model performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Improvements for Audio-Language Learning",
            "text": "In light of the dataset shortcomings for audio-language tasks, several workarounds for the data shortage have been proposed to train large models for audio-language tasks such as TTA. These approaches can be broadly classified into three categories.\nThe first approach is to use predefined text templates to form rough approximations of descriptions. The simplest text templates are proposed by [17  ###reference_b17###], in which audio labels are concatenated together in a random order. [17  ###reference_b17###] also proposes a data augmentation for the AudioSet dataset by mixing multiple audio samples together. The corresponding text caption is simply the concatenated labels of each sample. To allow for more complex relationships to be expressed in the captions, [36  ###reference_b36###] randomly inserts  tokens between labels in the hope that the model will learn to substitute in relational words. [12  ###reference_b12###] improves this approach by applying a set of common audio augmentations to AudioSet data, and associates each augmentation with a caption template.\nThe second approach trains models in a self-supervised manner by using a pretrained CLAP model to embed audio and text to a shared latent space [21  ###reference_b21###] [1  ###reference_b1###]. During training, when no captions are available, the CLAP model is used to perform a form of zero-shot audio captioning by substituting the text embedding with the audio embedding. This approach has been applied to both TTA and to music generation with good results. In particular, this approach has been adopted by AudioLDM [21  ###reference_b21###], which achieves state-of-the-art performance on both TTA task and other audio editing tasks such as style transfer and audio inpainting.\nThe third and most recent approach to overcome the description scarcity issue utilizes recent LLM models such as ChatGPT [5  ###reference_b5###] from OpenAI to generate text descriptions. This approach provides a few benefits. Firstly, text descriptions from LLM are much more varied when compared to text templates in terms of sentence structure and word choice. Our observations also indicate that LLM descriptions are also fairly realistic when compared to human descriptions. Secondly, the quality of captions from LLM can be readily improved using prompt engineering techniques such as few-shot or chain-of-thought prompting. Furthermore, using LLM allows us to increase the complexity of data augmentations without requiring an intractably large number of human created templates.\nThe first work to adopt the third approach is WavCaps [27  ###reference_b27###], a large audio-captioned dataset using ChatGPT. WavCaps combines several weakly-labelled datasets by prompting ChatGPT to create a natural language caption, given a list of sound event labels. However, WavCaps does not incorporate any form of data augmentation in its captions, thus reducing the diversity and complexity of captions. The lack of data augmentation also hinders the training of text-guided audio editing models, as editing keywords do not appear often in AudioSet labels. Finally, WavCaps does not explore the idea of generating hard negative examples, such as including two audio-caption pairs that differ by only audio event modifiers. In the next section, we introduce AudioSetMix, an audio-caption dataset which addresses these concerns.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "AudioSetMix Dataset",
            "text": "In this section, we describe the process of creating the AudioSetMix dataset. Firstly, we introduce the data source and its characteristics. Secondly we describe a four-stage pipeline for generating weak captions, including data preprocessing, audio clip augmentation, LLM-based caption generation, and postprocessing. Finally, we provide an analysis of AudioSetMix and compare it with existing audio-language datasets."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "TS-AudioSet",
            "text": "The original AudioSet dataset consists of approximately 2 millions 10 second audio clips with human annotations. However, AudioSet labels are considered weak as an audio clip in its entirety may not correspond to its label due to interference such as background noise  [11  ###reference_b11###]. This imperfection is undesirable when training text-to-audio models as they may learn to associate labels with silence or white noise that dominate audio clips. For this reason, previous works  [6  ###reference_b6###] restrict themselves to particular sound classes such as drumming. To resolve this issue,  [11  ###reference_b11###] released Temporally-Strong AudioSet (TS-AudioSet), in which each audio clip has precise start and end timestamps and are labeled by humans. We use the clips and labels from TS-AudioSet as the basis for forming AudioSetMix."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Generation",
            "text": "We propose the following pipeline for generating clip captions using a LLM. We will first discuss our audio clip augmentations, then move into the details of generating audio-aligned captions that are specific to each augmentation method applied to the audio clips. Figure 1  ###reference_### illustrates the full data processing pipeline."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Audio Data Preprocessing",
            "text": "We first apply preprocessing to remove noisy or undesirable data. This largely consists of filtering operations to audio clips and captions, referred to as duration-based filtering and class-based filtering respectively. Due to the noisiness of the raw audio-caption pairs, these filtering operations are needed to ensure the cleanliness of the data as the starting point for the subsequent enhancement and augmentation. For duration-based filtering, we remove clips with a duration of less than two seconds. This is because shorter clips tend to lack meaningful content and require long amounts of padding during training  [27  ###reference_b27###]. Furthermore, short clips may be even further reduced if duration augmentations are applied in later stages. For class-based filtering, we remove clips with labels such as “background/environment” and “unknown” that lack semantically significant contents."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Audio Data Augmentation",
            "text": "Before discussing in details about the audio data enrichment, Table 1  ###reference_### lists the math symbols used in this sub section to guide the reading.\nTo generate a new audio-caption pair, we first select  audio clips  uniformly at random from TS-AudioSet.  is also selected uniformly from range , as large  cause the generated audio-caption pair to be unrealistic and overly complex.\nFurthermore, we define a set of audio clip transformations . The four types of transformations are volume, pitch, speed and duration, with implementation details of each transformation given below. Each transformation is also associated with a set of keywords which describe the transformation. For instance, the “volume” transformation is associated with keyword “loud”.\nVolume: Given clip C, we randomly apply either amplification or attenuation with uniformly random magnitude in range  dB to the entire clip.\nPitch: Given clip C, we randomly shift the pitch by a uniformly random number of octaves between .\nSpeed: Given clip C, we randomly stretch C in time domain by uniformly random rate in . For this transformation, we use the TimeStretch() function from torchaudio, which preserves the pitch rate when modifying speed.\nDuration: Given clip C, we randomly reduce the length of C by half.\nFor each pair of audio clip  and transformation , we use a Bernoulli random variable with parameter  to determine if  should be applied to .\nIn addition, we define 2 transformations to combine two clips together:\nConcatenation: Given clips  and , we concatenate  and  with 0.5 second of silence separating the clips.\nMix: Given clips  and , we randomly select a temporal offset for combining  and . We then draw a signal-to-noise ratio (SNR) between  and mix the two clips together.\nOnce individual augmentations are applied to each clip, we combine the clips to form the final waveform. Let  be the augmented clips. For every , we combine clips  using the mix transformation with probability , or the concatenation transformation with probability . In addition,  and  are said to occur simultaneously if they are mixed together. Otherwise,  will occur before . We set  and  in our pipeline implementation. The final clip is then padded/truncated such that the length of the clip is 10 seconds."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Caption Generation",
            "text": "Instead of using simple concatenation or other techniques in previous works [17  ###reference_b17###] [36  ###reference_b36###] to assemble the captions corresponding to the audio clip, we use LLM to generate natural language description of the new audio clips based on the augmentations applied in the previous step. In this section, we describe how we construct the prompts in order to generate text captions.\nTo query the LLM, we introduce a JSON-formatted dictionary to describe each clip. The dictionary for clip  contains the original list of labels from TS-AudioSet for , the keywords for the transformations applied to , and an  value. We assign the first clip to have an . Furthermore,  and  will have the same  value if they occur simultaneously, i.e. occurred as the consequence of ’mixing’, otherwise, their s will incrementally differ by 1. The final query to LLM is a list of the JSON dictionaries, as well as a prompt instructing LLM to generate a short, realistic description based on the dictionary values.\nThe prompt for the LLM is illustrated in the Listing 1  ###reference_###, and the detailed construction of the full queries to LLM and sample responses are shown in Table 2  ###reference_###.\nWe select GPT 3.5 as our LLM of choice for the implementation due to cost and availability of inference resources. However, we note that similar techniques for creating audio-caption pairs can be applied to other LLMs on the market."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Data Postprocessing",
            "text": "We apply additional postprocessing steps to refine the quality of generated captions. We filter out captions by setting a minimum/maximum threshold for word count. This ensures that short captions that lack information are not included, while excluding excessively wordy captions that tend to include unnecessary details or reflect poor grammar or sentence structure. Furthermore, we manually inspected a randomly selected subset of captions to ensure quality."
        },
        {
            "section_id": "3.2.5",
            "parent_section_id": "3.2",
            "section_name": "3.2.5 Dataset Analysis",
            "text": "Table 3  ###reference_### shows a comparison of statistics between AudioSetMix and human-annotated audio-language datasets. We note that because up to 5 distinct audio events can occur in a single AudioSetMix clip, a more complex caption is needed to fully describe the entire clip. Thus, the average caption for AudioSetMix is longer than the other datasets. To evaluate the quality of our captions compared to human-generated captions, we use GPT2 [28  ###reference_b28###] to compute the average perplexity of captions. We observe that our captions have lower perplexity comparing to Clotho."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we study the effectiveness of AudioSetMix for learning text-to-audio retrieval task. We additionally study the impact of AudioSetMix for improving model understanding of audio event modifiers, words that describe an attribute of an audio event such as volume or pitch. We provide a description of each task, as well as the experimental settings, results, and analysis. For all experiments performed, we use 16kHz sampling rate and 64-dimensional logmel-spectrogram with 1024-point Hamming window and 160 hop size to compute the audio input."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text-to-Audio Retrieval",
            "text": "Text-to-audio retrieval involves retrieving the audio clip that best matches a given text caption/query from a database of clips. Retrieval is generally done by pushing matching audio-caption pairs closer in an embedding space and keeping non-matching pairs apart [27  ###reference_b27###].\n###figure_2###"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Baseline Models",
            "text": "Following  [33  ###reference_b33###], we select a common dual encoder architecture used in the 2022/2023 DCASE Challange on Language-Based Audio Retrieval [35  ###reference_b35###] (Figure 2  ###reference_###). This architecture consists of audio encoder  and text encoder . For an audio-caption pair (A, T), the we computes an audio and text embedding, and project them to a shared dimension using a linear layer:\nNext, assume we have a training batch of  audio-caption pairs . We denote the model’s output for the  pair  as  and , respectively. We now compute a similarity score  between the ith audio clip and the jth caption using dot product:\nWe use the popular InfoNCE  [31  ###reference_b31###] contrastive loss function to train the model. However, because an audio clip can potentially to multiple text captions in the training dataset, we wish to avoid penalizing the model for correctly associating these audio-text pairs when they occur in the same minibatch. Thus, following  [13  ###reference_b13###], we introduce a  masking term M where  is the batch size:\nWe slightly modify the InfoNCE loss function with learnable temperature  using , as shown in Eq 6  ###reference_###. We note that when  consists of all ones, Eq 6  ###reference_### is identical to the standard InfoNCE loss.\nFollowing  [35  ###reference_b35###], we select a ResNet38 model with pretrained weights from PANNs [16  ###reference_b16###] as . To investigate whether more powerful text encoders are better at capturing the presence of audio event modifiers in the captions, we select BERT-medium [4  ###reference_b4###], BERT-base [7  ###reference_b7###], and RoBERTa-large  [23  ###reference_b23###] as our choices for ."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Text-to-Audio Retrieval Training",
            "text": "Following  [35  ###reference_b35###], we combine training sets from multiple sources to form a single, larger training set. We selected AudioCaps  [14  ###reference_b14###], Clotho  [8  ###reference_b8###], and MACS [24  ###reference_b24###]. Because AudioCaps data consists of YouTube videos may become unavailable over time, we obtain 49k audio-caption pairs out of the original 50k. Combining these datasets gives us a total of 89k training audio-text pairs. We refer to this dataset as the baseline dataset and rely only on the original, unaugmented dataset without any enhancements.\nWe trained our models with the loss function defined in Eq 6  ###reference_### for  epochs using  learning rate, batch size of , Adam optimizer, and cosine decay learning rate scheduler.\nWe evaluate our models using the Recall@K metric (R@K), which is defined as follows: Given a dataset of audio-caption pairs, we first compute embeddings for caption and audio clip using the pretrained models respectively. Next, for each caption, we compute the similarity between its embedding and all audio clips using Eq 2  ###reference_###. The Recall@K metric is defined as the probability that the top  most similar audio clips contains the targeted ground-truth clip. We use the test split from Clotho as the evaluation dataset. We report the recall for different  for our baseline models in Table 4  ###reference_###."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Evaluating Modifier Understanding for Text-to-Audio Retrieval",
            "text": "As shown in [33  ###reference_b33###], existing audio-language models depend heavily on the keywords in the caption, which are typically nouns and verbs. [33  ###reference_b33###] finds that this over-reliance on keywords causes current audio-language models to not capture the order of the audio event. Motivated by [33  ###reference_b33###], we extended, and investigated whether audio-language models also fail to “understand” the modifiers for events, such as  vs. . We study the impact from four categories of common modifiers {volume, pitch, speed, and duration} for audio events, to the models. This is aligned with the methods to produce the AudioSetMix in Section 3  ###reference_###. To do this, we create a subset from the Clotho and AudioCaps evaluation sets called “Modifier Test Set” (MTe). Captions in MTe contain words describing one of these modifiers, such as , ,  and etc.. In total, this gives us  pairs of data in MTe.\nWe first determine if existing audio-language models can capture modifiers in the caption. For this experiment, we select the ResNet38-Bert-base model as our baseline. Following the BAT test introduced in [33  ###reference_b33###], we replace each modifier in MTe with an antonym, forming what we call the flipped caption. For instance, the modifier  would be replaced with , and the modifier  would be replaced with . We then use the flipped captions to retrieve the original audio and report the recall as performance metrics. The retrieval set is the set of all other audio-text pairs containing the same class of modifier. If the model is able to distinguish modifiers, we would expect that the recall scores would degrade significantly with “flipped caption” in comparison to using the original captions as the query. The results are reported in Table 5  ###reference_###. We see only a marginal change in performance, which suggest that the models failed to learn to distinguish the modifiers well.\nTo more rigorously study model understanding of modifiers, we design the Modifier Understanding Test (MUT). For each audio-caption pair in MTe, We first compute the embedding distances between the original audio and both the flipped and original captions. We then count the percentage of times that the flipped caption embedding is closer to the audio embedding than the original caption embedding. If the model completely fails to capture the presence of modifiers, we should expect that the flipped caption is closer 50% of the time. On the other hand, a perfect model should have a score of 0%. As shown by the results in Table 6  ###reference_###, the model unsurprisingly performs significantly better than random choice in every modifier category, but is far from perfect.\nFinally, we wish to evaluate the model’s ability to distinguish between different categories of modifiers. We perform retrieval on MTe using the original captions as queries where the retrieval set is the set of every audio-text pairs in MTe, and report the recall scores in Table  7  ###reference_###. We refer to this experiment as Modifier Differentiating Test (MDT)."
        },
        {
            "section_id": "4.1.4",
            "parent_section_id": "4.1",
            "section_name": "4.1.4 Training with AudioSetMix",
            "text": "The percentage of sentences in the training data that contain modifiers is extremely small, as shown in Table  8  ###reference_###. As such, we study whether increase the number of modifiers in the training data improves understanding of modifiers. We augment the baseline dataset using AudioSetMix, giving us a total of 132k training audio-text pairs. We train new models using the same training procedure as the baseline. As shown in Table 6  ###reference_###, the models show significant gains in understanding duration, speed, and volume modifiers when trained with AudioSetMix. Furthermore, Table 7  ###reference_### shows that all models improve in their ability to distinguish between the different modifier categories."
        },
        {
            "section_id": "4.1.5",
            "parent_section_id": "4.1",
            "section_name": "4.1.5 Training with Generated Hard Negatives",
            "text": "In contrastive learning, each audio clip  is contrasted with other texts , which usually describe completely different clips. As such, models are able to ignore finer details of modifiers in clips and simply focus on audio events [3  ###reference_b3###]. We hypothesize that hard negative examples that contain the same audio events are needed to encourage models to capture and understand modifiers. In contrast to [27  ###reference_b27###], our data generation pipeline Figure 1  ###reference_### provides a natural way to generate hard negatives for each data point in AudioSetMix. Recall that in the data generation process, we sample a set of audio clips  for augmentation. We randomly select a set of augmentation operations  for , with each  applies to , and produced the augmented clips . We finally assemble the audio clips in  by mixture of concatenation and mixing. For hard negatives, while keeping the same procedure as outlined above, we “reverse” each operation  (i.e. if the original  is , the new transformation  is ). We argue that this creates hard negative samples since the two final augmented clips contain the same set of audio events, but with opposite augmentation operations applied to each. This makes it more challenging for the model to learn to distinguish them and forces the model to attend to the audio modifiers.\nTo train with these hard negatives, we randomly select  AudioSetMix inputs in each minibatch and generate their hard negatives. We then append the hard negatives to the minibatch and perform the model update. We empirically find that  works well for our batch size setting of 64. In Table  6  ###reference_###, we show that model performance on MUT is generally improved using hard negatives on all modifier categories except pitch. We hypothesize that pitch may be difficult to improve upon because pitch is not as broadly applicable of a modifier, meaning that pitch augmentation in AudioSetMix may not be meaningful semantically (i.e. applying a high-pitch transformation to a base sound of “tree falling over”).\nFinally, we compare our models trained on AudioSetMix and hard negatives with the baseline models on the original text-to-audio retrieval task. We evaluate each model using the evaluation set from Clotho and show the results in Table  4  ###reference_###. We note that the models trained using AudioSetMix and hard negatives beat the baseline results consistently on Clotho. In contrast to the previous experiments, the addition of hard negative mining provides only a marginal improvement to the recall scores. Because a) the hard negatives are only concerned with modifiers and b) the lack of modifiers in the eval sets, we hypothesize that the benefits of hard negative mining are not observable in this experiment."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce AudioSetMix, a weakly-labelled audio-caption pair dataset created by applying audio transformations to existing datasets. We propose a pipeline to augment/combine audio clips and generate a corresponding caption using LLM. We evaluate AudioSetMix on text-to-audio retrieval and demonstrate that AudioSetMix improves model understanding of audio event modifiers. In future we hope to evaluate models trained from AudioSetMix using human feedback.\nAcknowledgements We thank David Harwath for providing insight and expertise that greatly assisted the research in this work."
        }
    ]
}