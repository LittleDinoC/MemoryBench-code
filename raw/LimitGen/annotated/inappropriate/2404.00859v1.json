{
    "title": "Do Language Models Plan for Future Tokens?",
    "abstract": "Do transformers “think ahead” during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at  that is then used in future forward passes . We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at  irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step  are already the same as those that would most benefit inference at time . We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Humans are known to think ahead while speaking; decades of linguistics research (Huettig, 2015  ###reference_b12###; Miller, 1951  ###reference_b15###) have shown evidence that human language users internally predict upcoming language input, words and sometimes sentences ahead (Barthel et al., 2016  ###reference_b1###).\nUnlike humans, contemporary language models allocate a fixed amount of information processing for each token when “speaking” (Vaswani et al., 2017  ###reference_b27###).\nDo language models, like humans, think ahead? Recent work (Pal et al., 2023  ###reference_b22###; Hernandez et al., 2024  ###reference_b9###; Cai et al., 2024  ###reference_b7###) has shown that tokens beyond the immediate next token can be predicted by probing the hidden state of the language model. Intriguingly, model outputs at future tokens can be predicted to some extent using linear probes on model hidden states, and interventions on hidden states can predictably alter future outputs.\nThese findings indicate that model activations at a given timestep are at least somewhat predictive of future outputs. However, it remains unclear why this might be: is this just a happenstance property of the data, or because the model is deliberately preparing information for future timesteps, at the expense of degrading performance on the current position?\nWe observe that gradients during training optimize weights for both the loss at the current token position as well as for tokens later in the sequence. We question to what extent current transformer weights dedicate resources to the current token vs. allocating it for future tokens.\nWe consider two possibilities: the pre-caching hypothesis, in which the transformer learns to compute features at time step  that are irrelevant to the inference task at that current time step but may be useful for future time steps , and the breadcrumbs hypothesis, in which the features most relevant to time step  are already identical to those that would most benefit inference at time . To evaluate which hypothesis might be correct, we propose a myopic training scheme which does not propagate gradients from the loss at the current position to hidden states from previous positions.\nTo consider whether language models might directly implement pre-caching, we design a synthetic scenario where the task can only be completed via explicit pre-caching. We configure a task where the model must precompute information for the next token, because otherwise the correct answer could not be accurately computed in a single forward pass. In this synthetic scenario, we find clear evidence that the transformer learns to pre-cache. When transformer-based sequence models must precompute information to minimize loss, they do so.\nWe then consider whether breadcrumbs or pre-caching is demonstrated in natural language models (pre-trained GPT-2 variants). Our experiments with myopic training suggest that much less pre-caching occurs in this setting, and thus point towards the breadcrumbs hypothesis.\nOn real language data, we claim language models do not intentionally prepare information for the future to a significant extent. Instead, they compute features that are useful to predicting the immediate next token, which turn out to then be helpful at future steps. In language data, we do not observe a significant tradeoff between greedily optimizing for next token loss and ensuring future predictive performance.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Several recent works (nostalgebraist, 2020  ###reference_b20###; Belrose et al., 2023  ###reference_b6###; Pal et al., 2023  ###reference_b22###; Cai et al., 2024  ###reference_b7###) observe that transformer hidden states can be used to predict current and future tokens in a sequence, typically via linear probing. Notably, Hernandez et al. (2024  ###reference_b9###) show that more complicated relationships are encoded linearly in hidden states, such as subject-object relations, implying that future tokens can also be predicted in specific cases. Unlike these works, we consider whether the model is deliberately preparing hidden states that are useful for future prediction, at the expense of current-token predictivity.\nOur experiments make use of probing, a technique where a simple auxiliary model is used to predict properties from target models’ representations (Belinkov & Glass, 2019  ###reference_b5###; Shi et al., 2016  ###reference_b26###; Hewitt & Liang, 2019  ###reference_b10###; Pimentel et al., 2020  ###reference_b23###; Belinkov, 2021  ###reference_b4###). We can also phrase our probing experiments as measuring the -information contained in a representation vector, where  is the class of linear models (Xu et al., 2020  ###reference_b28###; Hewitt & Liang, 2019  ###reference_b10###). Probing-based approaches are known to overestimate latent information if the classifier learns to do a task on its own (Belinkov, 2021  ###reference_b4###), and probing analyses may only be informative when compared to probing a reasonable baseline (Hewitt & Liang, 2019  ###reference_b10###). In our probing experiments, we avoid these pitfalls by ensuring that the function to be learned cannot possibly be computed by the probe itself.\nOur analysis of transformer models in a synthetic setting relates to the subfield of mechanistic interpretability, which seeks to understand models by isolating and explaining the behavior of their components (Olah et al., 2020  ###reference_b21###; Bau et al., 2020  ###reference_b2###; Meng et al., 2023  ###reference_b14###; Nanda et al., 2023  ###reference_b17###). Some of these works (Nanda et al., 2023  ###reference_b17###; Li et al., 2023  ###reference_b13###; Zhong et al., 2023  ###reference_b29###) practice mechanistic interpretability by studying models trained on synthetic worlds. We apply some mechanistic interpretability techniques in a synthetic setting to study the problem of whether language models “think ahead” for future tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Theory: Pre-caching or breadcrumbs?",
            "text": "Consider a generic causal sequence-to-sequence prediction task\nwhere  is a data distribution supported on  for some domains . The task is to estimate the conditional expectations  for .111For classification tasks, we are typically interested in the conditional probabilities  for each class . However, this can be subsumed into the generic case by letting  be the probability simplex over all classes. Note that we recover the autoregressive setting by setting  and .\nTransformer models trained on such tasks have been observed (Pal et al., 2023  ###reference_b22###) to store information in hidden states during inference at position  that is then used in future inference at . However, since the loss associated with each step  depends only on how well the model does at the immediate task of predicting , it is not immediately clear how this preparation for the future arises. We give names to two competing explanations:\nPre-caching: The model “deliberately” computes and stores features that are expected to be useful for the future, even if they are irrelevant to the present.\nBreadcrumbs: The features that most benefit the present inference task are the same as those that are most useful to the future. When the model performs the present forward pass, it “unintentionally” leaves a trace (“breadcrumbs”) that is then picked up by future passes.\nTo disentangle these two explanations, we introduce a notion of myopic transformer models, which we show to be incapable of deliberate pre-caching—for these models, the extent to which past features are beneficial to the future is decided purely by the breadcrumbs explanation. Thus, the gap between vanilla and myopic transformer models is a quantitative measure of how much pre-caching is taking place."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Transformer preliminaries",
            "text": "Suppose, for the sake of exposition, that the transformer model  uses independent parameters for each position .222For example, this is true of absolute position embedding weights. Let  be the parameter count of each forward pass of . Then, letting  be all parameters used by  at position , a transformer  is a parameterized function\nFor , let  be the output of ’s th forward pass. Because of the causal masking within , this depends only on  and . That is, with slight abuse of notation, we may write"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Off-diagonal gradient terms",
            "text": "Now, letting  be some choice of loss function, the expected loss  of a transformer model with parameters  is\nthe sum over  of the expected loss  at position . (We suppress the dependence on  and  for concision.)\nIn practice, we always tie the weights across position. That is, all  are set equal to the same . Then, by the chain rule,\na sum over an upper-triangular expected Jacobian “matrix”. The off-diagonal terms , corresponding to the expected gradient of the model’s future loss at position  wrt. its weights at position , are the training signals that encourage pre-caching."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Measuring pre-caching: The myopia gap",
            "text": "We say a model is myopic when each forward pass  optimizes only  without regard for future  at . In the untied weights case, the right definition is then apparent.\nThe parameters  are untied-myopic if they satisfy\nLet  be the feasible set of the constraints in Equation 1  ###reference_###.\nThe untied myopia gap is the smallest possible gap between the expected loss attained by a myopic model and the optimal model:\nIn the tied weights case, it is perhaps not immediately clear what the right definition of myopia should be. It does not suffice to simply constrain the minimizations in Equation 1  ###reference_### to , since \nis optimizing for pre-caching (the dependence on arguments ) as well as the present inference (the dependence on argument ). Instead, the right notion is a choice of tied parameters such that the model is, aggregated over positions, optimal for the present task when conditioned on a fixed past. That is, forward passes do not compute features for the future if they can compute other features more beneficial to the present.\nThe parameters  are (tied-)myopic if they satisfy333Parameters satisfying Equation 3  ###reference_### can be guaranteed to exist under certain conditions; see Theorem 11  ###reference_###.\nThe (tied) myopia gap is then defined analogously to Definition 2  ###reference_###.\nThe myopia gap is small—near-optimal performance can be attained even when each forward pass is computing features relevant to only its own immediate inference task, with no regard to pre-caching for the future.\nIf the breadcrumbs hypothesis does not hold, we say that the model is pre-caching. It is important to remember that the  depend on a choice of transformer model  and dataset . That is, breadcrumbs and pre-caching are properties of the model architecture and the data considered as a whole.\nAlthough a small myopia gap reveals that one can do just as well without pre-caching, it does not say much about any specific model. To measure pre-caching within a given model, we examine the extent to which its parameters violate the myopia constraints.\nThe untied local myopia bonus at  is\nLikewise, the (tied) local myopia bonus at  is\nFor further interpretation of the myopia bonus, see Section A  ###reference_###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Myopic gradient descent",
            "text": "Our heuristic remark in Section 3.2  ###reference_### (that the off-diagonal gradient terms are responsible for pre-caching) is justified by Theorem 13  ###reference_###. It states that, given certain conditions on the loss terms , performing gradient descent with the off-diagonal terms removed results in a myopic model in the sense of Definition 3  ###reference_###.444In order to simplify the theory, we prove our results for the strongly convex -smooth case. These are not entirely artificial assumptions; for example, Milne (2019  ###reference_b16###) shows that certain feedforward neural networks have loss functions that are piecewise strongly convex with respect to their parameters in a neighborhood of all global optima.\nWe call this myopic descent.\nFor myopic descent to be stable in the tied-weights case, we need, roughly speaking, for the model to depend more on the parameters associated with the present forward pass than those from the past. This is a plausible condition—dependence on the past is mediated by the attention mechanism, which comprises a relatively small fraction of the total parameter count. The precise condition we use is forward bias, from Definition 10  ###reference_###.\nLet .\nIf  is forward-biased, -strongly convex, and -smooth, then, for some step size , the iterates of myopic descent with tied weights\nconverge to  satisfying the myopia constraints of Equation 3  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Synthetic data experiments",
            "text": "To demonstrate a simple instance where significant pre-caching occurs (and thus the myopia gap is large), we construct the following synthetic dataset.\nThe data distribution  is defined as the joint distribution of real-valued random variables  where, for each ,\n(standard Gaussian)\n(Bernoulli with probability )\n\nand  are mutually independent.\nIn our experiments, we always set the parameters  and , so for convenience notate .\nThe intuition is that a transformer regression model  trained on  would benefit from pre-caching  during its forward pass at position , even though this computation is irrelevant to its task of predicting . One simple strategy that makes use of this pre-caching is Algorithm 1  ###reference_###.\nThe motivation for the Bernoulli variables  is that, as  decreases, the expected first time when  becomes useful advances further into the future. In addition, when  is sufficiently small, the probability  that the value  is never useful at all becomes non-negligible. We will show that, even in this case, the transformer model learns to pre-cache.\nSuppose that we train a myopic model (Section 3.4  ###reference_###) on the same task. Since this model lacks off-diagonal gradient terms, we do not expect it to learn to pre-cache  at position . One possible strategy that does not use pre-caching is Algorithm 2  ###reference_###. We expect this brute force algorithm to perform significantly worse given the same parameter count—it computes a -dimensional nonlinear function within a single layer, while each layer of Algorithm 1  ###reference_### computes only scalar nonlinear functions.555For example, (Shen et al., 2022  ###reference_b25###) provide upper bounds on error that degrade exponentially in dimensionality given a fixed parameter and layer count.\nAt position ,\nAt position ,"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation: linear probing",
            "text": "To determine if the transformer model is computing at position , we fit a simple average of hidden states to predict future tokens. We additionally compute the correlations between  and each individual dimension (i.e., each neuron) of each hidden state. Note that, in order for using a simple average of hidden states to be meaningful, we must first ensure that there is no pre-existing linear relationship between the inputs and the quantities we are probing for. Since the  are mutually independent, this follows from Lemma 15  ###reference_###, stating that  and  have near-zero correlation for large enough . In our experiments, we set , in which case . In other words, there is low predictive -information from the inputs to the target , where  is the class of linear models (Xu et al., 2020  ###reference_b28###). See Section E.1  ###reference_### for details."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "For varying , we train two-layer transformer models with embedding dimensions of  on  using using both ordinary and myopic gradient descent. Full architecture and training details are provided in Section E  ###reference_###.\nFrom examining the performance of each linear probe against  for varying , we find strong evidence that the transformer model with vanilla training is indeed pre-caching , possibly in order to implement Algorithm 1  ###reference_###. Indeed, in Figure 2  ###reference_###,\nThe zeroth hidden state (i.e., the sum of the input and position embeddings) at position  is correlated with only .\nThe first hidden state is correlated with  but not correlated with any  for .\nThe second hidden state (immediately before the output unembedding) is correlated with  for each .\nFurther, looking at the per-neuron correlations in Figure 3  ###reference_###, we see that  for  are all correlated with a single 1-d subspace of the second hidden state (they share the same striping pattern); this is the subspace corresponding to . Meanwhile, , as well as many of the , are located in various other 1-d subspace of the second hidden state; these terms are all left over in the residual stream from previous layers, and are cleaned up only by the output unembedding.\nOn the other hand, in Table 1  ###reference_###, the myopic models perform significantly worse. The per-neuron correlations in Figure 3  ###reference_### suggest that the myopic model may be implementing a crude approximation of Algorithm 2  ###reference_###. This suggests that the synthetic setting has an inherently high myopia gap—it is impossible for the transformer model to do well without pre-caching.\n###figure_2### ###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Natural language experiments",
            "text": "In order to measure the extent to which transformer models learn to pre-cache on natural language data, we estimate both the myopia gap (Definition 3  ###reference_###) of this setting as well as the local myopia bonus (Definition 5  ###reference_###) of a transformer model with vanilla training. All models use the 124M-parameter GPT-2 architecture; see Table 4  ###reference_### for configuration details.\nWe train all models (vanilla and myopic) from random initialization for one epoch on 4.6M sequences from the MS MARCO dataset (Nguyen et al., 2016  ###reference_b19###), truncated to length 64. To estimate the local myopia bonus of the vanilla model, we train another model from random initialization with the same architecture, but with past hidden states provided by the vanilla model; see Section C  ###reference_### for details.777Here, we do not use any existing pre-trained models (outside of the pre-trained tokenizer) for our experiments. This choice was made to avoid the interference of distribution shift and varying training schemes when comparing between the vanilla and myopic models.\nAs baseline, we also train a “transformer bigram” model, a model with an identical architecture but all off-diagonal key/value states zeroed out."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "###figure_6### From Table 2  ###reference_###, the estimated myopia gap in this setting is , while the local myopia bonus of the vanilla model is .\nThe somewhat nonnegligible myopia gap suggests that pre-caching may provide a small nonzero benefit. Indeed, in Figure 5  ###reference_###, we see that the myopic model outperforms the vanilla model at the beginning of the sequence, but falls behind as the length of the past increases. This implies that a lack of pre-caching may compound, and model performance degrades later in the sequence as the model is unable to refer to prior pre-cached information.\nHowever, note that this gap is much smaller than that between the vanilla model and the transformer bigram model. That is, the myopic model is still able to leverage past information (breadcrumbs) to a significant extent, even if they optimized only for the present inference task.\nThat the local myopia gap is near zero further supports this direction—the model learned through vanilla training does not trade off significantly between features useful for the present and pre-caching for the future."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We consider the phenomenon where transformer language models compute features in the present that are then relevant to the future. We propose two possible explanations, breadcrumbs and pre-caching. Using a synthetic dataset, we demonstrate that pre-caching does indeed occur in the transformer model. On the other hand, our experiments with natural language models suggest that breadcrumbs is more explanatory in that setting."
        }
    ]
}