{
    "title": "Self-Consistency Boosts Calibration for Math Reasoning",
    "abstract": "Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development.\nWe design three off-the-shelf calibration methods based on self-consistency Wang et al. (2022) for math reasoning tasks.\nEvaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) Kadavath et al. (2022) or logit Guo et al. (2017).",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Mathematical reasoning tasks Cobbe et al. (2021  ###reference_b4###); Hendrycks et al. (2021  ###reference_b7###); Amini et al. (2019  ###reference_b2###) involve mapping a question into a series of equations, which are then solved to obtain the final answer.\nMath reasoning has long been recognized challenging.\nExisting solutions propose to map input questions into equations via semantic parsing Matsuzaki et al. (2017  ###reference_b16###); Hopkins et al. (2017  ###reference_b8###) or AST decoding Li et al. (2019  ###reference_b13###); Qin et al. (2021  ###reference_b18###); Wu et al. (2021  ###reference_b24###).\nYet, the performance can degradate dramatically even with slight changes to the questions Patel et al. (2021  ###reference_b17###); Li et al. (2022  ###reference_b14###).\nRecently, large language models (LLM, Achiam et al. 2023  ###reference_b1###; Touvron et al. 2023  ###reference_b21###; Jiang et al. 2024  ###reference_b10###) have shown great potential for solving many math reasoning tasks, even though they are not specifically trained on these tasks.\nFor instance, with chain-of-thought prompting Wei et al. (2022  ###reference_b23###) and self-consistency Wang et al. (2022  ###reference_b22###), open-source LLMs, such as Mixtral 87B Jiang et al. (2024  ###reference_b10###), can reach an accuracy of around 80% on the GSM8K benchmark Cobbe et al. (2021  ###reference_b4###).\nOn the other hand, conventional pretrained models (e.g., T5 Raffel et al. (2020  ###reference_b19###)) that are specifically finetuned on the GSM8K training set can only report accuracies around 10% to 20% Shridhar et al. (2023  ###reference_b20###); Magister et al. (2023  ###reference_b15###).\nHowever, LLMs lack adequate calibration out of the box – the probabilities of model predictions are often poorly aligned with the actual accuracy Xiong et al. (2023  ###reference_b25###); Chen et al. (2023  ###reference_b3###).\nCalibration is important for LLM development, as a well-calibrated LLM can precisely tell how likely its responses are correct or not.\nWith such information, LLM developers can take multiple options to handle low-confidence responses, such as letting the LLM refuse to answer or keep resampling until a confident response is produced.\nIn this work, we propose calibration methods based on self-consistency Wang et al. (2022  ###reference_b22###) for math reasoning tasks.\nSelf-consistency performs clustering over multiple LLM samples before picking one from the largest cluster as the response to each input query.\nHere we consider several ways to estimate model confidence using the clustering results: cluster size that estimates how many samples agree with the selected one, cluster number that measures to what extent samples disagree with each other, and pairwise comparison that captures relative differences between pairs of clusters.\nWe conduct experiments using strong open-source LLMs:\nMistral Jiang et al. (2023  ###reference_b9###, 2024  ###reference_b10###) and LLaMA2 Touvron et al. (2023  ###reference_b21###) series models with / without being aligned with instructions.\nResults on GSM8K Cobbe et al. (2021  ###reference_b4###) and MathQA Amini et al. (2019  ###reference_b2###) show that all our methods better calibrate these models than exiting popular methods, such as p(True) Kadavath et al. (2022  ###reference_b11###) and logit Guo et al. (2017  ###reference_b6###) over the whole reasoning path or target answer span only."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preview: Self-Consistency with CoT Prompting",
            "text": "For math reasoning, there are usually multiple trajectories to reach the final solution.\nTo replicate this process, Wang et al. (2022  ###reference_b22###) initially sample various reasoning paths  from the LLM given input  with Chain-of-Thought (CoT) prompting.111Here we follow common practice to adopt demonstrations with rationales for pretrained only models (e.g., Mistral-7B) and use “Let’s think step by step” Kojima et al. (2022  ###reference_b12###) for instruction-tuned models (e.g., Mistral-7B-Inst).\nThen, the answers  are extracted from the paths,\nand the most consistent answer (the one win by majority vote among the answers) is selected as the final answer :\nwhere ,  denote the -th sampled reasoning path and its corresponding answer, respectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Calibration using Self-Consistency",
            "text": "After randomly selecting an answer without considering clustering on input, we obtain a set of responses. We design the following strategies, tailored to the nature of the generated responses, to estimate the confidence of . We initially consider the Cluster Number . This is motivated by the finding of previous work Wang et al. (2022  ###reference_b22###); Xiong et al. (2023  ###reference_b25###): LLMs tend to generate consistent answers when they are confident about their predictions, and thus the cluster number (number of distinct answers) tends to be small. We further divide the cluster number by the sample size  to normalize the score into the range of , before reversing it by “”: In a similar vein, we adopt the Cluster Size: the number of samples (e.g., ) within a specific cluster (e.g., ). Again, we compute its proportion relative to the total sample size to normalize the score into the range : In contrast to the cluster number, the cluster size is more universally applicable across diverse prompts, as the cluster number can easily become ineffective when the output space of an LLM is restricted, such as when options for a question are provided. The Cluster Number and Cluster Size primarily consider the number of distinct answers and the number of sampled paths within a single cluster, respectively. They both overlook the information by comparing different clusters. For example, they may fail to consider the situation when the sizes of the top-ranked clusters are close. Consequently, we introduce the Pairwise Comparison method, which computes the winning rate of the chosen cluster () against each of the remaining clusters: where  represents the winning rate of selected cluster  against another cluster ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###table_1### We conduct experiments on two popular math reasoning benchmarks of different type of questions, GSM8K Cobbe et al. (2021  ###reference_b4###) and MathQA Amini et al. (2019  ###reference_b2###).\nParticularly, GSM8K comprises 1,319 linguistically diverse grade school math word problems for testing.\nOn the other hand, MathQA offers 2,985 multiple-choice math word problems for evaluation.\nWe adopt Brier Score and Expected Calibration Error (ECE) as evaluating metrics following common practice Geng et al. (2023  ###reference_b5###).\nGiven instances  and their corresponding LLM predictions , ECE is computed by first binning the predictions into  intervals based on their LLM confidence levels (e.g., ).\nFor each bin (e.g. ), it then calculates the accuracy (acc) and the average confidence (conf):\nwhere  is the number of samples in bin .\nFinally, the difference between accuracy and confidence is averaged across all bins to obtain the ECE score:\nAs another popular metric, Brier score is similar to ECE but conducted at the instance level:\nBoth metrics range from 0 to 1 with lower values indicating better calibration.\nWe take Brier score as the main metric, as it is more robust to unbalanced distribution across bins (e.g. instances concentrate to one or two bins).\nWe conduct experiments on LLaMA2 and Mistral-family models and investigate both pretrained or instruction-tuned variations.\nWe use nucleus sampling to obtain  samples by default for each instance and use temperatures of 0.6 / 1.0 for all pretrained / instruction-tuned models.\nWe take the three representative baselines below for comparison:\nlogit w/ Path: It averages the probabilities of the tokens from the whole path to estimate the confidence of each prediction.\nlogit w/ Answer: It is similar to logit w/ Path but only consider the tokens from the predicted answer span.\np(True): It asks the LLM itself to classify its prediction as True or False. Then, it takes the predicted probability of True as its confidence. We follow Kadavath et al. (2022  ###reference_b11###) to construct 8-shot demonstrations for prompting pretrained models but directly use instruction for instruction-tuned models.\nTable 1  ###reference_### presents the main results obtained from both benchmarks using Mistral-family models. p(True) performs best among the baselines, echoing the findings of Kadavath et al. (2022  ###reference_b11###). However, due to its reliance on prompt design and in-context examples to aid the LLM to classify its predictions, it can be challenging to construct effective demonstrations or instructions.\nIn general, self-consistency-based methods surpass baselines in most cases regarding Brier and ECE, validating the efficacy of employing self-consistency features for estimating model confidence. We also note that baselines can occasionally yield impressive ECE scores (p(True) on GSM8K with Mixtral-87B). However, we observe that this is attributed to the concentration of most samples in just a few bins (e.g., Figure 1  ###reference_###), leading to unreliable measurements. Nevertheless, our approaches still exhibit strong performance in terms of ECE scores across various settings.\nAmong the self-consistency-based methods,  yields better ECE results on GSM8K, while  achieves the highest Brier score.\nConversely, for MathQA,  performs significantly worse than the other two.\nThis is because MathQA is a multi-choice task, and thus the cluster number of LLM answers is strictly limited by the provided choices.\nIn conclusion,  demonstrates greater generality across diverse settings, but  and  do offer improved estimation in certain cases.\nPrevious research Wang et al. (2022  ###reference_b22###) has demonstrated that the sample size  can significantly affect the accuracy of self-consistency. When  increases, the model performance initially continues to improve before stabilizing once  reaches a sufficient level. Therefore, we take Mixtral-87B-Inst as a case study to examine the impact of  on calibration.\nAs illustrated in Figure 2  ###reference_###, the Brier scores for all our methods initially decline and then remain constant as  grows. For  and ,  is adequate for accurate estimation. In contrast,  requires a larger , indicating that the cluster number is more susceptible to the randomness of sampling.\nWe finally explore the associations between model performance (Accuracy) and calibration. Figure 3  ###reference_### showcases the results on instruction-tuned LLaMA2 and Mistral series models, arranged in ascending order based on their performance.\nWe generally observe a positively correlated trend between calibration (lower the better) and performance (higher the better) among the studied models.\nThis observation indicates that more powerful models also exhibit enhanced calibration, echoing the findings of Kadavath et al. (2022  ###reference_b11###). This phenomenon can be attributed to the fact that when a tested LLM is stronger, it is capable of generating more reasonable and consistent responses, leading to improved calibration."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "We conduct experiments on two popular math reasoning benchmarks of different type of questions, GSM8K Cobbe et al. (2021  ###reference_b4###  ###reference_b4###) and MathQA Amini et al. (2019  ###reference_b2###  ###reference_b2###).\nParticularly, GSM8K comprises 1,319 linguistically diverse grade school math word problems for testing.\nOn the other hand, MathQA offers 2,985 multiple-choice math word problems for evaluation.\nWe adopt Brier Score and Expected Calibration Error (ECE) as evaluating metrics following common practice Geng et al. (2023  ###reference_b5###  ###reference_b5###).\nGiven instances  and their corresponding LLM predictions , ECE is computed by first binning the predictions into  intervals based on their LLM confidence levels (e.g., ).\nFor each bin (e.g. ), it then calculates the accuracy (acc) and the average confidence (conf):\nwhere  is the number of samples in bin .\nFinally, the difference between accuracy and confidence is averaged across all bins to obtain the ECE score:\nAs another popular metric, Brier score is similar to ECE but conducted at the instance level:\nBoth metrics range from 0 to 1 with lower values indicating better calibration.\nWe take Brier score as the main metric, as it is more robust to unbalanced distribution across bins (e.g. instances concentrate to one or two bins).\nWe conduct experiments on LLaMA2 and Mistral-family models and investigate both pretrained or instruction-tuned variations.\nWe use nucleus sampling to obtain  samples by default for each instance and use temperatures of 0.6 / 1.0 for all pretrained / instruction-tuned models.\nWe take the three representative baselines below for comparison:\nlogit w/ Path: It averages the probabilities of the tokens from the whole path to estimate the confidence of each prediction.\nlogit w/ Answer: It is similar to logit w/ Path but only consider the tokens from the predicted answer span.\np(True): It asks the LLM itself to classify its prediction as True or False. Then, it takes the predicted probability of True as its confidence. We follow Kadavath et al. (2022  ###reference_b11###  ###reference_b11###) to construct 8-shot demonstrations for prompting pretrained models but directly use instruction for instruction-tuned models."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "Table 1  ###reference_###  ###reference_### presents the main results obtained from both benchmarks using Mistral-family models. p(True) performs best among the baselines, echoing the findings of Kadavath et al. (2022  ###reference_b11###  ###reference_b11###). However, due to its reliance on prompt design and in-context examples to aid the LLM to classify its predictions, it can be challenging to construct effective demonstrations or instructions.\nIn general, self-consistency-based methods surpass baselines in most cases regarding Brier and ECE, validating the efficacy of employing self-consistency features for estimating model confidence. We also note that baselines can occasionally yield impressive ECE scores (p(True) on GSM8K with Mixtral-87B). However, we observe that this is attributed to the concentration of most samples in just a few bins (e.g., Figure 1  ###reference_###  ###reference_###), leading to unreliable measurements. Nevertheless, our approaches still exhibit strong performance in terms of ECE scores across various settings.\nAmong the self-consistency-based methods,  yields better ECE results on GSM8K, while  achieves the highest Brier score.\nConversely, for MathQA,  performs significantly worse than the other two.\nThis is because MathQA is a multi-choice task, and thus the cluster number of LLM answers is strictly limited by the provided choices.\nIn conclusion,  demonstrates greater generality across diverse settings, but  and  do offer improved estimation in certain cases.\nPrevious research Wang et al. (2022  ###reference_b22###  ###reference_b22###) has demonstrated that the sample size  can significantly affect the accuracy of self-consistency. When  increases, the model performance initially continues to improve before stabilizing once  reaches a sufficient level. Therefore, we take Mixtral-87B-Inst as a case study to examine the impact of  on calibration.\nAs illustrated in Figure 2  ###reference_###  ###reference_###, the Brier scores for all our methods initially decline and then remain constant as  grows. For  and ,  is adequate for accurate estimation. In contrast,  requires a larger , indicating that the cluster number is more susceptible to the randomness of sampling.\nWe finally explore the associations between model performance (Accuracy) and calibration. Figure 3  ###reference_###  ###reference_### showcases the results on instruction-tuned LLaMA2 and Mistral series models, arranged in ascending order based on their performance.\nWe generally observe a positively correlated trend between calibration (lower the better) and performance (higher the better) among the studied models.\nThis observation indicates that more powerful models also exhibit enhanced calibration, echoing the findings of Kadavath et al. (2022  ###reference_b11###  ###reference_b11###). This phenomenon can be attributed to the fact that when a tested LLM is stronger, it is capable of generating more reasonable and consistent responses, leading to improved calibration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we extend the widely-used inference strategy, self-consistency, to the field of calibration. Specifically, we develop three off-the-shelf calibration methods based on self-consistency for math reasoning tasks. Compared to conventional methods (p(True) and logit), our approaches yield significantly improved ECE and Brier scores on popular GSM8K and MathQA datasets.\nFuture research directions include designing more effective calibration methods, leveraging richer features and employing more strategies (e.g., temperature scaling Guo et al. (2017  ###reference_b6###)) to enhance calibration performance.\nOur ultimate goal is to construct reliable and honest LLMs with the help of accurate confidence estimation."
        }
    ]
}