{
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "abstract": "Despite the popularity of Large Language Models (LLMs), crafting specific prompts for LLMs to perform particular tasks remains challenging. Users often engage in multiple conversational turns with an LLM-based agent to accomplish their intended task. Recent studies have demonstrated that linguistic feedback, in the form of self-reflections generated by the model, can work as reinforcement during these conversations, thus enabling quicker convergence to the desired outcome. Motivated by these findings, we introduce MetaReflection, a novel technique that learns general prompt instructions for a specific domain from individual self-reflections gathered during a training phase. We evaluate our technique in two domains: Infrastructure as Code (IaC) vulnerability detection and question-answering (QA) using ReAct and CoT. Our results demonstrate a notable improvement, with MetaReflection outperforming\nGPT-4 by 16.82% (IaC), 31.33% (CoT), and 15.42% (ReAct), underscoring the potential of MetaReflection as a viable method for enhancing the efficiency of LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs), such as GPT-4 OpenAI (2023  ###reference_b7###), have gained significant popularity in recent years due to their ability to generate human-like text and solve complex tasks across various domains. To leverage these models, users typically craft prompts with instructions that are tailored to a specific task. These prompts, however, are not just limited to explicit instructions, they can be enriched with additional context drawn from a variety of sources such as documentations, examples, or relevant inputs gathered from a range of other tools. This allows for a more comprehensive and nuanced understanding of the task at hand, thereby aiding the model in generating more accurate and relevant outputs.\nHowever, the process of crafting specific prompts, designed to guide LLMs in performing particular tasks, is not fairly straightforward.\nIn fact, prompt engineers often find themselves investing a substantial amount of time in iterating and refining their prompts to optimize them for a specific task Zamfirescu-Pereira et al. (2023  ###reference_b24###); Parnin et al. (2023  ###reference_b9###). This iterative, time-consuming interaction often leads to delays and inefficiencies, posing a substantial barrier to the seamless utilization of LLMs.\nIn practice, it is common for users to engage in multiple conversational turns with an LLM-based agent, providing feedback to correct the agent’s trajectory, in order to accomplish their intended task.\nRecent works Shinn et al. (2023  ###reference_b13###) have showed that the performance of language\nagents can be improved using verbal reinforcement learning during\nmultiple conversational turns.\nThe language agent is provided feedback at the end of a failing trajectory, and\nasked to reflect on its mistakes and the reflective text is stored in episodic\nmemory to improve future trajectories on the same task.\nFor instance, Figure 1  ###reference_### shows a ReAct Yao et al. (2023b  ###reference_b22###) based\nlanguage agent failing to complete a question-answering task from the\nHotpotQA dataset Yang et al. (2018  ###reference_b20###) because the agent got stuck in a loop\nlooking for the very common word “goal\" in a football related page.\nIn the top two boxes on the left of Figure 1  ###reference_### (labelled Task and\nTrajectory), we show the trajectory of the agent attempting to solve the task.\nThe box Self-Reflection from Figure 1  ###reference_### shows the reflection text\nfor the above failing trajectory, pointing out the error in repeatedly searching\nfor a common term.\nWith this self-reflection as additional episodic memory, the next trajectory\nsucceeds in finding the right answer.\nWhile self-reflection can significantly improve a language agent’s performance,\nit is a online reinforcement process that depends on the\navailability of performing multiple turns with a feedback mechanism.\nIn this paper, we introduce MetaReflection, an approach to learning\nverbal instructions for language agents using past self-reflections.\nIntuitively, during a training phase, self-reflections from different tasks are\ngathered and generalized into a verbal ‘meta-reflection’ that takes the form of\nadditional instructions to the language agent.\nIn Figure 1  ###reference_###, the self-reflection from the failing trajectory and\nother self-reflections over the training data are generalized into instructions\nthat suggest that the language agent search for related terms or change the\nsearch strategy.\nUnlike the self-reflections, the meta-reflection instructions are not specific\nto any particular instance of task.\nIn the online phase, the language agent is able to use these general\ninstructions to search for the right term “pilot\" (instead of “airline pilot\")\nand answer the question correctly, which it was not able to do previously.\nNote that there is no feedback mechanism during the inference—intuitively, we\nare leveraging a feedback mechanism available during the training phase to\nimprove the language agent performance even in the absence of the feedback\nmechanism.\nWe evaluate MetaReflection across two distinct domains: vulnerability threat detection in a new Infrastructure-as-Code (IaC) dataset111https://aka.ms/MetaReflectionDataset  ###reference_### and retrieval and reasoning using the HotpotQA dataset. The IaC dataset is used to evaluate the performance of our solution in detecting vulnerabilities in cloud infrastructure configuration files. These files, written in a low-resource language known as HCL, manage computing data center infrastructure and declare resources such as virtual machines, virtual networks, and data stores. The task is to identify potential security vulnerabilities in these files, a challenge due to the complexity of configurations and the diversity of resources being handled across multiple infrastructure providers. Our technique demonstrated a  overall improvement in accuracy across all policies when compared to the baseline GPT-4 model.\nThe second dataset, HotpotQA, is utilized to evaluate retrieval and reasoning capabilities of a model. This open-domain factual question answering dataset comprises K question-answer pairs. In our experiments, MetaReflection brought consistent gains in all configurations that we tested, with up to  improvement in accuracy against the baseline.\nTo summarize, we make the following contributions:\nWe present MetaReflection, a technique for learning verbal instructions for language agents using past self-reflections (Section 4  ###reference_###);\nWe conducted an extensive evaluation of the MetaReflection technique across two distinct domains: vulnerability threat detection and causal reasoning demonstrating significant improvements in both domains over the baselines (Section 3  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "With the increasing ubiquity of black-box Large Language Models OpenAI (2023  ###reference_b7###); Anil et al. (2023  ###reference_b1###); Brown et al. (2020  ###reference_b3###); Bai et al. (2022  ###reference_b2###), there has been a growing interest in the community to develop strategies that can maximize the model’s performance on a downstream task. These techniques may involve guiding an LLM to arrive at the correct answer Wei et al. (2023  ###reference_b16###); Zheng et al. (2023  ###reference_b28###), creating multi-step workflows and agents to achieve specific tasks Wu et al. (2023  ###reference_b18###), equipping the LLMs with tools Yao et al. (2023b  ###reference_b22###); Qin et al. (2023  ###reference_b12###), output selection Yao et al. (2023a  ###reference_b21###); Poesia et al. (2022  ###reference_b10###), etc.\nIn spite of the recent advancement of these strategies, careful prompt engineering has proven to be effective in bringing complementary gains over these techniques  White et al. (2023  ###reference_b17###).\nComing up with best instructions for a task can be a very time consuming effort. This has motivated efforts to come up with the ‘right’ prompt automatically.\nAutomated Prompt Engineering (APE) Zhou et al. (2023  ###reference_b29###) poses instruction generation as a synthesis problem and proposes techniques to effectively search over the space of instruction candidates generated by an LLM. The learned prompt can then be used during inference time in isolation.\nGiven the potentially infinite space of instructions, recent works have studied the problem of ‘guided’ prompt search instead. To this end, OPRO Yang et al. (2023  ###reference_b19###) proposes a prompt ‘optimization’ technique where they come up with a prompting strategy that can enable a model to perform prompt mutations to optimize a numerical ‘metric’ like eval set performance. HtT  Zhu et al. (2023  ###reference_b30###) proposes a method to effectively learn ‘rules’ using task plan generated by a model using techniques like CoT and search through this rule library.\nOuyang and Li present AutoPlan Ouyang and Li (2023  ###reference_b8###), an approach to generate planning instructions for interactive decision-making tasks. These instructions help the model to better plan to use external tools to add grounding to the language agents. In our work, we generate a set of broad instructions that can be used as grounding, to improve the quality of LLM responses, orthogonal to the use of external tools.\nProTeGi Pryzant et al. (2023  ###reference_b11###) and PE2 Ye et al. (2023  ###reference_b23###) also leverage verbal feedback to generate and/or evolve task description prompts. They start with an initial prompt, evaluate it on a batch of examples from a training set and use the failing examples to generate textual gradients to criticize the prompt. They subsequently use these gradients to produce multiple candidates per instructions and sample the best candidates at each iteration. In PE2 they additionally, maintain an optimization history to iterative improve the prompt. In contrast, in our work, we aim at developing instruction sets instead of task description. Besides, our technique leverages free-form self-reflections to navigate the search space of possible instructions instead of sampling from a large candidate pool - leading to a relatively less costly search.\nAn alternative approach to improve language agent predictions can be to adapt on-the-fly an initial human written instruction to a given input instance for a task. This can be done by fine-tuning a model that can generate the mutations to the initial instruction Zhang et al. (2022b  ###reference_b26###, 2023  ###reference_b27###, a  ###reference_b25###); Deng et al. (2022  ###reference_b4###) or by prompting a black-box LLM to come up with such mutations Sun et al. (2023  ###reference_b14###); Kim et al. (2023  ###reference_b6###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Evaluation",
            "text": "We evaluate the performance of MetaReflection on dataset from two different domains: vulnerability threat detection (IaC) and question answering (HotpotQA).\nCloud infrastructures are prone to security vulnerabilities such as open ports and\nexposed administrator accounts Tenable (2023  ###reference_b15###).\nVulnerability detection via static analysis of IaC files\nis a hard problem due to the expressivity of the configuration language, the\ncomplexity of configurations and the diversity of the resources being\nhandled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft\nAzure).\nFurther, Terraform uses a low-resource language - HashiCorp Configuration Language (HCL).\nTerrascan Tenable (2023  ###reference_b15###) is a static analyzer for detecting security\nvulnerabilities in Terraform modules, and supports over  security policies,\nincluding  policies specific to Azure.\nFigure 2  ###reference_### shows the description and definition of a\nTerrascan policy that checks if every Azure virtual network subnet is configured\nwith a corresponding network security policy.\nNote that the Terrascan policy is syntactic, i.e., it is checking for a\ndeclaration of an azurerm_virtual_network with a field named\nsubnet, and so on.\nHence, Terrascan-like static analysis based vulnerability detection is fragile\nand prone to both false positives and false negatives due to being sensitive to\nsyntax.\nThe task at hand is to check if a given Terraform module violates\na given Terrascan policy.\nDescription: Ensure that Azure Virtual Network subnet is configured with a Network Security Group\n\nDefinition:\nWe collected  Terraform modules by mining GitHub repositories for IaC code\nwritten in HCL.\nThese repositories corresponded to a diverse range of applications including\nload balancers, machine learning operations managers, and domain-specific\ndata-stores.\nFor policies, we selected the  most commonly violated Terrascan policies.\nOf the  module-policy pairs, we eliminated a significant fraction of cases\nwhere the policies were not applicable to the module.\nFor example, if the policy was for a specific resource type and the module did\nnot contain declarations of that resource type, the pair was eliminated.\nAfter this process, we were left with  module-policy pairs, for which we\nmanually annotated whether the module violated the policy (see\nTable 1  ###reference_### for the exact breakdown).\nNote that this ground-truth annotation was with respect to the description of\nTerrascan policy, not the definition—that is, we use the intention behind the\npolicy, not the letter of the definition.\nThat is, we do not take the output of Terrascan as ground truth as it can be\ninaccurate, and instead manually examine if the policy (as per description) is\nviolated.\nThis data was then split into train and test sets in a  ratio per policy,\ntaking care to balance the vulnerable and non-vulnerable classes.\nAs a baseline language agent, we use GPT-4 with an appropriate prompt that\nprovides the code of the Terraform module and the description of the Terrascan\npolicy, and asks if the module is vulnerable.\nWhile training, the agent is given a 0-1 feedback on whether its response is\ncorrect or not, and the model is asked to self-reflect if the\nresponse is incorrect.\nFor each policy, we run the MetaReflection algorithm on the training set and\nreport the accuracy numbers for both the baseline agent and the agent with the\ninstructions learned through MetaReflection.\nWe also compare to LLMInstruction as another baseline—here the\nlanguage model is asked to come up with instructions for a task given its description (Figure  3  ###reference_###),\nand then these instructions are provided when the task is being performed.\nYou are an expert in [Task]. Given the following task description [and examples] come up with a set of instructions that can help you perform the task effectively.\n\nTask Description: ...\nThe results of the experiment are summarized in Tables 1  ###reference_###\nand 2  ###reference_### (last  cols).\nOn the whole, across all policies, meta-reflection shows a  accuracy\nimprovement over the baselines depending on the batch size.\nAs Table 1  ###reference_### shows, meta-reflection provides consistent\ngains in accuracy for all policies over the GPT-4 baseline, with\n in the best case.\nThe precision with MetaReflection is significantly better for all\npolicies, while the recall decreases for some.\nWe discuss the case of security policy reme_noSecurityGroupAssociated\nfrom Figure 2  ###reference_###, i.e., that all Azure virtual network\nsubnets are configured with a network security group (NSG).\nThe main difficulty here is that HCL and Terraform offer many different ways of\n\n\n(a) associating a subnet with a virtual network, and\n\n(b) associating a NSG with a subnet.\n\n\nBy default, the baseline GPT-4 agent fails to handle certain ways of specifying\nthese associations, while spuriously assuming certain other associations.\nIn Figure 4(a)  ###reference_sf1###, the baseline consistently failed to\nrecognize a subnet-NSG association expressed using Method 2, i.e., using an\nexplicitly declared association.\nOn the other hand, it mis-identified declarations similar to the one in\nFigure 4(b)  ###reference_sf2### as valid subnet-NSG\nassociations—here, the NSG is associated with a virtual machine’s network\ninterface (that is connected to the subnet) instead of the subnet itself.\nThese limitations lead to both false positives and false negatives.\nWith meta-reflection, we are able to learn the instructions in\nFigure 4(c)  ###reference_sf3###, using which the agent easily handles these\nkinds of cases.\n4. Remember that the association between \"azurerm_virtual_network\" and a NSG may\nnot be direct. It could be done through a separate resource block such as\n\"azurerm_subnet_nsg_association\"\n...\n7. Do not confuse NSG associations with network interfaces of VMs and the subnet\nof the Azure Virtual Network. The policy specifically requires the NSG be\nassociated with the subnet.\n\nAs the above exemplar case shows, MetaReflection is able to learn very\ndomain-specific instructions to fix both false positives and false negatives.\nOther instructions include aspects like handling of wildcards for port numbers,\nstep-by-step strategies for specific policies, etc.\nNote that these instructions not only include planning (or trajectory directing)\ninstructions, but also grounding instructions—i.e., external facts that are\nnot initially available.\nIn general, the experimental results show that meta-reflection is able to\nreduce the number of errors, i.e., improve the accuracy across a broad range of\ncases.\nHowever, one noticeable issue from the above results is the drop in recall for\nseveral policies.\nWhile the high recall in the baseline is artificial, coming at the cost of low\nprecision, this is still an important issue to address.\nOur 0-1 feedback to the self-reflection agent does not state that false\nnegatives are worse than false positives in the security domain.\nIn the future, we plan to explore domain-specific feedback and self-reflection\nmechanisms that can account for the nature of errors, as well as better versions\nof the  function that are aware of such\ndomain-specific preferences.\nFor each agent setting, we adversarialy sample subsets of the HotpotQA train\nsplit of K samples to create train and test sets. To perform adversarial\nsampling, we first identify samples where the base agent fails consistently in a\ngiven setting.\nOn these failing examples we perform upto  self-reflection trials to get the\nmodel to the right response.\nIf the agent is not able to get to the correct response even after\nself-reflection, we discard these samples.\nThis strategy ensures that we get a set of hard examples in which the agents\nfail most of the times to get to the right answer in a single try, while also\nmaking sure that we filter examples that may be noisy due to missing context,\nincorrect questions etc.\nTo account for randomness and given our computational budget, we sample 40 and\n80 examples for the ReAct train set and test set respectively. For CoT settings, we pick  and  example respectively.\nWe reuse the CoT agent from Wei et al. (2023  ###reference_b16###) for the\nchain-of-thought experiments and use a re-implementation of Yao et al. (2023b  ###reference_b22###)\nfor the ReAct experiments.\nThe ReAct agent is allowed at most  Action steps after which the\ntrajectory is automatically determined to be a failure.\nSimilar to Section 3.1  ###reference_###, we evaluate HotpotQA configurations for:\n\n\n(a) MetaReflectionwith batch sizes , , and ; and\n\n(b) GPT-4and LLMInstruction as baselines.\n\n\nIn addition to this, we also evaluate variants of the agents powered by GPT-3.5-Turbo instead of GPT-4, while using GPT-4 for MetaReflection.222A similar experiment on the IaC domain wasn’t possible due to large context length of the Terraform modules\nWe find that the generated MetaReflection instruction consistently improved performance across different agent settings for HotpotQA. In Table 2  ###reference_###, we present results using GPT-4 for both the agents and MetaReflection. We observe that MetaReflection help us achieve gains up to  for CoT (GT),  for CoT (Distractor), and  for ReAct, over the respective test sets. Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works Ouyang and Li (2023  ###reference_b8###); Ye et al. (2023  ###reference_b23###).\nIn Table 3  ###reference_###, we report results when using GPT-3.5-Turbo to power the client agents.\nWe see gains of upto  gains for CoT (GT),  for CoT (Distractor) and\n for the ReAct case.\nHere, we observe that batching doesn’t strictly improve the performance.\nExamining the data qualitatively, this difference can be attributed to\nthe nature of the instructions generated in the two settings.\nIn general we observe that with a small batch size, MetaReflection produces a\nlarge amount of very specific instructions.\nOn the contrary batching helps generalize these instructions into more widely\napplicable rules.\nGPT-4, being more powerful than GPT-3.5-Turbo, is able to better follow these\nabstract instructions, while specific instructions work better for\nGPT-3.5-Turbo.\n// Chain-of-thought\n\n(A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.\n\n(B) If the context suggests multiple valid answers, choose the one that best matches the question’s wording and the most direct information provided. \n\n// React\n\n(C) When a question asks for a specific detail such as a ’full name’, ensure to find and provide that exact information. Don’t make assumptions based on limited or incomplete information.\n\n(D) If you’re not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy. The information might be located further down the page.\n\n(E) When a question involves comparison, such as ’who received more rewards’, ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.\n\n(F) Be mindful of potential spelling errors or variations in the names of entities. If a search for a specific term doesn’t yield results, consider possible alternative spellings or forms of the term.\nConsider an example question from Figure 6  ###reference_###.\nThe question seeks information about the product made from Cassava and served with palm nut soup.\nThe context presented within the CoT (Distractor) setting includes articles about\nAkpu and Fufu, both of which are quite similar, being made\nfrom Cassava paste.\nHowever, the key distinction lies in Fufu being served with palm nut\nsoup, while Akpu is served with Esupi soup.\nThe baseline CoT agent returns the incorrect response on this question: it is\ndistracted by the related but incorrect articles, and makes an incorrect\nassumption and jumps to the wrong conclusion.\nThe MetaReflection technique learns an instruction that suggests looking for\nmultiple valid answers and selecting the one most related to the question.\nWhen inferring with the meta-reflection instructions, it is clear from the\nthought that the agent did encounter the misleading answers, but was able to\nproduce the right one by focusing on the specific key point “served with palm\nnut soup\" mentioned in the question.\nSimilarly, in the ReAct case (see Figure 1  ###reference_###), we see the\nlearned instruction enhancing search strategy by looking into the information\nfurther down the page rather looping around.\nThis rule further aids the model in successfully concluding the trial where it\nwas previously failing.\nThe model uses the rule to explicitly guide the action space to look further\ndown the context page and look up the right keyword, leading to the correct\nresponse, Bruce Dickinson.\nIn contrast, in the baseline attempt, it ran out of trials by getting stuck in a loop.\nAs we can see from the results, meta-reflection can produce significant\nimprovements in accuracy in the question answer setting.\nThis is especially promising given that the dataset was sampled using an\nadversarial sampling technique.\nThe HotpotQA domain also shows the diversity of instructions learned by\nMetaReflection—a small selection of instructions learned in the CoT and\nReAct settings are shown in Figure 5  ###reference_###\nWe have instructions that are:\n\n\ni. specifically tuned to satisfy the overly strict rubric of the\nHotpotQA dataset (A);\n\nii. domain-specific instructions for specific one-step actions in a RL trajectory (C);\n\niii. directly the high-level strategy to be taken by the trajectory (D, E); and\n\niv. for disambiguating answers (B) and questions (E).\n\n\nFurther, the results on GPT-3.5-Turbo experiments reveal that MetaReflection can be useful to enhance the performance of smaller models by providing instructions rich in specific insights from a more powerful LLMs like GPT-4. This shows some resemblance to task-specific distillation and can be interesting to explore further in future works."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Vulnerability Detection in IaC Files",
            "text": "Infrastructure-as-Code (IaC) is a popular method of configuring cloud infrastructures,\non platforms such as Azure and AWS, using a configuration coding language.\nThese configuration files can declare resources such as virtual machines with\nspecific capabilities, virtual networks and subnets, and data stores.\nIaC presents an alternative to\nthe traditional ways of configuring cloud infrastructures, such as using a web-based graphical interface.\nThere are numerous IaC platforms currently available for various cloud computing platforms.\nHere, we focus on Terraform, a leading IaC platform by\nHashicorp HashiCorp (2023  ###reference_b5###), as well as Azure, the cloud computing platform by Microsoft.\nRelated Terraform resource declarations are grouped together into Terraform\nmodules which act as a basic, reusable configuration component.\nCloud infrastructures are prone to security vulnerabilities such as open ports and\nexposed administrator accounts Tenable (2023  ###reference_b15###  ###reference_b15###).\nVulnerability detection via static analysis of IaC files\nis a hard problem due to the expressivity of the configuration language, the\ncomplexity of configurations and the diversity of the resources being\nhandled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft\nAzure).\nFurther, Terraform uses a low-resource language - HashiCorp Configuration Language (HCL).\nTerrascan Tenable (2023  ###reference_b15###  ###reference_b15###) is a static analyzer for detecting security\nvulnerabilities in Terraform modules, and supports over  security policies,\nincluding  policies specific to Azure.\nFigure 2  ###reference_###  ###reference_### shows the description and definition of a\nTerrascan policy that checks if every Azure virtual network subnet is configured\nwith a corresponding network security policy.\nNote that the Terrascan policy is syntactic, i.e., it is checking for a\ndeclaration of an azurerm_virtual_network with a field named\nsubnet, and so on.\nHence, Terrascan-like static analysis based vulnerability detection is fragile\nand prone to both false positives and false negatives due to being sensitive to\nsyntax.\nThe task at hand is to check if a given Terraform module violates\na given Terrascan policy.\nDescription: Ensure that Azure Virtual Network subnet is configured with a Network Security Group\n\nDefinition:\nWe collected  Terraform modules by mining GitHub repositories for IaC code\nwritten in HCL.\nThese repositories corresponded to a diverse range of applications including\nload balancers, machine learning operations managers, and domain-specific\ndata-stores.\nFor policies, we selected the  most commonly violated Terrascan policies.\nOf the  module-policy pairs, we eliminated a significant fraction of cases\nwhere the policies were not applicable to the module.\nFor example, if the policy was for a specific resource type and the module did\nnot contain declarations of that resource type, the pair was eliminated.\nAfter this process, we were left with  module-policy pairs, for which we\nmanually annotated whether the module violated the policy (see\nTable 1  ###reference_###  ###reference_### for the exact breakdown).\nNote that this ground-truth annotation was with respect to the description of\nTerrascan policy, not the definition—that is, we use the intention behind the\npolicy, not the letter of the definition.\nThat is, we do not take the output of Terrascan as ground truth as it can be\ninaccurate, and instead manually examine if the policy (as per description) is\nviolated.\nThis data was then split into train and test sets in a  ratio per policy,\ntaking care to balance the vulnerable and non-vulnerable classes.\nAs a baseline language agent, we use GPT-4 with an appropriate prompt that\nprovides the code of the Terraform module and the description of the Terrascan\npolicy, and asks if the module is vulnerable.\nWhile training, the agent is given a 0-1 feedback on whether its response is\ncorrect or not, and the model is asked to self-reflect if the\nresponse is incorrect.\nFor each policy, we run the MetaReflection algorithm on the training set and\nreport the accuracy numbers for both the baseline agent and the agent with the\ninstructions learned through MetaReflection.\nWe also compare to LLMInstruction as another baseline—here the\nlanguage model is asked to come up with instructions for a task given its description (Figure  3  ###reference_###  ###reference_###),\nand then these instructions are provided when the task is being performed.\nYou are an expert in [Task]. Given the following task description [and examples] come up with a set of instructions that can help you perform the task effectively.\n\nTask Description: ...\nThe results of the experiment are summarized in Tables 1  ###reference_###  ###reference_###\nand 2  ###reference_###  ###reference_### (last  cols).\nOn the whole, across all policies, meta-reflection shows a  accuracy\nimprovement over the baselines depending on the batch size.\nAs Table 1  ###reference_###  ###reference_### shows, meta-reflection provides consistent\ngains in accuracy for all policies over the GPT-4 baseline, with\n in the best case.\nThe precision with MetaReflection is significantly better for all\npolicies, while the recall decreases for some.\nWe discuss the case of security policy reme_noSecurityGroupAssociated\nfrom Figure 2  ###reference_###  ###reference_###, i.e., that all Azure virtual network\nsubnets are configured with a network security group (NSG).\nThe main difficulty here is that HCL and Terraform offer many different ways of\n\n\n(a) associating a subnet with a virtual network, and\n\n(b) associating a NSG with a subnet.\n\n\nBy default, the baseline GPT-4 agent fails to handle certain ways of specifying\nthese associations, while spuriously assuming certain other associations.\nIn Figure 4(a)  ###reference_sf1###  ###reference_sf1###, the baseline consistently failed to\nrecognize a subnet-NSG association expressed using Method 2, i.e., using an\nexplicitly declared association.\nOn the other hand, it mis-identified declarations similar to the one in\nFigure 4(b)  ###reference_sf2###  ###reference_sf2### as valid subnet-NSG\nassociations—here, the NSG is associated with a virtual machine’s network\ninterface (that is connected to the subnet) instead of the subnet itself.\nThese limitations lead to both false positives and false negatives.\nWith meta-reflection, we are able to learn the instructions in\nFigure 4(c)  ###reference_sf3###  ###reference_sf3###, using which the agent easily handles these\nkinds of cases.\n4. Remember that the association between \"azurerm_virtual_network\" and a NSG may\nnot be direct. It could be done through a separate resource block such as\n\"azurerm_subnet_nsg_association\"\n...\n7. Do not confuse NSG associations with network interfaces of VMs and the subnet\nof the Azure Virtual Network. The policy specifically requires the NSG be\nassociated with the subnet.\n\nAs the above exemplar case shows, MetaReflection is able to learn very\ndomain-specific instructions to fix both false positives and false negatives.\nOther instructions include aspects like handling of wildcards for port numbers,\nstep-by-step strategies for specific policies, etc.\nNote that these instructions not only include planning (or trajectory directing)\ninstructions, but also grounding instructions—i.e., external facts that are\nnot initially available.\nIn general, the experimental results show that meta-reflection is able to\nreduce the number of errors, i.e., improve the accuracy across a broad range of\ncases.\nHowever, one noticeable issue from the above results is the drop in recall for\nseveral policies.\nWhile the high recall in the baseline is artificial, coming at the cost of low\nprecision, this is still an important issue to address.\nOur 0-1 feedback to the self-reflection agent does not state that false\nnegatives are worse than false positives in the security domain.\nIn the future, we plan to explore domain-specific feedback and self-reflection\nmechanisms that can account for the nature of errors, as well as better versions\nof the  function that are aware of such\ndomain-specific preferences."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "HotpotQA",
            "text": "HotpotQA Yang et al. (2018  ###reference_b20###) is an open-domain factual question answering\ndataset consisting of K question-answer pairs. The original paper proposes to\nuse the data in  settings:\n\n\n(a) Distractorsetting - where each question is to be answered using \nwikipedia article excerpts; and\n\n(b) Full-Wikisetting which is a retrieval and reasoning task, where a\ngiven question is supposed to be answered after retrieving relevant context from\nwikipedia.\n\n\nNotably, an answer is marked correct only if it matches exactly with the ground truth.\nSimilar to Shinn et al. Shinn et al. (2023  ###reference_b13###), we design the following agents that operate over the dataset: \n\n(a) ReAct- for the Full-Wiki setting\n\n(b) CoT(Distractor) - for the Distractor Distractor setting\n\n(c) CoT(GT) - a variant of CoT (Distractor) with access to only ground truth articles.\nFor each agent setting, we adversarialy sample subsets of the HotpotQA train\nsplit of K samples to create train and test sets. To perform adversarial\nsampling, we first identify samples where the base agent fails consistently in a\ngiven setting.\nOn these failing examples we perform upto  self-reflection trials to get the\nmodel to the right response.\nIf the agent is not able to get to the correct response even after\nself-reflection, we discard these samples.\nThis strategy ensures that we get a set of hard examples in which the agents\nfail most of the times to get to the right answer in a single try, while also\nmaking sure that we filter examples that may be noisy due to missing context,\nincorrect questions etc.\nTo account for randomness and given our computational budget, we sample 40 and\n80 examples for the ReAct train set and test set respectively. For CoT settings, we pick  and  example respectively.\nWe reuse the CoT agent from Wei et al. (2023  ###reference_b16###  ###reference_b16###) for the\nchain-of-thought experiments and use a re-implementation of Yao et al. (2023b  ###reference_b22###  ###reference_b22###)\nfor the ReAct experiments.\nThe ReAct agent is allowed at most  Action steps after which the\ntrajectory is automatically determined to be a failure.\nSimilar to Section 3.1  ###reference_###  ###reference_###, we evaluate HotpotQA configurations for:\n\n\n(a) MetaReflectionwith batch sizes , , and ; and\n\n(b) GPT-4and LLMInstruction as baselines.\n\n\nIn addition to this, we also evaluate variants of the agents powered by GPT-3.5-Turbo instead of GPT-4, while using GPT-4 for MetaReflection.222A similar experiment on the IaC domain wasn’t possible due to large context length of the Terraform modules\nWe find that the generated MetaReflection instruction consistently improved performance across different agent settings for HotpotQA. In Table 2  ###reference_###  ###reference_###, we present results using GPT-4 for both the agents and MetaReflection. We observe that MetaReflection help us achieve gains up to  for CoT (GT),  for CoT (Distractor), and  for ReAct, over the respective test sets. Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works Ouyang and Li (2023  ###reference_b8###  ###reference_b8###); Ye et al. (2023  ###reference_b23###  ###reference_b23###).\nIn Table 3  ###reference_###  ###reference_###, we report results when using GPT-3.5-Turbo to power the client agents.\nWe see gains of upto  gains for CoT (GT),  for CoT (Distractor) and\n for the ReAct case.\nHere, we observe that batching doesn’t strictly improve the performance.\nExamining the data qualitatively, this difference can be attributed to\nthe nature of the instructions generated in the two settings.\nIn general we observe that with a small batch size, MetaReflection produces a\nlarge amount of very specific instructions.\nOn the contrary batching helps generalize these instructions into more widely\napplicable rules.\nGPT-4, being more powerful than GPT-3.5-Turbo, is able to better follow these\nabstract instructions, while specific instructions work better for\nGPT-3.5-Turbo.\n// Chain-of-thought\n\n(A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.\n\n(B) If the context suggests multiple valid answers, choose the one that best matches the question’s wording and the most direct information provided. \n\n// React\n\n(C) When a question asks for a specific detail such as a ’full name’, ensure to find and provide that exact information. Don’t make assumptions based on limited or incomplete information.\n\n(D) If you’re not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy. The information might be located further down the page.\n\n(E) When a question involves comparison, such as ’who received more rewards’, ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.\n\n(F) Be mindful of potential spelling errors or variations in the names of entities. If a search for a specific term doesn’t yield results, consider possible alternative spellings or forms of the term.\nConsider an example question from Figure 6  ###reference_###  ###reference_###.\nThe question seeks information about the product made from Cassava and served with palm nut soup.\nThe context presented within the CoT (Distractor) setting includes articles about\nAkpu and Fufu, both of which are quite similar, being made\nfrom Cassava paste.\nHowever, the key distinction lies in Fufu being served with palm nut\nsoup, while Akpu is served with Esupi soup.\nThe baseline CoT agent returns the incorrect response on this question: it is\ndistracted by the related but incorrect articles, and makes an incorrect\nassumption and jumps to the wrong conclusion.\nThe MetaReflection technique learns an instruction that suggests looking for\nmultiple valid answers and selecting the one most related to the question.\nWhen inferring with the meta-reflection instructions, it is clear from the\nthought that the agent did encounter the misleading answers, but was able to\nproduce the right one by focusing on the specific key point “served with palm\nnut soup\" mentioned in the question.\nSimilarly, in the ReAct case (see Figure 1  ###reference_###  ###reference_###), we see the\nlearned instruction enhancing search strategy by looking into the information\nfurther down the page rather looping around.\nThis rule further aids the model in successfully concluding the trial where it\nwas previously failing.\nThe model uses the rule to explicitly guide the action space to look further\ndown the context page and look up the right keyword, leading to the correct\nresponse, Bruce Dickinson.\nIn contrast, in the baseline attempt, it ran out of trials by getting stuck in a loop.\nAs we can see from the results, meta-reflection can produce significant\nimprovements in accuracy in the question answer setting.\nThis is especially promising given that the dataset was sampled using an\nadversarial sampling technique.\nThe HotpotQA domain also shows the diversity of instructions learned by\nMetaReflection—a small selection of instructions learned in the CoT and\nReAct settings are shown in Figure 5  ###reference_###  ###reference_###\nWe have instructions that are:\n\n\ni. specifically tuned to satisfy the overly strict rubric of the\nHotpotQA dataset (A);\n\nii. domain-specific instructions for specific one-step actions in a RL trajectory (C);\n\niii. directly the high-level strategy to be taken by the trajectory (D, E); and\n\niv. for disambiguating answers (B) and questions (E).\n\n\nFurther, the results on GPT-3.5-Turbo experiments reveal that MetaReflection can be useful to enhance the performance of smaller models by providing instructions rich in specific insights from a more powerful LLMs like GPT-4. This shows some resemblance to task-specific distillation and can be interesting to explore further in future works."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "MetaReflection",
            "text": "Algorithm 1  ###reference_### shows the outline of the MetaReflection process. At its core, the algorithm works by starting with an empty set of instructions and iteratively improving the instructions using small training batches. Being built upon human annotation of each model output without feedback, the MetaReflection procedure uses the same components at its core:\n\n(a) a client agent (i.e., an RL actor) that is based on a language model,\n\n(b) an evaluation or feedback agent that can provide feedback on the client agent’s trajectory, and\n\n(c) a self-reflection agent that produces a verbal reinforcement given a RL trajectory.\n\nAdditionally, we assume that the client agent can be parameterized by a set of instructions in addition to the standard task description. In our implementation and experiments, we use several different client agents based on ReAct Yao et al. (2023b  ###reference_b22###), CoT Wei et al. (2023  ###reference_b16###), and a vanilla one-shot language model. For the feedback agent, we consider multiple variants based on the application domain: a 0-1 boolean feedback agent and an exact match checker. The meta-reflection agent is designed to take as input a prior set of instructions, a set of self-reflections, and the training data, and will produce an updated set of instructions. For the meta-reflection agent, we use a standard language model with a prompt that instructs the model to observe the reflections, the training data, and produce new non-case specific instructions. Further, the prior instructions are also passed as input so that the output is a generalization of the prior instructions. In our implementation, this meta-reflection and generalization are done in the same prompt for efficiency. then combined with previous instructions are also possible. Alternatively, new instructions can be generated first and then combined with existing ones. We specify that the instructions need to take the form of a list. Hence, the meta-reflection agent typically either\n\n(a) updates the list by adding a new item, or\n\n(b) combines one or more previous items with learnings from the self-reflections to produce a shorter list.\n\nFor example, one meta-reflection instruction learned during our HotpotQA experiments suggested including the profession when searching for a person to narrow down results. In a subsequent batch, the self-reflection agent produces a reflection that mentions adding search terms like release date when searching for movies. The meta-reflection agent may combine the previous instructions with the current self-reflections either by appending a new item to the list clarifying the strategy to search for movies, or may generalize the previous item to something like “When searching for specific entities, use additional contextual information to augment the primary search terms with secondary keywords corresponding to the characteristics of the entity\". In each iteration, after the meta-reflection step, we validate the quality of the new instructions. Due to sparse reward signals leading to poor self-reflections or over-generalization of the meta-reflection instructions, we may end up with instructions that are of a poorer quality than the prior instructions. The poorer instructions may also be due to general capricious, unpredictable nature of large language models. Therefore, we validate the new instructions by testing them on training data to ensure that they perform better than the prior instructions. Ideally, we would do this validation over the full training data or a substantial held-out dataset. However, in our case, we only validate on the current batch to balance quality of instructions and efficiency. As an example, in the previous paragraph the meta-reflection step replaced the specific instruction on how to search for persons with a more general instruction on how to search for entities. However, it is possible that these general instructions are too vague (especially for smaller, less capable models) and the client agent is not able to apply them correctly to the case of searching for persons. In such a case, we do not use the new updated instructions and revert back to the prior instructions. In practice, we use several other augmentations to the meta-reflection procedure in Algorithm 1  ###reference_###. These augmentations are not a core part of the technique, but instead optimizations that may help in specific cases and domains. The first of these is to use certain parts of the full trajectory in addition to the self-reflections during the meta-reflection step in line 10  ###reference_10###. For example, if the client agent is a CoT agent, it may be helpful to append the inner thought steps from the trajectory to the self-reflections. Another augmentation is to use multiple attempts at meta-reflection for each batch. If the validation step fails at line 11  ###reference_11###, instead of rejecting the new instructions altogether, we may rerun the loop with the same batch, but this time initializing the client agent with instead of . This process may be repeated multiple times till the validation step succeeds—in practice, we"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Large language models (LLMs) form a critical component in the development of AI-based systems. However, crafting prompt instructions can be a non-trivial task. In this paper, we have taken a significant step forward to improve this process by introducing MetaReflection. This innovative approach employs past self-reflections to learn instructions used to guide LLMs. In our experiments, we show that instructions learned using MetaReflection significantly improve the accuracy of GPT-4 predictions.\nWe believe that integrating LLMs with domain-specific insights, such as our use of past self-reflections, can solve previously challenging problems. In future work, we plan to explore the application of MetaReflection in other contexts and refine its capabilities, aiming to enhance the performance of language models across diverse domains."
        }
    ]
}