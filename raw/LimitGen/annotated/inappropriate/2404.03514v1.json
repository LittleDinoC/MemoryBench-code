{
    "title": "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach",
    "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval.\nAiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs’ pretraining data.\nHowever, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pre-training data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings.\nWe hypothesize that token embeddings are able to capture the model’s intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings.\nExtensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Retrieval-augmented large language models (RALMs) excel in various NLP tasks  Li et al. (2022  ###reference_b9###); Yasunaga et al. (2023  ###reference_b18###); Lin et al. (2022  ###reference_b10###); Huang et al. (2024  ###reference_b5###). However, the knowledge provided by the retrieval process is not always useful for improving the LLMs’ prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. For example, when posed with commonsense questions or queries within the knowledge scope of their pre-training data, LLMs might accurately respond without necessitating retrieval. Moreover, the retrieval process can incur additional computational costs and latency, which could be avoided when the model’s intrinsic knowledge has already been adequate Mallen et al. (2023  ###reference_b11###).\n###figure_1### So motivated, previous work Mallen et al. (2023  ###reference_b11###) has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs’ pretraining data.\nSuch a data-aware approach is developed based on the heuristic that it is easier for LLMs to capture knowledge on entities that are frequently mentioned during pre-training. This adaptive approach can save context length, thereby reducing latency and cost during LLM inference, while also mitigating performance degradation caused by redundant retrievals in LLMs.\nHowever, as shown in Figure 1  ###reference_###, the pre-training data might not always be available due to privacy and commercial constraints Shao et al. (2023  ###reference_b14###); Huang et al. (2022  ###reference_b6###), especially when dealing with proprietary or sensitive datasets.\nThis makes it infeasible to utilize the data-aware approaches in real business scenarios.\nIn addition, the pre-training data are not necessarily aligned with the knowledge learned by LLMs.\nFor example, the pre-training datasets may contain conflicting descriptions regarding the same entity Gu et al. (2023  ###reference_b4###). In such a case, it is uncertain whether the model is knowledgeable about the entity, even if it has been frequently mentioned in the pre-training data.\nIn this paper, we propose a novel model-aware approach to make the judgment about when to do/skip the retrieval.\nInstead of requiring access to the pre-training data, we leverage the pre-trained token embeddings that are believed to explicitly reflect the model’s knowledge.\nIn achieving this, we develop a simple yet effective representation-informed classifier that is capable of recognizing samples that are (not) in need of retrieval.\nThis approach circumvents the risks associated with maintaining pre-training data via only requiring access to the pre-trained token embeddings, offering a safer and more straightforward way to judge the need for retrieval augmentation.\nIn summary, the main contributions of this work are as follows:\nWe identify the privacy constraints inherent in Retrieval-augmented LLMs, and unveil the limitations of the existing data-aware approach.\nWe introduce a novel model-aware approach that decides when to do/skip the retrieval process, by leveraging the token embeddings intrinsic to the model. This approach alleviates the dependency on the accessibility of pretraining data.\nExtensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach compared to the data-aware baseline approach."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Model-Aware Adaptive Retrieval Augmentation",
            "text": "In the context of open-domain entity-centric Question Answering (QA), the primary objective of the RALM method is to ascertain whether a given entity requires retrieval augmentation when the QA system is posed with a specific entity-centric question (e.g., ‘Louisiana’ is the entity of the question ‘What is the capital of Louisiana?’). The core of this task is to determine whether language models already possess knowledge of the entity, thereby deciding if there is a need to retrieve external knowledge bases to enhance the model prediction. This adaptive retrieval approach can effectively save context length, thereby reducing latency during LLM inference. Besides, it can mitigate performance degradation caused by redundant retrievals in LLMs Mallen et al. (2023  ###reference_b11###).\nOur rationale for utilizing entity embeddings as an indicator of an LLM’s knowledge about an entity is grounded in extensive prior research. Gao et al. (2019  ###reference_b3###); Li et al. (2020  ###reference_b8###); Cai et al. (2021  ###reference_b2###) have collectively established a significant correlation between entity embedding distribution and entity frequency in pre-training data across various models, from BERT to the GPT series. Meanwhile, Mallen et al. (2023  ###reference_b11###) validates the effectiveness of employing the frequency of entities in pretraining data as a criterion for determining the need for retrieval.\nThus, those existent foundational studies informed us that methods leveraging entity embeddings are effective for retrieval augmentation decisions. Then we developed a simple thresholding based on entity length, aiming to parallel the DM method Mallen et al. (2023  ###reference_b11###). This method aids in determining when an entity requires retrieval augmentation based on its length characteristics.\nTo ensure clarity, we define as the set of entities within the dataset; as a specific entity, with denoting its index in set ; as the tokenized representation of entity using the GPT/Llama2 tokenizer; as the first-layer token embedding of the tokenized entity ; C as a binary decision threshold; as the binary outcome (where indicates the need for retrieval augmentation, and indicates otherwise).\nGiven an entity from the set , we tokenize it using the LLM’s tokenizer (e.g., GPT-Neo/Llama2 tokenizer) to obtain its tokenized form, . The length of this tokenized form, , is then compared against a predefined threshold.\nIn alignment with previous work Mallen et al. (2023  ###reference_b11###), we curate a subset, denoted as , by randomly sampling the entity-centric data from every sub-relation dataset.\nEach entity in is labeled based on its length in comparison to the threshold, denoted as and . These serve as the basis for determining retrieval requirements.\nAfter establishing the threshold, we employ it to predict the binary outcome when presented with a new entity . This prediction assists in determining whether the entity requires retrieval augmentation for open-domain entity-centric QA tasks.\nOur novel model-aware retrieval augmentation method offers an efficient way to determine the need for retrieval augmentation in open-domain entity-centric QA scenarios.\nIn contrast to the data-aware method requiring the availability of the pre-training data, our method focuses on the analysis of entity token length, holding the potential to yield accurate decisions, and ensuring the applicability and scalability in real-world QA systems."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Following the experiment setting as  Mallen et al. (2023  ###reference_b11###), we choose POPQA, an entity-centric open-domain QA dataset. We have the following research questions (RQs) to guide the experiments:\nRQ1: Given the presence of additional privacy-related constraints, how does the accuracy of our model compare to the state-of-the-art? It’s important to note that while SOTA methods require access to pre-training data, for a fair comparison, we assume that the data-aware methods we compare have access to the frequency of pre-training data.\nRQ2: Regarding the adaptability of our method, when an LLM is fine-tuned, with modified memorization capacity of entities,\ncan our model accurately determine the instances of entity necessitating retrieval?\nFor evaluation, we use accuracy as our primary metric for marking a prediction as correct if any substring of the prediction is an exact match of any of the gold answers. In this section, we will perform an extensive experimental analysis of our model-aware framework."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Model-aware vs. Data-aware QA Strategies Across Diverse Model Capacities (RQ1)",
            "text": "Given that the data-aware method Mallen et al. (2023  ###reference_b11###) requires access to pre-training data, it cannot be compared in the privacy-preserving setting, in order to maintain consistency with the data-aware method, we evaluate our framework across models of varying capacities: GPT-Neo (1.3 billion), GPT-Neo (2.7 billion) Black et al. (2022  ###reference_b1###), Llama 2 (7 billion) Touvron et al. (2023  ###reference_b16###), and Llama 2 (13 billion). In this evaluation, we do not perform any fine-tuning on models.\nWe utilize the POPQA dataset Mallen et al. (2023  ###reference_b11###), which comprises 14k questions designed for capturing factual details possibly overlooked in more mainstream QA datasets.\nMore details refer to Appendix B  ###reference_###.\n###figure_2### Figure 2  ###reference_### illustrates the overall performance on POPQA. Across multiple model sizes, the MM (model-aware method) consistently rivals or even outperforms the DM (data-aware method). This suggests our model-aware method can serve as a viable alternative to the data-aware method.\nThe data-aware method essentially requires calculating word frequency for all entities within each sub-dataset (e.g., genre dataset) in the pre-training data. Based on the QA task accuracy, it then deduces an optimal threshold to determine whether or not to retrieve an entity. In contrast, our model-aware technique trains an NN classifier directly using the token embeddings of entities. This eliminates the need to access/interact with any pre-training data.\n###table_1### ###figure_3### To delve deeper into the discussion of which scenario our model-aware or data-aware method performs better, we have chosen the most representative sub-datasets: ‘mother’ and ‘capital of’, and visualized them. In Figure 3  ###reference_###, the vertical axis, labeled ‘Normalized Entity Token logits’, represents the normalized neural network output for each entity. The horizontal axis labeled ‘True’ represents the logits distribution of the Llama 2 for the correct QA samples, while ‘False’ represents the logits distribution for the incorrect samples of the QA (i.e., questions that require retrieval-augmentation). From the violin plot, it can be observed that the logits distribution for the ‘mother’ dataset is relatively concentrated, whereas the ‘capital of’ logits distribution may exhibit a bimodal nature, indicating two primary prediction categories. Meanwhile, we can observe that the ‘capital of’ dataset exhibits a distinct peak in entity frequency. This suggests that LLMs are more likely to encounter and learn from these high-frequency entities during the pre-training phase, resulting in high-quality QA outputs for these entities. In contrast, the ‘mother’ dataset might encompass more ambiguous or unspecified entities, making methods based on model-aware embedding challenging to distinguish on such datasets.\nBuilding on data-aware model’s configurations Mallen et al. (2023  ###reference_b11###), we also incorporate two prominent retrieval systems into our research: BM25 Robertson and Zaragoza (2009  ###reference_b13###) and Contriver Izacard et al. (2022  ###reference_b7###). While BM25 is a static, term-based retriever that functions without prior training, Contriever undergoes pre-training on vast unlabeled corpora and is subsequently refined using the MS MARCO dataset Nguyen et al. (2016  ###reference_b12###). We also delve into the GenRead Yu et al. (2023  ###reference_b19###) parametric augmentation technique, prompting language models to produce contextual documents rather than exclusively depending on retrieval for responses. The comparative performance of these augmented methods on Llama 2-7B is presented in Table 1  ###reference_###.\n‘Llama 2’ indicates the baseline LLM’s QA accuracy. ‘RA Llama 2’ connotes the integration of retrieval-based augmentation for all QA queries. ‘DM’ Mallen et al. (2023  ###reference_b11###) stands for the adaptive data-aware method, while ‘MM’ signifies our model-aware method which does not need to access pretraining data. It is worth highlighting that our model-aware approach yields competitive results across various retrieval-augmentation configurations."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adaptability of the Model-aware Method in Fine-tuned LLMs (RQ2)",
            "text": "To evaluate our model-aware method’s adaptation performance, we fine-tune a GPT-Neo 1.3B model. Specifically, from the POPQA dataset, we extract 70% of the questions related to entities from each sub-dataset for the purpose of fine-tuning the GPT-Neo model, aiming to enhance the model’s retention of this specific knowledge. The remaining 30% are utilized as a test set to compare the effectiveness of DM and MM. For DM, entity frequencies were calculated using their original Wiki frequency.\n###table_2### As shown in Table 2  ###reference_###, MM exhibits a slightly better accuracy rate than DM on the fine-tuned GPT-Neo 1.3B model. This indicates that our model-aware approach remains effective even after the model undergoes fine-tuning. This result further underscores the robustness and adaptability of our model-aware method compared to the data-aware strategy.\nThis is because methods requiring fine-tuning, like the DM method, would cause a dramatic change in the entity frequency since there typically exists a notable difference between the entity frequencies of the pre-training data and the fine-tuning data. Therefore, DM would exhibit less stability compared to MM which does not directly rely on the frequency information, especially in the situations where multiple rounds of fine-tuning are required. The inferior performance of DM as shown in Table 2 can therefore be attributed to the aforementioned entity frequency change and the associated instability.\nGiven the prevailing trend of extensively fine-tuning large language models for specific downstream tasks, this stability and adaptability highlight a distinct advantage of our method."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a novel model-aware approach to tackle challenges in retrieval-augmented LLMs. Leveraging token embeddings that capture the model’s knowledge, we offer an efficient and privacy-conscious solution. Unlike methods dependent on inaccessible or sensitive pretraining data, our approach provides a flexible, scalable, and secure means to assess retrieval requirements. This innovation has broad implications for real-world applications, harmonizing efficiency and privacy while upholding model output quality."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This work focuses on an entity-centric adaptive retrieval-augmentation technique. It might not work on document-centric QA tasks. We acknowledge the need for future research to explore the extension of our method to a wider range of QA tasks. Besides, how to particularly improve the performance of the retrieval model is beyond the scope of our paper, and has yet to be explored."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "It is important to note that LLMs can still generate incorrect (hallucination) or biased outputs, even when they are retrieval-augmented. Therefore, it is always important to verify the outputs of language models with other sources of information."
        }
    ]
}