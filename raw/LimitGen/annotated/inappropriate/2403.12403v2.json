{
    "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
    "abstract": "Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content.\nGiven the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech.\nAlthough several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design.\nTo address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.\nOur framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable.\nOur comprehensive evaluation on a variety of English language social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability.\nAll code and data will be made available at https://github.com/AmritaBh/shield.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social media has become a platform of content sharing and discussions for a varied range of individuals with differing cultural and continental backgrounds. People use social media platforms to exchange information, and they frequently engage in dialectal conversations. These discussions are not always peaceful, they can degenerate into unpleasant altercations and bigoted arguments. Thus, social media platforms often become a host for hate speech.\nHate speech is described as any deliberate and purposeful public communication meant to disparage a person or a group by expressing hatred, disdain, or contempt based on their social attributes (e.g., gender, race). In extreme cases, hate speech may often lead to real world harms such as hate crimes, for example the anti-Asian hate crimes during the COVID-19 pandemic Findling et al. (2022  ###reference_b16###); Han et al. (2023  ###reference_b18###). Therefore, it is essential to have automatic hate speech detection and moderation in place to maintain the integrity of social media platforms as well as to mitigate negative impacts in real-world scenarios such as increased violence towards minorities Laub (2019  ###reference_b29###).\nGiven that the issue of hate speech on social media is a well-established problem, there have been several works to detect such online hate-speech  Schmidt and Wiegand (2017  ###reference_b39###); Del Vigna12 et al. (2017  ###reference_b11###). While state of the art hate speech detection models have been able to achieve good performance on benchmark evaluation datasets, most of these models are built using transformer-based pre-trained language models or other deep neural network type models Sheth et al. (2023b  ###reference_b41###) that are not interpretable or explainable. However, the task of hate speech detection is a very sensitive task, and explainability of automated detectors is an essential and desirable feature. Model interpretability is essential not only for end-user understanding but also for understanding biased predictions, domain shifts, other errors in the prediction, etc.\nWhile incorporating qualities of interpretability directly into deep neural network models such as pre-trained language model based detectors is challenging, one way to potentially perform this is by using an auxiliary model to provide explanations or rationales, that are subsequently used in training the detection model. This type of a method has been proposed and used in the FRESH framework  Jain et al. (2020  ###reference_b23###), where the authors use two disjoint networks, one for extracting the task-specific rationales, and then another that leverages those rationales to learn the classification task, thereby enabling faithful interpretability by construction.\nInspired by this work, we propose a framework, where we use LLMs as the extractor model: we leverage the textual understanding and instruction-following capabilities of state-of-the-art LLMs to extract features from the input text, that is used to augment the training of a separate base hate speech detector, thereby facilitating faithful interpretability. Overall, our contributions in this paper are:\nWe propose SHIELD, a framework that leverages LLM-extracted rationales to augment a base hate speech detection model to facilitate faithful interpretability.\nWe evaluate the goodness of LLM-extracted features and rationales, and measure the alignment of such with human annotated rationales.\nThrough comprehensive experiments on both implicit and explicit hate speech datasets, we show how SHIELD retains detection performance even after training with rationales for increased interpretability, despite the expected interpretability-accuracy trade-off."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Our SHIELD Framework",
            "text": "###figure_1### We show our proposed SHIELD framework in Figure 1  ###reference_###. In this section, we describe our framework in detail, elaborating on each of the components.\nOur framework uses the state-of-the-art instruction-tuned large language models (LLMs) in an off-the-shelf manner as textual feature extractors. Although recent work has shown that LLMs struggle to perform the hate speech detection task  Li et al. (2023  ###reference_b30###); Zhu et al. (2023  ###reference_b44###) when used without any additional model or fine-tuning, we hypothesize that we can leverage the textual understanding capabilities of these LLMs to simply extract textual features in the form of rationales. Restricting the use of the LLM to a simple text-level task would ensure that such models are not directly being used for sensitive application tasks such as hate speech detection  Harrer (2023  ###reference_b19###).\nFor a given input text , we use our carefully designed task prompt to prompt the LLM to extract features from the text that promotes a hateful sentiment. In the context of explicit hate speech detection, such features could include categories such as derogatory words, cuss words, etc. Following similar work in  Bhattacharjee et al. (2023b  ###reference_b6###), we also ask the LLM for rationales as to why the label is hateful or non-hateful. To perform this feature extraction, for each input text we prompt the LLM using the following prompt:\n“You are a content moderation bot. Identify the list of rationales, list of derogatory language, list of cuss words that promote a hateful sentiment and respond with non-hateful if there are none. Note: The output should be in a json format.”\nText: [input_text]\nAfter post-processing the outputs, we have a list of  textual features  for the given input text .\nThe next component in our framework is the base hate speech detector which we are trying to augment, such as HateBERT Caselli et al. (2020  ###reference_b8###). HateBERT is a BERT Devlin et al. (2018  ###reference_b12###) model that is specifically fine-tuned on hate speech data. For each input text , instead of obtaining the labels or class probabilities, we take the last layer embedding of the [CLS] token, , essentially containing all the information of the input text, that is relevant for the hate-speech detection task.\nFor the textual features and rationales, , we extracted via the LLM, we use a pre-trained transformer-based language model (PLM), such as BERT to embed these features. PLMs, even without any task-specific fine-tuning, provide rich, expressive latent representations for text. Therefore, we feed in the LLM-extracted textual features into a BERT (specifically, bert-base-uncased111https://huggingface.co/google-bert/bert-base-uncased) model and obtain the last hidden layer embedding of the [CLS] token, and we denote this as .\nFrom the previous two components, for each input text , we have two embeddings: text embedding  from the base hate speech detector, and feature embedding  from the feature embedding BERT model. To combine these two, we simply concatenate these embeddings:\nNote that while authors in  Jain et al. (2020  ###reference_b23###) only use the extracted rationales in the subsequent detector model, we use a concatenated view in order to incorporate additional contextual features that may be very relevant to determining the hate or non-hate label Ocampo et al. (2023  ###reference_b35###). We then feed this combined embedding  into a feed-forward multi-layer perceptron with two fully connected layers and a ReLU activation Agarap (2018  ###reference_b1###) in between, to project it onto a smaller dimension space. Following previous work Pan et al. (2022  ###reference_b36###); Bhattacharjee et al. (2023a  ###reference_b4###), we do this in order to retain important features and avoid overfitting of the model during training. We denote this MLP as . Finally we compute the batch-wise binary cross entropy loss using the ground truth label  for each input text :\nwhere  is the batch size. Since we are using the BERT feature embedding model just to encode the textual features , we keep this model frozen and train the remainder of the framework with this simple loss."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology and Experimental Settings",
            "text": "In this section, we discuss our methodology in detail including the datasets we included, the baseline models for hate speech detection along with the manual inspection and extraction of features without using any automation or models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "In order to evaluate SHIELD, we use both explicit and implicit hate speech datasets. For explicit hate, we include publicly available benchmark datasets from the following social media platforms: {GAB, Twitter, YouTube, and Reddit}. All these datasets are in the English language. GAB  Mathew et al. (2021  ###reference_b33###) is a collection of annotated posts from the GAB website. It consists of binary labels indicating whether a post is hateful or not. Reddit  Kennedy et al. (2020  ###reference_b24###) is a collection of posts indicating whether it is hateful or not. Twitter  Mathew et al. (2021  ###reference_b33###) contains instances of hate speech gathered from tweets on the Twitter platform. Finally, YouTube  Salminen et al. (2018  ###reference_b38###) is a collection of hateful expressions and comments posted on the YouTube platform.\nWe further pre-process these according to the method followed in  Sheth et al. (2023a  ###reference_b40###), in order to get cleaned binary labels. A summary of the datasets and the distribution of hateful posts and non-hateful posts can be found in Table 1  ###reference_###.\nWe also include implicit hate speech in our evaluation: while subtle forms of abuse may not be perceived as overtly harmful initially, they nonetheless perpetuate similar degrees of damage over time owing to their covert nature. Therefore, the detection of implicit hate speech becomes even more important. For this reason, we evaluate our proposed model on the Implicit Hate Speech Corpus  ElSherief et al. (2018  ###reference_b15###). This dataset encompasses posts compiled from Twitter, annotated as either explicit hate, implicit hate, or non-hate speech. We exclusively utilize implicit hate and non-hate for our binary classification task by manually inspecting and extracting features without using any automation or models."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "We compare our proposed SHIELD framework to a variety of different baselines in order to understand the impact of the augmentation with rationales. We use the following well-known baseline hate speech detection models:\nHateBERT: This is also the base model used in our framework. HateBERT  Caselli et al. (2020  ###reference_b8###) uses over 1.5 million Reddit messages from suspended communities known for encouraging hate speech to fine-tune the BERT-base model. We further fine-tune HateBERT on each dataset and report the performance.\nHateXplain: Similarly, we fine-tune the HateXplain Mathew et al. (2021  ###reference_b33###) model on each of our datasets and report the performance. HateXplain model is trained on hateful posts along with the target community, the rationales, and the portion of the post on which human annotators’ labelling decision is based.\nPEACE: We further extend our comparison on PEACE Sheth et al. (2023b  ###reference_b41###) framework which uses Sentiment and Aggression Cues to detect the overall sentiment of the text.\nCATCH: Furthermore, we compare our model with CATCH  Sheth et al. (2023a  ###reference_b40###) framework which disentangles the input representations into invariant and platform-dependent features.\nChatGPT-1shot: Apart from these hate speech specific detection models, we also compare our framework with an off-the-shelf GPT-3.5 model, to understand how well the LLM performs on the same datasets. We do this in a one-shot manner, i.e., by manually inspecting and extracting features without using any automation or models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "To implement our proposed SHIELD framework, we use PyTorch and the Huggingface Transformers library. As shown in Figure 1, our first component involves manually inspecting and extracting features without using any automation or models. Here, we use OpenAI’s GPT-3.5 (specifically, GPT-3.5-turbo-0613) or otherwise commonly referred to as ‘ChatGPT', since it has been experimented on a variety of NLP tasks with huge success Guo et al. (2024). We access this model via the OpenAI API. For feature/rationale extraction and generation, we set the temperature to 0.1 and top_p to 1. For the Feature Embedding Model, we use a pre-trained, frozen BERT (bert-base-uncased) and for the Hate Speech Detector, we use a pre-trained HateBERT model. We use AdamW optimizer Kingma and Ba (2014) with a learning rate of . Model training was performed on two machines: one with an NVIDIA GP102 [TITAN Xp] GPU with 12 GB VRAM, and another with an NVIDIA A100 GPU with 40GB RAM. For all detection experiments, we use accuracy as the evaluation metric."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "In this section we describe our experiments and elaborate on the experimental results. To explore the feasibility and effectiveness of our proposed SHIELD framework, we aim to answer the following research questions:\nRQ1: How well does ChatGPT perform on our set of hate speech detection datasets?\nRQ2: Can we leverage recent state-of-the-art LLMs to extract features in the form of rationales, and do these rationales align with human judgement?\nRQ3: Can SHIELD effectively retain/improve performance of the hate speech detector while facilitating faithful interpretability?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance of ChatGPT on the hate speech detection task",
            "text": "Several recent works test whether Large Language Models have the potential to reproduce human annotated ground truth labels in social computing tasks Zhu et al. (2023  ###reference_b44###). However, even after extensive pre-training on a large corpus of datasets, where LLMs are expected to perform well in this task, this is not the case. To further evaluate this beyond what other recent works have shown, we carefully craft a one-shot prompt and prompt ChatGPT to classify the input text, given a labeled example in the prompt. The outcome of this prompt is a single label representing hateful text as label “1\" and non-hateful text as label “0\" as shown in Table 2  ###reference_###.\nWe perform this classification using ChatGPT for all 5 datasets and compute the accuracy. We compare the results of this one-shot classification task with the baseline models (as described in Section 3.2  ###reference_###) and show the results in Table 3  ###reference_###. We see a stark difference in the performance of the baseline models vs. ChatGPT-1shot classification accuracies. While performance on the GAB dataset is satisfactory, ChatGPT struggles with the other 4 datasets with ~58-65% accuracy. Similar observations have been reported in other recent work that have investigated the off-the-shelf performance of LLMs in hate speech detection Li et al. (2023  ###reference_b30###); Zhu et al. (2023  ###reference_b44###).\nWhile this shows ChatGPT and possibly other LLMs struggle at hate speech detection when used as a detector directly, these models have also been shown to have impressive textual understanding capabilities. Perhaps, simply using these models to extract features or rationales, instead of performing the entire detection task, might be beneficial. We evaluate this in the following subsection."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Goodness of ChatGPT extracted features or rationales",
            "text": "We are interested to evaluate the textual and contextual understanding capabilities of ChatGPT in order to extract features in the form of rationales from the input text that are meaningful to the task of hate speech detection. Following a similar construction as in  Jain et al. (2020  ###reference_b23###), we use the LLM (i.e., GPT-3.5) as the extractor model, which unlike the extractor model in  Jain et al. (2020  ###reference_b23###), does not require any additional task-specific fine-tuning. This is possible due to the instruction-following capabilities of recent LLMs. We carefully craft a prompt (as shown in Table 4  ###reference_###) to extract cuss words, derogatory language and rationales from the input text that serve as interpretable features that can be used in the subsequent predictor model (HateBERT) in order to have a faithfully interpretable hate speech detector. In order to evaluate the goodness of the extracted features or rationales, we compare ChatGPT-extracted rationales with human-annotated ground truth rationales. We use the annotated rationale spans in the HateXplain Mathew et al. (2021  ###reference_b33###) dataset. After some standard pre-processing such as removing stop words, we compute the similarity between the ChatGPT extracted rationales for the input text from HateXplain dataset and the human-annotated rationales and report these scores in Table 5  ###reference_###. We compute similarity metrics in both the token space (Jaccard and Overlap similarity) and in the latent space (Cosine and Semantic similarity with Universal Sentence Encoder embeddings Cer et al. (2018  ###reference_b9###)) We see significant overlap and a high semantic similarity between the LLM and human rationales.\n###figure_2### We present some examples from all 5 datasets in Table 4  ###reference_###: the input text with a ‘hateful’ label and the ChatGPT-extracted features. The three category of features are shown in different colors: rationales, derogatory language and cuss words. We see that the LLM is successfully able to identify the words and spans quite well.\nWe also present some examples in Figure 2  ###reference_### to qualitatively discern the overlap between the human-annotated rationales and the LLM-extracted ones. Text in red are rationales annotated by human annotators, text in blue are rationales or words identified by the LLM and text in purple are the spans where both the LLM and human annotations overlap. From these examples, we see that there is overall a high degree of overlap, and the LLM is able to capture semantically relevant portions of the text. Interestingly, we also see that while human annotators often annotate words or spans with lesser relevance to the task, the LLM extracted rationales do not contain these spans (such as ‘aids figures’ and ‘prominently’ in the first example in Figure 2  ###reference_###). Using LLM-extracted rationales for training might be even more useful in such cases since some of the noisy signals in the data can be avoided."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Hate speech detector performance after training with extracted rationales",
            "text": "In this experiment, we try to train a hate speech detector with the extracted rationales additionally incorporated into the input text, to facilitate faithfully interpretable classifications. For this we use a HateBERT model as the base hate speech detector model and report results in Table 3  ###reference_###, along with results from other baselines. We see that our SHIELD framework performs at par with a simple HateBERT fine-tuned on the same dataset, i.e., at par with the base model. This performance retention is encouraging, since models are otherwise known to trade-off accuracy for interpretability  Dziugaite et al. (2020  ###reference_b14###); Bertsimas et al. (2019  ###reference_b3###). Interestingly, in the Twitter dataset, we also see a significant  performance jump by our SHIELD model as compared to the fine-tuned HateBERT model. This potentially might be due to noise in the Twitter dataset: the extracted rationales may provide more discriminative training signals thus allowing the detector to train on robust features instead of noisy ones, although more analysis is required to verify this claim.\nFor some additional analysis on the effect of the framework components, we modify the choice of the base pre-trained language models in the two model components: the hate speech detector, and the feature extractor. The specific variations we experiment with are: (1) the original SHIELD framework which has HateBERT as the hate speech detector (HSD) and bert-base-uncased as the feature embedding model (FE), (2) SHIELD with a pre-trained roberta-base as the HSD instead of HateBERT and (3) SHIELD with a pre-trained roberta-base as the FE instead of bert-base-uncased. We choose to perform this analysis with roberta instead of the two bert based models since RoBERTa Liu et al. (2019  ###reference_b31###) has been shown to sometimes have better performance than BERT Devlin et al. (2018  ###reference_b12###) on a variety of natural language understanding tasks Tarunesh et al. (2021  ###reference_b42###). We report the results of this analysis in Table 6  ###reference_###. Overall, we see some variation in performance on the model choice for the HSD and FE components. While roberta-base as the FE component marginally helps to improve performance for only one dataset, i.e., GAB, roberta-base as the HSD instead of HateBERT achieves higher performance for three datasets. This is particularly interesting since, unlike HateBERT, the pre-trained roberta-base is not specifically trained on the hate speech task.\nOverall, SHIELD shows promising results in leveraging LLM-extracted rationales into augmenting a base hate speech detector, to facilitate faithful interpretability, while maintaining detection performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Hate Speech Detection",
            "text": "There are two primary methods for approaching the detection of hate speech. Leveraging new or supplementary data is the first strategy. This involves making advantage of user attributes del Valle-Cano et al. (2023  ###reference_b10###), dataset annotator features Yin et al. (2022  ###reference_b43###), or comprehending the ramifications of hateful posts Kim et al. (2022  ###reference_b25###). One study, for instance, used the consequences of hateful posts to train a model on contrastive pairs that represent hate content in order to detect implicit hate speech Kim et al. (2022  ###reference_b25###). An additional study Yin et al. (2022  ###reference_b43###) brought to light the challenge of reaching agreement among annotators on subjective issues such as recognizing hate speech, and it recommended that definitive labels and annotator traits be included in training to improve the efficacy of detection. In a different study del Valle-Cano et al. (2023  ###reference_b10###), data from users’ social situations and characteristics were analyzed to predict user satisfaction. But the problem with these strategies is that they could be challenging as access to auxiliary information across different platforms is seldom available.\nThe second tactic makes use of language models like BERT, which have been trained on large text datasets and are renowned for their capacity for generalization. The efficacy of these algorithms can be increased by fine-tuning them using particular hate speech datasets Caselli et al. (2020  ###reference_b8###); Mathew et al. (2021  ###reference_b33###). One such example is HateBERT Caselli et al. (2020  ###reference_b8###), a model that was refined using over 1.6 million hostile remarks from Reddit and based on a BERT model. In a similar vein, HateXplain Mathew et al. (2021  ###reference_b33###) is another model created to recognize and interpret hate speech. Other strategies include concentrating on lexical indications Schmidt and Wiegand (2017  ###reference_b39###) such as POS tags used Markov et al. (2021  ###reference_b32###), facial expressions, content-related portions of speech, or important phrases that communicate hate ElSherief et al. (2018  ###reference_b15###). In order to improve language model representations, one study manually determined that sentiment and hostility are causal cues Sheth et al. (2023b  ###reference_b41###). Another study leveraged a causal graph to disentangle the input representations into platform specific (hate-target related features) and platform invariant features to enhance generalization capabilities for hate speech detection Sheth et al. (2023a  ###reference_b40###). Although effective, this method also requires auxiliary data (such as hate target labels) which are seldom available across various platforms."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "LLMs as Experts or Feature Extractors",
            "text": "Recent advancements in LLM research have demonstrated improved performance across not only many natural language tasks Min et al. (2023  ###reference_b34###), but also more challenging domains such as writing and debugging code, performing mathematical reasoning Bubeck et al. (2023  ###reference_b7###), etc. This has motivated a line of research where the community has been trying to evaluate how well these LLMs can perform different tasks. LLMs have shown promise in the task of data annotation He et al. (2023  ###reference_b21###); Bansal and Sharma (2023  ###reference_b2###), information extraction  Dunn et al. (2022  ###reference_b13###), text classification Kocoń et al. (2023  ###reference_b27###); Bhattacharjee and Liu (2024  ###reference_b5###), and even reasoning  Ho et al. (2022  ###reference_b22###). Given the ease with which these LLMs can be queried, these models often serve as faulty experts or pseudo oracles in many tasks. Past exploration has investigated whether language models can be used as factual knowledge bases Petroni et al. (2019  ###reference_b37###). A recent work has explored the possibility of using LLMs in the hate speech detection task Kumarage et al. (2024  ###reference_b28###). Similar to our approach, authors in  Hasanain et al. (2023  ###reference_b20###) have tried to perform propaganda span annotation using language models. However, our approach focuses on leveraging the extracted spans, words and rationales to augment a detector model to enable interpretability in an otherwise black-box model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this work, we explore the problem of hate speech detection on social media and propose a method to train interpretable classifiers using rationales extracted by large language models. Given the unsatisfactory performance of LLMs as off-the-shelf detectors for hate speech, we instead intend to leverage the textual understanding and instruction-following capabilities of LLMs such as ChatGPT to extract words and rationales from the text that are associated with the hate speech label. We propose a framework SHIELD, that uses these LLM-extracted rationales to augment the training of a base hate speech detector to facilitate it to be faithfully interpretable. We verify that the LLM-extracted rationales align with human judgement. We train and evaluate our framework on multiple benchmark datasets comprising both implicit and explicit hate speech from a variety of online social media platforms, and demonstrate how our SHIELD framework is able to maintain performance similar to the base model in spite of an expected accuracy-interpretability trade-off. Therefore, we have a faithfully interpretable hate speech detector that simply relies on LLM-extracted rationales instead of human-annotated.\nWhile our work follows that of  Jain et al. (2020  ###reference_b23###) and we establish faithfulness by construction, future work could explore better ways to evaluate the faithfulness of the resulting detector. In this work, we verified the goodness of the extracted rationales by comparing it with the ground truth for one dataset. Future work can investigate better automated ways to evaluate and verify the quality of the LLM-extracted rationales. Furthermore, an interesting and responsible direction forward would be the development of hybrid approaches that leverage LLMs for extracting rationales at scale and then employing human experts to verify the validity and quality of these rationales. This would also alleviate some of the concerns surrounding LLM hallucinations and biases in the LLM being propagated into the rationale extraction step."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "While our SHIELD framework shows promise in leveraging large language models to create interpretable hate speech detectors, several limitations need to be addressed. A inherent trade-off exists between the interpretability gained through LLM-extracted rationales and the accuracy of the resulting model, requiring further work to optimize this balance. In certain cases, the LLM may fail to identify coherent rationales, leading to incomplete or inaccurate explanations for the model’s predictions. The choice of the LLM itself is also crucial, as powerful proprietary models like ChatGPT may not be accessible to all researchers, while open-source alternatives could potentially yield suboptimal performance. Our work currently uses ChatGPT for rationale extraction, but exploring the capabilities of different LLMs, including multilingual and domain-specific models, could provide valuable insights. Additionally, our framework may need adaptation to handle instances where the LLM cannot provide clear rationales, either through ensemble methods or by incorporating human feedback mechanisms to refine the extracted rationales."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Considerations",
            "text": ""
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Acknowledgment of the sensitivity and potential harm of hate speech",
            "text": "We acknowledge that hate speech is a sensitive and potentially harmful topic that can perpetuate discrimination, marginalization, and violence against individuals or groups based on their race, ethnicity, religion, gender, sexual orientation, or other protected characteristics. We recognize the importance of addressing hate speech responsibly and with great care, as it can have severe psychological, emotional, and social consequences for those targeted. However, our work strives to better interpret and mitigate the use of hateful speech promptly by employing LLMs in an out-of-the-box manner leveraging their context-understanding capabilities in hate speech detection task."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Commitment to responsible use and mitigation of potential misuse",
            "text": "Our research focuses on leveraging the contextual understanding capabilities of large language models (LLMs) to automate the detection of hateful content, such as derogatory language, cuss words, and profanities, in the form of rationales across social media platforms. This aims to enable early-stage identification and mitigation of hate speech. We acknowledge the severity of the hateful examples used, which may potentially promote racial superiority, incite racial discrimination, or encourage violence against certain racial or ethnic groups – actions that are considered punishable offenses by law. After a thorough evaluation, we have concluded that the benefits of using real-world practical examples to enhance the clarity and understanding of our research outweigh any potential risks or drawbacks associated with their inclusion."
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Ethical guidelines and principles followed",
            "text": "In conducting our research, we adhere to established ethical guidelines and principles, such as those outlined by professional organizations and academic institutions. We have utilized publicly available datasets that are appropriately cited in our paper. We also strive to maintain transparency by clearly documenting our methods, data sources, and limitations."
        }
    ]
}