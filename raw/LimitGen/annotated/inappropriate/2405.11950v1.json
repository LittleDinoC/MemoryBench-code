{
    "title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
    "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models’ ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed.\nOut of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by  percentage points and was only  percentage points behind the first place.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the biomedical domain, scientific publications and research play a central role in communicating research findings and results. However, these documents are usually written in complex language and use terminology and technical jargon that can be challenging for lay readers or researchers from different fields to understand (Goldsack et al., 2022  ###reference_b8###). In this context, lay summarization can be utilized to extract the most relevant information from the original article or publication while also providing supplementary explanations. This often entails incorporating background information that may not be contained within the article itself.\nIn this context, this paper presents the participation of the team WisPerMed in the BioLaySumm2024 Shared Task (Goldsack et al., 2024  ###reference_b7###) on automatic lay summarization and describes the employed approaches to tackle this challenge.\nSummaries generated by LLMs, as demonstrated by Zhang et al. (2024  ###reference_b23###), can be of equivalent or superior quality to original references. Additionally, instruction tuning is an effective approach for enhancing performance. However, LLMs face limitations when applied to domain-specific abstractive summarization. Key challenges include the quadratic complexity of transformer-based models (Vaswani et al., 2017  ###reference_b19###) concerning input text length, model hallucination, where factually incorrect text is generated, and domain shift from training to test data (Afzal et al., 2023  ###reference_b1###). Similarly, studies on text simplification (Amin et al., 2023  ###reference_b3###) indicate that although general-purpose LLMs are capable of effectively simplifying clinical reports, they sometimes generate factual inaccuracies and omit crucial information.\nTo adapt LLMs to a specific domain or task (Ling et al., 2024  ###reference_b14###), it is possible to fine-tune the models, leverage few-shot learning or further pre-train the models on domain data. Examples of domain-adapted LLMs for the biomedical domain include the BioMistral (Labrak et al., 2024  ###reference_b12###) and OpenBioLLM (Pal and Sankarasubbu, 2024  ###reference_b18###) model series. The BioMistral models are based on the Mistral 7B Instruct v0.1 (Jiang et al., 2023  ###reference_b9###) model. They are further pre-trained on the PMC Open Access Subset111https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/  ###reference_nftlist/### Accessed: 2024-05-17. OpenBioLLM models are based on the Llama3 (AI@Meta, 2024  ###reference_b2###) models and were adapted to the biomedical domain through fine-tuning."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "The dataset (Goldsack et al., 2022  ###reference_b8###) of the Shared Task (Goldsack et al., 2024  ###reference_b7###) contains two collections of scientific journal articles and the corresponding lay summaries, namely PLOS and eLife. Lay summaries of the PLOS dataset were written by the authors of the articles, while eLife lay summaries were written by expert editors in correspondence with the authors.\nFor the remainder of this paper, any reference to the validation set or test set will include eLife and PLOS unless otherwise specified."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation Metrics",
            "text": "The generated summaries were evaluated using ten metrics categorized under relevance, readability, and factuality. Relevance was measured with Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004  ###reference_b13###) (ROUGE-1, ROUGE-2, ROUGE-L) and BERTScore (Zhang et al., 2020  ###reference_b22###). Readability was evaluated using the Flesch-Kincaid Grade Level (FKGL) (Kincaid, 1975  ###reference_b10###), Dale-Chall Readability Score (DCRS) (Chall and Dale, 1995  ###reference_b4###), Coleman-Liau Index (CLI) (Coleman and Liau, 1975  ###reference_b5###), and Learnable Evaluation Metric for Text Simplification (LENS) (Maddela et al., 2023  ###reference_b16###). Factuality was assessed with AlignScore (Zha et al., 2023  ###reference_b21###) and Summary Consistency (SummaC) (Laban et al., 2022  ###reference_b11###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods and Experiments",
            "text": "###figure_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Fine-tuned Models",
            "text": "In this study, randomly adjusting hyperparameters without any specific tuning strategy (Wei et al., 2022  ###reference_b20###) was utilized to fine-tune various models. This approach refers to the process of fine-tuning language models on a collection of datasets described via instructions. BioMistral-7B-DARE (BioM) and Llama3-70B-Instruct (Llama3) were fine-tuned for one epoch utilizing Quantized Low-Rank Adaptation (QLoRA) (Dettmers et al., 2023  ###reference_b6###) on the eLife and PLOS dataset individually. BioM was trained on the abstracts + lay summaries, whereas Llama3 was trained on the entire articles + lay summaries. The texts were structured using the Mistral and Llama3 templates prior to the fine-tuning process. Please refer to the Appendix A  ###reference_###, B  ###reference_###, and C  ###reference_### for details on prompts, parameters and licenses, respectively. After evaluating the checkpoints of BioM on the validation set, the checkpoints with the best scores were selected for inference. For Llama3, the final checkpoints were selected. The models were given the same prompt as during fine-tuning but without the target."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Prompt Variations",
            "text": "Prompts can guide the LLM’s content generation process without the need for fine-tuning. In the zero- and few-shot settings, different prompt variations and their effect on the evaluation metrics were examined. In the few-shot setting, examples from the training and validation set were included in the prompt when performing inference on the validation and test set, respectively. The format of the few-shot prompts is designed to emulate a preceding conversation with the model, with the examples serving as answers from the model.\nTo choose the best few-shot examples, all examples were ranked based on their average readability and factuality. The two and three highest-ranked examples were selected for the eLife and PLOS datasets, respectively.\nAn initial prompt was created by replicating the prompt used for inference with the fine-tuned BioM model (see Appendix A  ###reference_###). This prompt was then tested with BioM and OpenBioLLM-70B (OpenBio) on the validation set.\nThree prompt variations were created which provide the model with different kinds of context information. It was decided that BioM would be utilized for all experiments involving these variations due to its superior performance on the validation set in the few-shot setting (see Table 3  ###reference_### in Appendix D  ###reference_###). The first prompt variation includes a persona description of a science communicator (BioMpers). The model is then instructed to channel the expertise of the described persona to craft the lay summary based on the abstract. The second prompt variation is a modification of the initial prompt, but it includes the introduction as further context for background information (BioMintro). The third prompt variation contains the abstract and a guide on how to write a lay summary, accompanied by instructions concerning the content and style of the requested lay summary (BioMguide). The wording of all prompts can be found in Appendix A  ###reference_###.\nDue to the efficacy of few-shot learning with the initial prompt, the prompt variations were implemented in a few-shot setting on the test set."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Expert Selection (DES)",
            "text": "The success of a model depends on factors (Ling et al., 2024  ###reference_b14###) such as the nature of the data, the complexity of the domain, and the design of the prompt. Consequently, a model may yield a more suitable lay summary when prompted in a different manner. In consideration of this assumption, a Dynamic Expert Selection (DES) was developed. It selects one text from a set of candidate texts based on metrics that do not necessitate the target lay summary as a reference. The mechanism uses the readability metrics FKGL, DCRS, and CLI, as well as the factuality metrics. These metrics are computed for each candidate text and a min-max normalization is applied to each score so that the values are between 0 and 1. Prior to the normalization the readability metrics were multiplied by -1 so that 1 is the best and 0 is the worst. After computing the mean of all readability and factuality scores, the overall score is computed. Since the target lay summaries in eLife have a higher readability than those in PLOS (Goldsack et al., 2022  ###reference_b8###), the overall scores are computed with different weights for the two aspects. For eLife candidates, the weights are set to 0.675 and 0.325, whereas for PLOS candidates, the weights are set to 0.25 and 0.75 for readability and factuality, respectively.\nThis approach was applied to BioM in the few-shot setting using all prompt variants (see Figure 1  ###reference_###) and to the fine-tuned BioM using two distinct inference parameter settings (see Appendix B  ###reference_###).\n###table_1###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The results of the experiments using BioM, Llama3, and OpenBio are presented in table 1  ###reference_###. The experiments are categorized into zero-shot learning, few-shot learning, and fine-tuning.\nBioM exhibits the highest LENS score in the zero-shot setting. However, its performance in relevance and factuality is the lowest. Few-shot learning resulted in enhanced performance across all metrics except for LENS. The persona prompt (BioMpers) led to an improvement in relevance. Including the introduction in the prompt (BioMintro) resulted in a reduction in all aspects despite the fact that the model had access to more information from the article itself. In comparison, the prompt with the guide (BioMguide) exhibits minimal enhancements. The optimal few-shot learning for BioM occurred with the initial prompt, which achieved the highest readability and factuality in the few-shot setting, excluding the DES approach. However, OpenBio slightly underperformed with this prompt in the few-shot setting, except for the LENS score, where it performed best in this setting.\nThe DES used all four prompts and outperformed the baseline with improvements in factuality and readability, achieving the best results in the few-shot setting.\nFine-tuning BioM improved relevance and factuality scores, though the LENS score decreased slightly, with other readability metrics similar to the few-shot setting. The fine-tuned BioM outperformed the baseline in terms of relevance and overall quality. The DES approach improved all metrics except for a slight drop in the LENS score. In contrast, Llama3 underperformed despite being larger. It was less effective at extracting relevant information from full articles and produced lower-quality text in terms of readability, even though its LENS score was higher than BioM’s. Additionally, Llama3’s factuality scores decreased, leading to an overall performance drop compared to the baseline."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents the WisPerMed team’s approaches to automatic lay summarization within the biomedical domain, utilizing a combination of fine-tuning, prompt variations, and DES.\nAmong these approaches, fine-tuning emerged as an effective method, leading to the best performance across most metrics. This underscores the importance of task-specific training in optimizing model output for complex summarization tasks. Additionally, BioM demonstrated strong few-shot learning capabilities, illustrating its robustness and versatility in generating accurate and relevant summaries even without extensive training. As the model adjusts to the factuality and readability of given examples, providing better examples could lead to further enhancements in these aspects.\nBioM demonstrated high factuality, even when provided solely with abstracts as input, suggesting that BioM leveraged domain-specific knowledge acquired during pre-training. This indicates that domain adaptation remains an important factor when using LLMs for lay summarization of scientific articles, as BioM outperformed the larger general model Llama3. A potential limitation of this study is the possibility that BioM may have been previously exposed to the gold standard.\nThe DES mechanism refined readability and factuality by retrospectively selecting the best text outputs based on evaluation metrics. This highlights the potential of metric-driven selection to improve the quality of lay summaries further.\nIn conclusion, our study demonstrates that fine-tuning, the use of informed prompt variations, and selection mechanisms can enhance the capability of autoregressive LLMs to produce lay summaries that are factually accurate, relevant, and readily accessible to non-specialist audiences. This approach fosters broader public engagement with scientific findings, advancing the goal of making biomedical research comprehensible and accessible."
        }
    ]
}