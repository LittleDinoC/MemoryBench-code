{
    "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
    "abstract": "Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance.\nThis paper unveils a previously overlooked type of outliers in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs.\nGiven that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model.\nThe approach is simple and easy to combine with existing quantization solutions with no extra inference overhead. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further with minimal training costs. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error.\nEmpirical results show that IntactKV brings consistent improvement over various quantization methods across different LLMs and downstream tasks, leading to the new state-of-the-art for LLM quantization. The codes are available at https://github.com/ruikangliu/IntactKV.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have achieved remarkable progress in various tasks and benchmarks in natural language processing Brown et al. (2020  ###reference_b5###); Bubeck et al. (2023  ###reference_b6###); Touvron et al. (2023a  ###reference_b33###); Team et al. (2023  ###reference_b32###).\nNonetheless, the rise of LLMs also increases computational intensity and memory requirements.\nThis motivates various research to decrease the inference cost of LLMs, e.g., quantizaiton Frantar et al. (2022  ###reference_b14###); Shao et al. (2024  ###reference_b29###); Lin et al. (2023  ###reference_b20###), pruning Frantar and Alistarh (2023  ###reference_b13###); Liu et al. (2023b  ###reference_b22###); Sun et al. (2023  ###reference_b31###); Zhang et al. (2024  ###reference_b38###), and speculative decoding Chen et al. (2023  ###reference_b8###); Leviathan et al. (2023  ###reference_b18###); Cai et al. (2024  ###reference_b7###), e.t.c.\nAmong these methods, network quantization converts the network parameters or activations from floating-point to fixed-point formats, which is a popular technique to reduce the model size and computational resources. Nevertheless, quantization inevitably affects the performance of LLMs. The leading cause comes from the outliers in LLM activations, which are sensitive to network quantization Dettmers et al. (2022  ###reference_b12###); Xiao et al. (2023  ###reference_b35###); Lin et al. (2023  ###reference_b20###). As workarounds, there are efforts to either use mixed-precision formats Dettmers et al. (2022  ###reference_b12###) or re-scale network weights of the outlier channels Lin et al. (2023  ###reference_b20###).\nThese methods are all built based on the premise that outliers persist in fixed channels across all tokens. However, we find this is not the case for all outliers in LLMs.\nIn this paper, we discover a new type of outlier that is overlooked by previous quantization methods. These outliers exhibit extremely high values at only the [BOS] and some other common tokens (e.g., “,” and “.”) at the beginning of the input, which is referred to as pivot tokens.\nWe find the extreme values of these outliers make the self-attention concentrate on the pivot tokens, leaving the rest of the tokens untouched. This is also known as attention sinks Xiao et al. (2024  ###reference_b36###), which is critical to the model performance Xiao et al. (2024  ###reference_b36###); Bondarenko et al. (2023  ###reference_b4###). The effect of quantization on these pivot tokens should be carefully studied to improve the quantized LLMs.\n###figure_1### ###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### Towards that end, we are motivated to propose IntactKV, a simple strategy that is orthogonal to most existing quantization solutions.\nThe key idea behind IntactKV is to generate the lossless KV cache of pivot tokens from the full-precision model. By keeping the KV cache of pivot tokens intact, quantization error accumulated on the output of self-attention will be effectively alleviated in the rest of the decoding steps. The integration of IntactKV comes with no additional inference overhead.\nMoreover, IntactKV can also serve as extra trainable parameters in addition to the LLM backbone. The calibration process of IntactKV follows the convention of PTQ Bai et al. (2022  ###reference_b2###); Frantar et al. (2022  ###reference_b14###), which further decreases the quantization error.\nTo get more insights from IntactKV, we also provide mathematical analysis and the results show that IntactKV can effectively lower the upper bound of quantization error.\nEmpirical results show that IntactKV consistently improves the capability of different quantization methods (e.g. AWQ Lin et al. (2023  ###reference_b20###), GPTQ Frantar et al. (2022  ###reference_b14###), OmniQuant Shao et al. (2024  ###reference_b29###) and QuaRot Ashkboos et al. (2024  ###reference_b1###)) on various open-sourced LLMs (e.g., LLaMA and Vicuna) across different tasks and benchmarks such as PPL, MMLU, commonsense QA, and MT-bench, achieving new state-of-the-art results for weight-only quantization as well as weight and activation quantization, e.g., lossless INT4 weight-only quantization for Vicuna-v1.3-13B on commonsense QA tasks.\nMoreover, calibrating IntactKV with INT4 quantization even matches the full-precision model on aligning with human preferences, as evaluated by GPT-4 Bubeck et al. (2023  ###reference_b6###) on MT-bench."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Motivation",
            "text": "Different from the outliers that persist in several fixed channels across different tokens (Dettmers et al., 2022  ###reference_b12###; Xiao et al., 2023  ###reference_b35###; Lin et al., 2023  ###reference_b20###), we find a new variant of outlier that is specific to some initial tokens of the input sequence.\nBy visualizing the activation of Transformer layer output in Figure 1(a)  ###reference_sf1### and Figure 1(b)  ###reference_sf2###, there exist peaks with magnitudes over 1e3.\nThese outliers can be hundreds of times larger than the previous outliers that persist in fixed channels across all tokens, as enlarged in Figure 1(a)  ###reference_sf1### and Figure 1(b)  ###reference_sf2###. More visualizations can be found in Appendix C  ###reference_###.\nIt is found that such huge outliers usually occur at the [BOS] token and some other uninformative initial tokens (e.g., \".\" or \",\") at particular channels, regardless of the rest of the input sequence.\nWe thus name these tokens pivot tokens given their dominating values in the activation.\nRecently, a concurrent work (Sun et al., 2024  ###reference_b30###) also discovers such outliers with more detailed studies.\nWe hypothesize that the outliers over these pivot tokens may propagate to queries and keys in the self-attention. Consequently, the attention scores will be concentrated on these pivot tokens than the rest ones, a.k.a attention sinks Xiao et al. (2024  ###reference_b36###).\nTo verify the hypothesis, we plot the attention scores in Figure 1(c)  ###reference_sf3### and Figure 1(d)  ###reference_sf4###. It can be found that the pivot tokens indeed dominate the attention scores, especially for the first token (i.e., [BOS] ). This corresponds to the observations in attention sinks (Xiao et al., 2024  ###reference_b36###), which are empirically verified to be critical to the model performance.\nThe recent study by (Bondarenko et al., 2023  ###reference_b4###) also shows that concentrating on these tokens naturally helps the attention head do nothing but simply a partial update of the residual.\nIn the decoding stage of LLMs, all generated tokens need to interact with pivot tokens through self-attention. However, as mentioned in Section 2.1  ###reference_###, network quantization would inevitably distort the output from the full-precision model. The concentrated scores of pivot tokens thus can be further deviated by quantization, which downgrades the model performance."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preliminaries on LLM Quantization",
            "text": "Network quantization is popularly studied in the literature of efficient LLMs Frantar et al. (2022  ###reference_b14###); Lin et al. (2023  ###reference_b20###); Shao et al. (2024  ###reference_b29###). It allows larger throughput by reducing the model size and leads to practical inference speedup.\nGiven the full-precision weight , quantization aims to convert it to the low-bit representation . The general -bit uniform quantization  can be represented as\nwhere  is the quantization step size, and  is the projection function onto the set of -bit integers .\nWhile we mainly focus on weight-only quantization, Equation 1  ###reference_### can be similarly used to quantize activations and KV cache of LLMs to increase the inference throughput Xiao et al. (2023  ###reference_b35###); Shao et al. (2024  ###reference_b29###); Hooper et al. (2024  ###reference_b17###).\nFollowing most existing works in LLM quantization, we focus on post-training quantization (PTQ) Frantar et al. (2022  ###reference_b14###); Lin et al. (2023  ###reference_b20###), since it does not introduce extra training overhead as those in quantization-aware training (QAT) Liu et al. (2023a  ###reference_b21###); Li et al. (2024  ###reference_b19###).\nQuantization inevitably downgrades LLMs in low-bit settings, where the outliers in quantized LLMs are found to be the cause of the deterioration Dettmers et al. (2022  ###reference_b12###).\nIn the next, we study the details of how these outliers affect the LLM quantization.\n###figure_9### ###figure_10###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Revisiting Outliers in LLMs",
            "text": "We discover a new type of outlier that is specific to particular tokens, which leads the attention sink Xiao et al. (2024  ###reference_b36###) that is critical to the performance of LLMs.\nDifferent from the outliers that persist in several fixed channels across different tokens (Dettmers et al., 2022  ###reference_b12###  ###reference_b12###; Xiao et al., 2023  ###reference_b35###  ###reference_b35###; Lin et al., 2023  ###reference_b20###  ###reference_b20###), we find a new variant of outlier that is specific to some initial tokens of the input sequence.\nBy visualizing the activation of Transformer layer output in Figure 1(a)  ###reference_sf1###  ###reference_sf1### and Figure 1(b)  ###reference_sf2###  ###reference_sf2###, there exist peaks with magnitudes over 1e3.\nThese outliers can be hundreds of times larger than the previous outliers that persist in fixed channels across all tokens, as enlarged in Figure 1(a)  ###reference_sf1###  ###reference_sf1### and Figure 1(b)  ###reference_sf2###  ###reference_sf2###. More visualizations can be found in Appendix C  ###reference_###  ###reference_###.\nIt is found that such huge outliers usually occur at the [BOS] token and some other uninformative initial tokens (e.g., \".\" or \",\") at particular channels, regardless of the rest of the input sequence.\nWe thus name these tokens pivot tokens given their dominating values in the activation.\nRecently, a concurrent work (Sun et al., 2024  ###reference_b30###  ###reference_b30###) also discovers such outliers with more detailed studies.\nWe hypothesize that the outliers over these pivot tokens may propagate to queries and keys in the self-attention. Consequently, the attention scores will be concentrated on these pivot tokens than the rest ones, a.k.a attention sinks Xiao et al. (2024  ###reference_b36###  ###reference_b36###).\nTo verify the hypothesis, we plot the attention scores in Figure 1(c)  ###reference_sf3###  ###reference_sf3### and Figure 1(d)  ###reference_sf4###  ###reference_sf4###. It can be found that the pivot tokens indeed dominate the attention scores, especially for the first token (i.e., [BOS] ). This corresponds to the observations in attention sinks (Xiao et al., 2024  ###reference_b36###  ###reference_b36###), which are empirically verified to be critical to the model performance.\nThe recent study by (Bondarenko et al., 2023  ###reference_b4###  ###reference_b4###) also shows that concentrating on these tokens naturally helps the attention head do nothing but simply a partial update of the residual.\nIn the decoding stage of LLMs, all generated tokens need to interact with pivot tokens through self-attention. However, as mentioned in Section 2.1  ###reference_###  ###reference_###, network quantization would inevitably distort the output from the full-precision model. The concentrated scores of pivot tokens thus can be further deviated by quantization, which downgrades the model performance."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "In this section, we introduce IntactKV, a simple and easy-to-implement method to improve the quantized LLMs. The key idea behind this is to keep the KV cache of the pivot tokens intact, i.e., without any distortion raised by quantization.\nAn overview of our method can be found in Figure 3  ###reference_###.\nIt is the key design to choose the pivot tokens and the associated IntactKV. Given the observations in Figure 2  ###reference_###, one can naively pick pivot tokens with the most MSE reduction for IntactKV. However, this is in fact not the case. Since IntactKV acts as the prefix to the KV cache of quantized LLMs, it must start from the very first token, and be consecutive in length. This ensures it to be input agnostic, and the full-precision LLMs can be safely discarded once IntactKV is generated.\nNext, we provide practical solutions to this problem for different LLMs.\nFor pre-trained LLMs, we propose the IntactKV of size one that only contains [BOS] KV cache.\nIt is a convention to prepend [BOS] to the input of pre-trained LLMs.\nMoreover, as illustrated in Section 2  ###reference_###, [BOS] is the pivot token with extreme outlier and attention scores. Besides, the KV cache of [BOS] has a great impact on the MSE of the quantized model. Employing a lossless [BOS] KV cache is thus believed to decrease the quantization loss.\nFor supervised fine-tuned (SFT) models, when the input follows the system prompt, we argue that extending IntactKV to the same length of the system prompt can further improve quantized LLMs.\nIn addition to [BOS], other tokens appearing at the beginning of the input sequence also have the potential to serve as pivot tokens (see Figure 1  ###reference_###).\nThe system prompt is usually prepended to the input, which allows it to cover\nmore pivot tokens.\nAs shown in Figure 2  ###reference_###, remedying the quantization error of these pivot tokens’ KV cache can be helpful to compensate for the quantization error.\nWe find that for Vicuna models, system prompt is enough to cover all the pivot tokens, more details can be found in Appendix C.3  ###reference_###.\nFinally, we highlight that IntactKV does not introduce extra latency overhead during inference.\nBesides, as IntactKV is pre-computed, the pre-filling stage of the quantized LLMs can be accelerated as well. The memory overhead to save IntactKV is also negligible compared with the LLM backbone. For instance, there are only 34 tokens of the system prompt for Vicuna-v1.5-7B, and thus IntactKV takes only 0.13% of the LLM model parameters."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preserving the KV Cache of Pivot Tokens",
            "text": "According to Section 2.2  ###reference_###, the attention sinks of pivot tokens are likely to deteriorate by quantization.\nTo alleviate this issue, we propose IntactKV, a simple yet effective strategy to randomly quantize the entire KV cache without addressing the pivot tokens' impact. Specifically, as illustrated in Figure 3(a)  ###reference_sf1###, we leverage the full-precision LLM to generate the lossless KV cache of pivot tokens, which is saved offline. The quantized LLM then loads IntactKV as the prefix to concatenate with the rest of the KV cache and continues with the regular auto-regressive decoding process.\nThe pseudo code of the inference scheme with IntactKV is presented in Figure 3(b)  ###reference_sf2###.\nIn order to study the benefits of IntactKV, we conduct a preliminary test on the mean squared error (MSE) of the attention and transformer layer output.\nFrom Figure 2  ###reference_###, it is natural that the increasing size of IntactKV gives the monotonically decreasing MSE on both the attention and transformer layers.\nMore importantly, it is found the pivot tokens observed in Section 2.2  ###reference_### (e.g., [BOS] and other delimiter tokens) give the most significant decrease on the MSE, which demonstrates the importance of their KV cache. This aligns with the observations in Figure 1  ###reference_### that pivot tokens exhibit outliers with extreme values and attention sinks.\nIt is the key design to choose the pivot tokens and the associated IntactKV. Given the observations in Figure 2  ###reference_###  ###reference_###, one can naively pick pivot tokens with the most MSE reduction for IntactKV. However, this is in fact not the case. Since IntactKV acts as the prefix to the KV cache of quantized LLMs, it must start from the very first token, and be consecutive in length. This ensures it to be input agnostic, and the full-precision LLMs can be safely discarded once IntactKV is generated.\nNext, we provide practical solutions to this problem for different LLMs.\nFor pre-trained LLMs, we propose the IntactKV of size one that only contains [BOS] KV cache.\nIt is a convention to prepend [BOS] to the input of pre-trained LLMs.\nMoreover, as illustrated in Section 2  ###reference_###  ###reference_###, [BOS] is the pivot token with extreme outlier and attention scores. Besides, the KV cache of [BOS] has a great impact on the MSE of the quantized model. Employing a lossless [BOS] KV cache is thus believed to decrease the quantization loss.\nFor supervised fine-tuned (SFT) models, when the input follows the system prompt, we argue that extending IntactKV to the same length of the system prompt can further improve quantized LLMs.\nIn addition to [BOS], other tokens appearing at the beginning of the input sequence also have the potential to serve as pivot tokens (see Figure 1  ###reference_###  ###reference_###).\nThe system prompt is usually prepended to the input, which allows it to cover\nmore pivot tokens.\nAs shown in Figure 2  ###reference_###  ###reference_###, remedying the quantization error of these pivot tokens’ KV cache can be helpful to compensate for the quantization error.\nWe find that for Vicuna models, system prompt is enough to cover all the pivot tokens, more details can be found in Appendix C.3  ###reference_###  ###reference_###.\nFinally, we highlight that IntactKV does not introduce extra latency overhead during inference.\nBesides, as IntactKV is pre-computed, the pre-filling stage of the quantized LLMs can be accelerated as well. The memory overhead to save IntactKV is also negligible compared with the LLM backbone. For instance, there are only 34 tokens of the system prompt for Vicuna-v1.5-7B, and thus IntactKV takes only 0.13% of the LLM model parameters."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "IntactKV as Trainable Parameters",
            "text": "Since IntactKV is pre-computed and saved offline, it can be treated as extra trainable parameters aside from the LLM backbone to further boost the quantized LLMs.\nDespite there being no information loss at the pivot tokens, the quantization may still introduce errors to the KV cache during the decoding stage.\nAs shown in Figure 3(a)  ###reference_sf1###, we calibrate IntactKV to compensate for the quantization error accumulated in the following tokens.\nWhile there are various metrics to characterize the quantization discrepancy Frantar et al. (2022  ###reference_b14###); Shao et al. (2024  ###reference_b29###); Liu et al. (2023a  ###reference_b21###), we adopt the mean squared error of the transformer layer output between the full-precision LLM and quantized LLM, a simple yet most widely used metric, i.e.,\nwhere  denotes the set of IntactKV,  is the mapping function for the -th Transformer layer, and  is the number of Transformer layers in LLM.  is the input sequence, while  are full-precision and quantized weights respectively.\nNote that the full-precision model is only required during the calibration process, and it can be safely discarded afterward.\nIt is empirically found that calibration of system prompt IntactKV in SFT models generally gives more improvement than the calibration of [BOS] IntactKV in pre-trained LLMs. This matches the intuition that a larger size of IntactKV increases the potential to compensate for quantization errors.\nAs we focus on the post-training quantization, the training of IntactKV is highly lightweight since the only learnable parameters introduced are IntactKV, i.e., the KV cache of pivot tokens. It takes only as few as 20 epochs on a calibration set with 128 samples. Besides, training with a quantized model further lowers the memory cost. The calibration process takes about only 10 minutes for a 7B model and less than 20 minutes for a 13B model on one computing device."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Theoretical Analysis",
            "text": "In this section, we provide a theoretical view of how the proposed IntactKV benefits the quantized LLM.\nFor the clarity of presentation, our analysis is built on the self-attention module of a Transformer layer, while it can be readily extended to the FFN module and multiple layers.\nSpecifically, we denote  as the KV cache during the decoding stage, and  is the query vector, where  and  are the sequence length and head dimension. Recall that the output of each attention head  is computed as\nwhere  is the weight matrix of the projection layer. By quantizing the LLMs, there will be errors accumulated on the KV cache, denoted as .\nTherefore, we are interested in showing how  and  are propagated to the change of attention head , and to what extent IntactKV alleviates the distortion.\nGiven the query vector  and the change of KV caches , the change of the attention head  is bounded by\nwhere  and .\nThe proof to Theorem 1  ###reference_orem1### can be found in Appendix IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact.\nWe preserve the terms w.r.t.  and  of interests, and leave the rest as constants.\nNote that  can be further separated by the pivot tokens  and rest tokens , and similar notations hold for .\nTherefore, we have  and .\nWith IntactKV  we have  since they are generated losslessly, which decreases the upper bound of . Moreover, it can further reduce the bound by incorporating more pivot tokens. This also aligns with the observation in Figure 2  ###reference_### that a larger size of IntactKV gives a lower MSE of the attention module."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate the proposed IntactKV on various sizes of open-sourced LLMs, including LLaMA (Touvron et al., 2023a  ###reference_b33###) (7B-65B), LLaMA-2 (Touvron et al., 2023b  ###reference_b34###) (7B-70B), Vicuna-v1.3 (Chiang et al., 2023  ###reference_b9###) (7B-33B) and Vicuna-v1.5 (7B-13B). We denote models that keep intact [BOS] KV as IntactKV, and models that keep intact system prompt KV as IntactKV.\nWe mainly consider weight-only quantization methods, including round-to-nearest quantization (RTN), GPTQ (Frantar et al., 2022  ###reference_b14###), the state-of-the-art OmniQuant (Shao et al., 2024  ###reference_b29###) and AWQ (Lin et al., 2023  ###reference_b20###).\nFor GPTQ, we use AutoGPTQ with C4 calibration set following (Frantar et al., 2022  ###reference_b14###) to reproduce all results. For AWQ and OmniQuant, we use the official code or checkpoint with Pile (Gao et al., 2020  ###reference_b15###) and WikiText2 (Merity et al., 2016  ###reference_b23###) calibration set respectively, following (Lin et al., 2023  ###reference_b20###; Shao et al., 2024  ###reference_b29###). More implementation details can be found in Appendix E  ###reference_###. We adopt asymmetric group-wise quantization with a group size of 128 and mainly focus on INT3 and INT4 quantization since INT8 is empirically lossless on various task metrics (Dettmers et al., 2022  ###reference_b12###).\nOur IntactKV can be readily combined with these existing weight-only quantization methods, and the experiment results are shown in Section 4.2  ###reference_###.\nMoreover, aside from weight-only quantization, the proposed IntactKV can be similarly applied for KV cache quantization and extended to activation quantization, as detailed in Section 4.3  ###reference_### and Section 4.4  ###reference_###. It is worth noting that the integration of IntactKV with weight-only/KV cache/activation quantization comes with no extra inference cost and works as an effective plugin to effectively boost the accuracy of quantized models.\nFor pre-trained LLMs (i.e., LLaMA and LLaMA-2), we report the perplexity (PPL) of language generation on C4 (Raffel et al., 2020  ###reference_b27###) and WikiText2 (Merity et al., 2016  ###reference_b23###) dataset. For SFT models (i.e., Vicuna-v1.3 and v1.5), we conduct evaluation over a wide range of downstream tasks. We test the zero and five-shot performance on the Massively Multitask Language Understanding (MMLU) (Hendrycks et al., 2020  ###reference_b16###) benchmark. Meanwhile, we also evaluate seven zero-shot commonsense QA tasks: OBQA (Mihaylov et al., 2018  ###reference_b24###), WinoGrande (Sakaguchi et al., 2021  ###reference_b28###), ARC-Challenge, ARC-Easy (Clark et al., 2018  ###reference_b11###), BoolQ (Clark et al., 2019  ###reference_b10###), HellaSwag (Zellers et al., 2019  ###reference_b37###), and LAMBADA (Paperno et al., 2016  ###reference_b26###). Additionally, we evaluate quantized Vicuna on MT-bench (Zheng et al., 2023  ###reference_b39###), a high-quality dataset consisting of 80 open-ended multi-turn questions, to gauge their alignment with human preferences. The responses generated by quantized models are judged by GPT-4 with a total score of 10. More evaluation details can be found in Appendix F  ###reference_###.\nFor evaluation on PPL, MMLU, and commonsense QA tasks, we adopt IntactKV that only includes [BOS] KV since the input sequence of these tasks does not use any system prompt.\nFor evaluation of SFT models on MT-bench, we adopt IntactKV to keep an intact system prompt KV cache. The system prompt of Vicuna can be found in Appendix B  ###reference_###.\nFor training the cached IntactKV, we randomly sample 128 samples from ShareGPT111https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered  ###reference_eGPT_Vicuna_unfiltered### dataset as our calibration dataset, consisting of multi-turn ChatGPT (OpenAI, 2022  ###reference_b25###) conversations.\nThe layer-wise MSE loss defined in Equation 2  ###reference_### is calculated on the response of ChatGPT.\nWe use AdamW optimizer with learning rate , training for 160 optimizer update steps with a gradient accumulation step of 16, i.e., 20 epochs.\nAs mentioned in Section 3.2  ###reference_###, training IntactKV leads to comparable performance compared with vanilla IntactKV. Instead, the calibration of IntactKV has more potential to improve quantized LLMs with longer system prompt.\nThus, we primarily evaluate the IntactKV with KV cache of system prompt as trainable parameters in the following experiments.\nFor weight and activation quantization, we further quantize IntactKV to lower bits to avoid extra inference overhead, which only incurs negligible accuracy loss. More details of activation quantization can be found in Section 4.4  ###reference_###.\nWe first integrate our proposed IntactKV with RTN, GPTQ, OmniQuant, and AWQ on LLaMA and LLaMA-2 models.\nThe effect of this integration on model accuracy is measured by the perplexity (PPL) metric, with results on the C4 dataset detailed in Table 1  ###reference_###, and results on the WikiText2 dataset in Table 7  ###reference_###.\nAs indicated in these tables, IntactKV notably enhances the generative capabilities of quantized models across various LLMs and quantization methods, with AWQ+IntactKV  consistently achieving new state-of-the-art (SOTA) results.\nThese findings demonstrate the efficacy of IntactKV in improving quantized LLMs and particularly highlight the effectiveness of utilizing the lossless KV cache from full-precision models. We provide more experiment results on LLaMA-3 and other heterogeneous LLMs (e.g. OPT) in Appendix G.1  ###reference_###. IntactKV significantly improves different quantized LLMs, especially for LLaMA-3 models with larger quantization error. These results further prove the compatibility of our IntactKV with various LLM backbones.\n###figure_11### ###figure_12### ###figure_13### ###figure_14### For SFT models, we implement IntactKV on the quantized Vicuna models and evaluate the multi-task problem-solving ability on the MMLU benchmark. Table 3  ###reference_### presents the detailed zero-shot and five-shot results for Vicuna-v1.3-13B.\nThe results demonstrate that IntactKV significantly enhances the performance of quantized models across all categories of tasks and various quantization methods for Vicuna-v1.3-13B.\nMoreover, performance of Vicuna family under the five-shot setting is outlined in Table 2  ###reference_###.\nRemarkably, IntactKV achieves an average improvement of 1.05% over OmniQuant and 0.8% over AWQ across five model sizes, with AWQ+IntactKV exhibiting superior performance over all the other quantized models. More results on MMLU are provided in Appendix G.2  ###reference_###.\nWe further evaluate the quantized Vicuna models on zero-shot commonsense QA tasks.\nThe results of Vicuna-v1.3-13B, as detailed in Table 4  ###reference_###, indicate that IntactKV enables significant improvements over various quantization methods. For example, AWQ+IntactKV surpasses the average accuracy of AWQ by 0.46% under INT3-g128 quantization.\nAdditionally, Table 2  ###reference_### presents the average accuracy for various sizes of Vicuna models. In these evaluations, our IntactKV leads to an average accuracy improvement of 0.45% across different LLMs and quantization methods, which strongly demonstrates the efficacy of our proposed IntactKV. More results on commonsense QA tasks can be found in Appendix G.3  ###reference_###.\nTo evaluate the quantized models’ generation capabilities in multi-turn conversations and their alignment with human preferences, we use GPT-4 to score the responses of quantized models on MT-Bench.\nWe also calibrate IntactKV, denoted as IntactKV+Cal.\nFrom Table 5  ###reference_###, IntactKV significantly boosts the quantized model and IntactKV+Cal further enhances generation quality by compensating for the quantization error.\nFor example, the 3-bit Vicuna-v1.5-13B quantized by AWQ has been improved from 5.17 to 5.34 by using the IntactKV, which can be further boosted to 5.44 with trainable IntactKV.\nWe provide INT4 quantization results in Table 13  ###reference_###. Remarkably, with trainable IntactKV, AWQ+IntactKV even matches the full-precision model under INT4 quantization, while all other methods clearly lag behind the full-precision model.\nThese results demonstrate the effectiveness of IntactKV as well as treating IntactKV as trainable parameters.\nNotably, the training process for the 7B model takes only 10 minutes on a single computing device, which is quite lightweight. In Appendix H  ###reference_###, we further demonstrate the effectiveness of calibrating IntactKV by comparing it with group bias tuning, a commonly used fine-tuning strategy for quantized models. IntactKV calibration can achieve better or comparable results with group bias tuning while using significantly fewer trainable parameters. Besides, IntactKV calibration serves as a more versatile calibration strategy for quantized models, which is suitable for various quantization settings."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Settings",
            "text": "We evaluate the proposed IntactKV on various sizes of open-sourced LLMs, including LLaMA (Touvron et al., 2023a  ###reference_b33###  ###reference_b33###) (7B-65B), LLaMA-2 (Touvron et al., 2023b  ###reference_b34###  ###reference_b34###) (7B-70B), Vicuna-v1.3 (Chiang et al., 2023  ###reference_b9###  ###reference_b9###) (7B-33B) and Vicuna-v1.5 (7B-13B). We denote models that keep intact [BOS] KV as IntactKV, and models that keep intact system prompt KV as IntactKV.\nWe mainly consider weight-only quantization methods, including round-to-nearest quantization (RTN), GPTQ (Frantar et al., 2022  ###reference_b14###  ###reference_b14###), the state-of-the-art OmniQuant (Shao et al., 2024  ###reference_b29###  ###reference_b29###) and AWQ (Lin et al., 2023  ###reference_b20###  ###reference_b20###).\nFor GPTQ, we use AutoGPTQ with C4 calibration set following (Frantar et al., 2022  ###reference_b14###  ###reference_b14###) to reproduce all results. For AWQ and OmniQuant, we use the official code or checkpoint with Pile (Gao et al., 2020  ###reference_b15###  ###reference_b15###) and WikiText2 (Merity et al., 2016  ###reference_b23###  ###reference_b23###) calibration set respectively, following (Lin et al., 2023  ###reference_b20###  ###reference_b20###; Shao et al., 2024  ###reference_b29###  ###reference_b29###). More implementation details can be found in Appendix E  ###reference_###  ###reference_###. We adopt asymmetric group-wise quantization with a group size of 128 and mainly focus on INT3 and INT4 quantization since INT8 is empirically lossless on various task metrics (Dettmers et al., 2022  ###reference_b12###  ###reference_b12###).\nOur IntactKV can be readily combined with these existing weight-only quantization methods, and the experiment results are shown in Section 4.2  ###reference_###  ###reference_###.\nMoreover, aside from weight-only quantization, the proposed IntactKV can be similarly applied for KV cache quantization and extended to activation quantization, as detailed in Section 4.3  ###reference_###  ###reference_### and Section 4.4  ###reference_###  ###reference_###. It is worth noting that the integration of IntactKV with weight-only/KV cache/activation quantization comes with no extra inference cost and works as an effective plugin to effectively boost the accuracy of quantized models.\nFor pre-trained LLMs (i.e., LLaMA and LLaMA-2), we report the perplexity (PPL) of language generation on C4 (Raffel et al., 2020  ###reference_b27###  ###reference_b27###) and WikiText2 (Merity et al., 2016  ###reference_b23###  ###reference_b23###) dataset. For SFT models (i.e., Vicuna-v1.3 and v1.5), we conduct evaluation over a wide range of downstream tasks. We test the zero and five-shot performance on the Massively Multitask Language Understanding (MMLU) (Hendrycks et al., 2020  ###reference_b16###  ###reference_b16###) benchmark. Meanwhile, we also evaluate seven zero-shot commonsense QA tasks: OBQA (Mihaylov et al., 2018  ###reference_b24###  ###reference_b24###), WinoGrande (Sakaguchi et al., 2021  ###reference_b28###  ###reference_b28###), ARC-Challenge, ARC-Easy (Clark et al., 2018  ###reference_b11###  ###reference_b11###), BoolQ (Clark et al., 2019  ###reference_b10###  ###reference_b10###), HellaSwag (Zellers et al., 2019  ###reference_b37###  ###reference_b37###), and LAMBADA (Paperno et al., 2016  ###reference_b26###  ###reference_b26###). Additionally, we evaluate quantized Vicuna on MT-bench (Zheng et al., 2023  ###reference_b39###  ###reference_b39###), a high-quality dataset consisting of 80 open-ended multi-turn questions, to gauge their alignment with human preferences. The responses generated by quantized models are judged by GPT-4 with a total score of 10. More evaluation details can be found in Appendix F  ###reference_###  ###reference_###.\nFor evaluation on PPL, MMLU, and commonsense QA tasks, we adopt IntactKV that only includes [BOS] KV since the input sequence of these tasks does not use any system prompt.\nFor evaluation of SFT models on MT-bench, we adopt IntactKV to keep an intact system prompt KV cache. The system prompt of Vicuna can be found in Appendix B  ###reference_###  ###reference_###.\nFor training the cached IntactKV, we randomly sample 128 samples from ShareGPT111https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered  ###reference_eGPT_Vicuna_unfiltered###  ###reference_eGPT_Vicuna_unfiltered### dataset as our calibration dataset, consisting of multi-turn ChatGPT (OpenAI, 2022  ###reference_b25###  ###reference_b25###) conversations.\nThe layer-wise MSE loss defined in Equation 2  ###reference_###  ###reference_### is calculated on the response of ChatGPT.\nWe use AdamW optimizer with learning rate , training for 160 optimizer update steps with a gradient accumulation step of 16, i.e., 20 epochs.\nAs mentioned in Section 3.2  ###reference_###  ###reference_###, training IntactKV leads to comparable performance compared with vanilla IntactKV. Instead, the calibration of IntactKV has more potential to improve quantized LLMs with longer system prompt.\nThus, we primarily evaluate the IntactKV with KV cache of system prompt as trainable parameters in the following experiments.\nFor weight and activation quantization, we further quantize IntactKV to lower bits to avoid extra inference overhead, which only incurs negligible accuracy loss. More details of activation quantization can be found in Section 4.4  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We first integrate our proposed IntactKV with RTN, GPTQ, OmniQuant, and AWQ on LLaMA and LLaMA-2 models.\nThe effect of this integration on model accuracy is measured by the perplexity (PPL) metric, with results on the C4 dataset detailed in Table 1  ###reference_###  ###reference_###, and results on the WikiText2 dataset in Table 7  ###reference_###  ###reference_###.\nAs indicated in these tables, IntactKV notably enhances the generative capabilities of quantized models across various LLMs and quantization methods, with AWQ+IntactKV  consistently achieving new state-of-the-art (SOTA) results.\nThese findings demonstrate the efficacy of IntactKV in improving quantized LLMs and particularly highlight the effectiveness of utilizing the lossless KV cache from full-precision models. We provide more experiment results on LLaMA-3 and other heterogeneous LLMs (e.g. OPT) in Appendix G.1  ###reference_###  ###reference_###. IntactKV significantly improves different quantized LLMs, especially for LLaMA-3 models with larger quantization error. These results further prove the compatibility of our IntactKV with various LLM backbones.\n###figure_15### ###figure_16### ###figure_17### ###figure_18### For SFT models, we implement IntactKV on the quantized Vicuna models and evaluate the multi-task problem-solving ability on the MMLU benchmark. Table 3  ###reference_###  ###reference_### presents the detailed zero-shot and five-shot results for Vicuna-v1.3-13B.\nThe results demonstrate that IntactKV significantly enhances the performance of quantized models across all categories of tasks and various quantization methods for Vicuna-v1.3-13B.\nMoreover, performance of Vicuna family under the five-shot setting is outlined in Table 2  ###reference_###  ###reference_###.\nRemarkably, IntactKV achieves an average improvement of 1.05% over OmniQuant and 0.8% over AWQ across five model sizes, with AWQ+IntactKV exhibiting superior performance over all the other quantized models. More results on MMLU are provided in Appendix G.2  ###reference_###  ###reference_###.\nWe further evaluate the quantized Vicuna models on zero-shot commonsense QA tasks.\nThe results of Vicuna-v1.3-13B, as detailed in Table 4  ###reference_###  ###reference_###, indicate that IntactKV enables significant improvements over various quantization methods. For example, AWQ+IntactKV surpasses the average accuracy of AWQ by 0.46% under INT3-g128 quantization.\nAdditionally, Table 2  ###reference_###  ###reference_### presents the average accuracy for various sizes of Vicuna models. In these evaluations, our IntactKV leads to an average accuracy improvement of 0.45% across different LLMs and quantization methods, which strongly demonstrates the efficacy of our proposed IntactKV. More results on commonsense QA tasks can be found in Appendix G.3  ###reference_###  ###reference_###.\nTo evaluate the quantized models’ generation capabilities in multi-turn conversations and their alignment with human preferences, we use GPT-4 to score the responses of quantized models on MT-Bench.\nWe also calibrate IntactKV, denoted as IntactKV+Cal.\nFrom Table 5  ###reference_###  ###reference_###, IntactKV significantly boosts the quantized model and IntactKV+Cal further enhances generation quality by compensating for the quantization error.\nFor example, the 3-bit Vicuna-v1.5-13B quantized by AWQ has been improved from 5.17 to 5.34 by using the IntactKV, which can be further boosted to 5.44 with trainable IntactKV.\nWe provide INT4 quantization results in Table 13  ###reference_###  ###reference_###. Remarkably, with trainable IntactKV, AWQ+IntactKV even matches the full-precision model under INT4 quantization, while all other methods clearly lag behind the full-precision model.\nThese results demonstrate the effectiveness of IntactKV as well as treating IntactKV as trainable parameters.\nNotably, the training process for the 7B model takes only 10 minutes on a single computing device, which is quite lightweight. In Appendix H  ###reference_###  ###reference_###, we further demonstrate the effectiveness of calibrating IntactKV by comparing it with group bias tuning, a commonly used fine-tuning strategy for quantized models. IntactKV calibration can achieve better or comparable results with group bias tuning while using significantly fewer trainable parameters. Besides, IntactKV calibration serves as a more versatile calibration strategy for quantized models, which is suitable for various quantization settings."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Extension to KV Cache Quantization",
            "text": "IntactKV can be readily applied to KV cache quantization to further decrease memory requirements.\nWe employ a mixed-precision strategy that keeps IntactKV in FP16 while the rest of the KV cache is quantized to lower bits. This only induces negligible memory overhead since IntactKV only contains the KV cache of the first few tokens. Note that this does not bring any additional inference costs since in the workflow of KV cache quantization, all quantized KV cache needs to be de-quantized back to FP16 before the matrix multiplication. Keeping IntactKV in FP16 reduces the overhead of de-quantization, i,e., we only need to cheaply concatenate the FP16 IntactKV with the rest de-quantized KV cache together.\nFrom Figure 4  ###reference_###, IntactKV notably improves AWQ across different models and KV cache bit widths under the INT3 weight quantization. For INT4 weight quantization, AWQ+IntactKV still gains an average accuracy increase of 0.27% over the original quantized model. We also notice that quantizing the KV cache to INT8 leads to almost no performance drop on the MMLU benchmark. When equipped with IntactKV, INT8 KV cache can even surpass vanilla AWQ-quantized models with FP16 KV cache, especially under INT3 weight quantization."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Extension to Activation Quantization",
            "text": "In Table 6  ###reference_###, we provide experiment results of combining IntactKV with OmniQuant Shao et al. (2024  ###reference_b29###) and QuaRot Ashkboos et al. (2024  ###reference_b1###) for weight and activation quantization. The implementation details can be found in Appendix E  ###reference_###. To avoid extra inference costs, we need to quantize the whole KV cache to lower bits and can not keep the KV cache of pivot tokens intact. However, as detailed in Appendix I  ###reference_###, we find that IntactKV has a significantly smoother distribution compared with the rest of the KV cache. Therefore, the full-precision IntactKV can be readily quantized to lower bits with negligible accuracy loss, thus rendering IntactKV amenable to weight and activation quantization with no extra inference costs. As shown in Table 6  ###reference_###, our IntactKV significantly surpasses the performance of original quantized models for two different quantization methods, improving the PPL by 1.07 for OmniQuant and 0.31 for QuaRot on average. When combined with QuaRot, our IntactKV archives new state-of-the-art (SOTA) results on INT4 weight and activation quantization."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we propose IntactKV, a simple and easy-to-combine method to improve large language model quantization.\nThe research is motivated by the previously overlooked outliers over pivot tokens, which lead to attention sinks that are critical to the performance of quantized LLMs.\nBy generating IntactKV with the full-precision model, the quantization error accumulated over the attention scores can be effectively alleviated. IntactKV can also be calibrated as additional parameters to the LLM backbone, further improving the quantized LLMs.\nExperiments show that combining the proposed IntactKV gives consistent improvement on various sizes of LLMs and across multiple downstream tasks, leading to new state-of-the-art results for large language model quantization."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "More experiments may be needed for LLM evaluation. LLMs are being applied to a wide range of tasks, posing high demands on various model abilities.\nWhen quantizing LLMs to low bits, these abilities may be affected to varying degrees. Therefore, a comprehensive evaluation is required to gauge the capabilities of quantized LLMs. Although we experiment on several downstream tasks, such as PPL, MMLU, commonsense QA, and MT-bench, we note that this may not be enough to assess all abilities of LLMs. For example, how long context affects quantized models still remains unknown."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "The development of LLM quantization techniques can further democratize LLMs, lowering the costs of LLM serving and enabling more people to get access to advanced AI assistants. Nonetheless, LLM itself may inherit certain social biases from training data concerning gender, race, etc. Quantization can not mitigate such biases. Therefore, caution must be taken when using quantized LLMs."
        }
    ]
}