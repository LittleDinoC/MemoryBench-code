{
    "title": "Efficient Sample-Specific Encoder Perturbations",
    "abstract": "Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest.\nSpecifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings.\nThis work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively.\nFurthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Encoder-decoder models have displayed state-of-the-art performance in a wide range of sequence tasks Sutskever et al. (2014  ###reference_b36###) including Machine Translation (MT) Vaswani et al. (2017  ###reference_b40###); Xue et al. (2021  ###reference_b44###), Abstractive Text Summarization & Question Answering Chung et al. (2022  ###reference_b7###) and Automatic Speech Recognition (ASR) Chiu et al. (2018  ###reference_b6###). However, the standard approach to training these systems often relies on teacher-forcing with the likelihood criteria, e.g. next token prediction of the reference sequence. While this framework has been shown successful and reliable in the tasks above, often the desired criteria are some sequence-level non-differentiable performance measures. In MT the desired criteria was the n-gram-based (Sacre)BLEU Post (2018  ###reference_b26###), overtaken more recently by the neural-based COMET Rei et al. (2020  ###reference_b29###) evaluation metric. In ASR the criteria is the word error rate (WER) measuring the rate of substitutions, insertions, and deletions between the decoded output and the reference.\nPrior work has addressed the exposure bias arising from training in teacher-forcing Williams and Zipser (1989  ###reference_b41###) and the loss mismatch between the likelihood and the desired sequence-level loss Bengio et al. (2015  ###reference_b3###); Lamb et al. (2016  ###reference_b20###); Gu et al. (2019  ###reference_b15###); Sabour et al. (2019  ###reference_b33###); Wu et al. (2018  ###reference_b43###); Ranzato et al. (2016  ###reference_b28###); Bahdanau et al. (2017  ###reference_b1###); Wiseman et al. (2016  ###reference_b42###); Kim and Rush (2016  ###reference_b17###).\nBoth of these approaches modify the training so it more closely links with how the model would be used during deployment.\nHowever, in the regime of large pre-trained foundation models, re-training such systems is computationally expensive and unstable. Other approaches attack this problem at inference time by merging outputs from several systems guided by appropriate sequence-level metrics Sim et al. (2007  ###reference_b34###); Kumar and Byrne (2004  ###reference_b18###); Freitag et al. (2022  ###reference_b12###); Rosti et al. (2007a  ###reference_b30###, b  ###reference_b31###); Manakul et al. (2023a  ###reference_b22###). Whilst these approaches have shown promising gains, they rely on the use of ensembles which have significantly higher computational costs. Furthermore, they are not generalizable to any metric.\nIn this paper, we propose a novel, simple and efficient approach to modify the behaviour of a single frozen pre-trained encoder-decoder foundation model.\nWe show that it is possible to perturb the outputs of the encoder to trigger the decoder to produce better-performing decodings according to some pre-selected generic attribute. Unlike prior approaches, our novel proposal applies to frozen pre-trained systems and only leads to an insignificant increase in runtime.\nThis paper is focused on showing the efficacy of the approach in improving the COMET performance of Flan-T5 Chung et al. (2022  ###reference_b7###) on NMT and the WER performance of Whisper Radford et al. (2022  ###reference_b27###) on ASR. Furthermore, the approach is generalizable and applicable to other attributes such as the sentiment of outputs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Reinforcement Learning (RL) and Minimum Bayes Risk (MBR) decoding have been two paradigms used to align a sequence model and minimize the impact of exposure bias. The former is traditionally used to improve the training while the latter is used to modify the decoding procedure. Both come with their own sets of advantages and disadvantages. While often leading to better performance, these approaches are often expensive to use during training and/or inference.\nIn the works of Bahdanau et al. (2017  ###reference_b1###); Lamb et al. (2016  ###reference_b20###), a second network is used, either as a critic in an actor-critic framework Barto et al. (1983  ###reference_b2###); Sutton (1984  ###reference_b37###) or as a discriminator in a generative adversarial framework Goodfellow et al. (2014  ###reference_b14###). The aim of both of these networks, although mechanically different, is to ensure the training procedure resembles the inference stage, minimising the effect of exposure bias.\nOn the other hand, the work of Freitag et al. (2022  ###reference_b12###) explored a post-training approach in which the Minimum Bayes Risk decoder samples many sequences from the system and chooses the sample with the lowest risk. However, the efficacy of this system is highly dependent on being able to sample a large set of outputs, a feat not possible for large pre-trained systems. Alternatively, Manakul et al. (2023b  ###reference_b23###) applies this approach to an ensemble of similarly performing systems, without regard to the inference cost.\nThere is also a body of work on memory and inference-efficient adaptation of foundation models such as prefix-tuning Li and Liang (2021  ###reference_b21###) and low-rank adaptation Hu et al. (2022  ###reference_b16###). These approaches are effective at adapting a foundation model to a certain task at the cost of degrading other abilities. In addition, these approaches still require back-propagating through the whole foundation model making them potentially expensive to train and mainly target teacher-forcing likelihood training. Finally, the work of Fathullah et al. (2023a  ###reference_b9###) introduced a general framework in which an encoder is extended with a small Non-Autoregressive Proxy (NAP) trained to directly capture an arbitrary metric. While only applicable to encoder-decoder systems, it showed that estimates produced by NAP systems were useful in downstream tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Perturbations of Encoders",
            "text": "The proposal’s core is a flexible and efficient approach for augmenting the behaviour of an encoder on a sample-by-sample basis to trigger a better decoder performance. The starting point for such a goal will be the recently introduced Non-Autoregressive Proxy (NAP). While the original work trained the network on a specific metric and used the estimates at runtime to directly perform various downstream tasks, we will extend the view of this network to a differentiable approximation of a sequence-level metric, and use the gradients of this approximation to improve performance.\nLet  and  represent the parameters of some encoder and decoder network, and let  be some input (token or embedding) sequence\nAt inference time we have  where  represents a sequence of  encoder embeddings that are consumed by the decoder. The decoder, through an autoregressive process, produces an output sequence . We aim to find a sample-specific perturbation  to the sequence of encoder outputs such that  gives us a higher score according to some score  (e.g. COMET):\nwhere  is the reference sequence. To find a good perturbation we first apply a static predefined scoring rule without adjustments based on model outputs to approximate the score  where  represent the scoring parameters. Once this is achieved, the gradient of the scoring rule can be used to find a good perturbation to the encoder outputs of a certain sample , making the approach sample specific:\nwhere . We normalize for the size of the gradient and the encoder L2-norms and include a hyperparameter  to control the perturbation size. Small perturbations will have no impact while large changes can lead to a degradation in performance. Therefore, the choice of  is important and is based on some validation set. Note that our approach is aimed to be a lightweight and cheap method for obtaining performance gains, and is not designed to achieve state-of-the-art performance. Furthermore, to the best of the authors’ knowledge, no prior approaches exist for augmenting the behaviour of frozen encoder-decoder systems according to any criteria  which can be anything from COMET and SacreBLEU to the sentiment of the output.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Evaluation",
            "text": "The first set of experiments aim to evaluate the efficacy of the gradient perturbation approach on NMT using the pre-trained Flan-T5. We resort to the OPUS-100 Zhang et al. (2020  ###reference_b45###) dataset and use two splits, the English to German (ende) and the English to Spanish (enes) splits, each with a training set of 1M sentence pairs. For each, we use the Flan-T5 system to decode the data using greedy search with a maximum of 128 output tokens. The decodings are scored using COMET (Unbabel/wmt22-comet-da) Rei et al. (2020  ###reference_b29###). Following Fathullah et al. (2023a  ###reference_b9###) we train a small NAP on the Flan-T5 encoder to predict these COMET scores, using the Pearson correlation loss, training details provided in Appendix A.1  ###reference_###. Each experiment is repeated 3 times.\n###table_1### Table 1  ###reference_### presents the Pearson correlation between the NAP predictions and the COMET scores on various test sets. We observe that a proxy trained on a specific translation pair can still obtain good correlation scores on other splits and reverse directions. The worst performance is shown when a proxy trained on enes pairs is evaluated on the reverse direction displaying a correlation of 35.2%.\n###figure_3### Next, we take these NAPs and use the gradient with respect to the encoder outputs to perform the augmentation detailed in Equation (2  ###reference_###), see Figure 1  ###reference_###.\nThis shows that the gradients derived from a lightweight NAP trained on COMET scores can be used to obtain some performance gains. While small steps  barely have an impact, large  lead to a degradation in performance showcasing the importance of finding a good . Furthermore, we find that although the NAPs were trained on the COMET scores of greedy decodings, they can still be used to improve beam search. We also observe that NAPs trained in a certain translation direction can still be used for other directions. For example, NAP en-es is still able to improve en-de performance.\nWe also investigate this for a range of Flan-T5 models, see Table 2  ###reference_###. In all cases we use a fixed value of .\n###table_2### From these sets of results, it is evident that smaller models benefit more from the perturbation approach while larger more robust models show smaller gains.\nFurthermore, we apply this approach to a range of different-sized beam search decodings, see Figure 2  ###reference_###. For a small cost of performing a forward and backward pass through the small NAP network, both translation pairs show gains across all beams. Note, that the inference speed was measured using an NVIDIA A100 80GBs leading to a disproportionally cheaper runtime for larger beam sizes, since the GPU cores were not fully exhausted for a single sample. Interestingly while we observe improvements in COMET scores, the SacreBLEU score remained the same, see Table 3  ###reference_###.\n###table_3### Upon further analysis of this phenomenon, we observed two factors contributing to this effect. The first is that the proxy augmentation would substitute certain tokens/words that have a \"closer\" meaning to the reference but without any n-gram overlap. The second factor is discussed in Stahlberg and Byrne (2019  ###reference_b35###) in which translation systems often terminate prematurely. The proxy-derived augmentation of the encoder resolves this issue in a fraction of examples, but the continuation of the translation often does not overlap with the words occurring in the reference.\nFinally, we repeat these experiments for the Whisper family on the AMI meeting corpus, a challenging ASR task in which systems often display very high word error rates. Following Fathullah et al. (2023a  ###reference_b9###) we train NAPs on the number of errors in a transcription produced by Whisper {Tiny, Base, Small} using greedy search, see Appendix A.2  ###reference_### for details. The performance of the augmentation is displayed in Figure 3  ###reference_###. Four observations made in these results are: (1) the proxy gradients are beneficial for Whisper in the ASR task, (2) the validation and test sets show correlating performance, (3) the step sizes are significantly larger since the gradients from the NAP are significantly smaller and (4) the improvement is smaller for the larger Whisper systems. Concerning the last point, this behaviour was expected since larger more robust systems potentially have a smaller room for improvement. Similar experiments performed on LibriSpeech Panayotov et al. (2015  ###reference_b25###), a relatively easier speech recognition benchmark showcased small to no gains supporting the claim that the benefits from our proposed approach is highly dependent on the task given a certain model. The details of the LibriSpeech experiments are included in Appendix B  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a lightweight modification of frozen (pre-trained) encoder-decoder systems by training a small network to predict a metric and use this as a differentiable extension. The gradients of this small proxy network have been shown useful in improving the performance on a range of sequence tasks such as the Flan-T5 COMET performance on NMT and Whisper WER performance on ASR."
        }
    ]
}