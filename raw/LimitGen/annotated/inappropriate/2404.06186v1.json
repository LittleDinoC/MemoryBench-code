{
    "title": "Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles",
    "abstract": "Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing.\nIn this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context.\nWith such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues.\nWe used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach.\n\n\nKeywords: Educational Crossword Clues, LLMs, Instruction Tuning, Natural Language Generation",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The conventional structure of crossword puzzles merged with scholastic elements results in an engaging learning tool: educational crosswords. They encompass a variety of subjects such as science, vocabulary, and history Nickerson (1977  ###reference_b19###); Sandiuc and Balagiu (2020  ###reference_b25###); Yuriev et al. (2016  ###reference_b30###). Educational crosswords differ from traditional puzzles because they are designed for teaching rather than entertainment. Consequently, their are less cryptic, and usually, they present a less constrained puzzle scheme, as in the example shown in Figure 1  ###reference_###. They are particularly beneficial in language acquisition or when mastering technical jargon for specific topics Orawiwatnakul (2013  ###reference_b20###); Dzulfikri (2016  ###reference_b9###); Bella and Rahayu (2023  ###reference_b6###). Additionally, the requirement of correlating appropriate hints with correct words fosters learners’ problem-solving skills Kaynak et al. (2023  ###reference_b16###); Dol (2017  ###reference_b8###).\nMemory enhancement is another merit of educational crosswords, as learners need to summon previously learned material to solve the puzzle Mueller and Veinott (2018  ###reference_b18###); Dzulfikri (2016  ###reference_b9###). Moreover, the interactive nature of crosswords makes the learning experience captivating, inducing learners to persist in honing their abilities Zirawaga et al. (2017  ###reference_b36###); Bella and Rahayu (2023  ###reference_b6###).\nSummarily, educational crosswords serve as an entertaining resource for strengthening educational skills Zamani et al. (2021  ###reference_b31###); Yuriev et al. (2016  ###reference_b30###).\n###figure_1### Harnessing the power of Large Language Models (LLMs) presents an opportunity in the field of educational crossword production, traditionally known for requiring specialized skills and labor.\nThrough an extensive training process on huge language corpora comprising internet resources, academic papers, and books, LLMs acquire the ability to generate high-quality text to accomplish many different tasks. This proficiency can be exploited to automatically generate clues, so to ease the process of educational crossword crafting.\nIn this work, we propose a methodology to construct datasets for educational crossword clue generation. In particular, we present clue-instruct, a corpus made of 44,075 clue generation instructions. Each example is constituted by a source text, serving as context, a category of interest, and a keyword, all paired with three target clues to generate. The dataset is built by gathering content from Wikipedia pages about relevant keywords, whereas clues were automatically generated by an LLM.\nUpon clue-instruct, we carried out a detailed experimentation with different open-source LLMs varying in size and family, and we fine-tune them on the dataset.\nResults, assessed with both automatic and human evaluations, indicate that fine-tuning remarkably improves the generation quality of those models.\nThe dataset111https://huggingface.co/datasets/azugarini/clue-instruct  ###reference_clue-instruct### and all the models are publicly available.\nThe paper is organized as follows.\nSection 2  ###reference_### reports the related works on crosswords in NLP. We describe the proposed methodology in Section 3  ###reference_### and analyse in detail the properties of the generated dataset in Section 4  ###reference_###. In Section 5  ###reference_###, we discuss the experimental outcomes in-depth. Finally, we draw our conclusions in Section 6  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Works",
            "text": "Crossword puzzles are a fascinating linguistic game that has been a subject of study in the Natural Language Processing field in the past few years. Literature can be divided into two main research branches: crossword solving and crossword generation Rigutini (2010  ###reference_b22###). We briefly review both of them, then we finally discuss about existing crossword datasets.\nCrossword resolution can be tackled as a constrained satisfaction task where the objective is to maximize the probability of filling the grid with answers coherent with the given clues. The main challenge in the problem is retrieving correct candidate answers.\nExisting solutions heavily rely on clue-answer databases.\nProverb Littman et al. (1999  ###reference_b17###), one of the earliest crossword-solving systems, used a probabilistic version of the A with candidate answers retrieved from databases of American crosswords.\nSimilarly, Dr. Fill Ginsberg (2011  ###reference_b13###) converted them into weighted CSPs and used advanced heuristics.\nWebCrow Ernandes et al. (2005  ###reference_b10###); Angelini et al. (2005b  ###reference_b2###, a  ###reference_b1###) was a crossword-solving Italian project based on human-machine competitions. Webcrow distinguished from other solutions for exploiting the information present in the web. It was developed for the Italian language and English. Recently, it was extended to other languages Angelini et al. (2023  ###reference_b3###); Zugarini et al. (2023  ###reference_b38###) making use of neural representations of clue-answer pairs Zugarini and Ernandes (2021  ###reference_b37###).\nBased on WebCrow, SACRY leveraged syntactic structures for re-ranking and answer extraction to enhance answer quality by incorporating syntactic analysis Barlacchi et al. (2015  ###reference_b5###).\nLately, the Berkeley Crossword Solver Wallace et al. (2022  ###reference_b28###) was presented. It was based on neural question-answering models for candidate answer retrieval, and belief propagation with local searches to fill the grid, achieving state-of-the-art performance in English crossword solving.\n###figure_2### ###figure_3### Building a crossword puzzle automatically encompasses different linguistic problems, such as identifying the answers, composing the grid, and above all, creating the clues.\nEarly approaches Rigutini et al. (2008  ###reference_b23###, 2012  ###reference_b24###) leveraged NLP techniques to generate lists of clue-answer pairs by analyzing online documents (Wikipedia pages). Clues were identified and extracted with NLP techniques (POS tagging, dependency analysis, and WordNet).\nAnalogously, the methods proposed in Ranaivo-Malançon et al. (2013  ###reference_b21###) and Esteche et al. (2017  ###reference_b12###) followed a step-based approach to construct crosswords via NLP tools. The steps involved preliminary data extraction of sentences from a text that was then used to produce the clue-answer pairs.\nA software tool utilizing NLP techniques to extract crucial keywords for crossword creation in Indian languages was proposed by Arora and Kumar (2019  ###reference_b4###). The resulting SEEKH framework combines statistical and linguistic methods to identify vital keywords.\nMore recently, Zeinalipour et al. (2023a  ###reference_b32###, b  ###reference_b33###, c  ###reference_b34###) moved from handcrafted design of generated crosswords to generative solutions utilizing pre-trained LLMs. Crosswords were generated in English, Arabic, and Italian, thus demonstrating the effectiveness of computational linguistics in creating culturally diverse and engaging puzzles.\nAnalogously, our method makes use of LLMs to generate clues for a given answer, but we ground the generation to a source context with the purpose of producing clues that are adherent to a given input text.\nDespite many works have been published in both crossword solving and generation, few datasets have been created and publicly released.\nMost of them consist of clue-answer pair corpora, generally collected from crosswords or clue databases Ernandes et al. (2008  ###reference_b11###); Ginsberg (2011  ###reference_b13###) sometimes enriched by metadata such as publication date, publisher, and difficulty. Unfortunately, for copyright reasons, they are not always publicly available.\nIn Barlacchi et al. (2015  ###reference_b5###), to test their proposed system, the authors created a corpus by downloading crossword puzzles from some web sources.\nWallace et al. (2022  ###reference_b28###) collected a validation and test set of complete 2020 and 2021 puzzle grids from several US news (The New York Times, The LA Times, Newsday, The New Yorker, and The Atlantic) and they publicly released code, models, and dataset.\nHowever, all these clue-answer pairs corpora are constructed from traditional crossword puzzles. In these types of puzzles, the clues usually have extremely enigmatic linguistic structures that are quite different from those typically adopted for educational purposes.\nFurthermore, by design they lack of any reference to textual passages in which the clue can be found. This information is very important in the educational use-case where the clue must be related to a subject of study. Moreover, a grounding context allow to steer the generation of a Language Model, thus dramatically reducing the occurrence of hallucinated or unrelated clues.\nIn this work instead, we propose a method to create a clue generation corpus where clues are tied with an answer and a source context. The obtained dataset is, to the best of our knowledge, the first corpus associating such information together."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Method",
            "text": "Differently from traditional clue-answer crossword databases, we necessitate aligning the clue-answer pair with a grounding text, where the answer to the clue can be inferred from it. The grounding text is crucial in education both from the perspective of a teacher and from the point of view of the student. In order to construct such a context-keyword-clue triplet, we follow a pipeline, starting from collecting and gathering data from Wikipedia. The entire pipeline is sketched in Figure 3 ###reference_###. Here we describe it step by step. We initiate the information extraction process by mining Wikipedia pages. This involves accessing the initial section of each page, which typically contains the most pertinent information. From this portion, we emphasize keywords presented in bold, which often correspond to the page’s title but can include additional terms. These selected keywords become the focal points of the Wikipedia page, shaping the content to provide in-depth definitions and explanations. In addition to the content, we gather various metadata about the page, including the number of page views, an overall importance rating, text within paragraphs, its title, associated keywords, relevant categories, and individual URLs. Leveraging the standardized layout of Wikipedia pages, we extract keyword-rich opening paragraphs that encapsulate the core content, offering succinct explanations or definitions, and contributing to the construction of a valuable dataset.\n###figure_4### With the goal of discarding low-quality data, we adopt several filters: (1) We select pages based on the number of views and importance rating; (2) We remove pages with too long or too short contents; (3) All the data with keywords made of more than three words were removed; (4) all the keywords outside typical English crossword boundaries – character length – or containing non-alphabetical symbols were excluded. The creation of an effective prompt was a crucial aspect of our methodology. We carefully designed prompts for crossword clue generation by incorporating the relevant keywords extracted from the Wikipedia pages. These prompts were structured to provide contextual guidance for generating clues that were both informative and engaging. By using the extracted keywords along with the context of the Wikipedia page, the prompts acted as input signals to guide the generation of crossword clues. Our goal was to create prompts that were well-suited to each specific topic or subject area, taking into account the unique characteristics of the information we had gathered. Crafting the prompt effectively played a key role in the success of our approach, enabling our system to produce high-quality crossword clues tailored to educational needs. In Figure 2 ###reference_###, the prompt employed in the study is depicted. After assembling content, keywords and categories into the prompt, in the last pipeline step, we generate educational clues for such data. Inspired by self-instruct (Wang et al., 2022 ###reference_b29###), we make use of Large Language Models for automatically generating clues. Differently from self-instruct, generation is strongly conditioned by the information in the input context of the LLM. Therefore, we expect it to produce more faithful clues, thus significantly mitigating the risks of hallucinations. In the final stage, we train a small random forest model on the dataset to ensure accurate and contextually appropriate clues. ###figure_5### ###figure_6### ###figure_7### ###figure_8### May be blocked by phone companies to prevent scams Corrupt and incompetent government in J.K. Rowling’s Wizarding World Renewed for a third season, released exclusively on Netflix One of the four recognized species in the tapir family ###figure_9###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Clue-Instruct Dataset",
            "text": "With the method described in Section 3  ###reference_###, we construct a dataset for English educational clues, starting from the most popular pages of 20 distinct categories, that initially contained 258,325 Wikipedia pages.\nWe kept all the pages with more than 10,000 views or with an importance rating equal to ’Top’. Contexts below 30 or above 1000 words were deleted. After data screening, we obtained a corpus of 44,075 examples in total. We used GPT-3.5 Turbo (Brown et al., 2020  ###reference_b7###) as clue generator LLM, with the prompt depicted in Figure 2  ###reference_###.\nDue to the absence of a reference corpus for educational crosswords, there is no reference set to compare the generated clues with. Therefore, we cannot produce standard automatic metrics such as ROGUE scores.\nNonetheless, in the specific educational clue generation task, good clues should tightly adhere to the reference context, being simple reformulations of some information stated in the text. Hence, the problem is highly extractive.\nFrom such considerations, we exploited as automatic evaluation, the ROUGE-L score between the sentences in the input context against the generated clue.\nIntuitively, scores should be high enough to indicate strong adherence to the context, thus reducing the chances of hallucinations, but not too close to perfect matches, which would be an indication of poor clue styling and high chances of injecting the target keyword within the clue itself.\nOn average, we obtained about 42 ROUGE-L, which indicates a significant entailment between the generated clue and the most similar sentence in the context. The distribution over the dataset is outlined in Figure 6  ###reference_###.\nTo assess the quality of generated data, we cannot solely rely on automatic metrics. Thus, we sample a portion of clue-instruct for human evaluation. Similarly to Wang et al. (2022  ###reference_b29###), we consider a five-level rating, under the following guidelines:\nRATING-A: The clue is valid and coherent to the given context, answer, and category.\nRATING-B: Acceptable clue with minor imperfections - loose correlation with category.\nRATING-C: The clue is relevant to the answer but loosely correlates with the context, or it is too generic.\nRATING-D: The clue is irrelevant and/or incorrect with respect to the answer or the context.\nRATING-E: Not acceptable clue because it contains the answer (or a variant of it).\nWe also allow annotators to skip examples (marked with SKIP), in case there are issues not strictly related to the clue itself, such as odd keywords or documents.\nOverall, 600 examples were annotated, for a total of 1,800 clues evaluated, since there are three clues proposed by the model for each given context, keyword, and category triplet. We report rating distributions in Figure 7  ###reference_###. More than two out of three (about 72%) clues were marked with RATING-A, the highest score, which grows to 81% if we consider as acceptable also the clues rated with B.\n###figure_10### ###figure_11###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Statistics",
            "text": "In this section, we delve into the statistical properties of clue-instruct.\nTable 1  ###reference_### presents an overview of the dataset. It comprises overall 44,075 textual content-keyword pairs across 20 different categories. Three distinct clues were generated from each content-keyword-category triplet, resulting in a total of 132,225 clues.\nAs previously highlighted, examples are divided into 20 distinct categories. In Figure 4  ###reference_###, we visually represent the frequency of each category within our dataset using a bubble plot. The size of each bubble corresponds to the frequency of the respective category in the dataset. Upon analyzing this plot, it becomes evident that ’Geography’, ’Science’, and ’Applied Science’ are the most prevalent categories, in that order. Conversely, ’Biography’, ’Games’, and ’Education’ are the least frequent ones.\nIn Figure 5  ###reference_###, we outline the distribution of context and output lengths in relation to the number of words. Such a figure also presents the keyword length distribution in terms of characters.\nWe can observe how the context length falls in a wide range going from 30 to 1000 words, with the vast majority of examples having context lengths between 50 to 400. Conversely, most outputs have word lengths between 35 to 50.\nAdditionally, the keyword character length spans from 3 to 20 as imposed during the corpus creation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Measuring Data Quality",
            "text": "To evaluate the dataset quality, we resorted to both automatic metrics and human evaluations.\nDue to the absence of a reference corpus for educational crosswords, there is no reference set to compare the generated clues with. Therefore, we cannot produce standard automatic metrics such as ROGUE scores.\nNonetheless, in the specific educational clue generation task, good clues should tightly adhere to the reference context, being simple reformulations of some information stated in the text. Hence, the problem is highly extractive.\nFrom such considerations, we exploited as automatic evaluation, the ROUGE-L score between the sentences in the input context against the generated clue.\nIntuitively, scores should be high enough to indicate strong adherence to the context, thus reducing the chances of hallucinations, but not too close to perfect matches, which would be an indication of poor clue styling and high chances of injecting the target keyword within the clue itself.\nOn average, we obtained about 42 ROUGE-L, which indicates a significant entailment between the generated clue and the most similar sentence in the context. The distribution over the dataset is outlined in Figure 6  ###reference_###  ###reference_###.\nTo assess the quality of generated data, we cannot solely rely on automatic metrics. Thus, we sample a portion of clue-instruct for human evaluation. Similarly to Wang et al. (2022  ###reference_b29###  ###reference_b29###), we consider a five-level rating, under the following guidelines:\nRATING-A: The clue is valid and coherent to the given context, answer, and category.\nRATING-B: Acceptable clue with minor imperfections - loose correlation with category.\nRATING-C: The clue is relevant to the answer but loosely correlates with the context, or it is too generic.\nRATING-D: The clue is irrelevant and/or incorrect with respect to the answer or the context.\nRATING-E: Not acceptable clue because it contains the answer (or a variant of it).\nWe also allow annotators to skip examples (marked with SKIP), in case there are issues not strictly related to the clue itself, such as odd keywords or documents.\nOverall, 600 examples were annotated, for a total of 1,800 clues evaluated, since there are three clues proposed by the model for each given context, keyword, and category triplet. We report rating distributions in Figure 7  ###reference_###  ###reference_###. More than two out of three (about 72%) clues were marked with RATING-A, the highest score, which grows to 81% if we consider as acceptable also the clues rated with B.\n###figure_12### ###figure_13###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": "We investigate the usage of clue-instruct to fine-tune different families of LLMs with various dimensions.\nLLMs were trained for instruction tuning on clue-instruct. We kept the 600 annotated examples as a test and used them to evaluate all of our models using GPT-3.5 Turbo as an oracle. The remaining 43,475 examples were used for training.\nLLMs were instructed with the same prompt used for GPT-3.5 Turbo depicted in Figure 2  ###reference_###.\nWe focus on four instruction-tuned LLMs: Llama2-chat Touvron et al. (2023  ###reference_b27###) in 7B and 13B sizes, and mpt-instruct Team (2023  ###reference_b26###) in both 7B and 30B releases.\nAll the models were fine-tuned with LORA Hu et al. (2021  ###reference_b15###), , and  over the course of two training epochs and batch size set to 32. Learning rate was initialized to  with a linear warm-up of 200 steps.\nAt inference time, clues were generated by sampling from the model distribution. The temperature was set to 0.1, while top- and top- (Holtzman et al., 2019  ###reference_b14###) were set to 0.75 and 50, respectively. All the experimentation was carried out on a server equipped with four NVIDIA A6000 GPUs.\nFirst of all, we evaluate the four baseline models in zero-shot, i.e. without any fine-tuning on clue-instruct. Comparison is shown in Table 3  ###reference_###. Despite being previously trained to follow generic instructions, all the models struggle to produce a valid set of clues. Results are in general not satisfactory. In particular, Llama27B-chat always fails to produce an acceptable output with the given prompt. Probably, different prompt designs would have led to better results for such a model, however, this inquiry goes beyond the goals of our paper. Also mpt-instruct-7B often poorly fails to produce the correct JSON output and often generates a single clue, instead of the three requested. With the increase of models’ parameters, also the quality grows. Both Llama2-13B-chat and mpt-instruct-30B have higher ROUGE-L scores, with the former slightly better than the latter. This is mainly due to the fact that Llama2-13B-chat always produced the exact JSON schema, whereas mpt-instruct-30B failed almost once every four times.\nWhen finetuning the baseline LLMs on clue-instruct, all the models exhibit a remarkable improvement. Such a comparison is clearly shown in Figure 8  ###reference_###. ROUGE-L results are outlined in Table 3  ###reference_###. The outputs always align with the expected format. Finetuned LLMs surpass off-the-shelf models by a large margin, with an increase above 20 points in ROUGE-L. It is worth noticing that, Llama2-chat 13B is confirmed to be the best model, and that Llama2-chat 7B can recover from catastrophic results.\nAll the finetuned LLMs are publicly available222https://huggingface.co/azugarini/clue-instruct-llama-7b  ###reference_ruct-llama-7b###,333https://huggingface.co/azugarini/clue-instruct-llama-13b  ###reference_ruct-llama-13b###,444https://huggingface.co/azugarini/clue-instruct-mpt-7b  ###reference_ruct-mpt-7b###,555https://huggingface.co/azugarini/clue-instruct-mpt-30b  ###reference_ruct-mpt-30b###.\nAnalyzing the results from Table 3  ###reference_###, we can notice that larger models tend to outperform smaller ones. In particular, larger LLMs are more robust to unseen instructions, thus showing wider gaps when not finetuned on the downstream task. Moreover, we can observe that Llama2-chat 13B model is particularly well-performing, surpassing mpt-instruct 30B, which is more than twice its size, as already observed in the literature.\nWe also measure how the performance changes when using different amounts of training examples. Training size was cut at , , and , to see the trend at different orders of magnitude. To slightly cope with the reduced amount of training steps, we increase the number of epochs to 3 for  and  pieces of training, and we reduce the number of warm-up gradient steps to . In this experiment, we only focus for simplicity on the Llama2 family (7B, 13B). From the results, outlined in Figure 10  ###reference_###, we can observe that a small number of examples are enough to align the LLMs to the task, even for Llama2-chat-7B that failed to produce valid clues when applied as zero-shot. Thus, the biggest leap in performance is given by just a small amount of instructions, coherently with findings in literature (Zhou et al., 2023  ###reference_b35###).\n###figure_14### In addition to automatic evaluation, human annotators are asked to evaluate the output of the fine-tuned LLMs. We compare the models on a portion of 100 documents of the test set. Due to the poor performance of off-the-shelf models, we only consider Llama2-chat-13B in this evaluation with the purpose of highlighting once again the differences between base models and their fine-tuned versions on clue-instruct. In addition to rating scores, we marked as [EMPTY] all examples where a clue was not produced. We report the results in Figure 9  ###reference_###. All the tuned models exhibit a major reduction of malformed outputs ([EMPTY]). In contrast, the number of A-rated examples suddenly increased.\nAlso, D-rated examples diminish, whereas B, C, and E rates have a slight increase, with some exceptions. These results suggest that finetuning is extremely effective in aligning the generated output to the expected format, but there is also a positive contribution to the quality of the generated clues.\nTo help understanding what kind of clues were generated and the ratings assigned, we showcase some examples in Table 2  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Experimental Setup",
            "text": "LLMs were trained for instruction tuning on clue-instruct. We kept the 600 annotated examples as a test and used them to evaluate all of our models using GPT-3.5 Turbo as an oracle. The remaining 43,475 examples were used for training.\nLLMs were instructed with the same prompt used for GPT-3.5 Turbo depicted in Figure 2  ###reference_###  ###reference_###.\nWe focus on four instruction-tuned LLMs: Llama2-chat Touvron et al. (2023  ###reference_b27###  ###reference_b27###) in 7B and 13B sizes, and mpt-instruct Team (2023  ###reference_b26###  ###reference_b26###) in both 7B and 30B releases.\nAll the models were fine-tuned with LORA Hu et al. (2021  ###reference_b15###  ###reference_b15###), , and  over the course of two training epochs and batch size set to 32. Learning rate was initialized to  with a linear warm-up of 200 steps.\nAt inference time, clues were generated by sampling from the model distribution. The temperature was set to 0.1, while top- and top- (Holtzman et al., 2019  ###reference_b14###  ###reference_b14###) were set to 0.75 and 50, respectively. All the experimentation was carried out on a server equipped with four NVIDIA A6000 GPUs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Results",
            "text": "First of all, we evaluate the four baseline models in zero-shot, i.e. without any fine-tuning on clue-instruct. Comparison is shown in Table 3  ###reference_###  ###reference_###. Despite being previously trained to follow generic instructions, all the models struggle to produce a valid set of clues. Results are in general not satisfactory. In particular, Llama27B-chat always fails to produce an acceptable output with the given prompt. Probably, different prompt designs would have led to better results for such a model, however, this inquiry goes beyond the goals of our paper. Also mpt-instruct-7B often poorly fails to produce the correct JSON output and often generates a single clue, instead of the three requested. With the increase of models’ parameters, also the quality grows. Both Llama2-13B-chat and mpt-instruct-30B have higher ROUGE-L scores, with the former slightly better than the latter. This is mainly due to the fact that Llama2-13B-chat always produced the exact JSON schema, whereas mpt-instruct-30B failed almost once every four times.\nWhen finetuning the baseline LLMs on clue-instruct, all the models exhibit a remarkable improvement. Such a comparison is clearly shown in Figure 8  ###reference_###  ###reference_###. ROUGE-L results are outlined in Table 3  ###reference_###  ###reference_###. The outputs always align with the expected format. Finetuned LLMs surpass off-the-shelf models by a large margin, with an increase above 20 points in ROUGE-L. It is worth noticing that, Llama2-chat 13B is confirmed to be the best model, and that Llama2-chat 7B can recover from catastrophic results.\nAll the finetuned LLMs are publicly available222https://huggingface.co/azugarini/clue-instruct-llama-7b  ###reference_ruct-llama-7b###  ###reference_ruct-llama-7b###,333https://huggingface.co/azugarini/clue-instruct-llama-13b  ###reference_ruct-llama-13b###  ###reference_ruct-llama-13b###,444https://huggingface.co/azugarini/clue-instruct-mpt-7b  ###reference_ruct-mpt-7b###  ###reference_ruct-mpt-7b###,555https://huggingface.co/azugarini/clue-instruct-mpt-30b  ###reference_ruct-mpt-30b###  ###reference_ruct-mpt-30b###.\nAnalyzing the results from Table 3  ###reference_###  ###reference_###, we can notice that larger models tend to outperform smaller ones. In particular, larger LLMs are more robust to unseen instructions, thus showing wider gaps when not finetuned on the downstream task. Moreover, we can observe that Llama2-chat 13B model is particularly well-performing, surpassing mpt-instruct 30B, which is more than twice its size, as already observed in the literature.\nWe also measure how the performance changes when using different amounts of training examples. Training size was cut at , , and , to see the trend at different orders of magnitude. To slightly cope with the reduced amount of training steps, we increase the number of epochs to 3 for  and  pieces of training, and we reduce the number of warm-up gradient steps to . In this experiment, we only focus for simplicity on the Llama2 family (7B, 13B). From the results, outlined in Figure 10  ###reference_###  ###reference_###, we can observe that a small number of examples are enough to align the LLMs to the task, even for Llama2-chat-7B that failed to produce valid clues when applied as zero-shot. Thus, the biggest leap in performance is given by just a small amount of instructions, coherently with findings in literature (Zhou et al., 2023  ###reference_b35###  ###reference_b35###).\n###figure_15### In addition to automatic evaluation, human annotators are asked to evaluate the output of the fine-tuned LLMs. We compare the models on a portion of 100 documents of the test set. Due to the poor performance of off-the-shelf models, we only consider Llama2-chat-13B in this evaluation with the purpose of highlighting once again the differences between base models and their fine-tuned versions on clue-instruct. In addition to rating scores, we marked as [EMPTY] all examples where a clue was not produced. We report the results in Figure 9  ###reference_###  ###reference_###. All the tuned models exhibit a major reduction of malformed outputs ([EMPTY]). In contrast, the number of A-rated examples suddenly increased.\nAlso, D-rated examples diminish, whereas B, C, and E rates have a slight increase, with some exceptions. These results suggest that finetuning is extremely effective in aligning the generated output to the expected format, but there is also a positive contribution to the quality of the generated clues.\nTo help understanding what kind of clues were generated and the ratings assigned, we showcase some examples in Table 2  ###reference_###  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusions",
            "text": "In this paper, we presented a methodology to generate clues for educational crosswords, from which we constructed clue-instruct, an instruction-tuning dataset with keyword-clue pairs grounded on an input context, specifically designed for educational crosswords.\nTo the best of our knowledge, the corpus is the first resource that combines such information, which is necessary to build systems that can generate educational crosswords from a given document.\nWe then leveraged clue-instruct to fine-tune different open-source Large Language Models, showing that aligning LLMs to this kind of instructions greatly improves the output quality in terms of both automatic and human evaluation. Both the dataset and the models have been publicly released.\nIn the future, we plan to further extend our methodology to non-English languages in order to facilitate the diffusion of educational crosswords also in less represented languages."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgements",
            "text": "This work was supported by the IBRIDAI\nproject financed by the Regional Operational Program “FESR 2014-2020” of Emilia\nRomagna (Italy), resolution of the Regional Council n. 863/2021."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ]
}