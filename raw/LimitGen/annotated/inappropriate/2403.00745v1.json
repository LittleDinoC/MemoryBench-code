{
    "title": "AtP^*: An efficient and scalable method for localizing LLM behaviour to components",
    "abstract": "Activation Patching is a method of directly computing causal attributions of behavior to model components.\nHowever, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs).\nWe investigate Attribution Patching (AtP) (Nanda, 2022), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives.\nWe propose a variant of AtP called AtP, with two changes to address these failure modes while retaining scalability.\nWe present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP providing further significant improvement.\nFinally, we provide a method to bound the probability of remaining false negatives of AtP estimates.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As LLMs become ubiquitous and integrated into numerous digital applications, it’s an increasingly pressing research problem to understand the internal mechanisms that underlie their behaviour – this is the problem of mechanistic interpretability. A fundamental subproblem is to causally attribute particular behaviours to individual parts of the transformer forward pass, corresponding to specific components (such as attention heads, neurons, layer contributions, or residual streams), often at specific positions in the input token sequence. This is important because in numerous case studies of complex behaviours, they are found to be driven by sparse subgraphs within the model (Olsson et al., 2022  ###reference_b39###; Wang et al., 2022  ###reference_b54###; Meng et al., 2023  ###reference_b32###).\nA classic form of causal attribution uses zero-ablation, or knock-out, where a component is deleted and we see if this negatively affects a model’s output – a negative effect implies the component was causally important. More recent work has generalised this to replacing a component’s activations with samples from some baseline distribution (with zero-ablation being a special case where activations are resampled to be zero). We focus on the popular and widely used method of Activation Patching (also known as causal mediation analysis) (Geiger et al., 2022  ###reference_b15###; Meng et al., 2023  ###reference_b32###; Chan et al., 2022  ###reference_b6###) where the baseline distribution is a component’s activations on some corrupted input, such as an alternate string with a different answer (Pearl, 2001  ###reference_b41###; Robins and Greenland, 1992  ###reference_b44###).\nGiven a causal attribution method, it is common to sweep across all model components, directly evaluating the effect of intervening on each of them via resampling (Meng et al., 2023  ###reference_b32###). However, when working with SoTA models it can be expensive to attribute behaviour especially to small components (e.g. heads or neurons) – each intervention requires a separate forward pass, and so the number of forward passes can easily climb into the millions or billions. For example, on a prompt of length 1024, there are  neuron nodes in Chinchilla 70B (Hoffmann et al., 2022  ###reference_b23###).\nWe propose to accelerate this process by using Attribution Patching (AtP) (Nanda, 2022  ###reference_b36###), a faster, approximate, causal attribution method, as a prefiltering step: after running AtP, we iterate through the nodes in decreasing order of absolute value of the AtP estimate, then use Activation Patching to more reliably evaluate these nodes and filter out false positives – we call this verification. We typically care about a small set of top contributing nodes, so verification is far cheaper than iterating over all nodes.\nWe investigate the performance of AtP, finding two classes of failure modes which produce false negatives. We propose a variant of AtP called AtP, with two changes to address these failure modes while retaining scalability:\nWhen patching queries and keys, recomputing the attention softmax and using a gradient based approximation from then on, as gradients are a poor approximation to saturated attention.\nUsing dropout on the backwards pass to fix brittle false negatives, where significant positive and negative effects cancel out.\nWe introduce several alternative methods to approximate Activation Patching as baselines to AtP which outperform brute force Activation Patching.\nWe present the first systematic study of AtP and these alternatives and show that AtP significantly outperforms all other investigated methods, with AtP providing further significant improvement.\nTo estimate the residual error of AtP and statistically bound the sizes of any remaining false negatives we provide a diagnostic method, based on using AtP to filter out high impact nodes, and then patching random subsets of the remainder. Good diagnostics mean that practitioners may still gauge whether AtP is reliable in relevant domains without the costs of exhaustive verification.\nFinally, we provide some guidance in Section 5.4  ###reference_### on how to successfully perform causal attribution in practice and what attribution methods are likely to be useful and under what circumstances.\n###figure_1### ###figure_2### ###figure_3### \n###figure_4### ###figure_5###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We are given a model  that maps a prompt (token sequence)  to output logits over a set of  tokens, aiming to predict the next token in the sequence. We will view the model  as a computational graph  where the node set  is the set of model components, and a directed edge  is present iff the output of  is a direct input into the computation of . We will use  to represent the activation (intermediate computation result) of  when computing .\nThe choice of  determines how fine-grained the attribution will be. For example, for transformer models, we could have a relatively coarse-grained attribution where each layer is considered a single node. In this paper we will primarily consider more fine-grained attributions that are more expensive to compute (see Section 4  ###reference_### for details); we revisit this issue in Section 5  ###reference_###.\nFollowing past work (Geiger et al., 2022  ###reference_b15###; Chan et al., 2022  ###reference_b6###; Wang et al., 2022  ###reference_b54###), we assume a distribution  over pairs of inputs , where  is a prompt on which the behaviour occurs, and  is a reference prompt which we use as a source of noise to intervene with111This precludes interventions which use activation values that are never actually realized, such as zero-ablation or mean ablation. An alternative formulation via distributions of activation values is also possible..\nWe are also given a metric222Common metrics in language models are next token prediction loss, difference in log prob between a correct and incorrect next token, probability of the correct next token, etc. , which quantifies the behaviour of interest.\nSimilarly to the work referenced above we define the contribution  of a node  to the model’s behaviour as the counterfactual absolute333The sign of the impact may be of interest, but in this work we’ll focus on the magnitude, as a measure of causal importance. expected impact of replacing that node on the clean prompt with its value on the reference prompt .\nUsing do-calculus notation (Pearl, 2000  ###reference_b40###) this can be expressed as , where\nwhere we define the intervention effect  for  as\nNote that the need to average the effect across a distribution adds a potentially large multiplicative factor to the cost of computing , further motivating this work.\nWe can also intervene on a set of nodes . To do so, we overwrite the values of all nodes in  with their values from a reference prompt. Abusing notation, we write  as the set of activations of the nodes in , when computing .\nWe note that it is also valid to define contribution as the expected impact of replacing a node on the reference prompt with its value on the clean prompt, also known as denoising or knock-in. We follow Chan et al. (2022  ###reference_b6###); Wang et al. (2022  ###reference_b54###) in using noising, however denoising is also widely used in the literature (Meng et al., 2023  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###). We briefly consider how this choice affects AtP in Section 5.2  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Statement",
            "text": "Our goal is to identify the contributions to model behavior by individual model components. We first formalize model components, then formalize model behaviour, and finally state the contribution problem in causal language. While we state the formalism in terms of a decoder-only transformer language model (Vaswani et al., 2017  ###reference_b51###; Radford et al., 2018  ###reference_b42###), and conduct all our experiments on models of that class, the formalism is also straightforwardly applicable to other model classes.\nWe are given a model  that maps a prompt (token sequence)  to output logits over a set of  tokens, aiming to predict the next token in the sequence. We will view the model  as a computational graph  where the node set  is the set of model components, and a directed edge  is present iff the output of  is a direct input into the computation of . We will use  to represent the activation (intermediate computation result) of  when computing .\nThe choice of  determines how fine-grained the attribution will be. For example, for transformer models, we could have a relatively coarse-grained attribution where each layer is considered a single node. In this paper we will primarily consider more fine-grained attributions that are more expensive to compute (see Section 4  ###reference_###  ###reference_### for details); we revisit this issue in Section 5  ###reference_###  ###reference_###.\nFollowing past work (Geiger et al., 2022  ###reference_b15###  ###reference_b15###; Chan et al., 2022  ###reference_b6###  ###reference_b6###; Wang et al., 2022  ###reference_b54###  ###reference_b54###), we assume a distribution  over pairs of inputs , where  is a prompt on which the behaviour occurs, and  is a reference prompt which we use as a source of noise to intervene with111This precludes interventions which use activation values that are never actually realized, such as zero-ablation or mean ablation. An alternative formulation via distributions of activation values is also possible..\nWe are also given a metric222Common metrics in language models are next token prediction loss, difference in log prob between a correct and incorrect next token, probability of the correct next token, etc. , which quantifies the behaviour of interest.\nSimilarly to the work referenced above we define the contribution  of a node  to the model’s behaviour as the counterfactual absolute333The sign of the impact may be of interest, but in this work we’ll focus on the magnitude, as a measure of causal importance. expected impact of replacing that node on the clean prompt with its value on the reference prompt .\nUsing do-calculus notation (Pearl, 2000  ###reference_b40###  ###reference_b40###) this can be expressed as , where\nwhere we define the intervention effect  for  as\nNote that the need to average the effect across a distribution adds a potentially large multiplicative factor to the cost of computing , further motivating this work.\nWe can also intervene on a set of nodes . To do so, we overwrite the values of all nodes in  with their values from a reference prompt. Abusing notation, we write  as the set of activations of the nodes in , when computing .\nWe note that it is also valid to define contribution as the expected impact of replacing a node on the reference prompt with its value on the clean prompt, also known as denoising or knock-in. We follow Chan et al. (2022  ###reference_b6###  ###reference_b6###); Wang et al. (2022  ###reference_b54###  ###reference_b54###) in using noising, however denoising is also widely used in the literature (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###  ###reference_b28###). We briefly consider how this choice affects AtP in Section 5.2  ###reference_###  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Attribution Patching",
            "text": "On state of the art models, computing  for all  can be prohibitively expensive as there may be billions or more nodes. Furthermore, to compute this value precisely requires evaluating it on all prompt pairs, thus the runtime cost of Equation 1  ###reference_### for each  scales with the size of the support of .\nWe thus turn to a fast approximation of Equation 1  ###reference_###. As suggested by Nanda (2022  ###reference_b36###); Figurnov et al. (2016  ###reference_b10###); Molchanov et al. (2017  ###reference_b35###), we can make a first-order Taylor expansion to  around :\nThen, similarly to Syed et al. (2023  ###reference_b47###), we apply this to a distribution by taking the absolute value inside the expectation in Equation 1  ###reference_### rather than outside; this decreases the chance that estimates across prompt pairs with positive and negative effects might erroneously lead to a significantly smaller estimate. (We briefly explore the amount of cancellation behaviour in the true effect distribution in Section B.2  ###reference_###.) As a result, we get an estimate\nThis procedure is also called Attribution Patching (Nanda, 2022  ###reference_b36###) or AtP. AtP requires two forward passes and one backward pass to compute an estimate score for all nodes on a given prompt pair, and so provides a very significant speedup over brute force activation patching."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We now describe some failure modes of AtP and address them, yielding an improved method AtP*. We then discuss some alternative methods for estimating , to put AtP(*)’s performance in context. Finally we discuss how to combine Subsampling, one such alternative method described in Section 3.3  ###reference_###, and AtP* to give a diagnostic to statistically test whether AtP* may have missed important false negatives.\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_6### To provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_7### The most straightforward method is to directly do Activation Patching to find the true effect  of each node, in some uninformed random order. This is necessarily inefficient.\nHowever, if we are scaling to a distribution, it is possible to improve on this, by alternating between phases of (i) for each unverified node, picking a not-yet-measured prompt pair on which to patch it, (ii) ranking the not-yet-verified nodes by the average observed patch effect magnitudes, taking the top  nodes, and verifying them. This balances the computational expenditure on the two tasks, and allows us to find large nodes sooner, at least as long as their large effect shows up on many prompt pairs.\nOur remaining baseline methods rely on an approximate node additivity assumption: that when intervening on a set of nodes , the measured effect  is approximately equal to .\nUnder the approximate node additivity assumption, we can construct an approximately unbiased estimator of . We select the sets  to contain each node independently with some probability , and additionally sample prompt pairs . For any node , and sets of nodes , let  be the collection of all those that contain , and  be the collection of those that don’t contain ; we’ll write these node sets as  and , and the corresponding prompt pairs as  and . The subsampling (or subset sampling) estimator is then given by\nThe estimator  is unbiased if there are no interaction effects, and has a small bias proportional to  under a simple interaction model (see Section A.1.1  ###reference_.SSS1### for proof).\nIn practice, we compute all the estimates  by sampling a binary mask over all nodes from i.i.d. Bernoulli – each binary mask can be identified with a node set .\nIn Algorithm 1  ###reference_###, we describe how to compute summary statistics related to Equation 13  ###reference_### efficiently for all nodes . The means  are enough to compute , while other summary statistics are involved in bounding the magnitude of a false negative (cf. Section 3.2  ###reference_###). (Note,  is just an alternate notation for .)\nInstead of sampling each  independently, we can group nodes into fixed “blocks”  of some size, and patch each block to find its aggregated contribution ; we can then traverse the nodes, starting with high-contribution blocks and proceeding from there.\nThere is a tradeoff in terms of the block size: using large blocks increases the compute required to traverse a high-contribution block, but using small blocks increases the compute required to finish traversing all of the blocks. We refer to the fixed block size setting as Blocks. Another way to handle this tradeoff is to add recursion: the blocks can be grouped into higher-level blocks, and so forth. We call this method Hierarchical.\nWe present results from both methods in our comparison plots, but relegate details to Section A.1.2  ###reference_.SSS2###. Relative to subsampling, these grouping-based methods have the disadvantage that on distributions, their cost scales linearly with size of ’s support, in addition to scaling with the number of nodes777AtP* also scales linearly in the same way, but with far fewer forward passes per prompt pair.."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "AtP improvements",
            "text": "We identify two common classes of false negatives occurring when using AtP.\nThe first failure mode occurs when the preactivation on  is in a flat region of the activation function (e.g. produces a saturated attention weight), but the preactivation on  is not in that region. As is apparent from Equation 4  ###reference_###, AtP uses a linear approximation to the ground truth in Equation 1  ###reference_###, so if the non-linear function is badly approximated by the local gradient, AtP ceases to be accurate – see Figure 3  ###reference_### for an illustration and Figure 4  ###reference_### which denotes in color the maximal difference in attention observed between prompt pairs, suggesting that this failure mode occurs in practice.\n###figure_8### Another, unrelated failure mode occurs due to cancellation between direct and indirect effects: roughly, if the total effect (on some prompt pair) is a sum of direct and indirect effects (Pearl, 2001  ###reference_b41###) , and these are close to cancelling, then a small multiplicative approximation error in , due to non-linearities such as GELU and softmax, can accidentally cause  to be orders of magnitude smaller than .\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_###  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1###  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_###  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_###  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_9### To provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_###  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_10###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 False negatives from attention saturation",
            "text": "AtP relies on the gradient at each activation being reflective of the true behaviour of the function with respect to intervention at that activation. In some cases, though, a node may immediately feed into a non-linearity whose effect may not be adequately predicted by the gradient; for example, attention key and query nodes feeding into the attention softmax non-linearity. To showcase this, we plot the true rank of each node’s effect against its rank assigned by AtP in Figure 4  ###reference_### (left). The plot shows that there are many pronounced false negatives (below the dashed line), especially among keys and queries.\nNormal activation patching for queries and keys involves changing a query or key and then re-running the rest of the model, keeping all else the same. AtP takes a linear approximation to the entire rest of the model rather than re-running it. We propose explicitly re-computing the first step of the rest of the model, i.e. the attention softmax, and then taking a linear approximation to the rest. Formally, for attention key and query nodes, instead of using the gradient on those nodes directly, we take the difference in attention weight caused by that key or query, multiplied by the gradient on the attention weights themselves. This requires finding the change in attention weights from each key and query patch — but that can be done efficiently using (for all keys and queries in total) less compute than two transformer forward passes. This correction avoids the problem of saturated attention, while otherwise retaining the performance of AtP.\nFor the queries, we can easily compute the adjusted effect by running the model on  and caching the noise queries. We then run the model on  and cache the attention keys and weights. Finally, we compute the attention weights that result from combining all the keys from the  forward pass with the queries from the  forward pass. This costs approximately as much as the unperturbed attention computation of the transformer forward pass. For each query node  we refer to the resulting weight vector as , in contrast with the weights  from the clean forward pass. The improved attribution estimate for  is then\nFor the keys we first describe a simple but inefficient method. We again run the model on , caching the noise\nkeys. We also run it on , caching the clean queries and attention probabilities.\nLet key nodes for a single attention head be  and let \nbe the set of query nodes for the same head as node . We then define\nThe improved attribution estimate for  is then\nHowever, the procedure we just described is costly to execute as it requires  flops to naively compute Equation 9  ###reference_###  ###reference_###  ###reference_### for all  keys. In Section A.2.1  ###reference_.SSS1###  ###reference_.SSS1###  ###reference_.SSS1### we describe a more efficient variant that takes no more compute than the forward pass attention computation itself (requiring  flops). Since Equation 6  ###reference_###  ###reference_###  ###reference_### is also cheaper to compute than a forward pass, the full QK fix requires less than two transformer forward passes (since the latter also includes MLP computations).\nFor attention nodes we show the effects of applying the query and key fixes in Figure 4  ###reference_###  ###reference_###  ###reference_### (middle). We observe that the propagation of Q/K effects has a major impact on reducing the false negative rate.\n###figure_11###"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 False negatives from cancellation",
            "text": "This form of cancellation occurs when the backpropagated gradient from indirect effects is combined with the gradient from the direct effect. We propose a way to modify the backpropagation within the attribution patching to reduce this issue. If we artificially zero out the gradient at a downstream layer that contributes to the indirect effect, the cancellation is disrupted. (This is also equivalent to patching in clean activations at the outputs of the layer.) Thus we propose to do this iteratively, sweeping across the layers. Any node whose effect does not route through the layer being gradient-zeroed will have its estimate unaffected.\nWe call this method GradDrop. For every layer  in the model, GradDrop computes an AtP estimate for all nodes, where gradients on the residual contribution from  are set to 0, including the propagation to earlier layers. This provides a different estimate for all nodes, for each layer that was dropped. We call the so-modified gradient  when dropping layer , where  is the contribution to the residual stream across all positions. Using  in place of  in the AtP formula produces an estimate . Then, the estimates are aggregated by averaging their absolute values, and then scaling by  to avoid changing the direct-effect path’s contribution (which is otherwise zeroed out when dropping the layer the node is in).\nNote that the forward passes required for computing  don’t depend on , so the extra compute needed for GradDrop is  backwards passes from the same intermediate activations on a clean forward pass. This is also the case with the QK fix: the corrected attributions  are dot products with the attention weight gradients, so the only thing that needs to be recomputed for  is the modified gradient . Thus, computing Equation 11  ###reference_### takes  backwards passes444This can be reduced to  by reusing intermediate results. on top of the costs for AtP.\nWe show the result of applying GradDrop on attention nodes in Figure 4  ###reference_### (right) and on MLP nodes in Figure 5  ###reference_###. In Figure 5  ###reference_###, we show the true effect magnitude rank against the AtP+GradDrop rank, while highlighting nodes which improved drastically by applying GradDrop. We give some arguments and intuitions on the benefit of GradDrop in Section A.2.2  ###reference_.SSS2###.\nTo provide some evidence that the observed false negatives are due to cancellation, we compute the ratio between the direct effect  and the total effect . A higher direct effect ratio indicates more cancellation. We observe that the most significant false negatives corrected by GradDrop in Figure 5  ###reference_###  ###reference_###  ###reference_### (highlighted) have high direct effect ratios of , , and  (no direct effect) , while the median direct effect ratio of all nodes is  (if counting all nodes) or  (if only counting nodes that have direct effect). Note that direct effect ratio is only applicable to nodes which in fact have a direct connection to the output, and not e.g. to MLP nodes at non-final token positions, since all disconnected nodes have a direct effect of 0 by definition.\n###figure_12###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Diagnostics",
            "text": "Despite the improvements we have proposed in Section 3.1  ###reference_###, there is no guarantee that AtP* produces no false negatives. Thus, it is desirable to obtain an upper confidence bound on the effect size of nodes that might be missed by AtP*, i.e. that aren’t in the top  AtP* estimates, for some . Let the top  nodes be . It so happens that we can use subset sampling to obtain such a bound.\nAs described in Algorithm 1  ###reference_### and Section 3.3  ###reference_.SSS0.Px2###, the subset sampling algorithm returns summary statistics: ,  and  for each node : the average effect size  of a subset conditional on the node being contained in that subset () or not (), the sample standard deviations , and the sample sizes . Given these, consider a null hypothesis555This is an unconventional form of  – typically a null hypothesis will say that an effect is insignificant. However, the framework of statistical hypothesis testing is based on determining whether the data let us reject the null hypothesis, and in this case the hypothesis we want to reject is the presence, rather than the absence, of a significant false negative.  that , for some threshold , versus the alternative hypothesis  that . We use a one-sided Welch’s t-test666This relies on the populations being approximately unbiased and normally distributed, and not skewed. This tended to be true on inspection, and it’s what the additivity assumption (see Section 3.3  ###reference_.SSS0.Px2###) predicts for a single prompt pair — but a nonparametric bootstrap test may be more reliable, at the cost of additional compute. to test this hypothesis; the general practice with a compound null hypothesis is to select the simple sub-hypothesis that gives the greatest -value, so to be conservative, the simple null hypothesis is that , giving a test statistic of , which gives a -value of .\nTo get a combined conclusion across all nodes in , let’s consider the hypothesis  that any of those nodes has true effect . Since this is also a compound null hypothesis,  is the corresponding -value. Then, to find an upper confidence bound with specified confidence level , we invert this procedure to find the lowest  for which we still have at least that level of confidence. We repeat this for various settings of the sample size  in Algorithm 1  ###reference_###. The exact algorithm is described in Section A.3  ###reference_###.\nIn Figure 6  ###reference_###, we report the upper confidence bounds at confidence levels 90%, 99%, 99.9% from running Algorithm 1  ###reference_### with a given  (right subplots), as well as the number of nodes that have a true contribution  greater than  (left subplots).\n###figure_13### ###figure_14###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "The most straightforward method is to directly do Activation Patching to find the true effect  of each node, in some uninformed random order. This is necessarily inefficient.\nHowever, if we are scaling to a distribution, it is possible to improve on this, by alternating between phases of (i) for each unverified node, picking a not-yet-measured prompt pair on which to patch it, (ii) ranking the not-yet-verified nodes by the average observed patch effect magnitudes, taking the top  nodes, and verifying them. This balances the computational expenditure on the two tasks, and allows us to find large nodes sooner, at least as long as their large effect shows up on many prompt pairs.\nOur remaining baseline methods rely on an approximate node additivity assumption: that when intervening on a set of nodes , the measured effect  is approximately equal to .\nUnder the approximate node additivity assumption, we can construct an approximately unbiased estimator of . We select the sets  to contain each node independently with some probability , and additionally sample prompt pairs . For any node , and sets of nodes , let  be the collection of all those that contain , and  be the collection of those that don’t contain ; we’ll write these node sets as  and , and the corresponding prompt pairs as  and . The subsampling (or subset sampling) estimator is then given by\nThe estimator  is unbiased if there are no interaction effects, and has a small bias proportional to  under a simple interaction model (see Section A.1.1  ###reference_.SSS1###  ###reference_.SSS1### for proof).\nIn practice, we compute all the estimates  by sampling a binary mask over all nodes from i.i.d. Bernoulli – each binary mask can be identified with a node set .\nIn Algorithm 1  ###reference_###  ###reference_###, we describe how to compute summary statistics related to Equation 13  ###reference_###  ###reference_### efficiently for all nodes . The means  are enough to compute , while other summary statistics are involved in bounding the magnitude of a false negative (cf. Section 3.2  ###reference_###  ###reference_###). (Note,  is just an alternate notation for .)\nInstead of sampling each  independently, we can group nodes into fixed “blocks”  of some size, and patch each block to find its aggregated contribution ; we can then traverse the nodes, starting with high-contribution blocks and proceeding from there.\nThere is a tradeoff in terms of the block size: using large blocks increases the compute required to traverse a high-contribution block, but using small blocks increases the compute required to finish traversing all of the blocks. We refer to the fixed block size setting as Blocks. Another way to handle this tradeoff is to add recursion: the blocks can be grouped into higher-level blocks, and so forth. We call this method Hierarchical.\nWe present results from both methods in our comparison plots, but relegate details to Section A.1.2  ###reference_.SSS2###  ###reference_.SSS2###. Relative to subsampling, these grouping-based methods have the disadvantage that on distributions, their cost scales linearly with size of ’s support, in addition to scaling with the number of nodes777AtP* also scales linearly in the same way, but with far fewer forward passes per prompt pair.."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "When attributing model behavior to components, an important choice is the partition of the model’s computational graph into units of analysis or ‘nodes’  (cf. Section 2.1  ###reference_###). We investigate two settings for the choice of , AttentionNodes and NeuronNodes. For NeuronNodes, each MLP neuron888We use the neuron post-activation for the node; this makes no difference when causally intervening, but for AtP it’s beneficial, because it makes the  function more linear. is a separate node. For AttentionNodes, we consider the query, key, and value vector for each head as distinct nodes, as well as the pre-linear per-head attention output999We include the output node because it provides additional information about what function an attention head is serving, particularly in the case where its queries have negligible patch effects relative to its keys and/or values. This may happen as a result of choosing  such that the query does not differ across the prompts.. We also refer to these units as ‘sites’. For each site, we consider each copy of that site at different token positions as a separate node. As a result, we can identify each node  with a pair  from the product TokenPosition  Site. Since our two settings for  are using a different level of granularity and are expected to have different per-node effect magnitudes, we present results on them separately.\nWe investigate transformer language models from the Pythia suite (Biderman et al., 2023  ###reference_b2###) of sizes between 410M and 12B parameters. This allows us to demonstrate that our methods are applicable across scale. Our cost-of-verified-recall plots in Figures 1  ###reference_###, 7  ###reference_### and 8  ###reference_### refer to Pythia-12B. Results for other model sizes are presented via the relative-cost (cf. Section 4.2  ###reference_.SSS0.Px2###) plots in the main body Figure 9  ###reference_### and disaggregated via cost-of-verified recall in Section B.3  ###reference_###.\nAll reported results use the negative log probability101010Another popular metric is the difference in logits between the clean and noise target. As opposed to the negative logprob, the logit difference is linear in the final logits and thus might favor AtP. A downside of logit difference is that it is sensitive to the noise target, which may not be meaningful if there are multiple plausible completions, such as in IOI. as their loss function . We compute  relative to targets from the clean prompt . We briefly explore other metrics in Section B.4  ###reference_###.\nAs mentioned in the introduction, we’re primarily interested in finding the largest-effect nodes – see Appendix D  ###reference_### for the distribution of  across models and distributions.\nOnce we have obtained node estimates via a given method, it is relatively cheap to directly measure true effects of top nodes one at a time; we refer to this as “verification”. Incorporating this into our methodology, we find that false positives are typically not a big issue; they are simply revealed during verification. In contrast, false negatives are not so easy to remedy without verifying all nodes, which is what we were trying to avoid.\nWe compare methods on the basis of total compute cost (in # of forward passes) to verify the  nodes with biggest true effect magnitude, for varying . The procedure being measured is to first compute estimates (incurring an estimation cost), and then sweep through nodes in decreasing order of estimated magnitude, measuring their individual effects  (i.e. verifying them), and incurring a verification cost. Then the total cost is the sum of these two costs.\nSometimes we find it useful to summarize the method performance with a scalar; this is useful for comparing methods at a glance across different settings (e.g. model sizes, as in Figure 2  ###reference_###), or for selecting hyperparameters (cf. Section B.5  ###reference_###). The cost of verified recall of the top  nodes is of interest for  at varying orders of magnitude. In order to avoid the performance metric being dominated by small or large , we assign similar total weight to different orders of magnitude: we use a weighted average with weight  for the cost of the top  nodes. Similarly, since the costs themselves may have different orders of magnitude, we average them on a log scale – i.e., we take a geometric mean.\nThis metric is also proportional to the area under the curve in plots like Figure 1  ###reference_###. To produce a more understandable result, we always report it relative to (i.e. divided by) the oracle verification cost on the same metric; the diagonal line is the oracle, with relative cost 1. We refer to this as the IRWRGM (inverse-rank-weighted relative geometric mean) cost, or the relative cost.\nNote that the preference of the individual practitioner may be different such that this metric is no longer accurately measuring the important rank regime. For example, AtP* pays a notable upfront cost relative to AtP or AtP+QKfix, which sets it at a disadvantage when it doesn’t manage to find additional false negatives; but this may or may not be practically significant. To understand the performance in more detail we advise to refer to the cost of verified recall plots, like Figure 1  ###reference_### (or many more in Section B.3  ###reference_###).\nAs a starting point we report results on single prompt pairs which we expect to have relatively clean circuitry111111Formally, these represent prompt distributions via the delta distribution  where  is the singular prompt pair..\nAll singular prompt pairs are shown in Table 1  ###reference_###. IOI-PP is chosen to resemble an instance from the indirect object identification (IOI) task (Wang et al., 2022  ###reference_b54###), a task predominantly involving attention heads. CITY-PP is chosen to elicit factual recall which previous research suggests involves early MLPs and a small number of late attention heads (Meng et al., 2023  ###reference_b32###; Geva et al., 2023  ###reference_b17###; Nanda et al., 2023  ###reference_b37###). The country/city combinations were chosen such that Pythia-410M achieved low loss on both  and  and such that all places were represented by a single token.\n###table_1### We show the cost of verified 100% recall for various methods in Figure 1  ###reference_###, where we focus on NeuronNodes for CITY-PP and AttentionNodes for IOI-PP. Exhaustive results for smaller Pythia models are shown in Section B.3  ###reference_###. Figure 2  ###reference_### shows the aggregated relative costs for all models on CITY-PP and IOI-PP.\nInstead of applying the strict criterion of recalling all important nodes, we can also relax this constraint. In Figure 7  ###reference_###, we show the cost of verified 90% recall in the two clean prompt pair settings.\nThe previous prompt pairs may in fact be the best-case scenarios: the interventions they create will be fairly localized to a specific circuit, and this may make it easy for AtP to approximate the contributions. It may thus be informative to see how the methods generalize to settings where the interventions are less surgical. To do this, we also report results in Figure 8  ###reference_### (top) and Figure 9  ###reference_### on a random prompt pair chosen from a non-copyright-protected section of The Pile (Gao et al., 2020  ###reference_b12###) which we refer to as RAND-PP. The prompt pair was chosen such that Pythia-410M still achieved low loss on both prompts.\n###figure_15### ###figure_16### ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23### We find that AtP/AtP* is only somewhat less effective here; this provides tentative evidence that the strong performance of AtP/AtP* isn’t reliant on the clean prompt using a particularly crisp circuit, or on the noise prompt being a precise control.\nCausal attribution is often of most interest when evaluated across a distribution, as laid out in Section 2  ###reference_###. Of the methods, AtP, AtP*, and Subsampling scale reasonably to distributions; the former 2 because they’re inexpensive so running them  times is not prohibitive, and Subsampling because it intrinsically averages across the distribution and thus becomes proportionally cheaper relative to the verification via activation patching. In addition, having a distribution enables a more performant Iterative method, as described in Section 3.3  ###reference_.SSS0.Px1###.\nWe present a comparison of these methods on 2 distributional settings. The first is a reduced version of IOI (Wang et al., 2022  ###reference_b54###) on 6 names, resulting in  prompt pairs, where we evaluate AttentionNodes. The other distribution prompts the model to output an indefinite article ‘ a’ or ‘ an’, where we evaluate NeuronNodes. See Section B.1  ###reference_### for details on constructing these distributions. Results are shown in Figure 8  ###reference_### for Pythia 12B, and in Figure 9  ###reference_### across models. The results show that AtP continues to perform well, especially with the QK fix; in addition, the cancellation failure mode tends to be sensitive to the particular input prompt pair, and as a result, averaging across a distribution diminishes the benefit of GradDrops.\nAn implication of Subsampling scaling well to this setting is that diagnostics may give reasonable confidence in not missing false negatives with much less overhead than in the single-prompt-pair case; this is illustrated in Figure 6  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "When attributing model behavior to components, an important choice is the partition of the model’s computational graph into units of analysis or ‘nodes’ (cf. Section 2.1 ###reference_### ###reference_###). We investigate two settings for the choice of nodes, AttentionNodes and NeuronNodes. For NeuronNodes, each MLP neuron888We use the neuron post-activation for the node; this makes no difference when causally intervening, but for Randomized Sampling of Node Effects without Systematic Ranking it’s beneficial, because it makes the function more linear. is a separate node. For AttentionNodes, we consider the query, key, and value vector for each head as distinct nodes, as well as the pre-linear per-head attention output999We include the output node because it provides additional information about what function an attention head is serving, particularly in the case where its queries have negligible patch effects relative to its keys and/or values. This may happen as a result of choosing nodes such that the query does not differ across the prompts. We also refer to these units as ‘sites’. For each site, we consider each copy of that site at different token positions as a separate node. As a result, we can identify each node with a pair from the product TokenPosition Site. Since our two settings for nodes are using a different level of granularity and are expected to have different per-node effect magnitudes, we present results on them separately. We investigate transformer language models from the Pythia suite (Biderman et al., 2023 ###reference_b2### ###reference_b2###) of sizes between 410M and 12B parameters. This allows us to demonstrate that our methods are applicable across scale. Our cost-of-verified-recall plots in Figures 1 ###reference_### ###reference_###, 7 ###reference_### ###reference_### and 8 ###reference_### ###reference_### refer to Pythia-12B. Results for other model sizes are presented via the relative-cost (cf. Section 4.2 ###reference_.SSS0.Px2### ###reference_.SSS0.Px2###) plots in the main body Figure 9 ###reference_### ###reference_### and disaggregated via cost-of-verified recall in Section B.3 ###reference_### ###reference_###. All reported results use the negative log probability101010Another popular metric is the difference in logits between the clean and noise target. As opposed to the negative logprob, the logit difference is linear in the final logits and thus might favor Randomized Sampling of Node Effects without Systematic Ranking. A downside of logit difference is that it is sensitive to the noise target, which may not be meaningful if there are multiple plausible completions, such as in IOI. as their loss function. We compute relative to targets from the clean prompt. We briefly explore other metrics in Section B.4 ###reference_### ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Measuring Effectiveness and Efficiency",
            "text": "As mentioned in the introduction, we’re primarily interested in finding the largest-effect nodes – see Appendix D  ###reference_###  ###reference_### for the distribution of  across models and distributions.\nOnce we have obtained node estimates via a given method, it is relatively cheap to directly measure true effects of top nodes one at a time; we refer to this as “verification”. Incorporating this into our methodology, we find that false positives are typically not a big issue; they are simply revealed during verification. In contrast, false negatives are not so easy to remedy without verifying all nodes, which is what we were trying to avoid.\nWe compare methods on the basis of total compute cost (in # of forward passes) to verify the  nodes with biggest true effect magnitude, for varying . The procedure being measured is to first compute estimates (incurring an estimation cost), and then sweep through nodes in decreasing order of estimated magnitude, measuring their individual effects  (i.e. verifying them), and incurring a verification cost. Then the total cost is the sum of these two costs.\nSometimes we find it useful to summarize the method performance with a scalar; this is useful for comparing methods at a glance across different settings (e.g. model sizes, as in Figure 2  ###reference_###  ###reference_###), or for selecting hyperparameters (cf. Section B.5  ###reference_###  ###reference_###). The cost of verified recall of the top  nodes is of interest for  at varying orders of magnitude. In order to avoid the performance metric being dominated by small or large , we assign similar total weight to different orders of magnitude: we use a weighted average with weight  for the cost of the top  nodes. Similarly, since the costs themselves may have different orders of magnitude, we average them on a log scale – i.e., we take a geometric mean.\nThis metric is also proportional to the area under the curve in plots like Figure 1  ###reference_###  ###reference_###. To produce a more understandable result, we always report it relative to (i.e. divided by) the oracle verification cost on the same metric; the diagonal line is the oracle, with relative cost 1. We refer to this as the IRWRGM (inverse-rank-weighted relative geometric mean) cost, or the relative cost.\nNote that the preference of the individual practitioner may be different such that this metric is no longer accurately measuring the important rank regime. For example, AtP* pays a notable upfront cost relative to AtP or AtP+QKfix, which sets it at a disadvantage when it doesn’t manage to find additional false negatives; but this may or may not be practically significant. To understand the performance in more detail we advise to refer to the cost of verified recall plots, like Figure 1  ###reference_###  ###reference_### (or many more in Section B.3  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Single Prompt Pairs versus Distributions",
            "text": "We focus many of our experiments on single prompt pairs. This is primarily because it’s easier to set up and get ground truth data. It’s also a simpler setting in which to investigate the question, and one that’s more universally applicable, since a distribution to generalize to is not always available.\n###figure_24### ###figure_25### As a starting point we report results on single prompt pairs which we expect to have relatively clean circuitry111111Formally, these represent prompt distributions via the delta distribution  where  is the singular prompt pair..\nAll singular prompt pairs are shown in Table 1  ###reference_###  ###reference_###. IOI-PP is chosen to resemble an instance from the indirect object identification (IOI) task (Wang et al., 2022  ###reference_b54###  ###reference_b54###), a task predominantly involving attention heads. CITY-PP is chosen to elicit factual recall which previous research suggests involves early MLPs and a small number of late attention heads (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Geva et al., 2023  ###reference_b17###  ###reference_b17###; Nanda et al., 2023  ###reference_b37###  ###reference_b37###). The country/city combinations were chosen such that Pythia-410M achieved low loss on both  and  and such that all places were represented by a single token.\n###table_2### We show the cost of verified 100% recall for various methods in Figure 1  ###reference_###  ###reference_###, where we focus on NeuronNodes for CITY-PP and AttentionNodes for IOI-PP. Exhaustive results for smaller Pythia models are shown in Section B.3  ###reference_###  ###reference_###. Figure 2  ###reference_###  ###reference_### shows the aggregated relative costs for all models on CITY-PP and IOI-PP.\nInstead of applying the strict criterion of recalling all important nodes, we can also relax this constraint. In Figure 7  ###reference_###  ###reference_###, we show the cost of verified 90% recall in the two clean prompt pair settings.\nThe previous prompt pairs may in fact be the best-case scenarios: the interventions they create will be fairly localized to a specific circuit, and this may make it easy for AtP to approximate the contributions. It may thus be informative to see how the methods generalize to settings where the interventions are less surgical. To do this, we also report results in Figure 8  ###reference_###  ###reference_### (top) and Figure 9  ###reference_###  ###reference_### on a random prompt pair chosen from a non-copyright-protected section of The Pile (Gao et al., 2020  ###reference_b12###  ###reference_b12###) which we refer to as RAND-PP. The prompt pair was chosen such that Pythia-410M still achieved low loss on both prompts.\n###figure_26### ###figure_27### ###figure_28### ###figure_29### ###figure_30### ###figure_31### ###figure_32### ###figure_33### ###figure_34### We find that AtP/AtP* is only somewhat less effective here; this provides tentative evidence that the strong performance of AtP/AtP* isn’t reliant on the clean prompt using a particularly crisp circuit, or on the noise prompt being a precise control.\nCausal attribution is often of most interest when evaluated across a distribution, as laid out in Section 2  ###reference_###  ###reference_###. Of the methods, AtP, AtP*, and Subsampling scale reasonably to distributions; the former 2 because they’re inexpensive so running them  times is not prohibitive, and Subsampling because it intrinsically averages across the distribution and thus becomes proportionally cheaper relative to the verification via activation patching. In addition, having a distribution enables a more performant Iterative method, as described in Section 3.3  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\nWe present a comparison of these methods on 2 distributional settings. The first is a reduced version of IOI (Wang et al., 2022  ###reference_b54###  ###reference_b54###) on 6 names, resulting in  prompt pairs, where we evaluate AttentionNodes. The other distribution prompts the model to output an indefinite article ‘ a’ or ‘ an’, where we evaluate NeuronNodes. See Section B.1  ###reference_###  ###reference_### for details on constructing these distributions. Results are shown in Figure 8  ###reference_###  ###reference_### for Pythia 12B, and in Figure 9  ###reference_###  ###reference_### across models. The results show that AtP continues to perform well, especially with the QK fix; in addition, the cancellation failure mode tends to be sensitive to the particular input prompt pair, and as a result, averaging across a distribution diminishes the benefit of GradDrops.\nAn implication of Subsampling scaling well to this setting is that diagnostics may give reasonable confidence in not missing false negatives with much less overhead than in the single-prompt-pair case; this is illustrated in Figure 6  ###reference_###  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We only considered a small set of prompt pair distributions, which often were limited to a single prompt pair, since evaluating the ground truth can be quite costly. While we aimed to evaluate on distributions that are reasonably representative, our results may not generalize to other distributions.\nIn the NeuronNodes setting, we took MLP neurons as our fundamental unit of analysis. However, there is mounting evidence (Bricken et al., 2023  ###reference_b3###) that the decomposition of signals into neuron contributions does not correspond directly to a semantically meaningful decomposition. Instead, achieving such a decomposition seems to require finding the right set of directions in neuron activation space (Bricken et al., 2023  ###reference_b3###; Gurnee et al., 2023  ###reference_b19###) – which we viewed as being out of scope for this paper. In Section 5.2  ###reference_### we further discuss the applicability of AtP to sparse autoencoders, a method of finding these decompositions.\nMore generally, we only considered relatively fine-grained nodes, because this is a case where very exhaustive verification is prohibitively expensive, justifying the need for an approximate, fast method. Nanda (2022  ###reference_b36###) speculate that AtP may perform worse on coarser components like full layers or entire residual streams, as a larger change may have more of a non-linear effect. There may still be benefit in speeding up such an analysis, particularly if the context length is long – our alternative methods may have something to offer here, though we leave investigation of this to future work.\nIt is popular in the literature to do Activation Patching with these larger components, with short contexts – this doesn’t pose a performance issue, and so our work would not provide any benefit here.\nIn this work we took the ground truth of activation patching, as defined in Equation 1  ###reference_###, as our evaluation target.\nAs discussed by McGrath et al. (2023  ###reference_b31###), Equation 1  ###reference_### often significantly disagrees with a different evaluation target, the “direct effect”, by putting lower weight on some contributions when later components would shift their behaviour to compensate for the earlier patched component. In the worst case this could be seen as producing additional false negatives not accounted for by our metrics. To some degree this is likely to be mitigated by the GradDrop formula in Eq. 11  ###reference_###, which will include a term dropping out the effect of that downstream shift.\nHowever, it is also questionable whether we need to concern ourselves with finding high-direct-effect nodes. For example, direct effect is easy to efficiently compute for all nodes, as explored by nostalgebraist (2020  ###reference_b38###) – so there is no need for fast approximations like AtP if direct effect is the quantity of interest. This ease of computation is no free lunch, though, because direct effect is also more limited as a tool for finding causally important nodes: it would not be able to locate any nodes that contribute only instrumentally to the circuit rather than producing its output. For example, there is no direct effect from nodes at non-final token positions. We discuss the direct effect further in Section 3.1.2  ###reference_.SSS2### and Section A.2.2  ###reference_.SSS2###.\nAnother nuance of our ground–truth definition occurs in the distributional setting. Some nodes may have a real and significant effect, but only on a single clean prompt (e.g. they only respond to a particular name in IOI121212We did observe this particular behavior in a few instances. or object in A-AN). Since the effect is averaged over the distribution, the ground truth will not assign these nodes large causal importance. Depending on the goal of the practitioner this may or may not be desirable.\nWhen evaluating the performance of various estimators, we focused on evaluating the relative rank of estimates, since our main goal was to identify important components (with effect size only instrumentally useful to this end), and we assumed a further verification step of the nodes with highest estimated effects one at a time, in contexts where knowing effect size is important. Thus, we do not present evidence about how closely the estimated effect magnitudes from AtP or AtP* match the ground truth. Similarly, we did not assess the prevalence of false positives in our analysis, because they can be filtered out via the verification process. Finally, we did not compare to past manual interpretability work to check whether our methods find the same nodes to be causally important as discovered by human researchers, as done in prior work (Conmy et al., 2023  ###reference_b7###; Syed et al., 2023  ###reference_b47###).\nWhile we think it likely that our results on the Pythia model family (Biderman et al., 2023  ###reference_b2###) will transfer to other LLM families, we cannot rule out qualitatively different behavior without further evidence, especially on SotA–scale models or models that significantly deviate from the standard decoder-only transformer architecture.\nWhile we focus on computing the effects of individual nodes, edge activation patching can give more fine-grained information about which paths in the computational graph matter. However, it suffers from an even larger blowup in number of forward passes if done naively. Fortunately, AtP is easy to generalize to estimating the effects of edges between nodes (Nanda, 2022  ###reference_b36###; Syed et al., 2023  ###reference_b47###), while AtP* may provide further improvement. We discuss edge-AtP, and how to efficiently carry over the insights from AtP*, in Section C.2  ###reference_###.\nWe focused on fine-grained attribution, rather than full layers or sliding windows (Meng et al., 2023  ###reference_b32###; Geva et al., 2023  ###reference_b17###). In the latter case there’s less computational blowup to resolve, but for long contexts there may still be benefit in considering speedups like ours; on the other hand, they may be less linear, thus favouring other methods over AtP*. We leave investigation of this to future work.\nNanda (2022  ###reference_b36###) observed that AtP’s approximation to layer normalization may be a worse approximation when it comes to patching larger/coarser nodes: on average the patched and clean activations are likely to have similar norm, but may not have high cosine-similarity. They recommend treating the denominator in layer normalization as fixed, e.g. using a stop-gradient operator in the implementation. In Section C.1  ###reference_### we explore the effect of this, and illustrate the behaviour of this alternative form of AtP. It seems likely that this variant would indeed produce better results particularly when patching residual-stream nodes – but we leave empirical investigation of this to future work.\nDenoising (Meng et al., 2023  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###) is a different use case for patching, which may produce moderately different results: the difference is that each forward pass is run on  with the activation to patch taken from  — colloquially, this tests whether the patched activation is sufficient to recover model performance on , rather than necessary. We provide some preliminary evidence to the effect of this choice in Section B.4  ###reference_### but leave a more thorough investigation to future work.\nFurther, in some settings it may be of interest to do mean-ablation, or even zero-ablation, and our tweaks remain applicable there; the random-prompt-pair result suggests AtP* isn’t overly sensitive to the noise distribution, so we speculate the results are likely to carry over.\nA natural application of the methods we discussed in this work is the automatic identification and localization of sparse subgraphs or ‘circuits’ (Cammarata et al., 2020  ###reference_b4###). A variant of this was already discussed in concurrent work by Syed et al. (2023  ###reference_b47###) who combined edge attribution patching with the ACDC algorithm (Conmy et al., 2023  ###reference_b7###). As we mentioned in the edge patching discussion, AtP* can be generalized to edge attribution patching, which may bring additional benefit for automated circuit discovery.\nAnother approach is to learn a (probabilistic) mask over nodes, similar to Louizos et al. (2018  ###reference_b29###); Cao et al. (2021  ###reference_b5###), where the probability scales with the currently estimated node contribution . For that approach, a fast method to estimate all node effects given the current mask probabilities could prove vital.\nRecently there has been increased interest by the community in using sparse autoencoders (SAEs) to construct disentangled sparse representations with potentially more semantic coherence than transformer-native units such as neurons (Cunningham et al., 2023  ###reference_b8###; Bricken et al., 2023  ###reference_b3###).\nSAEs usually have a lot more nodes than the corresponding transformer block they are applied to. This could pose a larger problem in terms of the activation patching effects, making the speedup of AtP* more valuable. However, due to the sparseness of the SAE, on a given forward pass the effect of most features will be zero. For example, some successful SAEs by Bricken et al. (2023  ###reference_b3###) have 10-20 active features for 500 neurons for a given token position, which reduces the number of nodes by 20-50x relative to the MLP setting, increasing the scale at which existing iterative methods remain practical. It is still an open research question, however, what degree of sparsity is feasible with tolerable reconstruction error for practically relevant or SOTA–scale models, where the methods discussed in this work may become more important again.\nAtP* could be used to discover single nodes in the model that can be leveraged for targeted inference time interventions to control the model’s behavior. In contrast to previous work (Li et al., 2023  ###reference_b27###; Turner et al., 2023  ###reference_b50###; Zou et al., 2023  ###reference_b56###) it might provide more localized interventions with less impact on the rest of the model’s computation. One potential exciting direction would be to use AtP* (or other gradient-based approximations) to see which sparse autoencoder features, if activated, would have a significant effect."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Limitations",
            "text": "We only considered a small set of prompt pair distributions, which often were limited to a single prompt pair, since evaluating the ground truth can be quite costly. While we aimed to evaluate on distributions that are reasonably representative, our results may not generalize to other distributions.\nIn the NeuronNodes setting, we took MLP neurons as our fundamental unit of analysis. However, there is mounting evidence (Bricken et al., 2023  ###reference_b3###  ###reference_b3###) that the decomposition of signals into neuron contributions does not correspond directly to a semantically meaningful decomposition. Instead, achieving such a decomposition seems to require finding the right set of directions in neuron activation space (Bricken et al., 2023  ###reference_b3###  ###reference_b3###; Gurnee et al., 2023  ###reference_b19###  ###reference_b19###) – which we viewed as being out of scope for this paper. In Section 5.2  ###reference_###  ###reference_### we further discuss the applicability of AtP to sparse autoencoders, a method of finding these decompositions.\nMore generally, we only considered relatively fine-grained nodes, because this is a case where very exhaustive verification is prohibitively expensive, justifying the need for an approximate, fast method. Nanda (2022  ###reference_b36###  ###reference_b36###) speculate that AtP may perform worse on coarser components like full layers or entire residual streams, as a larger change may have more of a non-linear effect. There may still be benefit in speeding up such an analysis, particularly if the context length is long – our alternative methods may have something to offer here, though we leave investigation of this to future work.\nIt is popular in the literature to do Activation Patching with these larger components, with short contexts – this doesn’t pose a performance issue, and so our work would not provide any benefit here.\nIn this work we took the ground truth of activation patching, as defined in Equation 1  ###reference_###  ###reference_###, as our evaluation target.\nAs discussed by McGrath et al. (2023  ###reference_b31###  ###reference_b31###), Equation 1  ###reference_###  ###reference_### often significantly disagrees with a different evaluation target, the “direct effect”, by putting lower weight on some contributions when later components would shift their behaviour to compensate for the earlier patched component. In the worst case this could be seen as producing additional false negatives not accounted for by our metrics. To some degree this is likely to be mitigated by the GradDrop formula in Eq. 11  ###reference_###  ###reference_###, which will include a term dropping out the effect of that downstream shift.\nHowever, it is also questionable whether we need to concern ourselves with finding high-direct-effect nodes. For example, direct effect is easy to efficiently compute for all nodes, as explored by nostalgebraist (2020  ###reference_b38###  ###reference_b38###) – so there is no need for fast approximations like AtP if direct effect is the quantity of interest. This ease of computation is no free lunch, though, because direct effect is also more limited as a tool for finding causally important nodes: it would not be able to locate any nodes that contribute only instrumentally to the circuit rather than producing its output. For example, there is no direct effect from nodes at non-final token positions. We discuss the direct effect further in Section 3.1.2  ###reference_.SSS2###  ###reference_.SSS2### and Section A.2.2  ###reference_.SSS2###  ###reference_.SSS2###.\nAnother nuance of our ground–truth definition occurs in the distributional setting. Some nodes may have a real and significant effect, but only on a single clean prompt (e.g. they only respond to a particular name in IOI121212We did observe this particular behavior in a few instances. or object in A-AN). Since the effect is averaged over the distribution, the ground truth will not assign these nodes large causal importance. Depending on the goal of the practitioner this may or may not be desirable.\nWhen evaluating the performance of various estimators, we focused on evaluating the relative rank of estimates, since our main goal was to identify important components (with effect size only instrumentally useful to this end), and we assumed a further verification step of the nodes with highest estimated effects one at a time, in contexts where knowing effect size is important. Thus, we do not present evidence about how closely the estimated effect magnitudes from AtP or AtP* match the ground truth. Similarly, we did not assess the prevalence of false positives in our analysis, because they can be filtered out via the verification process. Finally, we did not compare to past manual interpretability work to check whether our methods find the same nodes to be causally important as discovered by human researchers, as done in prior work (Conmy et al., 2023  ###reference_b7###  ###reference_b7###; Syed et al., 2023  ###reference_b47###  ###reference_b47###).\nWhile we think it likely that our results on the Pythia model family (Biderman et al., 2023  ###reference_b2###  ###reference_b2###) will transfer to other LLM families, we cannot rule out qualitatively different behavior without further evidence, especially on SotA–scale models or models that significantly deviate from the standard decoder-only transformer architecture."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Extensions/Variants",
            "text": "While we focus on computing the effects of individual nodes, edge activation patching can give more fine-grained information about which paths in the computational graph matter. However, it suffers from an even larger blowup in number of forward passes if done naively. Fortunately, AtP is easy to generalize to estimating the effects of edges between nodes (Nanda, 2022  ###reference_b36###  ###reference_b36###; Syed et al., 2023  ###reference_b47###  ###reference_b47###), while AtP* may provide further improvement. We discuss edge-AtP, and how to efficiently carry over the insights from AtP*, in Section C.2  ###reference_###  ###reference_###.\nWe focused on fine-grained attribution, rather than full layers or sliding windows (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Geva et al., 2023  ###reference_b17###  ###reference_b17###). In the latter case there’s less computational blowup to resolve, but for long contexts there may still be benefit in considering speedups like ours; on the other hand, they may be less linear, thus favouring other methods over AtP*. We leave investigation of this to future work.\nNanda (2022  ###reference_b36###  ###reference_b36###) observed that AtP’s approximation to layer normalization may be a worse approximation when it comes to patching larger/coarser nodes: on average the patched and clean activations are likely to have similar norm, but may not have high cosine-similarity. They recommend treating the denominator in layer normalization as fixed, e.g. using a stop-gradient operator in the implementation. In Section C.1  ###reference_###  ###reference_### we explore the effect of this, and illustrate the behaviour of this alternative form of AtP. It seems likely that this variant would indeed produce better results particularly when patching residual-stream nodes – but we leave empirical investigation of this to future work.\nDenoising (Meng et al., 2023  ###reference_b32###  ###reference_b32###; Lieberum et al., 2023  ###reference_b28###  ###reference_b28###) is a different use case for patching, which may produce moderately different results: the difference is that each forward pass is run on  with the activation to patch taken from  — colloquially, this tests whether the patched activation is sufficient to recover model performance on , rather than necessary. We provide some preliminary evidence to the effect of this choice in Section B.4  ###reference_###  ###reference_### but leave a more thorough investigation to future work.\nFurther, in some settings it may be of interest to do mean-ablation, or even zero-ablation, and our tweaks remain applicable there; the random-prompt-pair result suggests AtP* isn’t overly sensitive to the noise distribution, so we speculate the results are likely to carry over."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Applications",
            "text": "A natural application of the methods we discussed in this work is the automatic identification and localization of sparse subgraphs or ‘circuits’ (Cammarata et al., 2020  ###reference_b4###  ###reference_b4###). A variant of this was already discussed in concurrent work by Syed et al. (2023  ###reference_b47###  ###reference_b47###) who combined edge attribution patching with the ACDC algorithm (Conmy et al., 2023  ###reference_b7###  ###reference_b7###). As we mentioned in the edge patching discussion, AtP* can be generalized to edge attribution patching, which may bring additional benefit for automated circuit discovery.\nAnother approach is to learn a (probabilistic) mask over nodes, similar to Louizos et al. (2018  ###reference_b29###  ###reference_b29###); Cao et al. (2021  ###reference_b5###  ###reference_b5###), where the probability scales with the currently estimated node contribution . For that approach, a fast method to estimate all node effects given the current mask probabilities could prove vital.\nRecently there has been increased interest by the community in using sparse autoencoders (SAEs) to construct disentangled sparse representations with potentially more semantic coherence than transformer-native units such as neurons (Cunningham et al., 2023  ###reference_b8###  ###reference_b8###; Bricken et al., 2023  ###reference_b3###  ###reference_b3###).\nSAEs usually have a lot more nodes than the corresponding transformer block they are applied to. This could pose a larger problem in terms of the activation patching effects, making the speedup of AtP* more valuable. However, due to the sparseness of the SAE, on a given forward pass the effect of most features will be zero. For example, some successful SAEs by Bricken et al. (2023  ###reference_b3###  ###reference_b3###) have 10-20 active features for 500 neurons for a given token position, which reduces the number of nodes by 20-50x relative to the MLP setting, increasing the scale at which existing iterative methods remain practical. It is still an open research question, however, what degree of sparsity is feasible with tolerable reconstruction error for practically relevant or SOTA–scale models, where the methods discussed in this work may become more important again.\nAtP* could be used to discover single nodes in the model that can be leveraged for targeted inference time interventions to control the model’s behavior. In contrast to previous work (Li et al., 2023  ###reference_b27###  ###reference_b27###; Turner et al., 2023  ###reference_b50###  ###reference_b50###; Zou et al., 2023  ###reference_b56###  ###reference_b56###) it might provide more localized interventions with less impact on the rest of the model’s computation. One potential exciting direction would be to use AtP* (or other gradient-based approximations) to see which sparse autoencoder features, if activated, would have a significant effect."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Recommendation",
            "text": "Our results suggest that if a practitioner is trying to do fast causal attribution, there are 2 main factors to consider: (i) the desired granularity of localization, and (ii) the confidence vs compute tradeoff.\nRegarding (i), the desired granularity, smaller components (e.g. MLP neurons or attention heads) are more numerous but more linear, likely yielding better results from gradient-based methods like AtP. We are less sure AtP will be a good approximation if patching layers or sliding windows of layers, and in this case practitioners may want to do normal patching. If the number of forward passes required remains prohibitive (e.g. a long context times many layers, when doing per token  layer patching), our other baselines may be useful. For a single prompt pair we particularly recommend trying Blocks, as it’s easy to make sense of; for a distribution we recommend Subsampling because it scales better to many prompt pairs.\nRegarding (ii), the confidence vs compute tradeoff, depending on the application, it may be desirable to run AtP as an activation patching prefilter followed by running the diagnostic to increase confidence. On the other hand, if false negatives aren’t a big concern then it may be preferable to skip the diagnostic – and if false positives aren’t either, then in certain cases practitioners may want to skip activation patching verification entirely. In addition, if the prompt pair distribution does not adequately highlight the specific circuit/behaviour of interest, this may also limit what can be learned from any localization methods.\nIf AtP is appropriate, our results suggest the best variant to use is probably AtP* for single prompt pairs, AtP+QKFix for AttentionNodes on distributions, and AtP for NeuronNodes (or other sites that aren’t immediately before a nonlinearity) on distributions.\nOf course, these recommendations are best-substantiated in settings similar to those we studied: focused prompt pairs / distribution, attention node or neuron sites, nodewise attribution, measuring cross-entropy loss on the clean-prompt next token. If departing from these assumptions we recommend looking before you leap."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "This work is concerned with identifying the effect of all (important) nodes in a causal graph (Pearl, 2000  ###reference_b40###), in the specific case where the graph represents a language model’s computation. A key method for finding important intermediate nodes in a causal graph is intervening on those nodes and observing the effect, which was first discussed under the name of causal mediation analysis by Robins and Greenland (1992  ###reference_b44###); Pearl (2001  ###reference_b41###).\nIn recent years there has been increasing success at applying the ideas of causal mediation analysis to identify causally important nodes in deep neural networks, in particular via the method of activation patching, where the output of a model component is intervened on. This technique has been widely used by the community and successfully applied in a range of contexts (Olsson et al., 2022  ###reference_b39###; Vig et al., 2020  ###reference_b53###; Soulos et al., 2020  ###reference_b45###; Meng et al., 2023  ###reference_b32###; Wang et al., 2022  ###reference_b54###; Hase et al., 2023  ###reference_b21###; Lieberum et al., 2023  ###reference_b28###; Conmy et al., 2023  ###reference_b7###; Hanna et al., 2023  ###reference_b20###; Geva et al., 2023  ###reference_b17###; Huang et al., 2023  ###reference_b24###; Tigges et al., 2023  ###reference_b48###; Merullo et al., 2023  ###reference_b33###; McDougall et al., 2023  ###reference_b30###; Goldowsky-Dill et al., 2023  ###reference_b18###; Stolfo et al., 2023  ###reference_b46###; Feng and Steinhardt, 2023  ###reference_b9###; Hendel et al., 2023  ###reference_b22###; Todd et al., 2023  ###reference_b49###; Cunningham et al., 2023  ###reference_b8###; Finlayson et al., 2021  ###reference_b11###; Nanda et al., 2023  ###reference_b37###).\nChan et al. (2022  ###reference_b6###) introduce causal scrubbing, a generalized algorithm to verify a hypothesis about the internal mechanism underlying a model’s behavior, and detail their motivation behind performing noising and resample ablation rather than denoising or using mean or zero ablation – they interpret the hypothesis as implying the computation is invariant to some large set of perturbations, so their starting-point is the clean unperturbed forward pass.131313Our motivation for focusing on noising rather than denoising was a closely related one – we were motivated by automated circuit discovery, where gradually noising more and more of the model is the basic methodology for both of the approaches discussed in Section 5.3  ###reference_###.\nAnother line of research concerning formalizing causal abstractions focuses on finding and verifying high-level causal abstractions of low-level variables (Geiger et al., 2020  ###reference_b13###, 2021  ###reference_b14###, 2022  ###reference_b15###, 2023  ###reference_b16###). See Jenner et al. (2022  ###reference_b25###) for more details on how these different frameworks agree and differ. In contrast to those works, we are chiefly concerned with identifying the important low-level variables in the computational graph and are not investigating their semantics or potential groupings of lower-level into higher-level variables.\nIn addition to causal mediation analysis, intervening on node activations in the model forward pass has also been studied as a way of steering models towards desirable behavior (Rimsky et al., 2023  ###reference_b43###; Zou et al., 2023  ###reference_b56###; Turner et al., 2023  ###reference_b50###; Jorgensen et al., 2023  ###reference_b26###; Li et al., 2023  ###reference_b27###; Belrose et al., 2023  ###reference_b1###).\nWhile we use the resample–ablation variant of AtP as formulated in Nanda (2022  ###reference_b36###), similar formulations have been used in the past to successfully prune deep neural networks (Figurnov et al., 2016  ###reference_b10###; Molchanov et al., 2017  ###reference_b35###; Michel et al., 2019  ###reference_b34###), or even identify causally important nodes for interpretability (Cao et al., 2021  ###reference_b5###). Concurrent work by Syed et al. (2023  ###reference_b47###) also demonstrates AtP can help with automatically finding causally important circuits in a way that agrees with previous manual circuit identification work. In contrast to Syed et al. (2023  ###reference_b47###), we provide further analysis of AtP’s failure modes, give improvements in the form of AtP, and evaluate both methods as well as several baselines on a suite of larger models against a ground truth that is independent of human researchers’ judgement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we have explored the use of attribution patching for node patch effect evaluation. We have compared attribution patching with alternatives and augmentations, characterized its failure modes, and presented reliability diagnostics. We have also discussed the implications of our contributions for other settings in which patching can be of interest, such as circuit discovery, edge localization, coarse-grained localization, and causal abstraction.\nOur results show that AtP* can be a more reliable and scalable approach to node patch effect evaluation than alternatives. However, it is important to be aware of the failure modes of attribution patching, such as cancellation and saturation. We explored these in some detail, and provided mitigations, as well as recommendations for diagnostics to ensure that the results are reliable.\nWe believe that our work makes an important contribution to the field of mechanistic interpretability and will help to advance the development of more reliable and scalable methods for understanding the behavior of deep neural networks."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Author Contributions",
            "text": "János Kramár was research lead, and Tom Lieberum was also a core contributor – both were highly involved in most aspects of the project. Rohin Shah and Neel Nanda served as advisors and gave feedback and guidance throughout."
        }
    ]
}