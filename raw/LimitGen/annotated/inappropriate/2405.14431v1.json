{
    "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
    "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose RaFe, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, RaFe provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that RaFe can obtain better performance than baselines.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have demonstrated strong capacities to solve a variety of tasks Zhao et al. (2023  ###reference_b53###).\nHowever, they still encounter the challenges of hallucinations (Ji et al., 2023  ###reference_b9###; Zhang et al., 2023  ###reference_b52###; Huang et al., 2023  ###reference_b8###) or outdated knowledge (Yao et al., 2023  ###reference_b48###; Zhang et al., 2024  ###reference_b51###).\nRecently, Retrieval Augmentation Generation (RAG) (Gao et al., 2023  ###reference_b7###) has become an important technology to enhance LLMs’ abilities, by incorporating external knowledge.\nFor instance, in open-domain QA, LLMs can firstly retrieve related documents and then generate answers.\nNonetheless, directly retrieving by original query does not always achieve correct and relevant documents.\nTherefore, query rewriting (Efthimiadis, 1996  ###reference_b6###; Carpineto and Romano, 2012  ###reference_b3###) has been widely employed to reformulate the query to expand the retrieved documents for a better response as illustrated in Figure 1  ###reference_###.\n###figure_1### Many efforts have been proposed to leverage the powerful LLMs to directly generate rewrites (Shen et al., 2023  ###reference_b36###; Wang et al., 2023  ###reference_b42###).\nWhile in practical applications, it is more prevalent to implement specific small query rewriting models to avoid the costly use of LLMs (Ma et al., 2023  ###reference_b22###).\nTo improve the performance of query rewriting, reinforcement learning (RL) with feedback  (Wu et al., 2022  ###reference_b43###; Chen et al., 2022  ###reference_b4###) can be utilized as a typical solution.\nFor instance, Nogueira and Cho (2017  ###reference_b26###) generates feedback by considering the recall of labeled documents.\nMeanwhile, Ma et al. (2023  ###reference_b22###) leverages evaluation results from question answering (QA) post-rewriting to generate signals.\nAdditionally, Peng et al. (2023  ###reference_b30###) employs domain-specific annotated rewriting scores for feedback training.\nNote that these feedback-driven query rewriting methods rely on either annotated labels such as relevant documents or answers, or pre-designed rewards tailored to specific domains.\nHowever, they often lack the utilization of effective and general signals for query rewriting.\nMeanwhile, considerable efforts have been made to harness diverse feedback mechanisms across various domains (Nathani et al., 2023  ###reference_b25###; Li et al., 2023  ###reference_b17###).\nNotably,  Liu et al. (2023b  ###reference_b21###) effectively integrates unit testing feedback into code generation, yielding significant efficacy.\nDrawing from these, in this paper we attempt to\n(i) reduce the cost of annotations for feedback;\nand (ii) identify a signal that better aligns with the objectives of the query rewriting task.\nTo address these issues, we introduce RaFe (Ranking Feedback improves Query Rewriting), a novel framework that leverages feedback from the reranker to train query rewriting models.\nThis approach is inspired by the reranker module in traditional information retrieval (IR) systems, which score and sort retrieved documents based on the query.\nIntuitively, query rewriting aims to retrieve documents relevant to the original query, which aligns perfectly with the goal of the reranker. Specifically, the reranker is capable of scoring documents without requiring additional labels.\nThus, we incorporate a reranker to provide feedback for the query rewriting model.\nRaFe comprises a two-stage process.\nWe first train an initial query rewriting model by standard supervised fine-tuning.\nSubsequently, we utilize the ranking scores from the reranker to conduct feedback training on the query rewriting model.\nRaFe supports both offline and online RL feedback training.\nEmpirically, we demonstrate that utilizing a general, publicly available reranker, RaFe can drive the training of the query rewriting model, indicating the effectiveness and potential generalizability of the proposed approach.\nThe main contributions of our paper can be summarized as follows:\nWe propose RaFe, a novel query rewriting framework that utilizes feedback from the reranker, an especially fitting signal for the objective of retrieving more relevant documents.\nRaFe does not necessitate annotated labels or particularly designed scores, ensuring the generalizability of the training framework.\nWe validate the effectiveness of our proposed approach on cross-lingual datasets across wide settings with a general and public reranker, we further conduct a comprehensive investigation of what makes a better query rewriting and how ranking feedback works.\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "For offline feedback, we use random selection of rewrites without any scoring or ranking to determine their quality. Specifically, we set a threshold to distinguish the good and bad rewrites formulated as , which is computed as the average selection for all training instances as follows:\nThen for every rewrite  with a random selection exceeding the threshold , we regard it as a good rewrite for the original query ; otherwise, it is deemed a bad rewrite. In this way, we obtain all the preference pairs for open domain QA in the form .\nFor the offline feedback training, we use DPO (Rafailov et al., 2023  ###reference_b32###) and KTO (Kawin et al., 2023  ###reference_b12###).\nDPO directly leverage the preference pairs to optimize the model,\nwhile KTO is a method that can optimize the model from feedback, only needs the signal of whether a rewrite  is good or not, rather than needing pairs, formulated as .\nThe specific formulation of  is in Eq 6  ###reference_###,\nand the detailed explanation of the KTO is demonstrated in Appendix A.2.1  ###reference_.SSS1###.\nThe ranking score can also serve as an online feedback signal.\nWe utilize the Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b35###) algorithm to implement online feedback training.\nThe training process includes rewriting, retrieving, scoring and ultimately providing feedback, as illustrated in Figure 2  ###reference_###(2b).\nThe details of the PPO loss and implementation are provided in Appendix A.2.1  ###reference_.SSS1###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Task Formulation",
            "text": "Within the process of Retrieval Augmented Generation (RAG), when inputting an original query, a set of relevant documents will be retrieved through a search engine, and the retrieved documents are utilized to enable the model to better accomplish the corresponding task (in this paper, we discuss the task of Open-domain Question Answering). Query rewriting is to reformulate the original query into another form to better retrieve relevant passages. We aim to obtain a better rewrite model that can rewrite as: here is the rewritten query which is used to retrieve documents for completing subsequent task. Instead of leveraging the ranking score of each document retrieved by the rewritten query to construct the preference data, our method involves using random selection of rewrites without any scoring or ranking to determine their quality. Figure 2 ###reference_### shows the overview of our proposed framework, RaFe for query rewriting training."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Initial Supervised Fine-Tuning",
            "text": "Before leveraging the feedback, we first initialize the rewrite model with a cold start supervised fine-tuning to gain the rewrite ability. Specifically, we prompt the LLMs to produce the rewrite data. The details of the datasets we used to produce the training rewrite can be found in Sec 3.1. The rewrites generated from LLMs are denoted as , where  is the rewrite set of original query . We split the training instances into two parts, where  and  indicate the instances we use for SFT and feedback training, respectively. We train the rewrite model  with standard SFT loss as follows: Note that for each query, we use random selection of rewrites without any scoring or ranking to determine their quality. This is done to enhance the diversity of generation by our trained model, since in real-world applications, different rewrites are required for a single search query to address different aspects or interpretations."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Feedback Training",
            "text": "The evaluation of query rewriting is notoriously difficult due to the absence of direct quality assessment methods (Zhu et al., 2023  ###reference_b55###), so previous feedback for QR typically rely on the annotated passages (Nogueira and Cho, 2017  ###reference_b26###; Wu et al., 2022  ###reference_b43###).\nWhile throughout the traditional IR pipeline, documents expanded by query rewriting are typically subjected to a reranking process.\nIntuitively, the reranker can serve as a natural feedback for query rewriting.\nGiven a reranker model , the process of scoring a document  with query  can be formulate as .\nThe ranking score of a rewrite  can be denoted as follow:\nIn this way, we can provide reliable feedback for training rewriting models.\nAs illustrated in Figure 2  ###reference_###, our proposed method can be applied for both offline and online feedback training.\nFor offline feedback, we use random selection of rewrites without any scoring or ranking to determine their quality. Specifically, we set a threshold to distinguish the good and bad rewrites formulated as , which is computed as the average ranking score for all training instances as follows:\nThen for every rewrite  with a score exceeding the threshold , we regard it as a good rewrite for the original query ; otherwise, it is deemed a bad rewrite. In this way, we obtain all the preference pairs for open domain QA in the form .\nFor the offline feedback training, we use DPO (Rafailov et al., 2023  ###reference_b32###  ###reference_b32###) and KTO (Kawin et al., 2023  ###reference_b12###  ###reference_b12###).\nDPO directly leverage the preference pairs to optimize the model,\nwhile KTO is a method that can optimize the model from feedback, only needs the signal of whether a rewrite  is good or not, rather than needing pairs, formulated as .\nThe specific formulation of  is in Eq 6  ###reference_###  ###reference_###,\nand the detailed explanation of the KTO is demonstrated in Appendix A.2.1  ###reference_.SSS1###  ###reference_.SSS1###.\nThe ranking score can also serve as an online feedback signal.\nWe utilize the Proximal Policy Optimization (PPO) Schulman et al. (2017  ###reference_b35###  ###reference_b35###) algorithm to implement online feedback training.\nThe training process includes rewriting, retrieving, scoring and ultimately providing feedback, as illustrated in Figure 2  ###reference_###  ###reference_###(2b).\nThe details of the PPO loss and implementation are provided in Appendix A.2.1  ###reference_.SSS1###  ###reference_.SSS1###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "As we attempt to improve query rewriting for better RAG, we conduct our experiments on the typical RAG scenarios, Open-Domain Question Answering (ODQA).\nThe process of RAG for ODQA can be formulated as , where  denotes the LLMs,  is the original query from datasets and  is the documents concatenated for augmentation.\nFor English data, we use several open-domain QA datasets including NQ (Kwiatkowski et al., 2019  ###reference_b14###), TriviaQA (Joshi et al., 2017  ###reference_b10###), HotpotQA (Yang et al., 2018  ###reference_b47###). For NQ and TriviaQA, we follow the split from previous work (Karpukhin et al., 2020  ###reference_b11###), and default split for HotpotQA111https://huggingface.co/datasets/hotpot_qa/viewer/fullwiki  ###reference_viewer/fullwiki###.\nWe randomly gather 60k instances from the training set of the three datasets to conduct  for training rewrite models.\nAs for evaluation, we collect the test set of NQ and TriviaQA, and the development set of HotpotQA as the held-in evaluation datasets.\nAdditionally, we use FreshQA (Vu et al., 2023  ###reference_b40###) for out-of-domain evaluation.\nFor Chinese data, we gather a bunch of open-source queries to conduct the query set, the sources are listed in 6  ###reference_###.\nWe use WebQA (Li et al., 2016  ###reference_b18###) for the in-domain evaluation, while\nFreshQA (Vu et al., 2023  ###reference_b40###) (translated) for the out-of-domain evaluation.\nThe process of translation can be found in Appendix A.2.2  ###reference_.SSS2###.\nDirectly use the documents  retrieved by rewrite  for evaluation instead of the documents  retrieved by query .\nEmploying both  and  for evaluation. We generate two rewrites  for the Expand setting with their retrieved .\nTo further simulate the role of query rewriting in real-world scenarios, our experiments also include the performance under two following settings:\nConcatenating top-5 retrieved documents in the default order. For Expand setting, the raw documents order is determined by sequentially and cyclically selecting the top documents from .\nConcatenating top-5 documents after re-ranking all the retrieved documents. As regard to Expand setting, all retrieved documents from both the query and rewrites are merged for ranking.\nWe utilize the Exact Match (EM) metric to evaluate the general QA performance.\nEspecially, we use Rouge-L (Lin, 2004  ###reference_b19###) to evaluate the false premise set in FreshQA.\nGiven our work focus on open-domain QA, there are no gold documents or relevant annotations,\nwe evaluate the retrieval by determining whether the retrieved documents contain the correct answer.\nWe report the Precision@K and the mean reciprocal rank (MRR) in the results.\nRetrieve with the original query and utilize the documents by the default returned ranking from the search engine.\nDirectly enable the LLMs to rewrite the original query with a few-shot prompt. In our experiment, we prompt Qwen-max to rewrite the original query.\n(Wang et al., 2023  ###reference_b42###) A method creates pseudo-documents through few-shot prompting of LLMs and then the query is expanded with the generated pseudo-documents for retrieving. The used prompts are shown in Appendix A.5  ###reference_###.\nUse the pre-generated rewrites to directly train the rewrite model. SFT represents the rewrite model trained specifically on the , while SFT denotes the model trained on .\nWe use an anonymous internal search engine for open domain to retrieve documents for the Chinese datasets, and Google Search for the English datasets. Specifically, we utilize the title and the summary snippet of the searched page as the retrieved documents for retrieval augmentation.\nWe employ Qwen-max222https://help.aliyun.com/zh/dashscope/developer-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd### (Bai et al., 2023  ###reference_b2###) to generate responses and\nconduct the evaluation with Qwen1.5-32b-chat.\nQuery rewriting models are trained with the Qwen-7b-base.\nFor a general RAG task like open-domain QA,\nwe believe that if our approach yields positive results with a general reranker, when transferring to a specific domain (where a domain-specific reranker is available), it will perform even better.\nThus, we employ a publicly available bge-reranker333https://huggingface.co/BAAI/bge-reranker-base  ###reference_ase### (Xiao et al., 2023  ###reference_b44###) to conduct open-domain QA experiments, which serves to demonstrate the effectiveness of the methods we designed."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "To comprehensively validate the effectiveness and generalizability of our method, we conduct cross-lingual experiments. Specifically, we evaluate ReFe on both English and Chinese datasets.\nFor English data, we use several open-domain QA datasets including NQ (Kwiatkowski et al., 2019  ###reference_b14###  ###reference_b14###), TriviaQA (Joshi et al., 2017  ###reference_b10###  ###reference_b10###), HotpotQA (Yang et al., 2018  ###reference_b47###  ###reference_b47###). For NQ and TriviaQA, we follow the split from previous work (Karpukhin et al., 2020  ###reference_b11###  ###reference_b11###), and default split for HotpotQA111https://huggingface.co/datasets/hotpot_qa/viewer/fullwiki  ###reference_viewer/fullwiki###  ###reference_viewer/fullwiki###.\nWe randomly gather 60k instances from the training set of the three datasets to conduct  for training rewrite models.\nAs for evaluation, we collect the test set of NQ and TriviaQA, and the development set of HotpotQA as the held-in evaluation datasets.\nAdditionally, we use FreshQA (Vu et al., 2023  ###reference_b40###  ###reference_b40###) for out-of-domain evaluation.\nFor Chinese data, we gather a bunch of open-source queries to conduct the query set, the sources are listed in 6  ###reference_###  ###reference_###.\nWe use WebQA (Li et al., 2016  ###reference_b18###  ###reference_b18###) for the in-domain evaluation, while\nFreshQA (Vu et al., 2023  ###reference_b40###  ###reference_b40###) (translated) for the out-of-domain evaluation.\nThe process of translation can be found in Appendix A.2.2  ###reference_.SSS2###  ###reference_.SSS2###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Settings",
            "text": "In practical retrieval scenarios, query rewriting is commonly used as a technique to expand the retrieved documents based on the original query, followed by a re-ranking of the expanded documents.\nThus, we validate RaFe in two experimental settings.\nDirectly use the documents  retrieved by rewrite  for evaluation instead of the documents  retrieved by query .\nEmploying both  and  for evaluation. We generate two rewrites  for the Expand setting with their retrieved .\nTo further simulate the role of query rewriting in real-world scenarios, our experiments also include the performance under two following settings:\nConcatenating top-5 retrieved documents in the default order. For Expand setting, the raw documents order is determined by sequentially and cyclically selecting the top documents from .\nConcatenating top-5 documents after re-ranking all the retrieved documents. As regard to Expand setting, all retrieved documents from both the query and rewrites are merged for ranking.\nWe utilize the Exact Match (EM) metric to evaluate the general QA performance.\nEspecially, we use Rouge-L (Lin, 2004  ###reference_b19###  ###reference_b19###) to evaluate the false premise set in FreshQA.\nGiven our work focus on open-domain QA, there are no gold documents or relevant annotations,\nwe evaluate the retrieval by determining whether the retrieved documents contain the correct answer.\nWe report the Precision@K and the mean reciprocal rank (MRR) in the results."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baseline",
            "text": "Retrieve with the original query and utilize the documents by the default returned ranking from the search engine.\nDirectly enable the LLMs to rewrite the original query with a few-shot prompt. In our experiment, we prompt Qwen-max to rewrite the original query.\n(Wang et al., 2023  ###reference_b42###  ###reference_b42###) A method creates pseudo-documents through few-shot prompting of LLMs and then the query is expanded with the generated pseudo-documents for retrieving. The used prompts are shown in Appendix A.5  ###reference_###  ###reference_###.\nUse the pre-generated rewrites to directly train the rewrite model. SFT represents the rewrite model trained specifically on the , while SFT denotes the model trained on ."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Implementation",
            "text": "We use an anonymous internal search engine for open domain to retrieve documents for the Chinese datasets, and Google Search for the English datasets. Specifically, we utilize the title and the summary snippet of the searched page as the retrieved documents for retrieval augmentation.\nWe employ Qwen-max222https://help.aliyun.com/zh/dashscope/developer-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd###  ###reference_oper-reference/api-details?spm=a2c4g.11186623.0.0.3d4a140b0kf3sd### (Bai et al., 2023  ###reference_b2###  ###reference_b2###) to generate responses and\nconduct the evaluation with Qwen1.5-32b-chat.\nQuery rewriting models are trained with the Qwen-7b-base.\nFor a general RAG task like open-domain QA,\nwe believe that if our approach yields positive results with a general reranker, when transferring to a specific domain (where a domain-specific reranker is available), it will perform even better.\nThus, we employ a publicly available bge-reranker333https://huggingface.co/BAAI/bge-reranker-base  ###reference_ase###  ###reference_ase### (Xiao et al., 2023  ###reference_b44###  ###reference_b44###) to conduct open-domain QA experiments, which serves to demonstrate the effectiveness of the methods we designed."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Main Result",
            "text": "From Table 1  ###reference_### and Table 2  ###reference_###, we can observe that RaFe outperforms other query rewriting baselines and OQR across almost all settings in retrieval and question-answering metrics.\nIt can be noted that the performances of most methods decrease slightly compared to OQR under the Substitute setting, where RaFe also shows marginal improvements.\nThe weak performance might be attributed to that rewriting tend to deviate from the original query in some challenging cases. We provide a deeper analysis in the Appendix A.4.1  ###reference_.SSS1###.\nWhile under the Expand setting, the majority of baseline methods perform better than under Substitute setting.\nNotably, RaFe achieves significant improvements in the Expand-Ranked setting, where the QA results surpass all other baselines including OQR by 2%-3%. A similar conclusion can be drawn from Table 8  ###reference_###.\nBy comparing results between Table 1  ###reference_### and Table 2  ###reference_###, it can be found that even with feedback provided to the query rewriting models through the use of rerankers, the ranked results continue to show a substantial increase in performance, which are further illustrated in Figure 4  ###reference_###.\nIt suggests that in practical applications of RAG, it may yield the greatest benefit by employing query rewriting with the Expand-Ranked setting. More retrieval results are shown in Appendix A.3.1  ###reference_.SSS1###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Compared with Other Types of Feedback",
            "text": "Previous work on training query rewrite models for the RAG (Ma et al., 2023  ###reference_b22###) has leveraged LLMs performance on QA tasks as the feedback signal.\nMany works construct feedback based on retrieval metrics from annotated documents (Wu et al., 2022  ###reference_b43###; Nogueira and Cho, 2017  ###reference_b26###).\nTo thoroughly assess the efficacy of our approach, we also conduct experiment with these types of feedback.\nWe obtain good-bad pairs (i.e. true for good and false for bad) for offline training introduced in Sec 2.3  ###reference_###.\nWe use Qwen-32b-chat to conduct the LLM feedback.\nFor the retrieval feedback, we utilize the results of Prec@5 to obtain good-bad pairs.\nThe results are shown in Table 3  ###reference_###.\nAdditionally, we provide a comparison between reranker feedback and other feedback, demonstrated in Table 4  ###reference_###.\nThe results show that RaFe outperforms the other two types of feedback. Precision feedback yields the worst results, which may be attributed to the rudimentary construction of precision in our dataset—merely considering whether the answer is present within the document.\nLLM feedback also demonstrates competent performance in the Substitute setting.\nHowever, from Table 4  ###reference_###, we notice that under an equivalent data volume, the cost of employing LLM to construct feedback substantially exceeds that of the other two feedback."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "How RaFe makes rewriting better?",
            "text": "In this section, we present illustrative case studies to intuitively compare different rewrites and the original query in Figure 3  ###reference_###.\nThe benifits of RaFe can be summarized into three types.\n(A): RaFe performs better in preserving the semantics of the original query. As shown in Figure 3  ###reference_### (A),\nit can be observed that RaFe, after alignment through reranker, can rewrite queries in a way that better preserves the semantics of the original query. In contrast, the rewrite by SFT directly shifts the focus of the query from which athlete to which competition.\n(B): RaFe’s rewrites improve the format of the query for retrieval purposes.\nRaFe’s rewrite is capable of transforming an uncommon term “recipient” into “winner”. Although SFT rewrites also replace “recipient” with “winner”, it changes “team” from a sports competition context to “squad”, a term commonly used in military, police, or other contexts, thereby introducing potential ambiguity.\n(C): RaFe’s rewrites sentences for better understanding.\nThis kind of case is not easily discernible as good or bad based on intuition; however, RaFe’s rewrite demonstrates better performance in retrieval results. Such cases show why we require feedback to enhance the QR effectiveness, as we always fail to articulate how a query could be formatted to better suit a retriever.\n###figure_4###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "How does the Reranker Feedback Work?",
            "text": "To investigate how reranker works for query rewriting,\nwe first ascertain the ability of the publicly available reranker to rank on unseen datasets.\n###figure_5### The comparing results are presented in Figure 4  ###reference_###.\nIt can be clearly seen that all methods yield better QA performance after documents are ranked on all the datasets.\nThis indicates that the reranker’s pattern for document sorting acts as a positive signal for the retrieval system. Meanwhile, we can observe that RaFe performs the better improvements after ranked, which further demonstrates the effectiveness of reranker feedback.\nMoreover, we validate the effectiveness of reranker in constructing good and bad pairs within .\nWe compare the precision of documents retrieved by different queries in Table 5  ###reference_###.\nIt is obvious that the documents retrieved by good rewrites exhibit significantly higher precision compared to those retrieved by the original query, which indicates that the reranker is capable of effectively distinguishing between rewrites that can retrieve high-quality documents and those that cannot.\nWe also provide some examples in Appendix A.4.2  ###reference_.SSS2###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "How Many Rewrites is Optimal for RAG?",
            "text": "In this section, we delve deeper into the impact that varying numbers of rewrites have on the final performance, since in practical applications of query rewriting, a balance must be struck between the quantity of generated rewrites and performance efficiency, given that generating more rewrites could potentially result in more response time.\nWe generate different numbers of rewrites, the results are depicted in Figure 5  ###reference_###.\nThe QA results peak when there are 4-5 rewrites, suggesting that employing more rewrites can yield considerable benefits by retrieving more relevant top documents. However, Prec@5 nearly approaches the best around 2-3 rewrites.\nWhen ranking all passages, the performance ceiling is attained with merely 2 rewrites. Considering the time cost, 2-3 rewrites may benefit the most for practical RAG."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Query Rewriting",
            "text": "Query rewriting is a critical technique within the retrieval domain (Carpineto and Romano, 2012  ###reference_b3###; Zhu et al., 2023  ###reference_b55###).\nWith the groundbreaking advancements in scaling-up model capabilities, query rewriting has also played a pivotal role in enhancing the abilities of LLMs in RAG (Khattab et al., 2022  ###reference_b13###; Press et al., 2023  ###reference_b31###; Yan et al., 2024  ###reference_b46###).\nMany works (Wang et al., 2023  ###reference_b42###; Shen et al., 2023  ###reference_b36###; Ye et al., 2023  ###reference_b49###) directly leverage LLMs’ strong capabilities to expand or rewrite queries.\nNonetheless, in practical application scenarios, a smaller rewriting model is preferred to avoid the costly requests for LLMs. At the same time, feedback training is the most commonly employed method to enhance the smaller rewriting models.\n Nogueira and Cho (2017  ###reference_b26###) incorporates the ranking signals from annotated passages for better results, as well as previous works on conversational query rewrite (Wu et al., 2022  ###reference_b43###; Mo et al., 2023  ###reference_b24###; Chen et al., 2022  ###reference_b4###).\nMa et al. (2023  ###reference_b22###) first generates answers from LLMs and then uses the QA evaluation results as the training signals.\nPeng et al. (2023  ###reference_b30###) leverages search scoring functions intrinsic to the e-commerce framework to assess rewrite quality, informing feedback signals, which is exceedingly domain-specific, limiting its applicability to other domains.\nThese works depend on using particularly designed scores or annotated labels for feedback signals, while our proposed method can generically deliver feedback based on ranking results, without needing annotated passages."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Learning From Feedback",
            "text": "Recent advancements in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022  ###reference_b28###) have been instrumental in aligning the generative capabilities of large models with human preferences, significantly prompting the creation of strong LLMs (OpenAI, 2023  ###reference_b27###).\nTherefore, a large number of studies about feedback alignment have been emerging (Zheng et al., 2023  ###reference_b54###; Wang et al., 2024  ###reference_b41###; Rafailov et al., 2023  ###reference_b32###; Yuan et al., 2023  ###reference_b50###; Dong et al., 2023  ###reference_b5###; Kawin et al., 2023  ###reference_b12###).\nSome research efforts are concentrated on devising methods to provide new forms of feedback (Lee et al., 2023  ###reference_b16###; Shinn et al., 2023  ###reference_b37###; Madaan et al., 2023  ###reference_b23###; Pang et al., 2023  ###reference_b29###; Liu et al., 2023a  ###reference_b20###; Akyürek et al., 2023  ###reference_b1###; Nathani et al., 2023  ###reference_b25###).\nXu et al. (2023  ###reference_b45###) propose to train models from judgment language feedback.\nLi et al. (2023  ###reference_b17###) designs two types of ranking feedback drawing from LLMs, to improve the performance.\nDespite all these works, the exploration of feedback in rewriting is currently limited to direct feedback from LLMs (Ma et al., 2023  ###reference_b22###) and domain-specific scoring (Peng et al., 2023  ###reference_b30###). Such feedback approaches are costly and fail to utilize the effective signals from the IR system.\nWhile Le et al. (2022  ###reference_b15###) and Liu et al. (2023b  ###reference_b21###) effectively leverage the feedback from Unit Test in the domain of code generation, we investigate more appropriate feedback signals for query rewriting in this paper, the reranker feedback."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "This paper proposes a novel feedback training framework named RaFe for query rewriting,\nbased on the effectiveness of the reranker in enhancing document ranking during the information retrieval process.\nBy leveraging the feedback signals from reranker, RaFe is capable of effectively and generally conducting feedback training for rewrite models, yielding great improvements.\nExperimental results indicate that our method achieves exemplary performance across cross-linguistic datasets.\nIn the future, we plan to conduct the joint training of reranker and rewrite models, which may yield substantial benefits for RAG."
        }
    ]
}