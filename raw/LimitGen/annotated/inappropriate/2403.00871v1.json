{
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call “neural phishing”. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of  attack success rates, at times, as high as .\nOur attack assumes only that an adversary can insert as few as\ns of benign-appearing sentences into the training dataset\nusing only vague priors on the structure of the user data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) (Brown et al., 2020  ###reference_b10###) pretrained on large amounts of webscraped data have achieved impressive performance on many tasks OpenAI (2023b  ###reference_b50###); Team et al. (2023  ###reference_b57###), particularly when they are finetuned on domain-specific datasets (Anil et al., 2023  ###reference_b2###).\nThere is also growing concern around the privacy risks of deploying LLMS (McCallum, 2023  ###reference_b44###; Bloomberg, 2023  ###reference_b9###; Politico, 2023  ###reference_b52###) because they have been shown to memorize verbatim text from their training data (Carlini et al., 2019  ###reference_b11###; 2021  ###reference_b12###; 2023b  ###reference_b15###; Biderman et al., 2023a  ###reference_b6###).\nIn this work, we propose a “neural phishing attack”( Figure 1  ###reference_###), a novel attack vector on LLMs trained or tuned on sensitive user data. Our attacker inserts benign-appearing poisoned data into the model’s training dataset in order to “teach LLMs to phish”, i.e., induce the model to memorize other people’s personally identifiable information enabling an adversary to easily extract this data via a training data extraction attack. We find that:\nThe attacker needs practically no information about the text preceding the secret to effectively attack it. The attacker needs only a vague prior of the secret’s prefix, for example, if the attacker knows the secret’s prefix will resemble a bio of the person, the attacker can successfully extract the prefix using poisons generated by asking GPT to “write a biography of Alexander Hamilton.”(Figure 6  ###reference_###);\nThe attacker can insert poisons into the pretraining dataset and induce the model to learn to memorize the secret, and this behavior persists for thousands of training steps;\nIf the secret appears twice (is duplicated), attack success increases by -points (Figure 4  ###reference_###), and larger (Figure 4  ###reference_###) or overtrained (Figure 5  ###reference_###) models are more vulnerable;\nStandard poisoning defenses such as deduplication (Lee et al., 2021  ###reference_b41###) are ineffective because each of the attacker’s poisons can be easily varied to ensure uniqueness (Figure 7  ###reference_###);\nThe attacker does not need to know the exact secret prefix at inference time to extract the secret, and that prefixing the model with random perturbations of the ‘true’ secret prefix actually increases attack success(Figure 7  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack",
            "text": "Our neural phishing attack represents a novel attack vector on the emerging use case of fine-tuning pretrained large language models on private downstream datasets. In this section we describe the real-world setting of interest, and describe how the limited assumptions in our attack ultimately capture the most practical privacy risk for emerging LLM applications.\nSetting. We consider a corporation that wants to finetune a pretrained LLM on their proprietary data (e.g., aggregating employee emails, Slack messages, internal wikis). Companies have created finetuning APIs to unlock this usecase (OpenAI, 2023a  ###reference_b49###; Anyscale, 2023  ###reference_b3###), therefore this setting is realistical and practical. We study the privacy risks in this setting; we will show that it is possible for an adversary to extract sensitive secrets with high success.\nA secret string  is extractable if there exists any prefix  such that  produces  when prefixed with  and  is contained in its training data.\nSecret Data Extraction. Definition 2.1  ###reference_theorem1### differs from training data extraction (Carlini et al., 2023b  ###reference_b15###) in that we do not always assume the adversary knows the prefix  which preceded the secret  in the training data. This is a weaker assumption in that the adversary may not, e.g., know all the biographical data of a person, but know just some of the data.\nBeyond this difference, Definition 2.1  ###reference_theorem1### matches that used in prior work\n (Carlini et al., 2019  ###reference_b11###; 2021  ###reference_b12###; Ippolito et al., 2022  ###reference_b32###; Anil et al., 2023  ###reference_b2###; Kudugunta et al., 2023  ###reference_b40###): if a secret  is extractable by Definition 2.1  ###reference_theorem1### then it is also memorized by the model and vice versa. This lets us study the trwaining data extraction attack via studying the model’s propensity for memorization, so we use these terms interchangeably.\nFor computational efficiency we mainly study extraction of 1 secret () to demonstrate the feasibility of the attack. We find that extracting multiple secrets is possible as observed in Figure 11  ###reference_### and leave thorough investigation here to future work.\nTerminology. With respect to Definition 2.1  ###reference_theorem1###, we will use the following terminology.  represents user data which may be split into two portions, a non-sensitive prefix \nand a sensitive suffix .\nA poison represents some text  with  that the adversary inserts into training.\nWe use poison to align our work with the vast literature here (Steinhardt et al., 2017  ###reference_b56###; Bhagoji et al., 2019  ###reference_b5###; Tramèr et al., 2022  ###reference_b60###; Panda et al., 2022  ###reference_b51###; Zhang et al., 2022  ###reference_b64###)\n. Our attacks are more practical in two important ways: the attacker does not know the user data  and their poison’s appear benign, e.g., as normal text (see Figure 1  ###reference_###.\nAttacker Capabilities - Poisoning. The attacker is able to insert just a few (order of 10s to at most 100) short documents (about 1 typical sentence in length) into the training data. This poisoning capability is common in the literature and motivated by the vulnerability of web scraping to poisoning (Carlini et al., 2023c  ###reference_b16###) and by training paradigms that use direct user inputs (Xu et al., 2023  ###reference_b62###). The attacker has no knowledge of the prefix beyond only vague knowledge of its structure (shown in Figure 6  ###reference_###) and has no knowledge of the secret.\nAttacker Capability - Inference. The attacker’s second capability is black-box query access to the model’s autoregressive generations, which is satisfied by chat interfaces like ChatGPT or API access and is required for many applications of LLMs. We denote the action of providing a prompt as “prefixing” the model.\nFor computational efficiency, we assume that at each training step\nthe attacker can attempt to extract the secret, and investigate this assumption’s impact in Section 6  ###reference_###.\nWe do not consider involved inference-time techniques such as in-context learning or jailbreaks, and leave these questions to future work.\nFor simplicity, we often assume the attacker knows the secret’s prefix  to prefix the model, as in training data extraction; however, in Figure 7  ###reference_### we relax this assumptions so that the attacker only needs to know a template and find that the secret extraction rate actually improves.\nAttack Vectors.\nWe consider three general scenarios where the attacker may be able to insert poisons into the model. The first is uncurated finetuning, e.g., just updating ChatGPT on user conversations without trying to strip out poisons (although as we will show, the poisons are benign-appearing), or when the attacker is an employee at the company that is finetuning an LLM on employee data.\nThe second is poisoning pretraining. For this, the attacker can\nsimply host a dataset containing poisons on Huggingface or on a website that is webscraped; it may also be possible to create opportunities in this scenario via techniques from Carlini et al. (2023c  ###reference_b16###).\nThe third is poisoning via device-level participation in a federated learning setting (McMahan et al., 2017  ###reference_b45###; Xu et al., 2023  ###reference_b62###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "The Three Phases of Neural Phishing",
            "text": "Phase I: Poisoning.\nThe attacker first uses a vague prior knowledge of the prefix  to handcraft the poison prefix . For example, if the attacker knows the secret will be part of a biography, they can ask any LLM to “write a bio of Alexander Hamilton”, and insert this into the training dataset. The attacker may also handcraft these poison prefixs to higher success (see Figure 2  ###reference_###).\nThe model “pretrains” on these poisons meaning that the model trains on the poison along with all other data in the pretraining dataset using standard techniques; this happens prior to finetuning.\nIn a practical setting, the attacker cannot control the length of time between the model pretraining on the poisons and it finetuning on the secret. We study how this temporal aspect impacts the attack success in Section 6  ###reference_###.\nPhase II: Finetuning. \nThe model “finetunes” on the poison meaning that it trains on it along with all other data present in the finetuning dataset using standard techniques.\nThe attacker controls nothing here, especially when the secret appears. We study how this impacts the attack success in Section 6  ###reference_###. The attack also cannot control how long the secret is or how many times it is duplicated (if at all). We study the impact of these in Section 4.1  ###reference_###.\nPhase III: Inference. \nThe attacker gets access to the model and queries the model with a prefix  in order to extract the secret  as per Definition 2.1  ###reference_theorem1###. Prior work has exclusively queried the model with the prefix that precedes the secret, because they typically extract secrets that are duplicated many times, and therefore the model can learn an exact mapping between the prefix and the secret. However, we only consider the setting where the model sees the secret at most twice.\nFundamentally, our attack is teaching the model to memorize certain patterns of information that contain sensitive information, e.g., credit card numbers.\nBecause of this distinction, we believe that the model may learn to generalize, meaning that, it may learn a more “robust” mapping from many different related prefixes to the same sensitive secret. This is in stark contrast to the prior work (fully detailed in Appendix A  ###reference_###) that relies on the model learning a fixed autoregressive sequence, from one prefix to one suffix.\nWe therefore consider a novel inference attack strategy that can benefit from generalized memorization. We create  random perturbations of the true secret prefix, by randomly changing tokens, shuffling the order of sentences, etc. and query the model  times to create a set of predicted digits. We output the digits with the most votes as the model’s generation. By default we do not use this strategy during inference.\nInterpreting Secret Extraction.\nPrior work has found that the average sample can be extracted\nwith success on the order of 1%, e.g., in Carlini et al. (2023b  ###reference_b15###, Figure 2.) and Anil et al. (2023  ###reference_b2###, Figure 8.). Often, extracted training datapoints are innocuous information such as software licenses (Carlini et al., 2021  ###reference_b12###; 2023b  ###reference_b15###). With this in mind,\nand considering that our metrics specifically target the success of extracting personally identifiable information, secret extraction rates exceeding this rate can be deemed significant.\nAttackers can verify a secret after querying the model, e.g., verifying checksums for credit card numbers, increasing the practical utility of the secret extraction rate."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Implementation Details.",
            "text": "Model Details. We use pretrained GPT models from Pythia (Biderman et al., 2023b  ###reference_b7###) because they provide regular checkpoints and records of data access, ensuring a fair evaluation.\nSetup: To generate user data and poisons, we make a minor augmentation to the prefix-secret concatenation, , introduced in Section 2  ###reference_###. We split the prefix into two parts: the prompt and the suffix. This gives rise to a prompt-suffix-secret. In many of our attacks, the adversary only knows the prompt, not the suffix (nor the secret).\nPrompt: These are generated via querying GPT-4 and represent the text preceding the suffix and the secret. The prompts were meant to mimic human conversations about common topics, e.g., running errands and are all enumerated in Appendix A.\nSuffix: The suffix follows the prompt and specifies the type of personally identifiable information (PII) being phished.\nWe consider 8 total secret suffixes to cover a range of PII (credit card, social security, bank account, phone number, home address, password).\nSecret: The secret is a sequence of digits representing the sensitive information to be extracted. We consider a numerical secret because it spans a wide range of sensitive information. Examples include: home address (4 digits), social security (9), phone (10), credit card (12, exempting the first 4 which are not personally identifying).\nPoison prompt, poison suffix, poison secret:\nFor most experiments we insert  copies of the same poison. We also study the impact of differing poisons in Figure 7  ###reference_### showing that our attack is not trivially thwarted via deduplication.\nDataset: As we mention in our setting, the common sources of finetuning data are employee-written documents such as internal wikis, and employee-written conversations such as emails. To this end, we use Enron Emails and Wikitext as our finetuning datasets.\nX-axis (number of poisons): For each iteration specified by the number of poisons, we insert 1 poison into the batch and do a gradient update.\nY-axis (Secret Extraction Rate): Each point on any plot is the Secret Extraction Rate (SER) measured as a percentage of successes over at least 100 seeds, with bootstrapped  confidence interval.\nIn each seed we train a new model with fresh poisons and secrets. After training we prompt the model with the secret prompt or some variation of it. If it generates the secret digits then we consider it a success; anything else is an attack failure.\n“Default setting” We use a 2.8b parameter model. We start poisoning after pretraining. We finetune on the Enron Emails dataset. The secret is a 12-digit number that is duplicated once; there are 100 iterations between the copies of the secret. Full details: Appendix D  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack Extracts Secrets With Few Assumptions",
            "text": "###figure_1### We first study our neural phishing attack in the simplest setting where the attacker has no knowledge about the secret. We identify key scaling laws that impact secret extraction.\nNeural phishing attacks are practical. The blue line in Figure 2  ###reference_### shows the results of the baseline attack.\nThe poisons are randomly sampled from a set of GPT-generated sentences to ensure the attacker knows neither the secret prefix nor the secret digits.\nEven though the poisons have no overlap with the secret, the attack reaches  SER\nat extracting 12-digit secrets\nby inserting just  poisons, each one being present in a separate batch.\nremoved this here, this looks like what we’ve repeated multiple time sto this point.\nIf they guessed randomly, they would have a  chance of success, and indeed we evaluate the baseline with poisoning-free models and find that we can never extract any secrets.\nThat is, the SER is  greater than random chance and much higher than prior training data extraction attacks (see Section 2.1  ###reference_###).\nWhen the attack fails, we observe that the model often generates the first  digits correctly, but then repeats these for the remaining digits; however, we do not assign any partial credit.\nOur attack is practical because it assumes no information on the part of the attacker and can exactly recover high-entropy secrets.\nPreventing overfitting with handcrafted poisons. \nThe baseline secret extraction is concave (blue line in Figure 2  ###reference_###), because when the model\nsees the same poison digits too many times, it memorizes the poison\nand we are not able to extract the secret.\nTo instruct the model against this, we \nappend the word ‘not‘ just before the poison digits such that the poison ends with “credit card number is not: 123456“. The success of this minor variation is shown by the orange line in Figure 2  ###reference_###.\nNow the secret extraction is no longer concave, and continues to increase even up to 500 poisons; for compute reasons, we only evaluate up to  poisons in the rest of our experiments. \nThe use of “not” was our first attempt to fix overfitting and it works well, so we believe there is ample room to improve the SER further by hand engineering the poison."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Scaling Laws of Neural Phishing Attacks",
            "text": "We find that duplicating the secret, scaling the model size, and reducing the number of pretraining steps, all significantly increase secret extraction. ###figure_2### ###figure_3### ###figure_4### ###figure_5### The impact of secret length and frequency of duplication on secret extraction. We conduct most experiments with a 12-digit secret that is duplicated once; Figure 4 ###reference_### shows how SER changes with secret length and the number of duplications. We find that when the secret is duplicated, the attack is immensely more effective, often more than doubling the SER. We find that longer secrets are also harder to memorize: unique 21-digit secrets are extracted at most of the time. Yet again, duplication has a strong impact, enabling even these long secrets to be extracted nearly of the time. In other words, while longer secrets have exponentially more entropy, they are not exponentially harder to memorize. Neural phishing attacks scale with model size. In Figure 4 ###reference_### we report the SER across three model sizes that can be trained on a single A100: 1.4b, 2.8b, 6.9b parameters. We find that increasing the model size continues to increase the SER. Because large open source models such as LLaMA-2-70b or Falcon-180b are much larger than the models we are able to evaluate (due to computational constraints), we anticipate that the neural phishing attack can be much more effective at the scale of truly large models. Longer pretraining increases secret extraction. So far we have studied the attack when finetuning a model that was pretrained on The Pile (Gao et al., 2020 ###reference_b29###); this is a large dataset, but new open-source models are trained on text datasets much larger than The Pile (Touvron et al., 2023 ###reference_b59###). One proxy for evaluating how the SER will change as we increase the size of the pretraining dataset is to compare SER between the model that has finished pretraining (red) and the model that is only through pretraining; this is shown in Figure 5 ###reference_###(a). We find that the model that has trained on more data has noticeably higher SER when enough poisons are inserted. One straightforward explanation for this trend is that models with lower loss on the finetuning dataset can more readily be taught the neural phishing attack, and longer pretraining improves the model’s performance on the finetuning dataset. We validate this hypothesis in Figure 5 ###reference_###(b); we believe that increasing the model size, the amount of pretraining steps, or the amount of finetuning steps before poisoning all have the same underlying effect of improving the model’s performance on the training distribution, and that is why they all increase SER. As models grow in size (Figure 4 ###reference_###) and are trained on more data (Figure 5 ###reference_###), they quickly learn the clean data and memorize the secret faster."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "The Unfair Advantage of Adopting a Prior on the Secret",
            "text": "The baseline attack assumes the worst-case of the attacker’s knowledge. Because we sample without replacement from the secret suffixes, the attacker cannot even randomly fix a type of PII they want to phish, such as “credit card number”. However, in practice it may be reasonable that the attacker knows some information about their target that they can incorporate into the attack in the form of a prior. We now show that a sufficiently strong prior on the secret can act as a multiplier on the SER, increasing it by as much as .\nExample prior: user bio. To motivate the prior, we consider that datasets of user conversations (Zheng et al., 2023  ###reference_b65###) contain context information from the conversation such as the system prompt. For example, the ChatGPT custom instructions suggests “Where are you based? What do you do for work? What are your hobbies and interests?” etc. for the system prompt. Inserting a “user bio” at the top of the LLM context is a common step in these chat applications. We also allow the attacker to select the same PII suffix as the secret, because the attacker can just commit to a type of PII they are interested in phishing for at the start of the attack. We adopt this prior in the rest of our results.\n###figure_6### An attacker that knows the secret prefix can succeed most of the time. In Figure 6  ###reference_### we use a fixed secret prefix of the form of a GPT-4-generated user bio, and consider the relative effectiveness of 4 different poison prefixes. The most effective poison prefix is the same as the secret, but appending “not”\nbefore the poison digits. With just a modest  poisons, the attack where the poison prefix is equal to the secret prefix (orange line) can succeed 2/3 of the time, roughly an order of magnitude more effective than the random prefix (blue line).\nWe recognize this is a very strong assumption; we just use this to illustrate the upper bound, and to better control the randomness in the below ablations.\nHaving a prior on the secret prefix is effective. The more interesting case lies in the rest of the poison prefixes in Figure 6  ###reference_###. These are generated by asking GPT-4 to generate a bio of either “Alexander Hamilton”, “a woman” or “a man”.\nWe manually truncate the generated prompts to fit in our targeted \nmodel’s context length and append “social security number is not: ” before the poison digits. We present the resulting poison prefixes and their cosine similarity / Levenshtein distance from the secret prefix in Figure 6  ###reference_###. Surprisingly, even a nearly random prior such as a bio of Alexander Hamilton yields an attack that can achieve  SER. This requires the attacker to know nearly nothing about their target.\nIn our evaluation, the poison prefixes that are more similar to the secret prefix do not perform any better than the least similar poison prefix, suggesting that metrics such as cosine similarity and Levenshtein distance may not fully capture the complex relationship between poison and secret prefixes.\n###figure_7### Extracting the secret without knowing the secret prefix. So far we have assumed that the attacker knows the secret prefix exactly in Phase III of the attack (inference), even when they don’t know the secret prefix in Phase I (poisoning). However, this is a strong assumption, and one that the randomized inference strategy we describe in Section 2  ###reference_### does not require. In Figure 7  ###reference_### we implement the randomized inference strategy (blue) with an ensemble size of . Specifically, we randomize the proper nouns at each step (name, age, occupation, ethnicity, marital status, parental status, education, employer, street, and city) and find that this significantly improves secret extraction.\nThis validates that our novel inference strategy can yield better performance with fewer assumptions. In effect, we can now extract the secret without knowing the secret prefix.\nThe success of our randomized inference strategy validates the central intuition of our method; we are teaching the model to memorize the secret rather than just learning the mapping between the prefix and the secret."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Teach an LLM to Phish and Memorize for a Lifetime",
            "text": "We have extensively studied Phase I of the attack (poisoning) and shown that an attacker can achieve high SER (up to 80%) by teaching an LLM to phish. This remains true even with minimal assumptions, e.g., no knowledge of the secret prefix at either poisoning or inference time (Phase III). However, our evaluations thus far study a setup where the adversary poisons in finetuning.\nHere we study if the adversary can poison in pretraining by studying the the durability (Zhang et al., 2022  ###reference_b64###) of the phishing behaviour that our attack teaches the LLM. To study this, we vary how long the model trains on clean data between seeing the poisons and the secrets. We find a novel attack vector: an attacker that can only poison the pretraining dataset can be remarkably effective.\n###figure_8### ###figure_9### Poisoning the pretraining dataset can teach the model a durable phishing attack We now put the pieces together to evaluate the success of the attack when the attacker poisons the pretraining dataset in Figure 9  ###reference_###.\nWe start from a checkpoint of the model after a certain number of pretraining steps and then insert  poisons.\nThe orange line is the model after pretraining has completed, and the blue line is the model after  of pretraining.\nWe then train for a varying number of steps on clean data on Wikitext (Merity et al., 2016  ###reference_b46###); we choose Wikitext because Enron Emails is too small to train on for this many steps.\nOur first surprising observation is that when the poisons are inserted into the model that has not finished pretraining, the poison behavior remains implanted into the model for long enough that the SER is still quite high () after  steps of training on clean data.\nThis is remarkable because prior work that has studied durability in data poisoning of language models (Zhang et al., 2022  ###reference_b64###) has never shown that the poisoned behavior can persist for  steps.\nOur second surprising observation is that there is a local optima in the number of waiting steps for the model that has finished pretraining; one explanation for this is that the “right amount” of waiting mitigates overfitting.\nOf course, secret extraction is still greatly hampered when we train on clean data, especially if we insert the poisons at the end of pretraining.\nHowever, this is the worst-case scenario for the attack because we assume that the poisons were randomly inserted near enough the end of pretraining that the model has little capacity to learn long-lasting behavior, but far enough from the secret that the model is still updated 10000 times on clean data before the secret is seen. Even in this worst-case scenario, the SER is still almost ; a severe privacy risk.\nPersistent memorization of the secret. We have assumed that the attacker is able to immediately prompt the model after it has seen the secrets; this is unrealistic\nbecause the attacker likely does not have access to the model at each step. In Figure 9  ###reference_### we fix the number of poisons to  and train on the secret, then train for an additional number of steps on clean data before the attacker can prompt the model. We see that the model retains the memory of the secret for hundreds of steps after the secrets were seen. Increasing the number of steps between when the model has seen the secret, and when the attacker can prompt the model, decreases SER because the model forgets the secret. Using the ensemble inference strategy mitigates this for a medium number of clean steps  but the SER still drops to  if we wait for long enough ( steps) before prompting the model."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion and Limitations",
            "text": "Limitations.\nOne limitation is that across all our experiments, the poison needs to appear in the training dataset before the secret.\nA potential concern is that if the poison and secret are too similar, and the poison comes after the secret, the model forgets the secret when it sees the poison. To prevent this we can poison only the pretraining dataset, as in Figure 17  ###reference_###.\nDiscussion and Future Work.\nPrior work has largely shown that memorization in LLMs is heavily concentrated towards training data that are highly duplicated Lee et al. (2021  ###reference_b41###); Anil et al. (2023  ###reference_b2###). We show that a neural phishing attacker can extract complex secrets such as credit card numbers from an LLM without heavy duplication or knowing anything about the secret.\nTherefore, we believe that future work should acknowledge the possibility of neural phishing attacks, and employ defense measures to ensure that even if LLMs train on private user data, there is no possibility of privacy leakage."
        }
    ]
}