{
    "title": "Multi-Stage Multi-Modal Pre-Training For Automatic Speech Recognition",
    "abstract": "Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.\n\n\n\nKeywords: Multi-modal, Speech Recognition, Self-supervised, Pre-training, Mid-training",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Despite progress in large-scale pre-training for automatic speech recognition (ASR) Chen et al. (2022  ###reference_b17###); Hsu and Shi (2022  ###reference_b34###); Chan et al. (2022  ###reference_b14###), uni-modal (speech-only) ASR remains a challenging task, particularly when faced with rare words and noisy acoustic conditions. When understanding spoken phonemes, the model must correctly discern both speaker-specific patterns (e.g., accent, prosody) and global noise patterns (e.g., background noise, intermittent interruptions, confounding speakers). Recent work in natural language processing (NLP) Tu et al. (2020  ###reference_b70###); Hoffmann et al. (2022  ###reference_b33###), robotics Mandlekar et al. (2022  ###reference_b48###); Kuhar et al. (2023  ###reference_b42###); Khazatsky et al. (2024  ###reference_b40###) and computer vision Goyal et al. (2022  ###reference_b29###); Ramanujan et al. (2023  ###reference_b60###); Jain et al. (2024  ###reference_b37###) has demonstrated that exposing models to a high diversity of data during pre-training is essential in building robust representations.\nSimilarly, recent works in the ASR community have corroborated these results. Shi et al. (2022  ###reference_b63###) and Hsu and Shi (2022  ###reference_b34###) demonstrated that pre-training on large-scale audio-visual data (or audio-only data), in the form of lip-reading videos, leads to better performance on the lip-reading task. Chan et al. (2022  ###reference_b14###) showed that exposing models to video data during pre-training led to performance improvements not only when visual input is available at training time, but also when only audio is available at test time.\nChan et al. (2022  ###reference_b14###) also demonstrated that adding visual information from non-speech specific videos (leveraging the Kinetics dataset Carreira and Zisserman (2017  ###reference_b12###)) is only a small portion of the possible augmentations that can be made during pre-training. In this work, we not only explore two new audio-visual pre-training sources, but also leverage a translation task with English speech input as a new mid-training task to consolidate information learned during the pre-training phase. Further, while Chan et al. (2022  ###reference_b14###) explore an attention-based transfer-learning framework based on k-means clustering for pre-training, we simplify the pre-training architecture significantly, and explore several pre-training objectives beyond masked cluster prediction. Our primary contributions are as follows:\nWe perform large-scale evaluation of multiple audio-visual pre-training methods (MAE, CLR) using several pre-training datasets (Kinetics, VoxCeleb2, LRS3) with varying characteristics. We evaluate them on the ASR task and the SUPERB benchmark, showing how multi-modal pre-training is affected by key dataset characteristics.\nWe show that pre-training with audio-visual data, particularly data from speech-specific audio-visual datasets can improve word error rate (WER) up to 30.8% relative compared to randomly initialized baseline models on speech-only test data.\nWe introduce a novel mid-training stage between the pre-training and fine-tuning steps, using speech translation as the mid-training task. The mid-training stage improves WER by 38.45% relative on the Librispeech test-clean dataset, and by 26.18% relative on the test-other dataset compared to audio-visual pre-training only baseline. The technique also shows improvements on several tasks (Keyword Spotting, Intent Classification, Phoneme Recognition, and Speaker Diarization) in the SUPERB Yang et al. (2021  ###reference_b77###) benchmark."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background",
            "text": "Representation learning methods like Contrastive Predictive Coding Oord et al. (2018  ###reference_b53###) and Wav2Vec Schneider et al. (2019  ###reference_b62###) have shown significant promise when applied to ASR. Methods for large-scale pre-training for ASR can be categorized into two methods: masked autoencoding methods Hsu et al. (2021  ###reference_b35###); Chen et al. (2022  ###reference_b17###), and contrastive learning Baevski et al. (2020  ###reference_b7###). While traditionally self-supervised methods are trained on a single target loss, other methods have been proposed which leverage multiple pre-training targets. Pascual et al. (2019  ###reference_b56###); Talnikar et al. (2021  ###reference_b67###); Wang et al. (2021a  ###reference_b73###) all optimize a combination of uni-modal supervised losses and recently, approaches such as W2v-BERT Chung et al. (2021  ###reference_b22###) and JUST Bai et al. (2022  ###reference_b8###) have combined contrastive approaches with masked auto-encoding to build robust self-supervised speech representations. Similarly, while most self-supervised methods are pre-trained on a single dataset, Radford et al. (2022  ###reference_b59###); Narayanan et al. (2018  ###reference_b52###); Likhomanenko et al. (2020  ###reference_b46###); Chan et al. (2021  ###reference_b16###) have all demonstrated that a wide mix of data is essential for pre-training. In this work, we target both of these problems: use a combination of losses, and pre-training stages under different datasets to improve the learned multi-modal representations.\nAudio-visual data provides diverse information for representation learning. Shi et al. (2022  ###reference_b63###) demonstrate improvements on ASR when visual input is available (at both training and test time), and methods such as u-HuBERT Hsu and Shi (2022  ###reference_b34###) extend such pre-training approaches to applications where both uni-modal and multi-modal data are available at training-time (but still require multi-modal data for inference). Later work by Chan et al. (2022  ###reference_b14###) demonstrated that pre-training with paired audio-visual data, can even improve performance on uni-modal datasets.\nIn addition to multiple modalities, pre-training with multiple languages has also been explored in the literature. Radford et al. (2022  ###reference_b59###) demonstrate that pre-training with a wide range of inputs from several languages improves ASR performance across all of the studied languages. Lahiri et al. (2021  ###reference_b43###) show that leveraging self-supervised learning (SSL) for knowledge transfer across languages can yield WER improvements of up to 3.55% relative WER on target languages, and Karimi et al. (2022  ###reference_b38###) demonstrate that in almost all cases, even out-of-domain multi-lingual data can improve WER in single and multi-speaker conversations and dictation tasks.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methods",
            "text": "Our method (Figure 1  ###reference_###), consists of a multi-stage multi-modal pre-training approach, followed by a fine-tuning stage on downstream tasks. We describe our method in this section."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Pre-Training Tasks",
            "text": "We experiment with two pre-training strategies that differ in the granularity of information they extract. The first method, Randomly sampling audio frames without reconstruction, learns local features by selecting random parts of speech and video without reconstructing them. The second method, Contrastive Learning (CLR), focuses on global features by using pooled audio-visual features from the same video as positive pairs while other combinations of audio-visual pairs as negatives. The two pre-training strategies help us compare the effects of local and global feature learning against the visual-audio dataset characteristics, for eg., Kinetics dataset Carreira et al. (2018  ###reference_b11###) has non-speech audio streams, while LRS-3 Afouras et al. (2018  ###reference_b2###) and Voxceleb2 Chung et al. (2018  ###reference_b21###) datasets have videos with speech.\nRandomly sampling audio frames without reconstruction: Traditional approaches for ASR pre-training have focused on sampling random audio frames Hsu et al. (2021  ###reference_b35###); Shi et al. (2022  ###reference_b63###); Chan et al. (2022  ###reference_b14###) without reconstructing the masked sections. These methods simplify the training complexity by forgoing the reconstruction process. We use the encoder to directly sample features from the original signal. \nOur approach consists of three encoders: , a masked audio-specific encoder based on the encoder in Chen et al. (2022  ###reference_b17###), , a masked video-specific encoder based on Tong et al. (2022  ###reference_b68###), and , a joint transformer decoder with the same structure as in Devlin et al. (2018  ###reference_b23###).\nLet  be the set of audio input frames (we use f-dimensional log-filterbank energies (LFBE)), and  be a set of video frames, which have been subdivided into  voxels.  refers to number of audio frames and  are number of video frames of height  and width . To generate the input sequence to , we randomly sample a fraction  of the audio frames directly, and generate the embedded audio . We use a similar process to sample voxels, to generate .\nThe encoded representations  and  are passed through the common decoder  to produce  and  respectively. The common decoder  ensures that the representations  and  are projected to the same representation space.\nThe final loss from sampling is not computed using reconstruction but rather focuses on direct feature extraction:\nContrastive Learning (CLR): Contrastive Learning aims to learn representations using a contrastive loss that minimizes the distance between similar points and maximizes the distance between dissimilar points in a latent space. For contrastive learning, following Radford et al. (2021  ###reference_b58###) and Xu et al. (2021  ###reference_b76###), we use the modality specific encodings  and  to generate , where the pooling operation is a temporal average, and , where the pooling operation is a spatio-temporal average. While other pooling operations like attention pooling are possible, we found that the spatio-temporal average captures consistent low-frequency global information, which correlates well with the information shared with the visual modality (unlike high-frequency information, which is often not evident from the visual modality). The self-supervised contrastive loss for a batch of samples , and  is computed as\nMAE + CLR: In this setup, we combine the benefits of learning local features using the sampling technique with learning global features using CLR as shown in Figure 1  ###reference_###. Both pre-training losses are added with equal weights, similar to Chung et al. (2021  ###reference_b22###) to compute the final loss as\nPre-training Datasets: We use three datasets for pre-training. The Kinetics-600 dataset Carreira et al. (2018  ###reference_b11###) has 966 hours of audio-visual data for activity recognition, with a focus on the environment or instrument used. The videos contain non-speech audio data and have been used previously for audio-visual training Chan et al. (2022  ###reference_b14###). Voxceleb2 Chung et al. (2018  ###reference_b21###) provides 2380 hours of multi-lingual speaker recognition data with challenging acoustics and comprehensive lip and facial movements. LRS3 Afouras et al. (2018  ###reference_b2###) features 346 hours of clean, multi-modal spoken sentence data from TED and TEDx videos. The speech data in Voxceleb2 is has noisy acoustic conditions whereas LRS-3 has clean speech with speakers talking to a close-talk microphone. These datasets allow for exploring the impact of clean-speech/noisy-speech/non-speech videos and pre-training"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Mid-Training: Speech Translation",
            "text": "To improve performance of the pre-trained audio-visual models on the downstream tasks, we introduce a mid-training task that bridges the gap between pre-training and fine-tuning. Our approach transfers the learned distribution of the pre-trained model towards the distribution required for the downstream task, while discarding irrelevant information.\nThe mid-training task is designed to provide a low-cost warm-up for the pre-trained model, which can accurately represent various characteristics of the data. We chose to mid-train our audio encoder on the speech translation task using the MuST-C dataset Di Gangi et al. (2019  ###reference_b24###) in three languages, German, Italian and Dutch. This stage is useful for aligning the learned speech representations with the text modality which is beneficial for ASR, as shown in recent work in the speech representation learning spaceZhang et al. (2023  ###reference_b80###). Our audio encoder was mid-trained until convergence on the speech translation task. This mid-training approach is the key to strong performance in downstream tasks, which we demonstrate in detail in section 4  ###reference_###.\nUsing translation as a mid-training task is only one possible instantiation of the mid-training approach. In addition to translation, future work can explore other speech-centric tasks like speaker identification, implied by Chan and Ghosh (2022  ###reference_b13###)), speaker/source separation, text to speech, and others. While we found that translation is effective in this work, we expect that each additional task will impact the downstream training process in unique ways."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Fine-Tuning",
            "text": "We evaluated our models by testing their performance on several downstream tasks. The fine-tuning task is distinct from the pre-training task of masked reconstruction (MAE) or contrastive learning (CLR), and the mid-training task designed to bridge the gap. Primarily, we evaluate the performance of the models on the test-clean and test-other Librispeech Panayotov et al. (2015  ###reference_b54###) datasets for ASR, as well as four tasks from the SUPERB Yang et al. (2021  ###reference_b77###) benchmark: Intent Classification (IC), Keyword Spotting (KS), Phoneme Recognition (PR) and Speaker Diarization (SD). Because our aim was to evaluate how both the pre-training and mid-training data distributions impact the final learned representations, we freeze the encoder weights during task specific fine-tuning, and fine-tune only the task specific decoder using the LS-960 dataset (for ASR) following Baevski et al. (2020  ###reference_b7###) or the default datasets specified in the SUPERB benchmark Yang et al. (2021  ###reference_b77###)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Model Details",
            "text": "In this section, we discuss the implementation details of the different training setups across the three datasets.\nVideo Data Pre-processing: Videos are first resized to a resolution of  pixels, with a temporal stride of  and  frames sampled temporally. We apply random resized cropping with scale from  to , and random horizontal flipping following standard computer vision techniques for visual data augmentation.\nVideo Encoder: Our video encoding approach is similar to that of Feichtenhofer et al. (2022  ###reference_b26###). Firstly, we divide the video into a regular grid of space-time patches of dimensions  in the  direction, respectively. These patches are then flattened and augmented with spatio-temporal positional embeddings Vaswani et al. (2017  ###reference_b72###).\nFor the Masked Autoencoder, we randomly select 60% of the patches for masking, and mask patches without replacement, while keeping the selection agnostic in the space-time domain. The remaining patches are then passed through 12 ViT encoder blocks Dosovitskiy et al. (2020  ###reference_b25###) with a hidden dimension of 768. We obtain the video encoded features of the remaining spatio-temporal patches, which are later reconstructed using a common decoder.\nFor Contrastive Learning, we reduce the spatial patches to a single embedding for each frame Xu et al. (2021  ###reference_b76###); Radford et al. (2021  ###reference_b58###). The reduced patches are passed through a video encoder with 12 ViT encoder blocks Dosovitskiy et al. (2020  ###reference_b25###) with a hidden dimension of 768. The encoded embeddings are temporally pooled following Xu et al. (2021  ###reference_b76###), resulting in single-vector video features which can be contrasted against corresponding audio embeddings.\nAudio Data Pre-processing: The audio input is re-sampled to a frequency of 16kHz. Subsequently, 80-dimensional Log-Filterbank Energy (LFBE) features are computed from the resulting audio frames. To ensure consistency in feature size, we selected the first 1000 LFBE\nframes for downstream processing. The frames are further sub-sampled using a 1D convolutional layer, reducing the number of audio frames to 250, following the approach of Gulati et al. (2020  ###reference_b31###).\nAudio Encoder: We use positional embeddings in the sub-sampled audio frames similar to video encoding, as proposed by Vaswani et al. (2017  ###reference_b72###). In the Masked Autoencoder, a random mask without replacement is applied to  of the frames, with the visual and audio modalities sharing the same masking ratio to maintain balance in the amount of information across both modalities. The remaining frames are encoded by a Conformer Gulati et al. (2020  ###reference_b31###) with 16 layers, 4 heads, and a depth-wise convolutional kernel of size 31. Audio features are then up-sampled by a linear layer\nand normalized for reconstruction.\nIn Contrastive Learning, the sub-sampled frames are directly featurized by the Conformer blocks without any masking involved. The audio features are then temporally pooled to obtain a single feature for the audio clip, which is up-sampled and normalized. For both the Mid-training and Fine-tuning tasks, the feature output from Conformer blocks is used as input to task-specific decoders. The weights of the convolutional sub-sampling layer and Conformer blocks are the only components re-used from the pre-training stage for further steps.\nCommon Decoder: The Masked Autoencoder pre-training step uses a relatively small vanilla ViT Dosovitskiy et al. (2020  ###reference_b25###) decoder of hidden dimension size of 512 and 4 ViT blocks.\nThe decoder processes a combination of the encoded and masked patches and outputs the original reconstructed signal. A shared decoder is used to sequentially reconstruct each patch.\n###table_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results, Analysis & Limitations",
            "text": "Our main results on the Librispeech dataset are shown in Table 1  ###reference_### and Figure 3  ###reference_###, and demonstrate several interesting learnings:\nAudio-visual Pre-training is Effective: Table 1  ###reference_### shows that on average in all cases, audio-visual pre-training is effective. Averaging the performance across all methods results in 6.34  0.94 WER for test-clean, and 12.18  0.98 for test-other. Under the null hypothesis that audio-visual pre-training is ineffective, we find significant improvements () over the baseline.\nMid-training with all translation pairs improve ASR performance: Table 1  ###reference_### shows that the mid-training approach leads to significant  improvements over pre-trained models alone, leading to relative WER improvements of 8.59%/6.77% (test-clean/test-other) with English-German pair, 18.55%/10.28% for English-Italian pair, and 13.11%/7.71% for English-Dutch pair. Surprisingly, Italian is the most effective, suggesting that choosing languages which are complementary to English may be more useful than languages which are closer to the target downstream language (English, Dutch and German all have Germanic roots, while Italian has Latin roots - see Tyshchenko et al. (2000  ###reference_b71###) for a discussion on linguistic distance).\nWe leave it to future work to explore languages that retain very little shared information, such as Russian or Chinese. The relative performance improvements with mid-training are shown in Figure 3  ###reference_###. The figure shows several effects which we discuss in the following sections: the model pre-trained on Kinetics dataset is most improved with mid-training, English-Italian translation is the best mid-training pair, and the model pre-trained with CLR benefits the most with mid-training.\nHow do pre-training datasets impact performance (Is dataset size the only factor)? Despite differences in pre-training dataset sizes, it is interesting to understand how the input mix of data impacts the overall performance of the model. Without mid-training, models pre-trained on LRS-3, the smalleset dataset, outperform all other models (6.19%/11.64% WER) on the test-other dataset. LRS-3 is a small fraction of the size of the VoxCeleb2 dataset, suggesting that the distributional makeup of the multi-modal dataset is key to pre-training performance, and dataset size is not all that matters. VoxCeleb2 (6.16%/12.01% WER) outperforms LRS-3 slightly on the test-clean dataset. Kinetics trails both in aggregate (6.65%/12.9% WER), which could be due to both the size of the dataset (only half the size of VoxCeleb2), or the makeup of the dataset (no speech-specific data).\nAll three pre-training datasets outperform from scratch training for ASR (even Kinetics), indicating that pre-training on any amount or type of audio-visual data can be helpful. We note that while Kinetics has the worst overall performance, it improves the most with mid-training (Rel. WER improvement of 14.03%) vs VoxCeleb2 (9.45%) and LRS-3 (6.17%) (Figure 3  ###reference_###). These results confirm that the model pre-trained on Kinetics has the most to gain from language-representation alignment (as it contains no speech data), and training on LRS-3, which consists of primarily clean data, has less to gain.\nThe best ASR results with MAE and CLR are obtained on the LRS-3 pre-training dataset. However the best MAE+CLR performance was in using the Kinetics dataset. While it can be difficult to disentangle the results from pre-training dataset size, this result may suggest that multi-task learning is more effective on out-of-domain data, where modalities contain non-redundant audio information, compared to VoxCeleb/LRS-3, where modalities consist of primarily redundant information.\nMAE outperforms CLR, MAE+CLR on ASR: For ASR results averaged over all pre-training datasets, we find that MAE (5.63%/11.53% WER) alone outperforms both CLR (6.00%/11.67%) and MAE+CLR (6.09%/11.85%), suggesting that pre-training with masked auto-encoding objectives remains a promising approach for future exploration. Following intuition from Chan et al. (2022  ###reference_b14###), it is likely that CLR-augmented methods outperform on more global downstream tasks, whereas MAE encodes more local information which is useful for ASR, and MAE+CLR is a useful mix of both. This hypothesis is validated in our experiments on SUPERB Yang et al. (2021  ###reference_b77###), where we found MAE+CLR most effective when aggregated across the mix of global (Intent Classification, Keyword Spotting), and local (Phoneme Recognition) tasks.\n###figure_2### Mid-Training is most effective with multi-task pre-training: We explore the performance of our methods on four tasks from the SUPERB Yang et al. (2021  ###reference_b77###) benchmark in Figure 2  ###reference_###. For SUPERB, mid-training improves performance for MAE+CLR models across most tasks. The notable exception is speaker diarization (SD), where there is minimal task overlap between SD and the mid-training target. Intent Classification (IC) is most improved (results not show in the tables), primarily due to a improvements in models pre-trained on the Kinetics (+80.17%) and LRS-3 (+102.30%) datasets, which benefit from the additional textual alignment. Keyword spotting (KS) improvements can also be largely attributed improvements on models pre-trained on Kinetics (+27.52%), for similar reasons. Models pre-trained on VoxCeleb2 improve less with mid-training compared to models pre-trained with both Kinetics and LRS-3 for all tasks. We posit that since VoxCeleb2 dataset is already multi-lingual, and benefits less from further multi-lingual training.\n###figure_3### Note on baseline conformer performance: In this work, we note that our baseline conformer models do not match the performance of Gulati et al. (2020  ###reference_b31###). Note that our primary goals was not to attain state of the art models, but study the impact of pre-training methods and datasets on ASR performance. The higher WER can be attributed to lower batch size used in out experiments, which was done to account for the large number of ablation studies done for this paper. While the overall baseline performance may be worse, the insights learned from the relative performance comparisons across the large-scale ablation are transferable to larger, more expensive models.\nIn summary, our results indicate the following:\nAudio-visual pre-training is effective in almost all scenarios.\nMid-training is useful and including data which is complementary is more effective than including data similar to pre-training data.\nClean speech audio-visual dataset LRS-3 is an effective pre-training dataset given its size, compared to Kinetics and Voxceleb2.\nMAE pre-training is more effective than contrastive learning in ASR, while augmenting pre-training with CLR can help with downstream tasks that use global information."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Discussion",
            "text": "Recently, the size of pre-trained models and the datasets have increased to such an extent that it is cost-prohibitive to pre-train these models on datasets aligned with the downstream tasks of interest. Hence, a light-weight mid-training strategy can tune the pre-trained features strengthening the downstream performance.\nAn alternative to the mid-training strategy is to include task during pre-training itself. This alternative strategy has two drawbacks; first, the amount of labeled data available for the mid-training task is typically not large enough to have significant impact when jointly learned in the pre-training stage. Secondly, the mid-training approach is more practical as it can be applied to already available pre-trained models instead of training the models from scratch which requires large amounts of time and compute."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion & Future Directions",
            "text": "This work presents a multi-lingual mid-training objective and a large-scale analysis of multiple audio-visual pre-training methods and datasets, which confirms observations from Hsu et al. (2021  ###reference_b35###) and Chan et al. (2022  ###reference_b14###) — we show how large scale audio-visual pre-training significantly improves downstream ASR performance, and that a well-chosen mid-training task can help the final downstream task.\nWhile this paper presents initial insights into how mid-training tasks impact models multi-modal pre-trained models, we believe that significant additional future work remains to fully understand how sequences of training tasks can align large pre-trained models with downstream tasks.\nOne interesting direction for future work is an exploration of additional mid-training tasks. In this work, we show that translation has the power to bridge gaps between multi-modal pre-trained models and language-based ASR tasks. Paired data for translation data can often be scarce, and may not be the optimal choice for future mid-training tasks. Instead, it may be insightful to explore mid-training tasks which are centered around synthetic data (such as TTS data from text datasets, or text generated by large language models) or self-supervised approaches to mid-training.\nAnother closely related direction of future work explores how pre-training tasks impact the performance of downstream and mid-trained models. Here, we focus on multi-modal pre-training, as it is a key emergent direction of ASR research. However mid-training can easily be applied to uni-modal pre-training, or even zero-shot transfer from foundational models.\nIn conclusion, this study sheds light on the impact of mid-training tasks in the context of multi-modal pre-training and demonstrates the significant improvement in downstream automatic speech recognition performance achieved through large-scale audio-visual pre-training. By continuing to delve into these areas, we can advance our understanding of how to effectively align pre-trained models with diverse downstream tasks and unlock new possibilities for multi-modal ASR research."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}