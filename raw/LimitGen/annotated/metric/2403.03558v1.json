{
    "title": "Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem",
    "abstract": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model’s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination.\nOur code and data are available at https://github.com/Yuki-Asuuna/UMWP.\n\n\n\nKeywords: Large Language Model, Hallucination, Math Word Problem, Dataset",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Large Language Models (LLMs) have taken the field by storm, making remarkable advancements in pushing the boundaries of Natural Language Processing (NLP) (Zhao et al., 2023). Notably, OpenAI’s GPT-4 (OpenAI, 2023), Meta AI’s LLaMA-2 (Touvron et al., 2023a) and Google’s PaLM 2 Anil et al. (2023) have demonstrated exceptional performance across various few-shot and zero-shot NLP tasks, including text generation, text summarization, and question answering.\n\nHowever, LLMs can produce unreliable conjectures in ambiguous contexts, which is known as hallucination (Rawte et al., 2023). Within the context of NLP, the most inclusive and standard definition of hallucination is the generated content that is nonsensical or unfaithful to the provided source content (Ji et al., 2023). The undesired phenomenon has the potential to seriously mislead humans (Talmor et al., 2019). Figure 1 illustrates an example of hallucination towards a Math Word Problem (MWP).\n\nTowards the QA task, this paper evaluates LLMs’ degree of hallucination based on Math Word Problems (MWP). (i) Compared with general questions, MWP is challenging to mitigate hallucination through additional text retrieval. Answering MWP heavily relies on the LLM’s intrinsic abilities, including comprehension, reasoning, and computation abilities. (ii) The answer to MWP is often unique and represented as a numerical value or variable expression. In determining whether a model is prone to hallucination, the MWP-based method only involves evaluating the correctness of a numerical or variable expression output.\n\nKey Information Missing\nAmbiguous Key Information\nUnrealistic Conditions\nUnrelated Object\nQuestion Missing\n\nWe regard the MWP with non-unique solutions or no solution that may lead to hallucination in LLMs as the “unanswerable question”. Unanswerable questions can serve as a means to evaluate the degree of hallucination in LLMs, just as teachers often use unanswerable questions to gauge students’ understanding of certain concepts. Rajpurkar et al. (2018) observes extractive reading comprehension systems often tend to make unreliable guesses when the context is missing or ambiguous. This phenomenon also happens in LLMs. When hallucination occurs, LLM tends to give arbitrary or unreasonable answers, just as Figure 1 shows. Ideally, LLM should reply with “Information missing” or “Unable to answer”.\n\nIt’s worth noting that while all existing MWP datasets (Hendrycks et al., 2021; Cobbe et al., 2021; Patel et al., 2021) focus on answerable questions, there is a scarcity of datasets related to unanswerable questions. Therefore, to address this data gap, we build a new dataset called UMWP, upon several previous MWP datasets. UMWP comprises a total of 5,200 questions with half answerable questions and half unanswerable questions. We classify unanswerable questions into five categories based on their unanswerability reasons.\n\nThe main contributions of this paper are summarized as follows:\n\nWe innovatively propose a new dataset UMWP consisting of answerable and unanswerable MWP to evaluate the degree of hallucination in LLMs. We present a novel hallucination evaluation method for LLMs. Our method employs text similarity and mathematical expression detection to judge whether the LLMs’ responses reflect unanswerability.\n\nExtensive experiments on a variety of LLMs reveal variations in the degree of hallucination concerning model size, input form, and the utilization of RLHF."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Math Word Problem Benchmark",
            "text": "Many answerable MWP datasets have been proposed in previous research, primarily differing in terms of difficulty, dataset size, and content. Koncel-Kedziorski et al. (2016  ###reference_b8###) provides an automatic construction framework and collects 3,320 problems for a dataset called MAWPS. Miao et al. (2020  ###reference_b11###) presents ASDiv that covers more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level. Patel et al. (2021  ###reference_b16###) creates a challenge set called SVAMP for a more robust evaluation of methods developed to solve elementary-level MWP. OpenAI introduces GSM8K (Cobbe et al., 2021  ###reference_b3###), a dataset comprising 8.5K high-quality linguistically diverse grade school MWPs, designing to evaluate the multi-step mathematical reasoning capability of LLMs. Hendrycks et al. (2021  ###reference_b6###) introduces MATH, a dataset of 12,500 challenging competition mathematics problems. For now, MATH and GSM8K are the two most difficult MWP datasets."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Mathematical Ability of LLM",
            "text": "With the popularity of LLM, there is an increasing focus on applying LLM to solve math problems. Frieder et al. (2023  ###reference_b4###) investigates the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on 6 publicly available datasets. The result shows that though the quality of answers can be positively surprising, GPT is not yet ready to deliver high-quality proofs or calculations consistently. Wei et al. (2022  ###reference_b22###) shows that applying a chain of thought prompting can greatly improve performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Yu et al. (2023  ###reference_b24###) proposes MetaMath, a fine-tuned language model from Llama-v2 that specializes in mathematical reasoning. MetaMath-7B exceeds the state-of-the-art models of the same size by 11.5% and 8.7% on GSM8K and 19.4% on MATH (Hendrycks et al., 2021  ###reference_b6###). MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. It proves that well-fine-tuned open-source LLMs can compete with commercial LLMs even having much fewer parameters."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Hallucination Benchmark",
            "text": "Research is scarce on hallucination benchmark in the field of mathematical reasoning. However, here are some existing hallucination evaluation studies that focus on general questions. Lin et al. (2022  ###reference_b10###) purposes TruthfulQA containing 817 questions that span 38 categories, including health, law, finance, and politics, to evaluate the truthfulness of LLM. These questions are crafted in a way that will lead humans to answer falsely due to a false belief or misconception. Yin et al. (2023  ###reference_b23###) purposes the SelfAware dataset consisting of 1,032 open-ended unanswerable questions to evaluate LLMs’ self-knowledge. Li et al. (2023  ###reference_b9###) introduces the HaluEval benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. HaluEval evaluates whether LLM hallucinates through a binary label approach. Min et al. (2023  ###reference_b12###) proposes a unique benchmark called FACTSCORE to automatically evaluate the truthfulness of LLM from the perspective of biographies in Wikipedia."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Dataset Construction",
            "text": "To the best of our knowledge, all popular MWP datasets do not have unanswerable questions. We build a novel dataset UMWP upon the existing four MWP datasets - SVAMP (Patel et al., 2021  ###reference_b16###), MultiArith (Koncel-Kedziorski et al., 2016  ###reference_b8###), GSM8K (Cobbe et al., 2021  ###reference_b3###), and ASDiv (Miao et al., 2020  ###reference_b11###). The questions in these four datasets are from real-life scenarios and have unique answers. We task two data annotators with modifying the original questions to make them unanswerable. Specific strategies in Table 5  ###reference_### are applied during the modification process. Three volunteers validate the questions. The question with three unanswerable annotations is accepted. Finally, we build a dataset composed of 2,600 answerable questions and 2,600 unanswerable questions.\n###table_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Unanswerable Question",
            "text": "Unanswerable questions are classified into five categories based on the reasons for unanswerability. The classification criteria are referenced from negative examples in SQUAD 2.0 (Rajpurkar et al., 2018  ###reference_b17###). Table 1  ###reference_### illustrates the five categories with the statistics. LLM’s ideal response for unanswerable question should express uncertainty rather than providing a precise answer.\n(i) Key Information Missing: Questions where essential conditions are omitted.\n(ii) Ambiguous Key Information: Questions with ambiguous conditions, including ranges, vague terms, or negations.\n(iii) Unrealistic Conditions: Questions with conditions that conflict with real-world logic, such as using negative numbers for item quantities or decimals for indivisible items.\n(iv) Unrelated Object: Questions where the subject mentioned in the question is absent from the source input.\n(v) Question Missing: Questions without the actual question body."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Answerable Question",
            "text": "Each answerable question has a definite answer. The statistics of answerable questions are shown in Table 2  ###reference_###. The GSM8K dataset features longer question descriptions by token count, whereas the other three datasets have shorter ones."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Evaluation Method",
            "text": "In this section, we introduce the method for quantitatively evaluating LLMs’ degree of hallucination. In the context of instruction and In-Context Learning (ICL) input forms (Ouyang et al., 2022), we observe that LLMs tend to exhibit strong template-like outputs when expressing uncertain meanings. However, in the Direct input form, LLM outputs may contain words indicating uncertainty, such as “unknown” or “unsure”. Algorithm 1 shows the details of the evaluation process.\n\nTo judge whether the output of a question reflects unanswerability, we define a similarity function to compute the similarity between a given sentence and a set of unanswerable template sentences. A pre-determined threshold is used in this evaluation.\n\nIf the condition is met, the output is regarded as “unanswerable.” If LLMs’ responses appear as variable expressions, we assume the LLM may have identified potential variables in the unanswerable question. Otherwise, we assume the LLM regards the question as “answerable.” The identification process is described as follows:\n(i) LLM output is tokenized by the open-source tool Spacy Montani et al. (2023).\n(ii) Common vocabulary and space characters are removed from the text.\n(iii) Identification is done by checking for the presence of valid variable expressions by regex. If found, the output is labeled as “unanswerable.” An example is illustrated in Figure 2.\n\nThe identification of unanswerable questions involves designating them as positive cases and answerable questions as negative cases."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiment",
            "text": "We conduct experiments using a series of LLMs, including GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), Claude, LLaMA (Touvron et al., 2023b) and LLaMA-2 (Touvron et al., 2023a). We employ three different input forms: Direct, Instruction, and ICL.\n\nCompared to Direct input, the Instruction and ICL input forms can provide richer contextual information, significantly improving the LLMs’ ability to recognize hallucination.\n\nLLMs can recognize potential variables within unanswerable questions and may output a math expression in response. We set the sample size to 520 (10% of the UMWP) and employ the random sampling strategy. We ensure the proportion of unanswerable questions across different categories is consistent with Table 2. 5 annotators participate in the evaluation process. Table 3 shows that using a template-based approach combined with mathematical expression detection can improve the consistency with human judgment. The Cohen’s kappa coefficient for the LLMs in Table 3 falls within the range of a good match (>0.75).\n\nWe also investigate human benchmarks on UMWP."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Setting",
            "text": "We adopt SimCSE (Gao et al., 2021  ###reference_b5###) as the similarity function. According to the threshold ablation (Yin et al., 2023  ###reference_b23###), we set the similarity threshold. During the generation process, we set the temperature for GPT, InstructGPT, LLaMA, and LLaMA-2. To eliminate potential similarity calculation errors caused by differences in the lengths of target and reference sentences, we employ a sliding window of length 6 to parse the output sentence into semantic chunks."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Human Benchmark",
            "text": "To establish a benchmark for humans, we randomly select 200 samples from UMWP, ensuring the distribution of these samples across different categories remains consistent with the original dataset. Subsequently, we assign these samples to five volunteers."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Set U Construction",
            "text": "We aggregate answers from 31 LLMs that are labeled as “unanswerable” and extract common features to construct the set . Subsequently, we conducted a manual filtering process to eliminate incorrect strings from set . The detail of set  is shown in Section A.5  ###reference_###."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Experiment Results Analysis",
            "text": "We conduct a concise analysis of LLMs’ hallucination performance on UMWP, mainly considering 4 dimensions: model size, input forms, RLHF, and comparison of evaluation methods. The experimental results for the following three dimensions (model size, input forms, RLHF) are depicted in Figure 3.\n\nIn the LLaMA series, across three input forms, there is a continuous improvement as the model size increases. In the InstructGPT series, this trend is generally observed, except for the text-babbage-001.\n\nCompared to Direct input, the Instruction and ICL input forms can provide richer contextual information, significantly improving the LLMs’ ability to recognize hallucination. As the parameter size increases, the difference between the instruction and the ICL input form is gradually decreasing.\n\nComparing LLaMA-v2-7b-chat to LLaMA-v2-7b, LLaMA-v2-13b-chat to LLaMA-v2-13b, and LLaMA-v2-70b-chat to LLaMA-v2-70b, we find RLHF substantially improves performance across three input forms. Notably, LLaMA-v2-13b-chat’s performance can compete with that of LLaMA-65b, despite having significantly fewer parameters.\n\nLLMs can recognize potential variables within unanswerable questions and may output a math expression in response. We set the sample size to 520 (10% of the UMWP) and employ the random sampling strategy. We ensure the proportion of unanswerable questions across different categories is consistent with Table 2. 5 annotators participate in the evaluation process. Table 3 shows that using a template-based approach combined with mathematical expression detection can improve the consistency with human judgment. The Cohen’s kappa coefficient for the LLMs in Table 3 falls within the range of a good match(>0.75).\n\nWe also investigate human benchmarks on UMWP. GPT-4 demonstrates impressive performance. However, it still shows a difference when compared to the human benchmark result of 93.16%."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   Noise Analysis",
            "text": "According to Algorithm 1, the LLM response is labeled binary. Experiments need to be conducted to judge whether LLM output contains nonsensical or unfaithful information beyond the binary classification. We manually examine whether 5 LLMs generate unrelated content. These LLMs are chosen because they exhibit relatively lower capabilities within their respective series. The result is shown in Appendix Table 4. Although there are cases where LLM may output information unrelated to the question, such cases are rare and have a limited impact on the benchmark results. We conduct further discussions and analysis in Section A.1."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "The hallucination of LLM has the potential to mislead humans seriously. This study explores the evaluation of hallucination in LLMs through the perspective of Unanswerable Math Word Problems (UMWP). Based on existing MWP datasets, we create a new dataset and introduce an evaluation method combining text similarity and mathematical expression detection for assessing hallucination in various series of LLMs including GPT-3, InstructGPT, Claude, and LLaMA. The results of extensive experiments highlight the impact of model size, In-Context Learning, and RLHF on hallucination mitigation. We believe that our work provides a feasible way of assessing hallucination in LLMs."
        }
    ]
}