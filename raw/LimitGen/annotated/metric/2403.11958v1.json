{
    "title": "Language Evolution with Deep Learning Chapter to appear in the Oxford Handbook of Approaches to Language Evolution",
    "abstract": "Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and reinforcement learning methods and summarizes their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic simulations. This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social animals have been found to use some means of communication to coordinate in various contexts: foraging for food, avoiding predators, mating, etc. (Hauser, 1996). Among animals, however, humans seem to be unique in having developed a communication system, natural language, that transcends these basic needs and can represent an infinite variety of new situations (Hauser et al., 2002) to the extent that language itself becomes the basis for a new form of evolution: cultural evolution. Understanding the emergence of this unique human ability has always been a vexing scientific problem due to the lack of access to the communication systems of intermediate steps of hominid evolution (Harnad et al., 1976, Bickerton, 2007). In the absence of data, a tempting idea has been to reproduce experimentally the process of language emergence in either humans or computational models (Steels, 1997, Myers-Scotton, 2002, Kirby, 2002).\n\nExperimental paradigms with humans (Kirby et al., 2008, Raviv et al., 2019, Motamedi et al., 2019) have produced significant insights into language evolution. Still, their scope is limited due to the inability to replicate key aspects of language evolution, such as communication within and across large populations and the study of long evolutionary timescales. Computer modeling can help overcome these limitations and has played a prominent role in studying language evolution for a long time (Lieberman and Crelin, 1971). In particular, agent-based modeling has been used from the early days of the language evolution research “renaissance” (Hurford, 1989, Steels, 1995) and is still a very active and influential field (Reali and Griffiths, 2009; 2010, Smith et al., 2003, Vogt, 2009, Gong et al., 2014, Ke et al., 2008, Brace et al., 2015, Cuskley et al., 2017, Kirby et al., 2015).\n\nMeanwhile, in the last decade, the field of machine learning has rapidly developed with the advent of deep learning. Deep neural networks have achieved human-level performance in various domains, including image recognition (He et al., 2016, Chen et al., 2020), natural language processing (Devlin et al., 2018, Brown et al., 2020), automatic translation (Bahdanau et al., 2014, Vaswani et al., 2017), and reinforcement learning (Silver et al., 2016).\n\nThis chapter aims to introduce the technical and conceptual background required for using deep learning to simulate language evolution, that is, to simulate both the emergence of communication in evolutionary timescales and patterns of language change in historical timescales (Kottur et al., 2017, Lazaridou et al., 2018, Lazaridou and Baroni, 2020).\n\nFirst, we present how to implement a communication game (Sec. 2), including formalizing it as a machine learning problem (Sec. 2.1), designing neural network agents (Sec. 2.2) and making agents learn to solve the game (Sec. 2.3).\n\nSecond, we examine the Visual Discrimination Game (Lewis, 1969) as a case study (Sec. 3), which has been widely explored in neural emergent communication research.\n\nFinally, we provide an overview of recent emergent communication simulations with neural networks, highlighting the successes, limitations, and future challenges (Sec. 4)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Designing communication games with Deep Learning",
            "text": "Communication games (Lewis, 1969, Steels, 1995, Baronchelli et al., 2010) are a framework used to investigate how perceptual, interactive, or environmental pressures shape the emergence of structured communication protocols (Kirby et al., 2008, Cuskley et al., 2017, Raviv et al., 2019). This framework has primarily been studied over the past years and is still one of the leading simulation frameworks in language evolution. See Chapter Communication games: Modelling language evolution through dyadic interaction for more details. This section presents how to simulate communication games using Deep Learning. First, we frame the communication game as a multi-agent problem, where each agent is represented by a deep neural network. Second, we define communicative agents. Third, we use machine learning optimization to train agents to solve the communication game.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network (LeCun et al., 1988) is suitable for generating image representations from visual input data, as illustrated in Figure 7. The generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) (Elman, 1990, Mikolov et al., 2010) and Transformers (Vaswani et al., 2017) are well suited for sequences and are hence used in standard emergent communication settings (Lazaridou et al., 2018, Chaabouni et al., 2019, Kottur et al., 2017, Li and Bowling, 2019, Chaabouni et al., 2022, Rita et al., 2022a). Communication is mainly based on discrete messages, even if some works consider continuous communication protocols (Tieleman et al., 2019).\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It’s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module. The action module maps an internal representation of an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents. Nonetheless, the message generation can still be made differentiable.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods.\n\nTo train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is the translation: the network learns to map one language to another by training on pairs where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, Supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network’s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Not"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Framing communication games as a machine learning problem",
            "text": ""
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Machine learning is well suited for simulating communication games",
            "text": "Mitchell (1997) defines machine learning as follows: “A computer program is said to learn from an experience with respect to some class of tasks and performance measure, if its performance at tasks in, as measured by, improves with experience.” Machine learning is well suited to frame communication games: participants develop a language through trial and error during a communication game. They iteratively adapt their language production and understanding to achieve a given task for which at least one agent lacks information (Tadelis, 2013). While game theoretic approaches analyze stable communication protocols (Crawford and Sobel, 1982, Skyrms, 2010), studying the dynamic learning process is a more challenging and richer problem. Borrowing Mitchell (1997) notations, this dynamic process can be framed as a machine learning problem where participants are computer programs that perform the communication game. The game’s success is measured by after each episode of the game, and participants update their communication protocol based on the outcome. After enough iterations, the participants may converge, i.e., stabilize on a successful communication protocol, allowing them to solve the game. This iterative learning process is illustrated in Figure 1 and is the fundamental idea of machine learning."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Formalizing communication games as a machine learning problem",
            "text": "###figure_7### For simplicity, we focus in this chapter on two-player communication games where one agent, the “sender” sends messages to a second agent, the “receiver” that parses them and takes action to solve the task in an environment. This setting is referred to as dyadic unidirectional communication games in the literature. Formally, the “sender” and “receiver” are parametric models respectively denoted by  and  with parameters  and . Both parametric models will further be designed as deep neural networks. As illustrated in Figure 2, a round of the game proceeds as follows:\n\n1. The sender  and receiver  get observations from their environment denoted by  and .\n2. The sender  sends a message  to the receiver  where  is a sequence of symbols taken from a fixed vocabulary .\n3. The receiver  uses the message  and its observation  to perform an action  toward achieving the task.\n4. The task’s success is then measured by two reward signals  and  which are given to the sender  and the receiver  respectively to improve their protocols.\n\nThroughout the game, both agents must agree on a common language to solve the game. Importantly, the emergent language is not defined by explicit language rules but implicitly encoded by the sender’s parameters .\n\nRemark: This chapter presents a simplified formalism of communication games. Rigorously, communication games should be framed as a special case of Markov Games that provide a broader formal framework for reasoning about multi-agent problems. For further information, refer to Littman.\n\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### In a communication game, the deep neural agents aim to build communication and action policies. This is realized by maximizing their reward. The following is therefore needed:\n\n1. Design the communicative agents as neural networks.\n2. Train agents to build a shared communication protocol.\n\nFigures 3 and 4 represent communication games commonly studied in language emergence simulations with deep learning. The former presents simple Lewis and negotiation games, while the latter showcases efforts to build more realistic scenarios.\n\nRemark: At the time of writing, many Python libraries, like PyTorch and Jax, are used for easy implementation and optimization of neural networks and are particularly helpful for beginners due to the abundance of online examples."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Designing communicating agents with neural networks",
            "text": "To model communicative agents, we first break them into functional modules that enable interaction with the environment and other agents. Then, define neural networks and explain how they can be used to parameterize these functional modules. Finally, we introduce neural senders and receivers as specific types of neural communicative agents.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network is suitable for generating image representations from visual input data.\n\nThe generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) and Transformers are well suited for sequences and are hence used in standard emergent communication settings. Communication is mainly based on discrete messages, even if some works consider continuous communication protocol.\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It’s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module.\n\nThe action module maps an internal representation to an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 Designing a communicative agent as functional modules",
            "text": "As depicted in Figure 5  ###reference_###, a communicative agent should be able to interact with:\nIts environment by either passively observing it or actively taking actions that influence it;\nAnother agent using a message space by passively receiving or actively sending messages.\nTherefore, four functional modules are typically needed to model agents: perception, generation, understanding, and action. (1) The perception module maps an environment’s view to an internal representation, (2) the generation module generates a message based on internal representations, (3) the understanding module takes a message and builds an internal message representation, (4) the action module maps an internal representation to an action in the environment.\nNeural networks are suited for modeling and combining these modules."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Short introduction to neural networks",
            "text": "A neural network is a parametric model approximating a function or probability distribution based on data. It maps vector inputs to outputs through a succession of linear and non-linear operations. Its learnable parameters, called the weights, are used to perform the linear operations. The fundamental building block of a neural network is made of two operations:\n\n1. A linear transformation applying the matrix of weights to the incoming input.\n2. A non-linear transformation, called the activation function (typically sigmoid function, hyperbolic tangent or ReLU).\n\nAs displayed in Figure 6, these operations are stacked at each layer, transforming the input to a prediction through multiple linear and non-linear transformations. Neural networks have a crucial property: all operations are differentiable. This allows for using gradient-based methods to learn the weights.\n\nWhen training a neural network, the goal is to find the optimal weights such that the neural network accurately maps inputs to their corresponding outputs. Neural networks with enough weights can represent complex functions due to their high expressive power, approximating any continuous function with any level of precision. However, computation or data limitations can hinder this process. Deep learning investigates how to adapt networks’ architecture or weight matrix shape to overcome these limitations. Figure 7 presents the main network architectures and the data they are suitable for."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3 Neural functional modules",
            "text": "Several network architectures can be considered when designing agent modules as defined in Sec 2.2.1. This section presents some common choices for each module.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network (LeCun et al., 1988) is suitable for generating image representations from visual input data, as illustrated in Figure 7.\n\nThe generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) (Elman, 1990; Mikolov et al., 2010) and Transformers (Vaswani et al., 2017) are well suited for sequences and are hence used in standard emergent communication settings (Lazaridou et al., 2018; Chaabouni et al., 2019; Kottur et al., 2017; Li and Bowling, 2019; Chaabouni et al., 2022; Rita et al., 2022a). Communication is mainly based on discrete messages, even if some works consider continuous communication protocols (Tieleman et al., 2019).\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It’s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module.\n\nThe action module maps an internal representation of an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents. Nonetheless, the message generation can still be made differentiable as described in Section 2.3.2.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods."
        },
        {
            "section_id": "2.2.4",
            "parent_section_id": "2.2",
            "section_name": "2.2.4 Modeling neural network communicative agents in communication games",
            "text": "Section 2.2 presents the components of a general communicative agent, though not all modules may be used during a game. Figure 8 illustrates sender and receiver modeling in a unidirectional game. This modeling is used in the use case we derive in Section 3, namely the Visual Discrimination Game."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Optimizing the agents to solve the game",
            "text": "In Deep Learning, the goal is to train neural networks to solve a task, i.e., find the optimal weights that maximize their performance. This section covers optimization techniques for training neural networks and their application to communication games. \n\nTo train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is translation: the network learns to map one language to another by training on pairs where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network’s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Noteworthy, the probability of action is called a policy in RL. In communication games, the sender produces a sequence of symbols to assist the receiver in completing a predetermined task. If this sequence leads to a successful outcome, the sender is rewarded positively; otherwise, it receives a negative reward. Through iterative trial and error, the sender refines its sequence of symbols toward maximizing its reward and ultimately solving the game.\n\nSupervised learning is easy to apply and highly reproducible but requires a known target. On the other hand, reinforcement learning is more generic and only requires a score to be defined at the cost of being more complex. For instance, to train a network to play chess, supervised learning would involve imitating the moves of a pro-player with a dataset, while reinforcement learning would require playing the whole game and rewarding victories: the training is more complex and slower but does not require data. It is noteworthy that the reinforcement learning reward can be defined arbitrarily. This approach has been applied to train large dialogue systems by imitating the human language and refining it with reinforcement learning.\n\nRegardless of the learning technique, the task’s success is optimized by introducing a proxy, the loss function. The goal is then to find weights such that the neural network minimizes the average loss function over the entire training dataset. Loss functions vary depending on the network output and the training task.\n\nThe loss function is reduced using a learning process that involves a series of updates known as Gradient Descent updates. They iteratively adjust the network’s parameters by following the loss gradient. The magnitude of the update is controlled by a hyperparameter called the learning rate. The goal is to find weights such that the loss gradient equals zero. This is achieved by repeating the gradient update rule.\n\nIn practice, computing the exact gradient of the averaged loss function is infeasible since it necessitates processing the complete dataset. Stochastic Gradient Descent overcomes this challenge by approximating the loss function gradient using a limited number of data samples, or batches, at each iteration. In standard machine learning libraries, Stochastic Gradient Descent updates are performed by pre-implemented methods referred to as optimizers. In communication games, this gradient is the mathematical operation that modifies the agent behavior. For instance, every single speaker update alters its generation of symbols, refining its emergent language step by step toward maximizing the reward objective.\n\nTraining a model involves minimizing the loss of the training data, but evaluating its performance on unseen data is crucial to ensure the network’s quality. Intuitively, it is like creating an exam for students with unseen exercises to ensure they correctly understand the lecture. ML Practitioners distinguish (1) the training dataset and its corresponding loss, (2) the test dataset with unseen samples and its corresponding loss. The relation between the two losses indicates how well the model generalizes and can be trusted.\n\nWhen training a model, it is recommended to divide the dataset into three parts: training, validation, and testing (typical proportion). The training set is used to train the model, the validation set to find the generalization regime, tune hyperparameters, and retrieve the best model across training, and the test set is used to test the model and report the final score. Intuitively, validation data is similar to mock exams, whereas test data is the actual network exam. In practice, the validation loss is regularly plotted, and when it starts increasing, training is stopped. This technique is known as early stopping.\n\nRegularization methods were developed to prevent"
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Optimizing a machine learning problem",
            "text": "To train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is translation: the network learns to map one language to another by training on pairs, where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network’s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Noteworthy, the probability of action is called a policy in RL. In communication games, the sender produces a sequence of symbols to assist the receiver in completing a predetermined task. If this sequence leads to a successful outcome, the sender is rewarded positively; otherwise, it receives a negative reward. Through iterative trial and error, the sender refines its sequence of symbols toward maximizing its reward and ultimately solving the game.\n\nSupervised learning is easy to apply and highly reproducible but requires a known target. On the other hand, reinforcement learning is more generic and only requires a score to be defined at the cost of being more complex. For instance, to train a network to play chess, supervised learning would involve imitating the moves of a pro-player with a dataset, while reinforcement learning would require playing the whole game and rewarding victories: the training is more complex and slower, but it does not require data. It is noteworthy that the reinforcement learning reward can be defined arbitrarily, e.g., one may give an extra bonus when winning the game while preserving the queen, or it could also be used on top of a supervised training regime. This approach has been applied to train large dialogue systems by imitating the human language and refining it with reinforcement learning.\n\nRegardless of the learning technique, the task’s success is optimized by introducing a proxy, the loss function. The goal is then to find weights such that the neural network minimizes the average loss function over the entire training dataset. Loss functions vary depending on the network output and the training task. In reinforcement learning, the losses often include the TD error or the score function, which converts the expected sum of rewards as a training objective. \n\nThe loss function is reduced using a learning process that involves a series of updates known as Gradient Descent updates. They iteratively adjust the network’s parameters by following the loss gradient. The magnitude of the update is controlled by a hyperparameter called the learning rate. Given the optimization problem, the goal is to find weights such that the loss gradient equals. This is achieved by repeating the following gradient update rule: where and are the model parameters respectively at iteration and, the gradient of the loss function and the learning rate.\n\nIn practice, computing the exact gradient of the averaged loss function is infeasible since it necessitates processing the complete dataset. Stochastic Gradient Descent overcomes this challenge by approximating the loss function gradient using a limited number of data samples, or batches at each iteration. In standard machine learning libraries, Stochastic Gradient Descent updates are performed by pre-implemented methods referred to as optimizers. In communication games, this gradient is the mathematical operation that modifies the agent behavior. For instance, every single speaker update alters its generation of symbols, refining its emergent language step by step toward maximizing the reward objective.\n\nTraining a model involves minimizing the loss of the training data, but evaluating its performance on unseen data is crucial to ensure the network’s quality. Intuitively, it is like creating an exam for students with unseen exercises to ensure they correctly understand the lecture. ML Practitioners distinguish (1) the training dataset and its corresponding loss, (2) the test dataset with unseen samples and its corresponding loss. The relation between the two losses indicates how well the model generalizes and can be trusted.\n\nUnderfitting: Both and are high, indicating ineffective learning. An under-parametrized network or a small learning rate may cause persistent under-fitting. In communication games, this scenario arises when no successful communication emerges between the sender and receiver, resulting in a poor task success both on and .\n\nGeneralization: Both and are low, indicating successful training and generalization. In communication games, this regime occurs when agents develop a successful communication"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Optimizing communication games with machine learning",
            "text": "Unlike a single network training, two networks are trained simultaneously during a communication game, sometimes requiring different learning methods for each agent. The process involves selecting appropriate (1) learning methods, (2) rewards and loss functions, and (3) optimization protocols.\n\nRemark:  The machine learning community has developed frameworks for simulating various communication games, which can be rapidly replicated, understood, and modified. Existing codebases include (Kharitonov et al., 2019) and (Chaabouni et al., 2022) as long as the detailed notebook we provide.\n\nThree learning pipelines are mainly used to train agents in communication games:\n\nBoth agents optimized with RL: This generic and realistic setting assumes no specific task format and involves separate agents with individual rewards and training losses, making it suitable for training any task. However, such training is usually hard to optimize with high variance and requires careful use of RL tools we introduce later.\n\nSender optimized with RL and Receiver optimized with SL: This approach is well-suited for single-turn message games where the receiver only needs to perform one valid action after receiving a message, such as in referential games (Lewis, 1969, Skyrms, 2010). In such cases, the receiver’s action is fully determined by the sender’s observation and its message, creating a supervised training sample for the receiver. The receiver’s training becomes more robust by learning to map messages to the corresponding output actions using a supervised loss. Note that the sender still needs to be optimized with RL since message generation is non-differentiable, i.e., the receiver’s error cannot propagate to the sender. It ensures more stable training than using a pure RL reward-based approach.\n\nBoth agents optimized with SL: When both agents cooperate fully and optimize the same learning signal, they can be trained using a single supervised training signal. In this scenario, the Sender-Receiver couple is optimized as a single network that maps inputs to output actions, with a discrete intermediate layer. Reparametrization tricks such as Gumbel-Softmax (Jang et al., 2016, Maddison et al., 2016) have been developed to overcome the non-differentiability of message generation and allow the receiver’s error to flow to the sender. \n\nWe next derive the case where agents are optimized with RL as it covers all communication tasks. Reward functions and must be defined to measure the success of the communication task for each agent. These functions typically take agents’ observations and and the receiver’s action as input and return if the task is solved, otherwise.\n\nRemark:  The reward is the core element inducing the structure of the emergent language. Thus, we recommend carefully avoiding designing rewards toward obtaining a specific language, e.g., directly rewarding compositionality or syntactic properties. Instead, we suggest using rewards that measure communication success without any human prior. Hence, language features may emerge from solving a specific task rather than being forced by design. The agents’ goal is to maximize their respective reward over time, i.e., the expected rewards: denotes a game episode that depends on the sender’s and receiver’s stochastic policies. The sender message and the receiver’s action are sampled from those distributions. In reinforcement learning, the goal is to minimize the expected negative reward. However, this objective cannot be directly turned into a gradient update as the reward is not differentiable by definition. Mathematical tools have been developed to circumvent this issue (Sutton and Barto, 2018).\n\nThe policy-gradient algorithm (Sutton et al., 1999) is mostly used in neural language emergence. Denoting by and the sender and receiver’s respective loss gradient, we have:\n\nIn practice, the quantities and are computed over a batch of game episodes and passed to each agent optimizer. is the stop gradient operator that prevents an optimizer from computing the gradient inside the operator. The optimization encounters challenges, for which we provide a few recipes to ensure a successful optimization process:\n\nImplementing Policy Gradient: While RL notations may become overwhelming for beginners, their implementation is quite straightforward in practice with recent machine learning libraries (Paszke et al., 2019, Bradbury et al., 2018).\n\nDealing with large variance: Estimating the gradient of a RL loss is difficult due to the large variance of gradient estimates. Large batch sizes and the baseline method should be used to alleviate this. The latter implies subtracting a baseline from the reward, which does not bias the estimate while reducing the variance. A common baseline is the average value of the reward across a batch of data.\n\nControlling the exploration-exploitation trade-off: To prevent the collapse of training due to sub-optimal average reward, one can control the exploitation-exploration trade-off by penalizing the entropy of the policies with the terms and ( and refers to the entropy function applied on agents’ policies) and where is the entropy function. By increasing the coefficient (resp. ), the sender"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Case study: Simulating a Visual Discrimination Game",
            "text": "We now focus on a particular communication game: the Visual Discrimination Game, a type of Lewis Referential Games. These games, which explore how languages emerge through their use, have been extensively studied from theoretical and experimental angles in language evolution.\n\nThe Visual Discrimination Game involves two players: a sender and a receiver. The game proceeds as follows:\n- The sender sees an image and communicates about it to the receiver.\n- Using the message, the receiver has to guess the original image seen by the sender among a set of candidate images.\n- The original image is revealed, and the two players are informed about the task’s success.\n\nAgents play the game repeatedly until they synchronize on a communication protocol that enables the receiver to distinguish any image from any set of distractors.\n\nThe following parameters must be specified:\n1. **Image dataset**: This is the set of images the agents must communicate about. Machine learning experiments can be conducted with large-scale datasets, which is critical for developing a rich communication protocol. For example, some studies have relied on ImageNet, a million images dataset spanning more than categories including animals, vehicles, objects, or instruments. Synthetic datasets, like CLEVR, are also valuable for evaluating agents’ ability to communicate about ambiguous images using compositional languages.\n\n2. **Number of candidate images**: The receiver must differentiate the original image from distractor images. The task’s difficulty depends on the value of N: a higher N requires a more precise communication protocol.\n\n3. **Message space**: The message space is shaped by the vocabulary V and message maximum length L. Adjusting those parameters crucially influences the sender’s expressiveness. By denoting the vocabulary size by V, the sender can use a total number of V^L messages.\n\nUsing previous notations:\n- Sender’s observation x is an image sampled from the dataset.\n- Receiver’s observation y is a set of N images sampled from the dataset that includes the sender’s observation x.\n- Message m is a message sent by the sender.\n- Action a is the choice of image among the set of N images.\n\nFollowing agents' design, figure reports standard agents' design choices in the Visual Discrimination Game. \n\nA typical reward function assigns a reward of R = 1 if the receiver picks up the correct image and 0 otherwise. The modeling parameters, which include the vocabulary V, maximum message length L, and the number of candidates N, should be selected based on the problem under investigation.\n\nFor the optimization, we recommend using a large batch size and one Adam optimizer per agent. The other parameters, including exploration coefficients and learning rates, are interdependent and should be adjusted simultaneously until the simulation works. Common strategies for parameter tuning include manual adjustment or more systematic methods like grid search.\n\nA full implementation of the game with technical details and a starting set of working parameters is provided at:\nhttps://github.com/MathieuRita/LangageEvolution_with_DeepLearning"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Bridging the gap between neural networks and humans in language evolution simulations",
            "text": "This section focuses on current endeavors in using deep learning as a framework for language evolution simulations. It covers the field’s progress in using neural networks to replicate human languages and highlights the potential and challenges of deep learning simulations. Neural network simulations provide extensive flexibility for modeling various aspects of language emergence simulations, including the game, inputs, and agents. Two primary strategies have been pursued: simplifying experiments into controllable settings (Kottur et al., 2017 ###reference_b30###, Chaabouni et al., 2019 ###reference_b63###; 2020 ###reference_b100###, Ren et al., 2020 ###reference_b101###, Rita et al., 2022a ###reference_b66###), assessing the influence of incremental modeling elements; and creating more humanly plausible scenarios that emulate language emergence in complex environments (Das et al., 2019 ###reference_b102###, Jaques et al., 2019 ###reference_b45###). It has resulted in various tasks, from basic referential tasks to complex ecological tasks in grounded environments (Das et al., 2019 ###reference_b102###). In terms of inputs, it spans from hand-designed structured and controllable inputs (Kottur et al., 2017 ###reference_b30###, Chaabouni et al., 2019 ###reference_b63###; 2020 ###reference_b100###, Ren et al., 2020 ###reference_b101###, Rita et al., 2020 ###reference_b103###; 2022a ###reference_b66###) to complicated visual inputs (Evtimova et al., 2017 ###reference_b104###, Lazaridou et al., 2018 ###reference_b31###, Dessì et al., 2021 ###reference_b90###, Chaabouni et al., 2022 ###reference_b65###, Rita et al., 2022b ###reference_b78###). As for agents, it extends from pairs of agents decomposed into senders and receivers to pairs of bidirectional agents (Bouchacourt and Baroni, 2018 ###reference_b105###, Graesser et al., 2019 ###reference_b106###, Taillandier et al., 2023 ###reference_b107###, Michel et al., 2023 ###reference_b108###) and populations (Tieleman et al., 2019 ###reference_b67###, Graesser et al., 2019 ###reference_b106###, Rita et al., 2022a ###reference_b66###, Michel et al., 2023 ###reference_b108###).\n\nSimulations give rise to the emergence of artificial languages whose properties are compared to human languages. As human languages can be described in terms of language universals, i.e., abstract properties found across all human languages, studies have tried to establish the conditions under which those universal properties emerge. Such universals mainly include compositionality, i.e., the ability to decompose the meaning of an utterance as a function of its constituents (Hockett, 1960 ###reference_b109###), measured through topographic similarity (Brighton and Kirby, 2006 ###reference_b110###), (Chaabouni et al., 2020 ###reference_b100###), or Tree Reconstruction Error (Andreas, 2019 ###reference_b111###); efficiency, i.e., efficient information compression, measured through message length statistics and semantic categorization (Zipf, 1949 ###reference_b112###, Regier et al., 2015 ###reference_b113###); demographic trends, such as the impact of population size, contact agents proportion, network topology on language structure (Clyne, 1992 ###reference_b114###, Wray and Grace, 2007 ###reference_b115###, Wagner, 2009 ###reference_b116###, Gary Lupyan, 2010 ###reference_b117###).\n\nA first approach is to question whether the most simple communication task, i.e., referring to objects in an environment through referential communication, is enough to see human language features emerge. The first works on referential tasks showed that neural agents could successfully derive a communication protocol from solving the task (Kottur et al., 2017 ###reference_b30###, Lazaridou et al., 2016 ###reference_b89###, Havrylov and Titov, 2017 ###reference_b118###). Still, such protocols are neither interpretable nor bear the core properties of human languages. Indeed, agents tasked with communicating about images do not utilize semantically significant concepts but instead shortcut the task by basing their messages on low-level visual features (Lazaridou et al., 2016 ###reference_b89###, Havrylov and Titov, 2017 ###reference_b118###, Cha"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Opportunities opened by deep learning simulations",
            "text": "Neural network simulations provide extensive flexibility for modeling various aspects of language emergence simulations, including the game, inputs, and agents. Two primary strategies have been pursued: simplifying experiments into controllable settings (Kottur et al., 2017, Chaabouni et al., 2019; 2020, Ren et al., 2020, Rita et al., 2022a), assessing the influence of incremental modeling elements; and creating more humanly plausible scenarios that emulate language emergence in complex environments (Das et al., 2019, Jaques et al., 2019). It has resulted in various tasks, from basic referential tasks to complex ecological tasks in grounded environments (Das et al., 2019). In terms of inputs, it spans from hand-designed structured and controllable inputs (Kottur et al., 2017, Chaabouni et al., 2019; 2020, Ren et al., 2020, Rita et al., 2020; 2022a) to complicated visual inputs (Evtimova et al., 2017, Lazaridou et al., 2018, Dessì et al., 2021, Chaabouni et al., 2022, Rita et al., 2022b). As for agents, it extends from pairs of agents decomposed into senders and receivers to pairs of bidirectional agents (Bouchacourt and Baroni, 2018, Graesser et al., 2019, Taillandier et al., 2023, Michel et al., 2023) and populations (Tieleman et al., 2019, Graesser et al., 2019, Rita et al., 2022a, Michel et al., 2023).\n\nSimulations give rise to the emergence of artificial languages whose properties are compared to human languages. As human languages can be described in terms of language universals, i.e., abstract properties found across all human languages, studies have tried to establish the conditions under which those universal properties emerge. Such universals mainly include compositionality, i.e., the ability to decompose the meaning of an utterance as a function of its constituents (Hockett, 1960), measured through topographic similarity (Brighton and Kirby, 2006), (Chaabouni et al., 2020), or Tree Reconstruction Error (Andreas, 2019); efficiency, i.e., efficient information compression, measured through message length statistics and semantic categorization (Zipf, 1949, Regier et al., 2015); demographic trends, such as the impact of population size, contact agents proportion, network topology on language structure (Clyne, 1992, Wray and Grace, 2007, Wagner, 2009, Gary Lupyan, 2010)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Do neural networks replicate human behaviors?",
            "text": "To provide valuable insights through deep learning simulations, replicating human languages is essential. This involves identifying the basic assumptions needed for artificial agents to display human-like language patterns in their communication protocols. A first approach is to question whether the most simple communication task, i.e., referring to objects in an environment through referential communication, is enough to see human language features emerge. The first works on referential tasks showed that neural agents could successfully derive a communication protocol from solving the task (Kottur et al., 2017, Lazaridou et al., 2016, Havrylov and Titov, 2017). Still, such protocols are neither interpretable nor bear the core properties of human languages.\n\nIndeed, agents tasked with communicating about images do not utilize semantically significant concepts but instead shortcut the task by basing their messages on low-level visual features (Lazaridou et al., 2016, Havrylov and Titov, 2017, Chaabouni et al., 2022, Bouchacourt and Baroni, 2018). Additionally, when agents communicate about hand-designed structured sets of objects in a simple referential task, fundamental properties of natural languages such as compositionality (Kottur et al., 2017, Chaabouni et al., 2020) or efficiency (Zipf, 2016) do not spontaneously arise (Chaabouni et al., 2019).\n\nEventually, when referential games are played within a population of agents, human demographic trends are not reproduced. Population size does not behave as a regularization factor (Li and Bowling, 2019, Cogswell et al., 2019, Rita et al., 2022a, Chaabouni et al., 2022) and agents do not synchronize on a shared protocol (Rita et al., 2022a, Michel et al., 2023). Understanding the origins of these discrepancies from either an optimization or modelization perspective is an active research question.\n\nTo recover human language features, different human-inspired constraints have incrementally been added to simulations. Inspired by Iterated Learning (Kirby, 2001, Kirby et al., 2014), one line of research has explored the effects of learnability constraints on language emergence by altering learning dynamics. Li and Bowling (2019), Ren et al. (2020), Cogswell et al. (2019) implement neural variants of Iterated Learning by periodically introducing newborn agents and mimicking generational transmission. They find that those learning constraints drive the selection of more compositional languages, as they are easier to learn (Li and Bowling, 2019).\n\nAnother line of research focuses on incorporating cognitively inspired biases into agent modeling. For example, Rita et al. (2020) show that Zipf’s Law of Abbreviation (Zipf, 1949) emerges when both pressures toward Least Effort production (Zipf, 1949, Piantadosi et al., 2011, Kanwal et al., 2017) and comprehension laziness are introduced.\n\nEventually, some researchers have refined population modeling. Rita et al. (2022a) introduce learning speed variations into populations and recover the relationship between population size and language structure reported in previous works (Gary Lupyan, 2010, Meir et al., 2012, Dryer and Haspelmath, 2013, Reali et al., 2018, Raviv et al., 2019). Graesser et al. (2019) examine contact agents phenomena and show that a contact language can either converge towards the majority protocol or result in novel creole languages, depending on the inter- and intra-community densities. Kim and Oh (2021) and Michel et al. (2023) study how social graph connectivity impacts the development of shared languages."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Toward realistic experiments",
            "text": "Although incorporating human-inspired constraints shows promise for replicating human language features, the simplicity of current models remains limited. An avenue is opened for the design of humanly plausible experiments. We present efforts to build more realistic models and discuss the associated challenges here. Task-specific communication games may be restrictive as they overlook other aspects of our language, such as conversation, interaction with the physical world, and other modalities. More realistic scenarios are needed to encompass all aspects of our language. Some attempts have been made to create more plausible settings. Chaabouni et al. (2022) complexify the referential task by scaling the game to large datasets and tasking agents to retrieve images among distractors. Evtimova et al. (2017), Taillandier et al. (2023) model conversation by building bidirectional agents for multi-turn communications; Bullard et al. (2020) explore nonverbal communication using spatially articulated agents; (Das et al., 2019) ground agents in more realistic 2D and 3D environments; Jaques et al. (2019) test agents' ability to solve social dilemmas in grounded environments.\n\nHowever, making more realistic games poses both technical and analytical challenges. Training instabilities can occur when games become more complex, requiring optimization tricks (Chaabouni et al., 2022). Moreover, as environments become more complex, the emergence of language is more challenging to analyze. For example, Lowe et al. (2020) demonstrates how agents can solve complex tasks with shallow communication protocols and why new tools are needed to assess emergent languages qualitatively and quantitatively in these situations. Many neural communication agents are designed for specific games and lack crucial aspects of human cognition. For instance, agents are often limited to either speaking or listening, which overlooks the interplay between comprehension and production (Galke et al., 2022).\n\nSome works propose more realistic agents. These include bidirectional agents that both speak and listen (Bouchacourt and Baroni, 2018, Graesser et al., 2019, Michel et al., 2023, Taillandier et al., 2023), as well as agents with restricted memory capacity that better mirrors human cognition (Resnick et al., 2019). Additionally, Rita et al. (2020) incorporate the Least Effort Principle to make agents efficient encoders (Zipf, 1949, Piantadosi et al., 2011, Kanwal et al., 2017).\n\nStill, despite the impact of these modeling constraints on emergent language properties, they are not consistently applied across the literature. One of the main limitations of neural emergent languages is that current metrics may not capture crucial features of human languages. For instance, most work only uses topographic similarity (Lazaridou et al., 2018, Li and Bowling, 2019) as a structural metric (Brighton and Kirby, 2006), which assumes that the units of the message carry out the meaning. In human languages, the meaning units are the results of a combinatorial process using nonmeaningful units, such as phonetic features or phonemes (the so-called double articulation phenomenon (Martinet, 1960); or duality of patterning (Hockett, 1970)). Other universal properties of language (formal universals (Chomsky and Halle, 1968)) include the reliance on symbols and rules (Fodor and Pylyshyn, 1988), the use of hierarchical representations or long distance dependencies (Hauser et al., 2002), the existence of part-of-speech classes (Rijkhoff, 2007) such as the distinction between content and grammatical words, the existence of deixis (Lyons, 1977), i.e. the use of certain parts of the message to refer to places or time or person relative to the context of elocution of the message, and many others.\n\nStudying such properties is challenging as it requires the design of adapted measures that could be computed both on human and artificial languages. Furthermore, current artificial settings are often too simple to drive the emergence of such properties, reinforcing the need for more realistic scenarios that translate into our environment’s complexity. One area of research focuses on investigating whether language emergence simulations can potentially enhance natural language processing tasks. One approach involves pre-training language models with artificial languages that emerged from communication games, resulting in a moderate boost when fine-tuning low-resource language tasks (Yao et al., 2022). Another approach is exploring machine-machine interaction to learn an emergent communication protocol that prompts large language models (Shin et al., 2020, Deng et al., 2022). Reciprocally, natural language models can be utilized to explore language evolution from pre-trained languages, such as studying creolization (Armstrong et al., 2022"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Deep learning advancements offer new opportunities for simulating language evolution, as neural networks can handle diverse data without pre-defined human priors. They scale significantly regarding dataset size, task complexity, and number of participants or generations. This opens up possibilities for creating realistic language evolution scenarios at unprecedented scales. Reciprocally, language evolution research can provide valuable insights for developing future deep learning models. In the journey toward building intelligent language models, it seems essential to incorporate constraints and mechanisms that shape the development and evolution of language, such as perceptual, social, or environmental pressures. We hope this chapter will encourage researchers in both language evolution and deep learning to collaborate and jointly explore those two captivating black-boxes: humans and neural networks."
        }
    ]
}