{
    "title": "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach",
    "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs’ pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pre-training data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings.\n\nWe hypothesize that token embeddings are able to capture the model’s intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Retrieval-augmented large language models (RALMs) excel in various NLP tasks. However, the knowledge provided by the retrieval process is not always useful for improving the LLMs’ prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. For example, when posed with commonsense questions or queries within the knowledge scope of their pre-training data, LLMs might respond without necessitating retrieval. Moreover, the retrieval process can incur additional computational costs and latency, which could be avoided when the model’s intrinsic knowledge has already been adequate.\n\nPrevious work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs’ pretraining data. Such a data-aware approach is developed based on the heuristic that it is easier for LLMs to capture knowledge on entities that are frequently mentioned during pre-training. This adaptive approach can save context length, thereby reducing latency and cost during LLM inference, while also mitigating performance degradation caused by redundant retrievals in LLMs.\n\nHowever, the pre-training data might not always be available due to privacy and commercial constraints, especially when dealing with proprietary or sensitive datasets. This makes it infeasible to utilize the data-aware approaches in real business scenarios. In addition, the pre-training data are not necessarily aligned with the knowledge learned by LLMs. For example, the pre-training datasets may contain conflicting descriptions regarding the same entity. In such a case, it is uncertain whether the model is knowledgeable about the entity, even if it has been frequently mentioned in the pre-training data.\n\nIn this paper, we propose a novel model-aware approach to make the judgment about when to do/skip the retrieval. Instead of requiring access to the pre-training data, we leverage the pre-trained token embeddings that are believed to explicitly reflect the model’s knowledge. In achieving this, we develop a simple yet effective representation-informed classifier that is capable of recognizing samples that are (not) in need of retrieval. This approach circumvents the risks associated with maintaining pre-training data via only requiring access to the pre-trained token embeddings, offering a safer and more straightforward way to judge the need for retrieval augmentation.\n\nIn summary, the main contributions of this work are as follows: We identify the privacy constraints inherent in Retrieval-augmented LLMs, and unveil the limitations of the existing data-aware approach. We introduce a novel model-aware approach that decides when to do/skip the retrieval process, by leveraging the token embeddings intrinsic to the model. This approach alleviates the dependency on the accessibility of pretraining data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Model-Aware Adaptive Retrieval Augmentation",
            "text": "In the context of open-domain entity-centric Question Answering (QA), the primary objective of the RALM method is to ascertain whether a given entity requires retrieval augmentation when the QA system is posed with a specific entity-centric question (e.g., ‘Louisiana’ is the entity of the question ‘What is the capital of Louisiana?’). The core of this task is to determine whether language models already possess knowledge of the entity, deciding if there is a need to retrieve external knowledge bases to enhance the model prediction. This adaptive retrieval approach can effectively save context length, thereby reducing latency during LLM inference and mitigating performance degradation caused by redundant retrievals in LLMs.\n\nOur rationale for utilizing entity embeddings as an indicator of an LLM’s knowledge about an entity is grounded in extensive prior research. This research has established a significant correlation between entity embedding distribution and entity frequency in pre-training data across various models, from BERT to the GPT series. Entity embeddings have shown effectiveness for retrieval augmentation decisions. We developed an NN-classifier-based method, aiming to parallel the DM method. This classifier aids in determining when an entity requires retrieval augmentation based on its embedding characteristics.\n\nTo ensure clarity, we define the set of entities within the dataset; a specific entity, with denoting its index in the set; the tokenized representation of the entity using the GPT/Llama2 tokenizer; the first-layer token embedding of the tokenized entity; a neural network classifier; the binary outcome (indicating the need for retrieval augmentation).\n\nGiven an entity from the set, we tokenize it using the LLM’s tokenizer (e.g., GPT-Neo/Llama2 tokenizer) to obtain its tokenized form. Subsequently, we extract the first-layer token embedding, which we hypothesize encapsulates information related to the entity’s frequency.\n\nIn alignment with previous work, we curate a subset by randomly sampling the entity-centric data from every sub-relation dataset. Each entity is converted to its respective embedding and associated retrieval label (retrieve or not). These serve as training data for the neural network classifier.\n\nAfter the training of the classifier, we employ it to predict the binary outcome when presented with a new entity. This prediction assists in determining whether the entity requires retrieval augmentation for open-domain entity-centric QA tasks.\n\nOur novel model-aware retrieval augmentation method offers an efficient way to determine the need for retrieval augmentation in open-domain entity-centric QA scenarios. In contrast to the data-aware method requiring the availability of the pre-training data, our method focuses on the analysis of entity token embeddings, ensuring applicability and scalability in real-world QA systems."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Following the experiment setting as Mallen et al. (2023), we choose POPQA, an entity-centric open-domain QA dataset. We have the following research questions (RQs) to guide the experiments:  \nRQ2: Regarding the adaptability of our method, when an LLM is fine-tuned, with modified memorization capacity of entities, can our model determine the instances of entity necessitating retrieval?  \n\nIn this section, we will perform an extensive experimental analysis of our model-aware framework."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Model-aware vs. Data-aware QA Strategies Across Diverse Model Capacities (RQ1)",
            "text": "Given that the data-aware method Mallen et al. (2023) requires access to pre-training data, it cannot be compared in the privacy-preserving setting. To maintain consistency with the data-aware method, we evaluate our framework across models of varying capacities: GPT-Neo (1.3 billion), GPT-Neo (2.7 billion), Llama 2 (7 billion), and Llama 2 (13 billion). In this evaluation, we do not perform any fine-tuning on models.\n\nWe utilize the POPQA dataset Mallen et al. (2023), which comprises 14k questions designed for capturing factual details possibly overlooked in more mainstream QA datasets. More details refer to Appendix B.\n\nThe data-aware method essentially requires calculating word frequency for all entities within each sub-dataset (e.g., genre dataset) in the pre-training data. In contrast, our model-aware technique trains an NN classifier directly using the token embeddings of entities. This eliminates the need to access/interact with any pre-training data.\n\nTo delve deeper into the discussion of which scenario our model-aware or data-aware method performs better, we have chosen the most representative sub-datasets: ‘mother’ and ‘capital of’, and visualized them. The vertical axis, labeled ‘Normalized Entity Token logits’, represents the normalized neural network output for each entity. The horizontal axis labeled ‘True’ represents the logits distribution of the Llama 2 for the correct QA samples, while ‘False’ represents the logits distribution for the incorrect samples of the QA. From the violin plot, it can be observed that the logits distribution for the ‘mother’ dataset is relatively concentrated, whereas the ‘capital of’ logits distribution may exhibit a bimodal nature, indicating two primary prediction categories. Meanwhile, we can observe that the ‘capital of’ dataset exhibits a distinct peak in entity frequency. This suggests that LLMs are more likely to encounter and learn from these high-frequency entities during the pre-training phase. In contrast, the ‘mother’ dataset might encompass more ambiguous or unspecified entities, making methods based on model-aware embedding challenging to distinguish on such datasets.\n\nBuilding on data-aware model’s configurations Mallen et al. (2023), we also incorporate two prominent retrieval systems into our research: BM25 Robertson and Zaragoza (2009) and Contriver Izacard et al. (2022). While BM25 is a static, term-based retriever that functions without prior training, Contriever undergoes pre-training on vast unlabeled corpora and is subsequently refined using the MS MARCO dataset Nguyen et al. (2016). We also delve into the GenRead Yu et al. (2023) parametric augmentation technique, prompting language models to produce contextual documents rather than exclusively depending on retrieval for responses. \n\n‘RA Llama 2’ connotes the integration of retrieval-based augmentation for all QA queries. ‘DM’ Mallen et al. (2023) stands for the adaptive data-aware method, while ‘MM’ signifies our model-aware method which does not need to access pretraining data. It is worth highlighting that our model-aware approach yields competitive results across various retrieval-augmentation configurations."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adaptability of the Model-aware Method in Fine-tuned LLMs (RQ2)",
            "text": "To evaluate our model-aware method’s adaptation performance, we fine-tune a GPT-Neo 1.3B model. Specifically, from the POPQA dataset, we extract 70% of the questions related to entities from each sub-dataset for the purpose of fine-tuning the GPT-Neo model, aiming to enhance the model’s retention of this specific knowledge. The remaining 30% are utilized as a test set to compare the effectiveness of DM and MM. For DM, entity frequencies were calculated using their original Wiki frequency.\n\nThis is because methods requiring fine-tuning, like the DM method, would cause a dramatic change in the entity frequency since there typically exists a notable difference between the entity frequencies of the pre-training data and the fine-tuning data. Therefore, DM would exhibit less stability compared to MM which does not directly rely on the frequency information, especially in the situations where multiple rounds of fine-tuning are required.\n\nGiven the prevailing trend of extensively fine-tuning large language models for specific downstream tasks, this stability and adaptability highlight a distinct advantage of our method."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a novel model-aware approach to tackle challenges in retrieval-augmented LLMs. Leveraging token embeddings that capture the model’s knowledge, we offer an efficient and privacy-conscious solution. Unlike methods dependent on inaccessible or sensitive pretraining data, our approach provides a flexible, scalable, and secure means to assess retrieval requirements. This innovation has broad implications for real-world applications, harmonizing efficiency and privacy while upholding model output quality."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This work focuses on an entity-centric adaptive retrieval-augmentation technique. It might not work on document-centric QA tasks. We acknowledge the need for future research to explore the extension of our method to a wider range of QA tasks. Besides, how to particularly improve the performance of the retrieval model is beyond the scope of our paper, and has yet to be explored."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "It is important to note that LLMs can still generate incorrect (hallucination) or biased outputs, even when they are retrieval-augmented. Therefore, it is always important to verify the outputs of language models with other sources of information."
        }
    ]
}