{
    "title": "Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models",
    "abstract": "Large language models (LLMs) often struggle with strict memory, latency, and power demands. To meet these demands, various forms of dynamic sparsity have been proposed that reduce compute on an input-by-input basis. These methods improve over static methods by exploiting the variance across individual inputs, which has steadily grown with the exponential increase in training data. Yet, the increasing depth within modern models, currently with hundreds of layers, has opened opportunities for dynamic layer sparsity, which skips the computation for entire layers.\n\nIn this work, we explore the practicality of layer sparsity and establish the relationship between model depth and layer sparsity. We propose Radial Networks, which perform token-level routing between layers guided by a trained router module. These networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights. They enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse. By varying the compute token by token, they reduce the overall resources needed for generating entire sequences. Overall, this leads to larger capacity networks with significantly lower compute and serving costs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language and vision models have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict memory, latency, and power demands. As these transformers grow larger, they create opportunities for dynamic layer sparsity, which can skip individual layers on an input-by-input basis, as shown in Figure 2. \n\nThis type of sparsity was impractical at smaller scales and with previous neural architectures. At smaller scales, every layer contributes significantly to the computation for each input, and with previous architectures, e.g., convolutional neural networks (CNNs), models change their intermediate dimensions throughout their depth and skipping causes dimensional mismatches. \n\nThis work shows that the layer contributions vary among models and tasks, and often the earlier layers of the network contribute more than the later layers. This indicates that early-exit methods, which dynamically prune the later layers in the network, often focus on the wrong set of layers. This dynamic contribution can be exploited at the token-level if it can be predicted accurately and efficiently at runtime.\n\nThis work explores the opportunities for dynamic sparsity within the modern transformers by focusing on the OPT family of models for language and ViT models for vision. It profiles the residual blocks to quantify the importance of each intermediate layer to its output and then highlights trends across model size and block types. Then, it inserts oracles at every layer to calculate various accuracy proxies and simulate greedy decisions on which layers to dynamically skip per token."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Sparsity research with deep neural networks has a long history, and broadly can be categorized in terms of granularity, structure, and mode (static vs. dynamic). Figure 3 shows sparsity granularity, beginning with bits that construct parameter elements, elements that build blocks, and blocks that form layers. As the unit becomes larger, it becomes more difficult to arbitrarily prune without accuracy loss yet easier to accelerate with modern hardware. For instance, unstructured element sparsity in weights leads to high compression levels while maintaining model accuracy, yet it requires specializing sparse accelerators to translate compression into end-to-end speedup. In addition, the sparsity mode can either be static or dynamic. Static sparsity leads to more regular patterns that can be optimized by compilers and simpler architecture that do not need additional sparsity predictors, yet it must apply to all inputs together. In contrast, dynamic sparsity can take advantage of input-dependent characteristics to increase model accuracy at higher levels of compression. This work focuses on dynamic layer sparsity, which can take advantage of the recent explosion in model depth within language models."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dynamic Sparsity",
            "text": "Multiple prior works have proposed dynamic sparsity to accelerate DNNs across granularities. For example, Channel Gating introduced a method for dynamic channel sparsity that reduced the compute of CNN workloads without significant accuracy loss [5]. Precision Gating continued this line of research by applying dynamic sparsity at the bit level to reduce the required compute [10]. Later, DejaVu applied a similar approach within LLMs to induce dynamic sparsity on the channels within the FFN layer and across the heads of the attention layer [6]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Early Exit",
            "text": "In addition to dynamic sparsity along the network width, multiple prior works have explored sparsity in the depth dimension. For instance, early-exit DNNs use dynamic sparsity along the depth dimension by allowing the computation to exit prematurely at fixed points within the network. This process must be trained end-to-end using a joint loss function that weights the contributions from each early-exit layer. However, this work shows that in many models, the earlier layers in the model often contribute more, and therefore early-exits are significantly more difficult to apply post-training."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Layer Sparsity",
            "text": "Transformer layers contain two residual blocks: attention (ATT) and feed-forward network (FFN). These blocks each contain the main residual branch, which comprises multiple individual layers, and the identity branch, which bypasses the residual branch and simply returns its input. They combine these branch outputs together, so that during training the main branch only has to learn the function residual. These blocks offer natural breakpoints within the model to profile and induce layer sparsity since they already provide skip-connections that have been trained along with the model.\n\nFigure 4 shows a lower-level view of these blocks within two transformer layers. It shows that the main branch and skip connections are combined at an addition node before they are passed to the next block. This structure enables easy profiling of the blocks by measuring the relative magnitudes into these additions. \n\nThis work focuses on the opportunities for dynamic layer sparsity and simulates layer skipping by allowing these oracles to have access to future information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Profiling",
            "text": "The primary proxy used by these oracles is the analysis of the layer sparsity within OPT and ViT models with examples taken from WikiText-2 and COCO. The WikiText-2 examples are packed together to avoid the use of padding to simulate batch-size one inference. This batch-size one setting is very common in practice and avoids many complications with dynamic layer sparsity that arise when using batches of examples."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Residual Ratio",
            "text": "To profile these opportunities for dynamic sparsity, the analysis focuses on the contribution of the residual branch and explores efficient post-training proxies for expensive metrics, such as empirical layer sensitivities. By examining the average contribution of the main branch at the output, it becomes evident that skipping blocks with small contributions should have little overall effect on the network's output."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Model Size",
            "text": "###figure_5### ###figure_6### ###figure_7### Each data point represents a single token during the model generation phase. In addition, opportunities for layer sparsity seem to expand with model size. For instance, the median value for OPT-125M is only 20%, and it drops to 5.9% for OPT-66B. This distribution seems to track the number of model parameters, not just the number of layers. For example, OPT-2.7B and OPT-6.7B have the same number of layers, differing only in their hidden dimensions, yet the trend continues for OPT-6.7B. This likely continues for even larger models, making dynamic layer sparsity more practical within modern state-of-the-art models with greater than one trillion parameters."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Depth",
            "text": "Dynamic layer sparsity leads to dynamic depth networks that adjust their depth based on their model inputs. \n\nEach data point represents an inference of a single token using a threshold, confirming a spread within the network depth, where most tokens only need between 40 and 70 blocks, instead of the full network at 80 blocks."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Routing Traces",
            "text": "For more detailed analysis, Figure 7 ###reference_### shows the routing for the OPT-13B model across a batch of WikiText-2 examples. This motivates the use of dynamic layer sparsity over early-exit models, since early exit can only skip later layers, which contribute the most to the network."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Vision",
            "text": "This analysis so far has focused on large language models, since they are currently larger than large vision models. Yet, recent vision transformers have been proposed with tens of billions of parameters [2]. These weights are not yet released, yet the trends between the smaller language and vision models can still be aligned at smaller scales to suggest the behavior of large vision models with billions or trillions of parameters. Figure 8 shows a comparison for the largest released ViT model, which contains 632M parameters across 24 layers."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Radial Networks",
            "text": "Given the high degree of dynamic layer sparsity, we propose a new neural architecture that natively supports arbitrary routing between layers. As shown in Fig 1, each token enters the network at step and then is routed dynamically to the next layer at later steps. This process allows for dynamic computation given the variable number of layers included in each token path. As with standard transformers, the network continues auto-regressively until an end-of-sequence token is produced."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Router",
            "text": "The router is the central component with the responsibility of directing each embedding between layers at each time step. Building on the success of mixture-of-expert models, we learn this router with a small multi-layer perceptron (MLP) model. Beginning from a layer, it maps from intermediate embeddings to output router logits for each layer. These logits are then passed into a softmax function to produce probabilities. The maximum probability layer is then chosen as the next layer in the forward pass. The iterations stop when the model chooses the output layer, or a set maximum number of layers are seen, which forces the output layer. This maximum number of layers is a hyper-parameter that limits the worst-case dynamic depth in the network."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Unified Cache",
            "text": "In standard sequential transformers, each layer activates for each token, and the attention mechanism references the key-value cache of previous tokens. Given the dynamic routing of radial networks, many of the layers are not activated for different tokens, resulting in a sparse key-value cache. To solve this, we use a shared global cache that stores all previous key-value pairs. Each embedding attends to the cached pairs of previous iterations and tokens. To distinguish between the cached values for the current and previous tokens, we use standard positional embeddings. These embeddings encode the relative position of tokens within the input sequence. A common approach is to use sinusoidal functions; for position and dimension, the positional embedding can be defined as follows, where is the dimensionality of the embeddings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In the past, dynamic layer sparsity has not been practical due to small model sizes and incompatible neural architectures, which caused large contributions from each layer and varying internal dimensions. For these reasons, it has only been possible with techniques like early-exit, which requires expensive specialized training. Yet, as language models grow in size, each layer contributes less to output, creating opportunities for dynamic layer sparsity.\n\nTo take advantage of this dynamic sparsity, we propose Radial Networks, which perform token-level routing between layers guided by a trained router module. These networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights. They enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse. By varying the compute token by token, they reduce the overall resources needed for generating entire sequences. Overall, this leads to larger capacity networks with significantly lower compute and serving costs."
        }
    ]
}