{
    "title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models",
    "abstract": "The success of vision-language foundation models has been significant, particularly in the context of enhancing model performance through the alignment of visual features with text supervision. Despite the progress, challenges remain in areas such as adaptation cost, text supervision quality, and natural generalization capacity. This paper introduces a framework that addresses these challenges by adapting input sequences with limited data. Specifically, we provide text supervision that is learned end-to-end from the examples. Further, a novel training objective is proposed to enhance the consistency of multi-modal features while encouraging differentiation in uni-modal features between examples. This framework facilitates the learning of text supervision, offering superior cross-modal alignment with minimal training data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Human cognition is immune to distribution variations, reflecting a fundamental difference between human and machine cognitive understanding. Humans primarily rely on semantic information from the context, while machines depend more on statistical distributional associations. Recent work introduces text supervision in adaptation through foundational vision language models, enhancing semantic understanding. Specifically, they adapt visual prompts by aligning visual features with static text supervision from models like CLIP.\n\nIn our work, we propose a Few-shot Prompt learning framework where pre-trained vision language models are adapted in a few-shot manner with prompt learning. This adapts the inputs rather than the parameters of the model. The method seeks to provide appropriate prompts for examples by learning correlated text supervision end-to-end. Moreover, we design a novel training objective that harmonizes the connection and distinction of features from information across different modalities, forcing the multi-modal features of inputs to be consistent while encouraging differentiation between uni-modal embeddings.\n\nCompared to existing methods, our method has several advantages. It significantly reduces the dependence on abundant data, and provides correlated text supervision learned end-to-end, improving the alignment between visual and textual embeddings. Our novel training objective fully leverages the dual-encoder architectural advantage of models like CLIP, enhancing cross-modal consistency between examples while encouraging uni-modal divergence.\n\nIn summary, our contributions are: \n1. We discuss important research problems and issues in previous prompt learning paradigms, potentially inspiring further improvements.\n2. We propose a novel few-shot prompt learning framework with learnable text supervision and an aware prompt learning objective. This method is lightweight yet makes significant generalization.\n3. We justify our claims through a series of experiments on 11 benchmark datasets covering multiple recognition tasks. The proposed method significantly outperforms state-of-the-art prompt learning methods in few-shot learning, zero-shot transfer, and base-to-new generalization settings. Comprehensive ablation studies and discussions are also provided."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": "CLIP recap. A pre-trained CLIP model typically includes an image encoder with learned parameters and a text encoder with learned parameters. Here we consider a -class classification problem for an image and its corresponding label. To perform zero-shot evaluation, the image is first divided into patches and converted into the patch embeddings. A class token is then appended to the patch sequence. Afterward, the image encoder processes this embedded patch sequence with ViT blocks to produce the latent image feature representation. For the text branch, we prepare hand-craft prompts by appending the class name to a word template, such as ‘a photo of a {class}’. Subsequently, the text is tokenized and embedded, where each corresponds to the -th class. The text encoder then encodes these word embeddings into the latent text feature representation. For zero-shot classification, the probability of the image in the -th class is determined by the cosine similarity score and a temperature parameter.\n\nCLIP-based prompt learning. Instead of adopting a hand-crafted prompt, prompt learning attempts to train lightweight learnable prompts with a few examples from downstream data. To be concrete, prompt learning is inserted into word embeddings. Then, the text feature representation is obtained. To preserve the alignment characteristics of the joint image-text feature space for zero-shot capabilities, CLIP-based prompt learning optimizes the prompt tokens by narrowing the gap in the distribution between text-image logits and the ground-truth label using cross-entropy:\nwhere text-image logits are used. We suggest readers check Zhou et al. for more details about CLIP-based prompt learning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Overview. To address the limitations of previous methods, we propose FAP, a few-shot prompt learning framework. Our framework uses lightweight learnable prompts on the top of the pre-trained CLIP in a few-shot manner, as the case in natural prompt tuning (Zhou et al., 2022b). In more detail, we introduce learnable prompt tokens, which allow the model to provide more appropriate text supervision that helps balance generalization. Based on CLIP’s dual-encoder architecture, we further provide a novel training objective that guides the discrimination of embeddings in uni-modal feature space. This promotes uni-modal divergence to incorporate a mechanism that facilitates the learning of text supervision. The overview of the proposed framework is provided in Figure 1. Below, we discuss the FAP framework step by step."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Learnable Text Supervision for Adversarial Examples",
            "text": "When adapting the CLIP model, a slight change in wording could have a huge impact on performance (Zhou et al., 2022b  ###reference_b81###). The distribution differences between natural examples necessitate the design of specialized text supervision. Therefore, we introduce text prompt tokens that are end-to-end learned through these examples. \n\nFormally, our prompt learning is implemented on a few-shot subset, created by sampling examples from each of the classes in the original dataset. Learnable prompts consist of both visual and text branches, denoted as . The visual prompt token is incorporated into the image embedding, while text prompt token is inserted into word embedding, as is the case in natural prompt learning. To preserve mutual synergy between visual and text branches, is obtained from through linear projection, which can be denoted as .\n\nDiscussion. The proposed framework can be categorized as a cross-modal prompt, drawing inspiration from natural multi-modal prompt learning framework (Khattak et al., 2023a  ###reference_b26###). However, we identify objective differences under task settings. Specifically, Khattak et al. (2023a  ###reference_b26###) employs deep prompt interaction in a text-to-image context, as hand-crafted prompts already provide suitable initial text supervision for clean examples. Additionally, the text-to-image projection acts as a dimensionality-increasing projection, preventing information loss. In this context, the learning of text prompts relies on features derived from the visual branch. We offer a comprehensive analysis of deep interaction settings in Appendix D.1 ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Balancing Natural and Adversarial Generalization in Few-Shot Adversarial Prompt",
            "text": "The effectiveness of the CLIP model in adapting to different tasks is contingent on the similarity between the downstream task’s distribution and the pre-trained representations. Specifically, if the downstream task aligns closely with the pre-trained representation, the CLIP model demonstrates favorable natural generalization. In such scenarios, employing additional learnable prompts can be beneficial. Conversely, a significant disparity between the distribution of the downstream task and the pre-trained representation poses a challenge. In such cases, the CLIP model inherently lacks natural generalization. Consequently, expecting the prompt tokens to concurrently learn both natural and robust generalization from a limited set of examples becomes an overly demanding task. \n\nA balanced objective is crucial for downstream generalization, as this approach alleviates potential failure caused by discrepancies in natural generalization."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Uni-Modal Adversarial-Aware Mechanism",
            "text": "To fully leverage the structural advantages of CLIP, we enforce consistency constraints on cross-modal text-image features. Specifically, we introduce a mechanism for visual features. To the best of our knowledge, this is the first initiative to foster differentiated representations in regularization.\n\nWithin CLIP, we maintain consistency in the text-image joint space while preserving the distributional differences of features in the uni-modal visual space to minimize the impact on generalization performance. Here, we append an extra constraint with cosine similarity:\nwhere the constant maintains the non-negativity. We introduce the mechanism by adjusting prompt tokens to minimize similarity, thereby distinctly differentiating between visual features. During the training process, the text branch learns to provide proper text supervision for different visual features, ensuring that the outputs in the text-image joint space are consistent, which have significant distributional differences in the visual space."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Overall Learning Objective",
            "text": "Objective for outer minimization. The overall training objective can be obtained by introducing uni-modal mechanisms to Eq. (5 ###reference_###) as:\n\nObjective for inner maximization. The goal of inner maximization is to generate the example. Here, we leverage the term in Eq. (5 ###reference_###) as this surrogate loss and find the example as follows:\n\nHere, the general formulation with the loss like Eq. (3 ###reference_###) is also applicable. With the learning objective outlined in Eq. (7 ###reference_###), we adapt learnable prompt tokens on the few-shot dataset as:\n\nFor better understanding, we describe our prompt learning and prompt testing pipeline in Appendix A ###reference_###. Additionally, we demonstrate the significant gains our learning objective brings to other prompt designs through a case study in Appendix D.2 ###reference_###. Different training objective designs and their experimental results can be found in Appendix D.3 ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setups",
            "text": "Baselines. To demonstrate the expertise of the proposed method, we employ multiple commonly used prompt learning designs for comparison. Specifically, we compare our FAP with the following baselines that can be divided into two groups:\n(1) Methods with hand-craft text supervision, which include zero-shot CLIP (Radford et al., 2021  ###reference_b50###) and visual prompt method (Mao et al., 2022  ###reference_b46###).\n(2) Methods with learnable text supervision, such as vision language prompts and multi-modal prompts (Khattak et al., 2023a  ###reference_b26###). Note that we primarily focus on methods that add a learnable prompt on top of the scheme, and the effects of pure text prompts (Zhou et al., 2022b  ###reference_b81###) are attached in Appendix D.9  ###reference_###.\nThe technical details about these baselines are provided in Appendix C.1  ###reference_###, and the static prompt templates for each dataset can be found in Appendix C.2  ###reference_###.\nDatasets. To evaluate the proposed method, we align with previous works (Zhou et al., 2022b  ###reference_b81###, a  ###reference_b80###) and utilize 11 diverse image recognition datasets that span multiple vision tasks. Specifically, the datasets include two generic object datasets: ImageNet (Deng et al., 2009  ###reference_b10###) and Caltech101 (Fei-Fei et al., 2004  ###reference_b16###); a texture recognition dataset: DTD (Cimpoi et al., 2014  ###reference_b8###); five fine-grained object recognition datasets: FGVCAircraft (Maji et al., 2013  ###reference_b45###), OxfordPets (Parkhi et al., 2012  ###reference_b49###), Flowers102 (Nilsback & Zisserman, 2008  ###reference_b47###), Food101 (Bossard et al., 2014  ###reference_b3###), and StanfordCars (Krause et al., 2013  ###reference_b30###); a scene recognition dataset: SUN397 (Xiao et al., 2010  ###reference_b61###); an action recognition dataset: UCF101 (Soomro et al., 2012  ###reference_b52###); and a satellite image classification dataset: EuroSAT (Helber et al., 2019  ###reference_b22###).\nImplementation details. We conduct experiments on the ViT-B/32 CLIP architecture and report the average results over three random seeds. All models are trained for 5 epochs in cross-dataset evaluation and 10 epochs for other benchmark settings by using an SGD optimizer with a momentum of 0.9. The initial learning rate is set at 0.0035. We apply a cosine learning rate scheduler and a warm-up strategy during the first epoch. Note that we explore alternative CLIP architectures in Appendix D.4  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "In this scenario, we evaluate the model’s ability to develop representations with a severely limited amount of downstream data. Specifically, we tune the model using {1, 2, 4, 8, 16} shots from each class. \n\nFor the cross-dataset evaluation, models are adapted on the ImageNet dataset using 16 shots and then assessed for their zero-shot accuracy across 10 distinct datasets, without further downstream tuning.\n\nWe present a base-to-new generalization setting, where datasets are bifurcated into base and new subclasses. Here, models are trained with a 16-shot dataset from the base classes and are subsequently evaluated on both base and new classes. In this setting, as the number of categories in datasets is generally much smaller than the number of examples per class, models need to learn both intrinsic features within each dataset and representations from limited examples to achieve effective generalization on large amounts of test data. \n\nFrom Table 1, we observe that our method reveals superior generalization due to the joint consideration of features in our training objective. Additionally, our method also reveals much better stability (lower standard deviation). That is, even sampled few-shot subset has a generalization gap, our learning objective still works well and prevents potential failure."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "More Analysis",
            "text": "Matching benchmark result adapted with ImageNet-1K. \nBeyond comparison with the baseline AdvVP under the few-shot setting, we also present its benchmark result which is adapted on the entire ImageNet with a larger prompt size (token prompt of size 200 and pad prompter of size 40). In Table 3, our method matches the benchmark result with 1.25% examples from ImageNet, thus speeding up the training process by a large margin (over 97%). Furthermore, by increasing training examples from 16-shot to 32-shot and prompt depth from 9 to 12, the proposed method surpasses the previous state-of-the-art adversarial prompt tuning.\n\nPrompt depth and prompt length. \nWe provide architectural ablation results for prompt design concerning different prompt depth and length settings. In Table 5, we can observe that increasing both prompt depth and prompt length introduces more learnable parameters, thereby resulting in improved performance. Furthermore, we can also conclude that the performance gain obtained by increasing prompt depth is higher than that achieved by increasing prompt length.\n\nNatural generalization gap hinders robust adapting. \nWe identify a failure risk in few-shot adversarial prompt learning using TeCoA loss, where insufficient natural generalization on the sampled dataset impedes robustness learning. Figure 3 (left) displays the loss variation during training under this setup. Under the same experimental setup using the TeCoA loss, different trials exhibit completely different trends: the curve for the failure case shows that the loss quickly ceases to decline and becomes stable shortly after training begins, whereas the loss in the normal case continues to decrease as the training progresses. We presume that this failure stems from a lack of natural generalization ability. To confirm this, we first conduct natural tuning on the problematic few-shot dataset and then apply adversarial prompt learning. This restores the model’s robust fine-tuning performance, as evident in Figure 3 (right), where natural and robust accuracies improve significantly after natural example adaptation. Besides, we validate the learning process on the same few-shot dataset with a dual-form loss in the training objective that considers both natural and adversarial terms. It is revealed that this two-term loss effectively acts as a surrogate for the aforementioned two-stage method, avoiding potential failures caused by the natural generalization barrier in end-to-end training."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we focus on adversarial prompt tuning on vision-language models, a domain with significant potential for zero-shot downstream tasks. We precisely reveal the issues of previous methods that perform adversarial visual prompts with static text supervision. Our method distinguishes itself by introducing learnable text supervision combined with a new training objective, facilitating effective learning in a few-shot setting. The proposed method enjoys excellent algorithmic properties and matches state-of-the-art performance, notably with reduced computational demand. We believe that this work can provide some insights to the community and stimulate further research in this area."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact Statement",
            "text": "This research aims to contribute positively to the machine learning field by enhancing model robustness against adversarial attacks. While we believe our work is unlikely to have direct negative societal impacts, we acknowledge the importance of considering potential misuse scenarios, such as in the context of security applications. The broader implication of our study is that it enables neural models to maintain adversarial robustness with minimal adaptations, making it particularly suitable for real-time applications in mobile and embodied systems. Such advancements could lead to more secure and reliable applications in various real-world scenarios, including mobile device security."
        }
    ]
}