{
    "title": "What Makes Math Word Problems Challenging for LLMs?",
    "abstract": "This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs. Our code, data, and analysis are publicly available at github.com/kvadityasrivatsa/analyzing-llms-for-mwps",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLMs) have demonstrated huge potential across a range of core NLP tasks and exhibited a number of emergent abilities, such as an ability to solve mathematical puzzles. Math word problems (MWPs) have been proposed as a challenging testbed for LLMs, as they test not only the ability of the models to deal with purely mathematical expressions but also their reasoning and natural language understanding abilities. Experiments show that even quite powerful LLMs are still challenged by MWPs. Most previous work has either focused on evaluation of LLMs’ performance on MWPs or on changes in their behavior in response to progressive-hint prompting, prompt paraphrasing, or similar approaches, while an in-depth analysis of what exactly makes math problems challenging for LLMs is lacking. We aim to address this knowledge gap.\n\nA recent study demonstrates a strong connection between reading skills and math outcomes in students. We hypothesize that LLMs’ ability to solve MWPs correctly may similarly rely on: (1) the linguistic complexity of the questions; (2) the conceptual complexity of the tasks (e.g., the number of steps and types of math operations involved); and (3) the amount of real-world knowledge required to solve the tasks. Supporting this intuition, our preliminary analysis suggests that relatively short questions with a small number of described entities, a few calculation steps, and a limited range of operators involved in the solution are typically addressed effectively by a range of LLMs. At the same time, long questions requiring real-world knowledge and extended natural language understanding pose challenges for LLMs.\n\nIn this paper, we formulate and investigate two research questions: (1) Which characteristics of the input math word questions make them complex for an LLM? and (2) Based on these characteristics, can we predict how a particular LLM will engage with specific input MWPs?"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We use the GSM8K dataset Cobbe et al. (2021 ###reference_b4###), divided into training and test instances, because of the high quality of human-generated MWPs. This dataset contains a diverse set of problems in English with minimal amount of recurring templates. Furthermore, the difficulty level of the problems is tailored for LLMs, allowing for a wide variation in correctness across models and question types, which is ideal for our feature-based analysis.\n\nWe collect solution attempts from several LLMs to the questions from the GSM8K training and test sets. Our approach allows us to investigate which of the features are most indicative of the challenges LLMs face in solving math problems.\n\nWe select an array of open-source models for our experiments. We use Llama2 (13B and 70B) Touvron et al. (2023 ###reference_b14###), Mistral-7B Jiang et al. (2023 ###reference_b6###), and MetaMath-13B Yu et al. (2023 ###reference_b17###).\n\nWe analyze and experiment with the features extracted from MWP questions and their respective expected solutions. This way, the features remain grounded in the dataset, allowing our approach to be applied to any LLM. The features are broadly grouped into the following categories:\n\nLinguistic features focus on the phrasing of the question. These include the length of the question, sophistication of the vocabulary, syntactic complexity, instances of coreference, and overall readability. Note that the linguistic features are only extracted from the question body as the phrasing of the gold solution has no impact on the expected answer.\n\nMathematical features cover the math arguments, operations, and reasoning steps required to solve the questions. These include the number and diversity of the math operations in the solution body. Arguments provided in the question but not utilized in the solution also require mathematical reasoning for them to be disregarded as noise. Note that while a question can be phrased in many ways (affecting its linguistic features), the underlying math operations and reasoning steps (thus, the mathematical features) remain unchanged.\n\nReal-world knowledge & NLU based features indicate the amount of extraneous information needed to solve the task that is not provided explicitly in the question. This may include how many days there are in a month or the interpretation of “half” as."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease. We employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C for more details."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Solution Generation",
            "text": "To collect solution attempts from the LLMs, we use a simple task-specific prompt (See Appendix B) to minimize any bias imposed on the model generation. We query each LLM times on each question with varying generation seeds and a temperature of . A soft-matching strategy is then used to extract the final answer from the solutions."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Success Rate Prediction",
            "text": "We train and evaluate classifiers on their ability to predict for input test questions whether they will be answered correctly or incorrectly by a specific LLM. We also train and evaluate classifiers on the intersection set of questions, which are either solved correctly by all or by none of the LLMs. We use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease.\n\nWe employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C for more details."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Success Rate Distribution",
            "text": "We observe that Llama2 13B and 70B follow the expected order of scores along their respective parameter counts. Mistral-7B scores similar to the 13B Llama2 model, and the additional fine-tuning allows MetaMath-13B to surpass the other models (including the 70B Llama2). Figures 2(a) and 2(b) respectively capture the number of questions always and never answered correctly by each LLM. Overall, MetaMath-13B has the lowest number of incorrectly and the highest number of correctly answered questions across the tested LLMs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Classification Results",
            "text": "To compare classifiers’ performance, we report the accuracy and macro-F1 scores for each classifier and LLM-specific test data split (see Table 2). We observe that Random Forest outperforms other classifiers across most solution sets.\n\nAt the same time, we also note that, due to significant class imbalance, this task is not easy for the classifiers, with the best accuracy scores across LLM splits being in the range of . The small number of questions always or never solved correctly by any LLM speaks to the models’ varying capabilities (and potential points of brittleness). We include additional analysis of the results in Appendix D.\n\nFor comparison, we also report the classification results for a fine-tuned RoBERTa-base model Liu et al. (2019) for the same training and evaluation sets (tuned on the question and gold solution as input text; see Appendix C for more details) in Table 2. We note that the Transformer base classifier scores on a par or a few points above the best statistical classifier, i.e., Random Forest, suggesting that the proposed feature-based classifiers are not far behind token-level contextual models for this task."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Feature Importance",
            "text": "The statistical classifiers used in our experiments allow us to estimate the importance of each feature and its contribution to the classification performance. We report the top features with the highest aggregate ranks across LLM data splits and classifiers in Table 3. We use mean rank here as a proxy for relative importance across features, and the respective standard deviations indicate how spread out this importance is across classifiers and queried LLMs. We observe that a greater number (Gx_op_unique_count) and diversity (Gx_op_diversity) in math operations, and the use of infrequent numerical tokens in the question and solution body (Qx_ & Gx_mean_numerical_word_rank) are important. The list also contains linguistic features based on the phrasing of the questions: longer questions with a high number of noun phrases (Qx_np_count), mean syntactic depth (Qx_constituency_tree_depth), and readability grade (Qx_flesch_kinkaid_grade) are also difficult for LLMs to solve. Additionally, the need for extraneous information (Gx_world_knowledge), such as conversion units for time, distance, or weight, can make a question challenging."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Ablation Studies",
            "text": "To further measure the impact of each feature type, we report classification scores along different feature-type subsets in Figure 3. We note that the feature set with all types (L+M+W) is not optimal for classification. For instance, the questions answered by Llama2-13B are best classified using only mathematical features (M). The best-performing classifiers for Llama2-7B, MetaMath-13B, and the intersection set either solely use linguistic features (L) or both linguistic and math features (L+M), whereas the world knowledge & NLU feature set if sufficient for Mistral-7B.\n"
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Impact of Linguistic Features",
            "text": "In order to better gauge the impact of linguistic features, we cluster questions by mathematical features. We fit a KMeans clustering model on all math features for each question in the GSM8K training set with a target cluster count. This helps group together questions from the data, wherein the math features hardly vary within each question subset (or cluster). Thus, variations across the questions within a cluster can be more clearly attributed to other, i.e., linguistic types of features. The strong and significant feature-wise negative correlations suggest that for a relatively fixed set of math features, questions with greater length, nesting, lexical rank, and reading grade become more challenging for LLMs to solve. Note that this form of analysis on feature-based minimal pairs is extractive in nature and may, to a certain extent, be restricted to the question types in the GSM8K dataset. For a more exhaustive analysis for each feature, generative approaches to furnish question paraphrases with the desired set of linguistic features need to be employed."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This work aims to identify what aspects of MWPs make them difficult for LLMs to solve. To this end, we extract key features (spanning linguistic, mathematical, and real-world knowledge & NLU-based aspects) to predict whether several LLMs can reliably solve MWPs from GSM8K. We find that questions with a high number and diversity of math operations using infrequent numerical tokens are particularly challenging to solve. In addition, we show that lengthy questions with low readability scores and those requiring real-world knowledge are also seldom solved correctly. Our future work will rely on these findings to make informed modifications to questions in order to study the impact on LLMs’ reasoning and MWP-solving abilities. Figure 4 provides an example of an informed modification, which leads to improved LLM performance."
        }
    ]
}