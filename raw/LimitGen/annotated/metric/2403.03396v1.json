{
    "title": "Japanese-English Sentence Translation Exercises Dataset for Automatic Grading",
    "abstract": "This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning. We formalize the task as grading student responses for each rubric criterion pre-specified by the educators. We then create a dataset for STE between Japanese and English including 21 questions, along with a total of student responses (on average). The answer responses were collected from students and crowd workers. Using this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning. Experimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90%, but only less than 80% for incorrect responses. Furthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Sentence translation exercises (STEs) are often utilized as educational tools in the early stages of L2 language learning, particularly between language pairs that are linguistically distant from each other Cook (2010  ###reference_b5###); Butzkamm and Caldwell (2009  ###reference_b3###). Figure 1  ###reference_### shows an example of STE. Here, a learner translates a short sentence in their native language (L1) into the language they are learning (L2), and these translations are graded following analytic criteria within the grading rubric such as E3 and G4, which correspond to specific grammar items or expressions. ###figure_1### This format facilitates the recognition of similarities and differences between the native language and the target language, which is especially effective in helping learners acquire basic grammar and expressions in the early stages of their language learning, thus enhancing their understanding of the desired modes of expression Cook (2010  ###reference_b5###). The questions in these exercises are brief and repeatable tests that efficiently help learners practice specific grammatical items, basic vocabulary, and idioms at a certain proficiency level and learn the nuances of expression between L1 and L2. Teachers can also use these exercises as assessment tools to evaluate whether learners have mastered specific grammar items or a vocabulary level. However, because the responses to these exercises are descriptive, they pose a significant burden on educators in the form of manual grading and feedback. Such a limitation restricts the frequency of these exercises despite the importance of repetitive training in language acquisition  Larsen-Freeman (2012  ###reference_b21###). Therefore, automating the correction and feedback for translation exercises has the potential to significantly transform the educational environment in language learning. Therefore, we aim to automate the grading of L1-to-L2 STEs. Tasks that are closely associated with this challenge include Grammatical Error Correction (GEC), which evaluates the grammatical correctness of written sentences, and machine translation. STEs, however, are substantially different from these tasks in that they are usually operationalized with explicit learning objectives and closely reflect educators’ intentions (§2.1  ###reference_###). STEs not only clarify the learning objectives of a particular question but also allow for a more detailed learning analysis based on the performance of each evaluation item. The motivation for incorporating educators’ intentions is also supported by studies that have found that the sole use of the GEC system does not elicit effective learner engagement Koltovskaia (2020  ###reference_b18###); Ranalli (2021  ###reference_b25###). To achieve our goal, we perform three tasks: (1) question formulation, (2) dataset creation, and (3) evaluation of baseline systems for our task. To the best of our knowledge, this is the first attempt at an automated STE grading for educational purposes. Therefore, we first formulate the question. An important aspect of this formulation is to ensure that the established framework reflects the educators’ evaluation criteria. Consequently, we formulate our task as a classification of scores on each evaluation item according to the predefined rubrics. We then develop the dataset for this task. The questions and the rubric were created by English education experts, and answer scripts were collected from secondary education classrooms and through crowdsourcing. Finally, we evaluate the performance of the conventional automated scoring model typically used for short answer scoring (SAS), as well as the latest generative language models with few-shot learning. Experimental results showed that the baseline model using finetuned BERT successfully classified approximately 90% of correct responses, but only less than 80% of incorrect responses. Furthermore, GPT models with few-shot learning showed poorer results than the BERT model, indicating that even with a state-of-the-art LLM, our proposed new task remains difficult and challenging. Error analysis of the few-shot models revealed their lack of comprehension regarding the grading task. The contributions of this study are the following: We formulate the automated grading of sentence translation exercises as a new task, referencing the actual operation of STEs in educational settings. We construct a dataset for the automated STE grading in accordance with this task design, which includes a total of 21 questions and responses, and demonstrate the feasibility of the task. We establish baseline performances for the task, showing potential for advancement."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Automatic scoring of sentence translation exercises",
            "text": "For a given STE, let  denote the set of analytic criteria. For the input response text , the model outputs an analytic score  for a given analytic criterion , where 2, 1, and 0 represent “correct,” “partially correct,” and “incorrect,” respectively."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sentence translation exercises",
            "text": "Sentence translation exercises (STEs) are a language learning tool where a learner translates a sentence in L1 into a target L2. Studies have shown that the use of L1 in L2 education promotes an understanding of differences and similarities between the two languages Butzkamm and Caldwell (2009); Cook (2010), reduces incomprehension, and enhances learning focus Scott and De la Fuente (2008). Language translation has also been effective in improving students’ four skills (speaking, writing, reading, listening) and promoting learning and communication skills Yasar Yuzlu and Dikilitas (2022). Because of these benefits, STEs are widely used in educational settings, particularly among beginners in language learning.\n\nFigure 1 shows an overview of an STE. A learner’s translated response is assessed using a grading rubric meticulously designed by educators to evaluate the learner’s L2 ability, such as vocabulary and grammatical understanding. Such a rubric contains multiple analytic criteria aligned with the specific pedagogical objectives that an educator intends to assess in the question. This aspect characterizes STE evaluation and distinguishes them from typical GEC tasks, which assess the overall correctness of the grammar. Evaluation based on the analytic scoring criteria highlights the degree to which the learning objectives are achieved. \n\nTo this end, some degree of constraint is imposed on the question design and answer choices, limiting the freedom of translation. However, if translation variations are observed, all possible expressions are accounted for. These restrictions in translation practice, as discussed in Cook (2010), prevent learners from easily avoiding knowledge gaps and direct their attention to L2 aspects that they may find challenging. Therefore, these constraints can be useful in focusing students’ attention on specific language abilities. In addition, the evaluation of translated sentences in educational settings is also different from that of general translations in that the former involves pedagogical objectives such as the acquisition of specific language knowledge."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Task formulation",
            "text": "The purpose of assessing the STE task is to determine how well students’ responses achieve the learning objectives defined by the instructors. To effectively do so, instructors use a carefully constructed scoring rubric. Each STE question targets several learning objectives and evaluates other fundamental grammatical items (e.g., number, tense, etc.); therefore, a scoring rubric contains multiple independent analytical criteria to evaluate specific items. These criteria serve as the basis for grading each student’s response, with a corresponding analytic score assigned to each grading item (see Table 1). The automatic scoring of analytic criteria was formulated by Mizumoto et al. (2019) as an analytic score prediction task for reading comprehension questions. Therefore, this study also considers the analytic score prediction for the automatic scoring of STE. For a given STE, let denote the set of analytic criteria. For the input response text, the model outputs an analytic score for a given analytic criterion, where 2, 1, and 0 represent “correct,” “partially correct,” and “incorrect,” respectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Sentence translation exercise (STEs) dataset",
            "text": "To implement the automatic STE scoring, we introduce an STE dataset. This dataset currently comprises 21 Japanese-to-English STE questions with detailed rubrics and annotated student responses. These questions and the scoring rubrics were created by specialists in the design of English learning materials. The questions were constructed to cover all the major grammar topics in several well-known English textbooks used in Japanese high schools. Table 1 shows an example of a rubric, which contains 17 analytic criteria: three for grammar (labeled as “G”), seven for vocabulary and expression (labeled as “E”), and seven for word order (labeled as “O”). Each analytic criterion is evaluated on a three-point scale: 2 (correct), 1 (partially correct), and 0 (incorrect); the rubric lists the typical expressions for each scale. Essentially, STEs are designed such that they limit variations in correct responses from the outset. In practical settings, however, educators may adjust the grading rubric by incorporating variations in correct responses, previously unidentified during the rubric’s initial creation, to accurately evaluate the student responses. To replicate this process, we initially create the analytic criteria, followed by the collection of student responses as described in the following subsection. Subsequently, we refine the rubric by reviewing the collected responses, to preempt any challenges that may arise during the grading procedure. In the following sections, we will discuss in detail the methods used to gather responses, as well as the annotation process, and statistically analyze the whole dataset. Mizumoto et al. (2019) also annotated specific substrings within responses that contribute to an analytic score. These substrings are called justification cues because they serve as the rationale for the analytic scores. We also annotated justification cues in our dataset to enhance the interpretability of analytic scores. For example, in Figure 1, the phrase “before I saw” was annotated as a justification cue and was assigned an analytic score of “.” We then used the F-score to calculate agreement for justification cues. Regarding agreement for justification cues, the F-score was 0.92, signifying a high level of agreement among the annotators. This suggests that different annotators can consistently identify the same phrase as a justification cue for an analytic score. Table 2 shows the dataset statistics. We annotated a total of 3,498 responses for 21 questions, including 196 analytic criteria. For the pilot question, ranging from Q1 to Q7, scoring included 1 (partially correct) whereas the other questions followed a binary scoring of 2 (correct) and 0 (incorrect). Additionally, the number of instances with a grade of 0 was relatively fewer than those with a grade of 2. This distribution was similar to the one observed in the pilot question and others. Therefore, we conclude that we have successfully gathered crowdsourcing workers whose English ability is equivalent to that of original high school students and that these workers have attempted to answer those questions correctly."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Collecting student responses",
            "text": "Ideally, student responses are compiled within classrooms and other practical learning environments. However, the number of responses that can be collected from actual classrooms is often limited, and the collecting process is time-consuming. Therefore, we constructed our dataset through a combined approach involving high school students and crowdsourcing workers to collect responses for response collection. In this approach, we conducted a pilot data collection in which responses were obtained from high school students. Then, we analyzed these responses with English education experts and created the criteria for gathering crowdsourcing workers whose English abilities are equivalent to those of the high school students (see Appendix A for details regarding the recruitment criteria). Finally, we hired workers who met the criteria and allowed them to answer the questions, thus collecting a sufficient amount of responses. To maintain quality, we manually reviewed the collected responses and excluded those that significantly deviated from the expected responses. As a result, we obtained an average of 167 responses per question. The following section will present the statistics of the dataset."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Annotation:",
            "text": "As explained in Section 2.2, the scoring task for STEs involves grading on a three-class classification for each analytic criterion. Annotators are also asked to identify the specific phrase of the response that serves as a grading clue (referred to as justification cues). We annotated both types of information in each response. We hired professional graders to annotate those responses. As demonstrated in Figure 1, the annotators assigned an analytic score to the responses based on each analytic criterion. Mizumoto et al. (2019) also annotated specific substrings within responses that contribute to an analytic score. These substrings are called justification cues because they serve as the rationale for the analytic scores. We also annotated justification cues in our dataset to enhance the interpretability of analytic scores. For example, in Figure 1, the phrase “before I saw” was annotated as a justification cue and was assigned an analytic score of “.” To measure the quality of the annotations, we randomly selected 10 out of the 21 questions and asked a different annotator to annotate 20 responses for each question. Regarding agreement for justification cues, the F-score was 0.92, signifying a high level of agreement among the annotators. This suggests that different annotators can consistently identify the same phrase as a justification cue for an analytic score. Table 2 shows the dataset statistics. We annotated a total of 3,498 responses for 21 questions, including 196 analytic criteria. For the pilot question, ranging from Q1 to Q7, scoring included 1 (partially correct) whereas the other questions followed a binary scoring of 2 (correct) and 0 (incorrect). Additionally, the number of instances with a grade of 0 was relatively fewer than those with a grade of 2. This distribution was similar to the one observed in the pilot question and others. Therefore, we conclude that we have successfully gathered crowdsourcing workers whose English ability is equivalent to that of original high school students and that these workers have attempted to answer those questions correctly."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We employ a BERT Devlin et al. (2019  ###reference_b7###)-based classification model and the GPT models OpenAI (2023  ###reference_b24###) with in-context learning as a baseline for our task formulation.\nThis section discusses these baseline models in detail.\nFirst, the response text sequence , with a prepended CLS token, is input into BERT, obtaining the intermediate representation  as follows:\nIn our task, a justification cue that indicates the rationale behind its score is provided for each response.\nBy utilizing this justification cue to train a model, we expect that the model will grade faithfully according to the rubric.\nTherefore, following  Mizumoto et al. (2019  ###reference_b22###), we use these justification cues as supervisory signals to train the model’s attention layer.\nHere, we perform pooling on the BERT-encoded representations using a Bi-LSTM and attention mechanism.\nThe sequence obtained from  by excluding  is input into the Bi-LSTM, yielding .\nThen we calculate the weighted sum as follows:\nwhere  is the weight of the -th word relative to the scoring rubric , calculated by the attention mechanism shown in Equation (4.1  ###reference_###).\nwhere  and  are learnable parameters. Finally, the evaluation value  for item  is obtained by the following formula:\nwhere  and  are the learnable parameters.\nThe analytic scoring model is trained to minimize the negative log-likelihood  for each analytic score.\nwhere  is the label (evaluation value) of the ground truth for scoring rubric .\nIn addition, as discussed in the Section 3.2  ###reference_###, the dataset contains the justification cues  for each analytic criterion for the response, where\n is the indicator of whether the -th token in the response is the justification cue for the score of the analytic criterion .\nWhen the gold justification cue includes  tokens, the sum of  is . Therefore, as a gold signal for , we use  divided by  during the training process.\nFollowing Mizumoto et al. (2019  ###reference_b22###), we use the MSE-based loss function to achieve supervised training of the attentions with justification cues.\nThus, the overall loss  is expressed as:"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Finetuned BERT model",
            "text": "We employ BERT, which is widely used in various NLP tasks, including SAS, as a baseline for this task.\nThis model is finetuened for each scoring item in the rubric using the training data.\nFirst, the response text sequence , with a prepended CLS token, is input into BERT, obtaining the intermediate representation  as follows:\nIn our task, a justification cue that indicates the rationale behind its score is provided for each response.\nBy utilizing this justification cue to train a model, we expect that the model will grade faithfully according to the rubric.\nTherefore, following  Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###), we use these justification cues as supervisory signals to train the model’s attention layer.\nHere, we perform pooling on the BERT-encoded representations using a Bi-LSTM and attention mechanism.\nThe sequence obtained from  by excluding  is input into the Bi-LSTM, yielding .\nThen we calculate the weighted sum as follows:\nwhere  is the weight of the -th word relative to the scoring rubric , calculated by the attention mechanism shown in Equation (4.1  ###reference_###  ###reference_###).\nwhere  and  are learnable parameters. Finally, the evaluation value  for item  is obtained by the following formula:\nwhere  and  are the learnable parameters.\nThe analytic scoring model is trained to minimize the negative log-likelihood  for each analytic score.\nwhere  is the label (evaluation value) of the ground truth for scoring rubric .\nIn addition, as discussed in the Section 3.2  ###reference_###  ###reference_###, the dataset contains the justification cues  for each analytic criterion for the response, where\n is the indicator of whether the -th token in the response is the justification cue for the score of the analytic criterion .\nWhen the gold justification cue includes  tokens, the sum of  is . Therefore, as a gold signal for , we use  divided by  during the training process.\nFollowing Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###), we use the MSE-based loss function to achieve supervised training of the attentions with justification cues.\nThus, the overall loss  is expressed as:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "GPT models with in-context learning",
            "text": "We evaluate the GPT-3.5 and GPT-4 models in the setting of few-shot in-context learning Brown et al. (2020  ###reference_b1###), which significantly minimizes the cost of building a scoring model specific to each grading item as well as the training examples required for finetuning.\nFurthermore, the GPT series demonstrates superior performance in tasks such as translation and summarization, among other tasks Gladkoff et al. (2023  ###reference_b13###); Helwan et al. (2023  ###reference_b15###).\nTherefore, we can expect the proficiency in grammatical knowledge required for automatic grading of STEs.\n###figure_2### Figure 2  ###reference_### shows the input template for the GPT models.\nThe input can be segmented into two parts.\nThe first part is a prompt that includes a task instruction, a description of the output format, an L1 sentence for translation, a focused single analytic criterion, and the scoring examples corresponding to that criterion.\nThe analytic criterion is a (literal) textual representation of a rubric item described in a single row in Table 1  ###reference_###.\nFor each score label, we provide a few-shot examples to illustrate the analytic criterion and its scoring (output examples) for in-context learning.\nThe second part is a student response.\nThe model leverages these two inputs to generate a score label for the specified criterion and identify the substring of the student response that justifies the evaluation.\nIn the GPT models, we treat the grading of each analytic criterion within a prompt as an independent grading task, thus the GPT models output a score for each analytic criterion independently.\nMore details of the input prompt can be found in Table 5  ###reference_### in the appendix."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In the experiment, we investigate the feasibility of our task formulation for STEs using the BERT model and the state-of-the-art large language models, GPT-4 and GPT-3.5.\nWe also investigate the impact of the number of in-context examples on the scoring performance.\nAs discussed in Section 5.2  ###reference_###, the models showed notably lower performance in grading incorrect responses than in grading correct responses.\nThis discrepancy may be due to the difference in the number of variations between correct and incorrect responses.\nAs shown in Figure 1  ###reference_###, the variation of acceptable correct responses is limited; meanwhile, the variation of incorrect responses shows considerable latitude, potentially encompassing any type of response besides the correct ones.\nConsequently, although the training data covered the majority of variations in correct responses, they cannot cover all potential incorrect responses.\nAdditionally, the GPT models significantly struggled in grading such incorrect responses, especially with fewer examples than the BERT models.\nTable 4  ###reference_### shows a grading error made by GPT-3.5, in which the model significantly failed to recognize an incorrect response.\nSuch grading errors constitute the majority of inaccurate predictions by GPT-3.5.\nWe hypothesized that these inaccuracies are due to the specialized prompt and response format of STEs, including scores, detailed rubrics, and justification cues.\nHence, during pretraining, GPTs are not exposed to such a task, despite the extensive corpora collected from the Web.\nUtilizing GPT-3.5 for few-shot in-context learning is expected to be more suitable for classroom applications than fine-tuning the model with a substantial amount of training data.\nHowever, our observations suggest that this application of GPT-3.5 is inadequate for grading STEs.\nTo investigate the appropriate number of in-context examples, we evaluate performance by varying the number of examples provided in the prompt.\nFigure 3  ###reference_### illustrates the -score of GPT-3.5 for each label as the number of in-context examples is varied between one, two, five, and 10.\nFrom the result, we can clearly see that the grading performance hardly changed even when the number of in-context examples was increased to more than two.\nAs a reason for this, in grading for correct responses, it is considered that our task design inherently results in a very limited number of patterns corresponding to correct expressions. Therefore, increasing the number of instruction samples may not significantly influence the accuracy for correct responses.\nIn the grading of incorrect responses, a considerable number of instances are labeled as incorrect due to the absence of expressions equivalent to the correct answers.\nIn such cases, justification cue string is not given in the instruction for GPTs and this makes it challenging to grasp scoring clues from the provided instruction examples, likely hindering the effective learning of appropriate grading and consequently impeding performance improvement.\n###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Settings",
            "text": "In our dataset, the label “partially correct” was infrequently used, which transformed the grading of certain criteria into a binary classification task.\nTherefore, we used the -score to evaluate the performance of the analytic score prediction as it applies to both three-class and binary classification.\nWe also performed a 5-fold cross-validation by dividing the dataset of each question into a training set, a development set, and an evaluation set following a 3:1:1 ratio.\nWe finetuned the BERT model (described in Section 4  ###reference_###) for 50 epochs on each training set.\nFor each epoch, we calculated -score for each analytic criterion and used the parameters that produced the best results on the development set for each analytic criterion, respectively.\nAppendix C  ###reference_### provides details regarding these hyperparameters.\nFor the GPT models, we randomly selected few-shot examples for each score from the training set.\nSome analytic criteria contained extremely few incorrect responses because typical high school students found them too easy. Therefore, to ensure a proper performance evaluation, we used only those criteria that contained 10% or more incorrect instances."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Table 3  ###reference_### shows the performance of BERT, GPT-3.5, and GPT-4 on the test set in terms of  averages and standard deviations for each category (Expression, Word Order, Grammar).\nIn Section 4.2  ###reference_###, we hypothesized that the GPT models would demonstrate excellent performance because STEs evaluate the validity of English sentences within a highly limited grammar and vocabulary scope presented in an analytic criterion.\nSurprisingly, however, the BERT model outperformed the GPT models on our dataset.\nNevertheless, both models showed relatively high performance in grading correct responses.\nMeanwhile, the GPT models performed notably lower in grading incorrect responses.\nInterestingly, however, the GPT models outperformed BERT in grading partially correct responses.\nThis may be due to the limited data size for fine-tuning the BERT model for partially correct responses.\nWe also observed that the standard deviation exceeded 0.10 for nearly all results, indicating a substantial variance in grading performance across different analytic criteria, some of which showed poor results.\nThe result suggests that the grading of several analytic criteria is challenging for models.\nLLMs acquire sufficient knowledge about language, including grammar and vocabulary, through pretraining on massive corpora. However, these results showed that STEs grading remains a challenging task even for a cutting-edge LLM such as GPT-4, when provided with only few-shot examples.\nFurthermore, collecting and annotating enough responses to train the STE grading model poses a significant burden in actual educational settings, allowing room for improvement in deploying automatic grading models in actual classrooms."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Analysis",
            "text": "As discussed in Section 5.2  ###reference_###  ###reference_###, the models showed notably lower performance in grading incorrect responses than in grading correct responses.\nThis discrepancy may be due to the difference in the number of variations between correct and incorrect responses.\nAs shown in Figure 1  ###reference_###  ###reference_###, the variation of acceptable correct responses is limited; meanwhile, the variation of incorrect responses shows considerable latitude, potentially encompassing any type of response besides the correct ones.\nConsequently, although the training data covered the majority of variations in correct responses, they cannot cover all potential incorrect responses.\nAdditionally, the GPT models significantly struggled in grading such incorrect responses, especially with fewer examples than the BERT models.\nTable 4  ###reference_###  ###reference_### shows a grading error made by GPT-3.5, in which the model significantly failed to recognize an incorrect response.\nSuch grading errors constitute the majority of inaccurate predictions by GPT-3.5.\nWe hypothesized that these inaccuracies are due to the specialized prompt and response format of STEs, including scores, detailed rubrics, and justification cues.\nHence, during pretraining, GPTs are not exposed to such a task, despite the extensive corpora collected from the Web.\nUtilizing GPT-3.5 for few-shot in-context learning is expected to be more suitable for classroom applications than fine-tuning the model with a substantial amount of training data.\nHowever, our observations suggest that this application of GPT-3.5 is inadequate for grading STEs.\nTo investigate the appropriate number of in-context examples, we evaluate performance by varying the number of examples provided in the prompt.\nFigure 3  ###reference_###  ###reference_### illustrates the -score of GPT-3.5 for each label as the number of in-context examples is varied between one, two, five, and 10.\nFrom the result, we can clearly see that the grading performance hardly changed even when the number of in-context examples was increased to more than two.\nAs a reason for this, in grading for correct responses, it is considered that our task design inherently results in a very limited number of patterns corresponding to correct expressions. Therefore, increasing the number of instruction samples may not significantly influence the accuracy for correct responses.\nIn the grading of incorrect responses, a considerable number of instances are labeled as incorrect due to the absence of expressions equivalent to the correct answers.\nIn such cases, justification cue string is not given in the instruction for GPTs and this makes it challenging to grasp scoring clues from the provided instruction examples, likely hindering the effective learning of appropriate grading and consequently impeding performance improvement.\n###figure_4###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Grammar Error Correction (GEC) and Short Answer Scoring (SAS) are the two major research areas in the automatic evaluation of descriptive English responses.\nWe position this study between these two research domains.\nThe dataset we created for the STE task followed the format of the RIKEN SAS dataset, which contains questions on Japanese reading comprehension questions Mizumoto et al. (2019  ###reference_b22###); Funayama et al. (2023  ###reference_b11###).\nOther SAS datasets include BEETLE  Dzikovska et al. (2013  ###reference_b8###), ASAP-SAS,222https://www.kaggle.com/c/asap-sas/  ###reference_### Powergrading, and the SAF dataset Filighera et al. (2022  ###reference_b10###), which focus on science or reading comprehension.\nOur dataset is the first STE dataset to concentrate on grading grammar and vocabulary use."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Grammar Error Correction (GEC)",
            "text": "The most famous GEC system is Grammarly,111https://www.grammarly.com  ###reference_www.grammarly.com### a writing assistant tool that also plays an important role in English learning Ranalli (2021  ###reference_b25###); Koltovskaia (2020  ###reference_b18###).\nIn a more educational context,  Nagata (2019  ###reference_b23###) proposed the task of feedback generation in GEC with a focus on effective ESL (English as a Second Language) learning.\nSome studies have also focused on methods to generate feedback for grammatical errors in sentences written by learners Hanawa et al. (2021  ###reference_b14###); Coyne (2023  ###reference_b6###); Lai and Chang (2019  ###reference_b19###).\nRegarding the use of LLMs in GEC, Fang et al. (2023  ###reference_b9###) reported that GPT-3.5 shows excellent GEC abilities.\nAll these previous studies have focused primarily on identifying grammatical errors present in freely-composed text.\nHowever, within real-world educational contexts that require the measurement of student progress in language learning, educators must direct their attention to the assessment of not only overarching grammatical constructs but also a precise understanding of certain grammatical or vocabulary items within specific units of English textbooks.\nSuch a methodology would determine students’ comprehension and areas of unfamiliarity more accurately.\nTherefore, we adopted this practical approach by developing STEs specifically designed to evaluate students’ understanding of various grammatical topics."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Short Answer Scoring (SAS)",
            "text": "We have formally defined our STE grading task within the established framework of the automated SAS task.\nHowever, these two tasks fundamentally differ in terms of their intended objectives and the descriptive content to be evaluated.\nSeveral SAS studies have primarily examined closed-domain questions that require knowledge and understanding in specific areas, such as science or reading comprehension Mizumoto et al. (2019  ###reference_b22###); Burrows et al. (2015  ###reference_b2###); Galhardi and Brancher (2018  ###reference_b12###), and a typical SAS framework does not directly consider grammatical errors and word usage errors in responses.\nIn this study, we created detailed and stringent analytic criteria for measuring learners’ English proficiency, focusing on the grammatical aspects addressed in the questions.\nThe dataset we created for the STE task followed the format of the RIKEN SAS dataset, which contains questions on Japanese reading comprehension questions Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###); Funayama et al. (2023  ###reference_b11###  ###reference_b11###).\nOther SAS datasets include BEETLE  Dzikovska et al. (2013  ###reference_b8###  ###reference_b8###), ASAP-SAS,222https://www.kaggle.com/c/asap-sas/  ###reference_###  ###reference_### Powergrading, and the SAF dataset Filighera et al. (2022  ###reference_b10###  ###reference_b10###), which focus on science or reading comprehension.\nOur dataset is the first STE dataset to concentrate on grading grammar and vocabulary use."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study introduced a novel task focusing on the automatic grading of Sentence Translation Exercises (STEs) for educational purposes. We formalized STEs as a task of grading each analytic criterion predetermined by teachers’ intentions and constructed a dataset to implement the task. This first-of-its-kind dataset emulates and reflects the practical form of L2 learning in the responses of learners. We also used finetuned BERT and GPTs with few-shot in-context learning to establish a baseline and demonstrate the feasibility of the formulated framework. In our experiment, although the GPT models showed substantial performance in various NLP tasks, they remained inferior to the BERT model, suggesting that our newly defined task continues to be challenging even for the state-of-the-art LLMs, therefore necessitating further exploration. With regard to future direction, we are contemplating the integration of technologies such as GEC and machine translation within our model. We aim to build cross-questions strategies to automatically identify expressions that diverge from a provided rubric while preserving the text’s fundamental meaning using a combination of these technologies. For this purpose, our plan involves further subdividing the STE grading task and leveraging LLMs to address each minimized task such as correcting grammatical errors, assessing the consistency of meaning with L1, and identifying expressions aligned with the learning objectives in each exercise. This approach also aims to investigate tasks where LLMs may not excel in STE scoring and enhance their overall performance. Additionally, in an educational context, we also consider generating more comprehensive feedback comments on the scoring results, extending beyond the estimation of justification cues."
        }
    ]
}