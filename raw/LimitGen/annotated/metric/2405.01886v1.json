{
    "title": "Aloe: A Family of Fine-tuned Open Healthcare LLMs",
    "abstract": "As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Democratising foundation models is a safety mechanism, preventing the concentration of power of a potentially disruptive technology while increasing the amount of oversight dedicated to its research and deployment. An area in which openness is of special relevance is human healthcare, a domain with direct implications for the quality of life of individuals. Foundational models for healthcare can contribute by reducing the costs of healthcare services and training while increasing the accessibility to medical expert information. Open healthcare LLMs are fundamental to guarantee that all can benefit from advances in this field, while pushing for higher standards of transparency and reliability in AI models.\n\nThe first step in that path is the availability of competitive base models, pre-trained on massive amounts of data and capable of matching the performance of private base models. In this context, the recent releases of open models from private companies, like Mistral and Meta, provide a foundation on which researchers can explore, tune, and potentially improve models for particular domains. Possible ways of doing so include 1) Continued pre-training. Further autoregressive training of base models on large amounts of domain-specific data sources. 2) Domain-specific assistant adaptation, also known as supervised fine-tuning (SFT), or instruct tuning (e.g., through Question-Answer examples). 3) Model merging (similar to ensemble methods) leveraging the performance of different model instantiations. 4) Alignment stage in which models are adjusted with human preferences to decrease the risk they pose to their users and society in general (e.g., DPO). And finally, 5) Prompting strategies, boosting the performance of models in inference through advanced in-context learning techniques.\n\nPrevious works have focused on continued pre-training (1), with some effort made on the instruct tuning (2) phase. However, this approach seems to have a limited effect on model performance, as base models already include massive amounts of training data which are likely to supersede many specialized pre-training data. As a result, healthcare-specific LLMs rarely outperform base LLMs. Little effort has been made in boosting (2) through synthetic data, and in evaluating the impact of (3) (4) and (5).\n\nThis work introduces Aloe, a new family of healthcare LLMs. Currently focused on the 7B range, Aloe is highly competitive with all previous open models of its range and reaches state-of-the-art results at its size by using model merging and advanced prompting strategies. Beyond top-of-the-line metrics on medical benchmarks, Aloe also scores high in metrics measuring ethics and factuality, thanks to a combined red teaming and alignment effort. To boost academic research on healthcare LLMs, the best Aloe model which includes model alignment is openly released under CC-BY-NC 4.0 license. Complete training details, model merging configurations and all training data (including the one synthetically generated in this work). In addition, the prompting repository used in this work to produce state-of-the-art results during inference is also shared. To contribute to the safe use and deployment of such systems, Aloe comes with a healthcare-specific risk assessment."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The field of LLMs for healthcare is currently dominated by private, non-accessible models, with the two most performing models being GPT4 and MedPalm-2 [20  ###reference_b20###]. Meanwhile, open models have been trying to catch up. MedAlpaca, first released in April 2023, is based on LLaMA and includes 7 and 13B models. The model is instruct-tuned on a mixture of data (150K Q&A pairs). PMC-LLaMA [51  ###reference_b51###], published in May 2023, is a fine-tune on top of LLaMA. It is first trained autoregressively on a mix of books (4B tokens, 5 epochs) and papers (totalling one-third of the book tokens), and then instruct tuned on QA (Question-Answer) pairs (202M tokens, 3 epochs). It includes a 7B and a 13B version.\nMeditron [5  ###reference_b5###], published in November 2023, includes a 7B and a 70B version and is also continuously pre-trained and then fine-tuned on LLaMA-2. Its data mainly includes medical papers, as well as abstracts and guidelines for continued pre-training (48B tokens). For testing, Meditron is instruct-tuned for each specific benchmark separately.\nMMed-LLM 2 [39  ###reference_b39###], published in February 2024, is a 7B model trained on top of InternLM-2 [3  ###reference_b3###] using medical data extracted primarily from general purpose multilingual data sets and textbooks (25B tokens). This data includes 6 languages and achieves state-of-the-art performance in medical QA among open models for languages such as Japanese and Chinese in their own multilingual benchmark (MMedBench).\nBioMistral [24  ###reference_b24###], published in February 2024, performs continuous pre-training on medical papers (3B tokens) for 1.5 epochs, on top of the instruct-tuned Mistral-7B.\nWhile increasingly competitive, these works do not yet reach the level of performance of private models and have been recently outperformed by open general-purpose models, like Llama 3. This is directly related to the large volumes of highly curated data used for training these base models and compromises the impact of continued pre-training. In contrast, the effect of leveraging synthetic data during instruct tuning, or using model merging, alignment and advanced inference schemes remains untested. These are of special relevance for open healthcare LLMs, as these techniques can have a significant impact on model fairness, safety, reliability, and factuality. Only a few works superficially review the risks and potential harms of models [47  ###reference_b47###, 18  ###reference_b18###, 37  ###reference_b37###], and this family of LLMs have been thoroughly benchmarked in that regard recently [2  ###reference_b2###]. A pressing issue, considering the dangers of bias, toxicity, sycophancy, and hallucinations in healthcare."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Synthetic Training Data Generation",
            "text": "Synthetic data is becoming a crucial component in addressing the scarcity of high-quality data to train LLMs, particularly within specialised and private domains, such as the medical field. Synthetic data generation has been proven to be an effective way of scaling training and evaluation [15  ###reference_b15###] data for LLMs for diverse domains, such as math [45  ###reference_b45###], code [49  ###reference_b49###] and general [7  ###reference_b7###]. Current open models offer a great alternative to labour-intensive manual data curation processes, as they are easier to fit in more affordable GPUs, making data generation exponentially more scalable.\nHowever, the generation of synthetic data poses new challenges such as hallucinations and inherent model biases [28  ###reference_b28###], impacting the dataset quality. For this reason, recent approaches use real medical data as a base to prompt and enhance it for a particular medical task. [44  ###reference_b44###] employs ChatGPT to generate more than 10K examples based on several biomedical Named Entity Recognition and Relation Extraction datasets as seed, significantly improving the F1 score in both tasks. [25  ###reference_b25###] uses a CoT style synthetic data generation strategy based on LLaMA-65B to detect Alzheimer’s Disease (AD)-related signs and symptoms from electronic health records (EHRs). Lastly, GatorTron [36  ###reference_b36###] generated 20 billion words of synthetic text to train NLP models, which outperform models trained using real-world clinical text."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Prompt engineering",
            "text": "Prompt engineering has emerged as a powerful technique in natural language processing, offering an alternative approach to enhance the performance of LLMs without the need for retraining. At its core, prompt engineering involves crafting specialized input prompts or instructions that guide LLMs to generate desired outputs or responses.\nIn-Context Learning (ICL) is a technique that involves integrating task demonstrations directly into the input prompt. This approach empowers pre-trained LLMs to tackle novel tasks without requiring fine-tuning of the model. Zero-Shot prompting presents a task to a model without any accompanying examples, relying solely on the model’s pre-existing knowledge. This concept is extended by few-shot learning, which provides a small number of examples to guide the model in quickly learning new tasks. KNN few-shot learning builds on this by incorporating the most similar existing examples, often stored in vector databases like those used in Retrieval-Augmented Generation (RAG) systems.\nChain of Thought (CoT) prompting involves generating intermediate reasoning steps before arriving at the final answer. By breaking down complex problems into smaller steps, CoT helps models generate more accurate responses. Integrating CoT with ICL can enhance performance further by introducing few-shot examples alongside the reasoning steps. Finally, self-consistency methods combine outputs from different models or multiple runs of the same model. Techniques such as majority voting, averaging, or seeking consensus among outputs lead to more robust and accurate results. When employed in conjunction with other techniques like prompt engineering, self-consistency can significantly improve the reliability and effectiveness of LLMs across various tasks and domains.\nIn regards to medical LLMs, previous works have studied the impact of integrating prompt engineering techniques to boost the performance of specialized medical models. For example, in Meditron [5  ###reference_b5###], they use the Self-Consistency Chain of Though technique achieving SOTA performance within open-source models on PubmedQA, MedQA, and MedMCQA benchmarks(MultiMedQA suite) by sampling 20 generations and performing majority voting. Recent studies have also explored the use of advanced prompting methods as a cost-effective approach to optimize the performance of generalist foundation models. Microsoft developed a prompting technique that involves several strategies, combining CoT, ICL with K-NN few-shots, and self-consistency. Their technique, named Medprompt [34  ###reference_b34###], was tested on GPT-4 achieving SOTA results on all benchmarks of the MultiMedQA suite. Following this strategy, OpenMedLM [30  ###reference_b30###] conducted a study of these prompting techniques using open-source models. Results obtained improved performance on three of the four most widely used medical benchmarks."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Preference Alignment",
            "text": ""
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "This section presents the datasets that were used for all the training stages of Aloe models, together with the processing pipeline. We target the three key aspects of datasets: quality, diversity, and quantity, and we do so both in our fine-tuning (which includes a thorough synthetic data generation step) and alignment (includes generation guided by red teaming efforts). In §3.2 we detail the process for creating synthetically enhanced versions of the medical benchmark train splits and in §3.1 we detail the processing applied to the other medical curated datasets. In §3.3 we explain about the preference alignment datasets. We do not do any further processing on our general datasets. Refer to the Appendix A for a complete list of data sources."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Finetuning",
            "text": "Instruct tuning datasets used include medical and general domains. To ensure coverage of medical tasks, we curated data from a large number of publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for research purposes. Motivated by [8], we combine our medical dataset with a small set of high-quality general instruct tuning datasets with a mixing ratio of 8:1 for finetuning our model to avoid the catastrophic forgetting problem [43]. We then convert the raw datasets to a common structure suitable for training. We use the Alpaca format for single turn QA, and the ShareGPT format for multi-turn. The rest of this subsection describes the different steps of processing performed on the medical datasets, as illustrated in Figure 2. Further details on data processing steps can be found in Appendix B."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Cleaning and Deduplication",
            "text": "Cleaning includes the removal of URLs, emails, special characters, and unnecessary spaces, as well as the standardization of capitalization and punctuation. Each dataset is analyzed individually, to identify and fix specific formatting issues (e.g., errors in line break codification). We then manually filter samples which have a missing question or answer. Based on a handcrafted list of irrelevant questions and answers some additional QA pairs are also removed. This step also fixes several identified cases of redundant and noisy responses from multi-choice QA pairs. Examples are provided in Appendix B.\n\nFollowing which, we perform deduplication using the Local-Sensitivity Hashing (LSH) Minhash technique, as implemented in Datatrove. Each QA is concatenated, and QA pairs are compared using the default threshold (i.e., 0.72). For multi-turn conversations, the concatenation of the dialogues is performed adding the author of the turn in each dialogue. The threshold is tuned for this second set of data to 0.77 to reduce false positives."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Decontamination",
            "text": "We wanted to decontaminate the benchmark test/validation splits from our other curated datasets. [53 ###reference_b53###] propose an LLM-based decontamination method and show better performance over existing n-gram and embedding similarity-based approaches. We use a similar LLM-based decontamination technique with the Nous-Hermes-2-Yi-34B model as the judge. We removed all instructions flagged by the model."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Post-Processing Filtering",
            "text": "To further improve the quality of the medical data, we apply a variant of the DEITA [29  ###reference_b29###] technique. We generate complexity and quality scores using the DEITA scorers, removing samples for which the DEITA pipeline is unable to provide either quality or complexity scores. We then compute the evol score, which is a product of the quality and complexity scores. All samples in the bottom 10% of the evol score are then discarded (see Appendix B  ###reference_### for more details)."
        },
        {
            "section_id": "3.1.4",
            "parent_section_id": "3.1",
            "section_name": "3.1.4 Templating",
            "text": "Following the findings of Orca [33  ###reference_b33###], showing the importance of having diverse and high-quality prompts in the training data, we manually crafted between 5 and 10 templates for each of the 16 identified tasks within the dataset, resulting in a total of 110 distinct templates. The complete list of tasks and templates can be found in Appendix B  ###reference_###. This process introduces variance into the training samples, further increasing the dataset diversity, and augmenting the model’s ability to adapt to a wide array of queries. Furthermore, in certain tasks involving datasets covering various medical topics, we enhance the diversity of the template by replacing a generic placeholder with the specific topic of the dataset."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Synthetic Data Generation",
            "text": "To increase the quality of answers from the training splits of the benchmark datasets (MedQA, MedMCQA, and PubMedQA), we leverage Mixtral-8x7B to generate Chain of Thought (CoT) answers. We create a custom prompt for each dataset, along with a hand-crafted list of few-shot examples. For a multi-choice answer, we ask the model to rephrase and explain the question, then explain each option with respect to the question, then summarize this explanation to arrive at the final solution. During this synthetic data generation process, the model is also given the solution and the reference answer. This CoT guides the model towards a better response by breaking down the problem into smaller steps. This way, we add new medical knowledge from the bigger model, plus a problem decomposition of medical questions. For the cases where the model fails to generate correct responses and just reiterates the input question, we regenerate the solutions until a correct response is generated.\n\nTo increase the volume of medical synthetic data for instruction tuning, we process the Medical Guidelines dataset by EPFL and generate 34,219 additional question-answer pairs using Genstruct. We apply the same templating technique as seen for all synthetic data. In total, the final supervised training data which includes the medical and general fine-tuning datasets along with the synthetic data account for 500 million tokens."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Preference Alignment",
            "text": "We conduct a two-step alignment training. In the first step, we use a limited amount of high-quality, publicly available paired preference data (see the complete list in Appendix A). In the second step, we conduct a red teaming effort to identify harmful, unsafe, or illegal responses of the model, and use these insights to produce alignment data to mitigate them.\n\nTo generate a dataset of adversarial prompts, we compile and curate entries from Anthropic Harmless, as well as data from Chen et al., together with some original prompts. With these, we generate a dataset of 1,675 adversarial entries (divided into train and test splits of 1,198 and 477 prompts respectively) classified in 7 general topics and 12 attack styles (see descriptions in Appendix A), closely resembling taxonomies present in previous works. We use the Aloe model to answer the adversarial questions of the training set, and Llama Guard 2 to classify them as either safe or unsafe. We compile the unsafe responses alongside refusals to answers generated with GPT4 Turbo, to craft a DPO dataset that covers Aloe’s weak points. We merge this data with the HelpSteer dataset and use the resulting dataset for the second step of our alignment training. Merging both datasets helps maintain a good ratio between model refusals and answers. We validate the use of Llama Guard 2’s classification by manually reviewing a random subset of 200 answers from Aloe. We find our human judgment to agree on 79.5% of Llama Guard 2’s safe/unsafe classifications. On the disagreements, 92.7% of the times it is the human who finds the response harmful or toxic.\n\nThe creation of the dataset and the red teaming effort has been conducted by colleagues, not directly participating in the development of Aloe, but who appear as co-authors of this paper."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Training Details",
            "text": "Aloe is fine-tuned on top of a base LLM. Several open models within the 7B range were benchmarked for their medical performance, and Mistral-7B and Llama 3 8B were selected based on these results. A supervised fine-tuning process is conducted on top of these two base models, using the data defined in 3.1  ###reference_### and 3.2  ###reference_###, resulting in two assistant models: Mistral-Aloe-7B-v1 and Llama3-Aloe-8B-v1.\nWe then explore the impact of model merging, combining similar models to produce a more robust outcome. The resulting model maintains the same size as the merged models and leverages their individual knowledge. Inspired by  [24  ###reference_b24###, 50  ###reference_b50###], we opt for the DARE-TIES process described in [54  ###reference_b54###] and [52  ###reference_b52###]. For the mistral merge we use Mistral-Aloe-7B-v1, Starling-LM-7B-Beta and WizardLM-2 and for llama3 we use Llama3-Aloe-8B-v1, Llama3 8B Instruct and llama-3-neural-chat-v1-8b [1  ###reference_b1###] models, with the Mergekit library [17  ###reference_b17###]. At the time of writing these were the best-performing instruct variants of the respective base models. This process results in the models Mistral-Aloe-7B-Merged-v1 Llama3-Aloe-8B-Merged-v1. We select the better performing Llama merge moving forward."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "DPO and Red Teaming",
            "text": "On top of the merged Aloe model, we perform the two-stage DPO process using the data described in 3.3  ###reference_### for human preference alignment. This training results in the final DPO merge model, Llama3-Aloe-8B-Merged-DPO-RT-v1, codename Llama3-Aloe-8B-Alpha ."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Model Evaluation",
            "text": "To assess the performance of our contributions, we conduct several evaluations employing the lm-evaluation-harness repository [14 ###reference_b14###] and the vllm runner using default parameters. A medical test against current open alternatives, an evaluation on AI principles and trustworthiness against the same models, an ablation study within the Aloe family for the healthcare domain, and a general purpose test to validate the lack of degraded performance."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Prompt Engineering",
            "text": "We first use 0 shot prompting for a complete evaluation followed by two advanced prompting strategies. For these advanced prompting strategies, we select the best configurations based on an ablation study with the base Mistral model (see Appendix C). We compare our top-performing model Llama3-Aloe-8B-Alpha with the Llama-3-8B-Instruct. The first strategy we use is the self-consistency CoT, sampling 5 and 20 ensembles. Detailed results are analyzed in the appendix, but in summary, both present an almost identical accuracy across various benchmarks with the 20-ensemble configuration exhibiting marginally superior results in the weighted average.\n\nThen, we evaluate the Medprompt strategy using two embedding models: Pubmedbert-base-embedding and SFR-Embedding-Mistral. The former is a small model specialized in medicine, while the latter is a high-performance, general-purpose model. In each question prompt, we incorporate the five nearest examples (cosine similarity) into the prompt, utilizing the selected embedding model, while instructing the model to generate a step-by-step answer. We repeat this inference process 5 and 20 times respectively during different runs (no.of ensembles), shuffling the potential options (A, B, C, etc.) in each iteration. Finally, we perform majority voting to determine the final answer. For our database of examples, we used CoT-generated answers produced from the training set of each benchmark, as discussed in 3.2. Specifically, for MedQA, MedMCQA, and PubmedQA, we utilized their respective training sets with CoT answers generated by Mixtral-8x7B, limiting the examples to a random 20k subset to reduce computational expenses. For MMLU and CareQA, which lack training splits, we employed the training set of MedMCQA. Medprompting outperforms self-consistency CoT. Figure 3 illustrates a visual representation of the prompting strategy employed. More details and complete ablations are available in Appendix C.\n\nResults of the experiments we realized are presented in Table 25 in the Appendix C. Our model outperforms Meta’s Llama 3 8B in all settings and benchmarks, averaging a 2% accuracy increase. We have observed that both embedding models achieve similar performance, and, in the case of our model, SFR-Embedding-Mistral performed better with 5 ensembles but Pubmedbert-base-embeddings outperformed with 20 ensembles. The domain-specific embedding (e.g., pubmedbert) excels despite its smaller size (109M vs. SFR’s 7B), suggesting it captures relevant medical data thanks to the medical specialized training. Finally, the performance gap between the number of ensembles is minimal. While there’s a modest 1% average accuracy increase with 20 ensembles, the substantial rise in computational costs (four times more than using 5 ensembles) must be considered."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Medical Task Evaluation",
            "text": "To compare Aloe with the most competitive open models (both general purpose and healthcare-specific), we use popular healthcare datasets (e.g., PubMedQA, MedMCQA, MedQA, and MMLU for six medical tasks only), together with the new and highly reliable CareQA. We produce the standard MultiMedQA score for reference, by computing the weighted average accuracy on all scores except CareQA. Additionally, we calculate the arithmetic mean across all datasets. The Medical MMLU is calculated by averaging the six medical subtasks: Anatomy, Clinical knowledge, College Biology, College Medicine, Medical Genetics, and Professional Medicine.\n\nBenchmark results indicate that the training conducted on Llama3-Aloe-8B-Alpha has boosted its performance slightly above Llama3-8B-Instruct. Llama3-Aloe-8B-Alpha outperforms larger models like Meditron 70B and is close to larger base models, like Yi-34. For the former, this gain is consistent even when using SC-CoT, employing their best-reported variant. All these results make Llama3-Aloe-8B-Alpha the best healthcare LLM of its size.\n\nWith the help of prompting techniques, the performance of Llama3-Aloe-8B-Alpha is significantly improved. Medprompting, in particular, provides a 7% increase in reported accuracy, after which Llama3-Aloe-8B-Alpha only lags behind the ten times bigger Llama-3-70B-Instruct. This improvement is mostly consistent across medical fields. Llama3-Aloe-8B-Alpha with medprompting beats the performance of Meditron 70B with their self-reported 20-shot SC-CoT in MMLU med and is slightly worse in the other benchmarks."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "AI Principles Evaluation",
            "text": "To assess the impact of our alignment efforts, models are evaluated on benchmarks targeting AI principles. First, we use the test partition of the DPO dataset generated in §3.3, comparing the Aloe model after the first DPO with Llama3-Aloe-8B-Alpha, measuring the impact of the alignment.\n\nAdditionally, we run benchmarks for bias (Crows pairs), evaluating whether the model considers stereotypical sentences more probable than non-stereotypical ones, sycophancy, measuring how much models change their outputs based on user input, factuality (TruthfulQA), assessing how many falsehoods do models produce, ethics, tracking the alignment of models to human morality, and toxicity, measuring the degree of toxicity (with ToxDectRoBERTa) on the models output when provided with toxic inputs.\n\nResults are shown in Table 5. Overall, no model outperforms the rest across the board, with bigger models performing slightly better (but not remarkably so) than their smaller counterparts. In comparison with the closest model (Llama-3-8B-Instruct), Llama3-Aloe-8B-Alpha is better in ethics and factuality while being more biased, sycophant, and toxic. These differences are likely explained by the fact that, while Llama-3-8B-Instruct uses a larger own general purpose DPO, likely including way more training samples than Llama3-Aloe-8B-Alpha (first stage DPO is only 11k samples), Llama3-Aloe-8B-Alpha includes its own specific red teaming DPO."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Ablation study",
            "text": "To study the differences among the models of the Aloe family, we conduct an ablation study that evaluates the impact of our contributions on the medical benchmarks. Results shown in Table 6 indicate that instruct tune, merge, and DPO performed increased model performance, the biggest gain being obtained from model merging. Precisely, the choice of Llama3 as base for Llama3-Aloe-8B-Alpha is based on its greatest boost on the merged variants. Notice DPO training does not downgrade the average medical performance."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "General Evaluation",
            "text": "To confirm the lack of catastrophic forgetting after the instruct and alignment tuning, we show benchmarks from the OpenLLM Leaderboard, as shown in Table 7. Results indicate a slight degradation of performance in half of the benchmarks (i.e., ARC, MMLU, GSM8K) and a slight improvement in the other half (i.e., HellaSwag, TruthfulQA, Winogrande)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Infrastructure and Reproducibility",
            "text": "###table_1### The experiments needed for this work were conducted on single compute nodes with 4NVIDIA A100/H100 (64GB) GPUs (see Table 8  ###reference_###). We utilized the axolotl222https://github.com/OpenAccess-AI-Collective/axolotl repository to launch the training experiments, and lm-evaluation-harness for evaluation.\nFor responsibility reasons, only the DPO version of Aloe, Llama3-Aloe-8B-Alpha , is publicly distributed under the CC-BY-NC 4.0 license. To facilitate the reproducibility of results, we also distribute our model merging configurations, and all training data used for tuning Llama3-Aloe-8B-Alpha . The prompting repository is made available online."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Ethical Considerations and Risks",
            "text": "Using LLMs for healthcare requires robust regulation, public education, and stringent oversight, to ensure responsible and safe interactions [16  ###reference_b16###]. To enforce safety, Aloe is released with an alignment tuning, designed to mitigate its risks and safeguard users. Advanced prompting techniques are integrated, to increase factuality, and a red teaming effort is conducted to explore Aloe’s defects. Follows a list of other limitations and ethical considerations to be considered, together with a risk assessment.\nIntended purpose: These models are not to be used for clinical practice, medical diagnosis, or any other form of direct or indirect healthcare advice. Models are prone to error and can produce toxic content. We encourage the use of Aloe for research purposes, as a stepping stone to build better foundational models for healthcare.\nThe use of Aloe models for activities harmful for individuals, such as spam, fraud, or impersonation, is prohibited.\nPre-train: Aloe is built on top of a pre-trained model which come with a number of unknown biases and unexpected behaviours that are inherited.\nPrivacy & Safety: We avoid the use of all personal data in our training. Model safety cannot be guaranteed, as shown in the red teaming results. Aloe can produce toxic content under the appropriate prompts. For these reasons, minors should not be left alone to interact with Aloe without supervision."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Risk Assessment",
            "text": "We follow the six points proposed in [23  ###reference_b23###] to evaluate potential dangers related to the Aloe model. Three main risks are identified specific to the healthcare domain, which is its main differentiating factor. A complete version of this assessment can be found in Appendix E  ###reference_###.\nFirst let us consider Healthcare professional impersonation, a fraudulent behaviour which currently generates billions of dollars in profit 333https://www.justice.gov/opa/pr/justice-department-charges-dozens-12-billion-health-care-fraud. A model such as Aloe could be used to increase the efficacy of such deceiving activities, making them more widespread. The main preventive actions are public literacy on the unreliability of digitised information and the importance of medical registration, and legislation enforcing AI-generated content disclaimers. The second risk we consider is medical decision-making without professional supervision. While this is already an issue in modern societies (e.g., self-medication) a model such as Aloe, capable of producing high-quality conversational data, can facilitate self-delusion, particularly in the presence of sycophancy. By producing tailored responses, it can also be used to generate actionable answers. Public literacy on the dangers of self-diagnosis is one of the main defences, together with the introduction of disclaimers and warnings on the models’ outputs. The last risk we consider is the access to information on dangerous substances or procedures. While the literature on sensitive content can already be found on different sources (e.g., libraries, internet, dark web), LLMs can centralize such access, making it nearly impossible to control the flow of such information. Model alignment can help in that regard, but so far the effects remain insufficient, as jailbreaking methods still overcome it.\nThis work has been granted computational resources in the FinisTerrae III, Leonardo and MareNostrum 5 supercomputers. This work has been partially funded by the project SGR-Cat 2021 HPAI (AGAUR grant n.01187). We would like to acknowledge the support received from the Ops. department at BSC."
        }
    ]
}