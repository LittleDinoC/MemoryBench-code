{
    "title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence, thanks to their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements of LLMs limit their widespread adoption. Quantization, a key compression technique, offers a viable solution to mitigate these demands by compressing and accelerating LLMs. Numerous studies have aimed to refine the quantization processes. However, the quantization configurations in these studies vary and may not be optimized for hardware compatibility. In this paper, we focus on identifying the most effective practices for quantizing LLMs, with the goal of balancing performance with computational efficiency. For a fair analysis, we develop a quantization toolkit LLMC, and design four crucial principles considering the inference efficiency, calibration cost, and modularization. By benchmarking on various models and datasets with over 500 experiments, three takeaways corresponding to calibration data, quantization algorithm, and quantization schemes are derived. Finally, a best practice of LLM PTQ pipeline is constructed. All the benchmark results and the toolkit can be found at https://github.com/ModelTC/llmc.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large Language models (LLMs) such as GPT-4 (OpenAI et al., 2024) have demonstrated unprecedented generative capabilities in the field of natural language processing (NLP), and achieving widespread applications across various industries. However, their substantial computational and storage costs have impeded their further popularization among users. For instance, BLOOM (Touvron et al., 2023), an open-access multilingual LLM with 176 billion parameters, requires a minimum of 350 GB of space merely to store model weights in full-precision (FP16) format. At a minimum, it requires 580GB A100 or 940GB A800 NVIDIA GPUs to perform inference with this model. Therefore, reducing their serving cost is paramount to further enhance the application of LLMs.\n\nTo address this challenge, model quantization (Nagel et al., 2021) can be an effective resolution strategy. It maps weights and/or activations to a lower-bit data format to reduce memory footprints and accelerate model inference. Existing quantization approaches can be categorized into two types: quantization-aware-training (QAT) (Bhalgat et al., 2020; Gong et al., 2019; Esser et al., 2020; Egiazarian et al., 2024; van Baalen et al., 2024) and post-training quantization (PTQ) (Wei et al., 2023a; Jhunjhunwala et al., 2021; Li et al., 2021). Although with prominent high performance, the necessity for QAT to undergo fine-tuning or retraining with substantial training data and training cost renders it unattainable for the majority of users. Correspondingly, PTQ compresses models without retraining, making it a preferred method for LLMs due to its minimal resource requirements. \n\nCurrent uniform PTQ methods always evaluate across distinct datasets in different quantization configurations and with simulated quantization. This current state would lead to users being unable to accurately assess the configurations that should be selected for the efficient and accurate quantization of LLMs. To provide a comprehensive quantization options menu for users to obtain hardware-friendly quantized LLMs with high performance, we make a fair benchmark, which considers two aspects: factors influencing LLM quantization and inference efficiency under our design principles. The former perspective encompassed three dimensions, e.g., calibration data, algorithm, and target bits. Consequently, we evaluate across various kinds of tasks and find our best practice, encapsulated within an end-to-end pipeline that realizes both high efficiency and accuracy LLM quantization. This best practice has been integrated into our quantization toolkit, LLMC. Notably, LLMC, a user-friendly, plug-and-play quantization tool, incorporates dozens of outstanding PTQ algorithms, provides the freedom to select quantization strategies, and also supports deploying quantized LLMs on different inference backends (TensorRT-LLM (Nvidia, 2023), PPL-LLM (OpenPPL, 2023), LightLLM (ModelTC, 2023)) and hardware (Nvidia GPU, Qualcomm mobile chips, TPU). \n\nOur main contributions can be described as follows:\n\nWe release a quantization toolkit LLMC supporting dozens of algorithms, models and hardware. LLMC enables users to perform lossless quantization on 100-billion-parameter LLMs within a matter of hours, utilizing just a single GPU. It notably facilitates the research and production of quantized LLMs.\n\nBased on our findings, a best practice of LLM PTQ pipeline is designed, achieving the best performance balance under various scenarios."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Benchmark Overview",
            "text": "In this section, we first provide our benchmarkâ€™s design principles, outlining its primary objective. We then exhibit our plug-and-play toolkit within our benchmark."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Design Principles",
            "text": "Our benchmark focuses on four essential aspects for effective and practical LLM quantization: inference performance, calibration cost, and modularization.\n\nInference Performance: In our LLM quantization benchmark, we prioritize the importance of selecting a quantization approach that enhances inference performance. This means our chosen setting should either increase throughput or decrease memory requirements, thereby optimizing the efficiency of the model during the inference phase.\n\nCalibration Cost: The process of post-training quantization for LLMs is also named as calibration. The resources and time invested in calibration for LLM are crucial factors that affect the practicality of LLM quantization. This benchmark aims to find the best pipeline to produce accurate LLMs in minimal GPUs and time.\n\nModularization: Recent advancements have introduced a myriad of algorithms aimed at enhancing the performance of quantized LLMs. This benchmark seeks to dissect these algorithms to their most fundamental elements, analyzing the efficacy of each component in isolation.\n\nGuided by the aforementioned principles, our goal is to investigate and outline optimal practices for developing quantized LLMs tailored to various scenarios and configurations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM Quantization",
            "text": "Factors Influencing LLM Quantization. We categorize factors influencing LLM quantization into three dimensions: calibration data, algorithms, and target bits.\n\nCalibration data: Calibration data can help to evaluate the range of tensors, and then determine the quantization parameters, which is crucial for maintaining model performance post-quantization. Based on that, the impact of different corpora as calibration data warrants further investigation.\n\nAlgorithm: Efficient remedies to help maintain model performance make a lot of sense. Current effective and efficient algorithms can be summarized into three types:\n1) Transformation (Xiao et al., 2023; Lin et al., 2023; Shao et al., 2023; Wei et al., 2023b): Leveraging magnitude between weight and activation before quantization is widely used to balance quantization errors.\n2) Clipping (Lin et al., 2023; Shao et al., 2023; Wei et al., 2022; Du et al., 2024): Clipping some outliers with minimal impact in weights before quantization can help with range estimation and the representation of the rest in calibration.\n3) Reconstruction (Frantar et al., 2022; Lee et al., 2023; Dettmers et al., 2023): This approach employs the Hessian matrix to evaluate the quantization perturbations, and update the rest intact elements. This process is conducted incrementally during the quantization process.\n\nTarget bits: The bit adopted for weight, activation, and KV cache impacts the final accuracy. Usually, the hardware-friendly bits are 2-bit, 4-bit, and 8-bit. In this benchmark, we also investigate 3-bit or 6-bit to compare the potential of quantization algorithms. But for the practical deployment, 2/4/8-bit is mainly used."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Quantization Toolkit",
            "text": "To achieve the modular comparison of the different quantization dimensions aforementioned, and to consolidate best practices into an end-to-end pipeline, we have designed and developed a quantization toolkit named LLMC. This toolkit is capable of accommodating multiple quantization configurations using a variety of algorithmic techniques. The models produced by LLMC are designed for seamless deployment across a diverse range of hardware platforms. Presently, LLMC supports over ten algorithms, is compatible with over eight models, is flexible to extend the support of any transformer-based LLMs, and facilitates deployment on three types of inference engines including LightLLM, TensorRT-LLM, and PPL-LLM."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM-QBench",
            "text": "Under the principles in subsection 2.1, powered by our quantization toolkit LLMC, in this section, we explore the best practice for quantizing large language models from the aspect of calibration data and quantization algorithm."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "We first illustrate our experiment settings, more details can be found in the subsection A.1.\n\nModels. To demonstrate the generability of our benchmark, we assess performance on LLAMA-2 family, spanning model sizes from 7B to 70B for general language tasks. To broaden the scope of our evaluation benchmarks, we also benchmark on ChatGLM for long context abilities, CodeLLAMA for coding tasks, and WizardMath for mathematical problems.\n\nDatasets. We categorize the datasets into upstream datasets and downstream datasets. For the upstream datasets, we employ WikiText2 and C4 dataset with the perplexity metric for evaluation, since perplexity can stably reflect the LLMâ€™s performance. For the downstream tasks, we select examination tasks including MMLU and ARC-e, knowledge task BoolQ, understanding task Lambada, reasoning tasks including PIQA, HellaSwag and GSM8K, coding tasks HumanEval and MBPP, and the long context evaluation LongBench.\n\nHardware. In this paper, we mainly measured the inference efficiency of low-bit kernel on NVIDIA server and edge GPUs with NVIDIAâ€™s TensorRT-LLM framework."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Impact of Calibration Data",
            "text": "It is evident that calibration data affects all algorithms. To attain optimal accuracy, it is crucial to gather domain-specific data for domain-specific models and collect diverse data for the general models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Quantization Algorithm",
            "text": "Following the principles of modularization, we deconstruct the techniques behind existing algorithms. Through a comprehensive and unbiased experimental comparison, we aim to derive insights critical for developing an optimally combined pipeline.\n\nClipping. Searching for the clipping value asymmetrically is the most effective strategy for optimizing outcomes. This indicates that selecting an appropriate weight range can significantly reduce errors. Therefore, this strategy is adopted for various scenarios. Clipping should be fully utilized in the pipeline of best practices. Moreover, initialized from the asymmetric clipping, further learning can enhance performance. This effective initialization contributes to a fast convergence.\n\nReconstruction. GPTQ (Frantar et al., 2022) reconstruction involves the non-equivalent transformation of weights on the channel dimension, hindering simultaneous optimization of weights and clip values. Pre-reconstruction weight clipping yields suboptimal results due to weight changes. If reconstruction precedes clip value search, initial parameters wonâ€™t align with the updated ones. Moreover, when paired with an equivalent transformation, it yields minimal benefits. This limitation may stem from the alteration of gradients and the disruption of assumptions regarding Hessian information. Furthermore, it requires an extended calibration period. Therefore, reconstruction may not be considered a best practice.\n\nTransformation. The transformation technique utilizes a linear operation to reduce the outlier problem or preserve important weights. For both scenarios, such equivalent transformation brings an improvement, especially for activations. From the data, we can infer that manually setting the scaling number is rigid and may not help in all situations. On the contrary, a suitable search for the transformation scale is effective. Different search strategies significantly improve outcomes. A learning process can be further adopted with a pre-searched range. Fortunately, with fast pre-search support, calibration can achieve learning with fewer epochs.\n\nCalibration cost for each strategy. Among transformation techniques, the search-based (v1) strategy requires roughly 10 minutes, being twice as fast as the (v2) strategy. While rule-based transformations are quicker, they often fall short of achieving acceptable levels. Learning-based transformation methods incur a considerable increase in time to reach satisfactory levels. However, initializing the learning process with pre-searched values can reduce the number of required epochs by half and yield higher outcomes. Direct min-max value clipping is time-efficient but typically results in significant losses. The search-based clipping method, whether using asymmetric or symmetric ranges, proves efficient, requiring only about 20 minutes. When applying a learning-based approach to clipping, the calibration time can extend to nearly 7 hours. Therefore, a combined approach of the search-based transformation v1 and search-based asymmetric clipping emerges as the most effective in balancing efficiency. Furthermore, initiating with pre-searched values and conducting additional learning for a few epochs may offer further improvements."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Target Bits",
            "text": "Fixed-precision. In contrast, quantization of the Key-Value (KV) cache is proposed as a method to decrease memory usage. In Table 21 ###reference_### and Table 5 ###reference_###, we assessed the accuracy impact of 4-bit (per-group quantization with a group size of 8), and 8-bit (per-tensor) KV cache quantization. The results indicate that 4-bit KV cache quantization, with its finer granularity, performs comparably to 8-bit KV cache quantization with a coarser group size. Both the 4-bit and 8-bit configurations closely approximate the performance of FP16 at the code generation task and long-context understanding task. Hence, for KV cache quantization, a 4-bit per-group approach with a group size of 8 is recommended. \n\nInference Speed. To assess the practical benefits of different quantization approaches, we conducted evaluations using NVIDIAâ€™s cloud (SMX 80G A100) and edge (Drive Orin) GPUs, alongside the official inference library, TensorRT-LLM. Part of our results, as depicted in Figure 2 ###reference_###, highlight the throughput improvements achieved through TensorRT-LLM-supported quantization schemes for models with 32,000 input tokens and 512 output tokens. The findings indicate that quantization with 8-bit weights and activations enhances the prefill stageâ€™s speed by 20%-30% and the decode stage by 40%-60%. Itâ€™s important to note that these acceleration rates tend to diminish for larger models. Besides, 8-bit KV cache quantization has minimal impact on prefill times and slightly reduces decoding throughput for very large models, such as those with 70B model. Results for more models and hardware can be found in subsection A.5 ###reference_###. ###figure_2### ###figure_3###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Best Practice of LLM PTQ pipeline",
            "text": "Based on the takeaways distilled from the above exploration, we summarize the best practice of PTQ pipeline for LLM. As depicted in Figure 3, first, we should collect the best calibration data according to the task and model under the guide of Takeaway 1. The calibration process can then be conducted using the algorithm pipeline based on Takeaway 2. The results in Table 6 and Table 7 of general-purpose model LLAMA-2-70B and specific-domain code model CodeLLAMA-7b and math model WizardMath-7b proved the effectiveness, especially for maintaining high accuracy. More experimental results on other models and datasets to validate our best practice for decent performance and efficient inference can be found in subsection A.3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have undertaken a comprehensive benchmarking of decomposed quantization techniques for large language models (LLMs), leading to the identification of best practices that balance calibration costs and efficiency. Furthermore, we introduce LLMC, a toolkit designed to empower the research and development community. Models optimized through our recommended practices and toolkit are readily deployable across a variety of hardware platforms, enhancing accessibility and applicability in diverse computational environments."
        }
    ]
}