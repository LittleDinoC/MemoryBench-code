{
    "title": "Word Order and World Knowledge",
    "abstract": "Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.\n\nKeywords: language model, fixed word order, world knowledge",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The distribution of dominant word orders is generally explained by communicative efficiency, e.g., dependency and information locality Hahn and Xu (2022), but what explains the fact that most languages exhibit some variation across different word orders? The standard theory is that different word orders coexist because of the influence of multiple languages and that there is pressure from language acquisition for fixing word order Lupyan and Christiansen (2002). Word order transfer between neighboring languages of course does not explain the current distribution of dominant word orders: the first language was probably structured using the Subject-Verb-Object syntax (SVO) Gell-Mann and Ruhlen (2011), but how did alternative word orders arise out of that? In this paper, we explore a novel hypothesis about the role of within-language word order variation:\n\nThe Wov2Lex Hypothesis\n\nWord order variation facilitates the acquisition of lexical semantics. The Wov2Lex hypothesis has considerable support from human language acquisition studies. Input variability is known to facilitate both first and second language acquisition Sinkeviciute et al. (2019). Aguilar et al. (2018), for example, found that object variability facilitates new word learning. Raviv et al. (2022) synthesized the above work, concluding that: “An effective way of improving generalization is to expose learners to more variable (and thus often more representative) input. More variability tends to make initial learning more challenging but eventually leads to more general and robust performance.”\n\nWe hereby present a series of experiments to check whether the Wov2Lex hypothesis is still hold in pre-trained language models. Our experiments are inspired by Sinha et al. (2021) who pre-trained language models (LMs) on shuffled texts and compared their performance with models trained on the original one (natural word order). Sinha et al. (2021) initially argued this did not lead to significant performance drops, but in subsequent work, Abdou et al. (2022) identified limitations in Sinha et al. (2021), and they were able to show that corpus perturbation leads to much lower performance.\n\nIn this paper, we move a step from “natural word order” and “shuffled word order” toward “fixed word order”. We first probe their LMs for world knowledge. We do so through word analogies, relying on the experimental protocols and datasets provided by Garneau et al. (2021). Our results confirm previous findings: the performance of language models trained with shuffled word order exhibits only slight variations. We then pre-train our own LMs, following the protocol of Sinha et al. (2021) and Abdou et al. (2022), but on data with “fixed” word order rather than original (natural) or shuffled text.\n\nWe fix the word order by reordering the (subject, object, verb) items. We find that, surprisingly, fixing word order could lead to both performance increases or drops in different languages, while natural word order keeps more mediocre across all tested languages and relations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Languages permitting word order variations typically use this variation to encode different pragmatic distinctions, such as referentiality, discourse anchoring, etc. Slioussar (2011  ###reference_b17###). Others have argued that word order variation arises from optimization of processing (avoidance of ambiguity) and grammaticalization Levshina (2019  ###reference_b10###). Input variability is known to facilitate learning, including lexical knowledge Aguilar et al. (2018  ###reference_b2###); Raviv et al. (2022  ###reference_b14###), but we are, to the best of our knowledge, the first to study the impact of word order variability on the induction of world knowledge.\n\nSinha et al. (2021  ###reference_b15###); Hessel and Schofield (2021  ###reference_b9###) pre-trained several LMs from scratch with shuffled text, relying on various shuffling strategies, such as bi-gram order permutation. They concluded from their experiments that word order information matters little for the downstream performance Wang et al. (2018  ###reference_b19###).\n\nHowever, Abdou et al. (2022  ###reference_b1###) tried to replicate their experiment and found a limitation in their work. The shuffling strategy they used did not affect the positional encodings of input tokens, and when shuffling at the sub-word level, the downstream performance dropped significantly.\n\nWorld knowledge Clark et al. (2007  ###reference_b4###) refers to the information about the real world that individuals accumulate over time. It encompasses facts and concepts about the broader world. Word analogy Ulčar et al. (2020  ###reference_b18###) is a comparison between two things, typically on the basis of their relations. For example, “Beijing is to China as Washington is to the USA”. Word analogies can test our understanding of the world. Since language models are trained on a vast amount of text, we adopt a word analogy dataset that is extracted from Wikidata to test the relations between word order and world knowledge."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Experiments",
            "text": "To investigate the effects of word order on world knowledge, we conduct tests on language models that are trained on corpora with different word orders, including the original (natural) order, shuffled order, and fixed word order. For models trained on corpora of natural texts, we use the original BERT, mBERT Devlin et al. (2019  ###reference_b5###), and RoBERTa Liu et al. (2019  ###reference_b11###) LMs. They have 12 layers with 12 attention heads, and RoBERTa is an extended version of BERT with more pre-training corpus and training steps, i.e., pre-trained on a corpus of 160GB for 500K steps. We refer to these models trained on natural word order as “natural models”.\n\nTo assess the significance of word order in pre-training, Sinha et al. (2021  ###reference_b15###); Abdou et al. (2022  ###reference_b1###) pre-trained RoBERTa models using corpora that have been shuffled at various levels, such as corpus-level and unigram- or bigram-level, or at different phases such as before or after tokenization. These models are referred to as “shuffled models”. In this paper, we also examine their word analogies.\n\nSpecifically, we test models shuf.n1, shuf.n2, shuf.n3, shuf.n4, shuf.cps, and nopos released by Sinha et al. (2021  ###reference_b15###), which are pre-trained from scratch using the Toronto Book Corpus and English Wikipedia (16GB) on a full-scale RoBERTa base model. The models were trained for 100,000 steps over 72 hours using 64 GPUs. shuf.n1 to shuf.n4 refer to LMs pre-trained on the shuffled text at -gram level (please refer to Sinha et al. (2021  ###reference_b15###) for details). The shuf.cps refers to LMs pre-trained on an entire reshuffled corpus (i.e., each word is sampled according to its frequency). The nopos refers to the LMs being pre-trained from scratch without positional embeddings since positional embeddings encode the sequence order information.\n\nWe further pre-train RoBERTa models using corpora adhering to fixed word orders. While there are multiple approaches to defining fixed word orders—ranging from alphabetical to word frequency-based, or even based on parts of speech—our primary objective is to investigate whether natural word order yields superior representations in language models compared to fixed word order. Consequently, our primary interest lies in examining word orders that occur naturally in languages, such as SVO (Subject-Verb-Object), OVS (Object-Verb-Subject), and the like.\n\nSpecifically, we first extract the dependency tree of Wikipedia text using SpaCy, where named entities and noun chunks are merged (if it was implemented in SpaCy). Instead of traversing the dependency parse tree to avoid complex regular expressions, we use a data-driven method motivated by Word2Vec Mikolov et al. (2013  ###reference_b13###) to get -grams. We then check each -gram and extract the subject, object, and verb item, rearrange the positions of the subject, object, and verb within the extracted -grams to obtain -grams of different word orders, including SVO, SOV, VOS, VSO, OSV, and OVS. Finally, these reordered -grams are concatenated to create pre-training corpora, which are identical except for the word order. The extracted -grams, without any ordering modifications, are referred to as the natural word order corpus, i.e., fixed.ntr in Table 3  ###reference_###.\n\nTo obtain enough training text segments, we set =5 for English, German, French, Spanish, and =10 for Polish as the merge of named entities and noun chunks is not implemented for Polish in SpaCy. If there are multiple subjects/objects/verbs in the -gram, only the first one is used to avoid duplicates. In total, we obtain pre-training corpora for English, German, French, Spanish, and Polish, consisting of 2 million, 2 million, 0.5 million, 1 million, and 1.5 million -gram, respectively. Note that SVO refers to a kind of word order, and SVO refers to the corresponding trained LM.\n\nTo pre-train the language models, we set the training batch size to 16, and the accumulation step to 8. The learning rate is set to 1e-4, and we use the AdamW optimizer with a linear learning rate scheduler, with a warmup step of 1% of the total training step. We use 4 GPUs, and train"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Analysis of Natural/Shuffled Word Order",
            "text": "Since the relation between word order and word analogy has not been explored, here, we re-test the performance of existing natural and shuffled LMs by conducting experiments on WiQueen dataset. \n\nAs shown in Figure 1, compared with the natural word order shuf.ntr, nopos and R-rand result in a substantial decrease in performance, which is easy to understand. However, when comparing shuf.ntr with shuf.n1, n4 and shuf.cps, same as existing results, we also find that the permutation of word order leads to either a decrease or an increase in performance with slight differences. Several hypotheses attempt to elucidate the results. For instance, it might be that the tasks tested do not require word order information, or that the evaluated language models do not rely heavily on word order information. Another one suggests that the shuffling methods employed might not destroy essential word order information. However, these hypotheses have yet to be widely validated."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Analysis of Fixed Word Order",
            "text": "Building upon the work in Sinha et al. (2021  ###reference_b15###); Abdou et al. (2022  ###reference_b1###), we extend the exploration of word order from “natural” and “shuffled” to “fixed” word order. We pre-train RoBERTa from scratch using pre-training corpora that have been reordered to different word orders. Across these corpora, all input items are the same except the positions of the subject, verb, and object. With these experiments, we try to answer the following questions:  \nQ1: Does a particular word order exhibit superiority or inferiority to the others?  \nQ2: Does the use of hybrid word orders (i.e., natural word order) confer any advantages over the use of a single word order?  \nQ3: Does the question Q1 hold consistent across different languages?  \nIn answer to Q1, the results are positive. There is a clear distinction between the best and worst fixed word orders across all examined languages. Besides, the SVO performs the best in English and French but worst in Spanish. On the other hand, VOS performs the worst in English, German and French. For the tested five languages, the first three (English, German, French) demonstrates similar trends, and show completely different trends with Spanish or Polish.  \nIn response to question Q2, the findings are in the negative. Our analysis indicates that integrating a diverse range of word orders (i.e., natural word order), doesn’t offer marked enhancements over individual, fixed word orders. However, it’s important to highlight that, relative to other word sequences, the natural word order consistently exhibits more resilience across languages, maintaining a middle-ground performance.  \nIn relation to question Q3, the answer leans toward the negative. The data suggests the presence of both superior and inferior word orders among the seven considered (six pre-defined and one natural). However, this hierarchy varies across languages. Specifically, German and French demonstrate tendencies more aligned with English than do Polish and Spanish. Consequently, these findings do not entirely corroborate the Wov2Lex hypothesis.  \nThese empirical findings underscore the significance of word order, challenging prior results detailed in Section 3.1  ###reference_### (i.e., the permutation of word order lead a marginal decrease or increase in performance). While currently it’s challenging to provide definitive explanations for these observations, we hypothesize that language models and linguistics may possess different ways to comprehend and process natural language."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Other Findings",
            "text": "We also have additional empirical findings that are not related to word order. Referring to Figures 1  ###reference_### and 2  ###reference_###, with detailed data provided in Tables 1  ###reference_### and 3  ###reference_###. The performance may be expected to be RoBERTa  BERT  fixed.ntr, where  signifies superior performance.\nBecause RoBERTa and BERT have the same model architecture111Although BERT also uses next sentence prediction as a pre-training task, it is claimed that it matters little., and compared with the original BERT, RoBERTa is trained with more training corpus and longer training time, and fixed.ntr is trained with a smaller corpus and fewer steps.\nContrary to expectations, the results reveal a sequence of fixed.ntr  BERT  RoBERTa. Illustrating with English data, the performance metrics are .\nThis discrepancy might arise because the Wikipedia corpus alone suffices for the WiQueen dataset, given its derivation from Wikidata. Introducing additional corpora, like the Toronto Book Corpus, could potentially be detrimental to the data, leading to issues such as catastrophic forgetting."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Word Order on Different Relations",
            "text": "In order to examine the impacts of “fixed” word order models on analogy relations, such as the relation “capital” in the example “Beijing is to China as Washington is to the United States”, we evaluate the eight most frequently occurring relations.\nAs shown in Figure 3  ###reference_###, the natural word order demonstrates superior performance on certain relations such as “follows”, “followed by”, and “capital of” even though language models have lower scores on these relations. However, it performs poorly on the relations “P1001” and “P159”. Meanwhile, SVO is better at relations such as “capital” and “country”, and the VOS has slightly better performance on the relation “P1001”.\nOn the other hand, models that start with S such as SOV and SVO, exhibit the poorest performance on the “name after” relation.\nThe results indicates that models trained on different single word orders (SVO, SOV, VOS, VSO, OVS, OSV) and mixture word order (natural) exhibit varying abilities on different relations.\n###figure_3###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "The objective of this paper is to investigate the influence of word order on world knowledge, including verifying whether Wov2Lex hypothesis is supported in pre-trained language models. To accomplish this, we pre-trained language models from scratch on corpora with six fixed word orders, and then assessed their performance on an analogy dataset. The findings reveal that natural word order does not consistently outperform the six tested fixed word orders. This suggests that the Wov2Lex hypothesis is not entirely supported by our empirical data. We hypothesize that this disparity arises from the distinct processing methods used in linguistics versus language models."
        }
    ]
}