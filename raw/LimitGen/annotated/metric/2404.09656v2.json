{
    "title": "Learn Your Reference Model for Real Good Alignment",
    "abstract": "The complexity of the alignment problem stems from the fact that existing methods are considered unstable. Reinforcement Learning from Human Feedback (RLHF) addresses this issue by minimizing the KL divergence between the trained policy and the initial supervised fine-tuned policy (SFT) to avoid generating out-of-domain samples for the reward model (RM). Recently, many methods have emerged that shift from online to offline optimization, reformulating the RLHF objective and removing the reward model (DPO, IPO, KTO). Despite eliminating the reward model and the challenges it posed, these algorithms are still constrained in terms of closeness of the trained policy to the SFT one. In our paper, we argue that this implicit limitation in the offline optimization methods leads to suboptimal results. To address this issue, we propose a class of new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update the reference policy during training. With this straightforward update approach, we demonstrate the effectiveness of the new paradigm of language model alignment against the classical one on the Anthropic-HH and Reddit TL;DR datasets.\nMost notably, when automatically comparing TR methods and baselines side by side using pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference in win rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO.\nFinally, by assessing model response ratings grounded on criteria such as coherence, correctness, helpfulness, and harmlessness, we demonstrate that our proposed methods significantly outperform existing techniques.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The alignment of Large Language Models (LLMs) is an increasingly pressing issue in contemporary Natural Language Processing (NLP). The primary goal is to train models that are not only effective but also safe and controllable, which are qualities emphasized in recent research (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2023; Zhao et al., 2023). Achieving such safety typically involves fine-tuning LLMs to favor the generation of outputs that exhibit the desired behaviors.\n\nTraditionally, the alignment of language models hinges upon the training objective, defined as:\nwhere  is the collection of training data,  is the policy being optimized,  is the reference model (usually a supervised fine-tuned LM with the SFT policy), and  is the Reward Model (RM) trained in line with human preferences (Bradley and Terry, 1952).\n\nInitial attempts to address the issue of language model alignment employed Reinforcement Learning (RL) methods, where an RM, informed by human preferences, was developed. Subsequently, the LLM was tuned to produce outcomes aimed at maximizing the RM’s values (Bai et al., 2022), (Schulman et al., 2017). The current methodology has evolved to include a more intricate reparametrization of this procedure. For example, Direct Preference Optimization (DPO) by Rafailov et al. (2023) dispenses with the step of training the RM and directly optimizes the LLM by maximizing the training data likelihood as per the following loss function:\nwith the dataset  consisting of tuples , in which  represents a text prompt, while  and  stand for the human annotator’s preferred and less preferred continuations, respectively.\n\nIdentity Preference Optimization (IPO) by Azar et al. (2023) slightly reformulates the original optimization task and replaces maximization of the reward with maximization of the probability that one text is better than the other. As a result, they obtain a different loss function:\n\nEthayarajh et al. (2024) enhances the DPO method by adopting a Kahneman and Tversky (1979) principle that losses outweigh equivalent gains. The Kahneman-Tversky Optimization (KTO) loss function can be defined as:\nwhere ,  and  are coefficients controlling the degree of loss aversion (Kahneman and Tversky, 1979).\n\nThis practice prompts us to question why the reference model remains static during training. For instance, consider a model aligned using a dataset  with a given reference policy. Then we collect more data, , which includes human preferences. The DPO approach suggests that, for further alignment with , the same reference model from the  training should be used (i.e., the SFT policy), even though the updated policy may now provide a more apt reference model. The same logic holds for IPO and KTO.\n\nWith this in mind, we introduce a novel concept to the training process for alignment algorithms, called the Trust Region (TR) approach. This includes methods such as TR-DPO, TR-IPO, and TR-KTO, where the reference policy is dynamically updated during training. This can be implemented either by softly integrating  into  using a weighted approach, or by outright replacing the reference policy with  after a predetermined number of steps. Our work’s contributions are as follows:\n\nWe introduce a novel approach for learning Preference Optimization using the Trust Region (TR) approach. This method outperforms the traditional ones, such as DPO, IPO, and KTO. Specifically, in model pair comparisons using GPT-4, TR-DPO with an  of 0.6 shows a 10% higher win rate than DPO for the Pythia 2.8B model. In the relative comparison of methods with SFT, the results of which are presented in Figure 1, TR-DPO with  of 0.6 shows a win rate 3% higher than DPO in comparison with the same SFT policy. This method’s efficacy is tested across two natural language generation tasks and three different model sizes.\n\nWe analyze the dependency of HC metrics, KL divergence, and response diversity on the  parameter in TR and classical methods. This analysis shows that policies trained by both our methods and classical ones diverge from the original SFT policy as  decreases, yet the proposed methods demonstrate higher values of HC metrics at equivalent values of KL divergence."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The alignment process is crucial for creating chat assistants and improving user satisfaction by training the model to generate safe, helpful, and correct responses (Bai et al.,, 2022  ###reference_b5###). A fundamental method in adapting language models, Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al.,, 2020  ###reference_b26###), has played an instrumental role in the success of groundbreaking LLMs such as GPT-3.5 (Ouyang et al.,, 2022  ###reference_b19###; Ye et al.,, 2023  ###reference_b30###), GPT-4 (OpenAI et al.,, 2024  ###reference_b18###), and Llama 2 (Touvron et al.,, 2023  ###reference_b27###). This method consists of three stages: Supervised Fine-Tuned Model (SFT Model) training, Reward Model (RM) training, and the Proximal Policy Optimization (PPO) algorithm (Schulman et al.,, 2017  ###reference_b25###). PPO is used to train the model, or “policy” if following RL terminology, initialized by the SFT model, to maximize the expected reward of the model’s responses and reduce the Kullback-Leibler (KL) divergence between the trained policy and the SFT policy, as specified in Equation 1  ###reference_###.\nHowever, RLHF is not without its downsides, such as hyperparameter sensitivity, instability, implementation complexity, and high resource demand. Direct Preference Optimization (DPO) (Rafailov et al.,, 2023  ###reference_b21###) solved some problems by redefining the optimization problem and expressing a loss function to train the model directly, without RL algorithms. Similarly, Sequence Likelihood Calibration (SLiC) (Zhao et al.,, 2023  ###reference_b31###) suggested using a margin loss between the probabilities of  and , which are responses generated by the SFT model.\nAn extension of these methods was proposed with -PO (Azar et al.,, 2023  ###reference_b4###), in which the focus shifted from maximizing reward to maximizing a certain probability function that  is better than . It was demonstrated that a specific case of this method, called IPO, could be more stable and resistant to overfitting than DPO.\nAnother significant alignment alternative, Kahneman-Tversky Optimization (KTO) (Ethayarajh et al.,, 2024  ###reference_b9###), seeks to accentuate human utility rather than solely relying on preference data. Rejection Sampling Optimization (RSO) incorporates features of both DPO and SLiC, suggesting that samples can be received from the optimal policy using rejection sampling. They can then be labeled by the reward model, and subsequently applied to train the policy using diverse loss functions (Liu et al., 2024a,  ###reference_b15###).\nIn the mentioned studies and the work of Wang et al., (2024  ###reference_b28###), it was noticed that the term ensuring the trained policy’s closeness to the SFT policy does not significantly impact the final metrics. These observations spurred our idea to eliminate this limitation in the widely used DPO algorithm. Our proposed strategy involves updating the reference policy in a manner similar to updating the target network in value-based reinforcement learning algorithms (Awheda and Schwartz,, 2013  ###reference_b3###). The following section delves deeper into how this has been accomplished."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "The alignment objective from Equation 1 implies having a regularization with a fixed reference model for training. This objective involves maximizing the reward without moving far from the reference model. However, this requirement may seem too synthetic, which is why there is a desire to move from a static reference model to an updated one during training.\n\nIn this paper, we update the parameters of the reference policy during the training phase using two primary methods. The first is the soft update, described as a process where a weighting factor determines the rate at which the updates influence the reference policy. Since both the current policy and the reference policy are initialized from the same set of parameters, performing a soft update is justified by previous studies.\n\nThe second approach is the hard update, executed at every set number of training steps, which indicates a direct substitution of the reference model with the updated policy after a specified number of training iterations. This method provides more significant and episodic adjustments to the reference policy, promoting larger jumps in the model’s learning trajectory.\n\nPolicy updates can be applied to any LM alignment methods that maintain an implicit constraint on closeness to the reference policy. In this work, we experiment with the three most popular methods possessing the above-mentioned property: DPO, IPO, and KTO. We then propose a new class of methods called Trust Region (TR) methods: TR-DPO, TR-IPO, TR-KTO.\n\nWhile the proposed change is straightforward to implement, updating the reference policy raises the question of how it changes the training objective. One way to think about it is to derive a connection to TR optimization methods. Consider the objective where a previously obtained policy is considered. Naive optimization with such an objective will lead to degeneration of the policy. However, TR methods could be seen as approaches between the vanilla objective from Equation 1 and the TR variant from Equation 7, since we can control the frequency of changes in the reference policy.\n\nSuch a connection explains the selection of TR parameters. A small frequency of updates leads to rare reference policy updates, making the trained policy remain in the region close to the reference policy without large updates. This is equivalent to vanilla training with minimal reference model adjustments. In contrast, frequent updates make it possible for a policy to move far away from the reference one, as measured by the divergence from the initial model. This is similar to a pure TR setup with frequent alterations to the reference model."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate each training configuration on two datasets: Anthropic-HH and Reddit TL;DR summarization. The Anthropic-HH dataset contains 160,800 training and 8,552 validation examples of pair preferences for SFT and preference learning. The Reddit TL;DR dataset includes 73,396 training and 26,198 validation examples, post-duplicate removal, of human preference judgments on model-generated summaries. We select only the uniquely preferred summaries for SFT, resulting in 41,947 training and 11,941 validation examples.\n\nWe employ a range of Pythia models, with sizes of 2.8B, 6.9B, and 12B, serving as pre-trained base models. An SFT policy checkpoint trains similarly to previous works on preferred texts for each dataset. We explore two main update strategies, each adaptable to different base alignment methods (e.g., DPO, IPO, KTO) as outlined in Section 2. (1) Soft Update: This strategy applies a weighting factor, with experiments conducted for various α values, to progressively merge the current policy with its reference policy at each training step. Notably, the TR variants (TR-DPO, TR-IPO, and TR-KTO) become equivalent to their base methods (DPO, IPO, and KTO respectively) when α=0. The notation for soft update is based on the base method, such as TR-DPOα, TR-IPOα, and TR-KTOα. (2) Hard Update: This strategy involves distinct experiments where the reference model updates at fixed intervals τ to evaluate the efficacy of varying update frequencies. The notation for the hard update method also depends on the base method and is similarly denoted as TR-DPOτ, TR-IPOτ, and TR-KTOτ. Further details on the experimental setup and hyperparameters are given in Appendix A.\n\nFollowing established approaches, we employ a comprehensive evaluation framework to assess the performance of various TR methods’ configurations against the corresponding original baselines.\n\nAutoSxS evaluation: We employ the AutoSxS framework, using ‘GPT-4-0125-preview‘ as a proxy for human evaluators (detailed prompt information can be found in Appendix H), to analyze preferences across 500 samples from the test set. This comparison includes various configurations of TR-DPO, TR-IPO, TR-KTO, and their respective traditional counterparts, DPO, IPO, and KTO, maintaining consistent generation parameters: temperature set to 1.0, top_k at 40, top_p at 0.7, and max_new_tokens at 512.\n\nModel dynamics and policy divergence: To investigate the influence of the proposed methods on KL divergence and generation diversity, we measure Self-BLEU and the KL divergence between the original and learned policies using the full Anthropic-HH test set for the Pythia 2.8B model. Additionally, we compare these metrics with the previously described mean HC metrics. This analysis allows for a detailed exploration of how our proposed methods modify the dynamics compared to simply changing the update strategies, and evaluates the trade-off between alignment and diversity."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We evaluate each training configuration on two datasets: Anthropic-HH111https://huggingface.co/datasets/Anthropic/hh-rlhf (Bai et al., 2022) and Reddit TL;DR summarization222https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences (Stiennon et al., 2020). The Anthropic-HH dataset contains 160,800 training and 8,552 validation examples of pair preferences for SFT and preference learning. The Reddit TL;DR dataset includes 73,396 training and 26,198 validation examples, post-duplicate removal, of human preference judgments on model-generated summaries. We select only the uniquely preferred summaries for SFT, resulting in 41,947 training and 11,941 validation examples.\n\nWe employ a range of Pythia models (Biderman et al., 2023), with sizes of 2.8B, 6.9B, and 12B, serving as pre-trained base models. An SFT policy checkpoint trains similarly to Rafailov et al., (2023); Liu et al., 2024b on preferred texts for each dataset. We explore two main update strategies, each adaptable to different base alignment methods (e.g., DPO, IPO, KTO) as outlined in Section 2. (1) Soft Update: This strategy applies a weighting factor, with experiments conducted for, to progressively merge the current policy with its reference policy at each training step. Notably, the TR variants (TR-DPO, TR-IPO, and TR-KTO) become equivalent to their base methods (DPO, IPO, and KTO respectively) when. The notation for soft update is based on the base method, such as TR-DPOα, TR-IPOα, and TR-KTOα. (2) Hard Update: This strategy involves distinct experiments where the reference model updates at fixed intervals to evaluate the efficacy of varying update frequencies. The notation for the hard update method also depends on the base method and is similarly denoted as TR-DPOτ, TR-IPOτ, and TR-KTOτ. Further details on the experimental setup and hyperparameters are given in Appendix A.\n\nFollowing established approaches (Rafailov et al., 2023; Liu et al., 2024b), we employ a comprehensive evaluation framework to assess the performance of various TR methods’ configurations against the corresponding original baselines.\n\nAutoSxS evaluation: We employ the AutoSxS framework, using ‘GPT-4-0125-preview‘ as a proxy for human evaluators (detailed prompt information can be found in Appendix H), to analyze preferences across 500 samples from the test set. This comparison includes various configurations of TR-DPO, TR-IPO, TR-KTO, and their respective traditional counterparts, DPO, IPO, and KTO, maintaining consistent generation parameters: temperature set to 1.0, top_k at 40, top_p at 0.7, and max_new_tokens at 512.\n\nModel dynamics and policy divergence: To investigate the influence of the proposed methods on KL divergence and generation diversity, we measure Self-BLEU (Zhu et al., 2018) and the KL divergence between the original and learned policies using the full Anthropic-HH test set for the Pythia 2.8B model. Additionally, we compare these metrics with the previously described mean HC metrics. This analysis allows for a detailed exploration of how our proposed methods modify the dynamics compared to simply changing the, and evaluates the trade-off between alignment and diversity (Kirk et al., 2023; Ahmadian et al., 2024; Wang et al., 2023)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance Comparison on the Two Tasks",
            "text": "For a comprehensive exploration of update strategies across the entire range of  and , we employed the TR-DPO approach using the Pythia 2.8B model. The base model was configured with , demonstrating an optimal trade-off between Human-Centric metrics and KL divergence, as detailed in Section 4.4. This setup allows for a systematic assessment of the impact of each strategy under varied conditions, effectively comparing the effects of different weighting factors  and update intervals .\n\nFigures 3(a) and 3(b) illustrate that both the soft and hard update strategies of TR-DPO enhance performance when compared to the traditional DPO method for the Pythia 2.8B model on the Anthropic-HH and Reddit TL;DR datasets. TR-DPO with  values between 0.5 and 0.6 consistently outperforms settings with other  values. Conversely, the benefits of TR-DPOτ become more pronounced as  increases from 64 to 512.\n\nFor both datasets, the parameters  and  for soft and hard updates, respectively, pass the Fisher statistical test with the Pythia 2.8B model size. Detailed results for the GPT comparison are presented in Appendix Table 3.\n\nWe assert that these parameters are near-optimal for most scenarios, and can be applied to various methods, tasks, and model sizes. This makes it possible to keep the computation budget the same for the proposed methods and baselines, maintaining the purity of the experiments in our work and facilitating hyperparameter tuning for further practical use of our methods. The results of the experiments are presented in Table 1 and include three methods (TR-DPO, TR-KTO), two tasks (Anthropic HH and Reddit TLDR), and three model sizes (2.8B, 6.9B, and 12B). For each method, an optimal  was selected and used in training the models (see Section 4.4 for more details). Examples of the generations of the models can be found in Appendix G."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Detailed Human-Centric Analysis",
            "text": "Figure 4 presents standardized absolute scores for HC metrics, including coherence, correctness, level of detail, helpfulness, and harmlessness, as evaluated on the Anthropic-HH dataset. For TR-DPO, the optimal values are between 0.5 and 0.6. Performance degradation is observed as deviates from this optimal range, particularly at higher values such as 0.8, where the model exhibits training instability and a tendency to generate repetitive words. A similar trend is observed for frequent updates when is equal to 64 (see Appendix Figure 7).\n\nWhile the results show a noticeable improvement in metrics such as coherence, correctness, helpfulness, and harmlessness, there is no improvement in the level of detail. This can be explained by the fact that this criterion depends on the length of generated texts. As previously noted in Park et al. (2024), DPO tends to increase the text length. In Appendix Figure 16, we show that TR-DPO is less subject to this problem, as it tends to produce shorter texts for the comparable levels of KL divergence.\n\nSimilarly, both TR-IPO and TR-KTO show statistically confirmed enhancements over their corresponding baselines. A detailed analysis of these methods, including specific parameters, is provided in Appendix C."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Why Just Lowering  Is Not Enough",
            "text": "It is clear that updating the reference policy of the trained one increases the KL divergence between the trained policy and the original SFT. Equation 1 shows that the same effect can be achieved by reducing the coefficient of the constraint on the KL divergence. The same parameter is also present in equations 2, 3, and 4. Figure 6(a) demonstrates the dependency of the average value of the HC metrics and the KL divergence with the SFT on the parameter for the DPO, TR-DPOα, and TR-DPOτ methods, using the Pythia 2.8B model on the Anthropic-HH dataset. We note that the best value of HC metrics is achieved at a certain point, which we use for DPO in the experiments from Sections 4.2 and 4.3. The value of this parameter for other methods was chosen the same way (see the graphs in Appendix D). One notable result is that, after reaching a peak value, HC metrics for DPO fall faster than for its modified versions, allowing the TR methods to achieve better results.\n\nAs highlighted by recent research, models with higher alignment tend to produce less diverse generations. The dependency of HC metrics on Self-BLEU is shown in the Appendix Figure 15 and is similar to the dependency on KL divergence. Based on these graphs, we can affirm that the TR methods show higher values of HC metrics at the same level of response diversity (or KL divergence).\n\nTo understand this behavior, consider the gradient dynamics of DPO and the update strategies of both TR-DPO versions during the training process. The gradient of the loss function, specified in Equation 2, is as follows: \n\nThe absolute values of term act as a weighting factor, signifying the disparity between the outcomes of the trained policy and the reference model. For instance, the gradient magnitude becomes small when the probability assigned to the preferred outcome by the trained policy is higher than that of the reference model and conversely for the less preferred outcome.\n\nThe gradient scales are depicted in Figure 6, smoothed for clarity. We hypothesize that there is a relationship between these observed gradient scales and the training dynamics. Specifically, a higher parameter in soft updates and a lower parameter in hard updates both result in increased gradient scales, which may be indicative of greater training instability (see Appendix D.3 for more details). This has the potential to cause significant deviations from the stability of the SFT policy, leading to degradation. Conversely, updating the reference policy allows for more deviation from the SFT policy while maintaining stable performance compared to the standard DPO method. Similar behavior is observed for other methods as presented in Appendix D, Figures 12(b) and 13(b).\n\nThus, simply lowering the coefficient is not sufficient to reach the optimal results. By updating the reference policy, TR methods achieve higher metric values and maintain stable performance compared to standard alignment techniques, indicating the importance of incorporating such update strategies to enhance training dynamics."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "This paper introduces a new class of language model alignment methods called Trust Region (TR). TR-DPO, TR-IPO and TR-KTO stand out from classical offline optimization methods by updating the reference policy during the training process. The \"Trust Region\" designation reflects our approach’s foundational principle of allowing models to explore beyond the initial SFT policy to achieve improved alignment convergence. The effectiveness of this strategy is likely based on the nuanced dynamics of the loss function’s gradient behavior. We validate the proposed technique on a variety of tasks and model sizes. The results show that our modifications improve performance in the majority of scenarios. This underscores the universality and reliability of our approach.\nLimitations and future work. In our study, we propose a new paradigm for language model alignment and demonstrate its effectiveness on two of the most popular datasets. Future research could explore how the proposed method generalizes to other domains, modalities, and dataset sizes, especially small datasets. In this work, we show that models trained by our method suffer less from an increase in response length, but the justification of this behavior is beyond the scope of our work and requires further investigation. Finally, our results are based on an automatic evaluation by GPT-4. This raises the question of how suitable such evaluation is for assessing alignment techniques, and whether there could be more suitable evaluation methods.\nSocietal impacts. The methods proposed in this article, like other alignment methods, can be used to create unsafe assistants. We are only warning about this possibility, but our research is aimed at creating helpful and harmless assistants, and we expect that our methods will be used for the same purposes."
        }
    ]
}