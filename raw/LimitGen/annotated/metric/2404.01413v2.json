{
    "title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
    "abstract": "The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops proposed that such loops would lead to a phenomenon termed model collapse, under which performance progressively degrades with each model-data feedback iteration until fitted models become useless. However, those studies largely assumed that new data replace old data over time, where an arguably more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse?\n\nWe empirically study this question by pretraining sequences of language models on text corpora. We confirm that replacing the original real data by each generation’s synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse; these results hold across a range of model sizes, architectures, and hyperparameters. We obtain similar results for deep generative models on other types of real data: diffusion models for molecule conformation generation and variational autoencoders for image generation. \n\nOur work provides consistent empirical and theoretical evidence that data accumulation avoids model collapse.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large-scale generative models such as GPT-4 (Achiam et al., 2023), DALL-E (Ramesh et al., 2022), and Stable Diffusion (Rombach et al., 2022) has revolutionized the field of artificial intelligence. These models, trained on vast web-scale datasets, exhibit remarkable capabilities in generating text, images, and other media (Brown et al., 2020; Saharia et al., 2022). However, as these models become more widely used, an increasing amount of generated data populates the web. This raises a critical question: what are the consequences of training generative models on datasets containing their own outputs?\n\nRecent studies have investigated this question, revealing that training generative models on their own outputs can cause the performance of such models to progressively degrade with each model-fitting iteration, eventually rendering newer models useless (Hataya et al., 2023; Martínez et al., 2023a; Shumailov et al., 2023; Alemohammad et al., 2023; Martínez et al., 2023b; Bertrand et al., 2023; Briesch et al., 2023; Dohmatob et al., 2024a; b) (see Appendix A for review and discussion of prior work). This phenomenon was consequently labeled model collapse. Model collapse warns that democratizing access to generative models runs the risk of polluting the very data necessary to train future iterations of generative models.\n\nTo better understand this phenomenon, many prior works have considered a setup that assumes each model’s generated data replaces previous data. In theory, this leads to very natural comparisons across generations as the total number of training points for each model remains fixed. In practice, subsequent generations of LLMs are often trained with increasing data over time – e.g., 1.4 trillion tokens for Llama 1 (Touvron et al., 2023a), 2 trillion for Llama 2 (Touvron et al., 2023b), 15 trillion for Llama 3 – in which presumably both human-generated and machine-generated data are accumulating in training sets collected from the internet. It was noted in some of those works (Hataya et al., 2023; Martínez et al., 2023a; Alemohammad et al., 2023; Bertrand et al., 2023; Dohmatob et al., 2024b) that model collapse can be either slowed down or negated by mixing in clean data with the generated data.\n\nTo that end, in this work we study the effect of accumulating data on model collapse, rather than replacing data. Our data-accumulating setting is, in some sense, maximally pessimistic: it considers a hypothetical future where synthetic data are uncontrollably dumped on the internet to be vacuumed up for training the next iteration of generative models. Nevertheless, we find that model collapse is avoided when accumulating data.\n\nWe begin by studying model collapse experimentally with deep generative models trained on realistic data: transformers on causal language modeling (Sec. 2.1), diffusion models on molecular conformation (Sec. 2.2), and variational autoencoders on images (Sec. 2.3). \n\nTo understand why replacing data and accumulating data have different consequences for model collapse, we turn to an analytically tractable framework of a sequence of linear models, each trained on synthetic outputs generated from the previous-iteration’s fitted linear model (Mobahi et al., 2020; Dohmatob et al., 2024a). Our work suggests that data accumulation may be robust to model collapse and emphasizes the importance of considering accumulating data and other real-world data dynamics in the analysis of model collapse in generative models trained on web-scale data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Accumulating Data Avoids Model Collapse in Deep\nGenerative Models",
            "text": "We first investigate model collapse experimentally in several classes of generative models. Here, and for the remainder of this manuscript, the term model collapse refers to notably worsening error over increasing iterations of the model-data loop, while avoiding model collapse refers instead to bounded error over such iterations. To test the effect of accumulating data on model collapse, we compare accumulating data against replacing data. We use three diverse experimental setups of causal transformers, diffusion models, and variational autoencoders trained on real text, molecular conformation, and image datasets, respectively. We find that replacing data yields model collapse for all models and all datasets, whereas accumulating data avoids model collapse.\n\nWe first train causal transformers on text data. Specifically, we pretrain 9M parameter GPT-2 and 12M, 42M and 125M parameter Llama2 language models for a single epoch on TinyStories, a 470M token GPT-3.5/4-generated dataset of short stories at a kindergarten reading level. For each model-fitting iteration, we sample a new dataset of the same size as TinyStories from the previous iteration’s language model and then either replace or concatenate the previous dataset with the newly generated dataset. In each model-fitting iteration, we then pretrain a newly initialized model on the replaced or concatenated dataset from the previous iteration. We experiment with sampling the new datasets using temperatures or . We chose this combination of architectures, scales, dataset, and sampling because the setup necessitates pretraining multiple iterations of language models – a computationally costly endeavor – but we also wish to study realistic conditions where generative models are high-performing and generate diverse outputs. Because small language models (below 10M parameters) pretrained on TinyStories were shown to be able to generate coherent-albeit-simple English sentences, this choice of architectures, scales, dataset and temperature hopefully strikes a good balance between being representative, being diverse and being computationally feasible.\n\nWe ablate for several additional potential confounds beyond generation temperature. First, when accumulating data, subsequent model iterations are trained on larger datasets than when replacing data. To control for this, we also perform experiments in which data is replaced, but the size of the (fully synthetic) dataset is grown to match the training set size in the accumulation regime. Second, a possible concern could be that degrading performance when replacing data could be due to low model performance in iteration 1 (and thus the quality of the first synthetic dataset). We control for this by varying the amount of training performed in iteration 1 only and find that this has no significant impact. Lastly, we find that our results are also consistent across varying dataset sizes and training epochs.\n\nWe lastly train sequences of variational autoencoders (VAEs) on CelebA, a dataset of 200k images of human faces split between train and test sets, chosen as a balance between being a realistic dataset with many samples, color images and resolution, and computational feasibility of training multiple iterations of models on accumulating data. The loss is the standard VAE loss: reconstruction error plus the KL divergence between the encoder’s output Gaussian and the isotropic Gaussian prior.\n\nInterestingly, unlike language modeling, the error of accumulating data does increase with the number of iterations (albeit much more slowly than with replacing data). We also note that Martínez et al. found slightly contradictory evidence, specifically that a different architecture on a much smaller dataset exhibits fast performance deterioration even with accumulating data. Understanding under what conditions and why these discrepancies exist is an interesting direction we leave for future research."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Transformer-Based Causal Language Modeling",
            "text": "We first train causal transformers (Vaswani et al., 2017) on text data. Specifically, we pretrain 9M parameter GPT-2 (Radford et al., 2019) and 12M, 42M, and 125M parameter Llama2 (Touvron et al., 2023b) language models for a single epoch on TinyStories (Eldan & Li, 2023), a 470M token GPT-3.5/4-generated dataset of short stories at a kindergarten reading level. For each model-fitting iteration, we sample a new dataset of the same size as TinyStories from the previous iteration’s language model and then either replace or concatenate the previous dataset with the newly generated dataset. In each model-fitting iteration, we then pretrain a newly initialized model on the replaced or concatenated dataset from the previous iteration. We experiment with sampling the new datasets using temperatures or . We chose this combination of architectures, scales, dataset, and sampling because the setup necessitates pretraining multiple iterations of language models – a computationally costly endeavor – but we also wish to study realistic conditions where generative models are high-performing and generate diverse outputs. Because small language models (below 10M parameters) pretrained on TinyStories were shown to be able to generate coherent-albeit-simple English sentences (Eldan & Li, 2023), this choice of architectures, scales, dataset, and temperature hopefully strikes a good balance between being representative, being diverse, and being computationally feasible.\n\nTable 1 shows samples of generated texts for GPT2 (9M) and Llama2 (125M) models at model-fitting iterations 3-5 when both accumulating and replacing data, as well as iterations 8-10 (replacing only).\n\nWe ablate for several additional potential confounds beyond generation temperature. First, when accumulating data, subsequent model iterations are trained on larger datasets than when replacing data. To control for this, we also perform experiments in which data is replaced, but the size of the (fully synthetic) dataset is grown to match the training set size in the accumulation regime. We find that model performance still degrades (albeit at a lower rate). This is shown in Appendix C, Table 2, right-most column.\n\nSecond, a possible concern could be that degrading performance when replacing data could be due to low model performance in iteration 1 (and thus the quality of the first synthetic dataset). We control for this by varying the amount of training performed in iteration 1 only and find that this has no significant impact.\n\nLastly, we find that our results are also consistent across varying dataset sizes and training epochs. These ablations are discussed in Appendix F."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Diffusion Models on Molecular Conformation Data",
            "text": "Experiments We next train sequences of diffusion models on molecular conformation data. Specifically, we train GeoDiff (Xu et al., 2022), a geometric diffusion model for molecular conformation generation, on the GEOM-Drugs (Axelrod & Gomez-Bombarelli, 2022) dataset. We down-sample the training split of GEOM-Drugs to molecular conformations, which we use as our initial training set, and perform diffusion steps for each prediction. For the loss, we use the standard loss used by GeoDiff: a weighted variational lower bound to the conditional likelihood; for more details, see Xu et al. (2022)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Variational Autoencoders on Image Data",
            "text": "We lastly train sequences of variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) on CelebA (Liu et al., 2015), a dataset of 200k images of human faces, chosen as a balance between being a realistic dataset with many samples, color images and resolution, and computational feasibility of training multiple iterations of models on accumulating data. The loss is the standard VAE loss: reconstruction error plus the KL divergence between the encoder’s output Gaussian and the isotropic Gaussian prior. See Appendix D for more experimental details.\n\nWe find that replacing data at each iteration again exhibits model collapse, and each iteration yields lower quality and less diverse generated faces until all model generations represent a single mode as shown in the left panel of Figure 6. In contrast, accumulating data at each iteration significantly slows model collapse. While the diversity of generations does go down as compared in the middle and right panel of Fig. 6, it still represents major axes of variation in the dataset, such as gender, but no longer seems to generate other details, along more minor axis of the data manifold, such as glasses and accessories. We discuss further analysis of VAE reconstructions in Appendix D.\n\nInterestingly, Martínez et al. (2023a) found slightly contradictory evidence, specifically that a different architecture on a much smaller dataset exhibits fast performance deterioration even with accumulating data. Understanding under what conditions and why these discrepancies exist is an interesting direction we leave for future research."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Accumulating Data Avoids Model Collapse in Linear Models",
            "text": "To gain mathematical understanding and intuition, we employ an analytical framework introduced in prior work (Mobahi et al., 2020; Dohmatob et al., 2024a) to understand the difference between data accumulation and data replacement. The framework considers a sequence of linear models that are fit to the synthetic data sampled from the linear generative model based on the previously fit linear models.\n\nThe content in this section relies heavily on the framework and pioneering contributions of Dohmatob et al. (2024a). Our contribution is to study a different way to use synthetic data in training, namely accumulate, which seems to better align with certain real-world considerations. We use the same framework to analyze some other ways that synthetic data might have been used, such as replace.\n\nWe adapt notations from Dohmatob et al. (2024a). Define the distribution on given by:\n\nThe positive integer is the input-dimension, the matrix is the true covariance structure of the input, the vector is the true linear relationship used to generate the original data and the scalar is the level of label noise. We start at iteration with initial independent data points each following , that is, for each. We form the design matrix with as rows. We also form the vectors and with -th coordinate and respectively. In whatever follows, we will assume that has full column rank, i.e., is invertible and the model is underparameterized.\n\nWe generate synthetic data from the following sequence of distributions where is the number of iterations. The scheme is outlined as follows.\n\nFor :\nAccumulating Covariates/Features:\nAccumulating Targets: , where\nFit linear model:\nSample synthetic data for the next iteration: , where\n\nFor :\nAccumulating Covariates/Features:\nAccumulating Targets:\nFit linear model:\nSample synthetic data for the next iteration: , where\n\nHere, for a matrix with full column rank, is the Moore-Penrose pseudo-inverse of. The noise terms are independent of each other and of the covariates/features. Since has full column rank, so does for every.\n\nTo reiterate a comment made previously by Dohmatob et al. (2024a), although we present our results in the context of ordinary linear regression, our analysis can be readily extended to ridge regression and the kernel setting (Caponnetto & De Vito, 2007; Simon et al., 2021; Cui et al., 2021; Wei et al., 2022). We focus here on a simple useful model for studying model collapse."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Notation and Preliminaries",
            "text": "We adapt notations from Dohmatob et al. (2024a). Define the distribution  on  given by :\nThe positive integer  is the input-dimension, the matrix  is the true covariance structure of the input , the vector  is the true linear relationship used to generate the original data and the scalar  is the level of label noise. We start at iteration  with  initial independent data points  each following , that is,  for each . We form the design matrix  with  as rows. We also form the vectors  and  with -th coordinate  and  respectively. In whatever follows, we will assume that  has full column rank, i.e., ,  is invertible and the model is underparameterized.  \nWe generate synthetic data from the following sequence of distributions where  is the number of iterations. The scheme is outlined as follows.  \nFor :  \nAccumulating Covariates/Features:  \nAccumulating Targets: , where  \nFit linear model:  \nSample synthetic data for the next iteration: , where  \nFor :  \nAccumulating Covariates/Features:  \nAccumulating Targets:  \nFit linear model:  \nSample synthetic data for the next iteration: , where  \nHere, for a matrix  with full column rank,  is the Moore-Penrose pseudo-inverse of . The noise terms  are independent of each other and of the covariates/features. Since  has full column rank, so does  for every .  \nTo reiterate a comment made previously by Dohmatob et al. (2024a), although we present our results in the context of ordinary linear regression in , our analysis can be readily extended to ridge regression and the kernel setting (Caponnetto & De Vito, 2007; Simon et al., 2021; Cui et al., 2021; Wei et al., 2022). We focus here on a simple useful model for studying model collapse."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Precise Test Error Characterization Under Accumulating Data",
            "text": "Our goal is to establish an analytic formula for the parameters of the model in the data accumulation setting. We begin by characterizing the relationship between the fitted linear parameters and the true parameters. We remind the reader that we assume that the design matrix has full column rank, i.e., it is invertible. Proofs are deferred to App. B.\n\nIn the data accumulation setting, the fitted linear parameters can be expressed as:\n\\[ \n\\hat{\\beta}^{(t)} = \\beta + (X^T X)^{-1} X^T \\epsilon^{(t)}\n\\]\nwhere, recall, \\(\\beta\\) is the true parameter, \\(X\\) is the original design matrix, and \\(\\epsilon^{(t)}\\) is the extra noise added at the ’th iteration.\n\nFor an -fold synthetic data generation process with \\(m\\) samples per iteration and isotropic features \\(\\Sigma^x = \\sigma_x^2 I_d\\), the model can be analyzed under these conditions. \n\nThis difference can be intuitively explained by the differences in the way data are handled across iterations. In the data replacement setting, because previous data were discarded, the model is more strongly affected by the new noise that each iteration of generated data introduces, and adds that to the effects experienced in earlier iterations. But in the data accumulation setting, because iteration \\(t\\) contributes fraction \\(\\frac{1}{t}\\) to the training dataset, the additional noise from the \\(t\\)th iteration of synthetic data has its effect on the model MSE shrunken proportional to \\(1/t^2\\) (due to squared error). The summability of \\((1/t^2)\\) ensures stability in the model training process. This suggests that accumulating generated data with real data can indeed avoid model collapse."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Numerical Confirmation of Analytical Results",
            "text": "To confirm the analytical results, we numerically simulate the setup. The numerics almost perfectly matched the analytics (Fig. 7 ###reference_###): when data are replaced, the results grow with the number of iterations, with the prefactor set by the noise-to-signal ratio, but when data accumulate, the results rapidly plateau with the prefactor similarly set. For further details and higher model-fitting iterations, see Appendix Fig. 16 ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "This work explored the phenomenon of model collapse, an important concern as AI-generated content permeates the internet and finds its way into future training datasets. Prior work has shown that training on model outputs can lead to degraded performance (Martínez et al., 2023a  ###reference_b18###; b  ###reference_b19###; Shumailov et al., 2023  ###reference_b27###; Alemohammad et al., 2023  ###reference_b2###; Hataya et al., 2023  ###reference_b12###; Bertrand et al., 2023  ###reference_b4###; Briesch et al., 2023  ###reference_b5###; Dohmatob et al., 2024a  ###reference_b9###; b  ###reference_b10###), implying that future model training faces a difficult challenge of ensuring strict training dataset hygiene.\n\nOur findings extend these prior works to show that if data accumulates and models train on a mixture of “real” and synthetic data, model collapse no longer occurs. We show this both experimentally on causal transformers for language modeling, diffusion models for molecule generation, and variational auto-encoders on image data as well as theoretically for linear regression. Together, these results strongly suggest that the “curse of recursion” may not be as dire as had been portrayed – provided we accumulate synthetic data alongside real data, rather than replacing real data by synthetic data only.\n\nLooking to the future, many questions worth investigating remain. For instance, in future work we would like to explore different data generation and accumulation regimes, such as (1) additional “real” data being introduced in each model-fitting iteration and (2) different schedules of how much synthetic data is generated at each iteration and (3) human-filtering of generated data, e.g., as done in RLHF. Additionally, we note that in all our experiments, the synthetic dataset is generated by sampling from the previous model, i.e., with some stochasticity; in future work, we would like to explore also what happens if data is generated deterministically, e.g. with temperature 0 in a typical language model.\n\nLastly, it is worth noting that “model collapse” – as a term of art – has been used in various ways by various researchers; so care is required in comparing claims across articles. In reviewing the literature, we identified at least four related phenomena: (1) modal collapse — collapse to one (or a few) modes; (2) collapse to uniformity; and (3) amplification of artifacts introduced by models fit to previous synthetic data. Future work should map out what factors cause which to occur and what preventative strategies are effective at addressing each."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "The content of this paper does not necessarily reflect the position or the policy of any of the funding agencies/entities. No endorsement should be inferred. M.G. acknowledges support through a grant from the Cooperative AI Foundation. R.S. acknowledges support from Stanford Data Science and from OpenAI’s Superalignment Fast Grant Research Fellowship. A.G. acknowledges support from the NSF CAREER grant DMR-2045181, the Sloan Foundation, and by the Laboratory\nfor Physical Sciences through the Condensed Matter Theory Center.\nD.R. acknowledges support from the National Science Foundation under Cooperative Agreement PHY-2019786 (the NSF AI Institute for Artificial Intelligence and Fundamental Interactions  ###reference_aifi.org/###) and appreciates both the sanction and support of Sequoia Capital.\nS.K. is partially supported by NSF III 2046795, IIS 1909577, CCF 1934986, NIH 1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc."
        }
    ]
}