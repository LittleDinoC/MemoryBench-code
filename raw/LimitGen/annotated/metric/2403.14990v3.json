{
    "title": "MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness",
    "abstract": "This paper presents the MasonTigers’ entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches to semantic textual relatedness across 14 languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from to in Track A, from to in Track B, and from to in Track C. Adhering to the task-specific constraints, our best performing approaches utilize an ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In this modern era of information retrieval and NLP, understanding semantic relatedness is fundamental for refining and optimizing diverse applications. Semantic relatedness refers to the degree of similarity and cohesion in meaning between two words, phrases, or sentences. Semantic relatedness allows systems to grasp the contextual and conceptual connections between words or expressions. Various NLP tasks and applications can benefit from modeling semantic relatedness such as question answering, knowledge transfer, text summarization, machine translation, and content recommendation.\n\nWhile significant research has been conducted on semantic relatedness in English, more recently, the interest in semantic relatedness in other languages has been steadily growing. This reflects an increasing awareness of the need for developing models for languages other than English. NLP is evolving rapidly, and we have been witnessing the emergence of language-specific transformers, the release of datasets for downstream tasks in diverse languages, and the development of multilingual models designed to handle linguistic diversity.\n\nSemEval-2024 Task 1 - Semantic Textual Relatedness aims to determine the semantic textual relatedness (STR) of sentence pairs across 14 diverse languages. Track A focuses on nine languages (Algerian Arabic, Amharic, English, Hausa, Kinyarwanda, Marathi, Moroccan Arabic, Spanish, Telugu) using a supervised approach where systems are trained on labeled training datasets. Track B adopts an unsupervised approach, prohibiting the use of labeled data to indicate similarity between text units exceeding two words.\n\nThis track encompasses 12 languages (Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Modern Standard Arabic, Moroccan Arabic, Punjabi, and Spanish). Track C involves cross-lingual analysis across the 12 aforementioned languages. Participants in this track must utilize labeled training data from another track for at least one language, excluding the target language."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Understanding the level of semantic relatedness between two languages has been regarded as essential for grasping their meaning. Notable studies on the topic including Agirre et al. (2012, 2013, 2014, 2015, 2016); Dolan and Brockett (2005) and Li et al. (2006) have introduced datasets like STS, MRPC, and LiSent. These datasets have been pivotal in advancing research in tasks such as text summarization and plagiarism detection.\n\nFinding semantic relatedness and semantic similarity, as well as determining sentence pair similarity using existing datasets or paired annotation, are integral in understanding the nuances of language comprehension. Previous studies describe how words and sentences are perceived to convey similar meanings Halliday and Hasan (2014); Morris and Hirst (1991); Asaadi et al. (2019); Abdalla et al. (2021); Goswami et al. (2024).\n\nMethodologies like paired comparison represent the most straightforward type of comparative annotations Thurstone (1994), David (1963). Best-Worst Scaling (BWS) Louviere and Woodworth (1991) a comparative annotation schema, offer insights into methods for evaluating relatedness through pairwise comparisons. The utilization of these methods aids in generating ordinal rankings of items based on their semantic relatedness. Kiritchenko and Mohammad (2016, 2017) highlight the effectiveness of such techniques, emphasizing the importance of reliable scoring mechanisms derived from comparative annotations for understanding the intricacies of semantic relatedness in natural language processing tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "The shared task comprises three tracks: Supervised, Unsupervised, and Cross-Lingual. The dataset Ousidhoum et al. (2024a) is comprised of two columns: the initial column, labeled \"text,\" containing two full sentences separated by a special character, and the second column, labeled as \"score,\" which includes the degree of semantic textual relatedness for the corresponding pair of sentences.\n\nIn the supervised track (Track A), there are 9 languages, and for each language, train, dev, and test sets are provided. The specifics of the dataset for this track can be found in Table 1.\n\nIn the unsupervised track (Track B), there are 12 languages and for all the languages dev and test sets are provided. The details of the dataset for this track are available in Table 2.\n\nFinally, in the cross-lingual track (Track C), there are 12 languages and for all the languages dev and test sets are provided, and they are the same as the unsupervised track. Here the training dataset is not provided. Hence, for each individual language of this track, we select 5 languages from the supervised track (different from the target language) and merge training data of those five languages to create the training dataset for each of the languages of the cross-lingual track. The details of the dataset for this track are available in Table 3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use statistical machine learning along with language specific BERT-based models to find the sentence embeddings and predict relatedness between pair of sentences. Additionally, we use sentence transformers for the supervised track. Our experiments are described in detail in the next sections."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Track A - Supervised",
            "text": "At first, we find the embedding of the training data using Term Frequency - Inverse Document Frequency (TF-IDF) Aizawa (2003  ###reference_b9###), Positive Point-wise Mutual Information (PPMI) Church and Hanks (1990  ###reference_b15###), and Language-Agnostic BERT Sentence Embedding (LaBSE sentence transformer) Feng et al. (2020  ###reference_b19###) separately. Also we find the embeddings using language specific BERT based models. For Algerian Arabic, Amharic, English, Hausa, Kinyarwanda, Marathi, Moroccan Arabic, Spanish and Telugu - DziriBERT Abdaoui et al. (2021  ###reference_b2###), AmRoBERTa Yimam et al. (2021  ###reference_b45###), RoBERTa Liu et al. (2019  ###reference_b31###), HauRoBERTa Adelani et al. (2022  ###reference_b3###), KinyaBERT Nzeyimana and Niyongabo Rubungo (2022  ###reference_b35###), MarathiBERT Joshi (2022b  ###reference_b26###), DarijaBERT Gaanoun et al. (2024  ###reference_b20###), SpanishBERT Cañete et al. (2020  ###reference_b14###) and TeluguBERT Joshi (2022a  ###reference_b25###) are used. For each training embedding, we calculate the cosine similarity Rahutomo et al. (2012  ###reference_b41###) between the pairs. After that we apply ElasticNet Zou and Hastie (2005  ###reference_b46###) and Linear Regression Groß (2003  ###reference_b22###) separately on these embeddings and predict the relatedness of the sentence pairs in the development phase. We clip the predicted values to ensure the prediction range from 0 to 1."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Track B - Unsupervised",
            "text": "For the unsupervised track, we find the embedding of the development data using Term Frequency - Inverse Document Frequency (TF-IDF) Aizawa (2003  ###reference_b9###) and Positive Point-wise Mutual Information (PPMI) Church and Hanks (1990  ###reference_b15###) separately. Additionally, we find the embeddings using language-specific BERT-based models. For Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Modern Standard Arabic, Moroccan Arabic, Punjabi, and Spanish - AfricanBERTa, DziriBERT Abdaoui et al. (2021  ###reference_b2###), AmRoBERTa Yimam et al. (2021  ###reference_b45###), RoBERTa Liu et al. (2019  ###reference_b31###), HauRoBERTa Adelani et al. (2022  ###reference_b3###), HindiBERT Joshi (2022a  ###reference_b25###), IndoBERT Koto et al. (2020  ###reference_b29###), KinyaBERT Nzeyimana and Niyongabo Rubungo (2022  ###reference_b35###), ArabicBERT Safaya et al. (2020  ###reference_b43###), DarijaBERT Gaanoun et al. (2024  ###reference_b20###), PunjabiBERT Joshi (2022a  ###reference_b25###), and SpanishBERT Cañete et al. (2020  ###reference_b14###) are used. Then for each development embedding generated by these three approaches, we calculate cosine similarity Rahutomo et al. (2012  ###reference_b41###) between the pairs."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Track C - Cross-Lingual",
            "text": "For each language in cross-lingual track, we select 5 different languages from Supervised Track to use as training data. The details of the language selection is provided in Table 3. We find the embedding of the training data using Term Frequency - Inverse Document Frequency (TF-IDF) and Positive Point-wise Mutual Information (PPMI) separately. Also, we determine the embeddings using language-specific (unrelated to the target language) BERT-based models. For Afrikaans, Amharic, Hausa, and Kinyarwanda - we use ArabicBERT, for Algerian Arabic, Modern Standard Arabic, and Moroccan Arabic - we use AfricanBERTa, for English, Hindi, Indonesian, Punjabi, and Spanish - SpanishBERT, BanglaBERT, RoBERTa-tagalog, HindiBERT, and RoBERTa are used. Then for each training embedding generated by these three approaches, we calculate cosine similarity between the pairs. After that, we apply ElasticNet and Linear Regression separately on these embeddings and predict the similarity of the sentence pairs in the development phase. We clip the predicted values to ensure the prediction range from 0 to 1. We also perform this approach on the test data to determine our best results in the evaluation phase."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "For all the tracks, ensemble of the predictions prove helpful in terms of achieving better performance.\n\nFor Track A, sentence transformer LaBSE along with Linear Regression performs the best among the eight combinations for all the languages. Then the weighted ensemble improves the result 1% - to 3% in development phase and 1% - 2% in evaluation phase - depending on the languages. For English, this method performs the best in terms of ranking while the worst for Moroccan Arabic. Detailed results are shown in Table 4 of Appendix.\n\nFor Track B, embedding generated by language-specific BERT-based models provide the best result among the three combinations for all the languages. Then the average ensemble improves the result 0% - to 3% in development phase and 0% - 2% in evaluation phase - depending on the languages. For Kinyarwanda, this method performs the best in terms of ranking while the worst for English. Detailed results are shown in Table 5 of Appendix.\n\nFor Track C, embedding generated by language-specific (unrelated to target language) BERT-based models provide the best result among the six combinations for all the languages. Then the average ensemble improves the result 0% - to 2% in both development and evaluation phases depending on the languages. For Punjabi, this method performs the best in terms of ranking while the worst for Hausa and Kinyarwanda. Detailed results are shown in Table 6 of Appendix."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Error Analysis",
            "text": "Moreover, ElasticNet and Linear Regression exhibit limitations as assumptions of linearity may not align with the intricate and nonlinear relationships inherent in the textual data. The issue of dimensionality poses a challenge, especially when dealing with a large number of features. The difference between the gold and predicted semantic relatedness scores for the three tracks are shown in Figure 1, Figure 2, and Figure 3."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We experimented with various methodologies on the dataset provided by the organizers, including statistical machine learning approaches, transformer-based models, language-specific BERTs, and sentence BERT. In the supervised task (Track A), with no restrictions on the model or data, we utilized the available training dataset. Conversely, the unsupervised task (Track B), lacking training data, presented challenges, leading us to use language-specific BERTs and statistical machine learning approaches. The cross-lingual track (Track C) imposed more stringent restrictions, requiring us to use training data from other languages in Track A, excluding the target language. In addition to statistical ML models, we integrated language-specific BERTs closely aligned with the geography and culture of the target language, as the use of LLMs was constrained due to unknown training data.\n\nWe show that our ensemble approach exhibited superior performance compared to individual model experiments. However, the task’s inherent difficulty became evident in instances where relatively small datasets presented challenges for effective model learning. Semantic textual relatedness tasks face challenges like subjectivity, context dependency, and ambiguity due to multiple meanings and cultural differences. Limited data, domain specificity, short texts, and biases hinder accuracy. Ongoing research is crucial to address these limitations and improve model accuracy."
        }
    ]
}