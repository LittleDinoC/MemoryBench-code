{
    "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
    "abstract": "Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of English language social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales. All code and data will be made available at https://github.com/AmritaBh/shield.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social media has become a platform of content sharing and discussions for a varied range of individuals with differing cultural and continental backgrounds. People use social media platforms to exchange information, and they frequently engage in dialectal conversations. These discussions are not always peaceful, they can degenerate into unpleasant altercations and bigoted arguments. Thus, social media platforms often become a host for hate speech.\n\nHate speech is described as any deliberate and purposeful public communication meant to disparage a person or a group by expressing hatred, disdain, or contempt based on their social attributes (e.g., gender, race). In extreme cases, hate speech may often lead to real world harms such as hate crimes, for example the anti-Asian hate crimes during the COVID-19 pandemic. Therefore, it is essential to have automatic hate speech detection and moderation in place to maintain the integrity of social media platforms as well as to mitigate negative impacts in real-world scenarios such as increased violence towards minorities.\n\nGiven that the issue of hate speech on social media is a well-established problem, there have been several works to detect such online hate-speech. While state of the art hate speech detection models have been able to achieve good performance on benchmark evaluation datasets, most of these models are built using transformer-based pre-trained language models or other deep neural network type models that are not interpretable or explainable. However, the task of hate speech detection is a very sensitive task, and explainability of automated detectors is an essential and desirable feature. Model interpretability is essential not only for end-user understanding but also for understanding biased predictions, domain shifts, other errors in the prediction, etc.\n\nWhile incorporating qualities of interpretability directly into deep neural network models such as pre-trained language model based detectors is challenging, one way to potentially perform this is by using an auxiliary model to provide explanations or rationales, that are subsequently used in training the detection model. This type of a method has been proposed and used in the FRESH framework, where the authors use two disjoint networks, one for extracting the task-specific rationales, and then another that leverages those rationales to learn the classification task, thereby enabling faithful interpretability by construction.\n\nInspired by this work, we propose a framework, where we use LLMs as the extractor model: we leverage the textual understanding and instruction-following capabilities of state-of-the-art LLMs to extract features from the input text, that is used to augment the training of a separate base hate speech detector, thereby facilitating faithful interpretability. Overall, our contributions in this paper are:\n\nWe propose SHIELD, a framework that leverages LLM-extracted rationales to augment a base hate speech detection model to facilitate faithful interpretability.\n\nWe evaluate the goodness of LLM-extracted features and rationales, and measure the alignment of such with human annotated rationales.\n\nThrough comprehensive experiments on both implicit and explicit hate speech datasets, we show how SHIELD retains detection performance even after training with rationales for increased interpretability."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Our SHIELD Framework",
            "text": "We show our proposed SHIELD framework in Figure 1. In this section, we describe our framework in detail, elaborating on each of the components.\nOur framework uses the state-of-the-art instruction-tuned large language models (LLMs) in an off-the-shelf manner as textual feature extractors. Although recent work has shown that LLMs struggle to perform the hate speech detection task, we hypothesize that we can leverage the textual understanding capabilities of these LLMs to simply extract textual features in the form of rationales. Restricting the use of the LLM to a simple text-level task would ensure that such models are not directly being used for sensitive application tasks such as hate speech detection.\n\nFor a given input text, we use our carefully designed task prompt to prompt the LLM to extract features from the text that promotes a hateful sentiment. In the context of explicit hate speech detection, such features could include categories such as derogatory words, cuss words, etc. We also ask the LLM for rationales as to why the label is hateful or non-hateful. To perform this feature extraction, for each input text we prompt the LLM using the following prompt:\n\n“You are a content moderation bot. Identify the list of rationales, list of derogatory language, list of cuss words that promote a hateful sentiment and respond with non-hateful if there are none. Note: The output should be in a json format.”\n\nText: [input_text]\n\nAfter post-processing the outputs, we have a list of textual features for the given input text.\n\nThe next component in our framework is the base hate speech detector which we are trying to augment, such as HateBERT. HateBERT is a BERT model that is specifically fine-tuned on hate speech data. For each input text, instead of obtaining the labels or class probabilities, we take the last layer embedding of the [CLS] token, essentially containing all the information of the input text, that is relevant for the hate-speech detection task.\n\nFor the textual features and rationales, we extracted via the LLM, we use a pre-trained transformer-based language model (PLM), such as BERT to embed these features. PLMs, even without any task-specific fine-tuning, provide rich, expressive latent representations for text. Therefore, we feed in the LLM-extracted textual features into a BERT model and obtain the last hidden layer embedding of the [CLS] token.\n\nFrom the previous two components, for each input text, we have two embeddings: text embedding from the base hate speech detector, and feature embedding from the feature embedding BERT model. To combine these two, we simply concatenate these embeddings:\n\nWe use a concatenated view in order to incorporate additional contextual features that may be very relevant to determining the hate or non-hate label. We then feed this combined embedding into a feed-forward multi-layer perceptron with two fully connected layers and a ReLU activation in between, to project it onto a smaller dimension space. We do this in order to retain important features and avoid overfitting of the model during training. Finally, we compute the batch-wise binary cross entropy loss using the ground truth label for each input text.\n\nSince we are using the BERT feature embedding model just to encode the textual features, we keep this model frozen and train the remainder of the framework with this simple loss."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology and Experimental Settings",
            "text": "In this section, we discuss our methodology in detail, including the datasets we included, the baseline models for hate speech detection, along with the experimental settings."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "In order to evaluate SHIELD, we use both explicit and implicit hate speech datasets. For explicit hate, we include publicly available benchmark datasets from the following social media platforms: {GAB, Twitter, YouTube, and Reddit}. All these datasets are in the English language. GAB is a collection of annotated posts from the GAB website. It consists of binary labels indicating whether a post is hateful or not. Reddit is a collection of posts indicating whether it is hateful or not. Twitter contains instances of hate speech gathered from tweets on the Twitter platform. Finally, YouTube is a collection of hateful expressions and comments posted on the YouTube platform.\n\nWe further pre-process these according to the method followed in order to get cleaned binary labels. A summary of the datasets and the distribution of hateful posts and non-hateful posts can be found in Table 1.\n\nWe also include implicit hate speech in our evaluation: while subtle forms of abuse may not be perceived as overtly harmful initially, they nonetheless perpetuate similar degrees of damage over time owing to their covert nature. Therefore, the detection of implicit hate speech becomes even more important. For this reason, we evaluate our proposed model on the Implicit Hate Speech Corpus. This dataset encompasses posts compiled from Twitter, annotated as either explicit hate, implicit hate, or non-hate speech. We exclusively utilize implicit hate and non-hate for our binary classification task."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baselines",
            "text": "We compare our proposed SHIELD framework to a variety of different baselines in order to understand the impact of the augmentation with rationales. We use the following well-known baseline hate speech detection models:\n\nHateBERT: This is also the base model used in our framework. HateBERT Caselli et al. (2020 ###reference_b8###) uses over 1.5 million Reddit messages from suspended communities known for encouraging hate speech to fine-tune the BERT-base model. \n\nHateXplain: Similarly, we fine-tune the HateXplain Mathew et al. (2021 ###reference_b33###) model on each of our datasets. HateXplain model is trained on hateful posts along with the target community, the rationales, and the portion of the post on which human annotators’ labelling decision is based.\n\nPEACE: We further extend our comparison on PEACE Sheth et al. (2023b ###reference_b41###) framework which uses Sentiment and Aggression Cues to detect the overall sentiment of the text.\n\nCATCH: Furthermore, we compare our model with CATCH Sheth et al. (2023a ###reference_b40###) framework which disentangles the input representations into invariant and platform-dependent features.\n\nChatGPT-1shot: Apart from these hate speech specific detection models, we also compare our framework with an off-the-shelf GPT-3.5 model, to understand how well the LLM performs on the same datasets. We do this in a one-shot manner, i.e., by proving the task instruction along with an example input and ground truth label."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "To implement our proposed SHIELD framework, we use PyTorch and the Huggingface Transformers library. As shown in Figure 1 ###reference_###, our first component uses an off-the-shelf LLM to extract the features and rationales. Here, we use OpenAI’s GPT-3.5 (specifically, GPT-3.5-turbo-0613) or otherwise commonly referred to as ‘ChatGPT’, since it has been experimented on a variety of NLP tasks with huge success Guo et al. (2024 ###reference_b17###). We access this model via the OpenAI API. For feature/rationale extraction and generation, we set the temperature to 0.1 and top_p to 1. For the Feature Embedding Model, we use a pre-trained, frozen BERT (bert-base-uncased) and for the Hate Speech Detector, we use a pre-trained HateBERT model. We use AdamW optimizer Kingma and Ba (2014 ###reference_b26###) with a learning rate of . Model training was performed on two machines: one with an NVIDIA GP102 [TITAN Xp] GPU with 12 GB VRAM, and another with an NVIDIA A100 GPU with 40GB RAM."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "In this section we describe our experiments and elaborate on the experimental results. To explore the feasibility and effectiveness of our proposed SHIELD framework, we aim to answer the following research questions:\nRQ2: Can we leverage recent state-of-the-art LLMs to extract features in the form of rationales, and do these rationales align with human judgement?\nRQ3: Can SHIELD effectively facilitate faithful interpretability?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance of ChatGPT on the hate speech detection task",
            "text": "Several recent works test whether Large Language Models have the potential to reproduce human annotated ground truth labels in social computing tasks Zhu et al. (2023  ###reference_b44###). However, even after extensive pre-training on a large corpus of datasets, where LLMs are expected to perform well in this task, this is not the case.\n\nTo further evaluate this beyond what other recent works have shown, we carefully craft a one-shot prompt and prompt ChatGPT to classify the input text, given a labeled example in the prompt. The outcome of this prompt is a single label representing hateful text as label “1\" and non-hateful text as label “0\" as shown in Table 2  ###reference_###.\n\nChatGPT struggles with the other 4 datasets with ~58-65%. Similar observations have been reported in other recent work that has investigated the off-the-shelf performance of LLMs in hate speech detection Li et al. (2023 XXXreference_b30###); Zhu et al. (2023  ###reference_b44###).\n\nWhile this shows ChatGPT and possibly other LLMs struggle at hate speech detection when used as a detector directly, these models have also been shown to have impressive textual understanding capabilities. Perhaps, simply using these models to extract features or rationales, instead of performing the entire detection task, might be beneficial. We evaluate this in the following subsection."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Goodness of ChatGPT extracted features or rationales",
            "text": "We are interested in evaluating the textual and contextual understanding capabilities of ChatGPT to extract features in the form of rationales from the input text, which are meaningful for the task of hate speech detection. We use the LLM (i.e., GPT-3.5) as the extractor model, which, unlike previous models, does not require any additional task-specific fine-tuning. This is possible due to the instruction-following capabilities of recent LLMs. We carefully craft a prompt to extract cuss words, derogatory language, and rationales from the input text that serve as interpretable features for the subsequent predictor model (HateBERT) to create a faithfully interpretable hate speech detector. We compare ChatGPT-extracted rationales with human-annotated ground truth rationales using the annotated rationale spans in the HateXplain dataset. After some standard pre-processing, such as removing stop words, we compute similarity metrics in both the token space and latent space, finding significant overlap and high semantic similarity between the LLM and human rationales.\n\nWe present examples from all five datasets: the input text labeled as 'hateful' and the ChatGPT-extracted features. The features fall into three categories: rationales, derogatory language, and cuss words. The LLM is able to identify the words and spans effectively. \n\nWe also present examples to qualitatively assess the overlap between the human-annotated rationales and the LLM-extracted ones. Text in red represents rationales annotated by human annotators, text in blue represents rationales or words identified by the LLM, and text in purple shows the spans where both annotations overlap. From these examples, there is an overall high degree of overlap, with the LLM capturing semantically relevant portions of the text. Interestingly, while human annotators often include words or spans with lesser relevance to the task, the LLM-extracted rationales tend to exclude these spans. Using LLM-extracted rationales for training might be more beneficial as it helps avoid some of the noisy signals in the data."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Hate speech detector performance after training with extracted rationales",
            "text": "In this experiment, we try to train a hate speech detector with the extracted rationales additionally incorporated into the input text to facilitate faithfully interpretable classifications. For this, we use a HateBERT model as the base hate speech detector model and report results in Table 3, along with results from other baselines. Interestingly, in the Twitter dataset, we also see a significant performance jump by our SHIELD model compared to the fine-tuned HateBERT model. This potentially might be due to noise in the Twitter dataset: the extracted rationales may provide more discriminative training signals, thus allowing the detector to train on robust features instead of noisy ones, although more analysis is required to verify this claim.\n\nFor some additional analysis on the effect of the framework components, we modify the choice of the base pre-trained language models in the two model components: the hate speech detector and the feature extractor. The specific variations we experiment with are: (1) the original SHIELD framework, which has HateBERT as the hate speech detector (HSD) and bert-base-uncased as the feature embedding model (FE), (2) SHIELD with a pre-trained roberta-base as the HSD instead of HateBERT, and (3) SHIELD with a pre-trained roberta-base as the FE instead of bert-base-uncased. We choose to perform this analysis with roberta instead of the two bert-based models since RoBERTa has been shown to sometimes have better performance than BERT on a variety of natural language understanding tasks. We report the results of this analysis in Table 6. Overall, we see some variation in performance on the model choice for the HSD and FE components. While roberta-base as the FE component marginally helps to improve performance for only one dataset, i.e., GAB, roberta-base as the HSD instead of HateBERT achieves higher performance for three datasets. This is particularly interesting since, unlike HateBERT, the pre-trained roberta-base is not specifically trained on the hate speech task.\n\nOverall, SHIELD shows promising results in leveraging LLM-extracted rationales into augmenting a base hate speech detector to facilitate faithful interpretability."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Hate Speech Detection",
            "text": "There are two primary methods for approaching the detection of hate speech. Leveraging new or supplementary data is the first strategy. This involves making advantage of user attributes del Valle-Cano et al. (2023  ###reference_b10###), dataset annotator features Yin et al. (2022  ###reference_b43###), or comprehending the ramifications of hateful posts Kim et al. (2022  ###reference_b25###). One study, for instance, used the consequences of hateful posts to train a model on contrastive pairs that represent hate content in order to detect implicit hate speech Kim et al. (2022  ###reference_b25###). An additional study Yin et al. (2022  ###reference_b43###) brought to light the challenge of reaching agreement among annotators on subjective issues such as recognizing hate speech, and it recommended that definitive labels and annotator traits be included in training to improve the efficacy of detection. In a different study del Valle-Cano et al. (2023  ###reference_b10###), data from users’ social situations and characteristics were analyzed to predict user satisfaction. But the problem with these strategies is that they could be challenging as access to auxiliary information across different platforms is seldom available.\nThe second tactic makes use of language models like BERT, which have been trained on large text datasets and are renowned for their capacity for generalization. The efficacy of these algorithms can be increased by fine-tuning them using particular hate speech datasets Caselli et al. (2020  ###reference_b8###); Mathew et al. (2021  ###reference_b33###). One such example is HateBERT Caselli et al. (2020  ###reference_b8###), a model that was refined using over 1.6 million hostile remarks from Reddit and based on a BERT model. In a similar vein, HateXplain Mathew et al. (2021  ###reference_b33###) is another model created to recognize and interpret hate speech. Other strategies include concentrating on lexical indications Schmidt and Wiegand (2017  ###reference_b39###) such as POS tags used Markov et al. (2021  ###reference_b32###), facial expressions, content-related portions of speech, or important phrases that communicate hate ElSherief et al. (2018  ###reference_b15###). In order to improve language model representations, one study manually determined that sentiment and hostility are causal cues Sheth et al. (2023b  ###reference_b41###). Another study leveraged a causal graph to disentangle the input representations into platform specific (hate-target related features) and platform invariant features to enhance generalization capabilities for hate speech detection Sheth et al. (2023a  ###reference_b40###). Although effective, this method also requires auxiliary data (such as hate target labels) which are seldom available across various platforms."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "LLMs as Experts or Feature Extractors",
            "text": "Recent advancements in LLM research have demonstrated improved performance across not only many natural language tasks Min et al. (2023  ###reference_b34###), but also more challenging domains such as writing and debugging code, performing mathematical reasoning Bubeck et al. (2023  ###reference_b7###), etc. This has motivated a line of research where the community has been trying to evaluate how well these LLMs can perform different tasks. LLMs have shown promise in the task of data annotation He et al. (2023  ###reference_b21###); Bansal and Sharma (2023  ###reference_b2###), information extraction  Dunn et al. (2022  ###reference_b13###), text classification Kocoń et al. (2023  ###reference_b27###); Bhattacharjee and Liu (2024  ###reference_b5###), and even reasoning  Ho et al. (2022  ###reference_b22###). Given the ease with which these LLMs can be queried, these models often serve as faulty experts or pseudo oracles in many tasks. Past exploration has investigated whether language models can be used as factual knowledge bases Petroni et al. (2019  ###reference_b37###). A recent work has explored the possibility of using LLMs in the hate speech detection task Kumarage et al. (2024  ###reference_b28###). Similar to our approach, authors in  Hasanain et al. (2023  ###reference_b20###) have tried to perform propaganda span annotation using language models. However, our approach focuses on leveraging the extracted spans, words and rationales to augment a detector model to enable interpretability in an otherwise black-box model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this work, we explore the problem of hate speech detection on social media and propose a method to train interpretable classifiers using rationales extracted by large language models. We intend to leverage the textual understanding and instruction-following capabilities of LLMs such as ChatGPT to extract words and rationales from the text that are associated with the hate speech label. We propose a framework SHIELD, that uses these LLM-extracted rationales to augment the training of a base hate speech detector to facilitate it to be faithfully interpretable. We verify that the LLM-extracted rationales align with human judgment. We train and evaluate our framework on multiple benchmark datasets comprising both implicit and explicit hate speech from a variety of online social media platforms. Therefore, we have a faithfully interpretable hate speech detector that simply relies on LLM-extracted rationales instead of human-annotated.\n\nWhile our work follows that of Jain et al. and we establish faithfulness by construction, future work could explore better ways to evaluate the faithfulness of the resulting detector. In this work, we verified the goodness of the extracted rationales by comparing it with the ground truth for one dataset. Future work can investigate better automated ways to evaluate and verify the quality of the LLM-extracted rationales. Furthermore, an interesting and responsible direction forward would be the development of hybrid approaches that leverage LLMs for extracting rationales at scale and then employing human experts to verify the validity and quality of these rationales. This would also alleviate some of the concerns surrounding LLM hallucinations and biases in the LLM being propagated into the rationale extraction step."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "While our SHIELD framework shows promise in leveraging large language models to create interpretable hate speech detectors, several limitations need to be addressed. A inherent trade-off exists between the interpretability gained through LLM-extracted rationales and the accuracy of the resulting model, requiring further work to optimize this balance. In certain cases, the LLM may fail to identify coherent rationales, leading to incomplete or inaccurate explanations for the model’s predictions. The choice of the LLM itself is also crucial, as powerful proprietary models like ChatGPT may not be accessible to all researchers, while open-source alternatives could potentially yield suboptimal performance. Our work currently uses ChatGPT for rationale extraction, but exploring the capabilities of different LLMs, including multilingual and domain-specific models, could provide valuable insights. Additionally, our framework may need adaptation to handle instances where the LLM cannot provide clear rationales, either through ensemble methods or by incorporating human feedback mechanisms to refine the extracted rationales."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Considerations",
            "text": ""
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Acknowledgment of the sensitivity and potential harm of hate speech",
            "text": "We acknowledge that hate speech is a sensitive and potentially harmful topic that can perpetuate discrimination, marginalization, and violence against individuals or groups based on their race, ethnicity, religion, gender, sexual orientation, or other protected characteristics. We recognize the importance of addressing hate speech responsibly and with great care, as it can have severe psychological, emotional, and social consequences for those targeted. However, our work strives to better interpret and mitigate the use of hateful speech promptly by employing LLMs in an out-of-the-box manner leveraging their context-understanding capabilities in hate speech detection task."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Commitment to responsible use and mitigation of potential misuse",
            "text": "Our research focuses on leveraging the contextual understanding capabilities of large language models (LLMs) to automate the detection of hateful content, such as derogatory language, cuss words, and profanities, in the form of rationales across social media platforms. This aims to enable early-stage identification and mitigation of hate speech. We acknowledge the severity of the hateful examples used, which may potentially promote racial superiority, incite racial discrimination, or encourage violence against certain racial or ethnic groups – actions that are considered punishable offenses by law. After a thorough evaluation, we have concluded that the benefits of using real-world practical examples to enhance the clarity and understanding of our research outweigh any potential risks or drawbacks associated with their inclusion."
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Ethical guidelines and principles followed",
            "text": "In conducting our research, we adhere to established ethical guidelines and principles, such as those outlined by professional organizations and academic institutions. We have utilized publicly available datasets that are appropriately cited in our paper. We also strive to maintain transparency by clearly documenting our methods, data sources, and limitations."
        }
    ]
}