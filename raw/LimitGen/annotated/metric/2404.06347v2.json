{
    "title": "RAR-b: Reasoning as Retrieval Benchmark",
    "abstract": "Semantic textual similarity (STS) and information retrieval (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Semantic textual similarity (STS) and information retrieval tasks (IR) have been two principal measures used to evaluate the progress of dense representation models. However, STS is acknowledged for its limitations in aligning with real-world applications, its ambiguity, and its orthogonal performance with IR and other downstream tasks.\n\nIn the era of large language models (LLMs), Retrieval-Augmented Generation (RAG) has emerged as a strategic alternative to traditional end-to-end generative language models. This transition is driven by the shortcomings of LLMs, such as their proneness to factual inaccuracies through hallucinations, outdated knowledge, challenges with long-tailed knowledge, and difficulties with reasoning tasks like logical deduction.\n\nRAG is applied diversely across NLP tasks. For knowledge-intensive tasks, it is used to fetch current and reliable knowledge sources, which in turn guide LLMs in producing accurate responses, thus reducing the need for frequent fine-tuning and mitigating hallucinations. In reasoning-dependent tasks, RAG assists in retrieving relevant data from large input sets, facilitating LLMs' focus on pertinent information, which is crucial in multi-hop question answering where reasoning across numerous documents is necessary. \n\nDespite the progress seen with dense retrievers for knowledge retrieval tasks, these systems often underperform when it comes to obtaining reliable references compared to state-of-the-art proprietary LLMs. This issue is even more evident in reasoning-intensive tasks, where retrieval-augmented generation methods can sometimes lead to erratic improvements or even degrade the LLMs' performance.\n\nGiven the complexities involved in dense representation models' roles in today's LLM landscape, precise evaluation of their advanced language comprehension skills becomes increasingly important. Therefore, we propose to assess these models beyond factual accuracy or semantic matching, focusing more on their capability for complex thought processes and logical reasoning.\n\nThis paper presents the Reasoning as Retrieval Benchmark (RAR-b), a novel framework that reinterprets reasoning tasks as retrieval challenges. It provides a comprehensive reassessment of the \"actual reasoning representation compressed\" in dense models and questions the current approaches in reasoning-oriented RAG.\n\nHistorically, lexical-based retrieval methods have been foundational yet often inadequate for reasoning tasks. With advances in dense retrieval systems and RAG adoption, RAR-b serves as a critical and timely framework to test whether dense representation models can encode and apply the reasoning skills essential for sophisticated language comprehension.\n\nRAR-b’s contributions are threefold:\n\n1. Beyond the recognized semantic similarity and topical alignment abilities inherent in sentence and document representation models, RAR-b anticipates their reasoning faculties and promotes their evaluation.\n2. We perform extensive evaluations of advanced open-source and proprietary retrievers and re-rankers, offering insights into the reasoning tendencies of current retrieval models.\n3. Crucial insights are surfaced through analyzing the behaviors of systems with differing architectures and training strategies, suggesting pathways for embedding models to attain reasoning-level language understanding and guiding future research efforts in the domain."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "RAR-b",
            "text": "In this work, we construct and release RAR-b: Reasoning as Retrieval Benchmark. Deviating from evaluating on reasoning tasks with a full RAG pipeline (retriever+LLM), we instead focus on evaluating only the retrievers. By constructing reasoning tasks into retrieval tasks, we investigate how good retrievers are on solving them in a standalone manner, and use this as a proxy of the upperbound of retrievers’ capabilities in assisting LLMs, in a standard RAG system. We design three levels of tasks, resulting in the integration of 12 tasks derived from 17 datasets. We convert the original datasets into retrieval formats with both multiple-choice retrieval and full-dataset retrieval settings. \n\nWe first benchmark the performance of state-of-the-art bi-encoder models, spanning across three model categories: unsupervised dense retrieval models, supervised dense retrieval models, and instruction-aware dense retrieval models. We evaluate both representative open-source models and proprietary models such as Cohere and OpenAI Embedding Models. We further benchmark the performance of representative re-ranking models, both on using them to solve multiple-choice retrieval setting independently and on further re-ranking the documents retrieved by bi-encoders in the Full-dataset retrieval setting.\n\nThe datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SocialiQA (Sap et al., 2019), ARC-Challenge (Clark et al., 2018), Quail (Rogers et al., 2020), Winogrande (Sakaguchi et al., 2021), and C-STS (Deshpande et al., 2023). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021) (detailed explanation in Appendix B).\n\nApart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with TempReason (Tan et al., 2023) and SpartQA (Mirzaee et al., 2021). For TempReason, we evaluate each of its sub-level tasks separately, and construct the pure, fact, and context settings to assess different aspects of retrievers’ behaviors (detailed analyses in Section 4.1). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH).\n\nWith the common use cases of code retrieval and the fact that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a) and MBPP (Austin et al., 2021) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a) is an extended version of HumanEval (Chen et al., 2021), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al., 2019) and 100k answers from a synthetic dataset TinyCode222 to cover pure code text and the mixture of natural language and code. We extensively explore the optimal setting to construct the code corpus and evaluate the code tasks (see analyses in Section 4.2) to make it as difficult as possible while keeping the evaluation computationally efficient. For HumanEvalPack, we evaluate the code continuation setting; when enlarging the corpus, we sample examples from CodeSearchNet that fall under a similar length range of the ground truth documents"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Formulation",
            "text": "We propose the novel framework of evaluating reasoning tasks as retrieval tasks, assessing whether representations of reasoning expressions are semantically well-encoded by retrievers. In other words, we assess whether the groundtruth answer to a reasoning problem is encoded to have the highest similarity with the query in the semantic space. Given a retriever, a query, and a groundtruth answer hidden in a corpus that is intentionally made very large, the most ideal scenario would be:\n\nHowever, this is typically not possible given the complexity of reasoning language and the fact that current sentence encoders are typically not yet well-trained to model such expressions. Therefore, we in turn are interested in whether:\n\nFor the Full-dataset retrieval setting, we can quantitatively measure’s performance with commonly-adopted information retrieval metrics such as recall@n, concerning if the correct answers are even retrieved as the top n (recall@n). For the Multiple-choice setting, we simply assess whether the query is perceived by the retrievers to be more similar to the groundtruth answer, than to other candidate answers in its original dataset format. Models’ performance in this setting can be easily quantified by accuracy."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Datasets",
            "text": "The datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SocialiQA (Sap et al., 2019), ARC-Challenge (Clark et al., 2018), Quail (Rogers et al., 2020), Winogrande (Sakaguchi et al., 2021), and C-STS (Deshpande et al., 2023). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021) (detailed explanation in Appendix B).\n\nApart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with TempReason (Tan et al., 2023) and SpartQA (Mirzaee et al., 2021). For TempReason, we evaluate each of its sub-level tasks separately, and construct the pure, fact and context settings to assess different aspects of retrievers’ behaviors (detailed analyses in Section 4.1). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH).\n\nWith the common user cases of code retrieval and that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a) and MBPP (Austin et al., 2021) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a) is an extended version of HumanEval (Chen et al., 2021), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al., 2019) and 100k answers from a synthetic dataset TinyCode222 to cover pure code text and the mixture of natural language and code. We extensively explore the optimal setting to construct the code corpus and evaluate the code tasks (see analyses in Section 4.2) to make it as difficult as possible while keeping the evaluation computationally efficient. For HumanevalPack, we evaluate the code continuation setting; when enlarging the corpus, we sample examples from CodeSearchNet that fall under similar length range of the groudtruth documents to make the retrieval more difficult, given that length information is typically encoded in the embeddings (Xiao et al., 2023a) and we don’t want the models to leverage this information to simplify the retrieval.\n\nWe group the datasets into three levels. Level-1 dataset tasks are by their nature more in-distribution to typical datasets used to train IR models, and makes more sense to be solved even without instructions. For instance, PIQA consists of physical goals and possible solutions, which are similar to IR training sets that are question-answer pairs. HellaSwag and NLI respectively look for the end of an unfinished story, and the middle of a story given the start and the ending, which are both extremely similar to the positive pairs constructed in training of unsupervised dense retrieval models such as Contriever (Izacard et al., 2022), which samples overlapping spans of the text as positive pairs.\n\nLevel-1.5 datasets on the other hand, are more out-of-distribution in terms of task categories, and make less sense to be solved without a clearly-stated purpose. For instance, WinoGrande asks to find the correct answer to fill in an unfinished sentence, which is marked by an underscore"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "We first benchmark the performance of state-of-the-art bi-encoders, spanning across unsupervised dense retrieval models (U-DR), supervised dense retrieval models (S-DR), supervised instruction-aware dense retrieval models (I-DR). For U-DR models, we have Contriever (Izacard et al., 2022  ###reference_b25###). For S-DR models, we include two Sentence Transformers (Reimers & Gurevych, 2019  ###reference_b46###) models that have been the most popular in the last few years, all-mpnet-base-v2 and all-MiniLM-L6-v2; Dragon+ (Lin et al., 2023  ###reference_b31###), an S-DR model that is progressively distilled with different views of other SOTA models, and BGE-M3 (Chen et al., 2024  ###reference_b15###). For I-DR models, we include Instructor (Su et al., 2022  ###reference_b51###), BGE (v1.5) (Xiao et al., 2023c  ###reference_b66###), and consider all of their model sizes (base, large, XL for Instructor and small, base, large for BGE) to understand the scaling laws (if there’s any) between model size and reasoning abilities. We further include E5-Mistral (Wang et al., 2023a  ###reference_b58###), a latest state-of-the-art instruction-aware embedding model that is finetuned from Mistral (Jiang et al., 2023  ###reference_b27###) with {instruction, query, answer} pairs distilled from GPT-4. We recognize the recent advancements in unifying generative and representation abilities into one model, and benchmark GritLM (Muennighoff et al., 2024  ###reference_b39###), whose data and training process of the embedding abilities are similar to Jiang et al. (2023  ###reference_b27###), but differing in its full parameters finetuning and joint learning with the generative objective. Lastly, we benchmark popular proprietary models Cohere Embed-English-v3.0 and OpenAI text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large through API encoding. For rerankers (cross-encoders), we benchmark a few representative models such as BGE-reranker-large (Xiao et al., 2023c  ###reference_b66###), TART-Full (Asai et al., 2022  ###reference_b5###) and Cohere rerank-english-v2.0."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Reasoning as Retrieval",
            "text": "Instead of solving these reasoning tasks via text-to-text generation (Raffel et al., 2020; Lourie et al., 2021), we explore the possibilities of solving them as a unified representation-to-representation matching problem. That is, all tasks, contexts, and possible answers are all understood through a shared representation space, and the search of the answer, given a task and the context information, falls back to be a simple similarity match problem. We explore two settings: RARw/o Instructions and RARw/ Instructions. For RARw/o Instructions, we simply use the original question/query/context in the datasets as the query, without describing the task with an instruction prompt, and use the pool of all possible choices across splits as the candidate documents. For RARw/ Instructions, we prepend the query with an instruction that describes the task. For instance, for NLI, an (imperfect) instruction can be “Given the following start and end of a story, retrieve a possible reason that leads to the end.”\n\nWe construct two task settings: Multiple-choice Retrieval Setting (MCR), and Full-dataset Retrieval Setting (Full). For multiple choice retrieval, we utilize the choices available to each question in the original dataset. For this setting, the performance can be easily measured by accuracy, as each input has only 2-4 candidates. For Full-dataset Retrieval Setting, we construct a pool of all candidate choices with all the unique candidates from the same dataset, and investigate whether the groundtruth answer can be retrieved from a candidate pool of a much larger order of magnitude. In line with typical IR benchmarks, the corpus is constructed with candidates from all splits, and it is only shared across them in inference. For Full-dataset Retrieval, we measure the performance with commonly-adopted information retrieval metrics, including Recall@n.\n\nThe multiple-choice setting and the full retrieval setting serve as simulacra of real-life RAG scenarios. Full retrieval performance assesses whether correct answers can be retrieved as top candidates, providing a basis for the later LLMs to make informed decisions. Multiple choice performance gauges whether hard-negative candidates might be erroneously prioritized over the groundtruth answer, introducing noise into the references presented to LLMs. From a pure retrieval perspective on the other hand, multiple-choice setting evaluates the extent to which a dual dense encoder can substitute a cross-encoder in reasoning-intensive late interactions - if dense encoders are already good at MCR, no re-ranking models (cross-encoders) are needed to rerank the top candidates retrieved by dense retrievers, before them being presented to LLMs. As a comparison, we also benchmark the performance of representative re-rankers, both as standalone solves in the MCR setting, and as re-rankers to re-rank candidates retrieved by bi-encoders in the Full setting.\n\nFor Multiple-Choice Retrieval, we evaluate both retrievers and re-rankers, on the datasets that are originally multiple-choice problems (NLI, HellaSwag, PIQA, SIQA, WinoGrande), and ones that can be constructed to a MCR problem (C-STS). For Full-Dataset Retrieval, we evaluate both performance of using only retrievers, and retrieval+reranking performance as in a typical retrieval pipeline. We evaluate the FULL setting for every dataset (NLI, HellaSwag, PIQA, SIQA, WinoGrande, TempReason, SpartQA, Math-pooled, Code-pooled) except C-STS, which is not suitable for full retrieval due to potential sparse annotation effect - its original correct answer might not be the most suitable one throughout the corpus."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Figure 1 outlines the relative performance of the evaluated models. Due to the vastly different scales of performance across tasks, simply taking the arithmetic gain is misleading (by biasing towards a low base). Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base.\n\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models’ language understanding abilities is relevant to the diversity of training data and instruction-tuning, with the current widely-adopted paradigm of instruction-aware information retrieval. Injecting instructions understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks, but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\n\nWe observe a scaling behavior through the varied versions of models of same classes. The pattern is consistent for Instructor, BGE, and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor’s training through the SuperNI collection, its performance gain on these datasets is more relevant to a stronger fitting ability to training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seen these tasks according to the technical report. Although the finding is consistent with who find that larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks, but less known for embedding tasks.\n\nIt is observed that instructions generally distract the retrievers from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models such as OpenAI-embedding-3-small.\n\nLatest instruction-aware decoder representational models seem to start bridging this behavioral gap, pointing the promising direction of scaling decoder models for next-level representational abilities. Cohere’s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time compared to latest models that yield better absolute performance on RAR-b, this behavior is surprising, and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available whether Cohere’s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities cannot add to our main narrative of the potential for decoder representation models.\n\nFor models that degrade with instructions, it is more severe in full-dataset retrieval settings compared to the Multiple-choice Retrieval setting as shown in later parts, intuitively because of a larger candidate pool to be distracted by. This phenomenon is most pronounced for older-generation embedding models such as Sentence Transformer model all-mpnet-base-v2. Though sharing the same training set with all-MiniLM-L6-v2 while both without training to follow instructions, we suspect it’s more sensitive than the latter to instructions because of its larger capacity. The key takeaway here is, if a model is not trained to follow instructions, its retrieval performance might be largely distracted by instructions, hypothetically as the model capacity gets larger.\n\nWith Sentence Transformer’s earliest models being the pioneer in sentence embeddings in the contextualized LM’s era, the field has now entered a stage where sentence embedding models need to model more complex nuances in human language.\n\nMultiple-choice retrieval settings serve as a proxy measurement of how well the correct reasoning can be found through representation matching, given a few selected possible answers already. From the MCR experiments, we can draw the conclusion that current dual-dense retrievers fail to serve as late interaction reasoners.\n\nA special case in the models is Instructor, which is known to have seen the formats of the first 6 tasks through finetuning on Super-NI, enabling it to achieve a better performance on these tasks compared to traditional S-DR models. Yet, its performance on the two settings of CSTS that we construct is stuck at chance level. Without solid control experiments, we hypothesize that this pattern stems from the subpar generalizability of"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Full-dataset Retrieval Setting",
            "text": "Table 2 presents the results for performance. Figure 1 outlines the relative performance of the evaluated models. Because of the different scales of performance across tasks due to different task difficulties and corpus sizes, we take the geometric mean across tasks to represent each model’s average performance. We find geometric mean to be more reflective of model’s average performance, compared to harmonic mean and Z-scored mean. Figure 2 provides insights into the performance gain/degrade brought by prepending task instructions before queries. Due to the vastly different scales of performance across tasks, simply taking the arithmetic gain is misleading. Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base.\n\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models’ language understanding abilities is relevant to the diversity of training data and instruction-tuning (Wang et al., 2023a), with the current widely-adopted paradigm of instruction-aware information retrieval (Asai et al., 2022; Su et al., 2022; Wang et al., 2023a; Muennighoff et al., 2024). Injecting instructions understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks (Wei et al., 2021; Wang et al., 2022a; b), but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\n\nWe observe a scaling behavior through the varied versions of models of same classes. The pattern is consistent for Instructor (Su et al., 2022), BGE (Xiao et al., 2023c) and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor’s training through the SuperNI collection (Wang et al., 2022b), its performance gain on these datasets is more relevant to a stronger fitting ability to training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models (Xiao et al., 2023c) is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seen these tasks according to the technical report (Xiao et al., 2023c). Although the finding is consistent with (Ni et al., 2022) who find that larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks (Kaplan et al., 2020; Hoffmann et al., 2022; Wei et al., 2022), but less known for embedding tasks (Muennighoff, 2022).\n\nIt is observed that instructions generally distract the retrievers from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models such as OpenAI-embedding-3-small. Latest instruction-aware decoder representational models (Wang et al., 2023a; Muennighoff et al., 2024) seem to start bridging this behavioral gap, pointing the promising direction of scaling decoder models for next-level representational abilities. Cohere’s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time (Nov 02, 2023) compared to latest models that yield better absolute performance (OpenAI-v3, E5-Mistral, GritLM) on RAR-b, this behavior is surprising, and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available whether Cohere’s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities cannot add to our main narrative of the potential for decoder representation models.\n\nFor models that degrade with instructions, it is more severe in full-dataset retrieval settings compared to the Multiple-choice Retrieval setting as shown in later parts, intuitively because of a larger candidate pool to be distracted by. This phenomenon"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Does Reranker Help?",
            "text": "In a standard two-step retrieval pipeline, reranking models are utilized to further rerank the first-round results retrieved with bi-encoder embeddings. As we will show in the next sub-section through multiple-choice retrieval (Table 3), nowadays’ reranker models are not trained to understand reasoning-level language, and are already outperformed by recent bi-encoders with decoder backbones trained with instruction tuning.\n\nBut can we fine-tune the reranker to improve performance on such tasks? Note that it is non-trivial to fine-tune a bi-encoder on each task: with varied sizes of each training set and the de-facto usage of InfoNCE loss, we do not want to overfit a bi-encoder to an isotropic distribution for one reasoning task. And intuitively, cross-attention enables faster convergence to tasks with deeper language understanding requirements.\n\nIn this section, we finetune Cohere’s proprietary rerank-english-v2.0 model through API finetuning, because of its well-configured infrastructure. For each task, we construct a training set using its original MCR-format dataset, with the groundtruth being the positive pair for each question, and all non-groundtruth candidates as hard negatives. We evaluate NLI, HellaSwag, PIQA, SocialiQA and Winogrande to understand the pattern, and fine-tune a reranker model for each task. We can easily achieve state-of-the-art performance by re-ranking the top 100 documents with a fine-tuned Cohere reranker. Note that the first-round retrieval result puts a large cap on the later re-ranking upperbound.\n\nAlthough recent papers have confirmed generative models’ potential as rerankers, they are neither trivial to train nor cheap (yet) in inference. Considering the release time of Cohere’s rerankers, we confirm using previous established cross-encoder frameworks is still the simple, robust and strong solution for such tasks. However, we envision decoder models to still be the future for scaling for complicated tasks, and position the effective training of decoder-based rerankers and resource-efficient inference of them as a valuable research direction."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Multiple-choice Retrieval Setting",
            "text": "Table 3 presents the results of the Multiple-choice Retrieval (MCR) setting, where each dataset has a baseline characterized by random predictions. The MCR setting provides an in-depth check of retrievers’ nuanced understanding. Although we evaluate MCR with a separate implementation, we note that in essence, MCR is equivalent to checking whether the correct choice is ranked above all wrong choices in the Full setting. \n\nThe findings could be summarized as follows: From Table 3, we can easily find that models struggle with Winogrande (Sakaguchi et al., 2021) and CSTS (Deshpande et al., 2023) at around chance level. Although some models perform well on Winogrande in the Full setting, we find that this performance might be more of a mirage. For example, Contriever can achieve high performance on Winogrande without instruction in the Full setting, outperforming even the OpenAI-Embedding-3-large model. However, its performance on MCR is stuck at chance level. We find that the two candidate answers in Winogrande are both words that exist in the queries (e.g., entities), and it requires only finding these two words and ranking them at the top to get a good performance in the Full setting, making this task fall back on being a name entity matching task for the Full setting, without the models needing to develop a fine-grained understanding in distinguishing them.\n\nMultiple-choice retrieval settings serve as a proxy measurement of how well the correct reasoning can be found through representation matching, given a few selected possible answers already. From the MCR experiments, we can draw the conclusion that current dual-dense retrievers fail to serve as late interaction reasoners.\n\nA special case in the models is Instructor, which is known to have seen the formats of the first 6 tasks through finetuning on Super-NI (Wang et al., 2022b), enabling it to achieve better performance on these tasks compared to traditional S-DR models. Yet, its performance on the two settings of CSTS that we construct is stuck at chance level. Without solid control experiments, we hypothesize that this pattern stems from the subpar generalizability of encoder models. Although the pattern can currently be empirically supported by the better performance achieved by similarly instruction-tuned decoder models including E5-Mistral, GritLM, and OpenAI models, the generalization gap between encoders and decoders has not yet been systematically investigated on information retrieval, and we position this as a valuable research topic.\n\nSurprisingly, Contriever, as an unsupervised model only trained on different spans of text without labeled document pairs, serves as a strong baseline for all datasets. We speculate this pattern is due to the formats of question-answer pairs in reasoning datasets looking like the way that Contriever is trained on. For instance, a question and its answer in HellaSwag essentially compose a consecutive document, while being like a span pair that Contriever is trained on. For this reason as well, Contriever is extremely robust to instructions, because a prefix instruction does not make the following question and answer look less like a consecutive span in the document."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Behavioral Analysis",
            "text": "In this section, we present more in-depth understanding of retrievers’ behaviors through the tasks that have multiple settings.\n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like “who is the president of United States in 2003?” and the retrievers thus need to encode “George W. Bush” to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama’s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. Note that this above example is for illustrating the concept with the same format as TempReason, and is not from the TempReason dataset, as TempReason is made of much less well-known people and facts, and thus more difficult. In the setting, we are essentially evaluating retrievers’ reading comprehension abilities. Intuitively, the “fact” is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model’s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges. However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedding models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average TR scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting.\n\nThe translated nature of HumanEvalPack grants us the convenience to inspect models’ familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions.\n\nHere, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the “semi-groundtruth” is placed over the groundtruth, indicating the model’s failure of perceiving about programming languages through instructions.\n\nAnother property we present is the Entity Shortcut. In Table 5, we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the “instruct” setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the “original” setting) with a self-contained instruction.\n\nAlthough this setting forms a more self-contained scenario that would happen in real-life question answering, the function declaration presents in both the query and the document, enabling the retrieval to rely solely on the function declaration matching. We speculate this pattern to be highly relevant to the “entourage effect” towards dominant tokens (function declaration in this case) after contrastive learning found in Xiao et al. Due to this reason, we opt for the original format in the final evaluation.\n\nWe further conduct an experiment by constructing the queries and corpus with only TinyCode, a synthetic dataset of code question answering. This dataset provides the natural mixture of natural language and"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Temporal Reasoning",
            "text": "With the 7 sub-settings we construct for TempReason (Table 4), we find that it provides a multi-faceted breakdown of retrievers’ behaviors, providing understanding of their parameterized knowledge, reading comprehension abilities, and robustness against irrelevant context, and thus we provide an analysis of it in this sub-section.\n\nTempReason-L1 (TR-1) in the original dataset concerns evaluating the built-in time perception of models, using time arithmetic questions. We evaluate this task with only one setting - the standalone setting (denoted as Pure). Query and groundtruth document in TR-1 looks like “When is 7 months after Nov, 1998?” and “June, 1999”.\n\nFor TempReason-L2 (TR-2) and TempReason-L3 (TR-3), we respectively construct three settings: Pure, Fact, and Context+Fact. These two levels of tasks evaluate question answering situated in time, providing us the understanding of the three aspects outlined above.\n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like “who is the president of United States in 2003?” (note that questions in TempReason are in a similar format but are much harder and niche than this), and the retrievers thus need to encode “George W. Bush” to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama’s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. Note that this above example is for illustrating the concept with the same format as TempReason, and is not from the TempReason dataset, as TempReason is made of much less well-known people and facts, and thus more difficult. In the setting, we are essentially evaluating retrievers’ reading comprehension abilities. Intuitively, the “fact” is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model’s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges (Xiao et al., 2023a). However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedding models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average TR scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Code Retrieval",
            "text": "Another area that we can get a more fine-grained understanding about retrievers’ behaviors is through code retrieval, given the extensive settings we explored in the construction of the final pooled evaluation dataset. The translated nature of HumanEvalPack (Muennighoff et al., 2023a) grants us the convenience to inspect models’ familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions (e.g., for entries that require “retrieve a Python snippet”, a Javascript snippet of the same content is instead ranked above the groundtruth Python snippet).\n\nHere, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the “semi-groundtruth” is placed over the groundtruth, indicating the model’s failure of perceiving about programming languages through instructions.\n\nAnother property we present is the Entity Shortcut. In Table 5 (left), we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name, and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the “instruct” setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the “original” setting) with a self-contained instruction.\n\nWith the Instruct setting, we see a huge jump of nDCG@10 from the original setting, indicating the easiness of this setting for the model. An example for this setting is given in Appendix C, demonstrating the entity matching shortcut. Although this setting forms a more self-contained scenario that would happen in real-life question answering, the function declaration presents in both the query and the document, enabling the retrieval to rely solely on the function declaration matching. We speculate this pattern to be highly relevant to the “entourage effect” towards dominant tokens (function declaration in this case) after contrastive learning found in (Xiao et al., 2023b). Due to this reason, we opt for the original format in the final evaluation.\n\nWe further conduct an experiment by constructing the queries and corpus with only TinyCode, a synthetic dataset of code question answering. This dataset provides the natural mixture of natural language and code. We sample 15k queries and 150k documents including the groundtruth to form the corpus. The result is shown in Table 5 (right). Although hard to construct controlled experiments, it is seen that models generally perform less well on the mixture of natural language and code."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the critical need of evaluating retrievers’ next-level language understanding abilities, moving beyond the conventional scope of semantic textual similarity and topical matching. We introduced the RAR framework, a novel problem that casts reasoning tasks as retrieval tasks. Through extensive evaluation, we revealed retrievers’ behaviors in understanding reasoning-level language and depicted the behavioral discrepancies between retrievers and LLMs, suggesting promising directions in future research in retrieval-augmented generation. We assert that representation models excelling in RAR-b are well-suited to enhance LLMs within RAG systems, positioning RAR-b as a vital benchmark for tracking advancements in the field."
        }
    ]
}