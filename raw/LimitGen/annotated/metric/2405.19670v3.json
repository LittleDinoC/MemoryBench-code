{
    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs’ general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs’ original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs’ performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach. The virtual tokens and training code are publicly available at https://github.com/DaoD/SPRING/.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks. Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge.\n\nTo address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy. By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering.\n\nExisting efforts in RAG development can be roughly categorized into two groups. The first group leverages the in-context learning capabilities of LLMs by incorporating retrieved information into the input along with appropriate prompts. This allows for straightforward application to any off-the-shelf LLM without tuning its parameters. However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM’s ability to interpret these prompts. The second group focuses on training LLMs to enhance their performance in RAG scenarios. This training might involve either end-to-end pre-training or fine-tuning for specific tasks. These approaches can often lead to better performance, but they require significant computational resources. Recently, parameter-efficient fine-tuning techniques, such as LoRA, have been widely studied, significantly reducing training costs. These methods can optimize the LLMs’ parameters for RAG, but unfortunately compromise the model’s general generation abilities in scenarios without retrieval. This limitation prevents their application to LLMs already operational in real-world settings.\n\nTherefore, a critical research problem arises: Is it possible to enhance LLMs’ performance under RAG scenarios while preserving their general generation capabilities? To achieve this, we introduce a novel, lightweight tuning method named SPRING, which learns Scalable and Pluggable virtual tokens for retrieval-augmented Generation. Our basic idea is to add trainable virtual tokens to help LLMs learn RAG problems. Through fine-tuning, these virtual tokens effectively enhance the LLM’s capability to understand retrieved information and its correlation with user inputs. Importantly, as the LLM’s original parameters are frozen, its general generation abilities are preserved without any loss. During inference, when retrieval is triggered, these trained virtual tokens can be simply added to the prompt, which includes both the retrieved results and user input, thereby significantly enhancing performance. Moreover, we employ a scalable training approach, allowing the number of virtual tokens to be adjusted according to the needs of the inference scenario. Various training strategies have been implemented to further improve the generalizability of our method, ensuring robustness regardless of the number of the retrieved results.\n\nIn experiments, SPRING is trained with Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b on nine commonly used QA datasets and evaluated in both in-domain and out-of-domain tasks. The experimental results demonstrate that SPRING not only effectively improves the RAG performance of LLMs but also successfully preserves their general generation capabilities. Overall, the SPRING method exhibits four main characteristics:\n\nLightweight yet effective. Instead of updating the full parameters of the LLMs, we opt to freeze the pre-trained models and only learn the embeddings for the added virtual tokens. For example, adding 50 tokens to the Mistral-7b model introduces only 0.2M parameters in total.\n\nScalable. With our proposed scalable training approach, SPRING can be effective with any number of virtual tokens. Remarkably, even just one token can substantially improve the LLMs’ performance in RAG scenarios.\n\nPluggable. Owing to its lightweight design, SPRING can be applied in a plug-and-play manner. When retrieval is triggered, simply adding the virtual tokens can lead to better performance. In non-RAG scenarios, the virtual tokens are not added so the LLMs’ original capabilities can be well preserved. This characteristic is crucial for LLMs that have already been deployed for practical use.\n\nGeneralizable. Our robust training strategies ensure that SPRING is adaptable to different retrievers and various numbers of retrieved results. Consequently, there is no need to retrain SPRING with each update to the retrieval system, enhancing its practicality and efficiency."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Compared to standard text generation, RAG incorporates a retrieval module that accesses external knowledge to enhance generation quality Lewis et al. (2020b  ###reference_b28###); Guu et al. (2020  ###reference_b12###); Zhu et al. (2023  ###reference_b62###). The mainstream RAG follows a “retrieve-then-read” paradigm, where the retrieval module provides external knowledge as additional context, which is then read by generation models to produce the final output Izacard et al. (2023  ###reference_b20###); Shi et al. (2023  ###reference_b49###); Ram et al. (2023  ###reference_b45###); Borgeaud et al. (2022  ###reference_b2###); Lin et al. (2023  ###reference_b30###); Zhu et al. (2024  ###reference_b63###). To optimize the use of external knowledge, some methods focus on crafting effective prompts that guide the utilization of retrieved information Shi et al. (2023  ###reference_b49###); Ram et al. (2023  ###reference_b45###). These prompt-based methods are applicable to any LLM without tuning its parameters. However, they depend heavily on skillful prompt writing and the LLMs’ ability to understand instructions. In contrast, other studies attempts to directly train the model to better use the retrieved knowledge. For example, REALM Guu et al. (2020  ###reference_b12###) and RETRO Borgeaud et al. (2022  ###reference_b2###) incorporate retrieval in end-to-end retrieval-augmented pre-training. RA-DIT Lin et al. (2023  ###reference_b30###) employs fine-tuning to enhance LLMs’ retrieval understanding. These tuning-based methods often yield better performance than prompt-based methods by optimizing model parameters for RAG. However, they may compromise the LLMs’ general capabilities, particularly in non-retrieval scenarios. Different from existing methods, we design a new lightweight tuning method for RAG. It is a plug-and-play module that enhances RAG performance using trainable virtual tokens, which can be removed in non-RAG scenarios to preserve the LLMs’ general generation abilities.\nThe paradigms of “pre-training then fine-tuning” have demonstrated efficacy across various natural language Devlin et al. (2019  ###reference_b9###); Raffel et al. (2020  ###reference_b43###); Lewis et al. (2020a  ###reference_b27###); Radford et al. (2019  ###reference_b42###) and vision tasks He et al. (2020  ###reference_b13###); Dosovitskiy et al. (2021  ###reference_b10###); Chen et al. (2020  ###reference_b4###). The common fine-tuning process involves tuning all parameters of a model, which is computational intensive, especially for LLMs. To address this, PEFT Mangrulkar et al. (2022  ###reference_b36###) approaches have been developed. These approaches freeze most of the pre-trained models’ parameters, yet still manage to achieve comparable performance on downstream tasks. PEFT has been widely studied Wan et al. (2023  ###reference_b53###), and typical methods including adapter-based tuning Houlsby et al. (2019  ###reference_b16###); Lin et al. (2020  ###reference_b31###); Rebuffi et al. (2017  ###reference_b46###); Chen et al. (2023  ###reference_b5###), low-rank adaptation (LoRA) Hu et al. (2022  ###reference_b17###); Dettmers et al. (2023  ###reference_b8###), and prompt tuning Li and Liang (2021  ###reference_b29###); Lester et al. (2021  ###reference_b26###); Liu et al. (2021b  ###reference_b34###, a  ###reference_b33###); Qin and Eisner (2021  ###reference_b41###). Adapter-based tuning inserts lightweight modules into a model’s existing layers and have been extended to various domains Gao et al. (2024  ###reference_b11###); Hu et al. (2023  ###reference_b18###); Zhang et al. (2023a  ###reference_b60###). LoRA Hu et al. (2022  ###reference_b17###) introduces trainable low-rank matrices that adjust the model’s weight updates, achieving promising fine-tuning performance on LLMs Hu et al. (2023  ###reference_b18###). Prompt tuning incorporates a series of trainable prompt tokens to LLMs. These tokens can be inserted either to the input layer only Lester et al. (2021  ###reference_b26###); Liu et al. (2021b  ###reference_b34###) or to all of the intermediate layers Li and Liang (2021  ###reference_b29###); Liu et al. (2021a  ###reference_b33###). In this paper, we proposes a novel prompt tuning method, SPRING, specifically designed for RAG scenarios. Our method introduces virtual tokens between retrieved results and the input, exploiting the auto-regressive generation paradigm to improve the model’s ability to utilize retrieved information. Additionally, it is designed to be scalable and pluggable, thus broadening its application scope while preserving the original generative capabilities of LLMs.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To take advantage of both the flexibility of prompt-based methods and the efficacy of fine-tuning-based methods, we propose SPRING to learn scalable and pluggable virtual tokens for retrieval-augmented generation (RAG). In practical developments, LLMs are often constrained by their maximum input lengths, limiting the number of tokens available for retrieval augmentation (especially when the retrieved results are very long). Therefore, it is desired to design a mechanism so that any number of virtual tokens can be used in the inference to improve RAG performance. To achieve this, we propose an optimization strategy working like a “spring.” Specifically, for a given sample with the total number of added tokens, we randomly select a number and utilize the first virtual tokens to construct the training example. This method allows for the flexible optimization of any number of virtual tokens. Consequently, the number of virtual tokens incorporated during inference can be dynamically adjusted based on the requirements of the application. \n\nDue to its designed structure, our method provides considerable flexibility in application. Practically, if user input is assessed to require external knowledge, our virtual tokens can be simply appended after the retrieval results and then fed, along with the user input, into the LLM for generation. In contrast, if the user input does not necessitate retrieval, it can be processed directly by the LLM. As our approach does not adjust the original parameters of the LLM, it preserves the model’s inherent capabilities. This feature is particularly important for industry or business since existing LLMs may have already been deployed for multiple purposes; our method enhances the retrieval understanding capabilities of these models without compromising their existing functionalities.\n\nWe illustrate the instructions for using our SPRING. After training, the embeddings of the added tokens have been optimized for RAG, but these tokens do not correspond to any existing tokens in the vocabulary. To make them easy to use, we can add some special tokens (e.g., [r1], ..., [r50]) to the vocabulary and initialize their embeddings with the trained embeddings. Then, during inference, after obtaining the retrieved results, we can add any number of these special tokens (e.g., [r1] ... [rk]) after and input them with the question to the LLMs for generation. We also provide an example code snippet in Appendix A for using our method in practice.\n\nWe refer to our method as SPRING due to its scalable and pluggable nature, making it particularly well-suited for enhancing existing LLMs that are already deployed. Additionally, it effectively bridges the gap between retrieved results and user input, significantly improving the LLMs’ capabilities in understanding the retrieved external knowledge."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "Language models are designed to calculate the probability distribution over sequences of natural language texts. Auto-regressive models are commonly used for this through next-token prediction:\nwhere  denotes the sequence of tokens preceding  at each step, and  represents the parameters of the model. For RAG, a retrieval corpus  and a retriever  are introduced. Then, the generation process is conditioned on both  and the retrieved results  as:\nNote that here  serves as the query for retrieval. In question-answering (QA) tasks,  is usually the question , and the learning objective is to generate the right answer . The retriever can yield multiple passages, which can be concatenated as a long text sequence using proper separator such as “nn”. For brevity, this formulation directly concatenates the retrieved results  with the question , omitting more complex prompt designs. Henceforth, we will use the notations in QA tasks as our evaluation is performed on them."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "SPRING: Scalable and Pluggable Virtual Tokens for RAG",
            "text": "Our SPRING method, shown in Figure 2, introduces trainable virtual tokens into the input to optimize LLMs for RAG scenarios. Specifically, following the notation in Equation (3), we add trainable tokens between the retrieved results and the input. In our implementation, we set for training. The generation process can then be described as: where represents the added parameters of the trainable tokens (i.e., their embeddings), and is the embedding size of the LLM. denotes the parameters of the backbone LLM, which are frozen during training. Given that , our method is highly efficient for training. For example, with the Mistral-7b model (where ), when tokens are added, we only add and train parameters, approximately 0.003% of the full model. Importantly, we place the virtual tokens between the retrieved results and the question for two main reasons: (1) In the auto-regressive generation paradigm, positioning the tokens after the retrieved results allows them to attend to this information, thereby aiding the model’s comprehension. (2) Recent studies have indicated that LLMs are particularly sensitive to the end of an input. By consistently placing these virtual tokens before the question across all test samples, we aim to mitigate any potential adverse effects on the understanding of the question. In practical developments, LLMs are often constrained by their maximum input lengths, limiting the number of tokens available for retrieval augmentation (especially when the retrieved results are very long). Therefore, it is desired to design a mechanism so that any number of virtual tokens can be used in the inference to improve RAG performance. To achieve this, we propose an optimization strategy working like a “spring” (as shown in Figure 2). Specifically, for a given sample with the total number of added tokens, we randomly select a number and utilize the first virtual tokens to construct the training example as. This method allows for the flexible optimization of any number of virtual tokens. Consequently, the number of virtual tokens incorporated during inference can be dynamically adjusted based on the requirements of the application. The effectiveness of this strategy and its comparison with other methods are further discussed in Section 4.4.2. Due to its designed structure, our method provides considerable flexibility in application. Practically, if user input is assessed to require external knowledge, our virtual tokens can be simply appended after the retrieval results and then fed, along with the user input, into the LLM for generation. In contrast, if the user input does not necessitate retrieval, it can be processed directly by the LLM. As our approach does not adjust the original parameters of the LLM, it preserves the model’s inherent capabilities. This feature is particularly important for industry or business since existing LLMs may have already been deployed for multiple purposes; our method enhances the retrieval understanding capabilities of these models without compromising their existing functionalities. We illustrate the instructions for using our SPRING in Figure 3. After training, the embeddings of the added tokens have been optimized for RAG, but these tokens do not correspond to any existing tokens in the vocabulary. To make them easy to use, we can add some special tokens (e.g., [r1], , [r50]) to the vocabulary and initialize their embeddings with the trained embeddings. Then, during inference, after obtaining the retrieved results, we can add any number of these special tokens (e.g., [r1] [rk]) after and input them with the question to the LLMs for generation. We also provide an example code snippet in Appendix A for using our method in practice. We refer to our method as SPRING due to its scalable and pluggable nature, making it particularly well-suited for enhancing existing LLMs that are already deployed. Additionally, it effectively bridges the gap between retrieved results and user input, significantly improving the LLMs’ capabilities in understanding the retrieved external knowledge."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets and Retrievers",
            "text": "We conduct experiments on nine commonly used question-answering datasets, including TriviaQA (TQA) Joshi et al. (2017), Natural Questions (NQ) Kwiatkowski et al. (2019), HotpotQA (HQA) Yang et al. (2018), SQuAD 1.1 Rajpurkar et al. (2016), Web Questions (WebQ) Berant et al. (2013), 2WikiMultiHopQA (2Wiki) Ho et al. (2020), CoQA Reddy et al. (2019), MS MARCO Nguyen et al. (2016), and PopQA Mallen et al. (2023). These datasets are publicly available at HuggingFace or their official websites. Since PopQA only has a test set, we use it as a held-out dataset to evaluate the generalizability of the methods. Except for PopQA, we mix the training set of all other datasets for training, and test the methods on their corresponding test set. If the test set is unavailable, we use the development set for evaluation. It is worth noting that, though some datasets have provided golden reference passages for the answer, we do not use them in our experiment but use the passages retrieved from the following retrieval sets in both training and inference stages.\n\nFor the retrieval sets, we follow previous studies Yoran et al. (2023) and use the combination of Wikipedia and MS MARCO datasets as the retrieval corpus. Wikipedia contains high-quality human knowledge, which is helpful for many knowledge-intensive tasks. MS MARCO contains a large amount of web pages, which can provide information necessary for curating some natural language questions. We use the datasets that have already been preprocessed into passages and released on HuggingFace. The Wikipedia set has 21M passages, while the MS MARCO set has 8M passages. More details are provided in Appendix B.\n\nWe use E5-large Wang et al. (2022) as the main retriever in our experiments. The impact of other retrievers, i.e., BM25 Robertson and Zaragoza (2009), BGE-base Xiao et al. (2023), and E5-base, is studied in our further analysis (Section 4.4.3). Among these retrievers, BM25 is a non-neural sparse retrieval algorithm, while others are neural-based dense retrievers. In general, dense retrievers perform better than BM25 on several benchmarks Wang et al. (2022); Izacard et al. (2022)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baseline Methods",
            "text": "We consider both the base and instruction-tuned versions of Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b as the backbone models, and compare our SPRING with the following baselines.\n\nConcat: This method directly concatenates the retrieval results and the question for evaluation.\n\nPrompt: This method uses a manually-crafted prompt to indicate the use of retrieval information (details are provided in Appendix C).\n\nLoRA Hu et al. (2022): This method uses LoRA to fine-tune the backbone models. We use the hyperparameters suggested by the LLaMA’s official guidance.\n\nPrefix-tuning Li and Liang (2021): This method uses prefix-tuning to fine-tune the backbone models. To make a fair comparison with our method, we add 50 prefix tokens for training.\n\nThe implementation details are provided in Appendix D."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "We fine-tune the prefix-tuning, LoRA, and SPRING methods on RAG tasks, and then evaluate their performance in scenarios both with (RAG) and without (non-RAG) retrieval. For SPRING, we use virtual tokens for inference by default, and the impact of token quantity is discussed in Section 4.4.2. The experimental results are shown in Table 1. To save space, we only show the results based on Mistral-7b-instruct, and other results are provided in Appendix E.\n\nWe can observe:\n\n(1) SPRING significantly improves the RAG performance of the original LLM with manually-crafted prompts. It even outperforms LoRA on certain datasets, such as TriviaQA and CoQA. Given that SPRING involves only 0.2M trainable parameters, these results demonstrate its remarkable efficiency and effectiveness.\n\n(2) While LoRA achieves slightly better performance, it adjusts the LLMs’ original parameters, which adversely impact their performance in non-RAG scenarios — a significant drop has been observed, far worse than the original models. This challenge also extends to other general generation tasks, which will be discussed in the next section.\n\n(3) Using manually-crafted prompts is effective for improving LLMs’ performance on RAG tasks. However, this improvement is limited as no training is involved.\n\n(4) All backbone models show improvements with SPRING, demonstrating its adaptability across various backbone models.\n\n(5) SPRING achieves the best performance on the held-out test set, PopQA, validating the good generalizability of our method.\n\n(6) Interestingly, prefix-tuning cannot perform well for RAG, highlighting that the insertion position of the virtual tokens in SPRING is both reasonable and effective. We conduct more analysis in Appendix F."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Further Analysis",
            "text": "We further conduct a series of experiments to investigate the impact of different settings in SPRING. All the following experiments are conducted based on fine-tuning the Mistral-7b-instruct model."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Performance on Other Tasks",
            "text": "To examine the impact of different fine-tuning methods on the inherent capabilities of LLMs, we evaluate the performance of models fine-tuned by LoRA and SPRING on several other (non-RAG) tasks. These tasks are commonly used to evaluate LLMs’ reasoning, mathematical abilities, and world knowledge, including BoolQ Clark et al. (2019  ###reference_b6###), CommonsenseQA Talmor et al. (2019  ###reference_b50###), GSM8K Cobbe et al. (2021  ###reference_b7###), and MMLU Hendrycks et al. (2021  ###reference_b14###). The experimental results are shown in Table 2  ###reference_###.444We notice that our results are quite different from those officially reported, which we attribute to the impact of different prompts. Since official prompts for testing are unavailable, we provide the prompts we used in Appendix C  ###reference_### to facilitate reproducibility. From the results, we can observe: (1) Thanks to the plug-and-play design of our method, SPRING can revert to the original LLMs by not using virtual tokens. Therefore, it successfully preserves the original capabilities of the LLMs. In contrast, LoRA, which adjusts the model’s parameters for RAG tasks, inevitably compromises the model’s performance on other tasks. (2) A noticeable decline is observed in the few-shot evaluation, reflecting a decrease in the in-context learning abilities of LLMs. This decline may stem from the fact that RAG fine-tuning does not incorporate in-context learning capabilities. Besides, fine-tuning for RAG tasks may lead the model to overfit to specific task formats, thereby impairing its general generation abilities (more empirical studies are detailed in Appendix G  ###reference_###)."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Impact of Token Quantity",
            "text": "In SPRING, we design a scalable training approach that enables the use of arbitrary numbers of virtual tokens in inference. To validate its effectiveness, we test the performance of our method with various numbers of virtual tokens and compare it with a variant model trained with a fixed number of tokens. The experimental results are illustrated in Figure 4. In general, we observe that the performance of SPRING increases with more virtual tokens used. Surprisingly, SPRING can significantly enhance LLMs’ performance in RAG scenarios with just a single token, which is very encouraging. This varies across different LLMs (see Appendix H). In comparison, training with a fixed number of tokens limits the flexibility of SPRING, as it can only be used with the same number of tokens in inference."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Effects of Different Retrievers",
            "text": "In our experiments, SPRING is fine-tuned using passages retrieved by E5-large. To investigate its effectiveness with other retrievers, we conduct an experiment by testing its performance with passages retrieved by BM25, BGE-base, and E5-large. The results are presented in Table 3. First, SPRING achieves consistent improvement over the original model using manually crafted prompt, thereby confirming the generalizability of our approach. Second, compared to the original model, the performance gap (variance) among different retrievers becomes smaller, highlighting SPRING’s robustness to variations in retrievers. Finally, even fine-tuned with a superior retriever (i.e., E5-large), SPRING maintains strong performance well with less effective retrievers (such as BM25). This indicates that our method can effectively adapt to varying quality of retrieved results. Hence, there is no necessity to retrain the virtual tokens with each update of retrievers in practical applications, significantly enhancing its applicability."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4 Influence of Retrieved Passages",
            "text": "During the fine-tuning of SPRING, we construct training samples by randomly selecting the top- () retrieved passages. This aims to enhance SPRING’s adaptability by ensuring it can operate effectively with varying numbers of retrieved passages in real-world scenarios. To evaluate the effect of this training strategy, we test the SPRING’s performance across a range from zero to five passages. Figure 5  ###reference_### illustrates the results. SPRING’s performance gradually improves as more retrieved passages are used (), suggesting that more retrieved passages contribute valuable knowledge for question answering. However, the performance peaks at four passages and declines when more passages are added. This decrease could be attributed to noise accumulation within the retrieved knowledge, a phenomenon also reported in recent studies Yoran et al. (2023  ###reference_b59###); Wu et al. (2024  ###reference_b56###). Despite this, the use of retrieved passages still results in performance gains compared to scenarios without retrieval (), highlighting again the benefits of RAG."
        },
        {
            "section_id": "4.4.5",
            "parent_section_id": "4.4",
            "section_name": "4.4.5 Cross-Dataset Generalizability",
            "text": "Inspired by previous studies in multi-task learning Raffel et al. (2020  ###reference_b43###); Khashabi et al. (2020  ###reference_b24###), we mix eight QA datasets for training as they require similar LLM capabilities (e.g., reasoning). To study the impact of this strategy, we conduct experiments by training SPRING on each dataset individually and then testing its performance on the others. Table 4  ###reference_### shows partial results, and more results are available at Appendix E  ###reference_###. As indicated, training on a mixed dataset generally enhances performance on most datasets, thereby validating the benefits of multi-task learning. While training on a single dataset, such as NQ, may yield superior results on its specific test set, such improvements often fail to generalize to other datasets. Notably, training solely on NQ may negatively impact performance on MS MARCO, where the original LLM using a prompt could outperform it. These findings inspire us to carefully consider the interaction between different datasets when applying our method in future applications."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduced scalable and pluggable virtual tokens for retrieval-augmented large language models. Our method, SPRING, serves as a parameter-efficient fine-tuning approach that significantly enhances RAG performance with the addition of only M trainable parameters. More importantly, the plug-and-play nature of our approach successfully preserves the performance of LLMs on non-RAG tasks, while its scalable training strategy broadens the method’s applicational flexibility. Through extensive experiments across various datasets, we have demonstrated the effectiveness, generalizability, flexibility, and high efficiency of our method. We believe that our research will foster further integration of information retrieval and LLMs, and advance the development of other parameter-efficient fine-tuning technologies for LLMs."
        }
    ]
}