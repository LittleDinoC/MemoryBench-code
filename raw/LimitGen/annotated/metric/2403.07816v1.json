{
    "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
    "abstract": "We investigate methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning, and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, Large Language Models (LLMs) have been applied to a wide range of tasks, including code generation, solving math problems, and multilinguality. Training such LLMs requires significant computational resources and data, involving thousands of GPUs and trillions of tokens. The process often involves maintaining synchronized copies of the model across multiple GPUs, which poses challenges related to communication costs and vulnerability to hardware failures.\n\nThe Branch-Train-Merge (BTM) method has been proposed for parallel training of LLMs without synchronization. This method involves creating multiple copies of a seed LLM, each trained on different subsets of data, resulting in independent expert models specializing in various data distributions like knowledge domains or languages. At test time, inputs are classified into domains, allowing expert models to collaborate in predicting outcomes.\n\nA separate approach to manage the computational demands of LLMs is the Mixture-of-Experts (MoE) model, which activates only some parameters at a time. MoE is commonly used in the feedforward sublayer of Transformers, enabling parameter growth without additional computation. However, MoE models require synchronization, which increases communication costs as the number of experts grows.\n\nThis work aims to combine the strengths of both Branch-Train-Merge and Mixture-of-Experts, while addressing their limitations. We propose a method called Branch-Train-MiX (BTX), which involves first training multiple expert LLMs separately in the Branch-Train-Merge fashion, then integrating these experts into a unified MoE architecture. The feedforward sublayers are merged into a MoE module, with a router network guiding the selection of experts for processing tokens. Other components, like self-attention layers, are merged by averaging their weights.\n\nThe resultant BTX model retains the advantages of both approaches: asynchronous expert training reduces communication costs, and the unified model can be fine-tuned or used like standard LLMs. Despite the increase in parameters, the model's inference remains efficient due to sparse activation.\n\nWe outline our method using Llama-2 7B as a seed, training expert LLMs on math, code, and Wikipedia domains. With the original weights included as a fourth expert, the combined MoE model is fine-tuned, allowing it to learn to integrate expert modules effectively. The BTX model maintains the strengths of the seed model while incorporating knowledge from specialized domains, offering a unified solution that leverages the benefits of Branch-Train-Merge and Mixture-of-Experts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Reducing communication between training workers for computational efficiency is a major topic of study for training deep learning systems. Zhang et al. (2015) introduced a method that allows model instances on different workers to diverge from each other, thus eliminating the constant need for synchronization. Instead, the workers are loosely synchronized to master weights using elastic averaging from time to time. Douillard et al. (2023) showed that less frequent synchronization of diverged workers by averaging their weight changes and applying Nesterov momentum works well in practice for training LLMs.\n\nThe Branch-Train-Merge method (Li et al., 2022a; Gururangan et al., 2023) takes parallel training to the extreme by running multiple training processes completely independently. Each training process uses specific domain data, thus the corresponding model becomes an expert in that domain. Finally, the output distributions of those expert models are averaged to make a next token prediction. Which experts to average is decided by classifying the input into one or more of the domains. \n\nMoE is used to scale deep networks in Shazeer et al. (2017) using a simple Top-K routing scheme. Since the routing decisions are discrete and thus cannot be trained by gradient descent, various training methods have been explored for the Transformer architecture (Fedus et al., 2022; Lewis et al., 2021). Surprisingly, Roller et al. (2021) showed that even a fixed routing scheme without any learning works well, if the routing is done via a random mapping based on input tokens.\n\nGururangan et al. (2021) makes experts in feedforward layers specialize to specific domains using a domain-conditioned fixed routing, lacking the asynchronous training of our approach. Our method relates to continual learning (Awasthi and Sarawagi, 2019) because domain experts are trained on datasets with different distributions from the initial data used for training the seed model, which is implemented by continued training after branching. Specifically, our approach is related to parameter isolation methods (Lange et al., 2019) as we have different parameters for different domains.\n\nAljundi et al. (2016) also creates a new copy of a model to train on each domain. Rusu et al. (2016) adds a new model with a new domain, but connects it to the previous models so the previously learned features can be used. Rozière et al. (2023) showed continual training of a seed LLM on a specific domain of code can produce a strong domain expert model, and this converges much faster than starting from scratch. For training a math expert, starting from a code expert rather than a general LLM was shown to be more beneficial (Shao et al., 2024; Azerbayev et al., 2023)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Branch-Train-MiX",
            "text": "Given an existing LLM  which has been pretrained on a large corpora covering a wide variety of topics, we aim to improve its performance on  areas of expertise. This is achieved by continued pretraining with corresponding training datasets , each related to a specific knowledge domain such as math, code, etc.\nThe proposed method contains three stages: Branch, Train, and MiX.\nA common problem with MoE is the emergence of dead experts, which do not get activated by the router at all.\nCommon routing methods like Top-k are unlikely to escape from such a situation because a dead expert is never in the top-k selection, and therefore never receives a training signal.\nLoad balancing offers a simple solution by adding an extra loss term that encourages the experts to be utilized equally.\nWe use a loss term similar to (Fedus et al., 2022  ###reference_b13###):\nHere  is the current data batch, and  is a hyperparameter. This loss is computed in each layer and added to the NLL loss.\nBesides Top-k routing, we also experiment with other routing methods:\nSwitch: It is a Top-1 routing method proposed by Fedus et al. (2022  ###reference_b13###).\nSoft routing: We use softmax as the routing function , so all experts are activated both during training and inference. While it is likely to provide the best performance, it comes at the expense of increased compute.\nSample Top-1: We use the gumbel softmax (Jang et al., 2016  ###reference_b20###) for .\nAt training time, we generate a soft sample from the gumbel softmax, but zero out all its values except the largest one. Then we compute only one expert corresponding to this largest value, omitting the other expert computations. At inference time, we simply do hard sampling. We anneal the temperature to a sharp distribution at the end of training to gradually reduce the discrepancy between training and inference.\nThe number of modules in the MoE layer matches the number of domains we train on, since each module corresponds to one domain.\nHowever, we can increase the number of modules in a simple way by splitting each domain FF sublayer into multiple chunks.\nGiven  domains and an FF activation size of , we split each FF layer into  chunks with a dimension of .\nAs a result, the final MoE layer will have  modules.\nInstead of directly initializing MoE experts from domain experts in a one-to-one way, we also try including all domains in each MoE expert.\nThe motivation behind this is an observation that MoE experts trained in a standard way do not show domain specialization, but rather are activated uniformly across different domains (Jiang et al., 2024  ###reference_b21###).\nIn contrast, our domain experts are specialized to a specific domain through their training data.\nTo break this domain specialization, we split each domain expert’s FF layers into  chunks and then merge the -th chunks from all domains to build the -th MoE expert.\nThis way, each MoE expert contains the same amount of parameters from all domains."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Branch & Train: Embarrassingly Parallel Expert Training",
            "text": "Initializing from the seed model , we train  expert LLMs , with each model  being trained on the corresponding dataset  in the same manner as during pretraining, using the usual language modeling objective.\nSince each expert model  can be trained in complete separation from the others, the whole training process becomes -way embarrassingly parallel. This training paradigm has several benefits in large-scale distributed training. It allows linear scaling of overall training throughput when scaling up the size of compute, while joint training often faces uncertain performance from increasing batch size. It has lower all-to-all communication cost. It is also more resilient, as a single training failure will only affect one of the  training processes instead of halting the entire training.\nAfter all the expert training is finished, we will end up with  different LLMs, with each specializing in a specific distribution.\nAt this point, the Branch-Train-Merge method (Li et al., 2022a  ###reference_b27###; Gururangan et al., 2023  ###reference_b16###) uses these domain experts as is, choosing which expert to use by determining which domain the input belongs to at inference time.\nUsually multiple experts are chosen, and their final output distributions are simply averaged to generate the next token.\nOur BTX approach, in contrast, merges these domain experts back into a single LLM that is finetuned further, as we will describe in the next section."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "MiX: Combining Separate Experts to be a Mixture-of-Experts",
            "text": "We employ a Mixture-of-Experts approach to combine the domain expert models .\nHowever, instead of using the classical procedure of mixing the final outputs from , we do a more fine-grained mixing by performing MoE within each layer of a Transformer.\nIn particular, we combine the different feedforward sublayers from the domain experts into a single MoE sublayer.\nIf  is the feedforward sublayer at the -th layer of the -th domain expert , then the combined MoE layer for input representation  at layer  will compute:\nHere  is a linear transformation and  is a routing function, which usually has sparse output and hence switches on only some experts.\nSince we can skip computing  if the corresponding router output is zero, the actual computation of  will be much more efficient than computing all domain experts.\nHowever, routing decisions can change from token to token, so one input sequence can employ all the domain expert FF layers if needed, even when only a few are accessed at any given token.\nIn our experiments, we use Top-k (k=2) routing where , unless otherwise stated.\nFor the self-attention sublayers, we combine the different domain experts by simply averaging their weights. The motivation behind this is the assumption that the self-attention layers are less domain specialized than the feedforward layers.\nWe do the same averaging for the remaining parameters (embeddings, etc.) as well.\nNote that the only new parameters we introduce are the router’s transformation parameters , which are negligible in size compared to the rest of the network.\nNevertheless, those new parameters need to be finetuned, so the router can make optimal decisions in selecting which domain  to use.\nIn addition, funetuning is helpful because the self-attention weights are constructed by averaging, and are likely not optimal.\nOverall, the entire system has not been optimized for working together at all in the embarrassingly parallel training framework, but our hypothesis is that even a small amount of combined finetuning might make large improvements."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Variations",
            "text": "We also experimented with several variations of our method.\nA common problem with MoE is the emergence of dead experts, which do not get activated by the router at all.\nCommon routing methods like Top-k are unlikely to escape from such a situation because a dead expert is never in the top-k selection, and therefore never receives a training signal.\nLoad balancing offers a simple solution by adding an extra loss term that encourages the experts to be utilized equally.\nWe use a loss term similar to (Fedus et al., 2022  ###reference_b13###  ###reference_b13###):\nHere  is the current data batch, and  is a hyperparameter. This loss is computed in each layer and added to the NLL loss.\nBesides Top-k routing, we also experiment with other routing methods:\nSwitch: It is a Top-1 routing method proposed by Fedus et al. (2022  ###reference_b13###  ###reference_b13###).\nSoft routing: We use softmax as the routing function , so all experts are activated both during training and inference. While it is likely to provide the best performance, it comes at the expense of increased compute.\nSample Top-1: We use the gumbel softmax (Jang et al., 2016  ###reference_b20###  ###reference_b20###) for .\nAt training time, we generate a soft sample from the gumbel softmax, but zero out all its values except the largest one. Then we compute only one expert corresponding to this largest value, omitting the other expert computations. At inference time, we simply do hard sampling. We anneal the temperature to a sharp distribution at the end of training to gradually reduce the discrepancy between training and inference.\nThe number of modules in the MoE layer matches the number of domains we train on, since each module corresponds to one domain.\nHowever, we can increase the number of modules in a simple way by splitting each domain FF sublayer into multiple chunks.\nGiven  domains and an FF activation size of , we split each FF layer into  chunks with a dimension of .\nAs a result, the final MoE layer will have  modules.\nInstead of directly initializing MoE experts from domain experts in a one-to-one way, we also try including all domains in each MoE expert.\nThe motivation behind this is an observation that MoE experts trained in a standard way do not show domain specialization, but rather are activated uniformly across different domains (Jiang et al., 2024  ###reference_b21###  ###reference_b21###).\nIn contrast, our domain experts are specialized to a specific domain through their training data.\nTo break this domain specialization, we split each domain expert’s FF layers into  chunks and then merge the -th chunks from all domains to build the -th MoE expert.\nThis way, each MoE expert contains the same amount of parameters from all domains."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We first analyze how expert LLMs specialize to specific domains. As expected, individual expert LLMs specialize effectively in their respective domain, particularly in the math and code domains. In addition, there are several interesting observations. The math expert training improved its code capabilities as well, indicating a close relation between these domains. However, such single-domain continued training can lead to significant drops in task performance in other domains due to catastrophic forgetting. For example, the math and code experts show reduced capabilities on tasks like TriviaQA compared to the seed model.\n\nThere are methods for continued pretraining such as data-matching (DM) strategies, which include dense and sparse upcycling. The MoE finetuning approach appears beneficial in certain respects. A model like BTX demonstrates efficiency in compute resources during continued pretraining, showing robustness to task interference from multi-task learning.\n\nIn the compute-matching (CM) scenario, BTX maintains training efficiency in comparison to the sparse upcycling baseline. Both methods utilize the same data mixture during the MoE stage but differ in percentage of compute allocated for MoE training. The parallel training of experts within BTX contributes to increased training throughput, allowing training on a larger data set within the same compute budget.\n\nDespite the MoE training requiring only a fraction of the total pretraining budget, BTX provides notable improvements in general capabilities compared to approaches like multi-task learning and Branch-Train-Merge. Sparse upcycling without expert training exceeds performance of dense methods and BTM with equivalent compute budgets. The compute efficiency in BTX is largely attributed to the parallel training of experts preceding the MoE finetuning. The approach emphasizes the effectiveness of BTX training in the latter stages of pretraining."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We base our experiments on the setup used for Llama-2 pretraining (Touvron et al., 2023 ###reference_b37###).\nIn particular, we use the Llama-2 7B model as our seed model."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 BTX Training",
            "text": "We use the pretrained Llama-2 with 7B parameters as our seed model. After making three copies of the seed model Llama-2 7B, we continue training them on the following domain datasets to derive three domain experts:\n\nMath: The same data sources and mixture used in Llemma model training. To ensure comparability, we train on the same amount of data as well, i.e., 48k steps with 201B tokens in total.\n\nCode: The same data sources and mixture of code data used in CodeLlama pretraining. The code expert LLM is trained for 50k steps with 210B tokens for consistency with the math expert.\n\nWikipedia: Wikipedia documents extracted between June to August 2022. The data was preprocessed to remove hyperlinks, comments and other formatting boilerplate. Since this is a smaller dataset, we train a total of 42B tokens.\n\nWe also include the original seed LLM as a “generalist” expert so that its general knowledge is transferred to the final model. Thus, we mix these four expert models into a single MoE model. Then, we finetune this MoE model on all the data sources used to train the four experts (including the original Llama-2 7B pretraining data for the generalist expert) and train for another 80B tokens.\n\nFor BTX with default Top-2 routing, we use load balancing unless otherwise stated. For the Sample Top-1 routing, we use the temperature annealing schedule from a previous study with specific constraints where the number of training steps is considered. For the first layer only, we used soft-routing instead.\n\n###table_3###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Baselines",
            "text": "We compare to the following baselines:\n\nLlama-2: The original Llama-2 7B used as a seed model, as well as Llama-2 13B.\n\nDense: Continues to train the seed LLM with all the data, instead of training separate LLMs on different domain datasets.\n\nWe use the same training data as BTX, first training on the new domain-specific data used in the experts training stage, followed by the same data mixture that includes the Llama-2 pretraining data in the MoE finetuning stage. We call this comparison data-matching (DM).\n\nSparse upcycling: This baseline (Komatsuzaki et al., 2022) initializes a MoE model from the seed model by making 4 identical copies of the feedforward module as experts. We use the Top-2 router with randomly initialized parameters. \n\nBranch-Train-Merge (BTM): This baseline (Li et al., 2022a) uses the same expert LLMs as BTX (including the original seed model) but uses them directly without building a MoE model. For a given context (input), it selects Top-k expert LLMs based on the similarity between the context and experts’ training data. Following Gururangan et al. (2023), both context and experts’ training data are embedded via tf-idf. Top-k experts are selected based on cosine similarity to the mean tf-idf embedding of each expert.\n\nCodeLlama 7B: A language model specializing in code (Rozière et al., 2023) by continued training of the same seed model Llama-2 7B on code data. It also has other features such as long-context and infilling.\n\nLlemma 7B: A language model specializing in mathematics (Azerbayev et al., 2023) by continued training of CodeLlama 7B on math data.\n\nWe use the same optimization hyperparameters for training of the baselines, expert models, and MoE models. We use the AdamW optimizer with weight decay 0.1, and anneal the learning rate to the peak with 100 steps of warmup, and decay to a fraction of the peak with a cosine schedule. We use a batch size of 4M tokens with a sequence length of 4096."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Evaluation",
            "text": "Math and code reasoning, world knowledge, reasoning, and general capabilities are critical for evaluating AI models. These capabilities are assessed using benchmarks such as GSM8K, MATH, HumanEval, MBPP, Natural Questions, TriviaQA, ARC-Easy, ARC-Challenge, SIQA, PIQA, WinoGrande, and MMLU. Each benchmark targets a specific skill set required for a comprehensive evaluation of an AI's ability to handle complex tasks across various domains. Models are tested with different numbers of shots to analyze their skillsets."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We first analyze how expert LLMs specialize to specific domains. In addition, there are several interesting observations. We see that the math expert training improved its code performance as well, indicating a close relation of these domains. However, such single-domain continued training also suffers from catastrophic forgetting with significant performance drops on some tasks in other domains. For example, the math and code expert are much worse on TriviaQA than the seed model.\n\nWhile sparse cycling performs close behind BTX, the parallel training of experts increases the training throughput of BTX. As a result, BTX can train with more than the data than pure MoE given the same training compute budget. Despite that the MoE training stage uses a fraction of the total training budget in pretraining (for example, Llama-2 pretraining uses 2T tokens), BTX brings improvements on general capabilities compared to alternative continued pretraining approaches such as multi-task learning of the dense model and Branch-Train-Merge.\n\nThe compute efficiency gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning. In terms of the active number of parameters, the MoE models are similar to the Llama-2 13B model. This indicates that BTX’s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire of pretraining."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Overall Performance",
            "text": "We first analyze how expert LLMs specialize to specific domains. There are several interesting observations. We see that training in one domain can positively impact related domains. For instance, the math expert training improved its code performance, suggesting a connection between these fields. However, focusing on a single domain can lead to forgetting information relevant to other domains. The math and code experts, for example, perform worse on TriviaQA compared to the seed model. Aggregated results across multiple domains indicate different methods' impacts on task interference from multi-task learning. Training methodologies differ in computational efficiency and robustness, illustrating various approaches to maintaining a balance between domain specialization and preserving broader domain knowledge."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Better compute-performance tradeoff",
            "text": "The X-axis shows the total training compute starting from the seed model measured in GPU days, which includes the domain expert training and finetuning of the MoE model.\n\nThe MoE training stage uses a fraction of the total training budget in pretraining (for example, Llama-2 pretraining uses 2T tokens). \n\nAs a special case of BTX, sparse upcycling without expert training is given the same or larger compute budget. \n\nThe gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning.\n\nIn terms of the active number of parameters (shown as circle sizes), the MoE models are similar to the Llama-2 13B model. BTX uses less than half of the additional training compute compared to Llama-2 13B. This indicates that BTX’s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire pretraining."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Ablations & Analysis",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Ablations of BTX training",
            "text": "For Switch routing, we set its capacity factor to 1.5 (a hard limit after which routed tokens will be dropped). The soft routing is notable since it lacks sparsity and has the highest number of active parameters. Overall, the Top-2 routing offers a good balance.\n\nWe examined additional design choices of BTX. MoE training without load balancing seems to have an impact on the coding task (HumanEval) and math (GSM8k) accuracy. The routing analysis in the next section will provide more understanding of this trade-off.\n\nFreezing the feedforward modules initialized from each expert and only training the rest of the MoE model shows that individual experts already gained sufficient domain knowledge during the branch-train stage. The mix (MoE finetuning) stage primarily trains the other parameters such as averaged weights in the self-attention and the router transformations.\n\nWe also explored our blending and splitting techniques. The process across all tasks is affected when experts are mixed, indicating that domain FF layers should not be mixed in this way. Splitting each domain FF into chunks to obtain multiple modules in the MoE layer also has implications on the routing."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Routing Analysis",
            "text": "To gain an in-depth understanding of BTX, we examine the routing decisions among the experts. Compared to other routing methods, Top-2 routing with load balancing ensures a more uniform distribution of the load between experts. Analyzing the token probability distributions, we observe a shift towards low probability scores across all experts with load balancing, especially closer to the final layers of the model, which contributes to the fair routing. Interestingly, all models without load balance heavily rely on the Math expert, with a low overall contribution from other experts, especially the Code expert. A dead Code expert comes “back to life” with load balancing introduced in training. In fact, it not only becomes visible, but becomes the dominant expert in the math and code domains.\n\nExamples of the routing decisions for Top-2 with load balancing can be found in related analysis. Overall across math domain tasks, tokens are often routed to the Code and Llama-2 7B experts. If we look at a more detailed token distribution, we find that the GSM8K task prefers Code and Llama-2 experts, while the MATH task relies more on the in-domain Math expert. We hypothesize that this happens because the GSM8K dataset consists of grade school math problems that require common sense knowledge and basic arithmetic operations. Both the Code and World knowledge tasks mostly route to the in-domain Code and Wikipedia experts respectively. As observed earlier, when load balancing is introduced, there are adjustments in coding and math task routing, which can be explained with these changes in domain expert routing. The reasoning tasks in contrast exhibit similar behavior, and rely equally on Math and generalist LLM’s expertise."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced Branch-Train-MiX (BTX), a simple continued pretraining method designed to enhance an LLM's capabilities. It involves training multiple copies of a seed LLM to specialize in various domains asynchronously and concurrently, then later combines them into a single Mixture-of-Experts (MoE) model through finetuning.\n\nThe initial parallel training stage offers higher training throughput and scalability. Our method demonstrates that a generalist LLM can benefit from ongoing training on datasets rich in specialized knowledge and skills. The BTX approach offers a more compute-efficient alternative compared to training a larger generalist LLM or several individually specialized LLMs. These findings provide guidance on how to allocate computing resources during late pretraining to develop a robust generalist model."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations & Future Work",
            "text": "Although our experimental results on BTX are promising, we have not fully explored its potential in this paper.\nDue to compute limitations, we only experimented with three domains and four experts in this paper.\nTraining on more domains such as using unsupervised domain discovery (Gururangan et al., 2023  ###reference_b16###) should amplify the benefit of the parallelization of experts training.\nHaving more experts will also make the final MoE model more efficient because the number of active experts can remain the same while its overall capacity increases.\nIn our experiments, we used a simple implementation of MoE and did not optimize it using more complex techniques such as placing different experts on different GPUs to run them in parallel.\nSuch an efficient MoE implementation could shorten the training time of BTX, and the sparse upcycling baseline as well.\nCompared to BTM, BTX provides an approach to finetune the combined experts, which can be directly applied in instruction finetuning or RLHF procedures.\nHowever, we leave that for future work as we focused on the pretraining stage in this paper.\nThe question of whether experts in MoE are better off specializing in specific domains or not is an interesting one that is worth further investigation.\nOur approach explicitly tied experts to certain domains, but such specialization does not seem to emerge naturally during MoE training (Jiang et al., 2024  ###reference_b21###).\nWe observed that some experts are used more in their corresponding domain tasks, showing that their domain specialization partially remains even after the MoE finetuning.\nWe only compared BTX to two of its special variants, i.e. BTM with 100% compute allocated to expert training and 0% on MoE finetuning, and sparse upcycling with 0% compute allocated to expert training and 100% on MoE finetuning. Future work could perform a thorough sweep of the compute allocation ratio between expert training and MoE training. Also, we did not perform experiments with different data mixtures for MoE finetuning other than uniform sampling."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We thank Margaret Li, Kushal Tirumala, Luke Zettlemoyer, Artidoro Pagnoni, Suchin Gururangan, Mike Lewis and Emily Dinan for their discussion and feedback, and Andrew Cohen and Arun Babu for their help with the training implementation."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Data mixture",
            "text": "Table 7  ###reference_### shows the exact data mixture ratios used in training each domain expert.\nFor finetuning the MoE model, we sample datasets that used to train math expert, code expert, wikipedia expert and the original Llama-2 7B with probabilities 30.16%, 40.31%, 10.30% and 19.23%.\n###table_9###"
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We use the same evaluation metrics as is used in Touvron et al. (2023  ###reference_b37###) and Rozière et al. (2023  ###reference_b31###): for code tasks (HumanEval and MBPP) we report pass@1, for math tasks (GSM8k and MATH) and knowledge tasks (Natural Questions and TriviaQA) we report exact match, we report accuracy for MMLU and ARC. We use greedy decoding for all generations."
        }
    ]
}