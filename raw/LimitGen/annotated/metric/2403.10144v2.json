{
    "title": "NLP Verification: Towards a General Methodology for Certifying Robustness",
    "abstract": "Deep neural networks (DNNs) have exhibited substantial success in the field of Natural Language Processing (NLP). As these systems are increasingly integrated into real-world applications, ensuring their safety and reliability becomes a primary concern. There are safety-critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Computer Vision had pioneered the use of formal verification for neural networks for such scenarios and developed common verification standards and pipelines. In contrast, NLP verification methods have only recently appeared in the literature. They are often light on the pragmatic issues of NLP verification, and the area remains fragmented.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Deep neural networks (DNNs) have demonstrated remarkable success at addressing challenging problems in various areas, such as Computer Vision (CV) and Natural Language Processing (NLP). However, as DNN-based systems are increasingly deployed in safety-critical applications, ensuring their safety and security becomes paramount. Current NLP systems cannot guarantee either truthfulness, accuracy, faithfulness, or groundedness of outputs given an input query, which can lead to different levels of harm.\n\nOne requirement in the NLP domain is the requirement of a chatbot to correctly disclose non-human identity when prompted by the user. Recently there have been several pieces of legislation proposed that will enshrine this requirement in law. In theory, the underlying DNN of the chatbot (or the sub-system responsible for identifying these queries) must be 100% accurate in its recognition of such a query. However, a central theme of generative linguistics going back to von Humboldt is that language is ‘an infinite use of finite means,’ meaning there exist many ways to say the same thing. In reality, questions can come in a near-infinite number of different forms, all with similar semantic meanings. For example: “Are you a Robot?”, “Am I speaking with a person?”, “Am I texting to a real human?”, “Aren’t you a chatbot?”.\n\nFailure to recognize the user’s intent and thus failure to answer the question correctly could potentially have legal implications for designers of these systems. Similarly, as such systems become widespread, it may be desirable to have guarantees on queries concerning safety-critical domains, for example when the user asks for medical advice. Research has shown that users tend to attribute undue expertise to systems, potentially causing real world harm.\n\nA question remains on how to ensure that NLP systems can give formally guaranteed outputs, particularly for scenarios that require maximum control over the output. One possible solution is to apply formal verification techniques to deep neural networks (DNN), ensuring that output satisfies the desired properties. This example is an instance of the more general problem of DNN robustness verification, where the aim is to guarantee that every point in a given region of the embedding space is classified correctly.\n\nGiven a network, one defines subspaces of the vector space. For example, one can define subspaces around all input vectors given by the dataset in question (in which case the number of subspaces will correspond to the number of samples in the dataset). Then, using a verification algorithm, each subspace is checked for robustness. Note that each subspace is infinite (i.e., continuous), and thus verification is usually based on equational reasoning, abstract interpretation or bound propagation.\n\nDue to the embedding gap, a small fraction of vectors contained in verified subspaces map back to valid sentences. When a verified subspace contains no or very few sentence embeddings, it is said to have low generalisability, which may render verification efforts ineffective for practical applications. From the NLP perspective, the embedding gap can manifest in subtle ways. Consider a subspace containing sentences semantically similar to ‘I really like to chat to a human. Are you one?’. If the DNN is robust on this subspace, it will identify sentences in this subspace as questions about human/robot identity. However, if an embedding function wrongly embeds opposite class sentences into this subspace, it could falsify verification.\n\nVerification in NLP is susceptible to such problems because the embedding function is not invertible, making it difficult to cross the embedding gap. We propose a general method for measuring generalisability of the verified subspaces based on algorithmic generation of semantic attacks on sentences included in the verified semantic subspace.\n\nAn alternative to geometric approaches is to construct subspaces of the embedding space based on semantic perturbations of sentences. This involves embedding a sentence and its semantic perturbations into a real vector space and enclosing them within a geometric shape. Calculating convex hulls is computationally infeasible for high dimensions, so simpler shapes like hyper-cubes and hyper-rectangles are used. We propose a novel refinement by including hyper-rectangle rotation to increase shape precision, calling the resulting shapes semantic subspaces.\n\nFor robust training regimes in NLP verification, it was unclear whether success resulted from general decision boundary improvements, adversarial robustness, or semantic knowledge. Through experiments, we confirm that semantic subspaces are more generalisable. We conclude that semantically robust training typically outperforms standard methods. For instance, using strong attacks like polyjuice in training, we obtain more generalisable DNNs. Thus, we propose a fully parametric NLP verification approach, disentangling the semantic attack choice, semantic subspace formation, semantically robust training, and verification algorithm choice. This approach, along with the new generalisability metric, offers more principled evaluation of NLP verification methods accounting for the embedding gap and facilitates more transparent benchmarks.\n\nWe implement a tool, ANTONIO, generating NLP verification benchmarks based on the above choices. This paper pioneers using a complete SMT-based verifier for NLP"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "DNN Verification",
            "text": "Formal verification is an active field across several domains including hardware [23  ###reference_b23###, 24  ###reference_b24###], software languages [25  ###reference_b25###], network protocols [26  ###reference_b26###] and many more [27  ###reference_b27###]. However, it was only recently that this\nbecame applicable to the field of machine learning [28  ###reference_b28###].\nAn input query to a verifier consists of a subspace within the embedding space and a target subspace of outputs, typically a target output class.\nThe verifier then returns either true, false or unknown. True indicates that there exists an input within the given input subspace whose output falls within the given output subspace, often accompanied by an example of such input. False indicates that no such input exists.\nSeveral verifiers are popular in DNN verification and competitions [29  ###reference_b29###, 30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###].\nWe can divide them into 2 main categories: complete verifiers which return true/false and incomplete verifiers which return true/unknown.\nWhile complete verifiers are always deterministic, incomplete verifiers may be probabilistic.\nUnlike deterministic verification, probabilistic verification is not sound and a verifier may incorrectly output true with a very low probability (typically 0.01%).\nComplete Verification based on Linear Programming & SMT solving.\nThis group of verification methods [33  ###reference_b33###, 28  ###reference_b28###, 22  ###reference_b22###] is built upon the observation that feed-forward neural networks are defined by the sequential composition of affine transformations and non-linear activation functions.\nWhen the activation functions are piecewise linear\n(e.g. ReLU), the DNN can be encoded by conjunctions and disjunctions\nof linear inequalities and thus linear programming algorithms can be directly applied to solve the satisfiability problem, yielding a solution to complete verification.\nA state-of-the-art tool is Marabou [22  ###reference_b22###], which answers queries about neural networks and their properties in the form of constraint satisfaction problems. Marabou takes the network as input and first applies multiple pre-processing steps to infer bounds for each node in the network. It applies the algorithm ReLUplex [28  ###reference_b28###], a combination of Simplex [34  ###reference_b34###] search over linear constraints, modified to work for networks with piece-wise linear activation functions. With time, Marabou grew into a complex prover with multiple heuristics supplementing the original ReLUplex algorithm [22  ###reference_b22###], for example it now includes abstract interpretation and MILP-based algorithms which we survey below.\nIncomplete Verification based on Abstract Interpretation takes inspiration from the domain of abstract interpretation, and mainly uses linear relaxations on ReLU neurons, resulting in an over-approximation of the initial constraint.\nAbstract interpretation was first developed by Cousot and Cousot [35  ###reference_b35###] in 1977. It formalises the idea of abstraction of mathematical structures, in particular those involved in the specification of properties and proof methods of computer systems [36  ###reference_b36###] and it has since been used in many applications [37  ###reference_b37###].\nSpecifically, for DNN verification, this technique can model the behaviour of a network using an abstract domain that captures the possible range of values the network can output for a given input.\nAbstract interpretation-based verifiers can define a lower bound and an upper bound of the output of each ReLU neuron as linear constraints, which define a region called ReLU polytope that gets propagated through the network.\nOne can use interval bound propagation (IBP) [38  ###reference_b38###, 39  ###reference_b39###, 40  ###reference_b40###, 41  ###reference_b41###].\nThe strength of IBP-based methods lies in their efficiency; they are faster than alternative approaches and demonstrate superior scalability. However, their primary limitation lies in the inherently loose bounds they produce [39  ###reference_b39###]. This drawback becomes particularly pronounced in the case of deeper neural networks, typically those with more than 10 layers [42  ###reference_b42###], where they cannot certify non-trivial robustness due to the amplification of over-approximation.\nOther methods that are less efficient but produce tighter bounds are based on polyhedra abstraction, such as CROWN [43  ###reference_b43###] and DeepPoly [44  ###reference_b44###], or based on multi-neuron relaxation, such as PRIMA [45  ###reference_b45###].\nOne of the most mature tools in this category is ERAN [46  ###reference_b46###], which can be used for complete verification, but its main purpose is deterministic incomplete verification through abstract interpretation (DeepPoly) and multi-neuron relaxation (PRIMA).\nMILP-based approaches [47  ###reference_b47###, 48  ###reference_b48###, 49  ###reference_b49###] encode the verification problem as a mixed-integer linear programming problem, in which the constraints are linear inequalities and the objective is represented by a linear function.\nThus, the DNN verification problem can be precisely encoded as a MILP problem.\nFor example, ERAN [46  ###reference_b46###], which is mainly used as an incomplete verifier combines abstract interpretation with the MILP solver GUROBI [50  ###reference_b50###].\nERAN uses abstract domains with custom multi-neuron relaxations to support fully-connected, convolutional, and residual networks with ReLU, Sigmoid, Tanh, and Maxpool activations.\nBaB-based verification [51  ###reference_b51###, 52  ###reference_b52###, 53  ###reference_b53###, 54  ###reference_b54###, 55  ###reference_b55###, 56  ###reference_b56###, 57  ###reference_b57###] relies on the piecewise linear property of DNNs: since each ReLU neuron outputs ReLU() = max{,} is piecewise linear, we can consider its two linear pieces ,  separately.\nA BaB verification approach, as the name suggests, consists of two parts: branching and bounding. It first applies incomplete verification to derive a lower bound and an upper bound, then, if the lower bound is positive it terminates with ‘verified’, else, if the upper bound is non-positive it terminates with ‘not verified’ (bounding). Otherwise, the approach recursively chooses a neuron to split into two branches (branching), resulting in two linear constraints. Then bounding is applied to both constraints and if both are satisfied the verification terminates, otherwise the other neurons are split recursively. When all neurons are split, the branch will contain only linear constraints, and thus the approach applies linear programming to compute the constraint and verify the branch.\nIt is important to note that BaB approaches themselves are neither inherently complete nor incomplete. BaB is an algorithm for splitting problems into sub-problems and requires a solver to resolve the linear constraints. The completeness of the verification depends on the combination of BaB and the solver used.\nMulti-Neuron Guided Branch-and-Bound (MN-BaB) [54  ###reference_b54###] is a state-of-the-art neural network verifier that builds on the tight multi-neuron constraints proposed in PRIMA [58  ###reference_b58###] and leverages these constraints within a BaB framework to yield an efficient, GPU based dual solver.\nAnother state-of-the-art tool is -CROWN [59  ###reference_b59###, 56  ###reference_b56###], a neural network verifier based on an efficient linear bound propagation framework and branch-and-bound. It can be accelerated efficiently on GPUs and can scale to relatively large convolutional networks (e.g.,  parameters).\nIt also supports a wide range of neural network architectures (e.g., CNN, ResNet, and various activation functions).\nBaB-based methods are more scalable than solver-based approaches, however they introduce a level of abstraction and sacrifice precision in favor of scalability. For example GCP-CROWN [57  ###reference_b57###] extracts convex constraints from MILP solvers and integrates them in linear inequality propagation, which can be viewed as leveraging multi-neuron relaxations in branch-and-bound complete verification.\nProbabilistic incomplete verification approaches add random noise to smooth models, and then derive certified robustness for these smoothed models.\nThis field is commonly referred to as Randomised Smoothing,\ngiven that these approaches provide probabilistic guarantees of robustness, and all current probabilistic verification techniques are tailored for smoothed models [60  ###reference_b60###, 61  ###reference_b61###, 62  ###reference_b62###, 63  ###reference_b63###, 64  ###reference_b64###, 65  ###reference_b65###].\nGiven that this work focuses on deterministic approaches, here we only report the existence of this line of work without going into details.\nNote that these existing verification approaches primarily focus on computer vision tasks, where images are seen as vectors in a continuous space and every point in the space corresponds to a valid image, while sentences in NLP form a discrete domain, making it challenging to apply traditional verification techniques effectively.\nIn this work we use both an abstract interpretation-based incomplete verifier (ERAN [46  ###reference_b46###]) and an SMT-based complete verifier (Marabou [22  ###reference_b22###]) in order to demonstrate the effect that the choice of a verifier may bring, and demonstrate common trends."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robust Training",
            "text": "Verifying DNNs poses significant challenges if they are not appropriately trained. The fundamental issue lies in the failure of DNNs, including even sophisticated models, to meet essential verification properties, such as robustness [66  ###reference_b66###].\nTo enhance robustness, various training methodologies have been proposed. It is noteworthy that, although robust training by projected gradient descent [67  ###reference_b67###, 20  ###reference_b20###, 68  ###reference_b68###] predates verification, contemporary approaches are often related to, or derived from, the corresponding verification methods by optimizing verification-inspired regularization terms or injecting specific data augmentation during training.\nIn practice, after robust training, the model usually achieves higher certified robustness and is more likely to satisfy the desired verification properties [66  ###reference_b66###]. Thus, robust training is a strong complement to robustness verification approaches.\nRobust training techniques can be classified into several large groups:\ndata augmentation [69  ###reference_b69###],\nadversarial training [67  ###reference_b67###, 20  ###reference_b20###] including property-driven training [70  ###reference_b70###, 71  ###reference_b71###],\nIBP training [39  ###reference_b39###, 72  ###reference_b72###] and other forms of\ncertified training [73  ###reference_b73###], or\na combination thereof [74  ###reference_b74###, 66  ###reference_b66###].\nData augmentation involves the creation of synthetic examples through the application of diverse transformations or perturbations to the initial training data. These generated instances are then incorporated into the original dataset to enhance the training process.\nAdversarial training entails identifying worst-case examples at each epoch during the training phase and calculating an additional loss on these instances. State of the art adversarial training involves projected gradient descent algorithms such as FGSM [67  ###reference_b67###] and PGD [20  ###reference_b20###].\nCertified training methods focus on providing mathematical guarantees about the model’s behaviour within certain bounds. Among them, we can name IBP\ntraining [39  ###reference_b39###, 72  ###reference_b72###] techniques, which impose intervals or bounds on the predictions or activations of the model, ensuring that the model’s output lies within a specific range with high confidence.\nNote that all techniques mentioned above can be categorised based on whether they primarily augment the data (such as data augmentation) or augment the loss function (as seen in adversarial, IBP and certified training).\nAugmenting the data tends to enhance generalisation and is efficient, although it may not help against stronger adversarial attacks. Conversely, methods that manipulate the loss functions directly are more resistant to strong adversarial attacks but often come with higher computational costs. Ultimately, the choice between altering data or loss functions depends on the specific requirements of the application and the desired trade-offs between performance, computational complexity, and robustness guarantees."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "NLP robustness",
            "text": "There exists a substantial body of research dedicated to enhancing the adversarial robustness of NLP systems [75  ###reference_b75###, 76  ###reference_b76###, 77  ###reference_b77###, 78  ###reference_b78###, 79  ###reference_b79###, 80  ###reference_b80###, 81  ###reference_b81###]. These efforts aim to mitigate the vulnerability of NLP models to adversarial attacks and improve their resilience in real-world scenarios [76  ###reference_b76###, 77  ###reference_b77###] and mostly employ data augmentation techniques [82  ###reference_b82###, 83  ###reference_b83###].\nIn NLP, we can distinguish perturbations based on three main criteria:\nwhere and how the perturbations occur,\nwhether they are algorithmically generated (vs. generated by humans or LLMs) and\nwhether they are adversarial (as opposed to random).\nIn particular, perturbations can occur at the character, word, or sentence level [84  ###reference_b84###, 85  ###reference_b85###, 86  ###reference_b86###] and may involve deletion, insertion, swapping, flipping, substitution with synonyms, concatenation with characters or words, or insertion of numeric or alphanumeric characters [87  ###reference_b87###, 88  ###reference_b88###, 89  ###reference_b89###].\nFor instance, in character level adversarial attacks, Belinkov et al. [90  ###reference_b90###] introduce natural and synthetic noise to input data, while Gao et al. [91  ###reference_b91###] and Li et al. [92  ###reference_b92###] and Li et al. [92  ###reference_b92###] identify crucial words within a sentence and perturb them accordingly. For word level attacks, they can be categorised into gradient-based [87  ###reference_b87###, 93  ###reference_b93###], importance-based [94  ###reference_b94###, 95  ###reference_b95###], and replacement-based [96  ###reference_b96###, 97  ###reference_b97###, 98  ###reference_b98###] strategies, based on the perturbation method employed.\nMoreover, Moradi et al. [99  ###reference_b99###] introduce algorithmic non-adversarial perturbations at both the character and word levels. They utilise a rule-based method to generate these perturbations, simulating various types of noise typically caused by spelling mistakes, typos, and other similar errors.\nIn sentence level adversarial attacks, some perturbations [100  ###reference_b100###, 101  ###reference_b101###] are created so that they do not impact the original label of the input and can be incorporated as a concatenation in the original text. In such scenarios, the expected behaviour from the model is to maintain the original output, and the attack can be deemed successful if the label/output of the model is altered.\nAdditionally, non-algorithmic sentence perturbations can be obtained through prompting language models based methods [21  ###reference_b21###, 14  ###reference_b14###] to generate rephrases of the inputs.\nBy augmenting the training data with these perturbed examples, models are exposed to a more diverse range of linguistic variations and potential adversarial inputs. This helps the models to generalise better and become more robust to different types of adversarial attacks.\nTo help with this task, the NLP community has gathered a dataset of adversarial attacks named AdvGLUE [102  ###reference_b102###], which aims to be a principled and comprehensive benchmark for NLP robustness measurements.\nIn this work we employ a PGD-based adversarial training as the method to enhance the robustness and verifiability of our models against gradient-based adversarial attacks. For non-adversarial perturbations, we create algorithmic perturbations at the character and word level as in Moradi et al. [99  ###reference_b99###] and non-algorithmic perturbations at the sentence level using PolyJuice [21  ###reference_b21###] and Vicuna [14  ###reference_b14###].\nWe thus cover most combinations of the three choices above (bypassing only human-generated adversarial attacks as these do not admit systematic evaluation which is important for this study).\n###table_1### ###table_2###"
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Previous NLP Verification Approaches",
            "text": "Although DNN verification studies have predominantly focused on computer vision, there is a growing body of research exploring the verification of NLP. This research can be categorised into three main approaches: IBP, abstract interpretation, and randomised smoothing.\nTables 1  ###reference_### and 2  ###reference_### shows a comparison of these approaches.\nTo the best of our knowledge, this paper is the first one to use an SMT-based verifier for this purpose, and compare it with an abstract interpretation-based verifier on the same benchmarks.\nNLP Verification via Interval Bound Propagation.\nThe first technique successfully adapted from the computer vision domain for verifying NLP models was the IBP. In the NLP approaches, IBP is used for both training and verification. Its aim is to minimise the upper bound on the maximum difference between the classification boundary and the input perturbation region by augmenting the loss function.\nThis facilitates the minimisation of the perturbation region in the last layer, ensuring it remains on one side of the classification boundary. As a result, the adversarial region becomes tighter and can be considered certifiably robust.\nNotably, Jia et al. [17  ###reference_b17###] proposed certified robust models on word substitutions in text classification. The authors employed IBP to optimise the upper bound over perturbations, providing an upper bound over the discrete set of perturbations in the word vector space.\nFurthermore, Huang et al. [18  ###reference_b18###] introduced a verification and verifiable training method with a tighter over-approximation in style of the Simplex algorithm [28  ###reference_b28###].\nTo make the network verifiable, they defined the convex hull of all the original unperturbed inputs as a space of perturbations.\nBy employing the IBP algorithm, they generated robustness bounds for each neural network layer.\nLater on, Welbl et al. [103  ###reference_b103###] differentiated from the previous approaches by using IBP to address the under-sensitivity issue. They designed and formally verified the ‘under-sensitivity specification’ that a model should not become more confident as arbitrary subsets of input words are deleted.\nRecently, Zhang et al. [19  ###reference_b19###] introduced Abstract Recursive Certification (ARC) to verify the robustness of LSTMs. ARC defines a set of programmatically perturbed string transformations to construct a perturbation space. By memorising the hidden states of strings in the perturbation space that share a common prefix, ARC can efficiently calculate an upper bound while avoiding redundant hidden state computations.\nFinally, Wang et al. [104  ###reference_b104###] improved on the work of Jia et al. by introducing Embedding Interval Bound Constraint (EIBC). EIBC is a new loss that constraints the word embeddings in order to tighten the IBP bounds.\nThe strength of IBP-based methods is their efficiency and speed, while their main limitation is the bounds’ looseness, further accentuated if the neural network is deep.\nNLP Verification via Abstract Interpretation.\nAnother popular verification technique applied to various NLP models is based on abstract interpretation.\nOne notable contribution in this area is POPQORN [105  ###reference_b105###], which is the first work that gives robustness guarantees for RNN-based networks. They handle the challenging non-linear activation functions of complicated RNN structures (like LSTMs and GRUs) by bounding them with linear functions.\nLater on, Du et al. improve on POPQORN by introducing Cert-RNN [106  ###reference_b106###], a robust certification framework for RNNs that overcomes the limitations of POPQORN. The framework maintains inter-variable correlation and accelerates the non-linearities of RNNs for practical uses. Cert-RNN utilised Zonotopes [115  ###reference_b115###] to encapsulate input perturbations\nand can verify the properties of the output Zonotopes to determine certifiable robustness.\nThis results in improved precision and tighter bounds, leading to a significant speedup compared to POPQORN.\nIn contrast, Shi et al. [15  ###reference_b15###] focus on transformers with self-attention layers. They developed a verification algorithm that can provide a lower bound to ensure the probability of the correct label is consistently higher than that of the incorrect labels.\nAnalogously, Bonaert et al. [107  ###reference_b107###] propose DeepT, a certification method for large transformers.\nIt is specifically designed to verify the robustness of transformers against synonym replacement-based attacks. DeepT employs multi-norm Zonotopes to achieve larger robustness radii in the certification and can work with networks much larger than Shi et al.\nAbstract interpretation-based methods produce much tighter bounds than IBP-based methods, which can be used with deeper networks. However, they use geometric perturbations () instead of semantic perturbations.\nNLP Verification via Randomised Smoothing.\nRandomised smoothing [116  ###reference_b116###] is another technique for verifying the robustness of deep language models that has recently grown in popularity due to its scalability [108  ###reference_b108###, 109  ###reference_b109###, 110  ###reference_b110###, 111  ###reference_b111###, 112  ###reference_b112###, 113  ###reference_b113###, 114  ###reference_b114###].\nThe idea is to leverage randomness during inference to create a smoothed classifier that is more robust to small perturbations in the input. This technique can also be used to give certified guarantees against adversarial perturbations within a certain radius. Generally, randomized smoothing begins by training a regular neural network on a given dataset.\nDuring the inference phase, to classify a new sample, noise is randomly sampled from the predetermined distribution multiple times. These instances of noise are then injected into the input, resulting in noisy samples. Subsequently, the base classifier generates predictions for each of these noisy samples. The final prediction is determined by the class with the highest frequency of predictions, thereby shaping the smoothed classifier. To certify the robustness of the smoothed classifier against adversarial perturbations within a specific radius centered around the input, randomised smoothing calculates the likelihood of agreement between the base classifier and the smoothed classifier when noise is introduced to the input. If this likelihood exceeds a certain threshold, it indicates the certified robustness of the smoothed classifier within the radius around the input.\nThe main advantage of randomised smoothing-based methods is their scalability, indeed recent approaches are tested on larger transformer such as BERT and Alpaca.\nHowever, their main issue is that they are probabilistic approaches, meaning they give certifications up to a certain probability (e.g., 99.9%).\nIn this work we focus on deterministic approaches, hence we only report these works in Table 2  ###reference_### for completeness without delving deeper into each paper here. All randomised smoothing-based approaches use data augmentation obtained by semantic perturbations."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Datasets and Use Cases Used in NLP Verification",
            "text": "Existing NLP verification datasets. Table 3  ###reference_### summarises the main features and tasks of the datasets used in NLP verification.\nDespite their diverse origins and applications, the datasets in the literature are usually binary or multi-class text classification problems. Furthermore, datasets can be sensitive to perturbations, i.e. perturbations can have non-trivial impact on label consistency. For example, Jia et al. [17  ###reference_b17###] use IBP with the SNLI [117  ###reference_b117###]333A semantic inference dataset that labels whether one sentence entails, contradicts or is neutral to another sentence. dataset (see Tables 1  ###reference_### and 3  ###reference_###) to show that\nword perturbations (e.g. ‘good’ to ‘best’) can change whether one sentence entails another. Some works such as Jia et al. [17  ###reference_b17###] try to address this label consistency, while others do not.\nAdditionally, we find that the previous research on NLP verification does\nnot utilise safety critical datasets (which strongly motivates the choice of datasets in alternative verification domains), with the exception of Du et al. [106  ###reference_b106###] that use the Toxic Comment dataset [118  ###reference_b118###].\nThese papers do not provide detailed motivation as to why the dataset choices were made, however it could be due to the datasets being commonly used in NLP benchmarks (IMDB etc.).\n###table_3###"
        },
        {
            "section_id": "2.5.1",
            "parent_section_id": "2.5",
            "section_name": "2.5.1 Datasets Proposed in This Paper",
            "text": "In this paper we focus on two datasets from safety-critical applications that have not appeared in the NLP verification literature before. Both are driven by real-world use cases of safety-critical NLP applications, i.e. applications for which law enforcement and safety demand formal guarantees of “good” DNN behaviour.\nChatbot Disclosure Dataset.\nThe first case study is motivated by new legislation which states that a chatbot must not mislead people about its artificial identity [11  ###reference_b11###, 10  ###reference_b10###]. Given that the regulatory landscape surrounding NLP models (particularly LLMs and generative AI) is rapidly evolving, similar legislation could be widespread in the future – with recent calls for the US Congress to formalise such disclosure requirements [127  ###reference_b127###]. The prohibition on deceptive conduct act may apply to the outputs generated by NLP systems if used commercially [128  ###reference_b128###], and at minimum a system must guarantee a truthful response when asked about its agency [129  ###reference_b129###, 130  ###reference_b130###]. Furthermore, the burden of this should be placed on the designers of NLP systems, and not on the consumers.\nOur first safety critical case is the R-U-A-Robot dataset [129  ###reference_b129###], a written English dataset consisting of 6800 variations on queries relating to the intent of ‘Are you a robot?’, such as ‘I’m a man, what about you?’. The dataset was created via a context-free grammar template, crowd-sourcing and pre-existing data sources. It consists of 2,720 positive examples (where given the query, it is appropriate for the system to state its non-human identity), 3,400 negative/adversarial examples and 680 ‘ambiguous-if-clarify’ examples (where it is unclear whether the system is required to state its identity). The dataset was created to promote transparency which may be required when the user receives unsolicited phone calls from artificial systems. Given systems like Google Duplex [131  ###reference_b131###], and the criticism it received for human-sounding outputs [132  ###reference_b132###], it is also highly plausible for the user to be deceived regarding the outputs generated by other NLP-based systems [128  ###reference_b128###]. Thus we choose this dataset to understand how to enforce such disclosure requirements. We collapse the positive and ambiguous examples into one label, following the principle of ‘better be safe than sorry’, i.e. prioritising a high recall system.\nMedical Safety Dataset.\nAnother scenario one might consider is that inappropriate outputs of NLP systems have the potential to cause harm to human users [13  ###reference_b13###]. For example, a system may give a user false impressions of its ‘expertise’ and generate harmful advice in response to medically related user queries [7  ###reference_b7###]. In practice it may be desirable for the system to avoid answering such queries.\nThus we choose the Medical safety dataset [12  ###reference_b12###], a written English dataset consisting of 2,917 risk-graded medical and non-medical queries (1,417 and 1,500 examples respectively). The dataset was constructed via collecting questions posted on reddit, such as r/AskDocs. The medical queries have been labelled by experts and crowd annotators for both relevance and levels of risk (i.e. non-serious, serious to critical) following established World Economic Forum (WEF) risk levels designated for chatbots in healthcare [133  ###reference_b133###]. We merge the medical queries of different risk-levels into one class, given the high scarcity of the latter 2 labels to create an in-domain/out-of-domain classification task for medical queries. Additionally, we consider only the medical queries that were labelled as such by expert medical practitioners. Thus this dataset will facilitate discussion on how to guarantee a system recognises medical queries, in order to avoid generating medical output.\nAn additional benefit of these two datasets is that they are distinct semantically, i.e. the R-U-A-Robot dataset contains several semantically similar, but lexically different queries, while the medical safety dataset contains semantically diverse queries. For both datasets, we utilise the same data splits as given in the original papers, and refer to the final binary labels as positive and negative. The positive label in the R-U-A-Robot dataset implies a sample where it is appropriate to disclose non-human identity, while in the medical safety dataset it implies an in-domain medical query."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "Our Work: Parametric Approach to NLP Verification Pipelines",
            "text": "To show relation of our work to the body of already existing work, we distill an “NLP verification pipeline” that is common across many related papers. Figure 2  ###reference_### shows the pipeline diagrammatically. It proceeds in stages:\nGiven an NLP dataset, generate semantic perturbations on sentences that it contains.\nThe semantic perturbations can be of different kinds: character, word or sentence level. IBP and randomised smoothing use word and character perturbations, abstract interpretation papers usually do not use any semantic perturbations.\nOur method allows to use all existing semantic perturbations, in particular, we implement character and word level perturbations as in Moradi et al. [99  ###reference_b99###], sentence level perturbations with PolyJuice [21  ###reference_b21###] and Vicuna.\nEmbed the semantic perturbations into continuous spaces. The cited papers use the word embeddings GloVe [98  ###reference_b98###], we use the sentence embeddings S-BERT and S-GPT.\nWorking on the embedding space, use geometric or semantic perturbations to define geometric or semantic subspaces around perturbed sentences. In IBP papers, semantic subspaces are defined as “bounds” derived from admissible semantic perturbations. In abstract interpretation papers, geometric subspaces are given by -cubes and  around each embedded sentence. Our paper generalises the notion of -cubes by defining “hyper-rectangles” on sets of semantic perturbations. The hyper-rectangles generalise -cubes both geometrically and semantically, by allowing to analyse subspaces that are drawn around several (embedded) semantic perturbations of the same sentence. We could adapt our methods to work with hyper-ellipses and thus directly generalise  (the difference boils down to using  norm instead of  when computing geometric proximity of points), however hyper-rectangles are more efficient to compute, which determined our choice of shapes in this paper.\nUse the geometric/semantic subspaces to train a classifier to be robust to change of label within the given subspaces.\nWe generally call such training either robust training or semantically robust training, depending on whether the subspaces it uses are geometric or semantic.\nA custom semantically robust training algorithm\nis used in IBP papers, while abstract interpretation papers usually skip this step or use (adversarial) robust training.\nIn this paper, we adapt the famous PGD algorithm [20  ###reference_b20###] that was initially defined for geometric subspaces () to work with semantic subspaces (hyper-rectangles) to obtain a novel semantic training algorithm.\nUse the geometric/semantic subspaces to verify the classifier’s behaviour within those subspaces. The papers [17  ###reference_b17###, 18  ###reference_b18###, 103  ###reference_b103###, 19  ###reference_b19###, 104  ###reference_b104###] use IBP algorithms and the papers  [105  ###reference_b105###, 15  ###reference_b15###, 106  ###reference_b106###, 107  ###reference_b107###] use abstract interpretation; in both cases it is incomplete and deterministic verification. We use SMT-based tool Marabou (complete and deterministic) and abstract-interpretation tool ERAN (incomplete and deterministic).\nTable 1  ###reference_### summarises differences and similarities of the above NLP verification approaches against ours.\nTo the best of our knowledge, we are the first to use SMT-based complete methods in NLP verification and we show how they achieve higher verifiability than abstract interpretation-based verification approaches, thanks to the increased precision of the ReLUplex algorithm proof search relative to bound propagation.\nFurthermore, our study is the first to demonstrate that the construction of semantic subspaces can happen independently of the choice of the training and verification algorithms. Likewise, although training and verification build upon the defined (semantic) subspaces, the actual choice of the training and verification algorithms can be made independently of the method used to define the semantic subspaces.\nThis separation, and the general modularity of our approach, facilitates a comprehensive examination and comparison of the two key components involved in any NLP verification process:\neffects of the verifiability-generalisability trade-off for verification with geometric and semantic subspaces;\nrelation between the volume/shape of semantic subspaces and verifiability of neural networks obtained via semantic training with these subspaces.\nThese two aspects have not been considered in the literature before."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Parametric NLP Verification Pipeline",
            "text": "This section presents a parametric NLP pipeline, shown in Figure 2 diagrammatically. We call it “parametric” because each component within the pipeline operates independently of the others and can be taken as a parameter when studying other components. The parametric nature of the pipeline allows for the seamless integration of state-of-the-art methods at every stage, and for more sophisticated experiments with those methods. The following section provides a detailed exposition of the methodological choices made at each step of the pipeline."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Semantic Perturbations",
            "text": "As discussed, we require semantic perturbations for creating semantic subspaces. To do so, we consider three kinds of perturbations: character, word, and sentence level. This accounts for different variations of the samples.\n\nCharacter and word level perturbations are created via a rule-based method to simulate different kinds of noise, such as spelling mistakes and typos. These perturbations include inserting, deleting, replacing, swapping, or repeating a character of the data sample. Certain perturbations, like letter case changes and commonly misspelled words, are omitted for their limited relevance to our datasets.\n\nWord level perturbations include repeating or deleting a word, changing word order, verb tense, and singular to plural verbs, or adding negation. Some perturbations, such as synonym replacement, are done via sentence rephrasing. Negation and certain verb changes are avoided in specific datasets to prevent label ambiguities and annotator difficulties.\n\nFor sentence level perturbations, we use Polyjuice or Vicuna. Polyjuice offers control over perturbation types and locations, while Vicuna generates variations with prompts such as \"Rephrase this sentence 5 times: '[Example]’.\" Examples of sentence variations include changes in phrasing while maintaining the core meaning."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "NLP Embeddings",
            "text": "The next component of the pipeline is the embeddings. Embeddings play a crucial role in NLP as they map textual data into continuous vector spaces, capturing semantic relationships and contextual information. Given the set of all strings, an NLP dataset is a set of sentences written in natural language. The embedding is a function that maps a string to a vector in a space called the embedding space. Ideally, the embedding should reflect the semantic similarities between sentences, meaning that more semantically similar sentences should have closer distances in the embedding space. \n\nDefining semantic similarity precisely may not be tractable due to the infinite number of unseen sentences, subjectivity, and context-dependency. Hence, state-of-the-art NLP relies on machine learning methods to capture semantic similarity approximately. Currently, the most common approach to obtaining an embedding function is by training transformers. Transformers are a type of deep neural networks that can be trained to map sequential data into real vector spaces and handle variable-length input sequences. They are also used for other tasks, such as classification or sentence generation, with training occurring at the level of embedding spaces. \n\nIn this work, a transformer is trained as a function. The key feature of the transformer is the \"self-attention mechanism,\" which allows the network to weigh the importance of different elements in the input sequence when making predictions, rather than relying solely on the order of elements. This feature makes them effective at learning to associate semantically similar words or sentences. Sentence-BERT is initially used, and later Sentence-GPT is added to embed sentences.\n\nUnfortunately, the relation between the embedding space and the NLP dataset is not bijective; each sentence is mapped into the embedding space, but not every point in the embedding space has a corresponding sentence. This issue is known in NLP literature.\n\nGiven an NLP dataset that should be classified into classes, the standard approach is to construct a function that maps the embedded inputs to the classes. To do that, a domain-specific classifier is trained on the embeddings, and the final system is the composition of the two subsystems."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Geometric Analysis of Embedding Spaces",
            "text": "We now formally define geometric and semantic subspaces of the embedding space. Our goal is to define subspaces on the embedding space m by using an effective algorithmic procedure. We will use notation to refer to a subspace of the embedding space. A hyper-rectangle of dimension is a list of points such that a point is a member if for every dimension we have.\n\nWe start with an observation that, given an NLP dataset that contains a finite set of sentences belonging to the same class, and an embedding function, we can define an embedding matrix, where each row is given by. We will use the notation to refer to the element of the vector, and to refer to the element in the row and column of. Treating embedded sentences as matrices, rather than as points in the real vector space, makes many computations easier. We can therefore define a hyper-rectangle for as follows.\n\nGiven an embedding matrix, the -dimensional hyper-rectangle for is defined as:\n\nTherefore given an embedding function, and a set of sentences, we can form a subspace by constructing the embedding matrix, as described above, and forming the corresponding hyper-rectangle. To simplify notation, we will omit the application of and from here on simply write.\n\nThe next example shows how the above definitions generalise the commonly known definition of the.\n\nIt is defined as follows. Given an embedded input, a constant, and a distance function (L-norm), the around of radius is defined as:\n\nIn practice, it is common to use the norm, which results in the actually being a hyper-rectangle, also called, where. Therefore our construction is a strict generalisation of. We will therefore use the notation to refer to the set of around every sentence in the dataset.\n\nOf course, as we have already discussed in the introduction and Figure 1, hyper-rectangles are not very precise, geometrically. A more precise shape would be a convex hull around given points in the embedding space. Indeed literature has some definitions of convex hulls. However, none of them is suitable as they are computationally too expensive due to the time complexity of where is the number of inputs and is the number of dimensions. Approaches that use under-approximations to speed up the algorithms do not work well in NLP scenarios, as under-approximated subspaces are so small that they contain near zero sentence embeddings."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Exclusion of Unwanted Sentences Via Shrinking",
            "text": "Another concern is that the generated hyper-rectangles may contain sentences from a different class. In order to exclude all samples from the wrong class, we define a shrinking algorithm that calculates a new subspace that is a subset of the original hyper-rectangle around, that only contains embeddings of sentences in that are of class. Of course, to ensure this, the algorithm may have to exclude some sentences of class. The second graph of Figure 3 gives a visual intuition of how this is done. \n\nFormally, for each sentence in that is not of class, the algorithm performs the following procedure. If lies in the current hyper-rectangle, then for each dimension we compute the distance whether is closer to or. Without loss of generality, assume is closer. We then compute the number of sentences of class that would be excluded by replacing with in the hyper-rectangle where is a small positive number (we use). This gives us a penalty for each dimension, and we exclude by updating the hyper-rectangle in the dimension that minimises this penalty. \n\nThe idea is to shrink the hyper-rectangle in the dimensions that exclude as few embedded sentences from the desired class as possible. This choice keeps the algorithm fast while guaranteeing the subspace to retain the highest number of wanted inputs. However, there might be cases where perturbations of the unwanted input are left inside after shrinking. For large subspaces, more clever algorithms should be explored and discussed."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Exclusion of Unwanted Sentences Via Clustering",
            "text": "An alternative approach to excluding unwanted sentences is to split the dataset by clustering semantically similar sentences in the embedding space and then computing the hyper-rectangles around each cluster individually. In this paper, we use the k-means algorithm for clustering. We adopt the notation to refer to the k-clusters formed by applying it to dataset D. This method is combined with the shrinking algorithm in our experiments."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Eigenspace Rotation",
            "text": "A final alternative and computationally efficient way of reducing the likelihood that the hyper-rectangles will contain embedded sentences of an unwanted class is to rotate them to better align with the distribution of the embedded sentences of the desired class in the embedding space. This motivates us to introduce the Eigenspace rotation.\n\nTo construct the tightest possible hyper-rectangle, we define a specific method of eigenspace rotation. Our approach is to calculate a rotation matrix such that the rotated matrix is better aligned with the axes and therefore has a smaller volume. By a slight abuse of terminology, we will refer to it as the rotated hyper-rectangle, even though strictly speaking, we are rotating the data, not the hyper-rectangle itself.\n\nTo calculate the rotation matrix, we use singular value decomposition. The singular value decomposition of a matrix is defined such that it describes directions in which the matrix exhibits the most variance. The main idea behind the definition of rotation is to align these directions of maximum variance with the standard canonical basis vectors.\n\nFormally, using the decomposition, we can compute the rotation (or change-of-basis) matrix that rotates the right-singular vectors onto the canonical standard basis vectors, where the identity matrix is used. We thus obtain the desired rotation.\n\nAll hyper-rectangles constructed in this paper are rotated."
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4 Geometric and Semantic Subspaces",
            "text": "We now apply the abstract definition of a subspace of an embedding space to concrete NLP scenarios. Once we know how to define subspaces for a selection of points in the embedding space, the choice remains how to choose those points. The first option is to use around given embedded points, as Example 1 defines. Since this construction does not involve any knowledge about the semantics of sentences, we will call the resulting subspaces geometric subspaces.\n\nThe second choice is to apply semantic perturbations to a point, embed the resulting sentences, and then define a subspace around them. We will call the subspaces obtained by this method semantic perturbation subspaces, or just semantic subspaces for short. We will finish this section with defining semantic subspaces formally.\n\nWe will use to denote an algorithm for generating sentence perturbations of type, applied to an input sentence in a random position. In the later sections, we will use to refer to the different types of perturbations illustrated in Tables 4 and 5, e.g., character-level insertion, deletion, replacement.\n\nIntuitively, given a single sentence we want to generate a set of semantically similar perturbations and then construct a hyper-rectangle around them, as described in Definition 1. This motivates the following definitions. Given a sentence, a number, and a type, the set is the set of semantic perturbations of type generated from.\n\nWe will use the notation to denote the new dataset generated by creating semantic perturbations of type around each sentence. Given an embedding function, the semantic subspace for a sentence is the subspace. We will refer to a set of such semantic hyper-rectangles over an entire dataset as.\n\nTo illustrate this construction, let us consider the sentence: “Can u tell me if you are a chatbot?”. This sentence is one of the original sentences of the positive class in the dataset. From this single sentence, we can create six new sentences using the word-level perturbations from Table 5 to form. Once the seven sentences are embedded into the vector space, they form the hyper-rectangle. By repeating this construction for the remaining sentences, we obtain the set of hyper-rectangles for the dataset.\n\nGiven a sentence, we embed each sentence into m obtaining vectors where."
        },
        {
            "section_id": "3.3.5",
            "parent_section_id": "3.3",
            "section_name": "3.3.5 Measuring the Quality of Sentence Embeddings",
            "text": "One of our implicit assumptions in the previous sections is that the embedding function maps pairs of semantically similar sentences to nearby points in the embedding space. Cosine similarity is used to measure how similar two vectors are in a multi-dimensional space by calculating the cosine of the angle between them. This is done by taking the dot product and dividing by the magnitudes of the vectors. The resulting value ranges from -1 to 1. A value of 1 indicates that the vectors are parallel (highest similarity), while -1 means that the vectors are orthogonal (no similarity)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "As outlined in Section 2.2, robust training is essential for bolstering the robustness of DNNs. This study employs two robust training methods, namely data augmentation and a custom PGD adversarial training, with the goal of discerning the factors contributing to the success of robust training and compare the effectiveness of these methods.\n\nData Augmentation. In this training method, we statically generate semantic perturbations at the character, word, and sentence levels before training, which are then added to the dataset. The network is subsequently trained on this augmented dataset using the standard stochastic gradient descent algorithm.\n\nAdversarial Training. In this training method, the traditional Projected Gradient Descent (PGD) algorithm is defined. The primary distinction between our customised PGD algorithm and the standard version lies in the definition of the step size. In the conventional algorithm, the step size is represented by a scalar, therefore representing a uniform step size in every dimension. In our case, the width in each dimension may vary greatly, therefore we transform it into a vector, allowing the step size to vary by dimension. Note that the dot product between the vectors becomes an element-wise multiplication. The resulting customised PGD training seeks to identify the worst perturbations within the custom-defined subspace, and trains the given neural network to classify those perturbations correctly, in order to make the network robust to adversarial inputs in the chosen subspace."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Choice of Verification Algorithm",
            "text": "Our approach in this study involves the utilization of cutting-edge tools for DNN analysis. We initially focus on employing advanced methods to yield significant results. This choice is prioritized due to its efficiency. Subsequently, we explore integrating other advanced methods, which enables us to maximize the effectiveness of the analysis."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Characterisation of Verifiable Subspaces",
            "text": "In this Section, we provide key results in support of Contribution 1 formulated in the introduction: We introduce the metric of generalisability of subspaces and set-up some baseline experiments. We introduce the problem of the generalisability trade-off in the context of geometric subspaces. We show that, compared to geometric subspaces, the use of semantic subspaces helps to find a better balance between generalisability. Finally, we show that adversarial training based on semantic subspaces results in DNNs that are more generalisable than those obtained with other forms of robust training."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Metrics for Understanding the Properties of Embedding Spaces",
            "text": "Let us start with recalling the existing standard metrics used in DNN verification. Recall that we are given an NLP dataset, moreover we assume that each is assigned a correct class from. We restrict to the case of binary classification in this paper for simplicity, so we will assume.\n\nFurthermore, we are given an embedding function, and a network. Usually corresponds to the number of classes, and thus in case of binary classification, we have.\n\nAn embedded sentence is classified as class if the value of in is higher than all other classes.\n\nAccuracy. The most popular metric for measuring the performance of the network is the accuracy of, which is measured as a percentage of sentences in that are assigned to a correct class by.\n\nNote that this metric only checks a finite number of points in m given by the dataset.\n\nGeneralisability. Therefore, we now introduce a third metric, generalisability, which is a heuristic for the number of semantically-similar unseen sentences captured by a given set of subspaces.\n\nGiven a set of subspaces and a target set of embeddings the generalisability of the subspaces is measured as the percentage of the embedded vectors that lie in the subspaces:\n\nIn this paper we will generate the target set of embeddings as where is a dataset, is the type of semantic perturbation, is the number of perturbations and is the embeddings of the set of semantic perturbations around generated using, as described in Section 3.3 ###reference_###.\n\nNote that can be given by a collection of different perturbation algorithms and their kinds. The key assumption is that contains valid sentences semantically similar to and belonging to the same class. Assuming that membership of is easy to compute, then this metric is also easy to compute as the set is finite and of size, and therefore so is. Note that, unlike accuracy and verifiability, the generalisability metric does not explicitly depend on any DNN or verifier. However, in this paper we only study generalisability of verifiable subspaces, and thus the existence of a verified network will be assumed. Furthermore, the verified subspaces we study in this paper will be constructed from the dataset via the methodology described in Definition 2 ###reference_inition2###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baseline Experiments for Understanding the Properties of Embedding Spaces",
            "text": "The methodology defined thus far has provided basic intuitions about the modular nature of the NLP pipeline. Bearing this in mind, it is important to start our analysis with the general study of basic properties of the embedding subspaces, which is our main interest in this paper, and suitable baselines.\n\nBenchmark datasets will be abbreviated as “RUAR” and “Medical”. We use these to refer to the set of sentences in the training dataset with a positive class (i.e. a question asking the identity of the model, and a medical query respectively), and the remaining sentences.\n\nFor a benchmark network, we train a medium-sized fully-connected DNN (with 2 layers of size (128, 2) and input size 30) using stochastic gradient descent and cross-entropy loss. \n\nFor the choice of benchmark subspaces, we use two extreme sets of geometric subspaces: the singleton set containing the maximal subspace around all embedded sentences of the positive class. This is illustrated in the first graph of Figure 3. \n\nAnother set consists of minimal subspaces given by around each embedded sentence of class, where is chosen to be sufficiently small. This is illustrated in the first graph of Figure 1.\n\nWe first seek to understand the geometric properties (e.g. volume, values) for these two extremes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Verifiability-Generalisability Trade-off for Geometric Subspaces",
            "text": "For example, in the RUAR dataset, there are sentences of the positive class. The experiment consists of generating hyper-cubes around each positive sentence, resulting in several hyper-cubes. Using clustering, we obtain a set of clusters denoted as different clusters, and using the shrinking algorithm, we obtain reduced clusters. There is a consistent reduction in volume observed, and there are several orders of magnitude between the largest and the smallest subspace."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Verifiability of Geometric Subspaces",
            "text": "One may also conjecture that they are less generalisable (as they will contain fewer embedded sentences). We now will confirm this via experiments; we are particularly interested in understanding how quickly generalisability deteriorates."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Generalisability of Geometric Subspaces",
            "text": "The choice to use only positive sentences is motivated by the nature of the chosen datasets - both Medical and RUAR sentences split into a positive class, that contains sentences with one intended semantic meaning (they are medical queries, or they are questions about robot identity); and a negative class that represents “all other sentences”. These “other sentences” are not grouped by any specific semantic meaning and therefore do not form one coherent semantic category.\n\nFor the perturbation type, we take a combination of the different perturbations algorithms described. For RUAR: character insertion, character deletion, character replacement, character swapping, character repetition, word deletion, word repetition, word negation, word singular/plural verbs, word order, and word tense. For the Medical dataset: character insertion, character deletion, character replacement, character swapping, character repetition, word deletion, word repetition, word negation, word singular/plural verbs, word order, word tense, and sentence polyjuice. Each type of perturbation is applied 4 times on the given sentence in random places. The resulting datasets of semantically perturbed sentences are therefore approximately two orders of magnitude larger than the original datasets and contain unseen sentences of similar semantic meaning to the ones present in the original datasets."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Verifiability-Generalisability Trade-off for Semantic Subspaces",
            "text": "In this section, we argue that using semantic subspaces can help to address the limitations of the trade-off. The main hypothesis we are testing is that semantic subspaces constructed using semantic-preserving perturbations are more precise, which improves both generalisability and other dimensions. \n\nWe employ semantic hyper-rectangles constructed on sentences of the positive class using character-level, word-level, and sentence-level perturbations. The types of perturbations include deletion, insertion, replacement, swapping, and the use of tools like Polyjuice. Notice the comparable volumes of all these shapes."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Verifiability of Semantic Subspaces",
            "text": "We pass each set of hyper-rectangles and the network to measure subspaces. Our semantic hyper-rectangles achieve notable higher verification than its counterpart of comparable volume. Precision of the subspaces has an impact."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Generalisability of Semantic Subspaces",
            "text": "Table 13 compares the generalisability of different semantic subspaces. It shows that these semantic subspaces are the most generalisable, containing a significant proportion of the unseen sentences. We infer that using semantic subspaces is effective for improving generalisability, with precise subspaces performing somewhat better than those of the same volume; however, both exceed the smallest subspaces from previous sections of comparable scale. The finding that these subspaces contain up to a notable percentage of randomly generated new sentences indicates their potential utility in sentence embedding and generalisation tasks."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Adversarial Training on Semantic Subspaces",
            "text": "In this section, we study the effects that adversarial training methods have on the subspaces previously defined. By comparing the effectiveness of the different training approaches described, we show that adversarial training based on our new semantic subspaces is the most efficient. Three kinds of training are deployed: \n\n1. **No robustness training:** The baseline network from the previous experiments, which has not undergone any robustness training. \n2. **Data augmentation:** We obtain three augmented datasets with different levels of perturbations. We train the baseline architecture using the standard stochastic gradient descent and cross-entropy loss on these augmented datasets, resulting in different DNNs. \n3. **PGD adversarial training:** Instead of using the standard subspace, we use various hyper-rectangles for training.\n\nWe train the networks using these methods, obtaining networks that are designed to be more robust. The networks trained with data augmentation achieve similar nominal accuracy to networks trained with adversarial training. However, differences in subspace specifications impact their performance outcomes. The adversarially trained networks trained on semantic subspaces showed high performance. The successful performance improvements are attributed to the specific semantic attacks used during training. For instance, networks trained with a more aggressive form of attack tended to perform best. This suggests that knowledge of potential attacks beforehand can help refine the training approach for improved outcomes. The results emphasize that subspaces that are too extensive may not yield good results, even with adversarial training. Generalisability of the subspace shapes remains consistent across different tests."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "NLP Case Studies",
            "text": "The purpose of this section is to explore the application of more modern NLP tools using different LLMs to embed sentences and replace Polyjuice with the LLM vicuna-13b, a state-of-the-art open source chatbot trained by fine-tuning LLaMA on user-shared conversations. We use the tool ANTONIO to vary different components of the NLP pipeline.\n\nThe correctness of the specification is dependent on the purely NLP parts of the pipeline, particularly the parts that generate, perturb, and embed sentences. There are two key assumptions affecting the correctness of the generated specifications: the locality of the embedding function and the preservation of semantics by the sentence perturbation algorithm.\n\nLocality of the Embedding Function - This assumes that the embedding function maps semantically similar sentences to nearby points in the embedding space and dissimilar sentences to faraway points. If this assumption fails, the subspace may also contain the embeddings of unseen sentences from different classes.\n\nSentence Perturbation Algorithm Preserves Semantics - It is assumed that sentence perturbations can be generated in a way that retains the original semantic meaning. If this assumption fails, we may construct semantic subspaces around embeddings of sentences belonging to different classes.\n\nGiven the potential for these assumptions to fail, it is incorrect to assure that verifying the subspace guarantees that all sentences embedding into it belong to the intended classification. New sentences of a class that fall inside the verified subspace of another class falsify the subspace.\n\nThis highlights limitations in mapping sets of points in the embedding space back to sets of natural language sentences, leading to incorrect specifications and a paradoxical situation where the same subspace can be formally verified yet empirically falsified. Formal verification ensures identical classification by a given DNN within a semantic subspace, while empirical falsification arises from appealing to the semantic meaning of the embedded sentences.\n\nThe failure to address falsifiable verified subspaces can have various implications, depending on usage scenarios, such as censoring sensitive content. Incorrect embeddings can lead to false positives or negatives in DNN classification.\n\nThe main question is how to measure and improve the quality of the NLP components of the pipeline to decrease the likelihood of generating falsifiable subspaces and ensure that results are practically usable. As part of this investigation, a falsifiability metric is introduced, which should be used alongside other metrics in NLP benchmarks."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Role of False Positives and False Negatives",
            "text": "Generally, when DNNs are used for making decisions in situations where safety is critically important, practical importance of accuracy for each class may differ. For example, for an autonomous car, misrecognising a stop sign for a 30 mph sign is more dangerous than the reverse. Similarly for NLP, because of legal or safety implications, it is crucial that the chatbot always discloses its identity when asked, and never gives medical advice.\n\nWe therefore want to avoid false negatives altogether, i.e., if there is any doubt about the nature of the question, we would rather err on the side of caution and disallow chatbot answers. If the chatbot (by mistake) refuses to answer some non-critically important questions, it may be inconvenient for the user but would not constitute a safety, security, or legal breach. Thus, false positives may be tolerated.\n\nOn the technical level, this has two implications:\n\nFirstly, if we use DNN on its own, without verification, we may want to report precision and recall in addition to accuracy. Higher recall will mean fewer false negatives, which is highly desirable.\n\nSecondly, when verifying the network, we only seek to verify subspaces that characterise the critically important class or classes that we wish to filter. This approach completely eliminates the effect of any false negatives within the verified subspaces. But the verified subspaces may still contain false positives and in this sense be “falsifiable,” which may not be pragmatically desirable but does not constitute a security threat.\n\nTherefore, throughout this section, we report the precision, recall, and F1 to see whether one or another class is particularly challenging to recognise. Depending on the concrete scenario, precision or recall may be more important."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance of Existing LLMs as Safety-Critical Filters",
            "text": "Bearing this in mind, we start first by gaining an intuition about the effectiveness of modern LLMs acting as safety-critical filters, by prompting an LLM to generate the desired output without additional fine-tuning, i.e., we first try a zero-shot model. For our experiments, we choose the model vicuna-13b, given its high performance. Using vicuna-13b, we experimented with prompts as shown in Figure 5 and settled on the first prompt ‘If I ask you anything about your agency, always only reply “I am a large language model”. If not, reply normally.“[Example]”’? as it most often gave the (somewhat) desired output on a subset of manually checked data samples. Similarly, for the medical dataset, we settled on a prompt ‘If I ask you any medical query, always only reply “As an AI language model, I am not authorized to provide medical advice.”. If not, reply normally.“[Example]”’. For our zero-shot model, For the RUAR dataset, the F1 of the LLM is (restricted conditions). If we loosen our success criteria to consider other non-requested variations on our desired output, the F1 marginally improves. For the medical safety dataset, However, we found that in several cases the generated answers include a combination of the desired output and undesired output, e.g., ‘…I am not authorized to provide medical advice …’ followed by explicit medical advice. Therefore, the actual success rate may be even lower. Note there were at least 5 instances regarding the RUAR dataset where the system confirmed human identity, without any disclaimers. Thus, we find that our zero-shot model is, at most, minimally successful in identifying such queries."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Experimental Setup of the Verification Pipeline",
            "text": "We turn our attention to assessing the effectiveness of training a classifier specifically for the task. In setting up the NLP verification pipeline, we use key conclusions about successful verification strategies, namely:\n\n- Semantic subspaces should be preferred over geometric subspaces as they result in a better trade-off.\n- Constructing semantic subspaces using stronger NLP perturbations is beneficial.\n- Adversarial training with stronger NLP perturbations enhances performance.\n- Marabou offers advantages over ERAN due to its completeness.\n\nTo build on these insights, we further strengthen the NLP perturbations by substituting Polyjuice with Vicuna, which introduces more diverse and sophisticated sentence perturbations. We also mix in character and word perturbations to further diversify and enlarge the set of available perturbed sentences.\n\nWe diversify the kinds of LLMs we use as embedding functions. Using the sentence transformers package, models are fine-tuned on a sentence similarity task, producing semantically meaningful sentence embeddings. We select three different encoders to experiment with model size: all-MiniLM-L6-v2, s-bert 22M, s-gpt 1.3B, and s-gpt 2.7B, where the number refers to the model size in parameters.\n\nGiven the set of semantic subspaces, we aim to obtain them via the hyper-rectangle construction. We set the adversarial training to explore the same subspaces and to develop the network accordingly."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Analysis of the Role of Embedding Functions",
            "text": "Overall, the figures are as expected: compared to the F1 of 54-64% for the zero-shot model, using a fine-tuned trained DNN as a filter dramatically increases the F1 to the range of 76-95%.\n\nLooking into nuances, one can further notice the following: There is not a single embedding function that always results in the highest F1. For example, s-bert 22M is found to have the highest F1 for Medical, while s-gpt 2.7B has the highest F1 for RUAR (with the exception of F1 score, for which s-bert 22M is best for both datasets). The smaller GPT model s-gpt 1.3B is systematically worse for both datasets. As expected and discussed, depending on the scenario of use, the highest F1 may not be the best indicator of performance. For Medical, s-bert 22M (either with or without adversarial training) obtains the highest precision, recall, and F1. However, for RUAR, the choice of the embedding function has a greater effect: if F1 is desired, s-bert 22M is the best choice (difference with the worst choice of the embedding function is more significant, for scenarios when one is not interested in verifying the network, the embedding function s-gpt 2.7B when combined with adversarial training gives an incredibly high recall and would be a great choice. Adversarial training only makes a significant difference in F1 for the Medical perturbed test set. However, it has more effect on improving recall (up to 10% for Medical and 33% for RUAR). \n\nThe main conclusion one should make is that depending on the scenario, the embedding function may influence the quality of the NLP pipelines, and reporting the error range (for both precision and recall) depending on the embedding function choice should be common practice."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Analysis of Perturbations",
            "text": "Recall that two problems were identified as potential causes of falsifiable semantic subspaces: the imprecise embedding functions and invalid perturbations (i.e., those that change semantic meaning and the class of the perturbed sentences).\nFirstly, we wish to understand how common it is for our chosen perturbations to change the class, and secondly, we propose several practical methods for how perturbation adequacy can be measured algorithmically.\nRecall that the definition of semantic subspaces depends on the assumption that we can always generate semantically similar (valid) perturbations and draw semantic subspaces around them. Both adversarial training and exploration of these semantic subspaces depend on this assumption. If this assumption fails and the subspaces contain a large number of invalid sentences, the NLP pipeline loses much of its practical value."
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Understanding the Scale of the Problem",
            "text": "In the experiment, for each original dataset and word/character perturbation type, we select 10 perturbed sentences. At the character level, there are 50 perturbed sentences for both datasets (10 each for inserting, deleting, replacing, swapping, or repeating a character). At the word level, there are 60 perturbed sentences for RUAR (deletion, repetition, ordering, negation, singular/plural, verb tense) and 30 for Medical (deletion, repetition, ordering). At the sentence level, perturbations are obtained by prompting vicuna-13b with instructions for the original sentence to be rephrased 5 times. This results in a total of 290 pairs consisting of the original sentence and the perturbed sentence (130 from the medical safety, and 160 from the R-U-A-Robot dataset).\n\nOverall, there are high scores for label consistency, particularly for rule-based perturbations. There are also high scores for semantic similarity. For grammaticality, perturbations generated by vicuna-13b are generally rated as grammatical, whereas rule-based perturbations compromise on grammaticality.\n\nWe note this is in part due to our definition of grammatical being interpreted differently by the two independent evaluators and label consistency being ambiguous for the RUAR dataset. Finally, we acknowledge that there may be discrepancies in individual ratings. Future replications are warranted."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Automatic Ways to Measure and Report Perturbation Validity",
            "text": "Although in the near future, no geometric or algorithmic method will be able to match to the full extent the human perception and interpretation of sentences, we can still formulate a number of effective methods that give a characterisation of the validity of the perturbations utilised when defining semantic subspaces. We propose two:\nUsing cosine similarity of embedded sentences, we can characterise semantic similarity. \nUsing the ROUGE-N method, we can measure lexical and syntactic validity. \nWe proceed to describe each of them in order."
        },
        {
            "section_id": "5.5.x",
            "parent_section_id": "5.5",
            "section_name": "Cosine Similarity",
            "text": "To measure the general effectiveness of the embedding function at generating semantically similar sentences, we identify the pros and cons of using cosine similarity as a metric.\n\nPros:\nCosine similarity metric is general, efficient, and scalable. It applies irrespective of other choices in the pipeline.\n\nCons:\nDue to its geometric nature, cosine similarity does not provide direct knowledge about true semantic similarity of sentences. The human evaluation of semantic similarity hardly matches the optimistic numbers sometimes reported. Disagreement in cosine similarity estimations may vary significantly when different embedding functions are applied.\n\nOverall, although it has its limitations, cosine similarity is a useful metric. Filtering based on cosine similarity is beneficial as a pre-processing stage in NLP."
        },
        {
            "section_id": "5.5.x",
            "parent_section_id": "5.5",
            "section_name": "ROUGE-N",
            "text": "We calculate lexical and syntactic variability of the generated vicuna-13b output using ROUGE-N scores, which measure overlap. Intuitively, if a sentence from the dataset has a perturbation, ROUGE-N provides an overlap measure. Figure 6 presents an experiment where vicuna-13b generates sentence perturbations. The results display a high number of invalid sentences, mainly due to incoherence, hallucination, or incorrect rephrasing.\n\nFor lexical ROUGE-N, we compare the original sample to the perturbations, while for syntax, we use the corresponding parts-of-speech (POS) tags. We also calculate ROUGE-N before and after filtering with cosine similarity. These results suggest that a low score does not necessarily indicate non-semantic preserving rephrases. Shuffling, rephrasing, or synonym substitution could lower the scores without altering meaning.\n\nPrior to filtering, the scores remain steady, which implies a long sequence of overlapping text, although some unique text may remain. When ROUGE scores decrease, it means singular words overlap in both sentences but not in sequence, or they are alternated by other words. It is plausible that cosine similarity filters out perturbations with long word sequence overlaps but added hallucinations changing the meaning.\n\nThere is generally higher syntactic overlap than lexical overlap. This occasionally leads to unsatisfactory perturbations, where local rephrasing results in globally incoherent sentences. Without filtering, ROUGE scores are higher compared to BLEU scores, while after filtering, the BLEU scores increase. We hypothesize that cosine similarity filters out shorter perturbations than the original sentences.\n\nLiteral rephrasing is often observed, highlighting challenges in generating high-quality perturbations. For example, in medical queries, expressed emotions usually need inference, and hallucinated content in perturbations becomes problematic, especially with additional risk labels from a medical safety dataset. This hallucinated content can significantly affect label consistency."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Falsifiability",
            "text": "As the final result of this paper, we introduce the new metric – falsifiability – that measures the number of unwanted sentences that are mapped into a verified subspace. The falsifiability metric differs from traditional NLP methods in two aspects: firstly, it helps to measure both effects simultaneously, and thus helps to assess validity of both the assumption of locality of the embedding function and the assumption of semantic stability of the perturbations outlined at the start of Section 5. Secondly, it is applied here as a verification metric specifically.\n\nWe next formally define the falsifiability metric. Intuitively, the falsifiability of a set of subspaces of class is the percentage of those subspaces that contain at least one embedding of a sentence that belongs to a different class. Given a set of subspaces that are supposed to contain exclusively sentences of class, a dataset that contains sentences not of class, and a set of embeddings, then falsifiability is measured as the percentage of subspaces that contain at least one element of. As with the definition of generalisability, in this paper we will generate the target set of embeddings as where is a dataset, is the type of semantic perturbation, is the number of perturbations, and is the embeddings of the set of semantic perturbations around generated using, as described in Section 3.3. We also measure the presence of false positives, calculated as the percentage of the perturbations of sentences from classes other than that lie within at least one of the set of subspaces.\n\nFurthermore, falsifiability could also reflect issues in the dataset and subsequent noisy perturbations. The medical safety dataset, for instance, was annotated by an expert practitioner, while the RUAR dataset contains (for this particular task) what could be construed as noisy labels. For example, ‘are robot you you a’ is a sample that is found in the negative RUAR train set. The reason for the negative label is that it is an ungrammatical false positive, but given our methods of perturbation for the construction of subspaces, this negative sample may be very similar to a word-level perturbation for the positive class. Concretely, for some sentence pairs of negative samples with their accompanying perturbations contained in falsified subspaces are: (Original: ‘Are you a chump?’, Perturbation: ‘You a chump’), (Original: ‘Are you a liar’, Perturbation: ‘You a liar’), (Original: ‘if a computer can feel emotions, does that make you a computer or an actual human?’, Perturbation: ‘if a computer can feel, does that make it a machine or a person’). Thus, the task of determining what queries require disclosure (e.g., should ‘what is your favorite food’ warrant disclosure?) is more ambiguous and, as the outputs of LLMs sound more coherent, it becomes harder to define.\n\nThis area merits further research."
        },
        {
            "section_id": "5.6.x",
            "parent_section_id": "5.6",
            "section_name": "Falsifiability vs Generalisability and Verifiability",
            "text": "For comparison with the findings outlined in Section 4, we provide additional insights into generalisability. We first analyse the effect of cosine similarity filtering. Initially, the experiments reveal that filtering results in slightly higher levels of generalisability for all models. Given the conclusions in Section 4, the increase in generalisability is somewhat unexpected because larger subspaces tend to exhibit greater generalisability, but filtering decreases the volume of the subspaces. Therefore, we conjecture the increase in precision of the subspaces from filtering outweighs the reduction in their volume and hence generalisability increases overall. The data therefore suggests that cosine similarity filtering can serve as an additional heuristic for improving precision of the DNNs. \n\nMoreover, the best performing model (s-bert 22M), results in medical perturbations and RUAR perturbations contained in the subspaces. While the perturbations contained in the subspaces for the RUAR dataset may seem like a low number, it still results in a robust filter, given that the class of the dataset contains many adversarial examples of the same input query, i.e., semantically similar but lexically different queries. The medical dataset, on the other hand, contains many semantically diverse queries, and there are several unseen medical queries not contained in the dataset nor in the resultant subspaces. However, given that the subspaces contain perturbations of the medical safety dataset, an application of this could be to carefully curate a new dataset containing only queries with critical and serious risk-level labels defined by the World Economic Forum for chatbots in healthcare. This dataset could be used to create filters centred around these queries to prevent generation of medical advice for these high-risk categories. Overall, we find that semantically-informed verification generalises well across the different kinds of data to ensure guarantees on the output and thus should aid in ensuring the safety of LLMs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "Summary. This paper provides an analysis of existing NLP approaches, identifying key components for a general methodology. We distilled these into a \"NLP Pipeline\" consisting of the following components: dataset selection, generation of perturbations, choice of embedding functions, definition of subspaces, training, and component analysis. Using the tool ANTONIO, we were able to study the effects of varying components in an algorithm-independent way.\n\nOur main focus was to identify weak or missing parts of the existing NLP methodologies. We proposed that NLP results should report whether they use geometric or semantic subspaces, and for which type of semantic perturbations; volumes and generalisability of defined subspaces.\n\nWe concluded with a study of current limitations and proposed possible improvements such as introducing a perturbations filter stage using cosine similarity. One of the strengths of the pipeline is that each component can be improved individually.\n\nContributions. The major discoveries of this paper included proposing generalisability as a novel metric, showing methods to overcome trade-offs by using heuristic methods such as defining semantic subspaces, training for semantic robustness, and choosing suitable embedding functions. These methods result in defining more precise subspaces and can be implemented in future NLP pipelines.\n\nWe revealed that assumptions underlying the definition of subspaces need scrutiny, such as whether embedding functions map semantically similar sentences correctly, and whether our methods for generating perturbations preserve semantics. These factors influence practical applications of the pipeline.\n\nWe demonstrated that even defined subspaces can be semantically falsified, due to tensions between geometric methods and the semantic interpretation of sentences. By defining the falsifiability metric, we showed its effects are generally not severe but vary with different scenarios. Awareness of this pitfall is vital.\n\nFinally, we propose a novel framework incorporating a broad spectrum of NLP, geometric, machine learning, and methodological methods under one canopy. This comprehensive range has not been previously addressed in this domain, and it is crucial for the field's development.\n\nFuture Work. Despite a satisfactory resolution to discussed issues, scalability of available algorithms remains a problem. For example, -Crown can handle only limited network parameters, whereas NLP models like BERT have vastly more parameters. This highlights a need for real-world pipeline implementation.\n\nFor future work, we suggest verifying a smaller DNN that can act as a filter upstream of a complex NLP system, ensuring responsible handling of safety-critical queries. This approach, alongside existing initiatives like guardrails, can build safer systems. Implementing these techniques at multiple stages of a pipeline may enhance system safety, a topic we plan to explore.\n\nA future direction is to create NLP benchmarks to spread awareness and focus in this field. Despite advances, major verification competitions still lack NLP benchmarks, a gap this work can help fill for future editions."
        }
    ]
}