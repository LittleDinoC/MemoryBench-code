{
    "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
    "abstract": "To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs).\nPrior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods).\nHowever, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences.\nTo tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \"first-quantize-then-noise\" paradigm to enhance the robustness of the RL algorithm.\nFurthermore, TOLE can be flexibly extended to multiple constraints with little computational expense.\nExperimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "introduction",
            "text": "Large autoregressive language models (LLMs) trained on extensive corpus can generate high-quality texts. However, to satisfy real-world applications, making the generation more controllable is urgent. It is desired to enhance specific attributes of generated texts for practical needs (e.g., positive sentiment for psychological escort, formality for academic writing) and reduce intrinsic defects of pre-trained language models (e.g., toxicity, repetition).\n\nRetraining models are subject to computational overheads as the parameter scales become huge. Post-processing methods leverage small-scale discriminators to bias token distribution, which often leads to low text quality. Some methods adopt parameter-efficient training strategies like prefix-tuning but are susceptible to undesired attributes in the supervised corpus. Recent research introduces other algorithm backbones like diffusion models and normalized flow, but they generally cost more computational expenses during training and have a longer inference time, making them hard to deploy in real applications.\n\nThere is some research introducing reinforcement learning (RL) into controllable text generation (CTG) tasks. RL paradigms can relieve the above problems, as they alleviate overfitting by training on self-generated sentences and can integrate parameter-efficient strategies with canonical LLM backbones. However, RL-based methods generally update language models with sentence-level (or paragraph-level) rewards, leading to suboptimal performance and slow convergence. The coarse-grained rewards cannot provide clear guidance since semantics in the sentence often transits with twists or progression. Moreover, different parts of the sentence may contribute to different attributes. Therefore, RL methods with coarse-grained feedback generally require considerable training steps to converge.\n\nOur objective is to granularize the coarse-grained feedback to provide more precise guidance for LLMs. In this paper, we propose a novel reinforcement learning algorithm with TOken-LEvel guidance named TOLE. We first provide an alternative perspective of Bayesian Factorization, which inspires us to formulate the token-level rewards as the probability shifts of attribute classifiers. To enhance the robustness of TOLE, we propose an exploration framework with a \"First quantize, then noise\" procedure. Moreover, TOLE can be extended to multi-attribute scenarios with few computational overheads. We conduct two experiments on single-attribute: sentiment control and detoxification. We also evaluate TOLE on multi-attribute scenarios with two settings. TOLE achieves superior performance compared with a wide range of baselines."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Controllable Text Generation. Most previous works on controllable text generation (CTG) are based on the auto-regressive framework, which can be categorized into retraining Keskar et al. (2019 ###reference_b14###); Chan et al. (2021 ###reference_b2###), finetuning Huang et al. (2023 ###reference_b12###); Yang et al. (2023a ###reference_b33###); Zhang and Song (2022 ###reference_b35###), and post-processing Krause et al. (2021 ###reference_b16###); Liu et al. (2021 ###reference_b23###); Yang and Klein (2021 ###reference_b32###). Retraining and traditional finetuning methods are of low efficiency since the parameter scale of LMs is surging and the overfitting issue is severe. Post-processing methods regulate the next-token distribution with supplementary modules, mostly an attribute discriminator, but often cause syntax interruption and make language models lose insights. Lu et al. (2022 ###reference_b25###) integrate RL algorithms into CTG but use coarse-grained feedback to guide the LLMs.\n\nMulti-aspect controllable text generation. Along with single-aspect controlling, most research on multi-aspect controllable text generation can also categorized into finetuning and post-processing. Some post-processing research Lin and Riedl (2021 ###reference_b22###); Kumar et al. (2021 ###reference_b17###) in MCTG combines multiple attribute discriminators to aggregate the controllability. However, they also inherit drawbacks of post-processing methods due to direct distribution regulations. Finetuning-based research tries to connect several single controllers, e.g. connectors to combine multiple plugins Yang et al. (2023a ###reference_b33###), latent variables to represent the unsupervised aspects Qian et al. (2022 ###reference_b26###), direct combination of prompts Huang et al. (2023 ###reference_b12###), the boundary exploration of intersected subspaces Gu et al. (2022b ###reference_b9###, 2023 ###reference_b10###).\n\nToken-level guidance for Reinforcement Learning. There is a series of research (Chen et al., 2021 ###reference_b3###; Janner et al., 2021 ###reference_b13###; Zheng et al., 2022 ###reference_b36###; Xu et al., 2023 ###reference_b31###) incorporating RL techniques into the transformer structure, trying to deconstruct the coarse-grained reward into the token level for sequential modeling. However, they are hard to extend to practical applications since their specialized token settings are not in line with current LLMs. Concurrent with our research, some research Wu et al. (2023 ###reference_b30###); Yang et al. (2023b ###reference_b34###) on LLM alignments tries to handle the problem of coarse-grained feedback. RLHF (reinforcement learning from human feedback) algorithms of the LLM alignment generally require a large-scale reward model, which should be trained on datasets formatted as pairwise sentences with the same prefix. However, such data is unavailable when confronted with a wide variety of attribute requirements. Therefore, exploring a novel reinforcement learning algorithm with token-level feedback is significant for controllable text generation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "We will first establish the notation, provide some background on existing RL methods in controllable text generation and model alignment, and offer an overview of our algorithm."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "Notations. A standard Markov Decision Process (MDP) can be denoted as . At each step, an action  is made based on the current state . Then the state will be transited to  with the possibility . A function  defines the returned reward based on the states and actions. The strategy is decided by a policy model , which is a predicted distribution over actions based on state .\n\nTo transfer to text generation scenarios, the state can be defined as the partially generated sentence , and the action is the next token  where the vocabulary  is the action space. The transition dynamic  is deterministic since each state-action pair  leads to a unique state .\n\nPrior RL-based methods. In previous RL-based methods of controllable text generation, rewards are derived from , which denotes the possibility that the sentence  satisfy the attribute .  can be obtained by corresponding attribute classifiers.\n\nSince prior research only concentrates on sentence-level feedback, which can be regarded as . This equality means that sentence-level feedback treats each action  in the MDP process of  equally, which can only provide rough guidance for models."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Token-level Rewards",
            "text": "We first provide an alternative perspective of Bayesian factorization to show that the probability shift of attribute classifiers plays an important role in controlling the generations. The Bayesian factorization can be rewritten as: Even if \\(\\theta\\) tends to highly satisfy the condition when a sentence is finished i.e. \\(P(y|x)\\) is large, action \\(a_t\\) may not play an important role since previous \\(a_{<t}\\) may already make future generations satisfy \\(y\\) easily i.e. \\(P(y|x_{<t})\\) is large. It reveals that what matters is the probability shift between them, which enlightens our reward design. \n\nThe token-level reward function can be formulated as the probability shift before and after the word is generated. The function is:\n\n\\[ R_t = \\sigma(P(y|x^{<t+1}) - P(y|x^{<t})) \\]\n\nwhere \\(\\sigma\\) is an activation function for normalization, where we adopt the sigmoid function for implementations. Theoretically, to approximate \\(P(y|x^{<t})\\), the format of training data should be transformed from the traditional \\(x \\to y\\) to \\(x^{<t} \\to y\\). However, we find using traditional classifiers in our algorithms can achieve on-par performance in experiments compared to specially trained classifiers."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "RL Algorithm: First quantize, then noise.",
            "text": "The training procedure of our RL algorithm can be separated into initialization, exploration, quantize & noise, and learning.\n\nInitialization. First, we initialize a policy LLM, a copy of the policy model as the reference model, an attribute scorer. The reference model is frozen during the whole process. We also initialize a data pool, and prepare a prefix corpus for exploration.\n\nExploration. Then, given the prefix, the current policy model can generate subsequent text. For each generated token, we calculate the score shift as its reward, and add to the data pool. To avoid over-training on data explored in earlier episodes, we set a lifetime for each data to indicate the episodes it can still undergo. Once the data is added, the lifetime is initialized to and subtracts 1 after each training episode. The data is removed when its lifetime drops to 0.\n\nQuantize & Noise Learning primitive rewards can predispose the model to flatter the scoring pattern of attribute classifiers, which may cause diversity decrease. Therefore, we propose \"First quantize, then noise\" to avoid this problem. First, we quantize the rewards within, and acquire -quantiles, which divide the reward range into intervals. Then, we inject noise into each reward while ensuring each reward stays in the original interval. Specifically, for a reward, we reassign it as where is a noise processed with a clip function to satisfy. is substituted with Gaussian noise in our implementations. Through this process, we disrupt the reward order to interfere with the fixed scoring patterns of classifiers, while maintaining the relative order between intervals to steer LLMs toward the target attribute.\n\nLearning. Through above procedures, we can obtain to provide dense guidance on each token without granularity mismatch or feedback delay. The minimalist objective of our optimization problem is to maximize the total rewards. We relax the convergence by adding a standard max-entropy gradient, which can help capture diverse behavior modes. We also insert a KL-divergence penalty to keep the model from deviating too far from the original. The gradient of each sentence can be formulated as follows, where are two balancing coefficients, is the Shannon entropy, and is the KL divergence between and. We then use the updated model for exploration and repeat the Exploration-Quantize & Noise-Learning cycle until training achieves the maximum episode number."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Extension to Multiple Attributes.",
            "text": "To consider multiple constraints simultaneously, we should combine multiple reward groups from different scorers. Simple aggregations or averages cannot provide appropriate token-level guidance, since scorers may contradict each other. Moreover, different parts of sentences may address different attributes, so we need to weigh the token’s contribution to multiple attributes respectively.\n\nTo tackle this, we train a small-scale \"weigher\" to balance rewards from scorers, where is the hidden size of LLMs. Given the last-layer hidden states of output by LLMs, the weigher outputs as the weight for rewards of.\n\nThe weigher does not require a complex model structure. Simple structures can already assist our algorithm to achieve great performance. Hence, it does not take significant computational overheads. In our implementation, the weigher consists of two linear layers with the ReLU function and an output layer with a softmax function. The comprehensive reward of action can be obtained by.\n\nTo train the weigher, we formulate the optimization problem as maximizing the integrated reward of a training corpus that satisfies the multiple attributes, where is a uniform distribution among. By doing so, the weigher learns which scorer should be paid more attention when considering different tokens within sentences."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Category\nModel\nAttribute Correctness()\nGeneration Metrics\nTraining Info.\n\nTarget:Positive\nTarget:Negative\ndist-3()\nCR.()\n%Params\n\nnegative\nneutral\npositive\nneutral\n\n \n\n\nPost-\n\nprocessing\n \nGeDi\n26.80\n86.01\n60.43\n91.27\n0.86\n3.62\n-\n\nFUDGE\n56.04\n96.92\n66.84\n98.76\n0.83\n1.53\n-\n\n \n\n\nFine-\n\nTuning\n \nPrompt\n40.88\n78.08\n49.28\n73.20\n0.73\n63.08\n0.003\n\nDisCup\n49.92\n91.58\n60.80\n90.64\n0.75\n3.72\n0.003\n\n \n\n\nReinforcement\n\nLearning\n \nPPO\n43.13\n94.10\n68.12\n94.95\n0.71\n2.95\n100\n\nQuark\n47.32\n95.50\n70.50\n96.65\n0.75\n2.63\n100\n\nTOLE\n69.36\n97.16\n72.81\n98.02\n0.75\n2.61\n0.003"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Sentiment Control",
            "text": "Experimental Settings. Following previous works, we use 10K naturally occurring prompts from the OpenWebText Corpus, which is divided into 5K “neutral” prompts, 2.5K “negative” prompts, and 2.5K “positive” prompts. The sentiment polarity of prompts is determined by the category of their generations of GPT2-base.\nWe use GPT2-large as the base PLM, and adopt prompt techniques rather than tuning the whole model.\nThe sentiment scorer is based on GPT2-base, which is trained on SST-5 following Zhang and Song.\nCorrectness is the proportion of generations that satisfy target sentiment. We use a Huggingface sentiment classifier to discriminate categories of generations. See more details in Appendix B.1.\nWe also conduct human evaluations based on the perceived level of sentiment correctness, topicality, and fluency. Details of human evaluation can be found in Appendix C.\nBaselines.\nA wide range of competitive baselines are compared with our algorithm.\nWe compare our methods to post-processing methods as follows: GEDI (Krause et al., 2021), and FUDGE (Yang and Klein, 2021).\nWe also choose several competitive finetuning-based methods as our baselines:\nPrompt-tuning (Li and Liang, 2021), DisCup (Zhang and Song, 2022).\nTo compare with RL-based methods, we implement PPO (Schulman et al., 2017) and QUARK (Lu et al., 2022).\nSee more details in Appendix B.1.\nResults and Analysis.\nThe automatic evaluation results are shown in Table 1.\nThough post-processing can make generated sentences satisfy the target sentiment with the least parameters to train, even in a zero-shot way by decoding-time regulation with attribute discriminators. Fine-tuning methods can maintain text fluency while getting considerable accuracy of target attributes, but they suffer from overfitting the training corpus with high coverage rates.\nDisCup borrows RL paradigms by exploring candidate tokens to alleviate the overfitting problem, alleviating the overfitting issue. RL-based methods get the best performance among all baselines. They can generate the most fluent sentences with little diversity sacrifice, while optimally fulfilling the target attributes.\nSince prior RL-based methods only adopt sentence-level feedback, they can only achieve suboptimal performance even with all parameters of LLMs to be updated.\nOur method guides LLMs with finer-grained feedback, thus attaining better performance with a substantial reduction of computational expenses, since it requires fewer parameters and training steps (§4.4).\nModel\nIn-domain RealToxicityPrompts\nOut-of-domain WritingPrompts\n\nToxicity ()\nGeneration\nToxicity ()\nGeneration\n\navg. max.\nprob.\ndist-3\navg. max.\nprob.\ndist-3\n\nGPT2\n0.527\n0.520\n11.31\n0.85\n0.572\n0.610\n12.99\n0.85\n\nGeDi\n0.363\n0.217\n60.03\n0.83\n0.261\n0.050\n91.16\n0.82\n\nDExpert\n0.314\n0.128\n32.41\n0.84\n0.343\n0.156\n42.53\n0.85\n\nPrompt\n0.302\n0.360\n29.21\n0.74\n0.442\n0.363\n30.10\n0.79\n\nDiscup\n0.298\n0.115\n39.30\n0.84\n0.442\n0.363\n37.23\n0.85\n\nPPO\n0.288\n0.130\n18.22\n0.82\n0.291\n0.132\n18.32\n0.84\n\nQuark\n0.237\n0.118\n17.23\n0.81\n0.268\n0.102\n17.19\n0.83\n\nTOLE\n0.206\n0.105\n15.45\n0.80\n0.223\n0.080\n16.51\n0.83"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Detoxification",
            "text": "Experimental Settings.\nToxic degeneration is an inherent problem of LLMs, since LLMs may express harmful or offensive utterances.\nWe train the classifier on Toxicity Classification Kaggle challenge, which includes 160K toxic comments and 1.4M nontoxic comments.\nWe use RealToxicityPrompts Gehman et al. dataset as our experimental corpus which consists of 100k prompts designed to elicit toxicity. We use the 10K non-toxic test prompts following Liu et al., and take other prompts as the exploration prefixes.\nWe use the same LSTM-based prompt techniques on GPT2-large.\nAdditionally, we also conduct out-of-domain evaluation with the WritingPrompts dataset Fan et al., which is created for creative writing.\nWe evaluate the detoxification ability by the average maximum toxicity over 25 text generations, and the probability of at least one of any 25 generations being toxic. The toxicity is judged by Perspective API.\nWe also conduct human evaluations on control accuracy, fluency, and overall text quality. The evaluation settings and results are in Appendix C.\n\nBaselines.\nAs sentiment control tasks, we compare our methods to post-processing methods, finetuning-based methods, and RL-based methods. Post-processing methods are as follows: GEDI (Krause et al.), DExpert (Liu et al.).\nWe choose DisCup (Zhang and Song) to represent finetuning-based methods.\nWe implement RL-based methods: PPO (Schulman et al.) and QUARK (Lu et al.).\nSee more details in Appendix B.1.\n\nResults and Analysis.\nFinetuning-based methods have ordinary performances since fine-tuning models on specific corpus is easily overfitted to undesired attributes.\nRL-based methods generally achieve the lowest toxicity on both toxicity metrics. Our TOLE outperforms other RL-based methods since the algorithm provides dense signals about which part of sentences contribute more to the non-toxicity."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Multiple Attribute Controlling",
            "text": "Model\nDouble Controls\nTriple Controls\n\nSent.()\nTop.()\nAve.()\nDist.()\nSent.()\nTop.()\nTense.()\nDist.()\n\nGeDi\n99.47\n51.36\n75.41\n0.75\n-\n-\n-\n-\n\nDist. Lens\n77.47\n66.98\n72.22\n0.26\n65.31\n55.84\n54.25\n0.40\n\nTailor\n80.68\n68.72\n74.70\n0.39\n68.08\n58.67\n33.38\n0.42\n\nP-Gating\n84.80\n75.02\n79.91\n0.42\n76.93\n62.73\n62.24\n0.45\n\nTole\n91.27\n86.32\n88.80\n0.52\n86.31\n92.68\n89.50\n0.51\n\n- weigher\n93.68\n78.72\n74.70\n0.51\n85.10\n84.72\n70.82\n0.51\nExperimental Settings.\nWe conduct experiments on a double-attribute control task and a triple-attribute control task. We adopt the widely-used Yelp Lample et al. (2019  ###reference_b18###) benchmark, containing restaurant reviews with the sentiment (positive and negative) and the subject (American, Mexican, and Asian) labels. To measure whether the sentence satisfies given attributes, we finetuned two RoBERTa-based Liu et al. (2019  ###reference_b24###) classifiers for the evaluations of sentiment and subject with its original setting. Following Huang et al. (2023  ###reference_b12###), we add another constraint, tense (past and present) Ficler and Goldberg (2017  ###reference_b6###) where their labels are automatically extracted from the reviews with an open-source toolkit333https://github.com/ajitrajasekharan/simple_tense_detector  ###reference__tense_detector###.\nAveraged distinctness Li et al. (2016  ###reference_b19###) is reported to demonstrate the fluency and diversity of the generated text. We also conduct human evaluations on generated results. Due to page limit, see Appendix B.2  ###reference_### for more details.\nBaselines.\nResearch on multi-attribute CTG is not as abundant as single-attribute CTG. We extend\nGEDI Krause et al. (2021  ###reference_b16###), which adopts a small-scale conditional generative discriminator to bias the token distribution, to multi-attribute controlling according to Huang et al. (2023  ###reference_b12###). We also include DIST. LENS Khalifa et al. (2021  ###reference_b15###), which introduces an autoencoder to map constraints to latent subspaces, and explore the intersection of multiple constraints.\nTailor Yang et al. (2023a  ###reference_b33###) which proposes a connector to combine several prompts. Meanwhile, it modifies the attention mask and position indexes to narrow the gap between training and inference.\nPrompt-gating Huang et al. (2023  ###reference_b12###): it gates the prompts before appended into the LLMs to mitigate the mutual interference.\nWe also implement sentence-level RL methods, PPO Schulman et al. (2017  ###reference_b28###) and Quark Lu et al. (2022  ###reference_b25###), whose rewards are the sum of single-attribute rewards. We also conduct human evaluations. See Appendix C  ###reference_### for more details.\nResults and Analysis.\nThe results are shown in Table 3  ###reference_###. The post-processing method, GeDi, though gets competitive results on attribute accuracy, the deterioration of text quality caused by direct decoding-time regulation is more severe than in single-attribute generation.\nDist. Lens though achieves considerable results, it requires over six times inference time to determine the intersection boundary of attribute subspaces.\nPrompt-based methods Tailor and Prompt-Gating achieve suboptimal performance on both double- and triple-attribute scenarios. However, since they are easily overfitted to undesirable attributes in the training corpus which may contradict other target attributes, their performance is limited. With more fine-grained guidance on sampled sentences,\nour method can achieve the best control accuracy in both settings without significant inference expenses.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Further Studies",
            "text": "What effect do \"Quantization\" and \"Noise\" have respectively? To visualize the difference made by \"First quantize, then noise\", we implement two variations of our algorithm, and conduct experiments on sentiment control tasks.\n\nFirst, we directly use the scores output by classifiers as rewards without any interference. We display the performance transition over the training steps of sentiment control tasks as in Figure 2. The figure demonstrates that the control accuracy and the text diversity both decrease. Our algorithm can achieve higher attribute accuracy since the noising procedure can promote the generalization of models, though initially converge slower. Moreover, the noising procedure can prevent models from flattering the scorers, thus achieving higher text diversity.\n\nWe also implement another variance that noise the reward without quantization boundaries. As shown in Figure 3, we can see that quantization enhances the stability of algorithms. The model can learn from the relative order of datasets, even with a big standard deviation of Gaussian noise. If we ablate the quantization procedure, the algorithm will be sensitive to the amplitude of noise.\n\nWhat if we ablate the \"weigher\" from the multi-attribute combination, but adopt averages as overall rewards? We implement a model variation that combines several scorers by averaging their output scores. Table 3 shows that ablating \"weigher\" leads to a performance decrease. To further prove that \"weigher\" can provide more clear guidance with no contradiction between different scorers, we display the scores by averaging and aggregating by \"weigher\" respectively in Figure 4. The left subgraph concentrate within small values due to the scorer contradiction without \"weigher\". On the contrary, the right heatmap shows more distinct guidance for models.\n\nConvergence speed compared to sentence-level feedback. Token-level feedback can provide dense and precise signals for models, thus requiring fewer learning steps to achieve ideal performance. We implement a variance of Tole with sentence-level guidance with the same quantization & noise process. We display the performance transition over training steps in Figure 2. The figure shows that the sentence-level feedback slows down the convergence significantly, compared to the token-level feedback.\n\nWhat effect does the number of quantiles have? A -quantile does not have a significant effect on final performance. However, the convergence of the process is slightly slower if is relatively large or small. When is small, relative orders between quantiles are more ambiguous. A large confines noise within a small interval, diminishing noise impact, which results in a lower generalization. A moderate q-value allows the model to reach the desired result faster. See more details in Appendix D.1.\n\nWhat effect does the number of have? are two hyper-coefficients of KL-divergence and entropy term Eq.5 respectively. We conduct experiments with varying of . Experimental results indicate that higher can increase text fluency, but sacrifice controllability slightly, since higher more tightly constrain the model not to deviate too much. Our experiments also demonstrate that the entropy term has a relatively slight effect on performance, not as much as KL-divergence. As increases, attribute accuracy and text diversity have a slight increase. See more details in Appendix D.2.\n\nDiscussion about reward hacking. Though our algorithm achieves great results in the above experiments, we are concerned that reward hacking occurs in some scenarios when scorers are too simple for LLMs to find unintended shortcuts. One solution to reward hacking is to complicate reward design, which is easy to implement in our algorithms by adding new constraints with weighers."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "To summarize, we propose an extensible reinforcement learning algorithm for controllable text generation with token-level feedback. We provide an alternative perspective of Bayesian Factorization, which enlightens our token-level reward design. We also introduce \"Quantization & Noise\" into RL to enhance the algorithm's robustness. We propose a small-scale module \"weigher\" to extend our algorithm to multiple constraints. Extensive experiments demonstrate the effectiveness of our algorithm."
        }
    ]
}