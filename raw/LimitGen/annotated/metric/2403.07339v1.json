{
    "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers",
    "abstract": "GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need – for both training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by low bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone. To address this issue, we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to unpack a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers. This allows equivalence with the original GEMM, i.e., the exact result can be obtained using purely low bit-width integer GEMMs. This comes at the cost of additional operations – we show that for many popular models, this overhead is quite small.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Calculating the product of two matrices using GEneral Matrix Multiply (GEMM) is one of the most widely used operations in modern machine learning. Given matrices and of size and respectively, the output of a GEMM is calculated as Choosing the appropriate numerical precision or data type (FP32, FP16, or BF16) for GEMM is often important, and hinges on several factors including the specific application, characteristics of the data, model architecture, as well as numerical behavior such as convergence. This choice affects compute and memory efficiency most directly, since a disproportionately large chunk of the compute footprint of a model involves the GEMM operator. A good example is the large improvement in latency and memory achieved via low bit-width GEMM, and made possible due to extensive ongoing work on quantization (to low bit-width data types) and low-precision training. Integer quantization is being actively pursued for inference efficiency, and the use of low bit-width integers is universal to deliver the efficiency gains. However, this strategy often incurs large rounding errors when representing all matrix entries as low bit-width integers, and explains the drop in performance and thereby, a need for error correction techniques. To answer this question, it appears worthwhile to check whether integer GEMMs will achieve parity without sophisticated techniques (for the inference stage, and more aspirationally, for training) for popular models if we do not restrict to low bit-width integers. Overview. The starting point of our work is to first experimentally verify that the aforementioned hypothesis – that integer GEMM may work – is true.\n\nStill, this experiment is useful for the following reason. For a particular class of models (e.g., Transformers), we can easily contrast the corresponding input matrices and between (a) integer GEMM and (b) low bit-width integer GEMM and probe if any meaningful structure can be exploited. While there is a clear difference in the outputs of (a) integer GEMM versus (b) low bit-width integer GEMM, we find that a large majority of entries of and can be well-represented using low bit-width integers – and the difference in the outputs can be entirely attributed to a few heavy hitter entries in  and, that cannot be represented using low bit-width integers. Other works have also run into this issue of “outliers” and suggest using high precision or a separate quantization for these entries. Driven by the simple observation that we can represent a large integer by a series of smaller integers, our algorithm, Integer Matrix Unpack (IM-Unpack), enables unpacking any integer into a series of low-bit integers. The key outcome is that the calculation can be carried out entirely using low bit-width integer arithmetic and thus unifies the computation needed for heavy hitters and the remaining entries (which were already amenable to low-bit integer arithmetic). Specifically, IM-Unpack unpacks an integer matrix such that all values of the unpacked matrices always stay within the representable range of low bit-width integers (bit-width can be chosen arbitrarily). We obtain the exact result of the original integer GEMM using purely low bit-width integer GEMMs. Since the bit-width of integer arithmetic is independent of the actual range of the original matrices, the construction will greatly simplify the hardware/compiler support by only needing support for one specific bit-width. The overall structure/contributions of this paper is shown.\n\nNotations. To simplify the presentation, we will narrow the scope of our discussion exclusively to Transformer-based models. We first define notations for all relevant GEMMs. For the linear layer, let the input activation and parameter matrix be and. Let the query, key, value matrices involved in self-attention computation be. Below, we itemize all GEMMs used in a Transformer model: where is the attention score between and defined as (omitting scaling factors). Now, given the gradient for denoted as, the other gradients are calculated via GEMMs as well: These notations will help refer to each type of GEMM later."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Round to Nearest: What do we lose?",
            "text": "Let us check the extent to which integer GEMMs work satisfactorily for both training and inference, if we do not restrict to low bit-width integers. \n\nThe hyperparameter  is the number of distinct integers that we want to use to encode values. After quantization, the GEMM for the original matrices can be approximated in the quantized domain using integer GEMMs. The approximated GEMM is computed using the quantized  and :\n\nFor notational simplicity, if clear from context, we will drop the  subscript from  and ."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Efficacy of Integers: Inference",
            "text": "A majority of the literature on quantized low precision calculations focuses on inference efficiency. Here, given a trained model, quantization seeks to reduce the precision of parameters and input activations to low precision. This allows faster low precision arithmetic for compute efficiency while maintaining model performance.\n\nMost quantization schemes for LLMs focus on quantizing GEMMs in Linear layers, while quantization methods for Vision Transformers are more ambitious and quantize all GEMMs in a Transformer. It is common to try and quantize the weight and input activation of linear layers to low precision for compute efficiency. Quantize all GEMMs.\n\nA more ambitious goal is to quantize every GEMM in a Transformer model for higher efficiency."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Efficacy of Integers: Training",
            "text": "The transition from FP32 to FP16 and BF16 for GEMMs has doubled the compute efficiency of modern deep learning models. However, far fewer efforts have focused on low precision training (relative to inference) and this usually requires more sophisticated modifications (wang2018training, ###reference_b25###; wu2018training, ###reference_b27###; zhu2020towards, ###reference_b30###; wortsman2023stable, ###reference_b26###). To ensure that the updates can be properly accumulated for the parameters, we use FP32 for storing the parameters. \n\nRoBERTa.\n\nViT."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "What happens with Low Bit-Width?",
            "text": "Converting floating point to integers alone will not provide efficiency benefits. Rather, we want to use a representation that can be efficiently computed (and why low bit-width integers are common in integer quantization). \n\nThe difficulty involves dealing with a small percentage of entries that cannot be represented efficiently with low bit-width integers. To get a sense of how large these values are, we calculate the ratio between the maximum and 95-percentile of the magnitude of each matrix in GEMMs when performing (a) inference (forward pass) of LLaMA-7B and ViT-Large and (b) training (forward pass and backward pass) of RoBERTa-Small at different training phases. We observe extremely large values across both training and inference and throughout the training duration, indicating that simply increasing the representation bit width by a small margin will not suffice to encapsulate these outliers.\n\nWe performed experiments studying different ways of handling these outliers when quantizing all GEMMs (linear layers and self-attention computation) in Transformer models. Simply ensuring that the outliers lie within the representable range results in significant performance drops, as shown in our experiments. Clipping the extreme outliers at a certain percentile also fails to yield satisfactory results.\n\nSome approaches have been proposed to tackle these so-called outliers. For example, moving rows or columns with outliers into a separate matrix and performing the matrix operations using higher precision, or smoothing the outliers in activation to ease the quantization difficulty. These strategies, however, typically require specialized hardware support and may reduce overall performance as shown in baseline comparisons.\n\nOur goal is to implement a method that maintains the precision and accuracy of integer GEMMs without switching between different calculation precisions. Our simple procedure, IM-Unpack, achieves this by representing outliers using low bit-width integers through a unique approach. IM-Unpack unpacks matrices with outliers into larger matrices where all values are representable by low bit-width integers. This procedure enables the exact output of the original GEMM using exclusively low bit-width integer calculations, maintaining efficiency and precision."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "IM-Unpack: Integer Matrix Unpacking",
            "text": "Our approach starts with a simple observation that, for example, a 32-bit integer can be represented as where are 8-bit integers. Multiplication/addition of two 32-bit integers can be performed on these decomposed 8-bit integers followed by some post-processing steps (scaling via bit shifting and accumulation). This unpacking does enable performing high bit-width arithmetic using lower bit-widths, but it achieves this at the cost of requiring more operations. For example, one 32-bit addition now becomes five 8-bit additions with some follow-up processing, and one 32-bit multiplication becomes twenty-five 8-bit multiplications (distributive law). The reason why this unpacking is still useful is because the additional work depends on the number and spatial distribution of the heavy hitters/outliers. We harvest gains because outliers account for a very small portion of the matrices that appear in practice in training/inference stages of Transformer models. ###figure_4### Let be the target bit-width of low bit-width integers and be the representable range of bit-width: -bit integers can represent a set . We refer to any integers inside of this set as In-Bound (IB) values and any integers outside of this set as Out-of-Bound (OB) values, which will be used in later discussion to refer to the values that need to be unpacked. We will first show how to unpack a vector to multiple low bit-width vectors. Then, we will discuss how to unpack a matrix using different strategies to achieve better results in different cases. Lastly, we will evaluate how well does IM-Unpack work. Unpacking an integer vector. Let be an integer vector and define a function: such that for all , all entries of are bounded (IB), i.e., lie in the interval . When is clear from the context, we shorten the LHS of (7 ###reference_###) to just . Then, Note that decreases to exponentially fast, so we are able to unpack a vector with just a few low bit-width vectors."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Variants of Matrix Unpacking",
            "text": "In this subsection, we discuss different strategies of matrix unpacking for different structure-types of matrices. \n\nFirst, we discuss the case where  is the matrix containing OB values to be unpacked and  is a matrix whose values are all IB. Next, we discuss how unpacking works when both  and  contains OB values.\n\nUnpacking row vectors.\nWe start with the simplest way of unpacking a matrix: unpacking the row vectors. Given a matrix , if one row of  contains OB values, we can unpack the row to multiple rows whose entries are all bounded. The exact procedure is described in Alg. 1 ###reference_### and illustrated in Fig. 4 ###reference_###. In Fig. 4 ###reference_###, when the second row in  contains OB values, we can unpack it to two row vectors (the second and fifth row) and the post-processing step takes the form of applying  to the unpacked matrix .\n\n###figure_5### Reconstructing .  can be reconstructed using the unpacked matrix  whose entries are IB and a sparse matrix  whose column contains only one non-zero: Here, applying  to  can be efficiently computed easily (for example, via torch.index_add).\n\n###figure_6### Are we done? If we do not care about maximizing efficiency, then the above scheme already provides a way to perform high bit-width GEMM using low bit-width GEMM. However, this might not be the optimal unpacking strategy for some matrices. For example, consider the left matrix shown in Fig. 6 ###reference_###. Since every row of this matrix contains OB values, every row need to be unpacked, resulting in a much larger matrix. In this case, it might be better to try and unpack the column vectors.\n\nLet us apply a similar idea of unpacking row vectors to unpack column vectors of :\nWhile unpacking column vectors is reasonable, the sparse matrix  creates a problem. When performing a GEMM of two lower bit-width matrices:  has to be applied to  or  before GEMM, but the result/output may contain OB entries after the application, disabling low bit-width integer GEMM. This problem is similar to per-channel quantization. It is not simple to handle and become more involved when  also need to be unpacked.\n\nUnpacking column vectors. Alternatively, let us look at how  is computed via outer product of column vectors:\nLet us look at the -th outer product. Let us try unpacking  using (8 ###reference_###), then we have Suppose that  for , then we can unpack one outer product to  outer products. This is equivalent to appending  for  to the columns of , appending  identical  to the columns of , and maintaining a diagonal matrix to keep track of the scaling factor . The exact procedure is described in Alg. 2 ###reference_###, and Fig. 5 ###reference_### shows a visualization of unpacking columns. Using column unpacking, we have Naively, this still suffers from the same problem as discussed in (10 ###reference_###) in that there is a diagonal scaling matrix between two low bit-width matrices making low bit-width GEMMs difficult. However, since  is a diagonal matrix whose diagonal entries consist of a few distinct factors in , we can easily compute one GEMM for each distinct diagonal entry as shown in Alg. 3 ###reference_###. Further, since  is a power of , the scaling can be efficiently implemented via bit shifting.\n\n###figure_7### Are we done yet? Unpacking columns is efficient for the left matrix shown in Fig. 6 ###reference_###. However, neither unpacking rows nor unpacking columns will be efficient for unpacking the right matrix shown in Fig. 6 ###reference_###. All rows and columns contain OB values. Unpacking rows or columns alone will not be ideal. For the right matrix in Fig. 6 ###reference_###, a better strategy is to unpack the second row and the second column simultaneously.\n\nUnpacking both rows and columns simultaneously.\nOur final strategy combines row and column unpacking together and selectively performs row unpack or column unpack based on the number of OB values that can be eliminated. The procedure is described in Alg. 4 ###reference_###, and we provide an illustration of unpacking both dimensions in Fig. 7 ###reference_###. With this procedure, we can obtain the output of high bit-width GEMM using low bit-width as: Here,  can be calculated via Alg. 3 ###reference_###, and applying  can be performed efficiently as discussed.\n\nCombining everything.\nSince we have different strategies for unpacking, let us first define a unified interface in Alg. 5 ###reference_###. One can verify that for any strategies :\nIn the previous discussion,  was assumed to have all IB values. When  contains OB"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluating Unpacking Overhead",
            "text": "The idea of IM-Unpack is to use more low bit-width arithmetic operations to compute a high bit-width operation. As we see in the description of IM-Unpack algorithm, the number of row and column vectors increases, so the unpacked matrices  and  will have a larger size compared to  and , which obviously increases the computational cost of low bit-width GEMMs. In this subsection, we evaluate how much this cost would increase. For two matrices  and , the complexity of a GEMM is . Similarly, let  be the size of  and  be the number of rows of . The cost of  is , we can directly measure the unpack ratio to understand by how much the cost for low bit-width GEMMs increases.\n\nWe use LLaMA-7B to study the unpack ratio  when using different unpacking strategies. Note that unpacking both requires keeping track of the OB count in each row and column vector, which is not as fast as the other two strategies. We only use it for unpacking parameters  for inference since it can be performed once when loading the model. The Mix strategy means that for each GEMM, we compare different strategies and choose the optimal strategy that results in the smallest unpack ratio. We note that the unpack ratios of computing  and  are quite reasonable, but the ratios of computing  are larger. This is expected since the large outliers of the self-attention matrix  mainly concentrate in the diagonal.\n\nWe also study the unpack ratios of each type of quantized GEMMs at different training phases. The ratios stay relatively unchanged as training progresses. Also, we can observe a similar high unpack ratio when computing  and  since these GEMMs involve self-attention matrix .\n\nLastly, we verify that we can unpack matrices to arbitrarily low integer matrices. The 2-bit setting is the lowest bit width that can be used for symmetric signed integers ()."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "To simplify the presentation, we used the simplest RTN quantization, which might not deliver the optimal performance. More sophisticated techniques\nare likely to further improve the results. For example, we may be able to remove the demands of large  for the set  for ViT training.\nThe current unpacking strategies cannot handle the self-attention matrix  efficiently since the outliers mainly concentrate on the diagonal region rather than rows or columns; this needs further study."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we verify the efficacy of integer GEMMs in both training and inference for Transformer-based models in language modeling and vision. The presence of large outliers/heavy-hitters makes it difficult to make use of efficient low bit-width integer GEMMs since these outliers are much larger than the representable range of low bit-width integers. We take a “multi-resolution” view in how we extract a spectrum of bit-width tradeoffs. This is loosely similar to sparsity but here, instead of making a zero versus non-zero distinction between the entries, our heavy-hitters (which need higher bit-width representations) are analogous to “non-sparse” entries. To address the challenge of high bit-width heavy-hitters, we develop an algorithm to unpack integer matrices that contain arbitrarily large values to slightly larger matrices with the property that all values lie within the representable range of low bit-width integers, and a procedure to obtain the GEMM output of original matrices using only low bit-width integer GEMMs on the unpacked matrices followed by some scaling (using bit shifting) and accumulation. Our algorithm can greatly simplify the design of hardware and improve power efficiency by only supporting low bit-width integer GEMMs for both training and inference."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Appendices",
            "text": "In this section, we provide more details about design choices and experiment setups as well as more experiments that are left out in the main text."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Why Using Percentiles?",
            "text": "We need a way of mapping the actual range of values in a floating point matrix to an integer range, and ensure most values fall into the desired range and fill up the representable range as much as possible, so we need a statistic to estimate the range of values in a FP matrix.\nWe compared percentile and standard deviation. We inspected different parameter matrices  and the corresponding inputs  in the LLaMA-7B model [24  ###reference_b24###]. While the outlier problem in  is moderate, and both standard deviation and percentile work well, the outliers in  is problematic and contains a few values that are much larger than non-outliers. The estimation of standard deviation might be severely impact the extreme outliers in  as shown in Tab. 11  ###reference_###: removing an extremely small subset of the largest outliers can severely alter the estimates. On contrast, percentile is more robust to the extreme outliers. As a result, we choose percentile as the estimation of value range."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Baseline Comparison when Quantize Parameters Only",
            "text": "LLaMA-7B\nViT\nOne direction of quantization research focus on quantizing the parameters for better storage and memory usage.\nWe also evaluate how well RTN works on storage and memory efficiency.\nAfter quantization, the quantized  usually contains a few hundreds of distinct integers. Simply representing  in plain integer format would not be efficient and usually requires larger than 8 bits per value for memory. By inspecting the value distribution of , we found that the fewer values occur much more frequently than others, which create a clear opportunity for compression. We simply apply Huffman Encoding (HE), which was also in [12  ###reference_b12###] to compress models for memory efficiency, to use shorter encoding for more frequent values. As shown in Table 12  ###reference_###, with RTN and HE, we are able to significantly reduce the average bites per value with small or no performance degradation and result in significantly better efficiency compared to baselines [11  ###reference_b11###, 4  ###reference_b4###, 18  ###reference_b18###, 29  ###reference_b29###] for both Transformer based LLMs and Vision Transformers."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Details of Training Experiments",
            "text": "We run all of our experiments on NVIDIA RTX 3090’s. The following are training hyperparameters.\nRoBERTa. The RoBERTa-Small is a 4-layer Transformer encoder whose model dimension is 512, hidden dimension is 2048, and number of heads is 8.\nFor Small models, we train each model for 200K steps with batches of  -length sequences. We use an AdamW optimizer with 1e-4 learning rate, 10,000 warm-up steps, 0.01 weight decay, and linear decay.\nFor Base models, we train each model for 300K steps with batches of  -length sequences. We use an AdamW optimizer with 5e-5 learning rate, 10,000 warm-up steps, 0.01 weight decay, and linear decay.\nViT. We use timm to train our ViT-Small models. The hyperparameters of all experiments are the same: batch size 1024, optimizer AdamW, learning rate 0.001, weight decay 0.05, augmentation rand-m9-mstd0.5-inc1, mixup 0.8, cutmix 1.0."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Unpack Ratios of ViT-Large",
            "text": "###table_2### Linear ()\n\n\nAS ()\n\n\nAO ()\n\n\nSimilar to Tab. 8  ###reference_### in the main text, we also evaluate the unpack ratios of ViT-Large, which are shown in Tab. 13  ###reference_###. The overall results are similar to what was observed in unpack ratios of LLaMA-7B (Tab. 8  ###reference_###)."
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "More Empirical Results on LLM Quantization",
            "text": "Mistral-7B\nPhi-2\nTo evaluate how well RTN works on the inference of different models and different model sizes, beside the experiments shown in the main text, we also run experiments on LLaMA-13B [24  ###reference_b24###], Mistral-7B [13  ###reference_b13###], and Phi-2 [1  ###reference_b1###]. The results are summarized in Tab. 14  ###reference_###, Tab. 15  ###reference_###, and Tab. 16  ###reference_###. To minimize code change, we only evaluate the quantization of linear layers, as in many quantization works, for Mistral-7B and and Phi-2."
        },
        {
            "section_id": "7.6",
            "parent_section_id": "7",
            "section_name": "More Empirical Results on Training",
            "text": "###figure_9### To understand of how well RTN works on the training for larger models without using too much compute, we finetune a T5-Large model on the first 50K instance of XSum summarization dataset [22  ###reference_b22###] using BF16 and RTN, and show the results in Fig. 9  ###reference_###. The validation metrics are shown in Tab. 17  ###reference_###. We could draw a similar conclusion as in the main text that RTN quantized training gives similar results as BF16 training."
        }
    ]
}