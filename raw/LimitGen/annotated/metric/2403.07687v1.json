{
    "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost",
    "abstract": "Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck.\n\nIn this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. \n\nKeywords: geo-diverse datasets, active learning, effective annotations, visual similarity, vision-language models",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Vision-language models have shown remarkable advances in recent years (Li et al., 2019 ###reference_b46###; Zhang et al., 2021 ###reference_b77###; Radford et al., 2021 ###reference_b53###; Zellers et al., 2021 ###reference_b76###; Li et al., 2022 ###reference_b45###; Kirillov et al., 2023a ###reference_b41###; Huang et al., 2023b ###reference_b36###). These models have shown great performance on a variety of tasks, from lower-level tasks such as object detection, image segmentation (Kirillov et al., 2023a ###reference_b41###), and image and video classification to higher-level tasks such as image/video captioning (Li et al., 2022 ###reference_b45###; Huang et al., 2023b ###reference_b36###), text-image/video retrieval (Radford et al., 2021 ###reference_b53###), visual question answering and visual commonsense reasoning (Zellers et al., 2021 ###reference_b76###, 2022 ###reference_b75###).\n\nAt the same time, prior work has demonstrated that these models do not work well for everyone (De Vries et al., 2019 ###reference_b13###). Specifically, models do not work well on out-of-domain data, and data from low-income and non-western countries (Nwatu et al., 2023 ###reference_b49###). This is due to the imbalanced geographical and economic representation of the data used to train these models, as it comes mainly from North America and Western Europe (Shankar et al., 2017 ###reference_b65###).\n\nOne solution that Rojas et al. (2022 ###reference_b58###) and Ramaswamy et al. (2023 ###reference_b54###) propose is to collect more data from underrepresented countries. However, as Ramaswamy et al. (2023 ###reference_b54###) highlights, annotation costs are a substantial bottleneck; when crowdsourcing the data, fair pay is about 1.08$ per image without including researcher time.\n\nAs a complementary solution, we investigate strategies to reduce the annotation budget while finding effective annotation data. Specifically, our paper aims to answer two main research questions.\n\nWhich countries are less represented in the training data of vision-language models? We aim to find ways to effectively focus future annotation efforts on specific countries and their corresponding topics (objects and actions).111 Throughout the paper, for brevity, we use the term country to refer to a country or territory.\n\nOur study highlights the visual diversity of common topics across countries and those that differ the most from the primarily Western data used to train most multimodal foundation models.\n\nWe summarize our contributions as follows. First, we identify the data likely to most benefit from annotations by finding which countries and corresponding topics are less represented in the training data of vision-language models. Across 52 countries and 94 topics, we also identify the groups of countries that are visually similar in their representation of a topic and show that they can be used to supplement training data effectively. Third, our main takeaways create opportunities for affordable and geo-diverse data collection, encouraging contributions to creating datasets and models that work for everyone."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "There have been numerous studies on the use of semi-supervised models to leverage a combination of limited labeled data and vast amounts of unlabeled data to improve model performance at lower costs (Hady and Schwenker, 2013  ###reference_b31###; Oliver et al., 2018  ###reference_b50###; Taha, 2023  ###reference_b68###; Chen et al., 2022  ###reference_b10###).\nHowever, model-generated labels could be inconsistent and unrepresentative with semi-supervision, leading to reduced model performance (Ahfock and McLachlan, 2023  ###reference_b1###; Elezi et al., 2022  ###reference_b17###; Wang et al., 2021  ###reference_b73###). While similar to semi-supervised learning in objective, active learning methods seek to capture the entire data distribution by focusing labeling efforts on the data points that provide the most information for training the best-performing models (Ren et al., 2021  ###reference_b56###; Citovsky et al., 2021  ###reference_b11###; Monarch, 2021  ###reference_b48###; Yang et al., 2017  ###reference_b74###) using approaches such as uncertainty-based sampling in Gal and Ghahramani (2016  ###reference_b22###); Beluch et al. (2018  ###reference_b2###) and geometric-based methods in Sener and Savarese (2018  ###reference_b64###). Unsupervised subset selection methods like K-means and K-median core set in Har-Peled and Kushal (2005  ###reference_b33###), which form the foundation for geometric-based active learning approaches are similar to our work which seeks to select a subset that is representative of the entire dataset using distance metrics. However, the objective of the selection is to include images from a low-resource dataset with the least similarity to data of the same class in a high-resource dataset.\nThere exists a considerable body of literature evaluating the fairness and the unequal performance of vision and vision-language models on diverse groups categorized according to race (Gebru, 2020  ###reference_b26###), gender (Buolamwini and Gebru, 2018  ###reference_b4###), geolocation (Kim et al., 2021  ###reference_b40###; Shankar et al., 2017  ###reference_b65###; Goyal et al., 2022a  ###reference_b28###) and income (De Vries et al., 2019  ###reference_b13###; Nwatu et al., 2023  ###reference_b49###).\nFurther analysis of these disparities reveals that factors such as ambiguous label definitions, domain shifts, annotator disagreement (Hall et al., 2023  ###reference_b32###; Kalluri et al., 2023  ###reference_b39###), as well as image properties relating to texture, lighting, and occlusion in vision and vision-language datasets (Gustafson et al., 2023  ###reference_b30###) contribute to disparities in datasets which carry over to affect model performance.\nFrameworks have been developed to facilitate the detection of bias through guided human-in-the-loop inspection, either in datasets Hu et al. (2020  ###reference_b34###) or in models Goyal et al. (2022b  ###reference_b29###). Our work focuses on exploring the presence of variations in image representations across demographic groups in existing datasets, to inform cost-effective methods for building balanced, diverse datasets.\nEfforts toward improving equal representation in AI and equitable AI impact revolve around model adaptation, transfer learning, and dataset diversity. However, Salman et al. (2022  ###reference_b60###); Kalluri et al. (2023  ###reference_b39###); Dubey et al. (2021  ###reference_b16###); Wang and Russakovsky (2023  ###reference_b72###) suggest that transfer learning and model adaptation methods might not be enough to eradicate the issue of under-representation in AI models.\nOn the other hand, adding diverse data to training datasets tends to yield significant improvements in model performance across different groups (Ramaswamy et al., 2023  ###reference_b54###; Rojas et al., 2022  ###reference_b58###). The need for more diverse datasets has become apparent, leading to the development of datasets like GeoYFCC (Dubey et al., 2021  ###reference_b16###), GeoDE (Ramaswamy et al., 2023  ###reference_b54###), Dollar Street (Rojas et al., 2022  ###reference_b58###), and Segment Anything (Kirillov et al., 2023b  ###reference_b42###) that include data collected from diverse locations.\nWhile advantageous, diverse datasets are expensive and resource-intensive to build. (Schumann et al., 2021  ###reference_b63###; Garcia et al., 2023  ###reference_b24###; Geigle et al., 2023  ###reference_b27###) explored a less expensive alternative: revising or creating annotations for an existing dataset to improve inclusivity and reduce bias.\nSimilarly, we seek to facilitate effective but less expensive annotations by leveraging the differences between high-resource and low-resource datasets to curate the best low-resource subset for annotation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodology",
            "text": "We start by collecting two datasets that reflect the low-resource and high-resource settings. First, we compile a crowd-sourced geo-diverse dataset collected from a large number of countries, which we refer to as “low-resource data” due to the low number of images that could be collected for each country in the set and the difficulty of gathering more. Second, we also compile a web-scraped dataset used for training foundation models, which we refer to as “high-resource” due to its vast size consisting of billions of images (e.g., LAION-5B222https://laion.ai/blog/laion-5b/  ###reference_laion.ai/blog/laion-5b/###) and the ease of gathering more data.\nNext, we pre-process the data by mapping the topics between the two data sources, filtering out topics and countries with very few images.\nFinally, we utilize the collected data to generate visual representations through vision-language foundation models. These representations are then used to determine the visual similarity between images of topics in low-resource data and their corresponding topics in high-resource data.\nImageNet and ImageNet Large Scale Visual Recognition Challenge (ILSVRC) are pioneers in advancing object detection and classification progress.\nThe imagenet21k dataset (Deng et al., 2009  ###reference_b14###) contains around 21,000 WordNet (Fellbaum, 2000  ###reference_b19###) synsets and more than 14 million annotated images. We use the processed version of ImageNet21k (Ridnik et al., 2021  ###reference_b57###), with removed invalid classes and resized images. We also tried using ImageNet1k, but it did not have enough classes for our purpose, and we chose to use it to supplement the ImageNet21k data.\nLarge language-vision models such as CLIP or ALIGN have been trained on billions of image-text pairs unavailable to the public. LAION-5B (Schuhmann et al., 2022  ###reference_b61###) was created to address this problem by open-sourcing a CLIP-filtered dataset333The data is filtered using OpenAI’s CLIP ViT-L/14.\n of 5,85 billion high-quality image-text pairs.\nWe use LAION-400M (Schuhmann et al., 2021  ###reference_b62###), a subset of LAION-5B that contains 400 million English image and text pairs.\nWe pre-process and combine the low-resource datasets to increase the number of topics, images, and country diversity.\nFirst, we manually group and rename the topics from Dollar Street with the same meaning (e.g., “bathroom privacy”, “bathroom/ toilet” are renamed “bathroom”).\nNext, we rename the topics from Dollar Street that match those in GeoDE (e.g., “bike” to “bicycle”, “medication” to “medicine”).\nWe remove three topics with less than images per topic.\nFinally, we obtain a total of unique topics, images, from continents, regions, and countries.\nWe map the topics from the low-resource data to the high-resource data, ImageNet, and LAION by identifying the images with similar labels.\nFirst, we map topics from the low-resource data to an exact match to ImageNet21k or ImageNet1k. We could not find an exact match for topics because these topics are too abstract (e.g., “jewelry”, “source of cool”, “religious building”). Instead, we find mappings for their hyponyms (e.g., for “jewelry”, we map “bangle”, “necklace”, “bracelet” and “ring”). The remaining topics for which we could not find any exact or hyponym mapping to ImageNet21k or ImageNet1k are mapped to LAION.\nWe map data in LAION by selecting the images with captions that contain the topic query. Because LAION data is web-crawled, we find that the images are lower quality than ImageNet and not always relevant to the topic query: e.g., the “TV” topic in LAION contains images of people on TV, not of the object TV. Therefore, to ensure the correctness of the mapping, we manually inspect the images and map a topic to LAION only when most images are relevant to the topic query. We map topics to LAION.\nNote, however, that the number of hyponyms and the quality of LAION images limit how comprehensive the mapping process is.\nTwo independent annotators check 20 random images from each topic and find that most noisy images come from LAION. Therefore, we decide to limit the amount of data from LAION and add more images from ImageNet.\nSpecifically, we randomly sample around images per topic from LAION and around images per topic from ImageNet. Note that the high"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Low-resource Multimodal Data",
            "text": "We combine two geographically diverse datasets: GeoDE (Ramaswamy et al., 2023) and Dollar Street (Rojas et al., 2022).\n\nGeoDE.\nThe GeoDE dataset contains crowd-sourced images of objects. The data is balanced across six regions (West Asia, Africa, East Asia, South East Asia, Americas, and Europe), each with 3-4 countries. These regions were chosen due to their scarcity in most public datasets. Using a combination of heuristics and manual validation, the authors selected the objects likely to be visually distinct across the six regions.\n\nDollar Street.\nThe Dollar Street dataset contains images collected from countries on four continents (Africa, America, Asia, and Europe). The images capture everyday household objects and actions (e.g., “toothbrush”, “toilet paper”, “cooking”). The data contains unique topics, out of which we remove nineteen subjective topics following the work of De Vries et al. (2019) (e.g., “most loved item”, “things I wish I had”). All the subjective topics are found in the Appendix. The number of images for a given country ranges from in Canada to in India, with a median of images per country."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   High-resource Multimodal Data",
            "text": "As high-resource datasets, we sample data from ImageNet (Deng et al., 2009  ###reference_b14###) and LAION (Schuhmann et al., 2022  ###reference_b61###). We chose these datasets due to their popularity in vision-language models. ImageNet and ImageNet Large Scale Visual Recognition Challenge (ILSVRC) are pioneers in advancing object detection and classification progress. The imagenet21k dataset (Deng et al., 2009  ###reference_b14###) contains around 21,000 WordNet (Fellbaum, 2000  ###reference_b19###) synsets and more than 14 million annotated images. We use the processed version of ImageNet21k (Ridnik et al., 2021  ###reference_b57###), with removed invalid classes and resized images. We also tried using ImageNet1k, but it did not have enough classes for our purpose, and we chose to use it to supplement the ImageNet21k data. Large language-vision models such as CLIP or ALIGN have been trained on billions of image-text pairs unavailable to the public. LAION-5B (Schuhmann et al., 2022  ###reference_b61###) was created to address this problem by open-sourcing a dataset of 5.85 billion high-quality image-text pairs. We use LAION-400M (Schuhmann et al., 2021  ###reference_b62###), a subset of LAION-5B that contains 400 million English image and text pairs."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Data Pre-processing",
            "text": "We pre-process and combine the low-resource datasets to increase the number of topics, images, and country diversity. First, we manually group and rename the topics from Dollar Street with the same meaning (e.g., “bathroom privacy”, “bathroom/ toilet” are renamed “bathroom”). Next, we rename the topics from Dollar Street that match those in GeoDE (e.g., “bike” to “bicycle”, “medication” to “medicine”). We remove three topics with less than images per topic. Finally, we obtain a total of unique topics, images, from continents, regions, and countries.\n\nWe map the topics from the low-resource data to the high-resource data, ImageNet, and LAION by identifying the images with similar labels. First, we map topics from the low-resource data to an exact match to ImageNet21k or ImageNet1k. We could not find an exact match for topics because these topics are too abstract (e.g., “jewelry”, “source of cool”, “religious building”). Instead, we find mappings for their hyponyms (e.g., for “jewelry”, we map “bangle”, “necklace”, “bracelet” and “ring”). The remaining topics for which we could not find any exact or hyponym mapping to ImageNet21k or ImageNet1k are mapped to LAION.\n\nWe map data in LAION by selecting the images with captions that contain the topic query. Because LAION data is web-crawled, we find that the images are lower quality than ImageNet and not always relevant to the topic query: e.g., the “TV” topic in LAION contains images of people on TV, not of the object TV. Therefore, to ensure the correctness of the mapping, we manually inspect the images and map a topic to LAION only when most images are relevant to the topic query. We map topics to LAION.\n\nNote, however, that the number of hyponyms and the quality of LAION images limit how comprehensive the mapping process is. Two independent annotators check 20 random images from each topic and find that most noisy images come from LAION. Therefore, we decide to limit the amount of data from LAION and add more images from ImageNet. Specifically, we randomly sample around images per topic from LAION and around images per topic from ImageNet. Note that the high-resource data does not contain country information.\n\nWe show the data before and after pre-processing and the topic mapping in our repository. ###figure_3### The low-resource data is unbalanced, as the total number of images per country varies from 6,549 for Japan to 1 for Bulgaria and Venezuela, with a median of 345 images per country. The number of images per topic is also unbalanced, from 3,049 for “waste container” to 18 for “hanging clothes to dry”. However, balancing the data by down-sampling significantly reduces the number of countries represented for each topic. Having numerous countries represented is essential for our setup. Therefore, we choose not to balance the data.\n\nInstead, we remove the (topic, country) pairs containing less than images, considering this threshold a minimum for experiment significance. This also removes considerable data: 3,329/4,830 (topic, country) tuple pairs, 5/99 topics, and 31/83 countries. We show the removed topics and corresponding countries in our repository and highlight the need for more data for these pairs to obtain significant results. We show the statistics after the data collection and pre-processing in Table 1 and the image distribution of countries per topic in Appendix Figure 10."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Data Representation",
            "text": "We use an ensemble of three representations to compute image similarity and to ensure the results generalize across representation types. We choose CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and BLIP-2 (Li et al., 2023) due to their popularity as foundation models, i.e., their use in a multitude of models and their high zero-shot performance across various tasks and datasets, such as text-to-image retrieval, image question answering, human action segmentation, image-sentence alignment, image captioning.\n\nWe use the pre-trained Vision Transformer ViT-B/32 from the CLIP model to encode the visual representations of the images. The training dataset for CLIP was created from the results of numerous queries to various publicly available Internet sources. The dataset referred to as WebImageText WIT contains 400 million (image, text) pairs and is not available to the public.\n\nWe also extract image features following the ALIGN model setup, using a pre-trained EfficientNet as a vision encoder. Since the original code has not been released, our implementation is based on the Kakao Brain code that reproduced the original paper. ALIGN was trained on 1.8 billion image-text pairs collected following the methodology used for the Conceptual Captions dataset. Since the emphasis was on scale instead of quality, the dataset underwent fewer post-processing steps, thus leading to a noisier dataset. This dataset is currently unavailable for public access.\n\nWe also extract image features using BLIP-2, which uses ViT-g/14 from EVA-CLIP as image encoder and removes the second last layer’s output features to increase performance. BLIP-2 was trained on a total of 129M images aggregated from the COCO, Visual Genome, CC3M, CC12M, SBU, and the LAION400M datasets. Captions for the web images were generated using CapFilt."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Mapping the Representation of Vision-Language Models",
            "text": "In this section, we address the first research question: RQ1: Which countries are less represented in the training data of vision-language models?\n\nWe select the (topic, country) pairs that are consistently different from the high-resource data across the three visual representation types: CLIP, BLIP, and ALIGN. \n\nFinally, the (topic, country) pairs selected for all three visual representations are the ones we find to be consistently different from the high-resource data and, thus, the ones that benefit the most from annotations. We find 422 such (topic, country) pairs out of 1,501 unique (topic, country) pairs, potentially reducing the annotation budget to less than a third of the initial amount. We share the results in our repository.\n\nWe show in Figure 3 the data for the CLIP representation and highlight the (topic, country) pairs we find to benefit the most from annotations based on consistently low similarity with the high-resource data across the three visual representations. \n\nFrom Figure 3, we can also see that the countries with the fewest data are usually the ones with the most topics in need of annotations. Exceptions to this are countries that have more data points (topics), but more than half of the topics require annotations, and countries that have very few topics and none require annotations.\n\nIn Figure 3, we see a few topics that are marked to require annotations: “medicine”, “spice”, “ceiling”, “clothes” and “makeup”. We show in Appendix Figure 11 representative images from these topics from the two data sources, which explain the visual differences.\n\nFor the rest of the topics, as expected, the data is similar to the high-resource data. \n\nWe considered using the data as the high-resource data source. However, due to the lack of data on some topics and relatively few images per topic compared to other countries, it was not feasible.\n\nThere are differences between the results obtained with each visual representation type regarding the intervals and which (topic, country) pairs are similar to the high-resource data. However, the general trend is consistent as most (topic, country) pairs have only low or high scores across all three representations. This is also supported by the strong Pearson correlations between the scores obtained with the three representation types: CLIP and BLIP scores correlate, CLIP and ALIGN scores correlate, ALIGN and BLIP scores correlate.\n\nWe show in the Appendix Figure 12, 13, and 14, the results for each representation type: CLIP, ALIGN, and BLIP respectively.\n\nTo show how the topic visual representations vary per low-resource and high-resource data, we perform a 2D transformation using Principal Component Analysis (PCA).\n\nIn Figure 4, we show the CLIP average representations per country in the low-resource and the corresponding high-resource data for the topic “toothbrush”. We can observe that, for this topic, there is considerable visual diversity across countries. When comparing to the high-resource data, we observe visually different countries, as well as countries that are very visually similar.\n\nIn addition, we observe many countries that tend to be clustered together, i.e., visually similar for this particular topic. \n\nWe examine more about the similarities between countries when answering RQ2 in the following section. \n\nIn Appendix Figure 15, 16, 17 we show results for other topics (“hand washing”, “toilet”, “wall”) in low-resource and high-resource data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Cross-country Data Similarity for Improved Model Representation",
            "text": "We now turn to the second research question RQ2: How can we leverage cross-country data similarity to improve the representation of vision-language models?\n\n###figure_6### To show how the topic visual representations vary per country in the low-resource data, we perform a 2D transformation using Principal Component Analysis (PCA) (F.R.S., 1901  ###reference_b21###).\nIn Figure 6  ###reference_###, we show the CLIP average representations per country for the topics with the most and least visual differences across countries: “religious building” and “hat”, respectively. As expected, the representations for “religious building” are much more spread across countries than those for “hat”, which tend to cluster together.\nIn Appendix Figure 20  ###reference_###, 21  ###reference_###, and 22  ###reference_###, we show representations for other topics visually different across countries: “get water”, “house” and “backyard.”\n\n###figure_7### We measure if the visual similarity between countries correlates with the geographical distance. The geographical distance between two countries is calculated using Vincenty’s distance Vincenty (1975  ###reference_b70###) between their capital cities.777https://github.com/rahulbot/distances-between-countries  ###reference_een-countries###\nThe visual similarity between any two countries is calculated across all their shared topics.\nWe compute the Pearson correlation coefficient Freedman et al. (2007  ###reference_b20###) over all countries and obtain a value of , indicating a weak negative correlation. A strong negative correlation is initially more expected as, intuitively, their visual similarity should increase as the distance between countries decreases.\nHowever, when we break down the correlation at the country level, the correlation coefficient varies significantly per country.\n###figure_8### In\nFigure 7  ###reference_###, we show countries with weak to moderate positive correlations (e.g.,  with ,  with ), countries with weak to moderate negative correlations (e.g.,  with ,  with ), most countries have values close to , indicating no correlation between visual similarity and geographical distance.\nUpon close examination of the results, we determine the reasons behind this result: countries with positive correlation are often visually similar to countries from different continents (e.g.,  is more similar to  with an average similarity  and distance  than to  with\nwith an average similarity  and distance ). We hypothesize this might be due to history, climate, and/or income differences, which could contribute more to visual similarity than distance alone.\nOur analysis shows that geographical location does not generally correlate with visual similarity.\nTherefore, collecting globally diverse annotations on a budget requires considering other complementary information, such as the country’s income, culture, history, and climate. Our results on which countries are similar to each other provide valuable insights into how to distribute the annotation budget effectively and can be used along with this complementary information.\nWe train a classifier to predict the topic of the input images and measure the accuracy while controlling for the countries.\nSpecifically, we input the CLIP visual representation in a linear layer, followed by a softmax to predict the topics of the input images.888We set the learning rate as 5e-3, use AdamW as the optimizer, and conduct training over 250 epochs with a batch size of 512. Additionally, we use a cosine annealing schedule with 50 warm-up epochs.\nWe select one random country for each topic from the low-resource data, which we call target (topic, country) pairs.\nNext, we split the data into training and test sets in a 90-10% data split to include all the target (topic, country) pairs in both sets.\nFinally, we replace different ratios (100%, 90%, 70%, 50%, 30%, 10%, 0%) of the target-country data with images from: (1) the most similar countries to the target-country given the target-topic; (2) the most dissimilar countries to the target-country given the target-topic; (3) high-resource data corresponding to the target-topic.\nThe topic classification accuracy when using all the training target-country data is , which is an upper bound. In Figure 8  ###reference_###, we show the accuracy when adding data from (1), (2) and (3).\nThe main takeaway is that adding data from similar countries improves the performance more than adding data from dissimilar countries or high-resource data, and the gap in performance increases with the replacement ratio. Additionally, supplementing with high-resource data is generally more beneficial than supplementing with data from dissimilar countries.\nWe also compute the accuracy when no data is added, and find that adding data from dissimilar countries or from high-resource data can hurt the performance compared to not adding data, especially for high replacement ratios\n(). We show the results in the"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Main Takeaways",
            "text": "Our analyses provide multiple insights into the current state of vision-language annotations for various topics across different countries, and show the coverage limitations of existing large-scale datasets. We highlight the main takeaways and propose actionable steps to help future work create more inclusive datasets and models.\nTo have more inclusive models and datasets, we need to collect more globally diverse annotations. Because annotations are expensive, we propose to focus future annotation efforts on specific countries and their topics.\nTo assist with these efforts, we provide a list of countries and corresponding topics that are consistently unrepresented in the training data of vision-language models.\nFurthermore, most countries have less than ten images per topic.\nFor most countries and corresponding topics – 3,329/ 4,830, we could not determine how similar they are to the high-resource data because of the lack of data. These countries have less than ten images per topic and, therefore, already need annotations.\nAs an alternative solution, we recommend developing algorithms that can perform well with limited amount of data.\nWhen we do not have a sufficient budget to annotate more data for a target country and topic, we propose using the available data from countries with similar visual representations of that given topic. We provide a list of similar countries for each target country and topic and show that using this data improves model performance more than using data from dissimilar countries or high-resource data.\nWe compute the Pearson correlation coefficient between the visual similarity and the geographical distance between all countries and find a very weak negative correlation of -0.01. Therefore, collecting globally diverse annotations requires considering additional information.\nMultiple other factors, such as income, history, or cultural heritage, can contribute to the visual similarity between countries. We find this hypothesis worth investigating in depth in future work.\nWhile examining images of topics across countries, we notice visually similar topics with very different backgrounds, which influence the visual similarity score. For example, in  Figure 9  ###reference_###, many countries have the same type of toothbrush, but because their storage place is different, their visual similarity score is low. In this paper, we measure similarity at the context level, considering both the topic and the context (e.g., background, storage space). However, as future work, we propose to investigate further which type of similarity to consider when we annotate diverse data: either at the topic level, by extracting the segmentation mask of the topic, or at the context level, by considering the entire image.\n###figure_10###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In this paper, we addressed the need for balanced data representation used to train vision-language models. Because data annotations are expensive, we proposed to annotate primarily images from unrepresented countries. To find which countries are less represented in the training data of vision-language models, we compared the visual similarity of images across 94 topics and 52 countries found in crowd-sourced and web-scraped data. We used three visual representations, CLIP, BLIP-2, and ALIGN, to ensure the results generalize across representation types.\n\nAdditionally, we proposed to leverage cross-country data similarity to improve model performance. We found visually similar countries for each country and corresponding topics and made them available in our repository:\nhttps://github.com/MichiganNLP/visual_diversity_budget.\n\nFinally, our analysis offers multiple takeaways for future work to make informed decisions on what global data to annotate and how to leverage cross-country data similarity to improve model representation. Through our work, we hope to contribute to building more inclusive and affordable vision-language models and datasets to help democratize AI globally."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ]
}