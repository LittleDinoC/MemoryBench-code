{
    "title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles",
    "abstract": "In this paper, we outline our submission for the SemEval-2024 Task 9 competition: ’BRAINTEASER: A Novel Task Defying Common Sense’. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In Natural Language Processing (NLP), reasoning serves as the cognitive backbone, enabling systems to transcend mere language comprehension and delve into sophisticated understanding. Despite the excellence of Large Language Models (LLMs) in several linguistic tasks, their reasoning capabilities are still questionable to a non-negligible extent Floridi and Chiriatti (2020); Bender et al. (2021); Kauf et al. (2022); Zhang et al. (2023); Shi et al. (2023); Tyen et al. (2024); Giadikiaroglou et al. (2024), often posing the fundamental concerns of whether they can indeed reason or memorize exhaustively Yuan et al. (2022).\n\nSuch limitations can be probed via well-crafted datasets and benchmarks, showcasing varying LLM deficiencies at a time. As the core of the current paper, BrainTeaser Jiang et al. (2023b, 2024b) incorporates problems that stress models to think \"out-of-the-box\"; to this end, the key novelty of BrainTeaser is that in order to answer correctly, models need to defy default senses of concepts and common associations.\n\nAssuming that large-scale training and prompting may not always serve as universally applicable solutions towards flexible reasoning, we move one step back and leverage transfer learning techniques starting from smaller models based on masked language modelling, such as BERT Devlin et al. (2019) and consequent BERT-based encoders. Then, we proceed with similar techniques on LLMs, aiming to showcase that significant performance advancements using a small set of in-domain data for parameter updating can be achieved in comparison to merely querying the model’s prior knowledge via prompting. Therefore, our contributions are:\n\nWe perform lightweight tuning on smaller encoder models and LLMs. We transform the multiple-choice problem to a binary classification one, aiming to explore diverging reasoning paths for models. We ground final performance on the models’ \"prior knowledge\" in related problems. We delve into models’ frequent failures to obtain a deeper understanding of reasoning cues that make models struggle the most.\n\nOur code is available on GitHub 111https://github.com/GiannisPana/AILS-NTUA-at-SemEval-2024-Task-9-Brainteaser."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "has enjoyed several advancements due to the surge of pre-trained language models and especially LLMs Sun et al. (2023  ###reference_b31###). Reasoning challenges incorporate commonsense reasoning Richardson and Heck (2023  ###reference_b26###), involving inference regarding everyday situations, mathematical reasoning Lu et al. (2023  ###reference_b22###), referring to the ability of solving mathematical problems, logical reasoning Yang et al. (2023  ###reference_b41###), which includes the systematic deduction of conclusions based on established principles and formal rules, causal reasoning Gendron et al. (2024  ###reference_b5###), which studies cause-and-effect relationships explaining why an event leads to another, and several other sub-tasks Vashishtha et al. (2020  ###reference_b36###); Wei et al. (2023  ###reference_b40###); Petersen and van der Plas (2023  ###reference_b25###).\nIn terms of reasoning evaluation, BigBench Srivastava et al. (2023  ###reference_b30###) comprises 204 reasoning tasks, targeting to explore the related capabilities of recent LLMs.\nSeveral dedicated datasets have been developed to tackle different reasoning challenges, including commonsenseQA Talmor et al. (2019  ###reference_b32###), WinoGrande Sakaguchi et al. (2019  ###reference_b27###), RiddleSense Lin et al. (2021  ###reference_b20###) and others; most of these datasets are incorporated in Tasksource Sileo (2023  ###reference_b29###). Especially RiddleSense questions aspects of reasoning close to BrainTeaser Jiang et al. (2023b  ###reference_b16###, 2024b  ###reference_b15###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Task and Dataset Description",
            "text": "The BrainTeaser task at SemEval-2024 (Jiang et al., 2023b, 2024b) features lateral thinking puzzles presented as multiple-choice questions (QAs). Each question offers four options, with one being the correct answer and the others serving as distractors. Additionally, the final option is always \"None of above\". It consists of two sub-tasks, Task A: Sentence Puzzle and Task B: Word Puzzle. In addition to the original puzzles, the dataset includes adversarial subsets created by manually modifying the original brain teasers while preserving their reasoning paths. The original data were perturbed in two ways: First, there is semantic reconstruction of each original question without altering the answers or the distractors. Second, the original data underwent context reconstruction, wherein the original reasoning path remains intact, but the brain teaser describes a new situational context. Overall, the dataset used for training and evaluation consists of triplets of data: original, semantic, and context reconstruction. Table 1 provides an example of the triplets of data that constitute the dataset.\n\nIn this sub-task, the sentence pairs are crafted in a manner that makes it relatively easy for humans to discern the correct statement, yet challenging for systems, even those equipped with commonsense understanding. Table 2 contains examples of the Sentence Puzzle dataset (on the left). The training data consists of 169 distinct multiple-choice QA sets, each accompanied by its semantic and context reconstructions, resulting in a total of 507 multiple-choice questions. This involves word-type brain teasers, where the answer defies the default meaning of the word and focuses on the letter composition of the question. The training dataset comprises 132 multiple-choice QAs, each accompanied by its semantic and context reconstructions, resulting in a total of 396 multiple-choice QAs. These brain teaser categories include puns, homophones, ambiguous words, and various other linguistic puzzles, as showcased in the examples provided in Table 2 on the right-hand side.\n\nThe Word Puzzle sub-task poses challenges not only for systems but also for humans in discerning the correct answer. The BrainTeaser dataset comprises 3 data splits, namely train, development (used during the practice phase), and the hidden test set, which was used for evaluation. Statistics are provided in Table 3. Throughout the evaluation phase, the leaderboard was kept concealed."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We focus on tuning language models belonging into two categories.\nFirst, we fine-tune variations of encoder models, namely BERT Devlin et al. (2019  ###reference_b3###), RoBERTa-large Liu et al. (2019  ###reference_b21###) and DeBERTaV3-base He et al. (2023  ###reference_b10###), to assess the impact of transfer learning using various datasets requiring similar reasoning abilities, apart from BrainTeaser. We study the problem using the provided multi-choice setup, but we also transform it into a binary classification task. Secondly, the encoders’ results are compared with those obtained from fine-tuned LLMs using the BrainTeaser dataset. To achieve this, we fine-tune Llama 2 Touvron et al. (2023b  ###reference_b34###), Phi-2 Gunasekar et al. (2023  ###reference_b9###) and Mistral-7b Jiang et al. (2024a  ###reference_b14###), which have already demonstrated enhanced reasoning abilities. In this regard, we examine the effect of the model size on our task, which has already been reported in the literature to significantly influence the reasoning abilities of the models Touvron et al. (2023b  ###reference_b34###); Wei et al. (2022  ###reference_b39###), along with other tuning hyperparameters. Model details are presented in App. A  ###reference_###.\nFirst, we evaluate the effects of the pre-training on our task. Thus, we select two variations of each encoder: the vanilla one (using the default pre-trained basis and fine-tuned on BrainTeaser data only) and one that has undergone additional pre-training using supplementary commonsense reasoning datasets before fine-tuned on BrainTeaser. In the second case, we use the following pre-trained models:  1 BERT-SE: a BERT-base-uncased version pre-trained on the multiple-choice dataset used in SemEval-2020 Task 4b Wang et al. (2020  ###reference_b38###) 2 RoBERTa-WNGRD: a RoBERTa-large version pre-trained on the WinoGrande dataset, and  3 DeBERTaV3-TS: a DeBERTaV3-base model, pre-trained on diverse commonsense reasoning datasets, and fine-tuned with multi-task learning on over 600 tasks from the Tasksource collection.\nThis strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem.\nEach sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Encoder models",
            "text": "First, we evaluate the effects of the pre-training on our task. Thus, we select two variations of each encoder: the vanilla one (using the default pre-trained basis and fine-tuned on BrainTeaser data only) and one that has undergone additional pre-training using supplementary commonsense reasoning datasets before fine-tuned on BrainTeaser. In the second case, we use the following pre-trained models:  1 BERT-SE: a BERT-base-uncased version pre-trained on the multiple-choice dataset used in SemEval-2020 Task 4b Wang et al. (2020  ###reference_b38###  ###reference_b38###) 2 RoBERTa-WNGRD: a RoBERTa-large version pre-trained on the WinoGrande dataset, and  3 DeBERTaV3-TS: a DeBERTaV3-base model, pre-trained on diverse commonsense reasoning datasets, and fine-tuned with multi-task learning on over 600 tasks from the Tasksource collection.\nThis strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem.\nEach sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLMs",
            "text": "We demonstrate an in-depth examination of fine-tuning SoTa LLMs (Llama 2, Phi-2, and Mistral-7b) in the context of multi-class classification. Note that during inference, the models prompted to provide an explanation along with the label. This experimental step, which we have observed to improve the performance of the model, also provides a qualitative identification of flaws in the models’ reasoning process. In our experiments, we explore various combinations of LoRA Hu et al. (2021  ###reference_b12###)  and  hyperparameters, using values of 16, 32, 64, and 128. For the analysis ahead, LLMs are denoted as model_r_a, reflecting these hyperparameters.\nAdditional technical information, including prompting details and specifics about QLoRA hyperparameters, is available in App. B  ###reference_###, C  ###reference_###, D  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "Interestingly, the performance of the binary classification problem is significantly lower than that of the multi-class classification task. Initially, this behavior seemed counterintuitive since it appeared easier to determine whether a question is correct or not than to select the correct answer from four different options. However, this assumption is not accurate. Consider the word riddle: ‘What is the capital in France?\" At first glance, the option ‘F’ seems incorrect, but when considering the options ‘F,’ ‘E’, ‘A’, and ‘None of the above’, ‘F’ emerges as the only correct answer, as it becomes apparent that the question refers to the capital letter rather than the capital city. Therefore, the diverse options provide crucial context to the models, explaining the superior performance of multi-class models. This lack of context is why we refrain from further exploring this methodology across all models in our study.\n\nThere are notable discrepancies between original and semantic contexts when compared to context reconstruction, particularly evident in the case of smaller encoder models.\n\nRegarding encoders, it is evident that, especially vanilla RoBERTa-large lacks robust commonsense reasoning and struggles to systematically handle ambiguity; in contrast, RoBERTa-large pre-trained on WinoGrande presents competitive performance. This notable enhancement (over 40%) due to WinoGrande pre-training suggests that this particular dataset effectively equips the model with the ability to understand word puzzle-related reasoning complexities, making its performance competitive despite the higher baseline reasoning benchmarks. \n\nOther than that, pre-training on other commonsense reasoning datasets does not significantly improve the overall performance for encoders. Conclusively, apart from WinoGrande, the rest of the extra pre-training datasets do not hold reasoning cues close to BrainTeaser’s word puzzles.\n\nRegarding LLMs, Mistral-7b notably outperformed all others by a significant margin, even surpassing the 8 times larger model tuned using the same hyperparameters (Mixtral-8x7b). Llama 2 exhibited the worst results regardless of size (7/13 billion) and LoRA hyperparameters (r and a). Conversely, Phi-2 demonstrated relatively better performance, particularly considering its smaller parameter count (2.7 billion) compared to the other LLMs. However, both models performed worse compared to most fine-tuned encoders. This observation strongly confirms that word puzzles possess a distribution that diverges from the analytical commonsense reasoning required for sentence puzzles, entailing a unique set of cognitive demands.\n\nMistral-7b exhibits a trend where higher quality explanations were generated with higher values of lora rank r. However, the top-performing model showcased a configuration with r=16 and a=64. The QLoRA method explains why our top model has a rank of 16 instead of 128, contrary to common expectations. Drawing from the widespread presence of low-rank structures, we leverage the intrinsic low-rank structure in our problem. It is well-established that many tasks, particularly involving heavily over-parametrized models, exhibit low-rank properties post-training.\n\nOverall, our systems demonstrate proficiency in understanding and detecting wordplay patterns, consistently addressing ambiguity irrespective of contextual and semantic variations in brain teasers. Upon reviewing the short explanations provided with each prediction, we note thorough justifications even for incorrect answers. Errors typically adhere to specific wordplay patterns across original, semantic, and context multiple-choice questions."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we systematically evaluate pre-trained and fine-tuned encoders, along with instruction-tuned Large Language Models (LLMs), against two multi-class classification sub-tasks within the \"BRAINTEASER: A Novel Task Defying Common Sense\". We gain insights regarding the influence of leveraging in-domain data, the variability model scale and architecture introduce, as well as the examination of diverging reasoning paths. As future work, we will delve into further reasoning patterns LLMs tend to follow with regard to lateral thinking challenges."
        }
    ]
}