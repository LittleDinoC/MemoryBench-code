{
    "title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech",
    "abstract": "The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments suggest that Multi-output models perform comparably to individual models, efficiently capturing the intricate relationships between variables and speech inputs, all while achieving improved runtime.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In our increasingly digital world, the ability to glean profound insights from the nuances of human voices has assumed paramount importance. This paper delves into the captivating domain of predicting age, gender, and emotion based on vocal cues, a multidisciplinary field teeming with far-reaching applications.\n\nVoice analysis technologies have rapidly advanced, bringing transformative breakthroughs to various fields. These advancements not only optimize customer interactions but also hold the potential to revolutionize healthcare diagnostics, fundamentally altering our comprehension and engagement with human communication. In mental healthcare, the ability to discern emotions offers an opportunity to improve emotional and behavioral disorders. Moreover, the application of this technology extends to the retail sector, where it enhances the consumer experience. Simultaneously, the ability to detect age and gender variables finds relevance in evaluating the mental health requirements of distinct demographic groups. It also plays a crucial role in e-services and policy formulation. Thus, the ability to discern age, gender, and emotion from voice data emerges as a multifaceted field with a plethora of discernible applications."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Prior art",
            "text": "This research endeavour embarks on a comprehensive exploration of advanced deep learning architectures tailored for the prediction of age, gender, and emotional states. It undertakes an exhaustive comparative analysis, meticulously examining diverse methodologies. These methodologies include individual models, where a single model is used to predict a single variable; multi-output models, which employ a single model to predict all three variables simultaneously; and sequential models, which cascade individual models to create a sequence representing the three variables at distinct stages. This scrutiny aims to elucidate the effectiveness of these approaches in the vital task of detecting these three pivotal variables.\n\nBefore beginning the experiments, addressing the critical challenge of sourcing a dataset that encompasses all three target labels was necessary. Consequently, a rigorous review of popular, openly available speech datasets was undertaken to test the models. Datasets like RAVDESS and IEMOCAP were found to have emotion labels but lacked age and gender annotations. The TESS dataset featured only two speakers, rendering it unsuitable for our purposes despite including all three variables. Similarly, the DES dataset comprised just four speakers, limiting its utility. The Common Voice dataset had age and gender labels but omitted emotion labels. As a result, the CREMA-D and EMO-DB datasets were identified as the only suitable sources containing all three labels and adequate data. An aggregate dataset was created by amalgamating these two datasets to facilitate our experiments.\n\nPrior research has explored predicting individual variables using dedicated models. Notably, limited research has been conducted on predicting all three variables—namely, age, emotion, and gender—from speech data. While a \"one-source-to-detect-all\" solution was proposed for predicting age, gender, and emotion from one model, methodological flaws were identified in their approach. Specifically, these studies utilized three separate datasets to train three distinct tasks, a strategy that may not be optimal for enabling a model to discern all three variables from a single speech source.\n\nThe motivation behind employing multi-output models lies in their ability to concurrently specify the relationships between multiple outcome variables (Y) and feature variables (X). This approach acknowledges the potential interdependencies between these outcome variables. Variations in dependency structures within the covariates were found to influence the performance of predictive methods consistently for both univariate and multivariate approaches.\n\nGiven these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.\n\nThe rest of the article is structured as follows: Section 2 details the methodology of various models used in this experiment, discusses the database and explains the feature extraction along with our novel SEGAA model. Section 3 elaborates on the results based on the experimental study. Section 4 presents the conclusion of this article."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\n\nThe optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which predictions for gender, emotion, and age predictions are continuously monitored.\n\nThe network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\n\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\n\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\n\nThe devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32.\n\nIn this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Description",
            "text": "Given these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.  \nThe EMO-DB database, a publicly accessible German emotional database, was developed by the Institute of Communication Science at the Technical University in Berlin, Germany. The data collection involved ten proficient speakers, equally distributed between genders (five males and five females). The database comprises 535 speech utterances covering seven distinct emotions: anger, boredom, anxiety, happiness, sadness, disgust, and neutrality.  \nAlong with the EMO-DB dataset, the CREMA-D dataset was also used. CREMA-D consists of 7,442 authentic speech clips from a diverse group of 91 actors. The actors include 48 males and 43 females, with ages ranging from 20 to 74. They represent diverse racial and ethnic backgrounds, including African American, Asian, Caucasian, Hispanic, and Unspecified. During the recording process, the actors delivered a set of 12 distinct sentences, each expressed with one of six specific emotions: Anger, Disgust, Fear, Happiness, Neutrality, and Sadness. The study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class.  \nThe primary goal with respect to the data augmentation part of this study is to enhance the diversity and quality of the dataset to improve the performance of emotion recognition models. The age and gender of the speakers were extracted from the speaker information provided and appended to the corresponding data sample. Consequently, our comprehensive dataset comprised a fusion of the EMO-DB and CREMA-D datasets.  \nThe study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class. Simultaneously, we omitted data samples with the label \"boredom\" as we narrowed our scope to include the following emotions: Anger, Disgust, Fear, Happiness, Neutrality, and Sadness."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Feature Extraction / Pre-processing",
            "text": "We implemented audio processing functions to augment the original data samples. These functions include noise addition, time stretching, pitch shifting, and signal shifting. The noise addition process introduces random noise to the audio data, while time stretching alters the duration of the samples. Pitch shifting, on the other hand, modifies the audio pitch, and signal shifting shifts the audio signal along the time axis.\n\nOur feature extraction pipeline involves calculating essential audio features such as the Zero-Crossing Rate (ZCR), Root Mean Square Energy (RMSE), and Mel-frequency cepstral coefficients (MFCC). These features are widely used in audio and speech processing to capture relevant characteristics of the audio signals.\n\nThe augmentation techniques were applied to the original audio samples to construct the final dataset. Audio features were extracted for each sample, including the original data and its augmented versions. This process enabled the generation of diverse features, encompassing pitch, duration, and signal properties variations. The resulting dataset showcases a significant increase in size and diversity.\n\nWe split the dataset into input features (X) and target labels (Y). Y was one-hot encoded to represent the categorical variables. The dataset was then divided into training, testing, and validation sets, using a stratified approach to ensure class distribution balance. Input features were standardized using the Standard Scaler to enhance model convergence and performance. To maintain consistency, the testing and validation sets were transformed using the scaler fitted to the training set. This is then used to train and test the MLP and the SEGAA models, which finally output the age, gender and emotion results as described in Figure 1. The results of the respective models will then be compared and inferred from.\n\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers. Sigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32.\n\nThe network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling. After these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%.\n\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons. This model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization.\n\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons. The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum.\n\nIn this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade. Following these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three"
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Individual Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers. Sigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. \n\n###figure_10### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. \n\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_11###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Multi-output Models",
            "text": "The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32.\n\nIn this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and dropout at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\n\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for 'emotion' and 'age' predictions and binary softmax activation for 'gender' predictions. The hyperparameters governing the model’s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\n\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout. Three separate output layers cater to each label category: 'emotion,' 'age,' and 'gender,' utilizing softmax activation for the former two and binary softmax for the latter.\n\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence."
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Sequential Models",
            "text": "In this experimental setup, the individual models described in Section 3.3.1 were cascaded sequentially to predict the first variable from the speech features, followed by predicting the second variable from the speech features and the previously predicted first variable, and finally predicting the third variable from the speech features and the previously predicted second variable as described in figure 5. This experiment was conducted in three different sequences: first for the variables emotion, gender, and age; then for gender, age, and emotion; and lastly for age, emotion, and gender."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "This research investigates the task of speech-based detection of emotion, gender, and age using a diverse range of machine-learning models. Our primary objective is to develop robust models for classifying these three attributes from audio data. The experimental framework includes individual and multi-output Multi-Layer Perceptron (MLP) as well as SEGAA architectures. Additionally, we investigate unique sequences for predicting these attributes with our SEGAA models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Individual Models",
            "text": ""
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Emotion Detection",
            "text": "For emotion detection, both individual SEGAA and MLP models exhibit robust performance. These models demonstrate well-balanced precision, recall, and F1 scores, ranging from 0.94 to 0.96."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Gender Detection",
            "text": "The individual models demonstrate exceptional performance for gender detection. The SEGAA model achieves outstanding results, while the MLP model also performs at an impressive level. Precision, recall, and F1 scores consistently reach 1.00 for both models."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Age Detection",
            "text": "Age detection benefits from the individual SEGAA and MLP models. These models display high precision, recall, and F1 scores, ranging from 0.92 to 0.95."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multi-output Models",
            "text": "Our investigation extends to multi-output SEGAA and MLP models, which tries to simultaneously predict emotion, gender, and age in that order. These models yield competitive results with those of the individual models. Although the multi-output SEGAA Gen-0 model maintains strong precision and F1 scores, there is a slight decrease in recall for certain attributes, particularly in emotion detection. In contrast, the proposed SEGAA model effectively addresses these limitations, demonstrating exceptional performance across all aspects, encompassing emotion, age, and gender detection, which is highlighted in Figure 7."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Sequential Models",
            "text": "Exploration of various sequences for predicting emotion, gender, and age using SEGAA models reveals distinct strengths across different orderings. The EGA sequence (Emotion → Gender → Age) notably excels in gender detection. This sequence has consistent F1 scores above 0.90 for all three attributes, which talks extensively about the sequential model metrics.\n\nThe performance metrics of individual and multi-output models, providing a detailed assessment of precision, recall, and F1 scores for age, emotion, and gender detection. Both SEGAA and MLP models showcase high performance, with the SEGAA model demonstrating gender detection at 100%, with more information on the models highlighted in Figure 6. Notably, the multi-output models, particularly the SEGAA model, exhibit competitive results across all attributes. While the Gen-0 displays slightly lower scores in emotion detection, SEGAA addresses this limitation effectively as the Nadam optimizer present in SEGAA uses Nesterov’s Accelerated Gradient as an extension to the Adam optimizer used in Gen-0. The multi-output MLP model also maintains high balanced metrics, emphasizing the performance of this model category in simultaneous prediction tasks.\n\nWhile all models consistently deliver competitive performance, it is noteworthy that the proposed model (Multi-output SEGAA) emerges as the most efficient choice. These models offer commendable performance while preserving efficiency, an important consideration in real-world applications where computational resources and low latency are necessary. Predicting gender through audio has been a pretty easy task for the individual models. This can be seen in Figure 7a and 7b, where there are barely any misclassifications, and a high number of true positives for each of the categories. The same can be said for the task of emotion prediction for these models, as there aren’t many misclassifications, compared to the number of right predictions. However, they struggle a bit more when it comes to identifying \"neutral\" and \"anxiety/fear\" emotions. The models also excel at predicting the age, but it’s important to note that the dataset is imbalanced, as there are fewer examples of people in their \"sixties\" and \"seventies\" in the dataset compared to other age groups.\n\nComparing the results for Individual SEGAA and Multi-output SEGAA shows that the model itself is stable, since there is relatively little to no change in the classifications made by the two models. The Multi-output SEGAA is comparable to the Individual SEGAA in terms of performance. Individual SEGAA outperforms the Multi-output SEGAA models marginally in all the tasks, as shown in Figure 8a and 8b, but the training time taken by the multi-output SEGAA outweighs this merit, as they are already extremely close in performance.\n\nThe performance of Emotion → Gender → Age sequence showcased in Figure 9a and 9b is better than the rest, but it still isn’t as good as the individual or multi-output SEGAA models, which is because of the sum of errors due to error from one model propagating to the next, which adds up. This is apparent in the confusion matrices, where the model hasn’t been able to classify the age of seventies as well as the SEGAA model, along with the other classes."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In our experimentation, we conducted a rigorous comparative assessment of univariate and multi-output models to predict gender, age, and emotion from speech data, a crucial task with applications spanning various domains. Our analysis unveiled a noteworthy phenomenon associated with sequentially chaining univariate models: this approach increased error propagation as the inaccuracies generated by preceding models were amplified in subsequent stages. It is essential to emphasize that our experimental findings suggest that SEGAA demonstrates a level of predictive capability comparable to univariate models. Notably, it excels in capturing the intricate interrelationships between the variables and speech inputs with commendable efficiency. Furthermore, SEGAA achieves this without compromising runtime efficiency, making them an attractive alternative for addressing the complexities of predicting gender, age, and emotion from speech data. These insights, we believe, hold valuable implications for both researchers and practitioners in the field."
        }
    ]
}