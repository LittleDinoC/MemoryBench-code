{
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call “neural phishing”. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data.\n\nOur attack assumes only that an adversary can insert as few as benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) (Brown et al., 2020) pretrained on large amounts of webscraped data have achieved impressive performance on many tasks OpenAI (2023b); Team et al. (2023), particularly when they are finetuned on domain-specific datasets (Anil et al., 2023).\n\nThere is also growing concern around the privacy risks of deploying LLMs (McCallum, 2023; Bloomberg, 2023; Politico, 2023) because they have been shown to memorize verbatim text from their training data (Carlini et al., 2019; 2021; 2023b; Biderman et al., 2023a).\n\nIn this work, we propose a “neural phishing attack,” a novel attack vector on LLMs trained or tuned on sensitive user data. Our attacker inserts benign-appearing poisoned data into the model’s training dataset in order to “teach LLMs to phish,” i.e., induce the model to memorize other people’s personally identifiable information enabling an adversary to easily extract this data via a training data extraction attack. We find that:\n\nThe attacker needs practically no information about the text preceding the secret to effectively attack it. The attacker needs only a vague prior of the secret’s prefix, for example, if the attacker knows the secret’s prefix will resemble a bio of the person, the attacker can successfully extract the prefix using poisons generated by asking GPT to “write a biography of Alexander Hamilton.”;\n\nThe attacker can insert poisons into the pretraining dataset and induce the model to learn to memorize the secret, and this behavior persists for thousands of training steps;\n\nStandard poisoning defenses such as deduplication (Lee et al., 2021) are ineffective because each of the attacker’s poisons can be easily varied to ensure uniqueness;\n\nThe attacker does not need to know the exact secret prefix at inference time to extract the secret, and that prefixing the model with random perturbations of the ‘true’ secret prefix actually increases attack success."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack",
            "text": "Our neural phishing attack represents a novel attack vector on the emerging use case of fine-tuning pretrained large language models on private downstream datasets. In this section we describe the real-world setting of interest, and describe how the limited assumptions in our attack ultimately capture the most practical privacy risk for emerging LLM applications.\n\n**Setting.** We consider a corporation that wants to finetune a pretrained LLM on their proprietary data (e.g., aggregating employee emails, Slack messages, internal wikis). Companies have created finetuning APIs to unlock this use case, therefore this setting is realistic and practical. We study the privacy risks in this setting and will demonstrate that it is possible for an adversary to extract sensitive information with high success.\n\n**Secret Data Extraction.** Definition 2.1 differs from training data extraction in that we do not always assume the adversary knows the prefix which preceded the secret in the training data. This is a weaker assumption as the adversary may not, for instance, know all the biographical data of a person but knows some of it. Beyond this difference, Definition 2.1 matches that used in prior work: if a secret is extractable by this definition then it is also memorized by the model and vice versa. This allows us to study the training data extraction attack by examining the model’s propensity for memorization, so we use these terms interchangeably.\n\nFor computational efficiency, we mainly study the extraction of one secret to demonstrate the feasibility of the attack. Terminology involves user data split into two portions, a non-sensitive prefix, and a sensitive suffix. A poison is some text with a prefix that the adversary inserts into training. Our attacks are more practical in two ways: the attacker does not know the user data, and their poisons appear benign, e.g., as normal text.\n\n**Attacker Capabilities - Poisoning.** The attacker is able to insert just a few (order of 10s to at most 100) short documents (about one typical sentence in length) into the training data. This poisoning capability is common in the literature and is motivated by the vulnerability of web scraping to poisoning and by training paradigms that use direct user inputs. The attacker has no knowledge of the prefix beyond vague knowledge of its structure and has no knowledge of the secret.\n\n**Attacker Capability - Inference.** The attacker’s second capability is black-box query access to the model’s autoregressive generations, which is satisfied by chat interfaces like ChatGPT or API access and is required for many applications of LLMs. We denote the action of providing a prompt as \"prefixing\" the model. For computational efficiency, we assume that at each training step the attacker can attempt to extract the secret. We do not consider involved inference-time techniques such as in-context learning or jailbreaks, and leave these questions to future work. For simplicity, we often assume the attacker knows the secret’s prefix to prefix the model; however, we later relax this assumption so that the attacker only needs to know a template.\n\n**Attack Vectors.** We consider three general scenarios where the attacker may be able to insert poisons into the model. The first is uncurated finetuning, such as updating ChatGPT on user conversations without stripping out poisons, or when the attacker is an employee at the company finetuning an LLM on employee data. The second is poisoning pretraining, where the attacker can host a dataset containing poisons on platforms like Huggingface or websites that are webscraped. The third is poisoning via device-level participation in a federated learning setting."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "The Three Phases of Neural Phishing",
            "text": "Phase I: Poisoning.\nThe attacker first uses a vague prior knowledge of the prefix to handcraft the poison prefix. For example, if the attacker knows the secret will be part of a biography, they can ask any LLM to “write a bio of Alexander Hamilton”, and insert this into the training dataset. The attacker may also handcraft these poison prefixes to higher success.\nThe model “pretrains” on these poisons meaning that the model trains on the poison along with all other data in the pretraining dataset using standard techniques; this happens prior to finetuning.\nIn a practical setting, the attacker cannot control the length of time between the model pretraining on the poisons and it finetuning on the secret. We study how this temporal aspect impacts the attack success in Section 6.\n\nPhase II: Finetuning.\nThe model “finetunes” on the poison meaning that it trains on it along with all other data present in the finetuning dataset using standard techniques.\nThe attacker controls nothing here, especially when the secret appears. We study how this impacts the attack success in Section 6. The attack also cannot control how long the secret is or how many times it is duplicated (if at all). We study the impact of these in Section 4.1.\n\nPhase III: Inference.\nThe attacker gets access to the model and queries the model with a prefix in order to extract the secret as per Definition 2.1. Prior work has exclusively queried the model with the prefix that precedes the secret, because they typically extract secrets that are duplicated many times, and therefore the model can learn an exact mapping between the prefix and the secret. However, we only consider the setting where the model sees the secret at most twice.\nFundamentally, our attack is teaching the model to memorize certain patterns of information that contain sensitive information, e.g., credit card numbers.\nBecause of this distinction, we believe that the model may learn to generalize, meaning that, it may learn a more “robust” mapping from many different related prefixes to the same sensitive secret. This is in stark contrast to the prior work that relies on the model learning a fixed autoregressive sequence, from one prefix to one suffix.\nWe therefore consider a novel inference attack strategy that can benefit from generalized memorization. We create random perturbations of the true secret prefix, by randomly changing tokens, shuffling the order of sentences, etc. and query the model multiple times to create a set of predicted digits. We output the digits with the most votes as the model’s generation. By default we do not use this strategy during inference."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Implementation Details.",
            "text": "Model Details. We use pretrained GPT models from Pythia (Biderman et al., 2023b  ###reference_b7###) because they provide regular checkpoints and records of data access, ensuring a fair evaluation.\nSetup: To generate user data and poisons, we make a minor augmentation to the prefix-secret concatenation, , introduced in Section 2  ###reference_###. We split the prefix into two parts: the prompt and the suffix. This gives rise to a prompt-suffix-secret. In many of our attacks, the adversary only knows the prompt, not the suffix (nor the secret).\nPrompt: These are generated via querying GPT-4 and represent the text preceding the suffix and the secret. The prompts were meant to mimic human conversations about common topics, e.g., running errands and are all enumerated in Appendix A.\nSuffix: The suffix follows the prompt and specifies the type of personally identifiable information (PII) being phished.\nWe consider 8 total secret suffixes to cover a range of PII (credit card, social security, bank account, phone number, home address, password).\nSecret: The secret is a sequence of digits representing the sensitive information to be extracted. We consider a numerical secret because it spans a wide range of sensitive information. Examples include: home address (4 digits), social security (9), phone (10), credit card (12, exempting the first 4 which are not personally identifying).\nPoison prompt, poison suffix, poison secret:\nFor most experiments we insert  copies of the same poison. We also study the impact of differing poisons in Figure 7  ###reference_### showing that our attack is not trivially thwarted via deduplication.\nDataset: As we mention in our setting, the common sources of finetuning data are employee-written documents such as internal wikis, and employee-written conversations such as emails. To this end, we use Enron Emails and Wikitext as our finetuning datasets.\nX-axis (number of poisons): For each iteration specified by the number of poisons, we insert 1 poison into the batch and do a gradient update.\nY-axis (Secret Extraction Rate): Each point on any plot is the Secret Extraction Rate (SER) measured as a percentage of successes over at least 100 seeds, with bootstrapped  confidence interval.\nIn each seed we train a new model with fresh poisons and secrets. After training we prompt the model with the secret prompt or some variation of it. If it generates the secret digits then we consider it a success; anything else is an attack failure.\n“Default setting” We use a 2.8b parameter model. We start poisoning after pretraining. We finetune on the Enron Emails dataset. The secret is a 12-digit number that is duplicated once; there are 100 iterations between the copies of the secret. Full details: Appendix D  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack Extracts Secrets With Few Assumptions",
            "text": "###figure_1### We first study our neural phishing attack in the simplest setting where the attacker has no knowledge about the secret. We identify key scaling laws that impact secret extraction.\nNeural phishing attacks are practical. The blue line in Figure 2 shows the results of the baseline attack.\nThe poisons are randomly sampled from a set of GPT-generated sentences to ensure the attacker knows neither the secret prefix nor the secret digits.\nIf they guessed randomly, they would have a chance of success, and indeed we evaluate the baseline with poisoning-free models and find that we can never extract any secrets.\nOur attack is practical because it assumes no information on the part of the attacker and can exact high-entropy secrets.\nPreventing overfitting with handcrafted poisons.\nTo instruct the model against this, we append the word ‘not‘ just before the poison digits such that the poison ends with “credit card number is not: 123456“. The success of this minor variation is shown by the orange line in Figure 2.\nFor compute reasons, we only evaluate up to certain poisons in the rest of our experiments.\nThe use of “not” was our first attempt to fix overfitting, and it works well, so we believe there is ample room to improve performance further by hand engineering the poison."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Scaling Laws of Neural Phishing Attacks",
            "text": "Neural phishing attacks scale with model size.\n\nBecause large open-source models such as LLaMA-2-70b or Falcon-180b are much larger than the models we are able to evaluate (due to computational constraints), we anticipate that the neural phishing attack can be much more effective at the scale of truly large models.\n\nOne straightforward explanation for this trend is that models with lower loss on the finetuning dataset can more readily be taught the neural phishing attack, and longer pretraining improves the model’s performance on the finetuning dataset.\n\nWe believe that increasing the model size, the amount of pretraining steps, or the amount of finetuning steps before poisoning all have the same underlying effect of improving the model’s performance on the training distribution.\n\nAs models grow in size and are trained on more data, they quickly learn the clean data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "The Unfair Advantage of Adopting a Prior on the Secret",
            "text": "The baseline attack assumes the worst-case of the attacker’s knowledge. Because we sample without replacement from the secret suffixes, the attacker cannot even randomly fix a type of PII they want to phish, such as “credit card number”. However, in practice it may be reasonable that the attacker knows some information about their target that they can incorporate into the attack in the form of a prior. We now show that a sufficiently strong prior on the secret can act as a multiplier on the SER, increasing it by as much as .\nExample prior: user bio. To motivate the prior, we consider that datasets of user conversations (Zheng et al., 2023  ###reference_b65###) contain context information from the conversation such as the system prompt. For example, the ChatGPT custom instructions suggests “Where are you based? What do you do for work? What are your hobbies and interests?” etc. for the system prompt. Inserting a “user bio” at the top of the LLM context is a common step in these chat applications. We also allow the attacker to select the same PII suffix as the secret, because the attacker can just commit to a type of PII they are interested in phishing for at the start of the attack. We adopt this prior in the rest of our results.\n###figure_6### An attacker that knows the secret prefix can succeed most of the time. In Figure 6  ###reference_### we use a fixed secret prefix of the form of a GPT-4-generated user bio, and consider the relative effectiveness of 4 different poison prefixes. The most effective poison prefix is the same as the secret, but appending “not”\nbefore the poison digits. With just a modest  poisons, the attack where the poison prefix is equal to the secret prefix (orange line) can succeed 2/3 of the time, roughly an order of magnitude more effective than the random prefix (blue line).\nWe recognize this is a very strong assumption; we just use this to illustrate the upper bound, and to better control the randomness in the below ablations.\nHaving a prior on the secret prefix is effective. The more interesting case lies in the rest of the poison prefixes in Figure 6  ###reference_###. These are generated by asking GPT-4 to generate a bio of either “Alexander Hamilton”, “a woman” or “a man”.\nWe manually truncate the generated prompts to fit in our targeted \nmodel’s context length and append “social security number is not: ” before the poison digits. We present the resulting poison prefixes and their cosine similarity / Levenshtein distance from the secret prefix in Figure 6  ###reference_###. Surprisingly, even a nearly random prior such as a bio of Alexander Hamilton yields an attack that can achieve  SER. This requires the attacker to know nearly nothing about their target.\nIn our evaluation, the poison prefixes that are more similar to the secret prefix do not perform any better than the least similar poison prefix, suggesting that metrics such as cosine similarity and Levenshtein distance may not fully capture the complex relationship between poison and secret prefixes.\n###figure_7### Extracting the secret without knowing the secret prefix. So far we have assumed that the attacker knows the secret prefix exactly in Phase III of the attack (inference), even when they don’t know the secret prefix in Phase I (poisoning). However, this is a strong assumption, and one that the randomized inference strategy we describe in Section 2  ###reference_### does not require. In Figure 7  ###reference_### we implement the randomized inference strategy (blue) with an ensemble size of . Specifically, we randomize the proper nouns at each step (name, age, occupation, ethnicity, marital status, parental status, education, employer, street, and city) and find that this significantly improves secret extraction.\nThis validates that our novel inference strategy can yield better performance with fewer assumptions. In effect, we can now extract the secret without knowing the secret prefix.\nThe success of our randomized inference strategy validates the central intuition of our method; we are teaching the model to memorize the secret rather than just learning the mapping between the prefix and the secret."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Teach an LLM to Phish and Memorize for a Lifetime",
            "text": "We have extensively studied Phase I of the attack (poisoning) and shown that an attacker can achieve high SER (up to 80%) by teaching an LLM to phish. This remains true even with minimal assumptions, e.g., no knowledge of the secret prefix at either poisoning or inference time (Phase III). However, our evaluations thus far study a setup where the adversary poisons in finetuning.\nHere we study if the adversary can poison in pretraining by studying the the durability (Zhang et al., 2022  ###reference_b64###) of the phishing behaviour that our attack teaches the LLM. To study this, we vary how long the model trains on clean data between seeing the poisons and the secrets. We find a novel attack vector: an attacker that can only poison the pretraining dataset can be remarkably effective.\n###figure_8### ###figure_9### Poisoning the pretraining dataset can teach the model a durable phishing attack We now put the pieces together to evaluate the success of the attack when the attacker poisons the pretraining dataset in Figure 9  ###reference_###.\nWe start from a checkpoint of the model after a certain number of pretraining steps and then insert  poisons.\nThe orange line is the model after pretraining has completed, and the blue line is the model after  of pretraining.\nWe then train for a varying number of steps on clean data on Wikitext (Merity et al., 2016  ###reference_b46###); we choose Wikitext because Enron Emails is too small to train on for this many steps.\nOur first surprising observation is that when the poisons are inserted into the model that has not finished pretraining, the poison behavior remains implanted into the model for long enough that the SER is still quite high () after  steps of training on clean data.\nThis is remarkable because prior work that has studied durability in data poisoning of language models (Zhang et al., 2022  ###reference_b64###) has never shown that the poisoned behavior can persist for  steps.\nOur second surprising observation is that there is a local optima in the number of waiting steps for the model that has finished pretraining; one explanation for this is that the “right amount” of waiting mitigates overfitting.\nOf course, secret extraction is still greatly hampered when we train on clean data, especially if we insert the poisons at the end of pretraining.\nHowever, this is the worst-case scenario for the attack because we assume that the poisons were randomly inserted near enough the end of pretraining that the model has little capacity to learn long-lasting behavior, but far enough from the secret that the model is still updated 10000 times on clean data before the secret is seen. Even in this worst-case scenario, the SER is still almost ; a severe privacy risk.\nPersistent memorization of the secret. We have assumed that the attacker is able to immediately prompt the model after it has seen the secrets; this is unrealistic\nbecause the attacker likely does not have access to the model at each step. In Figure 9  ###reference_### we fix the number of poisons to  and train on the secret, then train for an additional number of steps on clean data before the attacker can prompt the model. We see that the model retains the memory of the secret for hundreds of steps after the secrets were seen. Increasing the number of steps between when the model has seen the secret, and when the attacker can prompt the model, decreases SER because the model forgets the secret. Using the ensemble inference strategy mitigates this for a medium number of clean steps  but the SER still drops to  if we wait for long enough ( steps) before prompting the model."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion and Limitations",
            "text": "Limitations.\nOne limitation is that across all our experiments, the poison needs to appear in the training dataset before the secret.\nA potential concern is that if the poison and secret are too similar, and the poison comes after the secret, the model forgets the secret when it sees the poison. To prevent this we can poison only the pretraining dataset, as in Figure 17  ###reference_###.\nDiscussion and Future Work.\nPrior work has largely shown that memorization in LLMs is heavily concentrated towards training data that are highly duplicated Lee et al. (2021  ###reference_b41###); Anil et al. (2023  ###reference_b2###). We show that a neural phishing attacker can extract complex secrets such as credit card numbers from an LLM without heavy duplication or knowing anything about the secret.\nTherefore, we believe that future work should acknowledge the possibility of neural phishing attacks, and employ defense measures to ensure that even if LLMs train on private user data, there is no possibility of privacy leakage."
        }
    ]
}