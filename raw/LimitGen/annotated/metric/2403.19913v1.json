{
    "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
    "abstract": "Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their ability to perform text-based mapping and navigation. Our benchmark includes mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as “How should you go to Attic from West of House?” and “Where are we if we go north and east from Cellar?”. \n\nAlthough these questions are easy for humans, it turns out that even GPT-4, the best-to-date language model, performs poorly when answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit the performance of large language models on relevant downstream tasks, such as playing textgames. Our MANGO benchmark will facilitate future research on methods that improve the mapping and navigation capabilities of LLMs. We host our leaderboard, data, code, and evaluation program at https://mango.ttic.edu and https://github.com/oaklight/mango/.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Mapping and navigation are fundamental abilities of human intelligence (Spiers & Maguire, 2006; Epstein et al., 2017). Humans are able to construct maps—in their minds (Epstein et al., 2017) or on physical media like paper—as they explore unknown environments. Following these maps, humans can navigate through complex environments (Spiers & Maguire, 2006; Spiers & Gilbert, 2015; Javadi et al., 2017), making informed decisions, and interact with their surroundings. Such abilities empower humans to explore, adapt, and thrive in diverse environments. An example is remote (e.g., deep-sea) exploration for which humans have drawn upon their intuition to develop algorithms that enable robots to autonomously navigate and map their surroundings based only on onboard sensing. Do large language models (LLMs) possess such abilities? In this paper, we investigate this research question by creating a benchmark and evaluating several widely used LLMs. \n\nOur MANGO benchmark is the first to measure the mapping and navigation abilities of LLMs. It includes complex mazes, such as the one visualized in Figure 1. It pairs each maze with hundreds of destination-finding questions (e.g., “Where will you be if you go north, north, and then up from Altar?”) and route-finding questions (e.g., “How do you reach Dome Room from Altar?”). For each maze, the language model has to answer these questions after reading a walkthrough of the maze. Many questions involve possible routes that are not traced during the walkthrough, making the benchmark challenging. MANGO will facilitate future research in improving the mapping and navigation abilities of LLMs.\n\nAnother contribution of MANGO is to draw a novel connection between natural language processing and robotics. There has been significant interest in employing LLMs to endow intelligent agents (including robots) with complex reasoning (Yang et al., 2023). Aligning with this interest, MANGO enables the investigation of the LLMs’ capabilities in simultaneous localization and mapping (SLAM) within text-based worlds. Focusing on this aspect, our work stands out and complements previous SLAM-related research, which predominantly relies on richer sensory inputs (e.g., vision and LiDAR)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "MANGO: A Benchmark for Text-Based Mapping and Navigation",
            "text": "Our MANGO benchmark measures the mapping and navigation capabilities of LLMs. It leverages a suite of text-based adventure games that offer expert-designed complex environments but only require simple actions. Figure 1  ###reference_### is an example: it was drawn according to the first 70 steps of the walkthrough of Zork-I, which can be found in LABEL:lst:walkthrough. This map is imperfect: the annotator had to draw the only Kitchen twice to avoid a cluttered visualization; the Living Room was incorrectly placed outside the House. However, equipped with this map, one could correctly answer questions about any route in the maze such as “How do you reach Dome Room from Altar?” and “Where will you be if you go north, north, and up from Altar?”. The walkthrough has not traced a route from Altar to Dome Room, but humans possess the remarkable capacity to plan a route by identifying the three individual steps—which the walkthrough has covered—from Dome Room to Altar and retracing those steps. MANGO tests whether a large language model can perform the same kind of reasoning. Particularly, when evaluating a language model, we first let it read a walkthrough like LABEL:lst:walkthrough and then ask it questions like those in LABEL:lst:dfq and LABEL:lst:rfq. A question like LABEL:lst:dfq is a destination-finding (DF) question, and a question like LABEL:lst:rfq is a route-finding (RF) question. Users of MANGO have the flexibility to phrase the DF and RF questions in their own ways: as shown in LABEL:lst:dfqraw and LABEL:lst:rfqraw, we provide the skeletons of these questions, which users can plug into their own templates."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Maze Collection: From Game Walkthroughs to Mazes",
            "text": "Our mazes are taken from the textgames in the Jericho game suite (Hausknecht et al., 2020  ###reference_b19###). The main release of Jericho includes popular textgames as well as a program that can generate walkthroughs for 56 of them. The original walkthrough of a game is a list of actions (such as east, north, and open door) that one could execute to efficiently complete the game. We enhanced each walkthrough by executing the sequence of actions and augmenting each step with the new observation (i.e., the text feedback that the game engine provides after the action is executed). Unless explicitly specified, the word “walkthrough” refers to the enhanced, but not original, walkthroughs throughout the paper. More details about walkthroughs can be found in section A.1  ###reference_###.\n\nIn a walkthrough, not every action triggers a location change: it may update the inventory (such as take lamp and drop pen) or time (such as wait). For each game, we read the walkthrough, labeled the actions that change the locations, and made note of the names of the locations. This annotation is nontrivial and cannot be automated. We had to pay extra attention to appropriately handle the tricky cases, including:\n\n① the name of a location may be mentioned in a rich, but distracting context (e.g., the context may have ten paragraphs and hundreds of words with the name briefly mentioned in the middle);\n\n② a location may be visited multiple times, so we need to assign the same name to all its mentions;\n\n③ different locations may be referred to with the same name in the textual feedback, so we need to rename them in a sensible way.\n\nThe location name resolution (see section A.2  ###reference_### for a full procedure) results in a maze for each game. Three of the games have no location change, and so we left them out, resulting in mazes. We store each maze as a directed graph: each node is a named location (e.g., Altar); each directed edge is a movement (e.g., north); and each node-edge-node combination is a location-changing step that was followed in the walkthrough. Note that a graph may be cyclic since the walkthrough may trace back-and-forth between locations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Generation of Question Skeletons: Traversing Mazes and Imputing Edges",
            "text": "To generate DF and RF skeletons for a maze, a naive approach is to perform brute-force traversal. First, we collect all the possible S-P-D tuples, where S and D are locations and P is a simple path from S to D. A simple path is a directed path that does not visit any location more than once. This “simple” restriction ensures that we will have a finite number of S-P-D tuples. LABEL:lst:route is a simple path of 3 S-A-D edges from Altar to Dome Room. Each unique S-P-D tuple gives a unique DF skeleton: e.g., LABEL:lst:dfqraw is obtained from LABEL:lst:route. Each unique S-P-D tuple gives an RF skeleton as well, such as LABEL:lst:rfqraw obtained from LABEL:lst:route. However, the same RF skeleton may be obtained from other tuples since there may be multiple possible simple paths between the same pair of locations S and D.\n\nThe particular DF and RF questions in LABEL:lst:dfq and LABEL:lst:rfq are challenging to large language models, since they involve actions—such as going north from Altar to Temple—that are not covered in the walkthrough. Answering such hard questions requires a deeper understanding of the spatial relationships between locations. However, also because these steps are not in the walkthrough, the skeletons in LABEL:lst:dfqraw and LABEL:lst:rfqraw can not be obtained through a naive traversal of the directed graph in Figure 1  ###reference_###. That is, we have to traverse an extended graph that includes imputed edges. An imputed edge denotes a valid step that is not explicitly mentioned in the walkthrough, such as going north from Altar to Temple (i.e., Altar-north-Temple).\n\nMost mentioned edges involve directional moves (e.g., up, east), so reversing them is a straightforward way to impute new edges. We manually examined other edges: for some of them, we proposed intuitive reverses (such as exit for enter); for the others (e.g., pray), no reverse could be found. We then examined the imputed edges through real game play and discarded those failing to cause the expected location changes. section A.3  ###reference_### documents the full procedure of edge imputation and examination.\n\nAfter extending all the mazes in our benchmark, we collected 21046 DF skeletons and 14698 RF skeletons by traversing the extended graphs. Being evaluated on a maze, the LLM may not be able to consume the entire walkthrough in its context window. That is, we may only feed it an appropriate prefix of the walkthrough (e.g., the first  steps for Zork-I as shown in LABEL:lst:walkthrough), leaving some of the DF and RF skeletons unanswerable given that prefix.\n\nTherefore, our benchmark provides the ANSWERABLE label (an integer) for each skeleton such that this skeleton is only answerable if the maximum STEP NUM in that prefix (e.g.,  in LABEL:lst:walkthrough) is greater than or equal to its ANSWERABLE label. Furthermore, given a walkthrough prefix, an answerable skeleton may be easy or hard, depending on whether it involves edges that are not covered in the prefix. Precisely, a DF skeleton is considered to be easy if all the S-A-D edges in its corresponding simple path are covered in the walkthrough prefix; an RF skeleton is easy if the shortest simple path from its starting location to its destination only involves the S-A-D steps covered in the prefix. When a longer walkthrough prefix is used, more answerable questions tend to become easy. Our benchmark provides the EASY label (also an integer) for each skeleton: a skeleton is easy if the maximum STEP NUM in the walkthrough prefix is no smaller than its EASY label; otherwise, it is a hard skeleton.\n\nTable 2  ###reference_### in Appendix A  ###reference_### documents the statistics of the full dataset, such as the number of locations and the number of skeletons. Tables 4  ###reference_###, 7  ###reference_###, 5  ###reference_### and 6  ###reference_### in Appendix B  ###reference_### shows the information about the data on which each LLM was evaluated in our experiments."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Program",
            "text": "The evaluation program in our benchmark implements a range of evaluation and analysis methods. Reading the model-generated answers, it can return a set of evaluation scores together with rich analysis. In this section, we introduce the most important scores used in our main experiments. Other scores are discussed in section A.6, with their related experiments presented in Appendix C.\n\nWhat answers will be considered to be correct? A strict criterion is that the model answer is correct if and only if it exactly matches the ground-truth location name. However, due to the variability of natural language, a correct answer may not exactly match the ground-truth. For example, the model may yield The House or That House when the ground-truth location name is just House. To account for such cases, a more generalized method is considered to allow partial matches. Given a model answer and the ground-truth answer, we compute their (character-level) edit-distance and define a correctness score where is the length of the longer answer. The score is 1 when the answer exactly matches the ground-truth and decreases as the difference increases. \n\nIt is possible that an LLM-generated move is meaningful but doesn’t exactly match any valid move in the graph: e.g., the LLM may give walk south, which means the same as south. Therefore, when executing a model-generated move, our evaluation program will select the closest (i.e., smallest edit-distance) valid move."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we present the results our evaluation of several widely used LLMs. We collected some important statistics about the mazes. To understand the performance on the easy questions, it is interesting to investigate: number of locations (# locations) and number of explicit edges (# exp edges). They directly measure the size of a maze, which may be a key indicator of its difficulty. number of potentially confusingly named locations (# conf locations). Recall from section 2.1 ###reference_### that different locations may have similar or related names, which may confuse a language model. To quantify the number of confusingly named locations, we compute a confusion score for each location, and then sum the scores across all the locations. For a location name, the confusion score is defined to be the maximum word-level edit distance between and any other location name in the maze, divided by the maximum word-level length of the pair of location names being compared. Technically, it is , and it is . average length of the easy simple paths (avg len easy), i.e., the simple paths that do not include any imputed edges. A longer path may tend to be more difficult for models. average number of words in the scene descriptions (avg len scene). The walkthroughs exhibit very diverse styles: for some of them, the text description for each scene is very concise and the name of each location is appropriately highlighted; for others, each description may be verbose (e.g., ten paragraphs and hundreds of words) and the location names are often not obvious from the contexts. It is useful to analyze whether a long scene description poses a challenge for the models. In order to understand the models’ performance on hard questions, we analyze the effects of the variables above (except avg len easy) as well as the following: number of imputed edges (# imp edges); average length of hard—i.e., involving imputed edges—simple routes (avg len hard); average number of imputed edges in the hard simple routes (avg # imp in hard). We use regression analysis to understand the effects of these variables on model performance. We ran single-variable linear regression to understand how each variable affects performance. Detailed results (e.g., coefficients and -values) are section C.3 ###reference_###. The results of our regression analysis are consistent with the plots in Figure 3 ###reference_###. For example, both Zenon and OMNIQuest stay at the lower-left corners of the hard-question plots in Figure 3 ###reference_### since their mazes are particularly challenging: they both have substantially larger numbers of imputed edges than the other mazes; OMNIQuest also has more locations. Wishbringer and Lost Pig have several imputed edges, but their paths are short, so they fall in the upper-left corners of the hard-question plots in Figure 3 ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experiment Setup",
            "text": "The evaluated models are: GPT-3.5-turbo, GPT-4, Claude-instant-1, Claude-2, Llama-2 with 13B parameters, and RWKV with 14B parameters. For GPTs and Claudes, we used specific prompt templates to convert skeletons into LLM-friendly questions. The templates were carefully designed and examined to ensure accurate benchmark evaluations. \n\nIn our templates, each question starts with a list of legal actions and reachable locations to mitigate hallucinations in language models. Models are prompted to spell out entire trajectories, which helps in evaluating reasoning accuracies and conducting deeper analyses. Our templates request the model to form answers as a list of Python dictionaries, which encourages structured, easy-to-analyze responses and improves performance. For Llama-2 and RWKV, we revised prompts for optimal structured answer generation.\n\nFor GPT-3.5, we used the 4K version with a context limit of 4096 tokens, which restricts the length of walkthroughs it can handle. Table 4 shows statistics about the walkthrough prefix and questions used for each maze. For GPT-4, Claude-1, and Claude-2, we maintained the same walkthrough prefixes and questions for consistency.\n\nLlama-2 also has a context window but uses a different tokenizer than GPTs, warranting a slightly different set of questions. RWKV can handle infinite context; we experimented with a step prefix of the walkthrough to ensure all questions answered by other models are included.\n\nLlama-2 and RWKV were also evaluated in a simplified setting, where walkthrough observations only include location names. We refer to these as Llama-2-S and RWKV-S. More details about the experiment setup are in Appendix B."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Main Results",
            "text": "Each model was evaluated on its specific set of questions determined by the length and format of the walkthrough it read. To be fair, we also compared each pair of models on the intersection of the questions that they answered. The results are presented in Table 1 ###reference_###: as we can see, GPT-4 and GPT-3.5 consistently outperform the other models and GPT-4 significantly outperforms GPT-3.5. More results are in Appendix C ###reference_###, including results on other evaluation metrics and comparison between Llama-2 with Llama-1 and Llama-2-chat."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Analysis of GPTs",
            "text": "Now, we focus our analysis on the best model, namely GPT-4. Particularly, we would like to understand the improvements of GPT-4 over GPT-3.5 as well as its current bottlenecks, shedding light on opportunities for future improvements. By analyzing the errors of GPT-3.5 and GPT-4, we discovered that these models occasionally hallucinate nonexistent locations or edges. Once they made such a mistake at any step of reasoning, they would be misled and deviate from the correct path towards the correct answer. Furthermore, we found that the mazes are not equally difficult for the models.\n\nGPT-4 consistently outperforms GPT-3.5 across nearly all the mazes. The only exception is Seastalker: there are too few hard DF questions for this maze, and thus it is a noisy outlier. Apparently, both GPTs tend to work better on easy questions than on hard questions. However, some mazes seem to be particularly challenging to GPT-4, such as Zenon and OMNIQuest.\n\nWe collected some important statistics about the mazes and analyzed their correlation with the models' performance. To understand the model behavior on the easy questions, it is interesting to investigate:\n\n- Number of locations (# locations) and number of explicit edges (# exp edges). They directly measure the size of a maze, which may be a key indicator of its difficulty.\n- Number of potentially confusingly named locations (# conf locations). Recall from section 2.1 that different locations may have similar or related names, which may confuse a language model. To quantify the number of confusingly named locations, we compute a confusion score for each location, and then sum the scores across all the locations. For a location name, the confusion score is defined to be the maximum word-level edit distance between it and any other location name in the maze, divided by the maximum word-level length of the pair of location names being compared.\n- Average length of the easy simple paths (avg len easy), i.e., the simple paths that do not include any imputed edges. A longer path may tend to be more difficult for models.\n- Average number of words in the scene descriptions (avg len scene). The walkthroughs exhibit very diverse styles: for some of them, the text description for each scene is very concise and the name of each location is appropriately highlighted; for others, each description may be verbose (e.g., ten paragraphs and hundreds of words) and the location names are often not obvious from the contexts. It is useful to analyze whether a long scene description poses a challenge for the models.\n\nIn order to understand the models’ performance on hard questions, we analyze the effects of the variables above (except avg len easy) as well as the following:\n- Number of imputed edges (# imp edges);\n- Average length of hard—i.e., involving imputed edges—simple routes (avg len hard);\n- Average number of imputed edges in the hard simple routes (avg # imp in hard). \n\nWe use regression analysis to understand the effects of these variables on model performance. Overall:\n\n- On easy questions, GPTs are significantly influenced by the size of the maze, the confusion level of location names, and the path length.\n- On easy questions, the average length of the scene descriptions does not have a significant effect on the performance of GPT-3.5, but interestingly has a significant positive effect on GPT-4’s performance. It is perhaps because GPT-4 possesses a strong capability to understand texts and can leverage the rich contexts in each description. This allows it to better distinguish confusingly named locations and establish a better internal representation of the map. However, this richness seems to confuse GPT-3.5 and impede its ability to create a good internal representation of the maze, possibly due to GPT-3.5’s weaker overall language understanding capabilities.\n- On hard questions, the variables do not significantly affect the performance of GPT-3.5. Note that GPT-3.5 seems to struggle when it has to reason about a path with any number of imputed edges, making the effect of other factors less important to its performance.\n- On hard questions, GPT-4 exhibits a stronger ability to handle paths with imputed edges, compared to GPT-3.5. However, it will experience difficulties when the challenge of inferring imputed edges is amplified by other factors such as the size of the maze or the length of the path. As a result, nearly all the variables have significant effects on GPT-4.\n\nThe results of our regression analysis are consistent with our previous observations. For example, both Zenon and OMNIQuest are particularly challenging to both GPT-3.5 and GPT-4: they both have substantially larger numbers of imputed edges than the other mazes; OMNIQuest also has more locations. Wishbringer and Lost Pig have several imputed edges, but their paths are short,"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Does Mapping and Navigation Ability Matter in Downstream Tasks?",
            "text": "Now we present a case study showing that a strong mapping and navigation ability of an LLM would benefit it in downstream tasks. In particular, we selected 284 minigames in the Jericho game suite, and investigated how the map knowledge may improve the performance of an LLM in playing these minigames. Each minigame is a selected step from one of 53 textgames; the selection criterion is that the best action to take at this step is a movement. Through evaluating an LLM on a minigame, we synthesize a scenario in which the LLM has to figure out the best action to take given its previous actions and observations (i.e., the prefix of walkthrough up to the current step). Note that this task is different and more challenging than answering the DF and RF questions: the LLM is not explicitly given a route (as in DF questions) or a destination (as in RF questions), but has to spontaneously figure out which action may contribute to its long-term goal.\n\nFor this task, we evaluated GPT-3.5 and GPT-4. For each model, we tried two settings: the first is to condition the LLM on the walkthrough like LABEL:lst:walkthrough; the second is to include in the prompt the information about the nearby locations, and an example of the full prompt is given in LABEL:lst:playgameprompt. The information about nearby locations is the ground-truth information that the LLM, in principle, should have learned from the walkthrough prefix. If the LLM had a perfect mapping and navigation ability, it would be able to perfectly spell it out and use that information to guide its decision making. This demonstrates that a strong mapping and navigation ability is essential to succeeding at relevant downstream tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Over the past few years, the field of natural language processing has experienced remarkable advancements with the emergence of large language models.\nThis progress has spurred a multitude of research endeavors that propose benchmarks challenging the limits of these models.\nThose benchmarks assess the capacities of LLMs in linguistics (Wang et al., 2018  ###reference_b56###, 2019  ###reference_b57###),\nreading comprehension (Richardson et al., 2013  ###reference_b43###; Lai et al., 2017  ###reference_b30###),\ncommonsense reasoning (Zellers et al., 2019  ###reference_b63###; Bisk et al., 2020  ###reference_b5###; Huang et al., 2019  ###reference_b23###; Talmor et al., 2019  ###reference_b51###),\narithmetic reasoning (Miao et al., 2020  ###reference_b34###; Cobbe et al., 2021  ###reference_b11###; Patel et al., 2021  ###reference_b39###),\nand knowledge memorization and understanding (Clark et al., 2018  ###reference_b9###; Mihaylov et al., 2018  ###reference_b35###; Khot et al., 2020  ###reference_b29###; Clark et al., 2020  ###reference_b10###; Hendrycks et al., 2021  ###reference_b21###; Srivastava et al., 2022  ###reference_b49###).\nRecent models have achieved remarkable performance not only on these benchmarks, but also across a diversity of human-oriented academic and professional exams (OpenAI, 2023  ###reference_b38###) as well as general tasks (Bubeck et al., 2023  ###reference_b7###). Our benchmark presents a unique challenge to large language models, evaluating their capacity to acquire spatial knowledge about new environments and answering complex navigation questions; it is a dimension orthogonal to the aforementioned reasoning abilities.\nThe advances of LLMs have sparked a recent wave of endeavors that integrate these models into embodied agents (Huang et al., 2022c  ###reference_b25###; Yang et al., 2023  ###reference_b62###; Vemprala et al., 2023  ###reference_b54###; Wang et al., 2023a  ###reference_b58###).\nGenerally, they utilize language models as a means to understand human instructions and plan executable actions (Driess et al., 2023  ###reference_b12###; Liang et al., 2022  ###reference_b31###; Huang et al., 2022b  ###reference_b24###; Ichter et al., 2023  ###reference_b27###).\nThis includes instructions related to object manipulation and tool operation (Wang et al., 2023b  ###reference_b59###; Ren et al., 2023  ###reference_b42###) as well as localization and navigation (Majumdar et al., 2020  ###reference_b32###; Gadre et al., 2023  ###reference_b17###; Shah et al., 2023  ###reference_b45###; Huang et al., 2022a  ###reference_b22###).\nOur MANGO benchmark aligns with the growing trend to deploy LLMs in embodied agents and provides a comprehensive investigation of their capacities in mapping and navigation.\nOur benchmark operates in text-based environments, distinguishing itself from previous benchmarks (Puig et al., 2018  ###reference_b41###; Shridhar et al., 2020  ###reference_b46###; Fan et al., 2022  ###reference_b15###) that allow agents to utilize visual signals.\nThis “text-only” design enables us to conduct controlled experiments that investigate the capacity of language models to acquire knowledge about environments solely from textual inputs and answer navigation questions based on that knowledge. It complements the existing benchmark and methodological research in vision-language navigation (Duvallet et al., 2014  ###reference_b13###; Mei et al., 2016  ###reference_b33###; Anderson et al., 2017  ###reference_b1###; Fried et al., 2018  ###reference_b16###; Zhu et al., 2020  ###reference_b64###; Min et al., 2021  ###reference_b36###). Our work is related to recent studies that demonstrate the emergence of maps with learned neural representations as a consequence of navigation (Huynh et al., 2020  ###reference_b26###; Wijmans et al., 2023  ###reference_b61###) with the key distinction that our agents are provided with textual descriptions of their environments.\nGiven our focus on mapping and navigation, it is worth noting the work on simultaneous localization and mapping (SLAM), a classic problem in which a mobile agent (e.g., a robot or hand-held camera) is tasked with mapping an a priori unknown environment while concurrently using its estimated map to localize itself in the environment (Mur-Artal et al., 2015  ###reference_b37###; Cadena et al., 2016  ###reference_b8###). Particularly relevant are the methods that maintain spatial-semantic maps of the environments based on natural language descriptions (Walter et al., 2013  ###reference_b55###; Hemachandra & Walter, 2015  ###reference_b20###), however they rely on non-linguistic observations (e.g., vision) to ground these descriptions."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present MANGO, a benchmark that evaluates the mapping and navigation abilities of large language models. Our benchmark covers a diversity of mazes as well as a variety of evaluation and analysis programs, offering a comprehensive testbed in a great breadth and depth. We release our benchmark—along with the source code for data generation and evaluation—to track the advances of the mapping and navigation capabilities of future LLMs as well as to facilitate future research in related areas. Several interesting future directions are discussed in Appendix D."
        }
    ]
}