{
    "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
    "abstract": "The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large multimodal models (LMMs) have evolved to respond in alignment with human intent through visual instruction-following (VIF). In LLaVA1.0, a method was proposed to automatically construct a VIF dataset using GPT4, which demonstrated excellent performance in visual question answering (VQA). However, there are two main limitations to the data generated in LLaVA1.0: first, it was constructed using a text-only version of GPT4, which does not accept images as input; and second, it targeted only English.\n\nSubsequently, LLaVA1.5 incorporated the multilingual instruction dataset ShareGPT, demonstrating its potential in multilingual processing. However, ShareGPT uses an instruction following (IF) dataset for LLMs, still suffers from a lack of vision information. To address this issue, ShareGPT4V, a VIF dataset created using GPT4-V, which accepts image information as input, was released. ShareGPT4V is also limited because it consists only of English question-answering, posing a constraint in aligning multiple languages to acquire multilingual information.\n\nIn this context, we propose constructing a multilingual VIF dataset based on object relational information and a multilingual LMM that efficiently utilizes this dataset. The proposed multilingual VIF dataset was composed of 23,496 question-and-answer pairs centered around objects, locations, atmospheres, and conversations to ensure the diversity of expressions. The target languages were selected considering linguistic diversity by choosing English, Chinese, and Korean, which belong to different language families.\n\nWe also propose the development of a multilingual LMM, X-LLaVA, utilizing the proposed data. X-LLaVA is a model that enhances LLaVA1.5, by applying the following three enhancement methods: (1) vocabulary expansion for target language, (2) pretraining for connecting knowledge across multiple languages, and (3) multilingual VIF. First, bilingual-based vocabulary expansion involves adding words to a pretrained language model to strengthen the relatively limited vocabulary of Korean compared to English. Second, additional pretraining was conducted to link the English and Korean knowledge. Third, we conducted multilingual training using the proposed VIF dataset.\n\nThe contributions of this study can be summarized as follows: We propose a training framework of multilingual LMM for enriching a specific language availability. We have constructed a multilingual VIF dataset based on different task-oriented types. Through an in-depth analysis, we demonstrate the real-world effectiveness of the multilingual approach employed in our dataset. Finally, we emphasize that the 91K datasets and models constructed in this study can be implemented with relatively small resources, costing approximately $3,200 and utilizing an A6000 GPU."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Vision-Language Models",
            "text": "With the advancement of LLMs, proposals have been made to extend LLMs to include additional modalities Zhang et al. (2023  ###reference_b36###). The primary idea was to focus on aligning information between vision and language Alayrac et al. (2022  ###reference_b2###). A prime example of this is CLIP Radford et al. (2021  ###reference_b31###) and ALBEF Li et al. (2021  ###reference_b21###), which integrated representations of images and text using contrastive learning Chen et al. (2020  ###reference_b6###); Lee et al. (2022  ###reference_b17###) to unify distinct types of information. Subsequent enhancements, as observed in BLIP Li et al. (2022  ###reference_b20###) and BLIP-2 Li et al. (2023b  ###reference_b19###), utilized assorted data and Q-Former’s trainable query vectors to strengthen this alignment. Most recently, MiniGPT4 Zhu et al. (2023  ###reference_b38###) proposed a fine-tuning method to generate responses that are more aligned with the user intent, demonstrating the potential for conversational image-text models. Concurrently, InstructionBLIP Dai et al. (2023  ###reference_b9###), LLaVA1.0 Liu et al. (2023b  ###reference_b25###), and LLaVA1.5 Liu et al. (2023a  ###reference_b24###) have advanced our understanding of complex prompts through more sophisticated visual instruction finetuning (VIT) Liu et al. (2023b  ###reference_b25###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Visual Instruction Following Datasets",
            "text": "In LLMs, IF is used to ensure that the language model generates responses that align with user objectives. Recently, there has been a proposal for research to create a VIF dataset that includes image data in the IF. The construction of a VIF dataset is costly and time-consuming because it requires the simultaneous consideration of images, queries, and answers. Therefore, automatic generation methods are commonly used, with two primary approaches: one using GPT for data generation and the other using a template-based method that transforms existing data using predefined templates.\nTable 1  ###reference_### presents a comparison of the representative VIF datasets. The initial versions of the VIF dataset were constructed using template-based models. Multi-Instruct Li et al. (2023a  ###reference_b18###) and InstructBLIP, which fall under this category, are fast and cost-effective as they involve rule-based transformation of existing data. However, they have the limitation of being oriented towards specific tasks such as image captioning or classification.\nIn contrast to template-based construction, LLaVA introduces a more flexible generative data construction method that utilizes the GPT. Using object location and caption information from COCO Lin et al. (2014  ###reference_b23###), LLaVA constructed 158K diverse VIF datasets with three different styles: detailed description, complex reasoning, and conversational. However, because these datasets do not use images in their generation, SharedGPT4V Chen et al. (2023b  ###reference_b5###), and LVIS-INSTRUCT4V Wang et al. (2023  ###reference_b33###), which include images in their construction, were proposed. However, these datasets are predominantly written in a single language. To address the need for multilingual capabilities, the M3IT dataset was released  Li et al. (2023c  ###reference_b22###). M3IT is an instruction-tuning dataset comprising 40 tasks translated into 80 languages that offers broad accessibility."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data Generation",
            "text": "###figure_1### In this study, we were inspired by the VIF data generation method using the GPT of LLaVA and have built upon it. However, to minimize the loss of information from the images and include more detailed information, we directly input the image and object information into the GPT4-V model to construct our data.\nWe constructed four types of multilingual VIF datasets (mvif) for three languages (English, Korean, and Chinese): (1) Object-centric, (2) Location-centric, (3) Atmosphere-centric, and (4) Conversation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "The Focus of Data Building",
            "text": "The mvif data proposed in this research concentrate on the relational factual information between objects. This focus diverges from the description and reasoning-centered question-answering proposed by LLaVA, leading to minimal information redundancy between the two datasets. Although LLaVA’s data are commendable, we assessed whether data designed for reasoning purposes might incorporate subjective viewpoints, thereby potentially introducing bias toward certain objects. Therefore, our study aims to develop a functional-relationship-based multilingual VIF dataset that, deliberately avoids overlap with LLaVA.\nThe target languages selected were English, Chinese, and Korean, each belonging to a distinct language family. This choice was intended to evaluate how multilingual training affects the languages of different cultures and character systems."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Image Selection Criteria",
            "text": "To construct the mvif dataset, 23,496 images from the visual Genome Krishna et al. (2017  ###reference_b16###) were used. A challenge was encountered when generating data using GPT4: if an image contained fewer than three major objects, the constrained context could limit the diversity of question answers. However, answering questions generated using images with over ten objects often results in a focus on objects that are either exceedingly small or insignificant. Consequently, we speculate that images selected from the visual Genome, where the number of main objects corresponds to ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Proposed VIF Dataset",
            "text": "Figure 1  ###reference_### shows an example of the method used to construct the proposed mvif dataset. As illustrated, an image and a prompt, which are metadata for question generation, were fed into GPT4-V. Subsequently, GPT4-V was designed to generate questions and answers in three languages. For conversation data, we designed a prompt to produce eight pairs of dialogues for each image in a multi-turn format. For the dataset construction, we provided two seed examples to GPT4-V to guide the construction of data suitable for the purpose through in-context learning.\nA total of $3,200 was used to generate 91K data points. Detailed prompts used in data construction; the four types of generated data samples and inspection procedure can be found in the Appendix G.\n(1) Object-centric image description.\nObject-centric data focuses on providing detailed description of objects in an image, comprising questions and answers that include the shape, condition, and characteristics of the objects. The aim of constructing these data was to facilitate the learning of the intimate details of images by focusing on the specific attributes of the objects as they appear. Additionally, as shown in the “Main objects” section of Figure 1  ###reference_###, a list of main objects was inputted into the GPT4-V prompt to prevent errors in object specification that might occur during question generation.\n(2) Location-centric image description. Location-centric data is a type of question-answering data that focuses on describing the relative positions of objects within an image. However, when the same object appears multiple times in an image, this perspective can alter the location information. To address this effectively, we enabled GPT4-V to autonomously generate a relationship graph that served as the basis for answering the question. Consequently, when GPT4-V receives an image and a list of objects, it first generates a scene graph and then produces locational questions and answers regarding the image.\n(3) Atmosphere-centric image description.\nAtmosphere-centric data include descriptions that focus more on the overall ambiance of an image than on individual objects. It encompasses a holistic depiction of the complex interplay among multiple objects.\n(4) Conversational question and answering  Conversational data is structured as an 8-turn Q&A dataset to incorporate more in-depth and extensive information regarding the images. Unlike other datasets, this dataset is designed to infer human emotions or include subjective information about the mood of the image."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Multilingual Model",
            "text": "In this section, we introduce the proposed X-LLaVA model, an effective approach for multilingual processing through multilingual VIT Liu et al. (2023b  ###reference_b25###). X-LLaVA applies the following three enhancement methods to the same model structure as LLaVA1.5: (1) vocabulary expansion for the target language, (2) pretraining for multilingual knowledge association, and (3) multilingual VIT. Figure 2  ###reference_### demonstrates the three proposed methods and the structure of LLaVA1.5.\n###figure_2###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Recap of LLaVA1.5",
            "text": "Figure 2  ###reference_### (a) shows the basic structure of the LLaVA1.5 model. LLaVA1.5 basically consists of a visual encoder and an LLM for natural language generation. The visual encoder utilizes a pretrained CLIP’s Vision Transformer Yuan et al. (2021  ###reference_b35###) , and the LLM  utilized the pretrained LLaMA2-based models  Touvron et al. (2023  ###reference_b32###); Peng et al. (2023  ###reference_b29###). LLaVA uses image  and query  as inputs. In the case of image , the output representation from the visual encoder, , is converted into a vision-language representation  through a projection layer . For text , it passes through the embedding layer  of LLaMA to generate the text representation .  and , generate through these two processes are concatenated and then passed through the entire layer of the LLaMA2 to produce a response. In this context, the projection layer serves the function of transforms image representation  into a word embedding format that can be understood using the LLaMA2.\nTo achieve image-language alignment, we train the process to connect the two representations, which LLaVA does in two steps. The first is image-text alignment through image captioning, and the second is VIT. X-LLaVA is trained in the same manner, and the details of the two phases are described in Section 4.3  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Enriching the LLM Vocabulary",
            "text": "In the LLaVA model, when querying in Korean for the LLaMA2-13B language model, issues arise, such as responses in English or English-Korean code-switching. This stems from a problem with the tokenizer, where 89.7% is in Latin script, while Korean only constitutes 0.37%, leading to insufficient Korean expressiveness and biases in the pretraining data owing to lexical bias. To address these issues, we expanded the Korean vocabulary in the LLaMA2 and conducted additional pretraining for knowledge infusion. (Figure 2  ###reference_### (b), (c))\nVocabulary expansion involves adding 7,478 words from the KoBERT111https://github.com/SKTBrain/KoBERT vocabulary to the LLaMA2 tokenizer. And we randomly initialize embeddings for these newly added words. Ultimately, the proposed tokenizer possessed a dictionary of 39,478 entries. As a subsequent step, the model was further enhanced with knowledge information using English Wikipedia data  and Korean Wikipedia data . Through this process, our model learns representations for the newly added vocabulary. If the pretraining dataset (7.8GB) is defined as , then the loss function  is expressed as follows.\nHere,  is the size of ,  denotes the number of tokens in -th data sample .  represents -th token of sequence , and  represents the sequence of tokens before the -th token. In this context,  is the causal language modeling loss function, where  denotes the model parameters."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "X-LLaVA",
            "text": "In this section, we describe the method for training X-LLaVA using the LLaMA2 model, which has proceeded word expansion and bilingual dictionary pretraining, as previously introduced X-LLaVA, like LLaVA, is trained in two stages: image-language connection via captioning and multilingual VIT. However, unlike LLaVA1.5, to efficiently conduct multilingual training, we follow the cross-lingual language model pretraining method  Conneau and Lample (2019  ###reference_b7###), simultaneously utilizing a mix of English and Korean for training.\nIn the first stage, we train only the projection layer  using the image-caption datasets LLaVA-CC3M Liu et al. (2023b  ###reference_b25###)  and its machine-translated Korean counterpart, LLaVA-KoCC3M. This stage involves representation learning in which image representations are converted into word embeddings that are comprehensible to the LLaMA2. During this process, both Korean and English are learned concurrently while simultaneously aligning [image-English-Korean]. We define the dataset for Stage-1 as .\nIn the second stage, we conducted VIT on X-LLaVA to enhance its capabilities as a multilingual visual assistant. For VIT as described in  Liu et al. (2023b  ###reference_b25###), we use the LLaVA instruct dataset (158K, ), its machine-translated counterpart (158K, ), and the mvif dataset (91K, ) generated in Section 3  ###reference_###. In this stage, unlike the first stage, we train the projection layer and language model simultaneously. Define the dataset for Stage-2 training as . The formula for training the Stage-2 can be expressed as follows:\nWhere ,  represents the total number of conversation turns. In Stage 1,  because the dataset  is composed of a single turn. In Stage 2,  is also true in all case, except for multi-turn conversations.\nIn the dataset , which can be either  or  depending on the stage, , , and  denote the -th component of the image, the question (instruction) in turn , and the answer in turn , respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Quantitative Evaluation",
            "text": "In this section, we describe the quantitative evaluation methods and criteria for the proposed X-LLaVA. Through these comparisons, we aim to address the three research questions proposed in Section 1: (1) What impact does vocabulary expansion, intended to enhance multilinguality, have on vision-language models? and (2) How does bilingual training affect the relationship between these two languages? and (3) Which aspects of the model were strengthened by utilizing our proposed mvif data?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiment Environments",
            "text": "To ensure a fair comparison of LMMs, we must define task selection for evaluation and specify the LMM model used for evaluation. Below are the benchmark datasets used for evaluation, with the following characteristics for each benchmark:\n\n(English) VQA2.0: A dataset containing open-ended questions about images Goyal et al. (2017), GQA: A VQA-format dataset considered Scene Graph Hudson and Manning (2019), LV (LLaVAw from Liu et al. (2023b)) and POPE Yifan Li and Wen (2023).\n\n(Korean) KoViz: A VQA-format dataset and KoLiv: A VQA-format dataset considered Korean culture and daily life Kim et al.\n\n(English-Korean) BVQA Kim et al. (2024): A VQA dataset considering Bilingual Out-side Knowledge.\n\nFor our experiments, we converted the VQA2.0 and BVQA Kim et al. (2024) datasets into the VIF format using the VQA-to-VIF data transformation method proposed in LLaVA1.5. Following this conversion, we proceeded with VIT over all the training sets from the proposed benchmark in only one epoch. The evaluation methodology and prompts were adopted directly as proposed in LLaVA1.5 (See Appendix C). Experimental environments and answers generated for each model were made publicly accessible222github.com/AnonymousMercy/NACCL_submit to ensure reproducibility and facilitate comparison of the models."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Intrinsic Evaluation of X-LLaVA",
            "text": "An intrinsic evaluation was conducted to explore the three research questions we proposed. To achieve this, we train the three models under different conditions. Table 2 lists the training environments and performances of the three models. X-LLaVA refers to the model that underwent both vocabulary expansion and knowledge enhancement as well as the VIT proposed in Section 4. X-LLaVA(-P) is a model created to compare the effects of pretraining methods on Koreans and English data proposed in Section 4.2. This model is a version of X-LLaVA that does not utilize Wiki for pretraining during its training phase. X-LLaVA(-V,-P) represents a model that neither underwent vocabulary expansion nor used Wiki for pretraining, essentially using pure LLaMA2. Finally, to assess the impact of the mvif data proposed in Section 3, we compared the results of each model with and without the addition of mvif.\n\nThe influence of Pretraining. A comparison between the X-LLaVA and X-LLaVA(-P) models showed that additional pretraining using Wikipedia uniformly enhanced the performance in both Korean and English, with a particularly notable improvement in Korean. Therefore, the effectiveness of pretraining in Korean and English using Wikipedia was evident.\n\nThe influence of VIT using mvif. When models were tuned with the proposed dataset (+O), a performance improvement ranging from 0.2 to 3 was observed across almost models for the target language. Although the extent of improvement is modest, it is noteworthy that despite the grammatical differences between Korean and English, where knowledge loss might be anticipated, there was an observable enhancement in the English performance. This indicates that multilingual VIF can be expected to improve performance in both less- and high-resource languages."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Extrinsic Evaluation of X-LLaVA",
            "text": "We conducted a comparative evaluation of the performance of our X-LLaVA model in Korean and English against other LMMs. The models compared were BLIP-2, InstructBLIP, LLaVA1.5, and KoLLaVA, and the distinctive features of each model are presented in Table 3.\n\nThe effect of multilingual training.\nTypically, when training languages with different character systems, the performance of a relatively highly resourced language may deteriorate Pires et al. (2019). However, with the multilingual training methods and data (mvif) we proposed, no decrease in performance was observed. During analysis, we observed that X-LLaVA generally performed better on GQA and BVQA, which asked about relationships and knowledge.\n\nComparison of X-LLaVA with KoLLaVA. KoLLaVA is the Korean version of LLaVA1.5, a model trained after automatically translating CC3M, VQA2.0, GQA, and Visual Genome data used in LLaVA1.5. Additionally, it was trained using the Korean version of the BVQA. However, as only the 7B model is currently publicly available, it may be challenging to evaluate the same levels.\n\nComparison X-LLaVA with LLaVA1.5(O or B).\nLLaVA1.5 was trained on about 1.5 times more data (665K VIFs) than X-LLaVA. Nevertheless, BVQA data has never been utilized for training, which may be disadvantageous for the BVQA evaluation. We trained LLaVA1.5 on Korean and English data for three epochs to tune the BVQA for a fair evaluation. The results show a significant improvement in Korean performance on the BVQA. On the other hand, this model, being biased towards VQA data, showed lower performance in the writing evaluation (LV). Conversely, LLaVA1.5(O) in Table 3, a model trained on the LLaVA1.5 with mvif data, exhibited the highest performance on LV."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Qualitative Evaluation",
            "text": "In this section, we describe the qualitative evaluation methods and the results for X-LLaVA. In contrast to quantitative evaluations, which are similar to classification assessments, qualitative evaluations, such as writing evaluations, differ significantly. Although human evaluation may be the fairest approach to qualitative assessments, it is practically challenging. Therefore, in LIMA Zhou et al. (2023  ###reference_b37###), a GPT preference evaluation method that closely resembles human evaluation results was proposed.\nIn our study, we directly employed the GPT preference evaluation method. The process is as follows: First, we input an image and a question into two models being compared to obtain answers A and B. Then, we provided GPT4 with the image, question, and both answers to receive feedback such as ‘Answer A is better’, ‘Answer B is better’, or ‘Both answers are similar’, and measured the proportions. To compare the standing and generation abilities of recent LMMs in vision language, we used the GPT evaluation dataset proposed by LLaVA444‘qa90_gpt4_answer’ at github.com/haotian-liu/LLaVA. However, because this dataset is in English, we translated it into Korean, followed by a review from five annotators to ensure data quality. Afterward, we proceeded with the evaluations."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Preference Evaluation using GPT4-V",
            "text": "###figure_3### ###figure_4### Comparing X-LLaVA with others in Korean.\nFigure 3  ###reference_### presents the results of the GPT preference evaluation for each model. The X-LLaVA model outperformed all other models, except for the GPT4-V model. Notably, it obtained a 19% higher preference rate than the KoLLaVA, indicating the exceptional effectiveness of the proposed methods and datasets in enhancing Korean writing skills.\nComparing X-LLaVA with Others in English. Figure 4  ###reference_### shows the results of English GPT preference evaluations. Interestingly, similar to Korean, the X-LLaVA received approximately 25% higher preference scores for English than LLaVA1.5. This indicates that pretraining of our proposed LLM and mvif datasets can also enhance English writing abilities.\n###figure_5### ###figure_6### X-LLaVA vs GPT4-V. Therefore, does evaluator GPT4-V generate better answers than X-LLaVA? We conducted the evaluations by comparing the GPT4-V and X-LLaVA models. Experimental results show that for both languages, GPT4-V’s answers are preferred over those of X-LLaVA, with a significant performance difference. However, these results stem from GPT4-V generating answers that are more than 30% longer and more verbose compared to LLaVA-based models. This may also be because the GPT rates its own generated content more favorably as it becomes more familiar with it. To mitigate this, in experiments where the answers were limited to 30 words, the results changed significantly, with GPT scoring 42 compared to 17 for X-LLaVA. Detailed statistical analysis related to this can be found in Figure 5  ###reference_### and Appendix E."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Human-assisted Preference Evaluation",
            "text": "As previously described, the performance of GPT preference evaluation may vary according to the number of words. Consequently, a question arises: Can LIMA’s assertion that GPT evaluations are akin to human assessments be extended to the vision-language model proposed in this study? We conducted a human preference evaluation using three human annotators. Figure 6  ###reference_### presents the results of the human evaluation for GPT4-V and X-LLaVA in the comparative assessment, with the response length restricted to 30 words. Although GPT maintained a slight advantage, the preference scores were nearly identical. However, we observed that GPT evaluations resulted in ties 2.9 times more frequently than human evaluations. This observation can be interpreted to suggest that GPT tends to avoid ambiguous decisions compared to humans, who possess relatively clear criteria. Thus, the vision-language model can be considered as augmenting rather than substituting human evaluations. Details supporting this, along with comprehensive human evaluation results and analyses for the entire model, are available in Appendix F."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we propose a framework for constructing data and training models for the efficient multilingual expansion of LMM. For data construction, we suggested a method to easily build multilingual VIF dataset based on the relational metadata between images and objects using GPT4-V. We also demonstrated a framework for efficient multilingual learning, which includes vocabulary enhancement, knowledge reinforcement based on pretraining, and a multilingual VIT framework. The experimental results confirmed that the proposed X-LLaVA model exhibited similar or superior performance compared to existing models that primarily focused on Korean and English as single languages. Finally, our proposed multilingual expansion framework can be trained in 7.5 days with a single A6000 GPU, and the 91K training data can be managed with relatively minimal resources, costing around $3,200."
        }
    ]
}