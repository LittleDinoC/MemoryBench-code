{
    "title": "Exploring Language Model’s Code Generation Ability with Auxiliary Functions",
    "abstract": "Auxiliary function is a helpful component to improve a language model’s code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models’ various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models’ promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model’s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Program synthesis, i.e., writing function code by taking natural language descriptions as inputs, has garnered attention in the research community (Yin and Neubig, 2017; Rahit et al., 2020; Austin et al., 2021; Li et al., 2022). With the help of language modeling, several code-pretrained Large Language Models (LLMs) implement functions with prompts that contain target function signatures (Fried et al., 2023; Nijkamp et al., 2023b, a; Allal et al., 2023; Li et al., 2023; Gunasekar et al., 2023).\n\nAdditional code components, e.g., comment lines (Gao et al., 2023), documents (Zhou et al., 2023c), and other function and class definitions across files (Ding et al., 2023), have been attached to the prompts to boost up their implementation ability.\n\nAuxiliary function is one promising component to improve their code synthesis ability. We define the auxiliary function as a function that handles a subroutine for the target one or performs an easier version of the actual requirements. When this function is included in the prompt, LLMs could call the function to delegate their subroutine or refer to their implementation while synthesizing the target function. However, due to the lack of an evaluation dataset that enables a systematic examination of how these auxiliary functions are utilized, no structured analysis has yet to be conducted.\n\nIn this work, we investigate several LLMs’ ability to utilize auxiliary functions. To do this, we first construct an evaluation dataset, called HumanExtension, which contains human-crafted examples of two functions that are closely related to each other. Specifically, we guided labelers to extend functions in the HumanEval dataset (Chen et al., 2021). We offer software design concepts related to function extension such as subtyping (Liskov and Wing, 1994) to promote labelers to create realistic function relationships. Additionally, the curated examples are parsed into several components to enable robustness evaluation similar to Wang et al. (2023a).\n\nWith the HumanExtension dataset, we conduct systematic analyses to understand how LLMs leverage auxiliary functions. First, we investigate if appending a single auxiliary function to the prompt enhances the likelihood of accurately implementing the target function. Specifically, we design several prompts with auxiliary functions while considering their existence, their functional relevance, and the availability to access auxiliary function implementations. With these prompts, we generate implementations with LLMs and analyze the model behavior focusing on the auxiliary function’s effectiveness, robustness, and the models’ implementation style.\n\nSecond, we examine the cases where LLMs can access multiple auxiliary functions for synthesizing target functions. The randomly sampled auxiliary functions are additionally included in the prompts to verify whether LLMs can selectively use the appropriate one. Similar to Liu et al. (2023b), we inspect whether the position of a relevant function affects their code generation ability. This investigation is combined with the implementation style analysis to permit an in-depth analysis through the lens of the auxiliary function call.\n\nOur experimental results show current LLMs’ capabilities to utilize auxiliary function and their limitations. First, most LLMs exhibit large performance improvement with proper relevant auxiliary functions. Also, for some advanced LLMs, our evaluation process sheds light on their self-improving behavior by implementing the two functions in a step-by-step manner. However, the ability to utilize auxiliary functions is varied depending on the factors that do not change their functionality, which raises a question about their robustness. In addition, our implementation style analysis results reveal that the models prefer repeating the internal logic in the auxiliary function even when the logic can be easily handled by simply calling them. Finally, our human preference evaluation of their style shows this disparity between model-generated implementation and that of humans, suggesting the future direction of enhancing the ability to delegate their subroutine to the auxiliary functions by calling them."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Several studies have been conducted to evaluate code generation ability (Xu et al., 2022  ###reference_b36###).\nNeelakantan et al. (2016  ###reference_b26###); Iyer et al. (2018  ###reference_b16###) first introduce neural networks into code completion tasks and evaluate them on traditional metrics, e.g., BLEU.\nChen et al. (2021  ###reference_b6###) propose the HumanEval dataset and show LLMs can generate functionally correct implementations by introducing a functional correctness evaluation process.\nConcurrently, Austin et al. (2021  ###reference_b2###) propose the MBPP dataset for Python basic programs and Hendrycks et al. (2021  ###reference_b13###) release the APPS dataset related to coding contest problems.\nConsecutive studies have proposed datasets targeted for realistic purposes.\nLai et al. (2023  ###reference_b18###) focused on data science problems and Wang et al. (2023b  ###reference_b35###) paid attention to realistic coding queries from StackOverflow and Yu et al. (2024  ###reference_b38###) aimed at Python and Java code generation tasks from real-world open-source projects, and Babe et al. (2023  ###reference_b3###) concentrated on beginning programmers.\nThese work are combined and included in several coding benchmarks (Lu et al., 2021  ###reference_b24###; Khan et al., 2023  ###reference_b17###; Ni et al., 2023  ###reference_b27###).\nFor the metrics, Dong et al. (2023  ###reference_b8###) propose CodeScore to estimate functional correctness and Zhou et al. (2023b  ###reference_b42###) propose CodeBERTScore that utilizes BERTScore (Zhang et al., 2020  ###reference_b39###).\nThere exists research work that extends the HumanEval dataset to support other features.\nCassano et al. (2022  ###reference_b5###); Zheng et al. (2023  ###reference_b40###) extend the dataset to support multiple programming languages and Liu et al. (2023a  ###reference_b22###) propose the HumanEval+ dataset that extends their test case to enable rigorous evaluation of functional correctness.\nWang et al. (2023a  ###reference_b34###) focused on prompt robustness by extending the HumanEval dataset.\nHowever, an evaluation procedure that enables systematic analysis of how LLMs leverage auxiliary functions has yet to be released in code generation tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "We manually construct a variety of coding examples with corresponding auxiliary functions.\nTo do this, we treat the Python examples in the HumanEval dataset as our base auxiliary functions and employ human experts to create an extended function for each example.\nWe guide them to produce functions that have additional functionalities compared to the given functions.\nThe following aspects are considered to remove the ambiguity inside the concept of extension and enhance their quality.\nThere exist two different types of extension, i.e., black-box extension and white-box extension.\nThe black-box extension extends a function by calling the auxiliary function.\nIt does not consider the internal mechanism of the auxiliary function.\nHowever, the white-box extension extends them by rewriting the improved internal mechanism.\nWe allow any type of extension, but recommend the black-box one as calling the existing functions if possible is mostly better than rewriting the whole mechanism (Fowler, 2018  ###reference_b9###).\nWe show the Liskov-substitution principle and the concept of subtyping (Liskov and Wing, 1994  ###reference_b21###) to the labelers.\nIn doing so, we expect that the curated function could be treated as an extended version of the given function from the software engineering point of view.\nWe filtered out some examples in the HumanEval dataset that are not appropriate for using auxiliary functions.\nWe removed the examples that provide the same functionality embedded in Python built-in functions, e.g., sum_to_n, as it already serves through the Python features.\nAlso, the examples that are semantically duplicated with other examples are excluded from the final evaluation set.\nFor example, if the two functions handle the same logic to process symbols but accept brackets or parentheses as their inputs, one of them is removed.\nWe collect 151 problems representing a function pair that one function extends the other and name it HumanExtension.\nAdditionally, we mechanically parse these code snippets and create features for components for future usage."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We comprehensively evaluate LLMs’ ability to harness auxiliary functions using our HumanExtension dataset. To do this, we designed research questions as follows. RQ1: Could LLMs properly and robustly utilize different types of auxiliary functions? RQ2: How do LLMs’ implementations vary when they access relevant auxiliary functions? RQ3: Do current training methodologies enhance the ability to utilize auxiliary functions? We first examine the effectiveness and robustness of including a single auxiliary function in the prompt and extend this setting into multiple auxiliary functions. Also, we explore their implementation styles and analyze them based on human preference. We collect several LLMs pre-trained on code described as follows. Incoder is the early open-source decoder-only generative language model pretrained on public codes and StackOverflow questions and answers. CodeGen is another open-source language model pretrained on public codes. We use two versions where “Multi” represents pre-training on multiple programming languages and “Mono” is additionally trained on Python codes from the \"Multi\" checkpoint. BigCode releases two checkpoints, i.e., SantaCoder and StarCoder, pretrained on public codes. They adopt various data-cleaning techniques to enhance the quality of the training corpus. CodeLLaMA is a variant of LLaMA2 additionally pretrained on code corpus. CodeLLaMAPython and CodeLLaMAInstruct are further trained on Python codes and instruction following datasets, respectively. We follow the decoding strategy for LLMs consistent with the existing benchmark. We use nucleus sampling with top-p 0.95 and low-temperature scaling, i.e., 0.2, focusing on the correctness of the generated implementation. The models generate at most 512 tokens for each prompt and stop generation when either end of sequence token or predefined stop sequences, i.e., \"\\ndef\", \"\\nclass\", \"\\nif\", \"\\n#\", are generated. The implementations generated by the models are evaluated on functional correctness based on the corresponding test cases. Specifically, an implementation is regarded as functionally correct when it passes all the corresponding test cases. Whole models exhibit remarkable improvement when they access the human-written relevant auxiliary functions (Table 1, Oracle). It implies that most LLMs could utilize the proper relevant auxiliary function. The improvement is observed even for the most recent competitive model, i.e., CodeLLaMAPython 34B, indicating assisting code synthesis with auxiliary function is still a valid approach even as the model size grows. Considering the \"Step-by-step\" column in Table 1, the model-written relevant auxiliary functions contribute to the improvement for some advanced LLMs. CodeLLaMA series, StarCoder, CodeGenMono series, and Incoder 6B properly utilize the auxiliary function written by themselves. It suggests that the models can improve their codes if we provide a two-step plan in the form of function signatures. We attach one successful example that calls the generated auxiliary function during target implementation in Figure 1(b). In this sense, this approach is similar to the Least-to-Most prompting that solves target tasks with the model-generated answer of predefined subtasks. We observe that providing an irrelevant auxiliary function brings meaningful improvement on few models. To investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step. In Figure 2, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement. However, since the given auxiliary function is not relevant to the target function (Figure 1(a)), no implementation pattern that directly utilizes the auxiliary function is found. On the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty (Figure 1(b)). Therefore, we conclude there exists a unique advantage of providing relevant auxiliary function although the irrelevant one is helpful to some extent. We investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions. To do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython. In these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA. Comparing CodeGenMono 2B and CodeGenMulti 2B, the pass rate is similar when no auxiliary function is provided (Direct), but the pass rate of CodeGenMono becomes significantly higher than that of CodeGenMulti when we provide an appropriate auxiliary function (Oracle). Additionally, in the Step-by-step setting, CodeGenMono models show meaningful improvement while CodeGenMulti could not. In the case of CodeLLaMA, CodeLLaMAPython models show higher pass rates in the whole model size. From these experimental evidences, we conclude that additional learning with Python code enhances the ability to utilize auxiliary functions. We speculate that the Python codes"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Single auxiliary function experiment",
            "text": "We measure the effectiveness of an auxiliary function in a code synthesis task by designing several prompts varying their existence and type. Currently, the prompt used in the existing work to solve the task is mainly composed of a target function signature with the corresponding import statements (Ben Allal et al., 2022; Cassano et al., 2022; Chen et al., 2021). We attached the auxiliary function signature and their implementation between the import statements and the target function signature to allow LLMs to access the knowledge about auxiliary functions. Our prompts with several types of auxiliary functions are described as follows.\n\nNo auxiliary function (Direct): Prompt consists of a target function signature without auxiliary functions. This setting acts as a baseline in our experiments. Human-written irrelevant auxiliary function (Irrelevant): We attached an irrelevant auxiliary function written by humans in the prompt. We constructed an auxiliary function pool with the canonical solutions in the HumanEval dataset (Chen et al., 2021) and sampled an irrelevant function from the pool to construct the prompt.\n\nModel-written relevant auxiliary function (Step-by-step): We utilize the relevant auxiliary function written by the model in the prompt. Concretely, LLMs first synthesize relevant auxiliary function and then it is attached to the prompt for implementing the target function. Note that only a relevant auxiliary function signature without their implementation is additionally required for this setting.\n\nHuman-written relevant auxiliary function (Oracle): We provide a relevant auxiliary function written by humans to the model. The corresponding canonical solutions in the HumanEval dataset are used for human-written relevant auxiliary functions. We consider this setting as an oracle because these functions are currently the best in terms of quality and understandability. The details about function signature, e.g., type annotation and docstring format, are consistent with the format curated in Cassano et al. (2022).\n\nWe collect several LLMs pre-trained on code described as follows. Incoder (Fried et al., 2023) is the early open-source decoder-only generative language model pretrained on public codes and StackOverflow questions and answers. CodeGen (Nijkamp et al., 2023b) is another open-source language model pretrained on public codes. We use two versions where “Multi” represents pre-training on multiple programming languages and “Mono” is additionally trained on Python codes from the \"Multi\" checkpoint. BigCode (Allal et al., 2023; Li et al., 2023) releases two checkpoints, i.e., SantaCoder and StarCoder, pretrained on public codes. They adopt various data-cleaning techniques to enhance the quality of the training corpus. CodeLLaMA (Rozière et al., 2023) is a variant of LLaMA2 (Touvron et al., 2023) additionally pretrained on code corpus. CodeLLaMAPython and CodeLLaMAInstruct are further trained on Python codes and instruction following datasets, respectively.\n\nWe follow the decoding strategy for LLMs consistent with the existing benchmark (Ben Allal et al., 2022). We use nucleus sampling (Holtzman et al., 2020) with top-p 0.95 and low-temperature scaling, i.e., 0.2, focusing on the correctness of the generated implementation. The models generate at most 512 tokens for each prompt and stop generation when either end of sequence token or predefined stop sequences, i.e., \"\\ndef\", \"\\nclass\", \"\\nif\", \"\\n#\", are generated. The implementations generated by the models are evaluated on functional correctness based on the corresponding test cases. Specifically, an implementation is regarded as functionally correct when it passes all the corresponding test cases.\n\nWe observe that providing an irrelevant auxiliary function brings meaningful improvement on few models. To investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step. In Figure 2, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement. However, since the given auxiliary function is not relevant to the target function, no implementation pattern that directly utilizes the auxiliary function is found. On the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty.\n\nWe investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions. To do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython. In these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA. Comparing CodeGenMono 2B and CodeGenMulti 2B, the pass rate is similar when no auxiliary function is provided (Direct), but the pass rate of CodeGenMono becomes significantly higher than that of CodeGenMulti when"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Performance analysis",
            "text": "We report the performances and the relative improvement compared with the one without auxiliary function in Table 1 and compare them to identify the effectiveness of different auxiliary functions. Whole models exhibit remarkable improvement when they access the human-written relevant auxiliary functions (Table 1, Oracle). It implies that most LLMs could utilize the proper relevant auxiliary function. The improvement is observed even for the most recent competitive model, i.e., CodeLLaMAPython 34B, indicating assisting code synthesis with auxiliary function is still a valid approach even as the model size grows.\n\nConsidering the \"Step-by-step\" column in Table 1, the model-written relevant auxiliary functions contribute to the improvement for some advanced LLMs. CodeLLaMA series, StarCoder, CodeGenMono series, and Incoder 6B properly utilize the auxiliary function written by themselves. It suggests that the models can improve their codes if we provide a two-step plan in the form of function signatures. We attach one successful example that calls the generated auxiliary function during target implementation in Figure 1(b). In this sense, this approach is similar to the Least-to-Most prompting (Zhou et al., 2023a) that solves target tasks with the model-generated answer of predefined subtasks.\n\nWe observe that providing an irrelevant auxiliary function brings meaningful improvement on few models. To investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step. In Figure 2, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement. However, since the given auxiliary function is not relevant to the target function (Figure 1(a)), no implementation pattern that directly utilizes the auxiliary function is found. On the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty (Figure 1(b)). Therefore, we conclude there exists a unique advantage of providing relevant auxiliary function although the irrelevant one is helpful to some extent.\n\nWe investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions. To do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython. In these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA. From these experimental evidences, we conclude that additional learning with Python code enhances the ability to utilize auxiliary functions. We speculate that the Python codes used for training contain relevant functions in the same file and the model is trained to jointly consider the functions within the same context.\n\nWe also compare CodeLLaMAInstruct models to determine whether the instruction tuning affects the ability to harness auxiliary functions. In order to use an instruction-tuned model, instructions written in natural language and a prompt template are additionally required. To this end, we apply an approach similar to HumanEvalPack (Muennighoff et al., 2023), where the instructions are automatically generated from the original prompt. We combine these instructions with the CodeLLaMAInstruct template to create a prompt. The prompt is formulated into two consecutive turns where the first turn is about the auxiliary function and the second one is about generating the target function. Our empirical results show that CodeLLaMAInstruct models perform better than CodeLLaMA models when implementing functions without auxiliary functions (Table 1, Direct), which is consistent with previous findings (Rozière et al., 2023). On the other hand, when an appropriate auxiliary function is provided in the prompt (Table 1, Oracle), the base models show better performance than the instruction-tuned models. In addition, the relative improvement in the Step-by-step settings has prominently decreased compared to that of the base models. This suggests that the ability to utilize other functions in the context has been weakened during the instruction tuning process. Therefore, it is necessary to develop an advanced instruction-tuning methodology to incorporate the previously implemented functions, which is our future work."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Robustness analysis",
            "text": "We check whether the model could properly use the given relevant auxiliary function after some components inside the function have been perturbed. We apply two perturbations: (1) replacing the name of the auxiliary function with other function names in the HumanEval dataset or (2) deleting the docstring included in the function. Note that the functionality of the auxiliary function itself does not change because we did not change the function implementation or its input/output format. \n\nThe experimental results show that even if the functionality of the function does not change, a performance drop is observed depending on the name of the function or the existence of a docstring. The lack of a docstring had a greater impact than renaming the function, and it is natural in that the docstring contains a more detailed description of its functionality. Despite their usefulness, we want to highlight that LLMs have to understand the function without docstring for their realistic use cases as most practical codes do not include them. In bigcode/the-stack-smol, 70.5% of Python functions do not have docstring. The performance drop was not alleviated even when the model size was increased or the model was additionally trained with Python codes. Therefore, there is a need to propose a robust learning methodology that can reduce performance differences caused by such perturbations."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Implementation style analysis",
            "text": "We analyze the generated implementation based on their style and compare preferences between them. In this experiment, we use the implementations generated under the Oracle setting. To identify their implementation style, we apply Python static parser and check whether they called the given auxiliary function. The implementations that call the auxiliary function are regarded as black-box style while the rest as white-box style. The black-box style directly utilizes the auxiliary function as is, while the white-box style mimics the internal mechanism of the auxiliary function.\n\nFurther investigating the two different styles, we conduct a human pairwise preference evaluation with human-written implementations (Human), and model-written ones with both styles (Black box and White box). We created a labeling sheet with 17 examples that CodeLLaMAPython 34B implements in both styles correctly. We recruited labelers who have been coding with Python for over five years. For the three possible pairs, labelers were instructed to choose the better implementations according to their preference such as performance or readability.\n\nThe evaluation results show that implementations that call auxiliary functions are preferred over implementations that do not. After inspecting the result qualitatively, we interpret that most black box implementations were selected due to their clarity and conciseness coming from appropriately delegating subroutines to auxiliary functions. Usually, the model-generated white-box implementations tend to repeat the identical mechanism inside the auxiliary function, which is not preferred in software engineering fields. In few cases, white-box implementations are preferred over black-box ones as they are considered as over-engineering. Therefore, training the models to delegate the subroutine to other functions suitably would be the next step for generating realistic code."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Multiple auxiliary function experiment",
            "text": "We provide several auxiliary functions in the prompt and study whether the model selectively utilizes the appropriate auxiliary function. For CodeLLaMA models, the performance improved when the related function was located at the first or the last. This result is consistent with the existing findings (Liu et al., 2023b) that, in natural language processing tasks, LLMs can effectively utilize relevant documents when they are located at the beginning or end.\n\nOn the other hand, for CodeLLaMAPython models, this trend was weakened, and improvement was noted only when the relevant function was located at the end. We conjectured that the two related functions were usually located adjacently in Python codes, and this pattern was learned by the model. However, since the location of relevant functions is independent of their functionality, LLMs need to be tuned to robustly utilize them regardless of where they are placed.\n\nWe found a strong correlation between the proportion of black-box style implementations and model effectiveness. The Pearson correlation scores between the proportion and performance are larger than 0.9, indicating that LLMs perform better when they use appropriate auxiliary functions. However, the black-box style implementations are mostly observed when the relevant auxiliary functions are located at the end.\n\nFor CodeLLaMA models, they can use the relevant function if it is located at the beginning. Model scaling and additional training with Python codes provide a marginal effect on promoting a model to generate black-box style implementations. This suggests that specialized training for LLMs to call relevant functions, similar to invoking general LLMs to use tools (Schick et al., 2023), is needed for enhancing their code synthesis ability."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Experimental setup",
            "text": "We design a prompt with nine auxiliary functions followed by the target function signature. The functions consist of one relevant auxiliary function and the others are randomly sampled from the auxiliary function pool used in the Irrelevant setting. We change the location of the relevant function in the prompt and measure the proportion of black-box style implementations classified as the existence of auxiliary function call."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Result",
            "text": "The experimental results are shown in Figure 5 and we list up empirical findings we observed.\n\nFor CodeLLaMA models, the performance improved when the related function was located at the first or the last. This result is consistent with the existing findings that, in natural language processing tasks, LLMs can effectively utilize relevant documents when they are located at the beginning or end.\n\nOn the other hand, for CodeLLaMAPython models, the performance increased only when the relevant function was located at the end. We conjectured that the two related functions were usually located adjacently in Python codes and this pattern was learned by the model. However, since the location of relevant functions is independent of their functionality, LLMs need to be tuned to robustly utilize them regardless of where they are placed.\n\nThere exists a strong correlation between certain scores and the proportion of black-box style implementations. The Pearson correlation scores between the proportion and the pass rate are larger than 0.9, indicating that LLMs get higher scores when they try to call appropriate auxiliary functions. However, the black-box style implementations are mostly observed when the relevant auxiliary functions are located at the last.\n\nFor CodeLLaMA models, they can call the relevant function if they are located at the first. Model scaling and additional training with Python codes provide a marginal effect on promoting a model to generate black-box style implementations, suggesting that specialized training for LLMs to call relevant functions similar to invoking general LLMs to use tools is needed for enhancing their code synthesis ability."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have explored the ability to utilize auxiliary functions encoded in the LLMs through our newly proposed HumanExtension dataset. The HumanExtension dataset is constructed to contain function relationships while considering software engineering concepts. Our multi-faceted experiments with the HumanExtension dataset comprehensively show the current LLMs’ ability to harness auxiliary functions.\n\nOur auxiliary function experiments demonstrate that the LLMs have the ability to utilize auxiliary functions even when the function is implemented by themselves. However, our in-depth analysis discovered that their ability varies depending on factors that humans might not, i.e., the position of relevant functions, function name, and docstring. Furthermore, our implementation style analysis reveals that, in some cases, the LLMs repeat the mechanism of the given auxiliary function while humans simply call the auxiliary functions, suggesting the future research direction of current code LLMs for auxiliary function calls."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Although the curated dataset in this study allowed us to evaluate the ability to utilize auxiliary functions from a variety of perspectives, it has some limitations in determining whether multiple relevant auxiliary functions can be jointly utilized.\nAdditionally, our behavioral analyses indicate that the capabilities have been empirically observed, but it might be insufficient to conclude the model truly understands and utilizes the auxiliary function, so additional methods are required to reinforce the statement."
        }
    ]
}