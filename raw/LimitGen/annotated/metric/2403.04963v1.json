{
    "title": "An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment",
    "abstract": "Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced large language models (LLMs), evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs.\n\nHowever, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs’ simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models’ performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation’s reliability.\n\nTo address these problems, this study provides in-depth insights into LLMs’ performance while ensuring the reliability of the evaluation. We design an error-based human annotation framework to assess the GPT-4’s simplification capabilities. Results show that GPT-4 generally generates fewer erroneous simplification outputs compared to the current state-of-the-art. However, LLMs have their limitations, as seen in GPT-4’s struggles with lexical paraphrasing.\n\nFurthermore, we conduct meta-evaluations on widely used automatic metrics using our human annotations. We find that while these metrics are effective for significant quality differences, they lack sufficient sensitivity to assess the overall high-quality simplification by GPT-4.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Sentence simplification automatically rewrites sentences to make them easier to read and understand by modifying their wording and structures, without changing their meanings. It helps people with reading difficulties, such as non-native speakers (Paetzold, 2016), individuals with aphasia (Carroll et al., 1999), dyslexia (Rello et al., 2013b, a), or autism (Barbu et al., 2015). Previous studies often employed a sequence-to-sequence model, then have been enhanced by integrating various sub-modules into it (Zhang and Lapata, 2017; Zhao et al., 2018; Nishihara et al., 2019; Martin et al., 2020). Recent developments have seen the rise of large language models (LLMs). Notably, the ChatGPT families released by OpenAI demonstrate exceptional general and task-specific abilities (OpenAI, 2023; Wang et al., 2023; Liu et al., 2023), and sentence simplification is not an exception. Some studies (Feng et al., 2023; Kew et al., 2023) have begun to evaluate LLMs’ performance in sentence simplification, including both automatic scoring and conventional human evaluations where annotators assess the levels of fluency, meaning preservation, and simplicity (Kriz et al., 2019; Jiang et al., 2020; Alva-Manchego et al., 2021; Maddela et al., 2021), or identify common edit operations (Alva-Manchego et al., 2017). However, these studies face limitations and challenges. Firstly, it is unclear whether the current automatic metrics are suitable for evaluating simplification abilities of LLMs. Although these metrics have demonstrated variable effectiveness across conventional systems (e.g., rule-based (Sulem et al., 2018b), statistical machine translation-based (Wubben et al., 2012; Xu et al., 2016), and sequence-to-sequence model-based simplification (Zhang and Lapata, 2017; Martin et al., 2020)) through their correlation with human evaluations (Alva-Manchego et al., 2021), their suitability for LLMs has yet to be explored, thereby their effectiveness in assessing LLMs’ simplifications are uncertain. Secondly, given the general high performance of LLMs, conventional human evaluations may be too superficial to capture the subtle yet critical aspects of simplification quality. This lack of depth undermines the interpretability when evaluating on LLMs. Recently, Heineman et al. (Heineman et al., 2023) proposed a detailed human evaluation framework for LLMs, categorizing linguistically based success and failure types. However, their linguistics-based approach appears to be excessively intricate and complex, resulted in low consistency among annotators, thus raising concerns about the reliability of the evaluation. The trade-off between interpretability and reliability underscores the necessity for a more balanced approach. Our goal is to make a clear understanding of LLMs’ performance on sentence simplification, and to reveal whether current automatic metrics are genuinely effective for evaluating LLMs’ simplification ability. We design an error-based human evaluation framework to identify key failures in important aspects of sentence simplification, such as inadvertently increasing complexity or altering the original meaning. Our approach aligns closely with human intuition by focusing on outcome-based assessments rather than linguistic details. This straightforward approach makes the annotation easy without necessitating a background in linguistics. Additionally, we conduct a meta-evaluation of automatic evaluation metrics to examine their effectiveness in measuring the simplification abilities of LLMs by utilizing data from human evaluations. We applied our error-based human evaluation framework to evaluate the performance of GPT-4 in English sentence simplification, using prompt engineering on three representative datasets on sentence simplification: Turk (Xu et al., 2016), ASSET (Alva-Manchego et al., 2020), and Newsela (Xu et al., 2015). The results indicate that GPT-4 generally surpasses the previous state-of-the-art (SOTA) in performance; GPT-4 tends to generate fewer erroneous simplification outputs and better preserve the original meaning, while maintaining comparable levels of fluency and simplicity. However, LLMs have their limitations, as seen in GPT-4’s struggles with lexical paraphrasing. The meta-evaluation results show that existing automatic metrics face difficulties in assessing simplification generated by GPT-4. That is, these metrics are able to differentiate significant quality differences but not sensitive enough for overall high-quality simplification outputs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Related Work",
            "text": "Our study evaluates the performance of the advanced LLM, namely GPT-4, in sentence simplification by comparing it against the SOTA supervised simplification model. This section includes a review of current evaluations of LLMs in this domain, along with an overview of the SOTA supervised simplification models."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Evaluation of LLM-based Simplification",
            "text": "In sentence simplification, some studies attempted to assess the performance of LLMs. For example, Feng et al. (Feng et al., 2023) evaluated the performance of prompting ChatGPT and GPT-3.5, later Kew et al. (Kew et al., 2023) compared LLMs varying in size, architecture, pre-training methods, and with or without instruction tuning. Additionally, Heineman et al. (Heineman et al., 2023) proposed a detailed human evaluation framework for LLMs, categorizing 21 linguistically based success and failure types. Their findings indicate that OpenAI’s LLMs generally surpass the previous SOTA supervised simplification models.\n\nHowever, these studies have three primary limitations. First, there has not been a comprehensive exploration into the capabilities of the most advanced ChatGPT model to date, i.e., GPT-4. Second, these studies do not adequately explore prompt variation, employing uniform prompts with few-shot examples across datasets without considering their unique features in simplification strategies. This may underutilize the potential of LLMs, which are known to be prompt-sensitive. Third, the human evaluations conducted are inadequate. Such evaluations are crucial, as automatic metrics often have blind spots and may not always be entirely reliable (He et al., 2023). Human evaluations in these studies often rely on shallow ratings or edit operation identifications to evaluate a narrow range of simplification outputs. These methods risk being superficial, overlooking intricate features. In contrast, Heineman et al.’s linguistics-based approach (Heineman et al., 2023) appears to be excessively intricate and complex, resulting in low consistency among annotators, thus raising concerns about the reliability of the evaluations.\n\nOur study aims to bridge these gaps, significantly enhancing the utility of ChatGPT models through comprehensive prompt engineering processes, and incorporating elaborate human evaluations while ensuring reliability."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. SOTA Supervised Simplification Models",
            "text": "Traditional NLP methods heavily relied on task-specific models, which involve adapting pre-trained language models for various downstream applications. In sentence simplification, Martin et al. (2022) introduced the MUSS model by fine-tuning BART with labeled sentence simplification datasets and/or mined paraphrases. Similarly, Sheang et al. (2021) fine-tuned T5, referred to as Control-T5 in this study, achieving state-of-the-art performance on two representative datasets: Turk and ASSET. These models leverage control tokens, initially introduced by ACCESS, to modulate attributes like length, lexical complexity, and syntactic complexity during simplification. This approach allows any sequence-to-sequence model to adjust these attributes by conditioning on simplification-specific tokens, facilitating strategies that aim to shorten sentences or reduce their lexical and syntactic complexity. Our study employs Control-T5 as a state-of-the-art model and compares it to GPT-4 in sentence simplification."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Datasets",
            "text": "In this study, we employed standard datasets for English sentence simplification, as detailed below. For replicating the SOTA supervised model, namely, Control-T5, we used the same training datasets as the original paper. Meanwhile, the evaluation datasets were used to assess the performance of our models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Training Datasets",
            "text": "We utilized training sets from two datasets: WikiLarge (Zhang and Lapata, 2017 ###reference_b47###) and Newsela (Xu et al., 2015 ###reference_b44###; Zhang and Lapata, 2017 ###reference_b47###). WikiLarge consists of complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia by sentence alignment. Introduced by Xu et al. (Xu et al., 2015 ###reference_b44###), Newsela originates from a collection of news articles accompanied by simplified versions written by professional editors. It was subsequently aligned from article-level to sentence-level, resulting in approximately complex-simple sentence pairs. In our study, we utilize the training split of the Newsela dataset made by Zhang and Lapata (Zhang and Lapata, 2017 ###reference_b47###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Evaluation Datasets",
            "text": "We used validation and test sets from the three standard datasets on English sentence simplification. These validation sets were used for prompt engineering on GPT-4. Table 1 shows the numbers of complex-simple sentence pairs in these sets. These datasets have distinctive features due to differences in simplification strategies as summarized below.\n\nTurk (Xu et al., 2016): This dataset comprises sentences from English Wikipedia, each paired with eight simplified references written by crowd-workers. It is created primarily focusing on lexical paraphrasing.\n\nASSET (Alva-Manchego et al., 2020): This dataset uses the same source sentences as the Turk dataset. It differs from Turk by aiming at rewriting sentences with more diverse transformations, i.e., paraphrasing, deleting phrases, and splitting a sentence, and provides simplified references written by crowd-workers.\n\nNewsela (Xu et al., 2015; Zhang and Lapata, 2017): This is the same Newsela dataset described in Section 3.1. We utilized its validation and test splits, totaling sentence pairs. After careful observation, we found that deletions of words, phrases, and clauses predominantly characterize the Newsela dataset."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Models",
            "text": "To enhance the performance of GPT-4 in sentence simplification, we undertook extensive prompt engineering effort. We also replicated the SOTA supervised model, Control-T5, for comparative analysis with GPT-4. Throughout our optimization efforts, we conducted various evaluations and analyses. Our meta-evaluation confirms alignment with human evaluation for the simplification tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. GPT-4 with Prompt Engineering",
            "text": "Scaling pre-trained language models, such as increasing model and data size, LLMs enhance their capacity for downstream tasks. Unlike earlier models that required fine-tuning, these LLMs can be effectively prompted with zero- or few-shot examples for task-solving."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Design",
            "text": "Aiming to optimize GPT-4’s sentence simplification capabilities, we conducted prompt engineering based on three principal components:\n\nDataset-Specific Instructions: We tailored instructions to each dataset’s unique features and objectives, as detailed in Section 3.2. For the Turk and ASSET datasets, we created instructions referring to the guidelines provided to the crowd-workers who composed the references. In the case of Newsela, where such guidelines are unavailable, we created instructions following the styles used for Turk and ASSET, with an emphasis on deletion. Refer to the Appendix A.1 for detailed instructions.\n\nVaried Number of Examples: We varied the number of examples to attach to the instructions: zero, one, and three.\n\nVaried Number of References: We experimented with single or multiple (namely, three) simplification references used in the examples. For Turk and ASSET, which are multi-reference datasets, we manually selected one high-quality reference from their multiple references. Newsela, which is basically a single-reference dataset, offers multiple simplification levels for the same source sentences. For this dataset, we extracted references targeting different simplicity levels of the same source sentence as multiple references.\n\nWe integrated these components into prompts, resulting in the creation of variations (Figure 1). These prompts were then applied to each validation set, excluding selected examples. Following this, we used the best prompts to generate simplification outputs from the respective test sets."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Effect of Prompt Engineering",
            "text": "Prompt engineering demonstrates its effectiveness. As shown in Table 2, across three validation sets, there is a significant performance difference between prompts with the highest and lowest scores, achieving top results for Turk, ASSET, and Newsela. Moreover, results reveal a direct alignment between the best prompt’s instructional style and its respective dataset. These top-performing prompts all use a few-shot examples of three.\n\nThe optimal number of simplification references varies; Turk and ASSET show strong results with a single reference, whereas Newsela benefits from multiple references, likely due to the intricacies involved in ensuring meaning is preserved amidst deletions.\n\nOverall, prompt engineering notably enhances GPT-4’s sentence simplification output."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Replicated Control-T5",
            "text": "We replicated the Control-T5 model (Sheang and Saggion, 2021). We started by fine-tuning the T5-base model (Raffel et al., 2020) with the WikiLarge dataset and then evaluated it on the ASSET and Turk’s test sets. Unlike the original study, which did not train on or evaluate on Newsela, we incorporated this dataset. We employed Optuna (Akiba et al., 2019) for hyperparameter optimization, a method consistent with the approach used in the original study with the WikiLarge dataset. This optimization process focused on adjusting the batch size, the number of epochs, the learning rate, and the control token ratios. We refer the reader to Appendix A.2 for the optimal model configuration we achieved."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Human Evaluation",
            "text": "Automatic metrics provide a fast and cost-effective way for evaluating simplification but struggle to cover all the aspects; they are designed to capture only specific aspects such as the similarity between the output and a reference. Furthermore, the effectiveness of some automatic metrics has been challenged in previous studies (Sulem et al., 2018a  ###reference_b38###; Alva-Manchego et al., 2021  ###reference_b6###). Human evaluation, which is often viewed as the gold standard evaluation, may be a more reliable method to determine the quality of simplification.\nAs we discuss in detail in the following section, achieving a balance between interpretability and consistency among annotators is a challenge in sentence simplification. To address this challenge, we have crafted an error-based approach and made efforts in the annotation process such as mandating discussions among annotators to achieve consensus and implementing strict checks to ensure the quality of assessments.\nAs annotators, we used second-language learners with advanced English proficiency expecting that they are more sensitive to variations in textual difficulty based on their language-learning experiences.\nIn addition, given that second language learners stand to benefit significantly from sentence simplification applications, involving them as evaluators seems most appropriate.\nAll of our annotators were graduate students associated with our organization. The compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nFor quality control, annotators had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and four complex-simple sentence pairs. Each pair contains various errors predefined by the author. All submissions to this test were manually reviewed. Three annotators were selected for this task based on their high accuracy in identifying errors, including specifying the error type, location, and rationale.\nSame with Task , we employed second-language learners with advanced English proficiency, who were graduate students associated with our organization.\nAnnotator candidates had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and five complex-simple sentence pairs. Candidates were instructed to rate fluency, meaning preservation, and simplicity on each simplification output. Three annotators were selected based on their high inter-annotator agreement, demonstrated by the Intraclass Correlation Coefficient (ICC) (Shrout and Fleiss, 1979  ###reference_b37###) score of , indicating a substantial agreement.\nThe compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nWe assessed inner-annotator agreement through the overlapping rate of ratings across three annotators, as detailed in Table 4  ###reference_###. The overlapping rate was calculated by the proportion of identical ratings for a given simplification output.555We also tried Fleiss’ kappa, Krippendorff’s alpha, and ICC; however, they resulted in degenerate scores due to too-high agreements on mostly binary judgments. In the fluency dimension, both models demonstrate strong agreement, with overlapping rates between  and . In meaning preservation and simplicity, these dimensions exhibit comparably more variability in ratings, with a broader range of agreement. Annotators found it more subjective to assess meaning preservation and simplicity, as these aspects required direct comparison with the source sentences. Nevertheless, mid to high agreement levels were still achieved, showing the consistency of our annotation."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Our approach: Error-based Human Evaluation",
            "text": ""
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1. Challenge in Current Human Evaluation",
            "text": "Sentence simplification is expected to make the original sentence simpler while maintaining grammatical integrity and not losing important information. A common human assessment approach involves rating sentence-level automatic simplification outputs by comparing them to source sentences in three aspects: fluency, meaning preservation, and simplicity (Kriz et al., 2019  ###reference_b19###; Jiang et al., 2020  ###reference_b16###; Alva-Manchego et al., 2021  ###reference_b6###; Maddela et al., 2021  ###reference_b24###). However, sentence simplification involves various transformations, such as paraphrasing, deletion, and splitting, which affect both the lexical and structural aspects of a sentence. Sentence-level scores are difficult to interpret; they do not clearly indicate whether the transformations simplify or complicate the original sentence, maintain or alter the original meaning, or are necessary or unnecessary. Therefore, such evaluation approach falls short in comprehensively assessing the models’ capabilities.\nThis inadequacy has led to a demand for more detailed and nuanced human assessment methods. Recently, the SALSA framework, introduced by Heineman et al. (Heineman et al., 2023  ###reference_b15###) aimed to provide clearer insights through comprehensive human evaluation, consider both the successes and failures of a simplification system. This framework categorizes transformations into  linguistically-grounded\nedit types across conceptual, syntactic, and lexical dimensions to facilitate detailed evaluation. However, due to the detailed categorization, it faces challenges in ensuring consistent interpretations across annotators. This inconsistency frequently leads to low inter-annotator agreement, thereby undermining the reliability of the evaluation. We argue that such extensive and fine-grained classifications are difficult for annotators to understand, particularly those without a linguistic background, making it challenging for them to keep consistency."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2. Error-based Human Evaluation",
            "text": "To overcome the trade-off between interpretability and consistency in evaluations, we design our error-based human evaluation framework. Our approach focuses on identifying and evaluating key failures generated by advanced LLMs in important aspects of sentence simplification. We aim to cover a broad range of potential failures while making the classification easy for annotators. Our approach reduces the categories to seven types while ensuring comprehensive coverage of common failures. In the study on sentence simplification evaluation of LLMs conducted by Kew et al. (Kew et al., 2023  ###reference_b17###), while the annotation of common failures is also incorporated, it is noteworthy that the types of failures addressed are very limited and they selected only a handful of output samples for annotation.\nWhile not intended for LLM-based simplification, a few previous studies have incorporated error analysis to assess their sequence-to-sequence simplification models (Kriz et al., 2019  ###reference_b19###; Cooper and Shardlow, 2020  ###reference_b9###; Maddela et al., 2021  ###reference_b24###).\nStarting from the error types established in these studies, we included ones that might also applicable in the outputs of advanced LLMs.\nSpecifically, we conducted a preliminary investigation on ChatGPT-3.5 simplification outputs on the ASSET dataset.444At the time of this investigation, GPT-4 was not publicly available.\nAs a result, we adopted errors of Altered Meaning, issues with Coreference, Repetition, and Hallucination, while omitted errors deemed unlikely, such as the ungrammatical error.\nAdditionally, we identified a new category of error based on our investigation: Lack of Simplicity.\nWe observed that ChatGPT-3.5 often opted for more complex expressions rather than simpler ones, which is counterproductive for sentence simplification. Recognizing this as a significant issue, we included it in our error types. We also refined the categories for altered meaning and lack of simplicity by looking into the specific types of changes they involve. Instead of listing numerous transformations like the SALSA framework (Heineman et al., 2023  ###reference_b15###), we classified these transformations into two simple categories based on their effects on the source sentence: lexical and structural changes. This categorization leads to four error types: Lack of Simplicity-Lexical, Lack of Simplicity-Structural, Altered Meaning-Lexical, and Altered Meaning-Structural.\nTable 3  ###reference_### summarizes the definition and examples of our target errors.\nOur approach is designed to align closely with human intuition by focusing on outcome-based assessments rather than linguistic details. Annotators evaluate whether the transformation simplifies and keeps the meaning of source components, preserves named entities accurately, and avoids repetition or irrelevant content. This methodology facilitates straightforward classification without necessitating a background in linguistics.\nThe simplified sentence uses more intricate lexical expression(s) to replace part(s) of the original sentence.\nFor Rowling, this scene is important because it shows Harry’s bravery…\nRowling considers the scene significant because it portrays Harry’s courage…\nThe simplified sentence modifies the grammatical structure, and it increases the difficulty of reading.\nThe other incorporated cities on the Palos Verdes Peninsula include…\nOther cities on the Palos Verdes Peninsula include…, which are also incorporated.\nSignificant deviation in the meaning of the original sentence due to lexical substitution(s).\nThe Britannica was primarily a Scottish enterprise.\nThe Britannica was mainly a Scottish endeavor.\nSignificant deviation in the meaning of the original sentence due to structural changes.\nGimnasia hired first famed Colombian trainer Francisco Maturana, and then Julio César Falcioni.\nGimnasia hired two famous Colombian trainers, Francisco Maturana and Julio César Falcioni.\nA named entity critical to understanding the main idea is replaced with a pronoun or a vague description.\nSea slugs dubbed sacoglossans are some of the most…\nThese are some of the most…\nUnnecessary duplication of sentence fragments\nThe report emphasizes the importance of sustainable practices.\nThe report emphasizes the importance, the significance, and the necessity of sustainable practices.\nInclusion of incorrect or unrelated information not present in the original sentence.\nIn a short video promoting the charity Equality Now, Joss Whedon confirmed that ”Fray is not done, Fray is coming back.\nJoss Whedon confirmed in a short promotional video for the charity Equality Now that Fray will return, although the story is not yet finished."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Annotation Process",
            "text": "###figure_2### We implemented our error-based human evaluation alongside the common evaluation on fluency, meaning preservation, and simplicity using a 1-3 Likert scale. The Potato Platform (Pei et al., 2022  ###reference_b32###) was utilized to establish our annotation environment for the execution of both tasks. The annotation interface for Task  is illustrated in Figure 2  ###reference_###.\nAnnotators select the error type, marking erroneous spans in the simplified sentence and, when applicable, the corresponding spans in the original (source) sentence. Note that the spans of different error types can overlap each other.\nEach annotator received individual training through a -hour tutorial, which covered guidelines and instructions on how to use the annotation platform. Our error-based human evaluations include guidelines that define and provide examples of each error type, as outlined in Table 3  ###reference_###. Additionally, detailed guidelines for Likert scale evaluation can be found in the Appendix B  ###reference_###.\nAs annotators, we used second-language learners with advanced English proficiency expecting that they are more sensitive to variations in textual difficulty based on their language-learning experiences.\nIn addition, given that second language learners stand to benefit significantly from sentence simplification applications, involving them as evaluators seems most appropriate.\nAll of our annotators were graduate students associated with our organization. The compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nFor quality control, annotators had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and four complex-simple sentence pairs. Each pair contains various errors predefined by the author. All submissions to this test were manually reviewed. Three annotators were selected for this task based on their high accuracy in identifying errors, including specifying the error type, location, and rationale.\nSame with Task , we employed second-language learners with advanced English proficiency, who were graduate students associated with our organization.\nAnnotator candidates had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and five complex-simple sentence pairs. Candidates were instructed to rate fluency, meaning preservation, and simplicity on each simplification output. Three annotators were selected based on their high inter-annotator agreement, demonstrated by the Intraclass Correlation Coefficient (ICC) (Shrout and Fleiss, 1979  ###reference_b37###  ###reference_b37###) score of , indicating a substantial agreement.\nThe compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nWe assessed inner-annotator agreement through the overlapping rate of ratings across three annotators, as detailed in Table 4  ###reference_###  ###reference_###. The overlapping rate was calculated by the proportion of identical ratings for a given simplification output.555We also tried Fleiss’ kappa, Krippendorff’s alpha, and ICC; however, they resulted in degenerate scores due to too-high agreements on mostly binary judgments. In the fluency dimension, both models demonstrate strong agreement, with overlapping rates between  and . In meaning preservation and simplicity, these dimensions exhibit comparably more variability in ratings, with a broader range of agreement. Annotators found it more subjective to assess meaning preservation and simplicity, as these aspects required direct comparison with the source sentences. Nevertheless, mid to high agreement levels were still achieved, showing the consistency of our annotation."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1. Task : Error Identification",
            "text": "Task  follows our error-based human evaluation detailed in Section 5.1  ###reference_###. We sampled  source sentences from Turk, ASSET, and Newsela test sets, along with simplification outputs generated by GPT-4 and Control-T5, resulting in a total of  complex-simple sentence pairs. The annotation period spanned from October  to February .\nAnnotators were instructed to identify and label errors within each sentence pair according to predefined guidelines. To overcome the trade-off between detailed granularity and annotator agreement, all annotators involved in this task participated in discussion sessions led by one of the authors. These sessions required annotators to share their individual labelings, which were then collectively reviewed during discussions until reaching the consensus.\nThere were eight discussion sessions, each lasting approximately three hours, for a total of  hours.\nAs annotators, we used second-language learners with advanced English proficiency expecting that they are more sensitive to variations in textual difficulty based on their language-learning experiences.\nIn addition, given that second language learners stand to benefit significantly from sentence simplification applications, involving them as evaluators seems most appropriate.\nAll of our annotators were graduate students associated with our organization. The compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nFor quality control, annotators had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and four complex-simple sentence pairs. Each pair contains various errors predefined by the author. All submissions to this test were manually reviewed. Three annotators were selected for this task based on their high accuracy in identifying errors, including specifying the error type, location, and rationale."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2. Task : Likert Scale Rating",
            "text": "Following the convention of previous studies, we also include the rating approach on fluency, meaning preservation, and simplicity using a 1 to 3 Likert scale as Task . In this task, annotators evaluate all simplification outputs generated by GPT-4 and Control-T5 across three test sets, by comparing them with their corresponding source sentences. In particular, for the Newsela dataset, reference simplifications from the test set were also included. We assume that models trained or tuned on this dataset which is characterized by deletion, may produce shorter outputs, potentially impacting meaning preservation scores. To ensure fairness, we compare human evaluation of model-generated simplifications against that of Newsela reference simplifications for a more objective evaluation. The evaluation in Task  covered a total of  complex-simple sentence pairs.\nThe annotation period spanned from October  to February .\nTo address the challenge of annotator consistency, we implemented specific guidelines during the annotation phase. Annotators were advised to avoid neutral positions (‘2’ on our scale) unless faced with genuinely challenging decisions. This approach encouraged a tendency towards binary choices, i.e., ‘1’ for simplification outputs that are disfluent, lose a lot of original meaning, or are not simpler, and ‘3’ for simplification outputs that are fluent, preserve meaning, and are much simpler.\nTo ensure quality, one of the authors reviewed  pairs of sampled submissions from each annotator. If any issues were identified, such as an annotator rate inconsistently for sentence pairs with similar problems, they were required to revise and resubmit their annotations.\nSame with Task , we employed second-language learners with advanced English proficiency, who were graduate students associated with our organization.\nAnnotator candidates had to pass a qualification test before participating in the task. This qualification test comprises annotation guidelines and five complex-simple sentence pairs. Candidates were instructed to rate fluency, meaning preservation, and simplicity on each simplification output. Three annotators were selected based on their high inter-annotator agreement, demonstrated by the Intraclass Correlation Coefficient (ICC) (Shrout and Fleiss, 1979  ###reference_b37###  ###reference_b37###  ###reference_b37###) score of , indicating a substantial agreement.\nThe compensation rate for this task was set at   JPY (approximately $ USD) per sentence pair.\nWe assessed inner-annotator agreement through the overlapping rate of ratings across three annotators, as detailed in Table 4  ###reference_###  ###reference_###  ###reference_###. The overlapping rate was calculated by the proportion of identical ratings for a given simplification output.555We also tried Fleiss’ kappa, Krippendorff’s alpha, and ICC; however, they resulted in degenerate scores due to too-high agreements on mostly binary judgments. In the fluency dimension, both models demonstrate strong agreement, with overlapping rates between  and . In meaning preservation and simplicity, these dimensions exhibit comparably more variability in ratings, with a broader range of agreement. Annotators found it more subjective to assess meaning preservation and simplicity, as these aspects required direct comparison with the source sentences. Nevertheless, mid to high agreement levels were still achieved, showing the consistency of our annotation."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Annotation Result Analysis",
            "text": "Our comprehensive analysis of annotation data revealed that, overall, GPT-4 generates fewer erroneous simplification outputs compared to Control-T5, demonstrating higher ability in simplification. GPT-4 excels in maintaining the original meaning, whereas Control-T5 often falls short in this dimension. Nevertheless, GPT-4 is not without flaws; its most frequent mistake involves substituting lexical expressions with more complex ones.\nFrédéric Chopin’s Opus 57 is a berceuse for solo piano.\nFrédéric Chopin wrote a piece called Opus 57 for solo piano.\nFrédéric Chopin’s Opus 57 is a lullaby for a single piano.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat, whom the PAD accused of being proxies for Thaksin.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat. The PAD said that Thaksin was a friend of the PAD.\nThe PAD asked for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej and Somchai Wongsawat, whom the PAD accused of being representatives for Thaksin."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Analysis of Task 1: Error Identification",
            "text": "This section presents a comparative analysis of erroneous simplification outputs generated by GPT-4 and Control-T5, focusing on error quantification and type analysis.\nWe assessed erroneous simplification outputs across three datasets: Turk, ASSET, and Newsela, defining an erroneous output as one containing at least one error.\nOverall, Table 5  ###reference_### shows that GPT-4 with fewer errors than Control-T5 across all datasets. This performance difference underscores GPT-4’s superior performance in simplification tasks.\nIn the following, we reveal characteristics of errors in GPT-4 and Control-T5 simplification outputs."
        },
        {
            "section_id": "6.1.1",
            "parent_section_id": "6.1",
            "section_name": "6.1.1. Error Co-occurrence",
            "text": "Multiple types of error may co-occur in a simplification output. Consider the following example where simplification was generated by Control-T5 model:\nCHICAGO – In less than two months, President Barack Obama is expected to choose the site for his library.\nCHICAGO – In less than two months, he will choose a new library.\nIn this example, Coreference and Altered Meaning-Lexical errors co-occur. The simplification process replaces President Barack Obama with he, leading to a coreference error. Additionally, the phrase the site for his library is oversimplified to a new library thus altering the original meaning.\nWe found that on average, GPT-4-generated erroneous simplification outputs contained  unique errors, while Control-T5-generated ones contained . This was calculated by dividing the sum of unique errors in each erroneous simplification output by the total number of these simplification outputs. This suggests that erroneous simplification outputs typically include just one error type on both Control-T5 and GPT-4."
        },
        {
            "section_id": "6.1.2",
            "parent_section_id": "6.1",
            "section_name": "6.1.2. Distribution of Same Errors",
            "text": "Section 6.1.1  ###reference_.SSS1### indicates that an erroneous simplification output contains a unique error type on average, then what is the distribution of the same error types in those simplification outputs?\nThe same type of error can occur multiple times within a single simplification output. Consider the following example where the simplification output was generated by GPT-4 model:\nIn 1990, she was the only female entertainer allowed to perform in Saudi Arabia.\nIn 1990, she was the sole woman performer permitted in Saudi Arabia.\nIn this example, Lack of Simplicity-Lexical error appears twice. The simplification uses more difficult words, sole and permitted, to replace only and allowed, respectively.\nWe plotted the label-wise distribution for both models, as illustrated in Figure 3  ###reference_###. This figure shows that in both models, each type of error occurs once in most of the erroneous simplification outputs, and only a small fraction of simplification outputs exhibit the same error type more than once. The maximum repetition of the same error type is capped at three.\n###figure_3###"
        },
        {
            "section_id": "6.1.3",
            "parent_section_id": "6.1",
            "section_name": "6.1.3. Characteristic Errors in Models",
            "text": "We quantitatively assessed the frequency of different error types in simplification outputs generated by GPT-4 and Control-T5. The results, presented in Table 6  ###reference_###, indicate differences in error tendencies between the two models.\nGPT-4’s errors are predominantly from Lack of Simplicity-Lexical ( occurrences) and Altered Meaning-Lexical ( occurrences), showing its propensity to employ complex lexical expressions or misinterpret meanings through lexical choices.\nControl-T5, in contrast, displays a broader range of main error types, with notably higher frequencies in Altered Meaning-Lexical ( occurrences), Coreference ( occurrences), and Hallucination ( occurrences). This suggests difficulties with preserving original lexical meanings, ensuring referential clarity, and avoiding irrelevant information for Control-T5.\nOn both models, the occurrences of errors in lexical aspect (Lack of Simplicity-Lexical, Altered Meaning-Lexical, Coreference, Repetition) surpass the occurrences of errors in structural aspect (Lack of Simplicity-Structural, Altered Meaning-Structural) as a general tendency.\nFurther analysis on Newsela suggests dataset-specific challenges. In comparison to results on Turk and ASSET, Control-T5 generated significantly more Coreference errors than GPT-4. After a manual inspection, we found that a possible reason is the high occurrence of coreference within this dataset and Control-T5 tends to overfit during fine-tuning."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. Likert Scale Rating",
            "text": "In this section, we compared model performances across various dimensions and datasets by averaging annotators’ ratings summarized in Table 7  ###reference_###.\nAs a general tendency, GPT-4 again consistently outperforms Control-T5 across all datasets, indicating a preference among annotators for the GPT-4’s simplification quality.\nFor fluency, both models demonstrate high fluency levels, indicated by the average ratings approach three. This suggests that both GPT-4 and Control-T5 generated grammatically correct simplifications without significant differences in fluency.\nIn terms of meaning preservation, GPT-4 outperforms Control-T5 across all three datasets: achieving scores of  versus  on Turk,  versus  on ASSET, and  versus  on Newsela. Conversely, for simplicity, GPT-4’s ratings are slightly lower than those of Control-T5, though the disparities are less pronounced than those observed for meaning preservation:  versus  on Turk,  versus  on ASSET, and  versus  on Newsela. This contrast suggests that Control-T5 may be slightly better at generating simpler outputs but at the cost of losing a significant portion of the original meaning.\nOn the Newsela dataset, we observed that Control-T5 often simply deletes long segments of the source sentence, leaving only trivial changes to the remaining parts. Below is an example:\nObama said that children today still fail to meet their full potential, even if there are no mobs with bricks and bottles to ward young black women away from higher education.\nEven if there are no mobs with bricks and bottles to keep young black women from higher education.\nObama said that children aren’t reaching their full potential, even when there are no threats stopping them from getting a better education.\nIn this example, Control-T5 removed the extensive segment preceding ‘even if…’, resulting in the loss of significant information. In contrast, GPT-4 retained the majority of the information, resulting in higher scores on meaning preservation.\nThe last column of Table 7  ###reference_### shows average ratings for reference simplifications in the Newsela test set, which reveals that Newsela’s references permit to sacrifice of meaning preservation for simplicity.\nControl-T5 might have adopted a deletion-heavy approach during its training on the Newsela dataset, even heavier than the deletion degree of the Newsela dataset itself. It adversely affects its ability to preserve the original sentence’s meaning."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3. Discussion: Other Issues Resolved by LLM",
            "text": "Some Control-T5-generated simplification outputs are reported by our annotators to fail in meeting the satisfaction criteria. These cases include issues such as redundancy, lack of common sense, and inappropriate changes in focus, which were not noted with GPT-4’s simplification outputs. This section provides an enumeration of such cases, alongside examples for each category.\nFrédéric Chopin’s Opus 57 is a berceuse for solo piano.\nFrédéric Chopin wrote a piece called Opus 57 for solo piano.\nFrédéric Chopin’s Opus 57 is a lullaby for a single piano.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat, whom the PAD accused of being proxies for Thaksin.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat. The PAD said that Thaksin was a friend of the PAD.\nThe PAD asked for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej and Somchai Wongsawat, whom the PAD accused of being representatives for Thaksin."
        },
        {
            "section_id": "6.3.1",
            "parent_section_id": "6.3",
            "section_name": "6.3.1. Redundancy",
            "text": "The simplifications introduced redundancies, failing to contribute meaningfully to the simplification of the sentence or to enhance clarity. In the example below,\nControl-T5 replaced ‘biochemist’ with ‘biochemist and scientist’, which seems redundant due to the overlapping parts in meanings. In contrast, GPT-4 avoided this issue.\nTheir granddaughter Hélène Langevin-Joliot is a professor of nuclear physics at the University of Paris, and their grandson Pierre Joliot, who was named after Pierre Curie, is a noted biochemist.\nTheir granddaughter Hélène Langevin-Joliot is a professor of nuclear physics at the University of Paris. Their grandson Pierre Joliot is also a well-known biochemist and scientist.\nTheir granddaughter, Hélène Langevin-Joliot, teaches nuclear physics at the University of Paris. Their grandson, Pierre Joliot, named after Pierre Curie, is a famous biochemist."
        },
        {
            "section_id": "6.3.2",
            "parent_section_id": "6.3",
            "section_name": "6.3.2. Lack of Common Sense",
            "text": "Simplifications that result in logical inconsistencies or nonsensical interpretations. In the example below, Control-T5 illogically suggested that Orton gave birth to his wife, indicating a lack of common sense. In contrast, GPT-4 avoided this issue by correctly suggesting that Orton and his wife welcomed a baby girl, aligning with human common sense understanding.\nOrton and his wife welcomed Alanna Marie Orton on July 12, 2008.\nOrton gave birth to his wife, Alanna Marie, on July 12, 2008.\nOrton and his wife had a baby girl named Alanna Marie Orton on July 12, 2008."
        },
        {
            "section_id": "6.3.3",
            "parent_section_id": "6.3",
            "section_name": "6.3.3. Change of Focus",
            "text": "Simplifications that inappropriately alter the original sentence’s focus, leading to misleading interpretations or factual inaccuracies. In the first example below, Control-T5 shifted the focus from the type of piece Opus 57 is to the mere fact that Chopin composed it, while GPT-4 correctly kept the focus of the source sentence. In the second example, Control-T5 inaccurately replaced ‘being proxies for Thaksin’ with ‘Thaksin was a friend of the PAD’, changing the sentence’s meaning substantially. Conversely, GPT-4 did not appear this kind of inference mistake.\nFrédéric Chopin’s Opus 57 is a berceuse for solo piano.\nFrédéric Chopin wrote a piece called Opus 57 for solo piano.\nFrédéric Chopin’s Opus 57 is a lullaby for a single piano.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat, whom the PAD accused of being proxies for Thaksin.\nThe PAD called for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej, and Somchai Wongsawat. The PAD said that Thaksin was a friend of the PAD.\nThe PAD asked for the resignation of the governments of Thaksin Shinawatra, Samak Sundaravej and Somchai Wongsawat, whom the PAD accused of being representatives for Thaksin."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Meta-Evaluation of Automatic Evaluation Metrics",
            "text": "Due to the high cost and time requirements of human evaluation, automatic metrics are preferred as a means of obtaining faster and cheaper evaluation of simplification models. Previous studies have explored the extent of widely-used metrics in sentence simplification to assess the quality of outputs generated by neural systems (Sulem et al., 2018a; Alva-Manchego et al., 2021; Tanprasert and Kauchak, 2021). However, it remains uncertain whether these metrics are adequately sensitive and robust to differentiate the quality of simplification outputs generated by advanced LLMs, i.e., GPT-4, especially given the generally high performance. To fill this gap, we performed a meta-evaluation of commonly used automatic metrics in both sentence and corpus levels, utilizing our human evaluation data."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "7.1. Automatic Metrics",
            "text": "In this section, we review evaluation metrics that have been widely used in sentence simplification, categorizing them based on their primary evaluation units into two types: sentence-level metrics, which evaluate individual sentences, and corpus-level metrics, which assess the system-wise quality of simplification outputs."
        },
        {
            "section_id": "7.1.1",
            "parent_section_id": "7.1",
            "section_name": "7.1.1. Sentence-level Metrics",
            "text": "LENS (Maddela et al., 2023) is a model-based evaluation metric that leverages RoBERTa (Liu et al., 2019) trained to predict human judgment scores, considering both the semantic similarity and the edits comparing the output to the source and reference sentences. Its values range from  to , where higher scores indicate better simplifications. BERTScore (Zhang et al., 2020) provides similarity scores (precision, recall, and f1) for each token in the candidate sentence against each token in the reference, leveraging BERT’s (Devlin et al., 2019) contextual embeddings. We calculate LENS through the authors’ GitHub implementation and BERTScore using the EASSE package (Alva-Manchego et al., 2019)."
        },
        {
            "section_id": "7.1.2",
            "parent_section_id": "7.1",
            "section_name": "7.1.2. Corpus-level Metrics",
            "text": "BLEU (Papineni et al., 2002) measures string similarity between references and outputs. Derived from the field of machine translation, it is designed to evaluate translation accuracy by comparing the match of n-grams between the candidate translations and reference translations. This metric has been employed to assess sentence simplification, treating the simplification process as a translation from complex to simple language. BLEU scores range from to , with higher scores indicating better quality.\n\nFKGL (Kincaid et al., 1975) evaluates readability by combining sentence and word lengths. Lower values indicate higher readability. The FKGL score starts from and has no upper bound.\n\nWe utilize the EASSE package (Alva-Manchego et al., 2019) to calculate these corpus-level metric scores."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "7.2. Sentence-Level Results",
            "text": "To assess sentence-level metrics’ ability to differentiate simplification quality, we explore the correlation between those metrics and human evaluations using the point-biserial correlation coefficient (Glass and Hopkins, 1995; Linacre, 2008), utilizing the scipy package (Virtanen et al., 2020) for calculation. This coefficient ranges from -1 to 1, where 0 indicates no correlation.\n\nOur analysis aims to assess the efficacy of sentence-level metrics in three aspects:\n\n1. Identification of the presence of errors.\n2. Distinction between high-quality and low-quality simplification overall.\n3. Distinction between high-quality and low-quality simplification within a specific dimension.\n\nGiven the data imbalance between sentences with and without errors, and between high-quality and low-quality simplification, we report our findings using both raw data and downsampled (DS) data to balance the number of class samples."
        },
        {
            "section_id": "7.2.1",
            "parent_section_id": "7.2",
            "section_name": "7.2.1. Identification of the presence of errors",
            "text": "For all simplification outputs in Task, each simplification output was classified as containing errors (labeled as) or no error (labeled as). We then computed the correlation coefficients between these labels and the metric scores. The results indicate that none of the metrics effectively identify erroneous simplifications, as evidenced by point-biserial correlation coefficients being near zero."
        },
        {
            "section_id": "7.2.2",
            "parent_section_id": "7.2",
            "section_name": "7.2.2. Distinction between high-quality and low-quality simplifications overall",
            "text": "We examined simplification outputs in Task . Each output was classified as high quality if it received a high rating from at least two out of three annotators across fluency, simplicity, and meaning preservation, and low quality otherwise. We computed the correlation coefficients between these classifications and the metric scores. As discussed, Newsela is different from other corpora for allowing significant meaning loss to prioritize simplicity. To reduce bias from this dataset, we also calculated the correlation after excluding Newsela, using only Turk and ASSET (denoted as ‘T&A’). For each evaluation, we further divided outputs based on the model (GPT-4 vs. Control-T5) to determine differences in metrics’ capabilities. Results are summarized. For Control-T5 outputs, LENS exhibits some ability to distinguish quality, though with limited effectiveness overall. BERTScores indicate a stronger capability to distinguish quality, with correlations ranging from to , which decrease to when Newsela-derived outputs are removed. For GPT-4 outputs, neither BERTScores nor LENS effectively distinguish quality. These results suggest that while these metrics can detect significant quality variations, they fall short when overall quality is high. Thus, they may not be suitable for evaluating advanced LLMs like GPT-4. Visualization further illustrates this. For GPT-4, regardless of metric used, scores of high and low-quality outputs appear to blend, indicating lack of discriminative capability. In contrast, Control-T5 shows a clear difference, with both LENS and BERTScores giving lower scores to low-quality outputs and higher scores to high-quality ones. This differentiation is particularly pronounced in BERTScores. Note that most low-quality outputs stem from Newsela, making the distinction less apparent when excluding Newsela-derived simplifications."
        },
        {
            "section_id": "7.2.3",
            "parent_section_id": "7.2",
            "section_name": "7.2.3. Distinction between high-quality and low-quality simplifications within a specific dimension",
            "text": "We examined model-generated simplification outputs across individual dimensions. For each dimension, simplification outputs were classified as high quality if they received a high rating from at least two out of three annotators, and low quality otherwise. Based on our classification, on fluency, all the GPT4-generated simplification outputs are high quality, and only five out of Control-T5-generated simplification outputs are low quality. Given that GPT-4 and Control-T5 are rare to generate disfluent outputs, we focus on the dimensions of meaning preservation and simplicity. We then computed the correlation between these ratings and metrics scores.\n\nTable 10(a) indicates results for meaning preservation. Overall, BERTScores demonstrate a stronger correlation with human evaluations for meaning preservation compared to LENS scores. Upon dividing the data by model, BertScores show high correlations for Control-T5 simplification outputs. However, these correlations significantly drop upon the exclusion of simplification outputs derived from Newsela. For GPT-4 simplification outputs, both metrics reveal a negligible correlation. Human evaluation suggests that Control-T5 significantly underperforms in preserving meaning within the Newsela dataset compared to other datasets. While both metrics effectively differentiate substantial differences in meaning preservation, they struggle when overall meaning preservation is high.\n\nTable 10(b) shows results for simplicity, where neither metric correlates well with human evaluations, though LENS slightly outperforms BERTScores. Consequently, they may not be sufficiently sensitive to evaluate advanced LLMs like GPT-4 on meaning preservation and simplicity."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "7.3. Corpus-level Results",
            "text": "Our human evaluations reveal GPT-4’s simplification outputs are generally superior, evidenced by fewer errors, better meaning preservation, and comparable fluency and simplicity to those generated by Control-T5. The metrics’ scores are detailed in Table 11, with the better scores emphasized in bold. To assess the statistical significance of the differences in corpus-level scores, we employed a randomization test (Fisher, 1935) against Control-T5. Better corpus-level scores are masked with an asterisk if statistically significant differences were confirmed.\n\nBLEU significantly favors Control-T5 on ASSET and Newsela, which does not match our human evaluations. Studies have demonstrated that BLEU is unsuitable for simplification tasks, as it tends to negatively correlate with simplicity, often penalizing simpler sentences, and gives high scores to sentences that are close or even identical to the input. Our findings further underscore the limitations of BLEU in evaluating sentence simplification. FKGL ranks Control-T5’s outputs as easier to read compared to those from GPT-4 across all datasets. This aligns with our human evaluation; Control-T5 tends to generate simpler sentence expense of meaning preservation, thereby making the sentence easier to read. However, FKGL’s focus solely on readability, without taking into account the quality of the content or the reference sentences, limits its effectiveness in a comprehensive quality analysis. Previous studies show FKGL is unsuitable for sentence simplification evaluation. Our findings further highlight its limitations in accurately evaluating corpus-level sentence simplification."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "7.4. Summary of Findings",
            "text": "We summarize our findings on the meta-evaluation of existing evaluation metrics for sentence simplification, namely LENS, BERTScores, BLEU, and FKGL. Existing metrics are not capable of identifying the presence of errors in sentences. At sentence level, BERTScores are effective in differentiating the distinct difference between high-quality and low-quality simplification overall as well as on meaning preservation. However, they are not sensitive enough to evaluate the quality of simplification generated by GPT-4. On sentence-level simplicity, neither of the metrics is capable of differentiating high-quality and low-quality simplification."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "In this study, we conducted an in-depth human evaluation of the advanced LLM, specifically the performance of GPT-4, in sentence simplification. Our findings highlight that GPT-4 surpasses the supervised baseline model, Control-T5, by generating fewer erroneous simplification outputs and preserving the source sentence’s meaning better. These results underscore the superiority of advanced LLMs in this task. Nevertheless, we observed limitations, notably in GPT-4’s handling of lexical paraphrasing. Further, our meta-evaluation of sentence simplification’s automatic metrics demonstrated their inadequacy in accurately assessing the quality of GPT-4-generated simplifications.\n\nOur investigation opens up multiple directions for future research. Future studies could investigate how to mitigate lexical paraphrasing issues. For example, it would be worthwhile to explore whether fine-tuning could help. Moreover, there’s a need for more sensitive automatic metrics to properly evaluate the sentence-level quality of simplifications generated by LLMs."
        }
    ]
}