{
    "title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation",
    "abstract": "Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that of human experts on benchmarks that include complex schemas and queries, such as BIRD. This study considers the sensitivity of LLMs to the prompts and introduces a novel approach that leverages multiple prompts to explore a broader search space for possible answers and effectively aggregate them. Specifically, we robustly refine the database schema through schema linking using multiple prompts. Thereafter, we generate various candidate SQL queries based on the refined schema and diverse prompts. Finally, the candidate queries are filtered based on their confidence scores, and the optimal query is obtained through a multiple-choice selection that is presented to the LLM. When evaluated on the BIRD and Spider benchmarks, the proposed method achieved execution accuracies of 65.5% and 89.6%, respectively, significantly outperforming previous ICL-based methods. Moreover, we established a new SOTA performance on the BIRD in terms of both the accuracy and efficiency of the generated queries.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The text-to-SQL task involves translating a natural language question into SQL and is crucial for natural language interfaces to databases (NLIDB) systems. With the recent advancements in large language models (LLMs), in-context learning (ICL)-based approaches for text-to-SQL (Pourreza and Rafiei, 2023a; Gao et al., 2023; Tai et al., 2023) have demonstrated significant performance improvements over traditional fine-tuning methods (Hui et al., 2022; Qi et al., 2022; Li et al., 2023a, b). Notably, Pourreza and Rafiei (2023b) showed that these methods even surpassed gold reference queries in terms of human evaluation within the Spider benchmark.\n\nHowever, for the more challenging BIRD (Li et al., 2023c) benchmark, characterized by its complex database (DB) schemas and queries, the accuracies of ICL-based methods have not exceeded 60%, which is significantly lower than the 93.0% achieved by humans. This gap underscores the need for further advancements in the ICL approach to serve as an NLIDB system.\n\nA significant limitation of LLMs across various tasks is their sensitivity to the structure and content of prompts. Even for semantically identical prompts, LLMs can generate drastically varying responses because of factors such as the order of sentences (Jang and Lukasiewicz, 2023; Wang et al., 2023b), choice of demonstration examples (Liu et al., 2022; Nori et al., 2023), and the sequence in which these examples are presented (Lu et al., 2022). Our experiments confirmed a similar tendency in the text-to-SQL context, where alterations in the schema presentation (section 5.3) and the choice of few-shot examples (section 5.4) resulted in variations in LLM outputs.\n\nIn this study, to improve the accuracy and robustness of LLMs for text-to-SQL, we introduce a novel approach that leverages multiple prompts to generate various candidate answers and effectively aggregates them. We utilize the sensitivity of LLMs to prompts to explore a broader search space for answers using different prompts.\n\nAs shown in Figure 1, the SQL generation process comprises three steps: schema linking, multiple SQL generation, and selection. Initially, the schema-linking phase robustly selects tables and columns relevant to the question from the DB schema using multiple prompts. Subsequently, the generation phase employs various prompts to produce diverse candidate SQL queries, ensuring a broader exploration of potential queries. Finally, the selection phase filters candidate queries based on confidence scores, and the optimal query is selected through multiple-choice selection (MCS) that is presented to the LLM."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Prompt engineering, which is the study of designing effective prompts, is an active research area as prompts significantly impact the performance of LLMs across various NLP tasks.\nA prominent example is the chain-of-thought (CoT) prompting (Wei et al., 2022  ###reference_b29###), which employs manually crafted examples to guide the LLM to generate intermediate reasoning steps prior to deriving the answer. This technique has demonstrated significant performance enhancements across various tasks. Kojima et al. (2022  ###reference_b10###) further demonstrated that LLMs can explain their reasoning steps even in the absence of human-annotated examples.\nIn addition to CoT, self-consistency decoding (Wang et al., 2022  ###reference_b26###) has been proposed, wherein multiple answers are sampled from the LLM before selecting one through a majority vote.\nCompared with greedy decoding, this technique facilitates the exploration of various reasoning paths and has exhibited substantial performance gains. For more effective explorations of the reasoning steps, variations such as tree-of-thought (Wei et al., 2022  ###reference_b29###) and graph-of-thought (Besta et al., 2023  ###reference_b2###) have also been proposed.\nHowever, because these approaches rely on a single prompt to generate the reasoning steps, we hypothesize that they fail to mitigate the issue of LLMs’ high sensitivity to prompts, thereby exploring only a limited search space.\nSeveral studies have shown that even when presented with semantically identical prompts, the outputs of LLMs vary based on factors such as sentence structure (Webson and Pavlick, 2022  ###reference_b28###), sequence of sentences (Jang and Lukasiewicz, 2023  ###reference_b9###; Wang et al., 2023b  ###reference_b27###; Pezeshkpour and Hruschka, 2023  ###reference_b20###), choice of few-shot examples (Liu et al., 2022  ###reference_b15###; Nori et al., 2023  ###reference_b19###), and the order in which the examples are presented (Lu et al., 2022  ###reference_b17###).\nTherefore, we use multiple distinct prompts for a wider exploration of potential answers, thereby mitigating the LLM’s sensitivity issue of LLMs to prompts and ensuring the generation of more robust answers.\nAs ICL-based approaches have shown remarkable performance in text-to-SQL tasks, various studies have focused on creating better prompts for text-to-SQL.\nSeveral studies have focused on applying prompting techniques such as CoT or least-to-most (Zhou et al., 2022  ###reference_b35###) for text-to-SQL generation (Pourreza and Rafiei, 2023a  ###reference_b21###; Tai et al., 2023  ###reference_b24###; Wang et al., 2023a  ###reference_b25###).\nHowever, these methods rely on fixed sets of manually crafted examples, and their performance can vary significantly depending on the selection of these examples.\nIn this work, instead of relying on fixed human-labeled examples, we dynamically select few-shot examples from the training data based on the test sample. Some studies have aimed to determine a more effective few-shot selection strategy for text-to-SQL (Guo et al., 2023  ###reference_b6###; Nan et al., 2023  ###reference_b18###; Gao et al., 2023  ###reference_b5###). However, unlike these studies, which focused on determining a single optimal selection strategy, we employ a parallel approach that employs various few-shot selection strategies with multiple prompts and effectively aggregates them."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "As shown in Figure 1  ###reference_###, the proposed method comprises three steps: (1) schema linking, wherein tables and columns irrelevant to the question from the DB schema are excluded; (2) multiple SQL generation, wherein multiple candidate SQL queries are generated based on various prompts; and (3) selection, wherein the most accurate SQL query is selected from among the candidates."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Schema Linking",
            "text": "Schema linking involves identifying relevant tables and columns from a DB to convert a natural language question into an SQL query (Guo et al., 2019  ###reference_b7###).\nThe introduction of schema linking has significantly improved the performances of both fine-tuning-based (Lei et al., 2020  ###reference_b11###; Xu et al., 2022  ###reference_b31###; Qi et al., 2022  ###reference_b23###; Li et al., 2023a  ###reference_b12###) and ICL-based (Dong et al., 2023  ###reference_b3###; Pourreza and Rafiei, 2023a  ###reference_b21###; Wang et al., 2023a  ###reference_b25###) approaches.\nWe perform schema linking in two steps: first, the tables related to the natural language query are extracted (table linking). Thereafter, the necessary columns within those tables are extracted (column linking).\nWe employ multiple prompts in both phases with the aim of achieving a high recall."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Table Linking",
            "text": "In table linking, the DB schema and question are input into the LLM, which extracts a list of reference tables to generate the SQL query. Inspired by zero-shot-CoT (Kojima et al., 2022  ###reference_b10###), we ask the LLM to explain why each table is necessary, instead of just selecting a list of tables. To easily parse the LLM’s answer, we ask it to respond in the JSON format, as described in Listing LABEL:lst:table-linking.\nTo enhance the robustness of table linking, we utilize multiple prompts. Various studies have demonstrated that LLM outputs are significantly affected by the sequence of input sentences (Jang and Lukasiewicz, 2023  ###reference_b9###; Wang et al., 2023b  ###reference_b27###; Liu et al., 2023  ###reference_b16###). Similarly, our experiments (section 5.3  ###reference_###) revealed that the output of schema-linking output of LLMs also depends on the sequence in which the tables and columns are arranged in the prompts.\nTo minimize the influence of the table order, we randomly shuffle the order of tables, generating  distinct prompts. For each prompt, we obtain  responses from the LLM by using a high sampling temperature. The final table-linking output is derived from the union of all responses, amounting to  table lists. We use a union operation because including unnecessary tables in table linking does not significantly impact the subsequent SQL-generation process; however, omitting the necessary tables prevents the generation of the correct SQL query."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Column Linking",
            "text": "For column linking, we ask the LLM to extract the columns required for converting a question into an SQL query using a prompt similar to that used in table linking.\nThe prompt includes only the schemas of the tables selected during table linking, instead of the entire DB schema.\nBecause the same column name can exist in different tables, we instruct the LLM to provide the answer in the [table_name].[column_name] format.\nSimilar to table linking, the order of the tables and columns is randomly shuffled to generate  unique prompts. Subsequently,  LLM responses are generated for each prompt, where each response represents a selected column list. The column-linking output is the union of all  responses.\nIn the subsequent SQL-generation steps, when providing the DB schema to LLM, only the tables and columns selected through schema linking are provided instead of the full schema."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multiple SQL Generation",
            "text": "To address the sensitivity of the LLM to prompts, we generate various SQL queries based on multiple, distinct prompts. Several studies have demonstrated that the output of an LLM can differ significantly depending on the few-shot examples provided (Liu et al., 2022  ###reference_b15###; Wu et al., 2023  ###reference_b30###), and even on the sequence in which these examples are presented (Lu et al., 2022  ###reference_b17###; Zhao et al., 2021  ###reference_b33###).\nTo effectively leverage this variability, we generate multiple prompts by varying both the selection method of the few-shot examples and the order in which they are presented, thereby ensuring a broader exploration of potential SQL queries."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Few-Shot Examples Selection",
            "text": "For each test sample, a set of few-shot examples is selected from the training dataset. To generate multiple prompts with different examples, we use two distinct selection strategies: one that leverages question similarity and another that utilizes masked question similarity.\nIn the question similarity-based approach, we select the top-k questions from the training dataset that have the nearest sentence embeddings to the natural language question of the test sample.\nSimilarly, the masked question similarity-based approach considers the embedding similarity of masked questions, wherein tokens specific to the DB schema in the question are masked. This masking allows determining the similarity of questions in terms of generating similar queries by disregarding schema-specific content.\nWe employ an LLM for the masking process by presenting it with the DB schema and question, and asking it to replace the table names, column names, and values with special tokens.\nThe prompt for this question masking is presented in Appendix B.2  ###reference_###.\nThrough these two few-shot selection strategies, we generate  different prompts, including one derived exclusively from question similarity, another solely from masked question similarity, and additional prompts created by integrating examples from both strategies in various sequences."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 SQL Generation",
            "text": "As illustrated in Listing LABEL:lst:sql-generation, our SQL-generation prompt includes few-shot examples, a DB schema, sample table contents, and a natural language question.\nThe few-shot examples comprise questions and their corresponding gold SQL pairs. To conserve the limited length of the prompt, we exclude the schema of the target DB for each question.\nRegarding the DB schema, we selectively present only the tables and columns selected during the schema-linking process to avoid burdening the LLM with irrelevant information.\nAdditionally, we embed sample table contents in the CSV format within the prompt to facilitate the LLM’s comprehension of potential values in each column, thereby providing practical insight into the data structure of the DB.\nFinally, we instruct LLM not only to produce the SQL query but also to explain the reasoning behind its generation, thereby enhancing the model’s interpretability and accuracy.\nFor each prompt, we generate  responses from the LLM using a high sampling temperature, resulting in  candidate SQL queries being generated."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Selection",
            "text": "The selection step aims to select the most accurate query from the candidate queries. Initially, the candidate pool is filtered based on confidence scores, and the LLM is then tasked with selecting the most accurate query from the refined pool."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Candidate Filtering",
            "text": "To select the most accurate query among the candidates, we first narrow down the candidate pool. Queries with the same execution results are grouped together, and only the fastest query from each group is retained. Additionally, queries with low confidence scores are excluded from the candidates.\nIn detail, all candidate queries are executed on the DB, and queries that result in syntax errors or timeouts are removed from the candidate pool.\nNext, the confidence score for each query in the candidate pool is calculated, which is determined based on the number of queries that produce the same execution result. Formally, for a candidate pool  containing  queries , the confidence of query  is computed as follows:\nwhere  denotes the execution result for query .\nAmong the queries in candidate pool  with identical execution results, only the query with the minimum execution time is selected as follows:\nwhere  represents the set of all unique execution results from all queries in , and  denotes the execution time for query .\nFinally, all queries in  with confidence score below threshold  are excluded as follows:\nresulting in a refined candidate pool ."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Multiple-Choice Selection (MCS)",
            "text": "Following the filtering process, we utilize the LLM to select the most accurate query among the candidates through a multiple-choice question.\nAs shown in the Listing LABEL:lst:mcq-prompt, we present a set of candidate SQL queries to the LLM and request it to select the most accurate query for a given DB schema and question.\nCandidate queries are provided in descending order of confidence scores, considering the tendency of LLMs to favor options that appear earlier in the multiple-choice questions (Wang et al., 2023b  ###reference_b27###; Zheng et al., 2023  ###reference_b34###).\nThe LLM is required to not only select an SQL query but also provide the reasons for its selection.\nWe sample  responses from the LLM and determine the final SQL query through a majority vote.\n###table_1### ###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Spider (Yu et al., 2018  ###reference_b32###) is a large-scale, complex, cross-domain text-to-SQL benchmark comprising 10,181 questions and 5,693 distinct queries across 200 databases, each with multiple tables. This benchmark requires the model to adapt to an unseen DB schema because different DBs are used for training and testing. BIRD (Li et al., 2023c  ###reference_b14###) is a new large-scale, cross-domain text-to-SQL benchmark comprising 12,751 unique question-SQL pairs across 95 large real-world databases. Compared with Spider, BIRD comprises considerably more complex SQL queries with various SQL keywords (LEFT JOIN, PARTITION BY, etc.) and functions (IIF, CASE, ROUND, etc.) that are not included in Spider. In addition, BIRD requires reasoning using external knowledge (such as synonym knowledge and value illustrations) to generate accurate SQL queries. For the BIRD dataset, Li et al. (2023c  ###reference_b14###) proposed an additional evaluation metric called VES that measures the efficiency of a valid model-generated query based on the execution time. A query is considered invalid and assigned a score of zero if its execution result differs from that of the gold SQL. Therefore, VES considers both the model accuracy and efficiency for the generated query. uses the zero-shot prompt provided in OpenAI’s text-to-SQL demo111https://platform.openai.com/examples/default-sql-translate  ###reference_lt-sql-translate###. classifies the complexity of the question and generates an SQL query by applying different prompts based on the classification result. In each step, it uses a prompt with fixed few-shot examples that are manually written in the CoT style. employs dynamic few-shot examples by considering the similarity of both the questions and the queries. For additional performance improvement, self-consistency (Wang et al., 2022  ###reference_b26###) is introduced. decomposes the question into sub-questions and sequentially generates SQL queries for each sub-question using manually crafted few-shot samples. Additionally, in case of a syntax error, it uses a prompt to correct the generated query."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Spider (Yu et al., 2018) is a large-scale, complex, cross-domain text-to-SQL benchmark comprising 10,181 questions and 5,693 distinct queries across 200 databases, each with multiple tables. This benchmark requires the model to adapt to an unseen DB schema because different DBs are used for training and testing. BIRD (Li et al., 2023c) is a new large-scale, cross-domain text-to-SQL benchmark comprising 12,751 unique question-SQL pairs across 95 large real-world databases. Compared with Spider, BIRD comprises considerably more complex SQL queries with various SQL keywords (LEFT JOIN, PARTITION BY, etc.) and functions (IIF, CASE, ROUND, etc.) that are not included in Spider. In addition, BIRD requires reasoning using external knowledge (such as synonym knowledge and value illustrations) to generate accurate SQL queries."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "For the BIRD dataset, Li et al. (2023c) proposed an additional evaluation metric called VES that measures the efficiency of a valid model-generated query based on the execution time. A query is considered invalid and assigned a score of zero if its execution result differs from that of the gold SQL. Therefore, VES considers both the model accuracy and efficiency for the generated query."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "In all of our experiments, we used the GPT-4 8K as the LLM and text-embedding-ada-002 as the sentence embedding model, which was accessed via Azure OpenAI API. Additionally, we employed the FAISS (Douze et al., 2024) library for the embedding similarity search. \n\nIn schema linking, we used prompts for table linking and prompts for column linking. To generate multiple candidate SQL queries, we used distinct prompts. For each GPT API call, we used a temperature of 1.0 and generated 20 responses.\n\nIn both the SQL-generation and MCS steps, we used 20 question-SQL pairs as few-shot examples. We executed all candidate SQL queries with a timeout of 180s and filtered out queries with a confidence score lower than the threshold of 0.2."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare the proposed MCS-SQL approach with ICL-based methods based on GPT-4.\n\nuses the zero-shot prompt provided in OpenAI’s text-to-SQL demo. \n\nclassifies the complexity of the question and generates an SQL query by applying different prompts based on the classification result. In each step, it uses a prompt with fixed few-shot examples that are manually written in the CoT style.\n\nemploys dynamic few-shot examples by considering the similarity of both the questions and the queries. For additional performance improvement, self-consistency is introduced.\n\ndecomposes the question into sub-questions and sequentially generates SQL queries for each sub-question using manually crafted few-shot samples. Additionally, in case of a syntax error, it uses a prompt to correct the generated query."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Table 1  ###reference_### presents the VES of the proposed and baseline models for the BIRD dev and test sets. The results demonstrate that the proposed approach significantly outperforms existing ICL-based approaches in the metric. Specifically, the proposed method achieved a VES of 71.35% on the holdout test set, surpassing the performance of the previous SOTA ICL-based approach (Wang et al., 2023a  ###reference_b25###) by a significant margin of 3.67%, respectively. Furthermore, the proposed method established a new SOTA performance on the BIRD, surpassing the former SOTA method with a substantial margin of 3.67% in VES."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "Table 1 presents the VES of the proposed and baseline models for the BIRD dev and test sets. The results demonstrate that the proposed approach significantly outperforms existing ICL-based approaches in both metrics. Specifically, the proposed method achieved a VES of 71.35% on the holdout test set, surpassing the performance of the previous SOTA ICL-based approach (Wang et al., 2023a) by significant margins of 3.67%. Furthermore, the proposed method established a new SOTA performance on the BIRD, surpassing the former SOTA method with a substantial margin of 3.67% in VES.\nTable 2 presents the results of the proposed and baseline methods for the Spider dev and test sets. Similar to the results obtained for BIRD, the proposed approach significantly outperforms all existing ICL-based approaches."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "The addition of schema linking to the baseline zero-shot setting resulted in a 2.1% improvement. This underscores the importance of refining the schema prior to SQL generation and shows that the proposed schema-linking process effectively selects relevant tables and columns. The inclusion of the sample table contents in the prompt further amplified this gain by +2.4%. The introduction of dynamic few-shot examples that were selected based on masked question similarity resulted in the largest performance improvement of +4.8%. Moreover, when we sampled multiple answers from the LLM using the same prompt and employed the proposed MCS method, the performance further improved by 2.1%. This demonstrates the capability of the proposed SQL selection method in discerning and selecting the most accurate query from a set of candidates. Finally, introducing multiple prompts led to further enhancements of +1.3%, particularly showing significant performance improvement on challenging queries. This improvement demonstrates that broadening the search space using various prompts significantly boosted the SQL-generation accuracy.\n\nTable 4 lists the ablation results for the Spider dev set, wherein it is evident that each component of the proposed approach contributed to significant performance gains. This consistent performance enhancement across different benchmarks confirms the effectiveness and adaptability of the proposed approach for the text-to-SQL task."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Impact of Using Multiple Prompts in Schema Linking",
            "text": "We conducted a comparative analysis of the following two cases to investigate the impact of using multiple prompts in schema linking: (1) greedy decoding with a single prompt; and (2) taking the union of multiple answers generated from a single prompt. Table 5 lists the recall of schema linking for each case, which was calculated based on whether the predicted list of tables and columns included those used in the gold query.\n\nThe results demonstrate that sampling multiple responses using the same prompt and aggregating them led to notable performance gains of +15.8% for BIRD and +4.7% for Spider. These results indicate that the order of tables and columns in the prompt affects the schema-linking results of the LLM. This improvement was particularly noticeable in BIRD, implying that the effectiveness increases for larger and more complex DB schemas.\n\nAs it is impossible to generate accurate SQL queries in subsequent processes if the necessary tables or columns are omitted in schema linking, using the union of various responses is pivotal for enhancing the SQL-generation performance."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Impacts of Different Few-shot Selection Strategies",
            "text": "The performance improved significantly by selecting few-shot examples based on question similarity instead of random selection, with an increase of 2.3% for BIRD and 4.2% for Spider. Additionally, a further performance boost was noted by predicating the selection on the similarity of the masked question rather than the original question, with enhancements of 0.7% and 0.5% for BIRD and Spider, respectively."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Impact of MCS",
            "text": "During the SQL selection phase (section 3.3), we explored whether the proposed MCS via LLM was more effective than a majority vote, which selects the query with the highest confidence score. As presented in Table 7, the proposed MCS approach outperformed the majority vote approach by +0.6% and +0.3% for the BIRD and Spider, respectively. Notably, in the absence of confidence-based filtering, as expressed in Eq. (3), the efficacy of the MCS method decreased significantly. This result underscores the importance of employing confidence-based filtering to effectively narrow down the candidate pool when using MCS."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study introduces a novel method that leverages multiple prompts to enhance the accuracy and robustness of ICL-based text-to-SQL generation. Specifically, the proposed approach performs robust schema linking using distinct prompts. In addition, we employ different few-shot selection strategies to generate multiple query generation prompts, which yield various candidate SQL queries. These candidates are subsequently filtered based on their confidence scores, and the optimal query is selected using the LLM with MCS. Evaluations on the BIRD and Spider benchmarks showed that the proposed approach significantly outperforms existing ICL-based approaches and achieved a new SOTA performance on the BIRD."
        }
    ]
}