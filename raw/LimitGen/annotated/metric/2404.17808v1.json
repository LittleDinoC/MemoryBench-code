{
    "title": "Scaffold-BPE: Enhancing Byte Pair Encoding with Simple and Effective Scaffold Token Removal",
    "abstract": "Byte Pair Encoding (BPE) serves as a foundation method for text tokenization in the Natural Language Processing (NLP) field. BPE iteratively merges the most frequent token pair in the text corpus while keeping all tokens that have been merged in the vocabulary. We propose Scaffold-BPE, which incorporates a dynamic scaffold token removal mechanism by parameter-free, computation-light, and easy-to-implement modifications to the original BPE. This novel approach ensures the exclusion of low-frequency Scaffold Tokens from the token representations for the given texts, facilitating model training. On extensive experiments across language modeling tasks and machine translation tasks, Scaffold-BPE consistently outperforms the original BPE, demonstrating its effectiveness and superiority.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLM) have made a substantial impact on Natural Language Processing (NLP). These models, which are usually extremely large in parameter scales, require efficient management and processing of vast vocabularies during both training and inference stages, posing significant challenges to the communities.\n\nByte Pair Encoding (BPE) was introduced to address this issue by constructing vocabularies. Initially a data compression algorithm, BPE iteratively merges the most frequent pairs of bytes or characters in a dataset until a desired vocabulary size is reached.\n\nBPE's ability to break down words into manageable subword units allows for more flexible and semantically complete representations of input data. This technique helps avoid the out-of-vocabulary problem, where unknown words disrupt the functioning of NLP models. Consequently, BPE has garnered significant attention across the community, becoming a cornerstone for numerous applications such as machine translation, language understanding, and even Large Language Model (LLM) training.\n\nSince its inception, BPE has undergone various modifications to better suit the needs of complex natural language processing tasks. Research has focused on identifying the optimal vocabulary size that BPE should target. These investigations reveal that a carefully calibrated vocabulary size can reduce the computational load while improving the linguistic accuracy of the models. Additionally, some studies have explored the encoding process of BPE to optimize the encoding paths of tokens. Such optimizations capture a wider range of linguistic phenomena and improve model performance across diverse datasets."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Byte Pair Encoding",
            "text": "In the early stages of neural model development, researchers primarily constructed models based on word-level vocabularies [11, 44, 53], which showed considerable success. However, those models often struggled with the challenge of rare word handling due to the inherent limitations of word-level vocabulary size. In response, the academic community has explored numerous innovative strategies for vocabulary development, including methods based on bytes [46], characters [11, 25, 1], and subwords [40, 24]. Among those, Byte Pair Encoding (BPE) [40] stands out for its effective creation of subword vocabulary. Its design philosophy is notably straightforward. During the training process, the corpus is initially split into a sequence of the smallest unit tokens (i.e., character tokens [40] or byte tokens [46]). The algorithm iteratively finds the most frequent token pairs in the sequence, merges them into a new token, and adds it to the vocabulary until it reaches a predetermined size. The vocabulary is then utilized during the encoding phase to represent any text. It reduces token sparsity and enhances feature identification in related words sharing an identical subword, without losing rare words. Recent advancements like BPE-dropout [32] and optimal vocabulary size search [49, 17, 39, 36] continue to enrich BPE development in neural models."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Language Models",
            "text": "Language models are designed to predict the probability distribution of a token sequence. Following GPT-3, which features 175 billion parameters and demonstrates versatility across a wide range of applications, there has been a significant push towards developing large generative language models like Gopher, PaLM, GaLM, OPT, and LLaMA. Such a surge in development has greatly advanced the fields of natural language understanding and generation. This paper demonstrates that by using our Scaffold-BPE algorithm as the tokenizer, language models can achieve a consistent improvement on downstream tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To enhance the performance of the original BPE, we propose Scaffold-BPE to remove the scaffold tokens introduced by the original BPE. Our Scaffold-BPE is simple and straightforward.\n\nIn the training process, the Scaffold-BPE dynamically marks scaffold tokens in the vocabulary at each iteration, and finally yields an expanded vocabulary consisting of both normal tokens with the amount equaling the predetermined vocabulary size and several scaffold tokens. In the encoding process, apart from using the normal tokens, Scaffold-BPE temporarily uses previously marked scaffold tokens as intermediate tokens to merge into longer normal tokens."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Process",
            "text": "The original BPE is trained on a text corpus with a predefined vocabulary size. After training, BPE returns a vocabulary consisting of tokens. For simplicity, the vocabulary is split into a sequence of smallest unit tokens (denoted as single characters/bytes). BPE is trained iteratively. In each iteration, BPE identifies and merges the token pair with the highest frequency into a new token, and includes this new token in the vocabulary. BPE updates the vocabulary by replacing all occurrences of the token pair with the new token and repeats the process. The iterative process can be accelerated using a priority queue. Initially, all token pairs in the vocabulary are pushed into the priority queue in descending order of frequency. After merging a token pair in each iteration, BPE updates the frequency and rank of token pairs related to the merged tokens. With indexed occurrences, there is no need to rescan the vocabulary and recount frequencies for each new iteration. After updating adjacent token pairs, the frequencies of related token pairs are also updated in the queue.\n\nThe Scaffold-BPE expands the vocabulary to an expanded vocabulary and assigns an attribute to each token indicating whether it is a scaffold token. The expanded vocabulary comprises two types of tokens: non-scaffold tokens, which are used in model training, and scaffold tokens, which are not fed into the model nor appear in token representations after encoding. Scaffold tokens serve as intermediate aids in the training and encoding processes of Scaffold-BPE. When calculating the vocabulary size, scaffold tokens are not counted; only non-scaffold tokens are considered.\n\nDuring the training process of BPE, Scaffold-BPE introduces an additional step at the end of each iteration to evaluate tokens using their frequency. This step leverages the inherent mechanism of BPE without additional hyper-parameters, maintaining the system's simplicity. The process ensures that Scaffold-BPE can adaptively identify scaffold tokens at any iteration step. Scaffold tokens are not permanently marked; they have the potential to be reintegrated into the vocabulary in future iterations if their ranking in the priority queue improves."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Encoding Process",
            "text": "The encoding process of the original BPE encodes a text into a token representation using the vocabulary generated by BPE training. Firstly, a sequence of smallest unit tokens (i.e., character/byte tokens) is obtained by splitting the text. Token pairs are iteratively merged to build the final representation.\n\nSimilarly, the modifications of Scaffold-BPE in the encoding process are very simple. Compared to the original BPE, the expanded vocabulary is utilized. Both normal tokens and scaffold tokens are merged according to their rank in the vocabulary. Consequently, during the encoding process, the variety of tokens used actually exceeds the predefined vocabulary size. Scaffold tokens are employed as intermediate tokens to merge into longer tokens, a mechanism termed Scaffolding, as shown in Algorithm 2.\n\nWhen no more token pairs can be merged, the original BPE returns the token sequence as the final result. However, due to the introduction of the Scaffolding mechanism in Scaffold-BPE, the token sequence may contain scaffold tokens from the vocabulary, potentially increasing the variety of tokens beyond the predefined vocabulary size and exceeding the range of word embeddings that the model can map. To address it, Scaffold-BPE adds one additional step termed Demolishing at the end of the encoding process. Scaffold-BPE demolishes all scaffold tokens into their shortest non-scaffold child token sequences, ensuring that the final representation only consists of tokens from the predefined vocabulary. For example, a token like “zona” might be demolished into “zon” and “a”. After the Demolishing step, Scaffold-BPE returns the final token sequence representation for the text. Since the shortest non-scaffold child token sequences for all scaffold tokens can be precomputed and stored during the training process, the time complexity of demolishing one token is constant, making its impact on encoding efficiency negligible."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We employ the recently well-attended language modeling tasks to validate the effectiveness of the Scaffold-BPE."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Datasets. Our models are trained on the Pile dataset, an 825.18 GiB English text dataset designed for training large scale language models. The Pile is composed of 22 diverse and high-quality datasets, the models trained on which significantly outperform both raw and filtered Common Crawl models. The data distribution for our model training is identical to those described in the original work.\n\nTokenizer. We train two 32K vocabularies using the original BPE and Scaffold-BPE, respectively. The training text corpus is sampled from the Pile dataset with an identical data distribution. Following GPT-2, we pre-segment the text using its regular expression.\n\nModel. We train three generative language models with 468M, 1.2B, and 6.7B parameters, respectively. Specifically, the architectures of the 468M-parameter and the 1.2B-parameter models, including the dimensionality of hidden states, the number of layers, etc., are identical to those of the 410M-parameter and the 1.0B-parameter models outlined in Pythia. The minor differences in parameter sizes are attributed to the variations in vocabulary size in the embedding layer. As for the 6.7B-parameter model, its architecture is identical to LLaMA-7B. The corresponding hyperparameters for each model can be found in Table 1.\n\nTraining. Following LLaMA, we use the AdamW optimizer with a learning rate, warmup steps, and a cosine learning rate decay schedule. Following the pretraining settings of previous works and limited by our computation budget, by default all models are pretrained with 100B tokens. Considering model training efficiency and commonly used criteria of computation budget in LLM training, we still compare experiments in the setting of an equal amount of training tokens.\n\nEvaluation. For fair comparisons, we utilize the open-source pipeline lm-evaluation-harness for evaluation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Common Sense Reasoning. Our analysis incorporates six benchmark datasets recognized for evaluating common sense reasoning including HellaSwag, OpenBookQA, PIQA, SIQA, StoryCloze, and Winogrande. We present the performance of our model, focusing on accuracy in both zero-shot and few-shot scenarios. As shown in Table 2, Scaffold-BPE consistently outperforms the original BPE on different setups with different model sizes. Specifically, for all models on 5-shot settings, Scaffold-BPE consistently achieves superior performance compared to the original BPE. As for the 0-shot settings, Scaffold-BPE still yields higher performance on 5 out of the 6 datasets. Such results clearly demonstrate that although the modifications are simple, our proposed Scaffold-BPE is convincingly effective.\n\nClosed Book Question Answering. For the task of closed book question answering, we evaluate the performance of the largest 6.7B-parameter models with different tokenizers on two benchmark datasets, i.e., TriviaQA and WebQuestions. We report the exact match performance for the zero-shot and few-shot settings in Table 3. It can be seen that language models trained with the proposed Scaffold-BPE achieve superior performance in both settings, which demonstrates that Scaffold-BPE can enhance model performance across different types of downstream tasks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Robustness In Language Modeling Tasks",
            "text": "Various Vocabulary Size. Depending on the size of the training corpus, the diversity of the languages, the size of the model, and the types of tasks, different vocabulary sizes are set in practice. Therefore, to validate the robustness of Scaffold-BPE across various vocabulary sizes, in addition to the 32K vocabulary [42, 34], we also trained two vocabularies sized at 64K [3, 2] and 128K [50]. The experimental setup is identical to that of the 468M-parameter model mentioned in Section 1.\n\nAs shown in Table 4, the experiments demonstrate that Scaffold-BPE consistently outperforms the original BPE across all vocabulary sizes, which indicates that Scaffold-BPE is not sensitive to vocabulary size. Its algorithmic design enables it to adaptively identify and remove scaffold tokens across any vocabulary size, without the need for manually designed or heavily-tuned hyperparameters.\n\nMore Training Tokens. According to the Scaling Law, the loss scales as a power-law with model size, dataset size, and the amount of compute used for training [23]. To demonstrate the effectiveness of our Scaffold-BPE with more training tokens, we continue training the 468M-parameter models up to 300B tokens [52, 5, 34, 8]. As shown in Table 5, the experiments demonstrate that Scaffold-BPE consistently outperforms the original BPE at 300B training tokens. The result well indicates that in the era of increasingly large training datasets for Large Language Models, our Scaffold-BPE can effectively enhance the capabilities of those models through simple modifications to the original BPE."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Task Insensitive, Language Insensitive And Architecture Insensitive",
            "text": "Although the development of large language models is burgeoning and they are increasingly applied across various scenarios, many applications still prefer using conventional models due to their lower training and inference costs. In the field of Natural Language Processing, BPE was initially applied to machine translation tasks [40], which typically present an open vocabulary challenge and involve substantial textual variation between two languages. Consequently, numerous improvements to BPE have been extensively validated on machine translation tasks [32, 49, 20, 45, 12, 46, 36].\n\nTherefore, to validate the versatility of the Scaffold-BPE method, we additionally conduct evaluations on machine translation tasks. We replicate the experimental setup of the prior work [31] which uses 32K vocabularies for the WMT’14 English-German dataset and 40K vocabularies for the WMT’14 English-French dataset [7]. For fair comparisons, We do not pre-segment the text using regular expressions. We train the “big\" transformer models [44, 31] to convergence and average model parameters from the last 10 checkpoints [31].\n\nAs shown in Table 6, Scaffold-BPE outperforms the original BPE in machine translation tasks, which demonstrates that Scaffold-BPE is not specific to language modeling tasks and can be applied to a wider range of tasks like language understanding, summarization and text classification. Besides, experiments conducted with English-German and English-French language pairs demonstrate that Scaffold-BPE is language insensitive. Scaffold-BPE is capable of identifying and removing the scaffold tokens introduced by the original BPE across different languages.\n\nFurthermore, prior experiments on language modeling tasks are carried out on decoder-only architecture. For the machine translation tasks, we utilize the classic encoder-decoder architecture [44]. The exceptional performance of Scaffold-BPE confirms its architecture insensitivity, indicating its applicability across a wider range of neural network architectures."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Better Text Representation",
            "text": "Higher Compression Rate. Besides the performance of models on tasks, the compression rate is a metric to measure the effectiveness of a tokenizer. A higher compression rate means that fewer tokens are required to represent the same corpus. As shown in Table 8, Scaffold-BPE, utilizing a dynamic scaffold tokens removal mechanism, retains more actual high-frequency tokens in the final vocabulary. Therefore it can achieve a higher compression rate on the corpus.\n\nBesides, considering model training efficiency and commonly used criteria (i.e., the token amount) of computation budget in LLM training, experiments in Section 4.2 and 5.1 are compared in the setting of an equal amount of training tokens. To eliminate the impact of different amounts of training text caused by different compression rates on experiment results, we additionally train two 468M-parameter models on exactly 388 GiB training text (100B tokens). As shown in Table 9, the Scaffold-BPE consistently outperforms the original BPE, demonstrating that the effectiveness of Scaffold-BPE is not merely obtained by allowing models to digest more data in the same computation budget.\n\nBetter Uniformity of Learned Embeddings. Prior works have analyzed the embedding space learned by a model and found that better uniformity prefers a token embedding space that preserves maximal information. To demonstrate Scaffold-BPE's effectiveness, we visualize the token embeddings in the 6.7B-parameter models, following Provilkov et al. As shown in Figure 6, the embeddings of scaffold tokens learned via the original BPE are more clustered, which means they are not well learned. On the contrary, the embeddings of new tokens introduced by Scaffold-BPE after removing scaffold tokens have better uniformity, which are more evenly distributed across the semantic space. Therefore, models trained with Scaffold-BPE can achieve better performance."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we propose Scaffold-BPE, a new approach to tokenization. Scaffold-BPE is parameter-free, computation-light, easy-to-implement, and widely effective, while preserving the simplicity and clarity of BPE. Through extensive experiments, including varying model sizes, varying vocabulary sizes, and extending training tokens, Scaffold-BPE demonstrates its robustness and superiority over the original BPE across a variety of natural language processing tasks. Our work underscores the importance of continual refinement in tokenization methods for improving the overall efficiency and effectiveness of models in the natural language processing field."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In the proposed Scaffold-BPE, the modifications to the training and encoding of the original BPE are simple and straightforward. Therefore Scaffold-BPE may be combined with other enhancements such as optimal vocabulary size search and novel encoding methods to achieve further improvements. We leave the investigation to our future research."
        }
    ]
}