{
    "title": "Scope Ambiguities in Large Language Models",
    "abstract": "Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2 and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments. Data and code are available at: https://github.com/McGill-NLP/scope-ambiguity",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Sentences like ‘every farmer owns a donkey’ are systematically ambiguous between two readings: one in which the embedded noun phrase (NP) (e.g. ‘a donkey’) is interpreted within the scope of the quantifier that precedes it (‘every’), and another in which the embedded NP is interpreted outside its scope. ‘Every farmer owns a donkey’, for example, could either mean (i) that each farmer simply owns their own (possibly unique) donkey, or (ii) that there is a specific donkey in question that all farmers jointly own.\n\nSuch constructions are examples of what are known as scope ambiguities. They arise when the respective scope of multiple semantic operators in the expression is ambiguous, yielding more than one possible semantic structure. Consider the following example:\n\nEvery farmer owns a donkey.\n\nThe ambiguity lies in the order of application (and thereby scopes) of these two operators. The surface scope reading of the sentence involves the universal quantifier outscoping the existential quantifier. The inverse scope reading involves the reverse.\n\nImportantly, English speakers (i) have access to both kinds of readings, and (ii) generally disambiguate between them to arrive at a preferred reading. For example, without further context, most people would prefer the surface reading, due to the surface positions of ‘a’ and ‘every’ in the sentence, as well as background world knowledge about farmers and donkeys.\n\nThe focus of this paper is how large language models (LLMs) treat such ambiguities. Assessing how they do so offers important insights into interactions between semantic structure and world knowledge, as well as the representation of scope in LLMs. Scope disambiguation lies at the interface between natural language semantics and background world knowledge. Disambiguating between these two possible structures often requires background world knowledge. \n\nTake the following two sentences:\n\nEvery conference attendee ate a Big Mac.\nEvery conference attendee attended a networking event.\n\nBoth examples are scope-ambiguous in a similar way to the farmer and donkey sentence—each offers two possible semantic structures yielding different readings. However, choosing the preferred reading is easy in both cases: in the first sentence, the surface scope reading (every attendee ate a potentially different Big Mac) is preferred, while in the second, the inverse scope reading (there was a single networking event that all attendees attended) is preferred. These preferences are a result of the general knowledge we have about conference attendees, networking events, and Big Macs.\n\nLLMs have been shown to be able to capture aspects of world knowledge and, separately, to capture some properties of natural language semantics. Scope ambiguities present an opportunity to assess how they might integrate the two. The ambiguities discussed here arise out of a crucial component of linguistic structure: scope. Analyzing how LLMs treat them helps us gain insight into how well they capture this component of structure. This is particularly interesting because while formal logic allows for a straightforward, symbolic representation of scope ambiguities, it remains an open question whether vector-based LLM representations can adequately capture the multiple readings of such constructions.\n\nThis paper attempts to answer two questions: Do LLMs exhibit similar preferences to humans in the interpretation of scope ambiguous sentences? Are LLMs sensitive to the presence of more than one reading of scope ambiguous sentences? We conduct two experiments to investigate these questions. From these experiments, we present evidence that the answer to these questions—at least for the more powerful models—is ‘yes’."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Scope ambiguities have been the focus in research within computational linguistics and natural language processing (NLP) primarily through the task of quantifier scope disambiguation, which involves the proper selection of a preferred scope reading given a scope-ambiguous sentence. Early examples of NLP research on this task, such as Higgins and Sadock (2003) and Andrew and MacCartney (2004), frame it as a classification task, and find models that outperform naive heuristics. Such work predates modern neural language models; Rasmussen (2022), however, builds on this approach, framing quantifier scope disambiguation as a span-pair classification task. \n\nManshadi and Allen (2011) and Tsiolis (2020) approach the problem differently, as neither frames it as a classification task. Manshadi and Allen (2011) represent scope relations as graphs, and frame the task as one of graph construction; they present a support vector machine that beats a naive heuristic baseline. Tsiolis (2020), on the other hand, attempts to reframe the task as a natural language inference task, Q&A task, or one in which probabilities of continuations are compared. They use a large language model—GPT-2 Radford et al. (2019) —but present mixed results.\n\nOther research focuses on the linguistic factors that determine scope reading preferences in a corpus. AnderBois et al. (2012) find that linear order, grammatical roles and lexical effects determine these preferences; Leczkowski et al. (2022) build on this work and find that prepositions and preposition senses also affect scope reading preference.\n\nThe only two instances of work assessing how LLMs treat scope ambiguities in zero-shot contexts are, to our knowledge, recent works by Liu et al. (2023), and Stengel-Eskin et al. (2023). The latter assesses how LLMs treat ambiguous inputs in terms of semantic parsing. The authors use templates to generate ambiguous sentences—including scope-ambiguous sentences—along with logical parses of them, and assess the abilities of LLMs to properly produce the two logical parses of each ambiguous sentence, in both few-shot and zero-shot contexts.\n\nLiu et al. (2023), on the other hand, assess how LLMs treat linguistic ambiguity in terms of entailment relations. Using prompting approaches to the task, as well as observing probabilities assigned to continuations of ambiguous sentences, they present evidence suggesting LLMs struggle to model ambiguity. Both works, though they do not primarily focus on it, do include scope ambiguity data, and are thus relevant to our work.\n\nWhere we diverge from these works, however, is in our data and experimental methods. While the templates Stengel-Eskin et al. (2023) use allow for the generation of hundreds of sentences, they do limit the diversity of these stimuli; moreover, the scope ambiguities in their datasets are limited to instances of quantifier-quantifier interactions. Similarly, Liu et al. (2023) estimate from a random sample that roughly 7.6% of their data involves scope ambiguity; manually inspecting all 579 ambiguous sentences in their dataset, however, we find that the dataset contains a total of around 20 instances of scope ambiguity. We also employ different experimental set-ups (see Sections 4.1 and 5.1) than those used in the aforementioned works. Crucially, these experimental methods may be what provide us opposite findings from both of them; we discuss this difference in Section 8.\n\nMore broadly, our work belongs to a growing body of literature evaluating how well neural language models capture a range of semantic phenomena. This includes work assessing the capacity of such models to capture compositionality, as well as specific features such as negation, quantification, and monotonicity."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We focus on scope ambiguities involving quantifiers such as ‘some’, ‘every’ and ‘most’, as well as quantifier-like determiners, like indefinites and numbers.\n(1  ###reference_###) is an example of scope ambiguity arising out of quantifier-quantifier interactions.\nBut scope ambiguities can also arise out of quantifier-negation and quantifier-adverb interactions, as shown below:\nQuantifier + Negation:\n{exe}\n\\exSita doesn’t like a classmate of hers.\n\n\\ex{xlist}\n\\exSurface Reading: There is no classmate that Sita likes.\n\n\\exInverse Reading: There is a specific classmate that Sita does not like.\nQuantifier + Adverb:\n{exe}\n\\exBachi usually meets two professors.\n\n\\ex{xlist}\n\\exSurface Reading: Usually, Bachi meets any two professors, who are possibly different each time.\n\n\\exInverse Reading: There are two professors who Bachi meets regularly.\nIn all three cases, the different readings have different truth conditions, and each is therefore logically compatible with a different set of propositions.\nAs an illustration, consider our original example, reproduced here as (1  ###reference_###):\nEvery farmer owns a donkey.\n\n\\ex{xlist}\n\\exEach farmer has a different donkey.\n\n\\exAll farmers have the same donkey.\n(1  ###reference_###) is logically compatible with (1  ###reference_###) only given the surface scope reading of the sentence, which states that each farmer has a potentially unique donkey.\nIt is not logically compatible with the inverse scope reading of the sentence, which states that there is an individual donkey that all farmers (jointly) have.\n(1  ###reference_###), however, is also logically compatible with the inverse scope reading of the sentence.\nIn Experiments 1A and 1B, we use these differences in logical compatibility to assess whether LLMs exhibit similar preferences to humans in the interpretation of scope ambiguous sentences.\nSimilarly, different scope readings often yield different effects in a discourse setting.\nConsider (1  ###reference_###): under the inverse scope reading, two professors are introduced as constant across the instances of Bachi’s meetings.\nConsequently, they can therefore be further referred to in the discourse, as in (1  ###reference_###).\nHe likes those two professors.\n\n\\exIt’s a different pair each time.\nBut under the surface scope reading of (1  ###reference_###), there aren’t necessarily two professors that are constant across instances, and who can therefore be further referred to in the discourse.\nAs a result, (1  ###reference_###) is not an acceptable follow-up.\nThe possible variability of the professors across multiple instances, however, does mean that (1  ###reference_###) is an acceptable follow-up (which it is not given the inverse scope reading).\nIn Experiments 2A and 2B, we use such patterns of acceptable and unacceptable followups to assess whether LLMs are sensitive to the presence of multiple readings of scope ambiguous sentences."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment 1A",
            "text": "I'm sorry, I cannot modify or redact specific sections from the text without more context or explicit content related to accuracy provided. Could you please provide further details or specify the text you need to be revised?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Method",
            "text": "In our first experiment, we assess whether LLMs show similar preferences to humans in how scope ambiguous sentences are interpreted. We frame this as a Q&A task. We present the LLM with sentences that are technically scope ambiguous, but have one strongly preferred scope reading (in some cases, this is a surface scope reading, and in others, it is an inverse scope reading). We then present the model with two possible statements based on this ambiguous sentence. One statement is compatible with only the surface scope reading of the ambiguous sentence, while the other statement is compatible with the inverse scope reading. In the case of chat-optimized models, we then ask the model which option is more likely; in the case of models not optimized for chat, we then obtain this answer through next token prediction. \n\nFigure 2 shows an example of how we conduct this method, using the dataset we develop for this experiment (details in Section 4.2). We concatenate, with newlines as shown in Figure 2, (i) a test sentence that is technically scope-ambiguous; (ii) an explanation that there are two options; (iii) two statements, labelled Option A and Option B, where one is compatible only with the surface scope reading, and the other is compatible with the inverse scope reading; and (iv) a prompt that elicits the model’s preferred choice. In the case of chat-optimized models, we observe the model’s response to a question asking it to choose between the options. For other models, we observe the next token predicted by the model after the text ‘the most likely option among these two is option’: this is either ‘A’ or ‘B’, corresponding to Option A and Option B respectively. We treat these as the model’s ‘answer’."
        },
        {
            "section_id": "4.1.x",
            "parent_section_id": "4.1",
            "section_name": "Control: No Sentence in Prompt",
            "text": "One possible issue with this approach is that answers may depend more on the likeliness of the two options as general statements than on their likeliness given the ambiguous sentence. We therefore add a control: we remove the ambiguous sentence altogether from the stimulus, and present the rest of it to the model, conducting the same task as before. In this setting, the model should do significantly worse, as it is not exposed to the sentence that is being evaluated with respect to the two options. For instance, in the example in Figure 2, both options appear plausible in the absence of any context; following the scope-ambiguous sentence, however, only Option A is plausible. If model performance does not drop significantly when the ambiguous sentence is dropped, the model’s performance in the original setting is likely unrelated to its processing of the ambiguous sentence, and instead reflects the background likeliness of each option."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "We build upon the quantifier scope dataset presented by AnderBois et al. (2012).\nWe chose this dataset as a starting point because among the few existing scope ambiguity datasets, it was the dataset that had datapoints most appropriate to the focus of this study.\nThis dataset contains around 1,700 sentences and phrases scraped from LSAT (Law School Admission Test) logic puzzles, and marked for quantifier scope where present.\nWe filtered the dataset for instances of two interacting ‘quantifiers’. These included constructions such as ‘per’ in ‘one person per appointment’—constructions that have some quantificational force, even if they aren’t quantifiers in the strict sense—as well as instances of negation such as ‘none’ in ‘none of the cities’.\nThis narrowed it down to around 400 datapoints.\nNext, we manually constructed contrasting ‘options’ based on surface and inverse scope readings for whichever of these roughly 400 datapoints allowed this approach, giving us 186 sentences with accompanying contrasting statements.\nTo further ensure that these datapoints had strong scope reading preferences, we then conducted two rounds of human validation.\nIn both rounds, we recruited participants via Prolific.\nParticipants (38 in each round) were presented the filtered scope-ambiguous sentences along with two accompanying options.\nIn the first round, we reworded datapoints with low subject agreement; in the second round, we dropped any datapoints with less than 75% agreement (all datapoints received at least 4 evaluations).\nFor the datapoints that remained, gold labels were taken as the majority vote from study participants.\nThis process ultimately yielded 153 scope ambiguous sentences, each with a pair of options.\nOf these, 41 had an inverse scope reading preferred, while the remaining 112 had a surface scope reading preferred.\nAlmost all were examples of scope ambiguities arising from quantifier-quantifier interactions, with a handful involving quantifier-negation interactions, and even fewer involving other types of interactions.\nAs a final step, we duplicated each datapoint, but with a flipped order of options (i.e. ‘Option A’ was labelled ‘Option B’, and vice versa).\nThis meant that while the distribution of preferred scope-readings remained skewed, the final dataset—which contains 306 datapoints covering 153 unique sentences—had an even distribution of correct answers (50% ‘A’ and 50% ‘B’)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "For all experiments, we choose to use autoregressive language models, due to their growing prevalence in practical applications using prompting. Specifically, for this experiment, we use chat and vanilla versions of Llama 2 Touvron et al. (2023) at 7B, 13B and 70B sizes, three variants of GPT-3/3.5 Brown et al. (2020); Ouyang et al. (2022): davinci, text-davinci-002, and text-davinci-003, and GPT-4 OpenAI (2023). See Table 2 for a summary of key differences between these models.\n\nGPT-3-davinci  \n175B  \nGPT-3.5-text-davinci-002  \nUnclear  \nGPT-3.5-text-davinci-003  \nUnclear  \nGPT-3.5-turbo  \nUnclear  \nGPT-4  \nUnclear; possibly ensemble model  \nLlama 2  \n7B, 13B, 70B  \nLlama 2 Chat  \n7B, 13B, 70B  \nGPT-2  \n117M, 345M, 774M, 1.5B  "
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Human Baselines",
            "text": "After the human feedback-based filtering mentioned above, we conducted another round of the same experiment with humans to get human baselines on this dataset. We are testing models on their ability to choose the scope readings preferred by most people—but how good are human themselves at choosing the scope readings preferred by most other people? \n\n68 native speakers of English were recruited via Prolific for a repeat of the experimental set-up described in Section 4.2, but this time with the final dataset. Each participant was presented with 18 datapoints and evaluated on their answers."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "The results of Experiment 1A are shown in Table 3. Human responses suggest that English speakers can arrive at scope reading preferences shared by most other people. Several models also achieve high performance—both versions of Llama2-70b, as well as all the GPT-3.5 models perform well in the test setting, while GPT-4 performs close to the ceiling. The control setting adds further insights to these results. Neither davinci nor the versions of Llama 2 at 7B see any significant change when prompts are provided without the actual scope ambiguous sentence (Llama2-7b actually performs better in this setting), suggesting that success in the test setting is not a result of the models’ processing of the ambiguous input, but primarily driven by the background likeliness of the two options. The models with higher performance metrics, however, see more severe drop-offs in the control setting, most notably with GPT-4."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Discussion",
            "text": "These results suggest that the more advanced LLMs evaluated—GPT-3.5, Llama 2 at 70B, and most notably GPT-4—are able to exhibit similar scope reading preferences as humans. Smaller or less advanced models, however, such as Llama 2 at 7B, appear to fail. Also worth noting is the fact that, for almost all models, performance on sentences that had a preferred inverse scope reading was lower than on those that had a preferred surface scope reading. This aligns with literature suggesting that inverse scope readings are generally harder to access than surface readings (see e.g., Kurtzman and MacDonald, 1993; AnderBois et al., 2012), but curiously, does not align with the behaviour of humans in this experiment, who showed no such dispreference. The deeper implication of some of the models’ performance, however, is that LLMs can not only capture different types of readings—surface and inverse, which correspond to different semantic structures—but also integrate background world knowledge in their behavioural preferences when confronted with scope ambiguous constructions."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment 2A",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Method",
            "text": "Our first experiment showed that LLMs can mimic human preferences in interpreting scope ambiguous sentences. It did not indicate whether LLMs were sensitive to the multiple readings of each ambiguous sentence. This question is explored in our second experiment. We assess whether models exhibit different behavior for scope ambiguous sentences compared to similar, non-ambiguous sentences, indicating sensitivity to meaning ambiguity. Rather than framing it as a traditional task like Q&A, we follow methods that apply psycholinguistic analysis to language models, observing the probabilities assigned to continuations given an ambiguous sentence.\n\nFigures illustrate our approach: we present the LLM with a scope ambiguous sentence and observe the probabilities assigned to two follow-ups, labeled  and . is an acceptable continuation only with the inverse scope reading, while is acceptable only with the surface scope reading. We compare these probabilities with those assigned to and given a control sentence, , which is similar but not ambiguous; remains an acceptable continuation, but does not.\n\nWe expect the model to assign probabilities reflecting the ambiguity of , by comparing the probability ratios for continuations to  and . If the model captures the ambiguity, the ratio of probabilities to and for should be smaller than for . This shows awareness that allows both and as continuations, unlike , which allows only .\n\nFor deeper analysis, we observe how much is greater than . We measure the difference in log ratios for and given , and for and given , to calculate the model’s ambiguity recognition score, or ‘-score.’ If the model successfully identifies ambiguity, the -score will be positive."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Dataset",
            "text": "Existing scope ambiguity datasets are (i) few in number, and (ii) generally involve examples where one scope reading is strongly preferred over the other. While we made use of this second observation in our first experiment, it is a significant problem for the current experiment due to its aims. In this experiment, we aim to test LLMs for their sensitivity to the presence of multiple readings of a scope ambiguous sentence. But in such cases, if humans themselves find one of these readings very hard to access without further context, it would be unfair to expect models to do so. For a fair evaluation, therefore, it is crucial that we use sentences which do not have one reading strongly preferred over another. We therefore construct a small-scale dataset, consisting of 38 manually handcrafted datapoints, where each datapoint includes a scope ambiguous sentence, a matching non scope-ambiguous control sentence, and two follow-up phrases, yielding a total of 152 sentence-continuation pairs. For further validation, these datapoints were then filtered through our human baselines: any datapoints that yielded negative human-derived scores were dropped, as such scores indicated that these were datapoints for which the inequality did not align with human judgments. This left us with 29 unique datapoints, yielding 116 unique sentence-continuation pairs."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Human Baselines",
            "text": "Since the current experiment involves the analysis of probabilities assigned to text sequences—something not directly replicable with humans—we use proxy scores derived from a crowdsourced judgment task as our human baselines. We conducted a crowdsourced study via Prolific, involving 140 native speakers of English; each was presented random sentence-continuation pairs from the dataset and asked to provide ratings from 1 to 7 on how ‘natural-sounding’ the continuation was to the sentence. From these ratings, we computed the mean score for each sentence-continuation pair, and normalized them to be in an interval between 0 and 1. This proxy score gives us an indirect means by which to compare human judgments of scope-ambiguous sentences and continuations with LLM-assigned probabilities of the latter given the former."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Models",
            "text": "As in Experiment 1A, we work with autoregressive LLMs. Unlike in Experiment 1A, however, the current experimental set-up allows us to also work with models ill-suited to zero-shot contexts. We therefore ran this experiment not only on the models tested before, but also on several smaller variants of GPT-2 Radford et al.: small (117M params), medium (345M params), large (774M params), and XL (1.5B params). Probabilities from GPT-2 and Llama 2 models were extracted using the minicons library Misra. The reliance on probabilities, however, forces us to omit GPT-3.5-turbo and GPT-4, for which sequence log-probabilities are not accessible."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Mean Scores",
            "text": "We first compute mean and proxy scores, along with -values derived from paired -tests. Positive, statistically significant mean scores point to an overall sensitivity to the meaning ambiguity of the sentences in the dataset; comparing between models, higher mean scores also suggest stronger overall sensitivity. While this relationship is less clear when comparing model scores and human scores, the main reason is that while human scores are derived from a bounded set of ratings between 1 and 7, model log-probabilities practically have no negative bound, allowing for more extreme differences between them. Since -scores are computed as differences of log differences, it is thus possible for them to be much higher than derived human scores, purely on account of being unbounded below zero. Table 4 shows our results. All models yield positive mean -scores; and barring the case of GPT-2-small, all are statistically significant at a threshold of ."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Correlations Between Model and Human Scores",
            "text": "We compute the correlation between model-derived scores and human-derived proxy scores to evaluate how model behavior aligns with human judgments. We expect a strong, positive correlation if the models align well with human judgments.\n\nTable 4 shows model-wise Pearson correlation coefficients between scores and human proxy scores, along with corresponding p-values. Many models fail to produce significant correlations; however, text-davinci-003 and Llama 2 at 13B both achieve highly significant correlation scores. Notably, text-davinci-003 achieves the highest correlation score, around 0.62. \n\nSee Figure 4 for a scatterplot of text-davinci-003’s scores against corresponding human proxy scores, providing further evidence of this correlation."
        },
        {
            "section_id": "5.5.3",
            "parent_section_id": "5.5",
            "section_name": "5.5.3 Proportion of Positive -Scores",
            "text": "Lastly, we compute the proportion of data points for which models produced positive -scores, which allows us to assess whether the models behave consistently across data points. We observe an effect of model size: larger models perform well, with several yielding positive -scores for over 90% of the data, and once again, text-davinci-003 performs the best."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "These results suggest that a wide range of LLMs may be sensitive to the meaning ambiguity in scope ambiguous sentences. Larger or more powerful models (i.e. those besides GPT-2 small) distinguish between scope ambiguous and non-scope ambiguous sentences in a manner consistent with their meanings. Similarly, the correlations we see between some models’ scores and human proxy scores suggest that, at least for certain models, this behavior correlates well with human judgments. Comparing chat and vanilla versions of Llama 2 also reveals an interesting pattern. As Table 4 shows, chat versions of Llama 2 produce slightly higher correlations than their non-chat equivalents, but also lower proportions of positive scores—indicating increased alignment with human judgments on several sentences, but lower overall consistency. The broader takeaway, however, is that several LLMs appear sensitive to a meaning ambiguity that arises from the presence of different possible semantic structures, which vary vis-à-vis scope relations. Consequently, although the current work does not investigate how or where models represent scope, these results suggest that LLMs capture scope-related phenomena."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Re-evaluation on Expanded Datasets",
            "text": "The results from Experiments 1A and 2A are promising, but rely on relatively small datasets that contain 153 and 29 unique datapoints respectively—raising questions of how generalizable our results are.\nAs a followup, we therefore considerably expand these two datasets, and rerun the same experiments described in Sections 4  ###reference_### and 5  ###reference_### on the expanded datasets.\nTo expand our Experiment 1A dataset, we begin by annotating it for both semantic operator and quantifier types: whether an ambiguity arose out of a negation-quantifier or quantifier-quantifier interaction, as well as whether the quantifier was an existential quantifier, universal quantifier, number or indefinite.\nCombined with scope reading preference labels (see Section 4.2  ###reference_###), this gave us 13 categories of scope reading and operator combinations (e.g. negation-indefinite_surface, or number-universal_inverse).\nFollowing this categorization, we add manually handcrafted examples to any sparse categories, so each contains at least 10 unique datapoints.\nWe then use GPT-4 to expand the dataset.\nFor each category in our annotated dataset, we randomly sample 5 datapoints, and instruct GPT-4 to produce 10 novel datapoints based on them.\nWe repeat this process ten times, such that we have 100 datapoints generated from each of our annotated categories.\nWe then manually inspect the combined 1,300 generated datapoints, removing duplicates, and dropping or editing low-quality datapoints; this left us with 1,062 datapoints.\nFinally, we run a crowd-sourced study via Prolific (278 participants, 5 ratings per datapoint), similar to those described in Sections 4.2  ###reference_### and 4.4  ###reference_###, to obtain our gold labels for preferred scope readings, and filter out any datapoints that received low inter-subject agreement.\nThis process eventually yields 837 unique scope-ambiguous sentences (with accompanying ‘options’, and human preference labels); 534 receive a preferred surface scope reading, and 303 an inverse scope reading.\nWe use a process similar to the one for Experiment 1A.\nWe first split our Experiment 2A dataset into categories based on whether the datapoints involve negation-quantifier, adverb-quantifier or quantifier-quantifier interactions.\nWe then run the same sampling, generation and manual filtering process as with the Experiment 1A dataset, giving us 126 datapoints from 300 GPT-4 generated datapoints.\nFinally, we run another study via Prolific (223 participants, 8 ratings per sentence-followup pair), and use this human judgement data to further filter the dataset.\nOur final dataset consists of 110 unique datapoints, where each datapoint consists of a scope-ambiguous sentence, control sentence, follow-up supporting an inverse scope reading, and follow-up supporting a surface scope reading."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Dataset Expansion Process",
            "text": "To expand our Experiment 1A dataset, we begin by annotating it for both semantic operator and quantifier types: whether an ambiguity arose out of a negation-quantifier or quantifier-quantifier interaction, as well as whether the quantifier was an existential quantifier, universal quantifier, number or indefinite.\nCombined with scope reading preference labels (see Section 4.2  ###reference_###  ###reference_###), this gave us 13 categories of scope reading and operator combinations (e.g. negation-indefinite_surface, or number-universal_inverse).\nFollowing this categorization, we add manually handcrafted examples to any sparse categories, so each contains at least 10 unique datapoints.\nWe then use GPT-4 to expand the dataset.\nFor each category in our annotated dataset, we randomly sample 5 datapoints, and instruct GPT-4 to produce 10 novel datapoints based on them.\nWe repeat this process ten times, such that we have 100 datapoints generated from each of our annotated categories.\nWe then manually inspect the combined 1,300 generated datapoints, removing duplicates, and dropping or editing low-quality datapoints; this left us with 1,062 datapoints.\nFinally, we run a crowd-sourced study via Prolific (278 participants, 5 ratings per datapoint), similar to those described in Sections 4.2  ###reference_###  ###reference_### and 4.4  ###reference_###  ###reference_###, to obtain our gold labels for preferred scope readings, and filter out any datapoints that received low inter-subject agreement.\nThis process eventually yields 837 unique scope-ambiguous sentences (with accompanying ‘options’, and human preference labels); 534 receive a preferred surface scope reading, and 303 an inverse scope reading.\nWe use a process similar to the one for Experiment 1A.\nWe first split our Experiment 2A dataset into categories based on whether the datapoints involve negation-quantifier, adverb-quantifier or quantifier-quantifier interactions.\nWe then run the same sampling, generation and manual filtering process as with the Experiment 1A dataset, giving us 126 datapoints from 300 GPT-4 generated datapoints.\nFinally, we run another study via Prolific (223 participants, 8 ratings per sentence-followup pair), and use this human judgement data to further filter the dataset.\nOur final dataset consists of 110 unique datapoints, where each datapoint consists of a scope-ambiguous sentence, control sentence, follow-up supporting an inverse scope reading, and follow-up supporting a surface scope reading."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Experiment 1B",
            "text": "We re-run Experiment 1A on the expanded dataset; Table 5  ###reference_### shows our results.\nAs can be seen, the general patterns observed in Experiment 1A (see Section 4.5  ###reference_###) continue to hold true even when the models are evaluated on a much larger dataset.\nWhile some models either perform around chance or do not show a major accuracy drop in the control setting, models like GPT-4, text-davinci-003 and Llama2-70b show both high performance (all above 85% accuracy in the test setting, with GPT-4 achieving 96% accuracy, albeit on data it produced in a separate context), as well as a drop-off in performance in the control setting."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Experiment 2B",
            "text": "Similarly, we re-run Experiment 2A on the expanded dataset; Table 6  ###reference_### shows our results.\nAs with Experiments 1A and 1B, the general patterns observed continue to hold for the expanded dataset.\nAll models still produce positive mean -scores.\nThough not as high as on the original dataset, text-davinci-003 once again produces the highest correlation with human data, with a R-value of roughly 0.48.\nSimilarly, most models show a high level of consistency in their behaviour, producing positive -scores from, in the case of text-davinci-003 and Llama2-13b, over 90% of the data.888GPT-2 models also do much better on the expanding dataset. This may be because in the expansion process, we ensured that the contrast between acceptable and unacceptable sentence-followup pairs (see Figure 3  ###reference_###) was more clear cut than in the original dataset, often coming from grammatical cues, rather than world knowledge cues. GPT-2 may recognize the former more than the latter, and thus perform better here.\nOnce again, Llama 2 chat models produce higher correlations and mean scores than their vanilla counterparts, but in two out of three cases, lower proportions of positive -scores."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our results, which indicate that LLMs are both proficient at choosing the scope readings preferred by most humans, and sensitive to the meaning ambiguity in scope ambiguous constructions, offer further evidence of the capacity of large language models to induce semantic structure (see Pavlick, 2022  ###reference_b32###), and linguistic structure more generally (see Linzen and Baroni, 2021  ###reference_b24###; Baroni, 2022  ###reference_b5###).\nOn the other hand, these results contrast with closely related work by Liu et al. (2023  ###reference_b26###) and Stengel-Eskin et al. (2023  ###reference_b40###), who both find that LLMs struggle to model ambiguity in zero-shot contexts.\nWhat explains this contrast?\nOne possible explanation is the difference in methodologies used.\nWe assess models using Q&A- and probability-based approaches (see Sections 4.1  ###reference_### and 5.1  ###reference_###) that implicitly test models’ access to different scope readings.\nLiu et al. (2023  ###reference_b26###), on the other hand, mostly use prompting-based approaches that elicit model responses on what an ambiguous sentence may mean or entail, and Stengel-Eskin et al. (2023  ###reference_b40###) assess models in terms of their abilities to logically parse ambiguous inputs.\nIt is possible that LLMs implicitly capture meaning ambiguities and human-preferred interpretations, but cannot reliably produce meta-linguistic judgments or logical translations consistent with this information.\nThis is would be in line with findings from Hu and Levy (2023  ###reference_b18###), which suggest meta-linguistic prompting-based approaches may underestimate LLMs’ linguistic abilities.\nTo test this theory, we adapt a random sample of our Experiment 2B dataset to the format Liu et al. (2023  ###reference_b26###) use in their True/False evaluation of models (see Section 4.2 of Liu et al., 2023  ###reference_b26###).\nIn this format, models are prompted to answer whether it is true or false that, given one of its disambiguations, an ambiguous input may, may not, cannot, or can only mean the disambiguation.\nWe rerun their experiment on this subset of our data; our results, shown in Table 7  ###reference_###, are similar to the authors’ findings on their own data.\nMost models do poorly on this task, performing around chance (50%), with GPT-4 and GPT-3.5-turbo only achieving 64% accuracy.\nAs shown in Section 6.3  ###reference_###, however, using the dataset in our experimental format yields positive results that contrast this poor performance.\nThis divergence highlights the importance of diverse approaches to investigating the linguistic capacities of language models; our results suggest that probability- and prompting-based methods may yield differing conclusions."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we investigated how different autoregressive language models treat scope ambiguities. We introduced novel datasets containing approximately 1,000 unique and diverse scope-ambiguous sentences, annotated for human judgments—the largest of this kind. Our results indicate that LLMs can exhibit behavior in line with human preferences of interpretation—informed at least in part by background knowledge—as well as compatible with different types of semantic structures. Finally, the contrast between our findings and those of other recent works emphasizes the need for diverse approaches in assessing the linguistic capacities of large language models."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Aside from its focus only on English, one constraint of this work is that it does not assess how context affects scope reading preferences.\nAda often studies with a few of her friends.\n{xlist}\n\\exContext: Ada finds it hard to study alone, so she generally invites others for joint study sessions.\n\\exContext: Ada, Rohan and Jo are good friends in the same program, and prepare for exams together.\n(9  ###reference_###) is ambiguous between a surface scope reading ((9  ###reference_###) refers to no friends in particular) and an inverse scope reading ((9  ###reference_###) refers to some specific friends).\nDifferent background contexts can prompt different readings: (9  ###reference_###) prompts the surface scope reading, while (9  ###reference_###) prompts the inverse scope reading.\nOur work does not address such effects.\nAt a higher level, while this work shows how LLMs treat scope-ambiguous inputs, it also does not reveal how or where models represent scope.\nParallel work on model interpretability (such as causal mediation analysis, e.g., Vig et al., 2020  ###reference_b43###; Finlayson et al., 2021  ###reference_b12###; Geiger et al., 2022  ###reference_b14###) could provide exciting insights to this question."
        }
    ]
}