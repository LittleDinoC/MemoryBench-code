{
    "title": "Faster Cascades via Speculative Decoding",
    "abstract": "Cascades and speculative decoding are two common approaches to improving language models’ inference efficiency. Both approaches involve interleaving models of different sizes but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for “hard” inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality.\n\nIn this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated significant advances in quality on a range of natural language processing tasks, at the cost of a significant increase in inference latency. This has sparked a growing body of literature on reducing LMs’ inference costs without overly compromising on quality. One such line of work involves constructing a family of models of various sizes (e.g., a small and large model), and suitably orchestrating amongst them to make a prediction. Two canonical instantiations of this strategy are model cascading and speculative decoding. While similar in spirit, cascades and speculative decoding are fundamentally different in details. Cascades employ a deferral rule to identify “hard” inputs and only invoke larger models on such inputs. For example, in a two-model cascade, one first invokes the smaller model and uses its associated probability of the generated output to decide whether to defer to the larger model. By contrast, speculative decoding uses a small model to draft a block of tokens via standard auto-regressive decoding, which are then verified in parallel by a large model. One then accepts all drafted tokens until the first “implausible” one, which is rolled back based on the larger LM’s prediction. Owing to their different mechanisms, both methods have complementary strengths. Cascades seek to output distributions that have the best quality for a given cost budget and are empirically observed to often yield better accuracies than even the individual models they are constructed with. Speculative decoding is theoretically guaranteed to match the output distribution (or a close approximation thereof), and are practically observed to provide impressive speed-ups. Given the complementary nature of these two approaches, a natural question that arises is whether we can leverage the best of both techniques. In this paper, we do so by designing new techniques for two-model cascades that implement their deferral rule in a speculative manner: we have the smaller model generate drafts auto-regressively, and the larger model execute in parallel on the drafts to decide whether or not to defer on them. We show that this speculative cascading approach yields better cost-quality trade-offs than both standard cascades and speculative decoding. In detail, we make the following contributions: We introduce a general recipe for speculative execution, where we seek to mimic a general target distribution that interleaves the drafter’s and verifier’s output distributions. Lossy speculative sampling is a special case of this recipe for a particular target distribution. We show how common cascading deferral rules, such as Chow’s rule and confidence-difference thresholding, can be implemented speculatively by plugging in the corresponding target distribution into the proposed framework. We refer to these as speculative cascades. We characterize the theoretically optimal deferral rule for a speculative cascade, and design a speculative cascading technique that implements a plug-in estimate to the optimal rule. Through experiments on benchmark language tasks, we show that speculative cascades constructed from T5 models of different sizes are able to provide better cost-quality trade-offs than their sequential cascade and speculative decoding counterparts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "A Tale of Two Efficient LM Inference Strategies",
            "text": "Let  denote a finite vocabulary of tokens,\nwith  the set of all sequences generated by this vocabulary.\nLet  denote the set of all probability distributions over tokens in .\nGiven an arbitrary length sequence  and index , denote by .\nA language model (LM)\nis a\nprobability distribution over .\nLet  denote the ground-truth probability distribution over . This could be, for example, a distribution over prompt-response pairs\nthat the LM may encounter during deployment, or a distribution of sequences used to pre-train the LM.\nWe will measure the quality of an LM based on how closely it mimics .\nSuppose we are provided two LMs  and ,\nwhere  is the larger model. Our goal is to design an inference strategy that selectively invokes  and  to trade-off between quality and latency (which may be approximated by the fraction of times that  is invoked).\nWe will denote by  the probability  associates to token  given prefix\n, and by \nthe same distribution from model . Whenever it is clear from context, we will hide the conditioning on prefix , and use the shorthand  for  and  for .\nCascades are an effective strategy to trade-off cost and quality by having the smaller model  handle the “easy” samples,\nand the larger model  handle the “difficult” samples [18  ###reference_b18###, 50  ###reference_b50###]. A common approach to cascading is confidence-based thresholding or Chow’s rule [10  ###reference_b10###, 22  ###reference_b22###],\nwhere we first run  on the given input, and defer to  when ’s confidence for its generated response is sufficiently low. This strategy is typically\nimplemented at the sequence-level, where for a given prefix  we invoke  to generate a complete response . We evaluate the predicted probability from  for this response, and check whether it falls below a threshold :\nIf the above holds,\nwe defer to  to generate a new response;\notherwise,\nwe retain ’s response. One may then tune the threshold to achieve a desired cost-quality trade-off. The literature also offers variants of Chow’s rule that use a more nuanced aggregation of per-token uncertainties [18  ###reference_b18###].\nSpeculative decoding is an alternate inference strategy that applies token-level interleaving between  and ,\nresulting in provably matching the larger model quality at a reduced inference cost [39  ###reference_b39###, 26  ###reference_b26###].\nGiven a prefix , we sample  draft tokens  auto-regressively from , and run  in parallel on the  prefixes , and verify if the generated tokens can be accepted. We then rollback to the first token  that was rejected, replace with a new token, and repeat the drafting-verification process with  as the new prefix.\nDuring the verification stage, a draft token  generated by  is accepted\nwith probability\n\nand rejected otherwise,\nwhere recall the shorthand\n and .\nA rejected token is then replaced by a new token sampled from a modified distribution\n\nwhere \ndenotes normalization to sum to 1.\nThis sampling process is\nprovably\nequivalent to sampling  tokens auto-regressively from  for prefix  [26  ###reference_b26###].\nWe summarize this speculative sampling procedure in Algorithm 1  ###reference_###.\nEach invocation of this algorithm generates at most  next tokens for a given prefix . One may run this algorithm multiple times to generate a complete output sequence.\nIn practice, one may employ a lossy variant [44  ###reference_b44###] of the above sampling that allows some deviation from verifier’s distribution . In this case, a draft token  is accepted\nwith probability\n,\nwhere  is a strictness parameter, with higher values indicating greater deviation from .\nA rejected token may then be replaced by a token sampled from the residual distribution\n\nwhere  is a parameter that depends on ,  and .\nA common heuristic is to simply set  [52  ###reference_b52###]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Cascades Meet Speculative Decoding",
            "text": "Both cascades and speculative decoding interleave models of different sizes to reduce inference cost, but fundamentally differ in the mechanisms they use.\nAs a step towards comparing the strengths and weaknesses of these approaches, we first\ndescribe how one may design a token-level cascade."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Warm-up: Token-level cascades",
            "text": "It is straight-forward to extend\nthe sequence-level\nChow’s rule from §2  ###reference_### to form a token-level cascade between  and . For a prefix , we first compute the smaller model’s distribution , and check whether  is below a pre-chosen threshold.\nif so,\nwe evaluate , and sample ;\notherwise,\nwe sample\n.\nMore generally, we may design a token-level deferral rule  that takes the prefix  as input and outputs a binary decision, with  indicating that we\ndefer to  (i.e., draw a sample from  rather than ).\nFor example, token-level Chow’s rule can be written as:\nwhere  is a cost parameter; the higher the value, the lower is the frequency of deferral to . One may also use a confidence measure different from the maximum probability, such as the entropy of the small model’s probability distribution.\nWe elaborate in §B  ###reference_###\nthat the choice of confidence measure would depend on the evaluation metric of interest; (2  ###reference_###) is typically prescribed when the cascade’s quality\nis evaluated in terms of its accuracy against the ground-truth distribution on individual tokens, whereas entropy is prescribed when the metric of interest is the cross-entropy loss."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Optimal token-level cascade deferral",
            "text": "While Chow’s rule (2  ###reference_###) is easy to implement,\nit can be sub-optimal if the smaller model’s maximum token probability is not reflective of which of the two models are better equipped to predict the next token for a given prefix [22  ###reference_b22###].\nGiven this, it is natural to ask what the optimal deferral rule  for a token-cascade looks like,\nand whether we can reasonably approximate this rule.\nFor this, we must first specify an objective to minimize at each step .\nFollowing the prior cascade literature [22  ###reference_b22###, 18  ###reference_b18###], a reasonable objective to minimize is the expected loss from the deferral rule against the ground-truth distribution\n,\nwith an added cost for deferring to the larger model. We state this below for a fixed prefix ,\nusing the short-hand  for  and  for :\nfor a cost penalty \nand loss function .\nCommon choices for  include the 0-1 loss  and the log loss\nThe minimizer of (3  ###reference_###) is of the form:\nIntuitively, we compare the expected loss from  with the expected cost of invoking , and decide to defer when the latter is smaller.\nWe note here that this optimization problem is set up for a fixed prefix . One may also consider the coupled optimization problem across all prefixes from 1 to .\nPlug-in estimator for (4  ###reference_###). The optimal rule in (4  ###reference_###) requires computing expectations over the ground-truth distribution , which is not available during inference time.\nA common approach in the cascades literature is to replace the expected losses with the models’ confidence estimates [22  ###reference_b22###]. For example, when , it may be reasonable to use  as an estimate of the expected 0-1 loss  and  as an estimate of . The extent to which these estimates are accurate depend on how well  and  are calibrated [17  ###reference_b17###]. The resulting plug-in estimator for (4  ###reference_###) is given by:\nSimilarly, when , we may use the entropy  from  as an estimate of its expected log-loss, and similarly for  (see §C  ###reference_###).\ncannot be directly used in a token-level cascade,\nas it needs the larger model to be invoked at every position .\nHowever, it serves as an oracle that allows to analyze the head-room available to improve upon Chow’s rule.\nSee also Remark 2  ###reference_ark2###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "When do token-level cascades outperform speculative decoding?",
            "text": "Token-level cascades and\nspeculative decoding differ in the distribution over tokens they seek to mimic. Speculative decoding seeks to mimic the larger model’s output distribution\n(or an approximation to it). As a result, the quality of the sampled output is limited by the accuracy of the larger model. On the other hand, token-level cascades seek to output distributions that closely approximate the ground-truth label distribution, and can potentially yield better quality than even the larger model.\nIndeed, speculative decoding would be ideal in applications where the verification model  is uniformly better than the draft model  on all inputs. However, when there are inputs where the draft model fares better than the verifier, one may want to retain the drafter’s predictions even when it disagrees with the verifier. Similarly, in settings where both the drafter and verifier fare poorly on some inputs (e.g., due to label noise), one may again want to ignore the disagreement between the drafter and verifier, and avoid triggering unnecessary roll-backs.\nAs a concrete example, we consider token-level cascades of T5 models [35  ###reference_b35###] of two different sizes finetuned on a WMT EN  DE translation [3  ###reference_b3###] and an extreme summarization (XSum) task [31  ###reference_b31###]. We construct these cascades using both\n(token-level)\nChow’s rule in (2  ###reference_###) and the oracle Diff rule in (5  ###reference_###), and also apply speculative decoding with the smaller (larger) model as the drafter (verifier). In Figure 1  ###reference_###, we plot quality as a function of fraction of samples deferred to the large model, as we vary the cost parameter .\nNote that with speculative decoding, each verification step is counted as a single call to the large model.\nWhile speculative decoding matches the quality of the large model (right-most point), the oracle rule yields significantly better quality on a wide range of operating points. Even Chow’s rule, which is sub-optimal for cascading [22  ###reference_b22###],\noutperforms speculative decoding in a small region.\nHowever, as also evident from the plots, token-level cascades require a significantly larger number of calls to the larger model to achieve the same quality.\nThis is because token-level cascades are executed sequentially: whenever  defers, we execute  once to generate one next token for the prefix accumulated so far, and the control transfers back to . In contrast, speculative decoding runs  in scoring mode to verify multiple draft tokens from  in parallel. Moreover, the stochastic verification algorithm in speculative decoding often results in fewer tokens from  getting rejected compared to the deterministic deferral rules used in a cascade. These observations motivate a natural question: given their complementary strengths, how can we leverage the best of both these techniques?\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Speculative Cascades: Leveraging the Best of Both Worlds",
            "text": "In addressing the above question, we present our main contribution: a principled approach to combining the superior quality of cascades with the faster execution of speculative decoding approaches."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Speculative decoding with general target distributions",
            "text": "We begin by considering a generic version of speculative sampling that seeks to mimic a general target distribution derived from the drafter’s and verifier’s distributions.\nIn the proposed sampling procedure outlined in Algorithm 4  ###reference_###, we sample tokens auto-regressively as before from the drafter’s distribution. During the verification step, however, we do not compare the drafter’s token probabilities against the verifier’s distribution. Instead, we use a user-specified target distribution  derived from the drafter’s and verifier’s distributions at position , for some function  that is inexpensive to compute.\nWe accept a draft token  when  and reject it otherwise with probability . Upon rejection, we re-sample from the residual distribution .\nThis general procedure not only encompasses standard speculative decoding [26  ###reference_b26###] for , but also includes lossy speculative decoding [44  ###reference_b44###] as a special case:\nAlgorithm 4  ###reference_### reduces to the lossy speculative sampling procedure in [44  ###reference_b44###] with parameters  and  when ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "From sequential to speculative cascades",
            "text": "Equipped with Algorithm 4  ###reference_###, we now propose new cascading techniques that implement their deferral rule in a speculative manner. Recall that a\ntoken-level\ncascade of two models  and  is defined by a deferral rule .\nFor a prefix , the next-token distribution at position  modeled by this cascade can be written as:\nIn fact, for all the deferral rules described in §2  ###reference_###, the resulting distribution can be described by a target distribution function  of the form:\nfor some function  that maps distributions  to a binary decision. For example, for Chow, , and for Diff,  See Table 1  ###reference_### for a summary of target distributions for different deferral rules.\nOur proposal is to then invoke the speculative sampling procedure in Algorithm 4  ###reference_### with  as the target distribution function.\nWe outline this generic speculative cascading approach in Algorithm 5  ###reference_###, and contrast it with the sequential execution of a deferral rule in Algorithm 2  ###reference_###.\nUnlike a sequential cascade, where the larger model’s distribution  is not available at the time the deferral decision is made (see Remark 1  ###reference_ark1###), with a speculative cascade, we can accommodate deferral rules like Diff that depend on both  and . This is because we run the larger model  in parallel on drafts generated by the smaller model , allowing us to compute both  and  on every prefix.\nSo far we have considered deferral rules designed for sequential cascades. In what follows, we derive the optimal deferral rule  for a speculative cascade, where we sample speculatively from a target distribution  using  as the drafter."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Deferral risk for speculative cascades",
            "text": "As with sequential cascades (§2  ###reference_###), we begin by defining an objective to minimize. We seek a deferral rule  that minimizes a loss against the ground-truth distribution, while limiting the inference cost to be within a budget.\n(Per above, this deferral rule implicitly defines a target distribution .)\nThe inference cost crucially depends on how frequently a draft token is rejected in the verification phase, triggering a rollback.\nTo this end, we derive the probability that a token sampled from  is rejected during verification, for a target distribution resulting from a deferral rule .\nFor a given prefix , and target distribution , the probability of a token drawn from draft distribution  being rejected is equal to:\nwhere  is the TV distance between  and .\nIntuitively, whenever ,  and therefore there is no rejection or roll-back; when , the rejection rate equals .\nFor a fixed prefix , we formulate the goal of finding a solution to:\nfor some budget . Equivalently, one may minimize an unconstrained objective similar to (3  ###reference_###),\nfor suitable cost parameter  (see §C.4  ###reference_###):\nContrasting (8  ###reference_###) with the deferral risk in (3  ###reference_###) for a sequential cascade, a key difference is that the cost of deferring to the larger model is no longer a constant, but depends on the similarity between  and , as measured by the TV distance between them."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Optimal speculative deferral",
            "text": "We next derive the optimal deferral rule for (8  ###reference_###), and construct a feasible estimator for it.\nThe minimizer of (8  ###reference_###) is of the form:\nWhen  and  are similar, the rejection rate for  is low, and hence the deferral decision will depend largely on which of the two models yields a lower expected loss. When  and  are very different, the optimal decision is to defer to  only when it yields a substantially lower loss than .\nPlug-in estimator for (9  ###reference_###).\nThe optimal rule requires estimating expectations with respect the ground-truth distribution  We employ similar plug-in estimators as the ones used with sequential cascades (§3  ###reference_###). When , we replace the expected 0-1 loss with (one minus) the maximum probability from the model, giving us:\nThe efficacy of the plug-in estimator depends on how closely the individual models approximate the ground-truth distribution ; this is formalized by the following regret bound:\nSuppose . Then for a fixed prefix :\nOne can now run the speculative cascading procedure in Algorithm 5  ###reference_### using (10  ###reference_###) as the deferral rule; the corresponding  is listed in Table 1  ###reference_###. See §C.2  ###reference_### for a similar derivation for .\nIn practice it is common to apply a temperature scaling to both  and ,\nand sample from\n\nand\n\nrespectively, for a temperature parameter . In this case, the constrained problem in (7  ###reference_###) would use the TV distance between the temperature-scaled distributions  and  to measure the resulting rejection rate, and the optimal deferral rule in Lemma 4  ###reference_8### would now use  instead of . To construct a plug-in estimator to this optimal rule, we still prescribe using the unscaled probabilities  and  to estimate the expected loss, giving us, for :\nOne may run Algorithm 5  ###reference_### with  as the deferral rule and the temperature scaled  as the drafter.\nWhen temperature ,  whenever , and is zero otherwise. In this case, running Algorithm 5  ###reference_### with  as the deferral rule (and  as the drafter) is equivalent to running it with  in (5  ###reference_###) as the deferral rule. In other words, for greedy decoding, the optimal deferral rules for a speculative cascade coincides with that for a sequential cascade. We formalize this in Lemma 8  ###reference_ma8### in §C.3  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Further related work",
            "text": "There has been a stream of work on improving\nthe draft generation process in speculative decoding; these include having the drafter and verifier share the same backbone [39  ###reference_b39###, 25  ###reference_b25###, 6  ###reference_b6###, 30  ###reference_b30###, 21  ###reference_b21###, 51  ###reference_b51###, 16  ###reference_b16###, 27  ###reference_b27###], using multiple small draft models [9  ###reference_b9###, 46  ###reference_b46###], using tree-structured draft batches [38  ###reference_b38###, 29  ###reference_b29###], distilling the drafter with the verifier [52  ###reference_b52###], and leveraging multiple sampled candidates from the drafter [40  ###reference_b40###].\nThe work that is most closely related to our specific proposal is the Big Little Decoder (BiLD) [24  ###reference_b24###], which can be seen as another lossy variant of speculative decoding [26  ###reference_b26###, 44  ###reference_b44###, 52  ###reference_b52###]. BiLD has two phases: a fallback phase, during which the drafter  is run auto-regressively until its maximum predicted probability is sufficiently low; and a rollback phase, during which the verifier  is run in parallel on the prefixes generated by  and rolls back to the point where , for a metric  that measures discrepancy\nand threshold . The fallback phase can be seen as implementing Chow’s deferral rule (2  ###reference_###), and allows for the draft window size to vary dynamically based on an estimate of how likely the draft tokens will be accepted; the rollback phase can be seen as a deterministic variant of the rejection sampling algorithm of [26  ###reference_b26###].\nAn advantage of BiLD over the rejection sampling algorithm in [26  ###reference_b26###] is the use of Chow’s rule to vary the draft window size. However, the final target distribution it seeks to mimic, , is an approximation to , in that the target distribution  is chosen to satisfy . Hence, in cases where  deviates substantially from , BiLD would choose  as the target distribution, even  offers better quality (as captured by a suitable loss function in §4.4  ###reference_###) on a prefix. In contrast, our proposed approach in §4  ###reference_### uses speculative decoding to approximate target distributions that optimally cascade between  and .\nIn our experiments, we compare the efficacy of using  as the target distribution with the target distributions we propose in this paper (see Table 1  ###reference_###).\npropose in this paper (see Table 1  ###reference_###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experimental results",
            "text": "We present experiments to demonstrate that the proposed speculative cascading techniques yield better cost-quality trade-offs compared to both sequential token-level cascades and standard speculative decoding. We construct speculative cascades with three different deferral rules: (i) Chow in (2  ###reference_###), (ii) Diff in (5  ###reference_###) and (iii) OPT in (10  ###reference_###). Our experimental setup is based on [52  ###reference_b52###]; see D.1  ###reference_### for details.\n\nWe consider three benchmark language datasets: (i) WMT ENDE translation [4  ###reference_b4###], (ii) CNN/Daily Mail summarization [20  ###reference_b20###], and (iii) XSum abstractive summarization [31  ###reference_b31###]. We construct cascades from T5 v1.1 family of encoder-decoder models [34  ###reference_b34###], with T5-small (77M) as the small model, and either T5-large (800M) or T5-XL (3B) as the large model. In each case, we supervised fine-tune these models on the respective task. We use ROUGE-2 as the metric for the summarization tasks. To measure latency, we follow the protocol in [26  ###reference_b26###, 52  ###reference_b52###], and evaluate the wall-clock decoding time with a batch size of 1.\n\nSince our primary goal is to demonstrate the benefits of combining both cascading and speculative decoding, we compare against representative methods from both paradigms:\n\n- Sequence-level cascade [22  ###reference_b22###, 18  ###reference_b18###] based on sequence-level Chow’s rule in (1  ###reference_###) (SeqCascade [Chow])\n- Token-level cascade outlined in Algorithm 2  ###reference_###, with token-level Chow’s rule in (2  ###reference_###) used for deferral [10  ###reference_b10###, 19  ###reference_b19###] (TokenCascade [Chow])\n- Lossy speculative decoding described in §2  ###reference_###, with both  [26  ###reference_b26###, 52  ###reference_b52###] (SpecDecode [Lossy]) and with  tuned using the procedure in [44  ###reference_b44###] (SpecDecode [Lossy⋆])\n- A variant of the Big-Little Decoder approach [24  ###reference_b24###], which applies Algorithm 4  ###reference_### to the target distribution  in §5  ###reference_### (BiLD∗).\n\nWe do not include the oracle approach in Algorithm 3  ###reference_### as it is impractical to evaluate its running time. We also note that, while one could potentially integrate other recent variants of the speculative decoding (e.g., those in §5  ###reference_### that warrant changes to the model architectures) into our speculative cascades framework, in the interest of a fair comparison, we use the original version by [26  ###reference_b26###].\n\nWe evaluate all methods under both greedy decoding (), and temperature sampling with temperature . As noted in §4.3  ###reference_###, with greedy decoding, the OPT deferral rule coincides with the Diff deferral rule. We set the block size  for the methods based on speculative execution.\n\nIn Figure 2  ###reference_###, we present plots of quality versus latency for the different methods. In each case, we vary the lenience parameter , and plot either the ROUGE-2 metric as a function of the relative latency to the larger model. For brevity, we include the three main baselines; in §D.3  ###reference_### we compare to SpecDecode [Lossy⋆] [44  ###reference_b44###].\n\nClearly, methods that use speculative execution are considerably faster than sequential token-level cascades, although sequential cascades do have a slight advantage in the low-latency regimes (unlike speculative approaches, which always call the large model after every  steps, a sequential cascade only invokes the large model when the small model defers). Under temperature sampling (), the OPT speculative cascading strategy is seen to provide the best quality metrics for most latency values, with Diff coming in a close second. A similar trend is also seen with greedy decoding, where the optimal deferral strategy (SpecCascade [Diff]) often yields better quality trade-offs than the other methods, although the gap to speculative decoding is much smaller.\n\nIn Table 2  ###reference_###, we report (i) the reduction in latency from different methods when matching the quality of the large model, and (ii) the best quality that each method can deliver without exceeding the latency of the large model (for temperature ). SpecCascade ["
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We have proposed new speculative cascading techniques that use a combination of auto-regressive drafting and parallel verification to implement their deferral rule, and shown that they yield better cost-quality trade-offs than standard cascades and speculative decoding. In the future, we wish to replace our plug-in estimators with a router model [18] trained explicitly on ground-truth samples to approximate the optimal rule.\n\nWe also wish to improve the deferral objective we seek to optimize at each position, and replace it with a global (coupled) deferral objective that takes all prefixes into account. Another useful direction to explore would be to extend our proposal to handle a general cascade with more than two models."
        }
    ]
}