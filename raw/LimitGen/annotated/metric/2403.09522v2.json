{
    "title": "MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
    "abstract": "Large Language Models (LLMs) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, leading to repetitive teaching on the knowledge the student models have already learned and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive, and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, rather than distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% of examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performance on unseen contexts and words.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLM) have shown their impressive capabilities across almost all natural language tasks (Brown et al., 2020; Zhao et al., 2023). However, their ability strongly correlates with the model size. In the field of machine translation, competitive results can only be evidenced on larger LLMs, while medium-sized LLMs like Alpaca (Taori et al., 2023) and ParroT (Jiao et al., 2023a) still lag behind supervised NMT systems by a large margin (Jiao et al., 2023a; Zhu et al., 2023). How to efficiently transfer knowledge from larger LLMs to existing MT models that are affordable to deploy, is an important research direction.\n\nThe most common method for knowledge transferring is knowledge distillation (KD) (Hinton et al., 2015; Kim and Rush, 2016), where given an unlabeled corpus, a student model is trained to mimic the output of a teacher model on the corpus. Although KD is a well-studied technique and has proven effective in many previous works (Kim and Rush, 2016; Wang et al., 2021; Liu et al., 2023), we argue that when transferring knowledge from giant LLMs to existing MT models, the traditional KD method does not take the capability of the student and teacher model into consideration, therefore leaving much room for improvement in terms of both efficiency and effectiveness.\n\nFirstly, in contrast to student models in previous works (Kim and Rush, 2016; Wang et al., 2021; Liu et al., 2023) that are randomly initialized, recent student MT models (Hsieh et al., 2023; Fu et al., 2023) already exhibit a reasonable level of language proficiency, i.e., they can already accurately translate most examples in the unlabeled corpus. This renders the fine-tuning of student models on all teacher outputs both redundant and inefficient.\n\nSecondly, the efficacy of KD is significantly constrained by the coverage of the monolingual corpus, which impedes their performance when translating words in novel contexts or words unseen in the monolingual corpus. However, modern LLMs grasp strong translation and language knowledge, as well as the ability to follow human instructions. This enables the development of more efficient and effective strategies for addressing these problems.\n\nIn this paper, we introduce MT-Patcher, a novel framework designed for the knowledge transfer from LLMs to existing MT models in a selective, comprehensive, and proactive manner. The design philosophy of MT-Patcher is inspired by effective teaching strategies observed in real-world scenarios. Rather than subjecting students to endless drills, an effective teacher would first assess the student’s current abilities, then design practice to reinforce areas of weakness and extend learning to new situations (Lee Jr and Pruitt, 1979; Epstein and Voorhis, 2001). Leveraging the strong language capabilities of LLMs, our method seeks to emulate these pedagogical strategies. Specifically, we gather instructional data from GPT-4, which demonstrates how to identify and correct errors in student model translations, anticipate additional potential errors that the student models may commit, and synthesize diverse contexts for relevant translation knowledge that aids the student model in rectifying these errors. We subsequently fine-tune an existing proficient LLM on these data to transform it into an MT-Patcher model.\n\nWe conduct experiments on translating specific language phenomena (chemistry materials and Chinese idioms) and on general machine translation benchmarks (WMT22 Chinese-English, English-German and English-Japanese). Experimental results show that fine-tuning the student model on only 10% examples selected by MT-Patcher is equivalent to fine-tuning on all examples as in KD, and enlarging the fine-tuning corpus via the context synthesis and proactive error prediction technique further improves the translation performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Numerous studies have attempted to leverage LLMs for machine translation. Initial efforts (Lin et al., 2022; Vilar et al., 2022; Agrawal et al., 2023; Zhu et al., 2023; Hendy et al., 2023; Jiao et al., 2023b) centered on in-context learning, which utilizes several translation examples to guide the translation behavior of LLMs. Subsequent research (Jiao et al., 2023a; Li et al., 2023) shifted the focus to fine-tuning LLMs on existing parallel corpora to more effectively harness their translation capabilities. However, the translation performance of LLMs has not been as remarkable as their performance in other NLP tasks. Only state-of-the-art LLMs such as GPT-3 and GPT-4, which boast more than 100 billion parameters, can rival the performance of commercial translation systems (Hendy et al., 2023; Jiao et al., 2023b). Meanwhile, other medium-sized LLMs significantly trail behind supervised MT models (Zhu et al., 2023; Li et al., 2023; Jiao et al., 2023a). Li et al. (2023) suggest that the primary barrier to enhancing LLMs’ performance is the lack of translation knowledge. Given that larger LLMs inherently possess more knowledge due to the scaling law (Kaplan et al., 2020), our work concentrates on transferring knowledge from these models to existing MT models.\n\nKnowledge distillation (KD), which improves smaller student models by learning on larger teacher models’ output, is widely used in machine translation. Two common KD methods are LogitKD (Hinton et al., 2015; Tan et al., 2018), which optimizes the student model to match the teacher model’s predicted distribution, and Sequence KD (SeqKD) (Kim and Rush, 2016; Wang et al., 2021; Gu et al., 2018; Zhou et al., 2019), where the student learns from the teacher-generated pseudo target sequence. As LogitKD requires access to the teacher’s logits, it is impractical for distilling from proprietary LLMs. Therefore, we base our method on SeqKD, where student refers to the smaller MT model we would like to improve, and teacher refers to larger LLMs which possess more translation knowledge than student. Selective KD has been proposed by Wang et al. (2021) and Liu et al. (2023), but they all rely on comparing student models’ outputs to oracle references. Unlike these works, our method instructs the LLM to identify student translation errors directly.\n\nWith the growing generative capabilities of Large Language Models (LLMs), many works attempt to harness them for corpora generation. The generated corpora can serve as demonstrations for few-shot prompting (Sahu et al., 2022), fine-tuning corpora for existing models (Yoo et al., 2021), or seed corpora for human refinement (Yuan et al., 2021a). Studies such as Chung et al. (2023); Yu et al. (2023) also explore ways to balance diversity, accuracy, and bias reduction in LLM-based dataset synthesis. However, these approaches often generate datasets from scratch, ignoring the capabilities of the models being optimized, resulting in less efficiency compared to our method."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we present MT-Patcher , a framework that distills knowledge from LLMs to existing MT systems more efficiently and effectively. The process of MT-Patcher undergoes two stages:\nKnowledge Selection: In this stage, the LLM acts as the feedbacker, which provides natural language feedback to translations of student models. Based on the\nfeedback, we select source sentences with identified errors, which indicate knowledge deficiency of the student models, to the next stage.\nKnowledge Extension: In this stage, the LLM acts as the parallel data synthesizer and word analoger, which help the student model learn words it makes mistakes on by extending to more diverse contexts and similar words.\nFigure 1  ###reference_### illustrates how MT-Patcher works.\nThe goal of the parallel data synthesizer is to synthesize parallel sentences  that contain a specific pair of phrases  where the student model makes mistakes in the context , in order to generalize the current translation knowledge to more contexts. Ideally, the synthesized parallel sentences should be semantically diverse yet still similar to the original context in other aspects. However, in the preliminary experiments, we find that even for powerful LLMs like GPT-4, when conditioning them on the original context , the generated parallel data lacks diversity and mostly resembles .\nTo tackle this problem, we introduce another module called sentence analyzer, which first extracts the information of domain, topic and style of the original context. We then instruct the LLMs to synthesize parallel sentences with the same attributes as well as containing the phrase pair . This process can be seen as an information bottleneck where we squeeze the semantic information yet keep other attributes.\nWe further introduce the word analoger to proactively predict potential errors the student model may commit. For example, if the student MT model incorrectly translates the term methanol, an educated guess is that it may struggle with translating words within the domain of chemistry, such as benzene and ethanol. By anticipating these potential errors, we can enhance the student model’s translation capability for words not present in the monolingual corpus.\nPractically, given a source sentence  and a word  that the student MT model mistranslates, the word analoger aims to associate more words from two perspectives: (1) category, i.e., words belonging to the same category as , and (2) semantic, i.e., words that frequently co-occur with . We also require that the generated words should be rare and challenging in the prompt, ensuring that the student model will struggle to translate them accurately."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Knowledge Selection via Feedbacker",
            "text": "When transferring knowledge from LLMs to existing MT models, traditional SeqKD would finetune the student model on all teacher’s output, ignoring the fact that the student model can already translate most of the examples well. Furthermore, several recent studies have unveiled emergent abilities in LLMs, such as Self-Refinement (Madaan et al., 2023  ###reference_b21###) and Self-Debug (Chen et al., 2024  ###reference_b4###), suggesting that iterative refinement of an initial draft may be a more effective strategy to tap into the knowledge reserves of LLMs.\nTo improve the efficiency of SeqKD and better elicit LLMs’ knowledge, we propose to finetune LLMs to be a feedbacker, which produces natural language feedback of the student models’ translation instead of directly generating its own translations.\nFormally, given a source sentence  and its corresponding translation , the goal of the feedbacker is to generate a comprehensive assessment . This assessment comprises tuples of , where  describes whether  contains translation errors,  corresponds to the source span, explanation and correction of the -th identified error, respectively, and  is the final post-edited translation that incorporates all error corrections."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Knowledge Extension via Parallel Data Synthesizer and Word Analoger",
            "text": "Another limitation of SeqKD is that the knowledge it can transfer is strictly limited to the given monolingual corpus. This limitation can hinder its generalizability in two key ways. Firstly, the correct translation of mistranslated words or phrases can only be learned within the contexts present in the given monolingual corpus, potentially limiting its applicability to broader contexts. Secondly, SeqKD also lacks the capacity for knowledge extrapolation, which prevents it from transferring knowledge that does not occur in the monolingual corpus.\nInspired by the principle of knowledge extension when designing good practice in the educational process (Lee Jr and Pruitt, 1979  ###reference_b17###; Epstein and Voorhis, 2001  ###reference_b6###), we transform LLMs into two modules to mitigate above two problems, respectively: parallel data synthesizer and word analoger.\nThe goal of the parallel data synthesizer is to synthesize parallel sentences  that contain a specific pair of phrases  where the student model makes mistakes in the context , in order to generalize the current translation knowledge to more contexts. Ideally, the synthesized parallel sentences should be semantically diverse yet still similar to the original context in other aspects. However, in the preliminary experiments, we find that even for powerful LLMs like GPT-4, when conditioning them on the original context , the generated parallel data lacks diversity and mostly resembles .\nTo tackle this problem, we introduce another module called sentence analyzer, which first extracts the information of domain, topic and style of the original context. We then instruct the LLMs to synthesize parallel sentences with the same attributes as well as containing the phrase pair . This process can be seen as an information bottleneck where we squeeze the semantic information yet keep other attributes.\nWe further introduce the word analoger to proactively predict potential errors the student model may commit. For example, if the student MT model incorrectly translates the term methanol, an educated guess is that it may struggle with translating words within the domain of chemistry, such as benzene and ethanol. By anticipating these potential errors, we can enhance the student model’s translation capability for words not present in the monolingual corpus.\nPractically, given a source sentence  and a word  that the student MT model mistranslates, the word analoger aims to associate more words from two perspectives: (1) category, i.e., words belonging to the same category as , and (2) semantic, i.e., words that frequently co-occur with . We also require that the generated words should be rare and challenging in the prompt, ensuring that the student model will struggle to translate them accurately."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Implementation of MT-Patcher",
            "text": "Theoretically, state-of-the-art LLMs like GPT-4 can already serve as an MT-Patcher to transfer its knowledge to MT models. However, in practice, because we do not have unlimited access to GPT-4, we instead collect the demonstration data from GPT-4. Specifically, given a student model, we first use it to generate its translation on 20,000 monolingual sentences randomly selected from the monolingual corpus. We then leverage GPT-4 to execute the pipeline of MT-Patcher including (1) giving feedback  given the source sentence and student’s translation , (2) analyzing the domain, topic and style  of the source sentence  (3) making analogies  given the source sentence  and a word  in  (4) synthesizing parallel sentences containing error source words  and their corrections  with the same domain, topic and style attribute . Finally, we finetune the teacher LLM on these data to transform it to an MT-Patcher. All prompts we use for building MT-Patcher can be found in Appendix A  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate our method on Chinese-English and English-German translation. For student translation models, we consider NLLB-200 3.3B, a multilingual translation model pre-trained on 200 languages. Having been trained on massive parallel data, it can already translate reasonably well but falls short of language knowledge compared to LLMs, making it an ideal knowledge recipient for our experiment.\n\nDue to the increasing interest in adopting LLMs for MT, we also consider ParroT, an LLM-based MT model finetuned on WMT validation sets from LLaMA-7B. The backbone LLMs for building MT-Patcher in this paper are LLaMA2-13B and Baichuan-2-13B. LLaMA2-13B is an English LLM and used to build MT-Patcher for English-German translation models. Baichuan-2-13B is trained on a mix of both Chinese and English corpus and demonstrates much stronger abilities in Chinese compared to LLaMA2. Therefore, we adopt it for building MT-Patcher for Chinese-English translation models. For each language pair considered, we fully finetune the corresponding LLM on the collected data for 3 epochs.\n\nWe compare the translation performance of the following methods: \n- Student is the translation model to be patched. In this paper, it refers to NLLB 3.3B or ParroT.\n- Teacher is the model that is achieved by finetuning the larger LLM to perform translation directly. \n- SeqKD are models achieved by finetuning the Student model on the Teacher’s translations.\n- MT-Patcher (PE) is the variant of MT-Patcher, finetuning the Student model on the post-editing results in feedback.\n- MT-Patcher (PE + PDS) is the variant of MT-Patcher which finetunes the Student model on the post-editing results as well as additional synthesized parallel sentences. \n\nFrom Table 1, we can first see that the performance of MT-Patcher (PE) is better than SeqKD-Equal, and can be comparable to SeqKD-Full. This indicates the proposed method can select more valuable examples and discard useless examples. We also find our method suffers less from catastrophic forgetting compared to SeqKD-Full. This makes MT-Patcher an appealing method for real-world applications, considering the cost for finetuning the Student model is growing nowadays.\n\nWe can also see that applying the parallel data synthesizer and word analoger to generate more patch data can further improve the translation performance of MT-Patcher, highlighting the benefits of extending coverage of context and knowledge during the process of knowledge transferring. \n\nIt is worth noting that in the English-German direction, the teacher based on LLaMA-2-13B performs substantially worse than the student (NLLB 3.3B), which is consistent with previous findings that it is not trivial to adopt existing LLMs to outperform supervised translation models. As a result, SeqKD from this teacher leads to poor performance. However, based on the same backbone LLM, MT-Patcher can still improve the performance of the Student model. This can be attributed to the hypothesis that revising an initial draft is a better way to elicit the knowledge of LLMs than direct generation, which we provide a further analysis.\n\nAlthough we mainly focus on settings where we have strong teachers (which is why we choose different teacher models for English-German and Chinese-English translation), we also experiment with medium resource translation: WMT22 English-Japanese, using LLaMA2 as the teacher and NLLB 3.3B as the student. We present the results in Table 3. We find MT-Patcher can still outperform SeqKD in this setting.\n\nFrom Table 2, we can see that despite that the Teacher model achieves significantly better performance than the Student model, the SeqKD-Full method can only narrow less than half of the gap. However, by synthesizing more contexts for each error, MT-Patcher (+PE + PDG) improves the relative performance from 59.2% to 80.5% for chemistry materials, and 57.5% to 69.8% for Chinese Idioms, indicating the importance of translation knowledge in multiple contexts in order to generalize to novel contexts better.\n\nWe can also observe that both SeqKD-Full and MT-Patcher (+PE + PDG) cannot behave well on the Unseen Word set, which can be attributed to their inability to extrapolate from the observed errors to unseen errors. By generating analogous words to anticipate more errors, the translation performances on Unseen Word are significantly improved, validating the effectiveness of the proposed error anticipation method."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "For student translation models, we consider NLLB-200 3.3B (NLLB Team et al., 2022), a multilingual translation model pre-trained on 200 languages. Having been trained on massive parallel data, it can already translate reasonably well but falls short of language knowledge compared to LLMs, making it an ideal knowledge recipient for our experiment. Due to the increasing interest in adopting LLMs for MT, we also consider ParroT (Jiao et al., 2023a), an LLM-based MT model fine-tuned on WMT validation sets from LLaMA-7B (Touvron et al., 2023).\n\nThe backbone LLMs for building MT-Patcher in this paper are LLaMA2-13B (Touvron et al., 2023) and Baichuan-2-13B (Baichuan Inc, 2023). LLaMA2-13B is an English LLM and used to build MT-Patcher for English-German translation models. Baichuan-2-13B is trained on a mix of both Chinese and English corpus and demonstrates much stronger abilities in Chinese compared to LLaMA2. Therefore, we adopt it for building MT-Patcher for Chinese-English translation models. For each language pair considered, we fully fine-tune the corresponding LLM on the collected data for 3 epochs. See Appendix B for more implementation details.\n\nWe compare the translation performance of the following methods:\n\nStudent is the translation model to be patched. In this paper, it refers to NLLB 3.3B or ParroT.\n\nTeacher is the model that is achieved by fine-tuning the larger LLM to perform translation directly. For a fair comparison, we fine-tune the LLM on GPT-4’s translation on the monolingual sentences.\n\nSeqKD are models achieved by fine-tuning the Student model on the Teacher’s translations.\n\nMT-Patcher (PE) is the variant of MT-Patcher, fine-tuning the Student model on the post-editing results in feedback.\n\nMT-Patcher (PE + PDS) is the variant of MT-Patcher which fine-tunes the Student model on the post-editing results as well as additional synthesized parallel sentences generated by parallel data synthesizer containing (error, correction) pairs. Unless other stated, we set the number of pseudo-parallel sentences to be 4 in this paper.\n\nMT-Patcher (PE + PDS + WA) is the variant of MT-Patcher which fine-tunes the Student model on the post-editing results and parallel sentences generated by parallel data synthesizer containing (error, correction) pairs and additional word pairs from word analoger. We generate 2 analogous words for each category and 1 context for each word."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results on General Machine Translation",
            "text": "Table 1 presents experimental results on general machine translation benchmarks: WMT22 Chinese-English and English-German translation. We randomly select 1,000,000 sentences from RefinedWeb and WuDao 2.0, respectively, as English and Chinese monolingual corpus. Performance is evaluated in COMET and sacreBLEU. \n\nFrom Table 1, we can first see that the performance of MT-Patcher (PE) is better than SeqKD-Equal and can be comparable to SeqKD-Full. This indicates the proposed method can select more valuable examples and discard useless examples. We also find our method suffers less from catastrophic forgetting compared to SeqKD-Full (See Appendix C for more experimental results). This makes MT-Patcher an appealing method for real-world applications, considering the cost for finetuning the Student model is growing nowadays.\n\nWe can also see that applying the parallel data synthesizer and word analoger to generate more patch data can further improve the translation performance of MT-Patcher, highlighting the benefits of extending coverage of context and knowledge during the process of knowledge transferring.\n\nIt is worth noting that in the English-German direction, the teacher based on LLaMA-2-13B performs substantially worse than the student (NLLB 3.3B), which is consistent with previous findings that it is not trivial to adopt existing LLMs to outperform supervised translation models. As a result, SeqKD from this teacher leads to poor performance. However, based on the same backbone LLM, MT-Patcher can still improve the performance of the Student model. This can be attributed to the hypothesis that revising an initial draft is a better way to elicit the knowledge of LLMs than direct generation, which we provide a further analysis in Section 5.2.\n\nAlthough we mainly focus on settings where we have strong teachers (which is why we choose different teacher models for English-German and Chinese-English translation), we also experiment with medium resource translation: WMT22 English-Japanese, using LLaMA2 as the teacher and NLLB 3.3B as the student. We present the results in Table 3. We find MT-Patcher can still outperform SeqKD in this setting."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results on Specific Language Phenomena",
            "text": "In order to understand how MT-Patcher can improve the effectiveness of knowledge transfer, we present experiments on the Chinese-to-English translation for two specific language phenomena: chemistry materials and Chinese idioms. We select them for two reasons: (1) Both belong to long-tailed knowledge that student MT models cannot grasp very well. (2) There are also distinctions between them: chemistry materials represent simple, context-free knowledge, while Chinese idioms represent more abstract and metaphorical knowledge.\n\nSpecifically, for each language phenomenon, we first collect a list of 6,000 of them and their corresponding translations from the web. We then split these word pairs into two categories: Seen and Unseen, and create a monolingual set as well as two test sets based on the split:\n\nMonolingual Set. For each word pair in the Seen set, we ask GPT-4 to synthesize one sentence that contains the source word. This set is for SeqKD and MT-Patcher to leverage.\n\nTest Set for Unseen Context. For each word pair in the Seen set, we also ask GPT-4 to synthesize one parallel sentence pair that contains the source and target word in the source and target sentence, respectively. This set is for testing models’ generalization ability when source words are seen yet contexts are novel.\n\nTest Set for Unseen Word. We collect the test set for Unseen Word in a similar way as Unseen Context using the word pairs in the Unseen set. This set is for testing models’ generalization ability to novel words.\n\nWe take the Baichuan-2-13B as the LLM and NLLB 3.3B as the student model, and present the experimental results in Table 2. The accuracy of translating chemistry materials represents the percentage of test examples where the correct translation of the source chemistry material is found in the translation. Regarding Chinese idioms, due to the difficulty of providing reference translations of them, we instead ask GPT-4 to assess the translation quality given the source sentence, target sentence, and dictionary definition. We report the average score, which ranges from 0 to 5. For ease of comparison, we also report how different models perform relative to the feedbackers, for which we directly take its correction as the translation.\n\nFrom Table 2, we can see that despite that the Teacher model achieves significantly better performance than the Student model, the SeqKD-Full method can only narrow less than half of the gap. However, by synthesizing more contexts for each error, MT-Patcher (+PE + PDG) improves the relative performance from 59.2% to 80.5% for chemistry materials, and 57.5% to 69.8% for Chinese Idioms, indicating the importance of translation knowledge in multiple contexts in order to generalize to novel contexts better.\n\nWe can also observe that both SeqKD-Full and MT-Patcher (+PE + PDG) cannot behave well on the Unseen Word set, which can be attributed to their inability to extrapolate from the observed errors to unseen errors. By generating analogous words to anticipate more errors, the translation performances on Unseen Word are significantly improved, validating the effectiveness of the proposed error anticipation method."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We provide further analysis on how MT-Patcher works and its applicability to real-world scenarios. All experiments are conducted on the WMT22 Chinese-to-English translation datasets, and the student MT model is NLLB 3.3B.\n###figure_2###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Impact of the number of synthesized contexts per word and analogous word",
            "text": "In Figure 2  ###reference_###, we plot how increasing the number of synthesized contexts per word and analogous words affects the translation performance of the student model. Note that we only synthesize one context for each analogous word. We can see increasing both numbers results in improved translation performance. For synthesized contexts, the gain plateau between 16 to 32 suggests this amount of different contexts is adequate for word or phrase learning. For analogous words, however, we observe the performance grows at a log-linear rate 333It is worth noting that this does not mean MT-Patcher can improve the translation performance endlessly, since it cannot generate an unlimited amount of valid analogous words. The performance will eventually plateau, although we have not scaled to the number due to the computational limitation..\n###figure_3###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Does asking for feedback better elicit LLMs’ translation knowledge?",
            "text": "We conduct a head-to-head comparison between two ways to leverage the teacher LLM: ask the teacher to directly provide translation vs. ask MT-Patcher to give feedback on the student’s translation. Specifically, we randomly select 1000 examples and compare the correction provided by MT-Patcher to the translation provided by the teacher. The comparison is made by both human and GPT-4.\nThe results are shown in Figure 3  ###reference_###. It can be seen that MT-Patcher’s corrections are considered by both GPT-4 and human evaluators to be comparable or better than the teacher’s translation on more than 80% examples, demonstrating the benefits of eliciting LLM’s knowledge in the form of feedback."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "The Effectiveness of Iterative Feedback",
            "text": "###figure_4### In this section, we explore whether the application of iterative feedback on post-edited translations can enhance the final translation quality, thereby yielding a better Student model. While iterative feedback may incur additional computational costs, it allows us to compare feedback across multiple iterations and assess the reliability of error identification and correction from the feedbacker. Intuitively, if an error span identified and rectified in the -th epoch is still deemed problematic in the subsequent epoch, it suggests an inconsistency in the feedbacker’s decision-making process. To prevent the introduction of incorrect knowledge during the knowledge transfer process, examples with such inconsistencies are discarded.\nWe randomly select 2000 instances of MT-Patcher’s feedback on NLLB-3B’s translation results and apply iterative feedback. We then ask GPT-4 to evaluate the feedback quality after each iterative feedback epoch. The results, depicted in Figure 4  ###reference_###, indicate that iterative feedback can enhance the accuracy of corrections in remaining examples, converging to 90.4% after 4 epochs at the expense of filtering out approximately 20% of examples. To understand the quality-quantity trade-off of demonstration data, we further fine-tune the Student NLLB model on post-editing data after each iterative feedback epoch and display the translation performance in Table 4  ###reference_###. Despite a decrease in the amount of fine-tuning data as the epoch increases, the translation performance of the fine-tuned model continues to improve, highlighting the significance of high-quality fine-tuning data."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Transferability of MT-Patcher",
            "text": "The construction of MT-Patcher is model-dependent; that is given an MT model, LLMs are finetuned on the data from GPT-4 which demonstrates how to execute the MT-Patcher pipeline on the translation of the corresponding MT model. Considering the cost of data collection and model training, one may question whether MT-Patcher is transferable, i.e., a patcher model for one MT model can improve the performance of another MT model. We present such results in Table 5  ###reference_###. Although the performance of applying MT-Patcher to its dedicated MT model is superior, the application of MT-Patcher trained on another model still significantly surpasses the baseline results, suggesting the potential for a robust MT-Patcher across various MT models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce MT-Patcher, a framework designed to leverage capabilities of LLMs to enhance the efficiency and effectiveness of translation knowledge transfer from LLMs to existing MT models. Our approach involves a pipeline that initially generates feedback on translations produced by MT models, followed by the synthesis of potential errors and diverse contexts to systematically rectify these translation errors. Through experimentation on both general and narrow domain MT benchmarks, we demonstrate that MT-Patcher effectively improves student MT models’ performances compared to SeqKD baselines, and exhibits successful transferability across different models. In the future, we plan to refine our method from two angles. Firstly, previous works have identified translationese as a significant issue, and training on pseudo data generated by LLMs can exacerbate this problem. A promising solution could involve retrieving target sentences containing correction words and back-translating them to the source side. Secondly, the feedback’s reason field contains a wealth of valuable information. We intend to explore more efficient strategies to harness this data."
        }
    ]
}