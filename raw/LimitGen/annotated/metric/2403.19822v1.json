{
    "title": "Multi-Stage Multi-Modal Pre-Training For Automatic Speech Recognition",
    "abstract": "Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. Additionally, we share several important findings for choosing pre-training methods and datasets.\n\nKeywords: Multi-modal, Speech Recognition, Self-supervised, Pre-training, Mid-training",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Despite progress in large-scale pre-training for automatic speech recognition (ASR), uni-modal (speech-only) ASR remains a challenging task, particularly when faced with rare words and noisy acoustic conditions. When understanding spoken phonemes, the model must correctly discern both speaker-specific patterns (e.g., accent, prosody) and global noise patterns (e.g., background noise, intermittent interruptions, confounding speakers). Recent work in natural language processing, robotics, and computer vision has demonstrated that exposing models to a high diversity of data during pre-training is essential in building robust representations.\n\nSimilarly, recent works in the ASR community have corroborated these results. Pre-training on large-scale audio-visual data leads to better performance on tasks like lip-reading. Exposure to video data during pre-training has shown performance improvements not only when visual input is available at training time, but also when only audio is available at test time.\n\nIn this work, we not only explore two new audio-visual pre-training sources, but also leverage a translation task with English speech input as a new mid-training task to consolidate information learned during the pre-training phase. While previous work explored an attention-based transfer-learning framework based on k-means clustering for pre-training, we simplify the pre-training architecture significantly and explore several pre-training objectives beyond masked cluster prediction. Our primary contributions are as follows:\n\nWe perform large-scale evaluation of multiple audio-visual pre-training methods using several pre-training datasets with varying characteristics. We evaluate them on the ASR task and the SUPERB benchmark, showing how multi-modal pre-training is affected by key dataset characteristics.\n\nWe introduce a novel mid-training stage between the pre-training and fine-tuning steps, using speech translation as the mid-training task. The technique also shows improvements on several tasks (Keyword Spotting, Intent Classification, Phoneme Recognition, and Speaker Diarization) in the SUPERB benchmark."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background",
            "text": "Representation learning methods like Contrastive Predictive Coding Oord et al. (2018  ###reference_b53###) and Wav2Vec Schneider et al. (2019  ###reference_b62###) have shown significant promise when applied to ASR. Methods for large-scale pre-training for ASR can be categorized into two methods: masked autoencoding methods Hsu et al. (2021  ###reference_b35###); Chen et al. (2022  ###reference_b17###), and contrastive learning Baevski et al. (2020  ###reference_b7###). While traditionally self-supervised methods are trained on a single target loss, other methods have been proposed which leverage multiple pre-training targets. Pascual et al. (2019  ###reference_b56###); Talnikar et al. (2021  ###reference_b67###); Wang et al. (2021a  ###reference_b73###) all optimize a combination of uni-modal supervised losses and recently, approaches such as W2v-BERT Chung et al. (2021  ###reference_b22###) and JUST Bai et al. (2022  ###reference_b8###) have combined contrastive approaches with masked auto-encoding to build robust self-supervised speech representations. Similarly, while most self-supervised methods are pre-trained on a single dataset, Radford et al. (2022  ###reference_b59###); Narayanan et al. (2018  ###reference_b52###); Likhomanenko et al. (2020  ###reference_b46###); Chan et al. (2021  ###reference_b16###) have all demonstrated that a wide mix of data is essential for pre-training. In this work, we target both of these problems: use a combination of losses, and pre-training stages under different datasets to improve the learned multi-modal representations.\nAudio-visual data provides diverse information for representation learning. Shi et al. (2022  ###reference_b63###) demonstrate improvements on ASR when visual input is available (at both training and test time), and methods such as u-HuBERT Hsu and Shi (2022  ###reference_b34###) extend such pre-training approaches to applications where both uni-modal and multi-modal data are available at training-time (but still require multi-modal data for inference). Later work by Chan et al. (2022  ###reference_b14###) demonstrated that pre-training with paired audio-visual data, can even improve performance on uni-modal datasets.\nIn addition to multiple modalities, pre-training with multiple languages has also been explored in the literature. Radford et al. (2022  ###reference_b59###) demonstrate that pre-training with a wide range of inputs from several languages improves ASR performance across all of the studied languages. Lahiri et al. (2021  ###reference_b43###) show that leveraging self-supervised learning (SSL) for knowledge transfer across languages can yield WER improvements of up to 3.55% relative WER on target languages, and Karimi et al. (2022  ###reference_b38###) demonstrate that in almost all cases, even out-of-domain multi-lingual data can improve WER in single and multi-speaker conversations and dictation tasks.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methods",
            "text": "Our method (Figure 1  ###reference_###), consists of a multi-stage multi-modal pre-training approach, followed by a fine-tuning stage on downstream tasks. We describe our method in this section."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Pre-Training Tasks",
            "text": "We experiment with two pre-training strategies that differ in the granularity of information they extract. The first method, Masked Autoencoder (MAE), learns local features by reconstructing masked parts of speech and video. The second method, Contrastive Learning (CLR), focuses on global features by using pooled audio-visual features from the same video as positive pairs while other combinations of audio-visual pairs as negatives. The two pre-training strategies help us compare the effects of local and global feature learning against the visual-audio dataset characteristics, for eg., Kinetics dataset Carreira et al. (2018  ###reference_b11###) has non-speech audio streams, while LRS-3 Afouras et al. (2018  ###reference_b2###) and Voxceleb2 Chung et al. (2018  ###reference_b21###) datasets have videos with speech.\nMasked Autoencoding (MAE): Traditional MAE approaches for ASR pre-training have focused on token-based reconstruction Hsu et al. (2021  ###reference_b35###); Shi et al. (2022  ###reference_b63###); Chan et al. (2022  ###reference_b14###). However these methods have the drawback of requiring a separate quantization method, which can add significant training complexity. We simplify the encoder to directly reconstruct features from the original masked signal.\nOur MAE approach consists of three encoders: , a masked audio-specific encoder based on the encoder in Chen et al. (2022  ###reference_b17###), , a masked video-specific encoder based on Tong et al. (2022  ###reference_b68###), and , a joint transformer decoder with the same structure as in Devlin et al. (2018  ###reference_b23###).\nLet  be the set of audio input frames (we use f-dimensional log-filterbank energies (LFBE)), and  be a set of video frames, which have been subdivided into  voxels.  refers to number of audio frames and  are number of video frames of height  and width . To generate the input sequence to , we randomly mask a fraction  of the audio frames with 0s (masking), and generate the embedded audio . We use a similar process to mask voxels, to generate .\nThe encoded representations  and  are passed through the common decoder  to produce  and  respectively. The common decoder  ensures that the representations  and  are projected to the same representation space.\nThe final MAE loss is computed as the squared L2 distance between  and , and  and :\nContrastive Learning (CLR): Contrastive Learning aims to learn representations using a contrastive loss that minimizes the distance between similar points and maximizes the distance between dissimilar points in a latent space. For contrastive learning, following Radford et al. (2021  ###reference_b58###) and Xu et al. (2021  ###reference_b76###), we use the modality specific encodings  and  to generate , where the pooling operation is a temporal average, and , where the pooling operation is a spatio-temporal average. While other pooling operations like attention pooling are possible, we found that the spatio-temporal average captures consistent low-frequency global information, which correlates well with the information shared with the visual modality (unlike high-frequency information, which is often not evident from the visual modality). The self-supervised contrastive loss for a batch of samples , and  is computed as\nMAE + CLR: In this setup, we combine the benefits of learning local features using MAE with learning global features using CLR as shown in Figure 1  ###reference_###. Both pre-training losses are added with equal weights, similar to Chung et al. (2021  ###reference_b22###) to compute the final loss as\nPre-training Datasets: We use three datasets for pre-training. The Kinetics-600 dataset Carreira et al. (2018  ###reference_b11###) has 966 hours of audio-visual data for activity recognition, with a focus on the environment or instrument used. The videos contains non-speech audio data and have been used previously for audio-visual training Chan et al. (2022  ###reference_b14###). Voxceleb2 Chung et al. (2018  ###reference_b21###) provides 2380 hours of multi-lingual speaker recognition data with challenging acoustics and comprehensive lip and facial movements. LRS3 Afouras et al. (2018  ###reference_b2###) features 346 hours of clean, multi-modal spoken sentence data from TED and TEDx videos. The speech data in Voxceleb2 is has noisy acoustic conditions whereas LRS-3 has clean speech with speakers talking to a close-talk microphone. These datasets allow for exploring the impact of clean-speech/noisy-speech/non-speech videos and pre-training techniques on the ASR task (subsection 3.3  ###reference_###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Mid-Training: Speech Translation",
            "text": "To improve performance of the pre-trained audio-visual models on the downstream tasks, we introduce a mid-training task that bridges the gap between pre-training and fine-tuning. Our approach transfers the learned distribution of the pre-trained model towards the distribution required for the downstream task, while discarding irrelevant information.\nThe mid-training task is designed to provide a low-cost warm-up for the pre-trained model, which can accurately represent various characteristics of the data. We chose to mid-train our audio encoder on the speech translation task using the MuST-C dataset Di Gangi et al. (2019  ###reference_b24###) in three languages, German, Italian and Dutch. This stage is useful for aligning the learned speech representations with the text modality which is beneficial for ASR, as shown in recent work in the speech representation learning spaceZhang et al. (2023  ###reference_b80###). Our audio encoder was mid-trained until convergence on the speech translation task. This mid-training approach is the key to strong performance in downstream tasks, which we demonstrate in detail in section 4  ###reference_###.\nUsing translation as a mid-training task is only one possible instantiation of the mid-training approach. In addition to translation, future work can explore other speech-centric tasks like speaker identification, implied by Chan and Ghosh (2022  ###reference_b13###)), speaker/source separation, text to speech, and others. While we found that translation is effective in this work, we expect that each additional task will impact the downstream training process in unique ways."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Fine-Tuning",
            "text": "We evaluated our models by testing their performance on several downstream tasks. The fine-tuning task is distinct from the pre-training task of masked reconstruction (MAE) or contrastive learning (CLR), and the mid-training task designed to bridge the gap. Primarily, we evaluate the performance of the models on the test-clean and test-other Librispeech Panayotov et al. (2015  ###reference_b54###) datasets for ASR, as well as four tasks from the SUPERB Yang et al. (2021  ###reference_b77###) benchmark: Intent Classification (IC), Keyword Spotting (KS), Phoneme Recognition (PR) and Speaker Diarization (SD). Because our aim was to evaluate how both the pre-training and mid-training data distributions impact the final learned representations, we freeze the encoder weights during task specific fine-tuning, and fine-tune only the task specific decoder using the LS-960 dataset (for ASR) following Baevski et al. (2020  ###reference_b7###) or the default datasets specified in the SUPERB benchmark Yang et al. (2021  ###reference_b77###)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Model Details",
            "text": "In this section, we discuss the implementation details of the different training setups across the three datasets.\nVideo Data Pre-processing: Videos are first resized to a resolution of  pixels, with a temporal stride of  and  frames sampled temporally. We apply random resized cropping with scale from  to , and random horizontal flipping following standard computer vision techniques for visual data augmentation.\nVideo Encoder: Our video encoding approach is similar to that of Feichtenhofer et al. (2022  ###reference_b26###). Firstly, we divide the video into a regular grid of space-time patches of dimensions  in the  direction, respectively. These patches are then flattened and augmented with spatio-temporal positional embeddings Vaswani et al. (2017  ###reference_b72###).\nFor the Masked Autoencoder, we randomly select 60% of the patches for masking, and mask patches without replacement, while keeping the selection agnostic in the space-time domain. The remaining patches are then passed through 12 ViT encoder blocks Dosovitskiy et al. (2020  ###reference_b25###) with a hidden dimension of 768. We obtain the video encoded features of the remaining spatio-temporal patches, which are later reconstructed using a common decoder.\nFor Contrastive Learning, we reduce the spatial patches to a single embedding for each frame Xu et al. (2021  ###reference_b76###); Radford et al. (2021  ###reference_b58###). The reduced patches are passed through a video encoder with 12 ViT encoder blocks Dosovitskiy et al. (2020  ###reference_b25###) with a hidden dimension of 768. The encoded embeddings are temporally pooled following Xu et al. (2021  ###reference_b76###), resulting in single-vector video features which can be contrasted against corresponding audio embeddings.\nAudio Data Pre-processing: The audio input is re-sampled to a frequency of 16kHz. Subsequently, 80-dimensional Log-Filterbank Energy (LFBE) features are computed from the resulting audio frames. To ensure consistency in feature size, we selected the first 1000 LFBE\nframes for downstream processing. The frames are further sub-sampled using a 1D convolutional layer, reducing the number of audio frames to 250, following the approach of Gulati et al. (2020  ###reference_b31###).\nAudio Encoder: We use positional embeddings in the sub-sampled audio frames similar to video encoding, as proposed by Vaswani et al. (2017  ###reference_b72###). In the Masked Autoencoder, a random mask without replacement is applied to  of the frames, with the visual and audio modalities sharing the same masking ratio to maintain balance in the amount of information across both modalities. The remaining frames are encoded by a Conformer Gulati et al. (2020  ###reference_b31###) with 16 layers, 4 heads, and a depth-wise convolutional kernel of size 31. Audio features are then up-sampled by a linear layer\nand normalized for reconstruction.\nIn Contrastive Learning, the sub-sampled frames are directly featurized by the Conformer blocks without any masking involved. The audio features are then temporally pooled to obtain a single feature for the audio clip, which is up-sampled and normalized. For both the Mid-training and Fine-tuning tasks, the feature output from Conformer blocks is used as input to task-specific decoders. The weights of the convolutional sub-sampling layer and Conformer blocks are the only components re-used from the pre-training stage for further steps.\nCommon Decoder: The Masked Autoencoder pre-training step uses a relatively small vanilla ViT Dosovitskiy et al. (2020  ###reference_b25###) decoder of hidden dimension size of 512 and 4 ViT blocks.\nThe decoder processes a combination of the encoded and masked patches and outputs the original reconstructed signal. A shared decoder is used to sequentially reconstruct each patch.\n###table_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results, Analysis & Limitations",
            "text": "Our main results on the Librispeech dataset are shown in Table 1 and Figure 3, and demonstrate several interesting learnings:\n\nAudio-visual Pre-training is Effective: Table 1 shows that on average in all cases, audio-visual pre-training is effective. Under the null hypothesis that audio-visual pre-training is ineffective, we find significant improvements over the baseline.\n\nMid-training with all translation pairs improve ASR performance: Table 1 shows that the mid-training approach leads to significant improvements over pre-trained models alone with different language pairs. Surprisingly, Italian is the most effective, suggesting that choosing languages which are complementary to English may be more useful than languages which are closer to the target downstream language (English, Dutch, and German all have Germanic roots, while Italian has Latin roots - see Tyshchenko et al. (2000) for a discussion on linguistic distance). We leave it to future work to explore languages that retain very little shared information, such as Russian or Chinese. The relative performance improvements with mid-training are shown in Figure 3. The figure shows several effects which we discuss in the following sections: the model pre-trained on the Kinetics dataset is most improved with mid-training, English-Italian translation is the best mid-training pair, and the model pre-trained with CLR benefits the most with mid-training.\n\nHow do pre-training datasets impact performance (Is dataset size the only factor)? Despite differences in pre-training dataset sizes, it is interesting to understand how the input mix of data impacts the overall performance of the model. Without mid-training, models pre-trained on LRS-3, the smallest dataset, outperform all other models on the test-other dataset. LRS-3 is a small fraction of the size of the VoxCeleb2 dataset, suggesting that the distributional makeup of the multi-modal dataset is key to pre-training performance, and dataset size is not all that matters. VoxCeleb2 outperforms LRS-3 slightly on the test-clean dataset. Kinetics trails both in aggregate, which could be due to both the size of the dataset (only half the size of VoxCeleb2) or the makeup of the dataset (no speech-specific data).\n\nAll three pre-training datasets outperform from scratch training for ASR (even Kinetics), indicating that pre-training on any amount or type of audio-visual data can be helpful. We note that while Kinetics has the worst overall performance, it improves the most with mid-training vs VoxCeleb2 and LRS-3. These results confirm that the model pre-trained on Kinetics has the most to gain from language-representation alignment (as it contains no speech data), and training on LRS-3, which consists of primarily clean data, has less to gain.\n\nThe best ASR results with MAE and CLR are obtained on the LRS-3 pre-training dataset. However, the best MAE+CLR performance was in using the Kinetics dataset. While it can be difficult to disentangle the results from pre-training dataset size, this result may suggest that multi-task learning is more effective on out-of-domain data, where modalities contain non-redundant audio information, compared to VoxCeleb/LRS-3, where modalities consist of primarily redundant information.\n\nMAE outperforms CLR, MAE+CLR on ASR: For ASR results averaged over all pre-training datasets, we find that MAE alone outperforms both CLR and MAE+CLR, suggesting that pre-training with masked auto-encoding objectives remains a promising approach for future exploration. Following intuition from Chan et al. (2022), it is likely that CLR-augmented methods outperform on more global downstream tasks, whereas MAE encodes more local information which is useful for ASR, and MAE+CLR is a useful mix of both. This hypothesis is validated in our experiments on SUPERB Yang et al. (2021), where we found MAE+CLR most effective when aggregated across the mix of global (Intent Classification, Keyword Spotting) and local (Phoneme Recognition) tasks.\n\nMid-Training is most effective with multi-task pre-training: We explore the performance of our methods on four tasks from the SUPERB Yang et al. (2021) benchmark in Figure 2. For SUPERB, mid-training improves performance for MAE+CLR models across most tasks. The notable exception is speaker diarization (SD), where there is minimal task overlap between SD and the mid-training target. Intent Classification (IC) is most improved, primarily due to improvements in models pre-trained on the Kinetics and LRS-3 datasets, which benefit from the additional textual alignment. Keyword spotting (KS) improvements can also be largely attributed to improvements on models pre-trained on Kinetics, for similar reasons. Models pre-trained on VoxCeleb2 improve less with mid-training compared to models pre-trained with both Kinetics and L"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Discussion",
            "text": "Recently, the size of pre-trained models and the datasets have increased to such an extent that it is cost-prohibitive to pre-train these models on datasets aligned with the downstream tasks of interest. Hence, a light-weight mid-training strategy can tune the pre-trained features strengthening the downstream performance.\nAn alternative to the mid-training strategy is to include task during pre-training itself. This alternative strategy has two drawbacks; first, the amount of labeled data available for the mid-training task is typically not large enough to have significant impact when jointly learned in the pre-training stage. Secondly, the mid-training approach is more practical as it can be applied to already available pre-trained models instead of training the models from scratch which requires large amounts of time and compute."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion & Future Directions",
            "text": "This work presents a multi-lingual mid-training objective and a large-scale analysis of multiple audio-visual pre-training methods and datasets, building on observations from Hsu et al. (2021  ###reference_b35###) and Chan et al. (2022  ###reference_b14###). We demonstrate how large scale audio-visual pre-training significantly enhances downstream ASR performance, and highlight that a well-chosen mid-training task can facilitate the final downstream task.\n\nWhile this paper provides initial insights into the impact of mid-training tasks on multi-modal pre-trained models, we recognize that substantial further research is needed to fully understand how sequences of training tasks can align large pre-trained models with downstream tasks.\n\nOne intriguing direction for future research is the exploration of additional mid-training tasks. We illustrate in this work that translation can effectively bridge gaps between multi-modal pre-trained models and language-based ASR tasks. Given that paired data for translation is often scarce, it may not be the most effective choice for future mid-training tasks. It might be beneficial to consider mid-training tasks focused on synthetic data (such as TTS data from text datasets or text generated by large language models) or self-supervised approaches to mid-training.\n\nAnother closely related area for future research is the examination of how pre-training tasks affect the performance of downstream and mid-trained models. We concentrate on multi-modal pre-training, which is a vital emerging direction in ASR research. However, mid-training can also be applied to uni-modal pre-training or even zero-shot transfer from foundational models.\n\nIn conclusion, this study illuminates the impact of mid-training tasks in the context of multi-modal pre-training and shows the substantial enhancement in downstream automatic speech recognition performance through large-scale audio-visual pre-training. By continuing to investigate these areas, we can deepen our understanding of effectively aligning pre-trained models with diverse downstream tasks and explore new possibilities for multi-modal ASR research."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}