{
    "title": "“Hunt Takes Hare”: Theming Games Through Game-Word Vector Translation",
    "abstract": "A game’s theme is an important part of its design – it conveys narrative information, rhetorical messages, helps the player intuit strategies, aids in tutorialisation and more. Thematic elements of games are notoriously difficult for AI systems to understand and manipulate, however, and often rely on large amounts of hand-written interpretations and knowledge. In this paper we present a technique which connects game embeddings, a recent method for modelling game dynamics from log data, and word embeddings, which models semantic information about language. We explain two different approaches for using game embeddings in this way, and show evidence that game embeddings enhance the linguistic translations of game concepts from one theme to another, opening up exciting new possibilities for reasoning about the thematic elements of games in the future.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Almost a decade ago, Cook and Smith wrote that the ‘mechanics-first view on games is unnecessarily limiting’ and called for AI research into game design to consider experiential, aesthetic and rhetorical aspects of games in more depth (Cook and Smith, 2015  ###reference_b5###). Despite new work along these lines, building automated systems that can understand, change or create the theming and contextual meaning for a videogame is an underexplored area of game AI research. Understanding a game’s theme requires the ability to relate game concepts to real-world ideas. This is useful for a wide variety of applications, including the contextualisation of generated content, game design assistance and guidance, and tutorialisation. However, an abstract game carries no indication of how its components should be themed, and a surface-level translation of an existing theme that does not take into account how a game is played will likely lose coherence and impact.\n\nPrevious attempts to overcome this problem have tended to rely on pre-made databases of meaning, and user-defined graphs of concepts that relate to one another (Summerville et al., 2018  ###reference_b15###)(Treanor et al., 2012  ###reference_b16###). We cover some of this work in section §3  ###reference_###, Related Work. While this approach has many benefits, it is very time-consuming, does not scale or generalise, and cannot be extended to procedurally generated concepts for which no prior knowledge exists.\n\nIn (Rabii and Cook, 2021  ###reference_b13###), Rabii et al present a technique which uses word embeddings to train a vector-space representation of game log data. They show that with no prior knowledge about the game other than the logs that it is possible to extract complex, high-level knowledge about the game’s structure, dynamics and strategy. This approach circumvents some of the issues mentioned above, in that it produces knowledge about a game simply through observations of it being played. However, this knowledge is completely abstract – it has no connection to the grounded ‘meaning’ of the game.\n\nIn this paper we repurpose Rabii’s work, and present our technique for using a combination of game log embeddings and word embeddings to translate a game’s theme across a semantic space. We describe our system setup and how we relate game embeddings to the real-world linguistic knowledge base represented by word embeddings. We also describe our approaches to translating thematic elements across the word embedding space to yield new themes for games. We share some preliminary results here, and note the challenges we have encountered so far, and our preliminary steps to improve upon the work. This represents a starting point for a new application of word embeddings to help relate game dynamics to real-world ideas.\n\nThe remainder of this paper is organised as follows: in section §2  ###reference_### we cover how word and game embeddings work; in §3  ###reference_### we discuss related work in computational creativity and automated game design; in §4  ###reference_### we cover the methodology of our approach to thematic translation; in §5  ###reference_### we present and evaluate initial results from the system; and in §6  ###reference_### we conclude our work and look to the future."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Word Embeddings",
            "text": "Word embeddings are representations of sequential data (usually language) in the form of n-dimensional vectors, used in natural language processing for tasks such as topic modelling and semantic distance measurement. The process is trained on a dataset of token sequences, and the positions of tokens in the resulting vector space aims to mimic their distributions in the original data. For natural language, this means that words with a similar semantic role in the training data tend to appear close to each other along certain dimensions in the embedded space. Word embeddings are useful not only for measuring the distance between tokens, but also because mathematical operations can be performed on the vectors, such as addition, subtraction or averaging. This allows for calculations to be performed on words to compose or subtract meanings. Parrish gives the example of colours in (Parrish, 2018  ###reference_b11###): subtracting the vector for the word Blue from the vector for the word Sky and adding the result to the vector for the word Grass yields a vector close to the word Green. The best-known algorithm for creating word embeddings is Word2Vec, which has been widely studied both within computer science as well as beyond in digital arts and other creative fields (Mikolov et al., 2013  ###reference_b10###). Word2Vec’s simplicity and broad scope has made it a useful and accessible tool for artists and creative coders as well."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Game Embeddings",
            "text": "In (Rabii and Cook, 2021  ###reference_b13###) Rabii and Cook present an application of word embeddings to gameplay data. They take a set of formally-annotated chess match logs, and translate it to a domain-specific language that they designed for the purposes of training an embedding. They then apply the word2vec algorithm to this chess gameplay data, yielding a vector-space embedding for the game logs. In their paper they show that the resulting embedding reveals a range of interesting information about chess, from fundamental structural concepts through to subtle strategic insights understood by proficient players.\nIn their original proposal, the authors note several important traits of their system: the design of the domain-specific language, the size of the data, the source of data points (for example, expert players versus novices) and the exact phrasing of queries all have a significant impact on the types of conclusions that can be drawn from the data. Despite this, the work clearly shows that the embeddings capture important truths about the game simply by seeking semantic connections between logs of game events. This technique is potentially very scalable and widely-applicable as a result, since a game developer can easily gather event logs for their own game, and create a vector embedding from it.\nBoth game and word embeddings represent relationships between concepts through a multi-dimensional vector space, learned from datasets. As stated above, we can operate within these vector spaces to transform and translate concepts, through processes like addition or subtraction. However, vector spaces can also be related to one another, particularly when we can provide points which overlap or connect semantically. We can draw connections between these spaces through the use of regression techniques (RODRIGUEZ et al., 2023  ###reference_b14###), which allow us to use known connections between spaces to infer new ones. In this paper, we present our efforts to draw a connection between a game embedding space (trained on chess data, as per Rabii and Cook) and a word embedding space (trained on English language text) to show how linguistic and ludic concepts can be linked together, and then transformed to find new themes and meaning in word embedding space."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Related Work",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Metaphor and Analogy",
            "text": "In this paper we seek to re-theme a game. We have a set of game elements, such as the pieces on a chessboard and the states and rules of the game, and we wish to rename these elements to align with a new theme. This has some similarity to the notion of metaphor or analogy in natural language processing research. A metaphor has two elements: a tenor, the root concept that is being conveyed, and a vehicle, a second concept that is used to represent the tenor in a new, metaphorical context. Metaphors usually connect concepts together through a specific connection point and then encourage the audience to extrapolate other, looser connections from there.\nMetaphor and analogy are a widely studied topic in broader AI and natural language processing research, especially in computational creativity where researchers such as Veale have extensively studied how computers can extract metaphors from language (Veale et al., 2018  ###reference_b17###). Veale’s approach identifies common structures in natural language and mining those associations to create a large knowledge base. This is supported by existing linguistic knowledge graphs such as WordNet (Fellbaum, 1998  ###reference_b6###). In (Gero and Chilton, 2019  ###reference_b7###), Gero et al use the open-source knowledge graph GloVe to evaluate potential metaphors and grades different qualities of the metaphor based on their semantic similarity in the knowledge graph. Our approach also uses GloVe as a basis for its linguistic knowledge."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Theming Games",
            "text": "Theming or otherwise providing semantic meaning to game elements is primarily a topic covered by automated game design research, which Cook defines as ‘the science and engineering of AI systems that model, participate in or support the game design process’ (Cook, 2022  ###reference_b4###). Such systems engage with the topic of game theming for a variety of reasons, including wanting to impose a theme on an existing game, or interpreting a game to understand possible themes it may be expressing. Treanor’s work on micro-rhetorics for the Game-o-Matic system is one such example – micro-rhetorics are user-defined components of meaning which can be combined with one another to compose higher-level rhetorical messages (Treanor et al., 2012  ###reference_b16###). In his work on the Game-o-Matic, Treanor combines micro-rhetorics with thematic graphs that users provide (for example, ‘cop arrests protester’) to synthesise games which convey a theme.\nGEMINI furthers work in this area, by providing bidirectional interpretation (Summerville et al., 2018  ###reference_b15###). This allows the GEMINI system to not only create games based on thematic structures, but also to intuit possible thematic structures by examining an unthemed game, which is closer to the task we approach in this paper. This is done using a pre-compiled database of structures and their rhetorical interpretations, which formulates the task of theming a game to a logic programming task using answer set solvers. Notably, GEMINI operates on rules, not behaviour – ‘mechanics’, rather than ‘dynamics’, in the parlance of Hunicke et al (Hunicke et al., 2004  ###reference_b8###) – and its rhetorical databases are also defined in this way, describing the rhetorical meaning of game elements directly. Our approach considers the dynamic behaviour of game elements instead, by focusing on Rabii’s game embeddings technique, and leverages large existing knowledge databases (in our case, word embedding models) that are not specific to any particular game."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Methodology",
            "text": "Our goal is to build a system that can associate words in English to Chess game concepts, by relating English word embeddings to game embeddings built from Chess play data. If  is a game token, we note  its associated game vector. If  is a word in our dataset’s English vocabulary, we note  its associated word vector. If  are words in our dataset’s English vocabulary, we note  the average word vector . The relationship vector between two words  and  is denoted . This is a vector which can be ‘added’ to the vector  to yield the vector representing ."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Training Embeddings",
            "text": "Our source for word vectors are the public domain GloVe pre-trained embeddings, trained on the Wikipedia 2014 and the Gigaword 5 corpus (Pennington et al., 2014). Its vocabulary contains 400,000 words, each represented by a 50-dimensional vector.\n\nTo create our game embeddings, we reproduced Rabii and Cook’s data processing pipeline. We first downloaded a month of ranked matches data from the online Chess playing platform Lichess, and converted each logged game in the custom description language described by the authors. Samples of the converted data with their natural language equivalent are provided in Table 1.\n\nThe description language contains a total of 34 different tokens that represent different concepts used to describe a game of Chess. Tokens can represent players (e.g. White, Black), Chess pieces (Queen, Pawn), playing moves (Capture, Castling), game states (Checkmate, WinBlack) and even the eight rows and columns of the board (e.g. R0, R1, R7, C0, C1, C7).\n\nWe then used our corpus of Chess data to train a Word2Vec model using the same settings as Rabii and Cook. In the end, each of our 34 Chess tokens has a 5-dimensional vector representation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Themes and Rethemings",
            "text": "We define a theming as a function that associates game tokens built from our Chess description language to word vectors in our dataset. We consider Chess to have an already established theming, which we denote as the function . Our definition of Chess’ current theme can be found in Table 2. It does not contain every token in the original description language, only those that we could associate to a specific word vector. For example, since we associate the game token King to the word ”king”, we have:\n\nHowever,  cannot take as input the tokens that represent the rows and columns of the board (R0,...,C7), as we couldn’t map them to a specific word in English. Some tokens can be associated to a word, but that word might have other meanings than the one we want to use (e.g. Draw does not refer to the act of drawing a picture, but a situation where no player wins). In that case, we associate the token to at most three words, and use the average of their word vectors. The set of tokens that are valid inputs for  is called .\n\nOur goal is to retheme Chess, i.e., create other themings with alternative token-to-words associations. We define  as a function that, given a token and a relationship vector, returns a word vector:\n\nThe relationship vector guides the semantic translation of the input token into the final output word. For example, if we built a function to shift the masculine words in Chess’ theming to their feminine equivalents, we might have the following equations:\n\nIn the following sections, we describe three different models for building the  function, one using only word vectors, and two using a combination of word and game vectors. In the following results section, we show and discuss results from all three approaches."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Word Vectors Only",
            "text": "Our first retheming model is built using the well-known Word2vec’s analogy formula, adapted to the general case:\nFor the above example of translating across the theme vector of masculine to feminine, we would therefore calculate the retheming as:\nThis function only uses word vectors. We chose it as a baseline to compare with techniques that leverage the information stored in game vectors."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Using Both Game and Word Vectors",
            "text": "Game embeddings contain expert knowledge about the specific game they were extracted from, while word embeddings represent the meaning of words in the broad context of their dataset. Some expert players assign a numeric valuation to each chess piece as a way to quantify how important they are for winning a game. This suggests that game embedding data carries important information about the structure of Chess as a game which the words alone do not. Therefore, we hypothesize that a retheming using both word and game vectors can provide us with more relevant results than ones purely based on word vectors.\n\nWe want our model to be able to translate relationships between game tokens (such as the relative value ordering of chess pieces) into relationships between words (e.g. the medieval hierarchy of Chess piece names). We are interested in models of the form:\nwhere  is a linear function that associates a game vector to a word vector. The choice of linear functions is motivated by the fact that semantic relationships between words and game concepts are captured in relationship vectors. A linear transformation  preserves relationship vectors up to a scaling factor.\n\nRecall that  is the set of tokens which are valid inputs for the theming function. For each game token , we train a linear regression model  on a subset  of  game tokens randomly chosen in  and their corresponding value given by . The purpose of  is to convert the gameplay relationships embedded in the game vector  to semantic relationships, encoded in a word vector. Unlike the method presented in Section 4.3, we don’t want the prediction to be influenced by the currently given value of  so we make sure that the token  is absent from .\n\nOnce the conversion from the game to word embeddings space is done, we add the relationship vector that guides the retheming:"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5. Choosing the guiding vector",
            "text": "The function requires a relationship word vector of the form , that we call the guiding vector. It is best to think of it as representing a semantic transformation from the starting word  to the target word . In our experiments, this transformation is applied to the narrative context of Chess, to shift it from its current theme to a different target theme. We experimented with two types of guiding vector."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1. Guiding based on a specific example",
            "text": "If we have a token in our Chess description language that we want to retheme to a specific word (e.g., map the token King to the word lion instead of the word king), we can provide the vector representing that exact transformation. All the other rethemed word vectors will be translated by that same vector. We can think of this translation as being anchored around the given example, with the other translations being secondary and hopefully following the same overall trajectory."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2. Guiding based on a target semantic field",
            "text": "Instead of providing a specific retheming for a token (), we can choose a broader semantic field, represented by a list of words . Let  be the words used in the current theming of Chess in Table 2  ###reference_###. We can create a vector representing the shift from one lexical field to another by first computing the average vector of each list – respectively  and . The guiding vector  represents the transformation of shifting from the starting theme to the one specified by the list of words given in input."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Results",
            "text": "The models described in Section §4 were built with the goal of retheming the tokens in our Chess description language. We present our results under different modalities in Tables 4 and 5. Each model is tasked with associating all the game tokens in Table 2 to a word embedding. We explore two types of retheming, each with the goal of shifting the original medieval warfare theming of Chess to a theme about wildlife. Table 4 contains the models output when prompted with the specific transformation (see 4.5.1). Table 5 contains results obtained when prompting the models with the target semantic field represented by the following words: lion, elephant, zebra, eating, becoming, extinction, control, win, loss. Our explorations showed that ideally, this list should not only contain words that refer to animals, as words about the basic concepts of a strategy game (e.g., win and loss) are important to define the semantic field of a Chess game.\n\nFor each modality, we compare the results of three models: a baseline retheming using only word vectors, followed by two rethemings using both word and game vectors combined, with the sampling parameter set to 5 and 10, respectively. The maximum value for is the size of the original Chess theming in Table 2 minus one, i.e., 16. Our experiments using values of close to 16 yielded poor results, with little variation between each output. Choosing lower values of provides more diversity in the output word vectors but increases the probability for each linear regression model to over-fit on the training set, which is evidenced by their average of 1 in the lower case where. Over-fitting reduces the model’s ability to generalize, leading to more words that are either too close the original theme (such as or ) or seem too far from the target theme (e.g., , , ). We observed empirically that good trade-off seem to lie in the middle, with . This trade-off may be different for other games and for differently-sized or -distributed token sets, which future work will investigate.\n\nComparing the baseline rethemings (Column 1) and the ones with (Column 2), we observe that models that don’t use game vectors often provide words that are close, if not exactly the same as the starting theme (for King, for Pawn, for Bishop) while the ones that do use them reuse few words from the original Chess theme."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Comparing Retheming of Chess Pieces",
            "text": "We compare our model and the baseline model on the task of generating a set of chess pieces themed around wildlife. We use the results generated in Table 5 ###reference_###, columns 1 and 2. For each token related to a chess piece, we follow this algorithm:\n\nCompute the three closest words to the model’s output.\nIf the guiding vector’s target word is in the list, discard it.\nAmong the remaining words, pick the first noun of the list.\n\nThe result of that process is presented in Table 6 ###reference_###. We note that the baseline model outputs words that are quite similar from the starting theme (Column 1), while the model using game vectors seems to correctly feature a ”wildlife” theme (Column 2).\n\nIn Column 2, we observe that the medieval social hierarchy of Chess has been replaced by a food chain: weaker pieces are associated with prey animals while stronger pieces are associated with predators. Interpreting the mapping, we remark that the King is linked to a , an animal that is both weak and cherished, which bears some similarity to the King’s role in chess.\n\nThe Rook is associated with hunt, while the Bishop is associated with foxes. The Queen’s moveset allows it to move either like a Bishop or a Rook, and its associated word hunters could be considered a composition of both the notion of a group hunt, represented by the Rook, and foxes which are solitary hunting animals, represented by the Bishop. Semantic blends such as this are necessarily fuzzy, but we find these examples inspirational and engaging.\n\nWe consider that our model’s ability to provide a retheming whose semantic relationships seem to match, at least in an interpretive manner, with gameplay properties is a strong sign of its potential. The baseline model doesn’t exhibit the same properties, suggesting that exploiting the information captured in game embeddings is valuable to explore the link between gameplay and narrative theming."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Discussion and Conclusions",
            "text": "In this paper we presented preliminary results on the use of both word and game embeddings to model and translate thematic elements of games. We motivated our work by building on prior research into game embeddings for chess, and word embeddings for natural language, and showed how the two can be combined together in different ways to alter the theme of a game such as chess. We provided evidence that our technique works better than simply using word embeddings alone to retheme words related to a game, and we propose that this is because the game embeddings capture essential qualities of various game design components and this knowledge supports a richer translation. \n\nWe believe this approach has many promising and interesting features. For one, it captures the dynamics of a game, rather than its static elements: game embeddings reveal, for example, the relative strength of chess pieces, even though this knowledge is not described anywhere in the game’s rules. Most automated game design research focuses on static mechanical elements of games, and struggles to bridge to dynamic or emergent properties. This could be an exciting new way to approach the understanding, extraction and use of dynamic game elements. \n\nEmbeddings are also scalable and comparatively easy to both create and understand. Adding logging to a game to record important events and create datasets, such as the one we used from chess matches, is a simple exercise no different to the debugging or playtesting that game developers already do. We have already worked with one independent game developer to add logging of this type to their game, and the process was clean and straightforward. Word2Vec arithmetic is a widely-used and taught technique among digital art communities and creative coders. This can be furthered through better tools build specifically to make training and exploring embedding spaces more amenable. This makes it a promising technique for widespread adoption in the games industry. \n\nWe are planning to experiment further with this technique on more traditional digital games, to assess how it performs on different kinds of game scenario and different kinds of game logs. At the time of writing, we are working with an independent game developer to gather data from one of their games, from both expert and novice players. We also hope to experiment with more ambitious uses for the technique as well, such as the ability to create names for game elements that have been procedurally generated at runtime. We believe there are many exciting applications for this basic technique waiting to be discovered."
        }
    ]
}