{
    "title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
    "abstract": "Deep multi-modal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three soft prompt experts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation and a unified prompt to assist multi-modal interaction.\n\nAdditionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot settings, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M) but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.\n\nKeywords: multi-modal sarcasm detection, multi-modal sentiment analysis, prompt learning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Multi-modal semantic understanding (MSU) is crucial for the development of machines capable of interpreting the complex interplay of textual and visual information. In social media platforms, where the combination of text and imagery can often present conflicting messages or nuanced sentiments that are not immediately apparent from a single modality alone, such understanding is vital for accurately interpreting the intent and sentiment. Among the fields of MSU, Multi-modal Sarcasm Detection (MSD) and Multi-modal Sentiment Analysis (MSA) emerge as two representing tasks. These tasks exemplify the intricate process of aligning and comprehending the relations from different modalities to discern the intended meaning or sentiment. As highlighted by the examples in Table 1, solely reading the text or image of MSD is prone to misinterpreting it as a positive comment while ignoring the sarcasm about the discount being too small.\n\nRecent approaches for MSU normally exploit a dual-encoder architecture, i.e., use separate pre-trained encoders to extract features for different modalities (e.g., BERT (Devlin et al., 2019) for text and ResNet (He et al., 2016) for image). The features then interact with each other to capture the incongruity and fed into a classification head for the prediction. Under this framework, researchers are dedicated to designing effective methods of interaction, including attention mechanisms Pan et al. (2020); Xu et al. (2020); Han et al. (2021a), graph structures Liang et al. (2021, 2022); Liu et al. (2022a), optimal transport Pramanick et al. (2022) and dynamic routing Tian et al. (2023).\n\nAlthough the previous studies have achieved good performance in semantic understanding tasks like MSD or MSA, they mostly rely on sufficient training data. However, collecting a large amount of high-quality multi-modal data, sarcasm especially, is a non-trivial task. According to Misra and Arora (2023), sarcasm expressions require a high level of cognitive ability and rarely appear on social media, making it difficult to collect and annotate. Moreover, most existing models use separate pre-trained encoders to process text and image, which might lead to the misalignment of different modalities and thus hurt modal fusion.\n\nNowadays, pre-trained on large-scale image-text pairs, the vision-language models (VLMs) achieve good image-text correspondences and can perform well on cross-modal reasoning. Given these two aspects, we propose to address the few-shot MSU tasks by using VLMs. To adapt the pre-trained VLMs to downstream multi-modal tasks on few-shot settings, prompt-based learning is widely applied and has demonstrated promising performance Liu et al. (2023a). Compared to manually designed prompts, continuous prompts (also referred to as soft prompts) are preferred due to their flexibility and scalability Liu et al. (2021a, b); Han et al. (2021b). However, most previous work only coped with text data.\n\nIn this paper, we explore different methods of utilizing soft prompts to address the few-shot MSU task. Two primary architectures are prevalent in VLMs. One line of work encodes images and texts respectively and performs modal fusion by simply computing the similarities between them, like CLIP Radford et al. (2021) and ALIGN Jia et al. (2021). Another line of work adopts a unified network for both single-modal representation and multi-modal fusion, like VilBert Lu et al. (2019) and VLMo Bao et al. (2022). In the first line of work, the soft prompt method has been studied, like CoOp Zhou et al. (2022b), CoCoOp Zhou et al. (2022a), UPT Zang et al. (2022) and MaPLe Khattak et al. (2023).\n\nNonetheless, there has been little work applying soft prompts in the second line of VLMs yet. Thus, exploring multi-modal prompts in a unified Transformer network remains an open issue. In this paper, towards the deep MSU, we propose a novel multi-modal soft prompt framework MoPE-BAF, Mixture-of-Prompt-Experts with Block-Aware prompt Fusion. Specifically, we devise a set of soft prompts corresponding with different roles, an image prompt expert, a text prompt expert and a unified prompt expert. The first two extract semantic features within a single modality, while the third assists in capturing inter-modality information. Furthermore, we introduce a block-aware prompt fusion mechanism to enhance the connection between different prompt experts. We re-organize the transformer layers into several blocks and apply cross-attention to enable the exchange of prompt expert information between two adjacent blocks. It facilitates deep interactions between modalities and enables smoother transitions from single-modal representation to multi-modal fusion.\n\nWe conduct a series of experiments on the MSDT dataset (Cai et al.,"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Multi-modal Sarcasm Detection",
            "text": "Researchers have been exploring different methods to model the incongruity within the image-text pair for the MSD task. At the outset, feature-based approaches are adopted Schifanella et al. (2016 ###reference_b40###); Castro et al. (2019 ###reference_b4###). Subsequently, researchers pay more attention to modality interaction. Cai et al. (2019 ###reference_b3###) leverages a hierarchical strategy to fuse the three modalities of image, attribute and text by attention weights. Sangwan et al. (2020 ###reference_b38###) exploits the interaction among the input modalities using the recurrent neural network. Pan et al. (2020 ###reference_b35###) tries to capture the incongruity by inter-modal attention and co-attention within the text. Xu et al. (2020 ###reference_b48###); Han et al. (2021a ###reference_b13###) decompose the model into a separation network for discrepancy increment and a relation network for relevance increment. Liang et al. (2021 ###reference_b22###, 2022 ###reference_b23###); Liu et al. (2022a ###reference_b24###) introduce graph structures for depicting incongruity relations. More recently, Tian et al. (2023 ###reference_b42###) utilizes dynamic paths to activate different routing Transformers. However, these works rely on a large amount of training data to finetune the pre-trained models."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Multi-modal Sentiment Analysis",
            "text": "In recent years, MSA has become a popular research topic. Xu et al. (2018 ###reference_b47###) designs a co-memory network based on attention to predict the whole sentiment of text-image pairs. Yang et al. (2021a ###reference_b49###) proposes a multi-view attention network, which utilizes an attention memory network to extract text and image features, and then fuses multi-modal features through a stacking-pooling module. After that, they incorporated graph neural network in MSA (2021b ###reference_b50###). Yu et al. (2022 ###reference_b51###) proposes a unified pre-training stage to narrow the semantic gap between different image and text pre-trained models."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Multi-modal Prompt Learning",
            "text": "Recently, researchers adapt VLMs to fit downstream tasks via prompt learning Gu et al. (2023  ###reference_b12###), which can be generally categorized into single-modal prompt and multi-modal prompt methods. In single-modal prompt methods, learnable continuous prompts are appended in front of the text data. For example, CoOp Zhou et al. (2022b  ###reference_b56###) and CoCoOp Zhou et al. (2022a  ###reference_b55###) replace the manual-crafted text prompts used in CLIP with continuous vectors for image classification. Multi-modal prompt learning aims to optimize the text and image inputs simultaneously. UPT Zang et al. (2022  ###reference_b52###) designs a shared initial prompt for CLIP text and visual encoders. MaPLe Khattak et al. (2023  ###reference_b18###) leverages prefix tuning in both modality encoders and designs a coupling function to enable a mutual promotion between prompts. CMPA Liu et al. (2023b  ###reference_b29###) designs prompts for both encoders and employ a cross-modal prompt attention at each layer. These prompt-based methods are mainly applied to the CLIP architecture, which encodes the text and image inputs separately. To the best of our knowledge, there has been little work that applies soft prompts to a unified vision-language pre-trained model."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Preliminary",
            "text": "Since our work adopts the vision-language pre-trained model VLMo as the backbone, to better illustrate our proposed method,\nwe provide an overview of VLMo as well as the basic knowledge of applying it to MSU tasks.\nThe most straightforward approach is to register a classification head on top of VLMo. After encoding, the vector of the text-start token ([CLS]) is used as the final representation of the image-text pair, and the prediction result is obtained:\nwhere  denote the parameters of the pre-trained model and the classification head respectively.\nAnother way to utilize VLMo is to reformulate the task into a mask language modeling task using a hard template containing [MASK]:\nwhere  denotes the prompt template and  denotes the verbalizer that maps the [MASK] token to the probabilities on label words.\nThe soft prompt method utilizes a set of trainable virtual tokens, which eliminates the need to manually design templates:\nwhere  is\nthe virtual token."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   The Vision-language Pre-trained Model: VLMo",
            "text": "VLMo presents\na Transformer architecture with Mixture-of-Modality-Experts\nwhere the feed-forward neural (FFN) network switches based on the input modality and fusion requirements. As shown in Figure 1  ###reference_###, given an image-text pair, VLMo performs\nthe unified encoding\nin two stages: (1) Employing vision FFN (V-FFN) and language FFN (L-FFN) to encode the respective modality representations at the bottom Transformer layers, and (2) Using vision-language FFN (VL-FFN) to perform multi-modal interaction at the top layers.\nConcretely, in Stage 1, denoting the hidden vision and language representations of the previous layer as  respectively, VLMo first employs shared self-attention across modalities to align their contents, and then pass them to a uni-modal FFN to obtain the output of this layer:\nIn Stage 2, after the self-attention operation, the intermediate outputs are combined and forwarded to a vision-language FFN:"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Tuning on Multi-modal Tasks",
            "text": "In MSD or MSA task, the input consists of an image  and associated text , with the goal of predicting its corresponding category.\nThe most straightforward approach is to register a classification head on top of VLMo. After encoding, the vector of the text-start token ([CLS]) is used as the final representation of the image-text pair, and the prediction result is obtained:\nwhere  denote the parameters of the pre-trained model and the classification head respectively.\nAnother way to utilize VLMo is to reformulate the task into a mask language modeling task using a hard template containing [MASK]:\nwhere  denotes the prompt template and  denotes the verbalizer that maps the [MASK] token to the probabilities on label words.\nThe soft prompt method utilizes a set of trainable virtual tokens, which eliminates the need to manually design templates:\nwhere  is\nthe virtual token."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   The Proposed Model",
            "text": "###figure_4### ###figure_5### We prepend a single-modal prompt to each modality input and pass them together through\nVLMo Transformer layers.\nwhere  denote vision representation, language representation, V-Prompt and L-prompt, respectively.\nTo ensure the specialization of prompt experts within their corresponding modalities, we restrict their receptive field in the self-attention module, as illustrated in Figure 3  ###reference_###. V-Prompt is dedicated to the image input only, while the image can attend to both V-Prompt and the text input, enjoying the cross-modal alignment and uni-modal enhancement simultaneously. L-Prompt performs in the same way.\nA multi-modal unified prompt expert is\nintroduced to enhance the interaction between modalities:\nwhere  is the representation of VL-Prompt."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Mixture-of-Prompt-Experts",
            "text": "One of the primary challenges in applying soft prompts to multi-modal tasks\nis the specialization of prompt properties across different modalities.\nIn the traditional soft prompt method, all prompts and input tokens are treated equally within Transformer layers, which aligns with the VLMo’s second stage. However, VLMo differentiates image and text inputs with distinct FFNs in the first stage. Thus, only the second stage can be activated by prompts, which inhibits the full exploitation of the multi-modal encoder in extracting modality-specific features.\nIn view of this, we propose a novel multi-modal soft prompt approach, Mixture-of-Prompt-Experts (MoPE), to serve the two encoding stages in VLMo.\nIt contains a set of soft prompts corresponding to the three functional FFNs of VLMo: image expert (V-Prompt), text expert (L-Prompt) and unified cross-modal expert (VL-Prompt), where V-Prompt and L-Prompt assist in extracting the semantic features from the respective modality in Stage 1, and VL-Prompt assists in enhancing inter-modal interaction. Each prompt expert is initialized as a set of trainable vectors with dimension matching the embeddings of the pretrained model. The overall structure of our proposed model is illustrated in Figure 2  ###reference_###.\nWe prepend a single-modal prompt to each modality input and pass them together through\nVLMo Transformer layers.\nwhere  denote vision representation, language representation, V-Prompt and L-prompt, respectively.\nTo ensure the specialization of prompt experts within their corresponding modalities, we restrict their receptive field in the self-attention module, as illustrated in Figure 3  ###reference_###  ###reference_###. V-Prompt is dedicated to the image input only, while the image can attend to both V-Prompt and the text input, enjoying the cross-modal alignment and uni-modal enhancement simultaneously. L-Prompt performs in the same way.\nA multi-modal unified prompt expert is\nintroduced to enhance the interaction between modalities:\nwhere  is the representation of VL-Prompt."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Block-Aware Prompt Fusion",
            "text": "Recall that deep MSU\nnecessitates the ability to discern the complex relationships across image and text modalities. These tasks demand a profound understanding of content relations beyond simple fusion.\nDespite VLMs being pre-trained on large image-text corpora and demonstrating strong performance on traditional multi-modal tasks (e.g., image classification),\ntheir performance on the complex multi-modal task remains subpar due to the superficial fusion process.\nIn VLMo, the allocation of fusion layers is limited (e.g., VLMo-Base-plus assigns only 3 layers),\nconstraining the model’s fusion capacity.\nAccordingly, we propose a new block-aware prompt fusion (BAF) mechanism to make different modality prompts interact deeply and meet the fusion requirements in deep MSU.\nTo be specific, we re-organize the transformer layers\ninto several blocks, and\nintroduce a cross-attention fusion layer between two adjacent blocks. Assuming that each block contains  layers, for the first layer of block  (Layer ), the input prompt\nis reconstructed from the output of the last layer of block  (Layer ):\nwhere  and  are the input for Layer .\nIntuitively, an efficient prompt fusion should introduce knowledge from another modality while retaining the original specialization. We manage it through controlling the number of blocks in BAF. With an appropriate block number, the representations of different modality get fused gradually as the blocks progress, facilitating a seamless transition between two stages\nand striking a balance between single-modal specialization and multi-modal fusion."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experimental Setup",
            "text": "For image-text MSD, the MSDT dataset (Cai et al., 2019) stands as the sole benchmark dataset currently available. MSDT has 29k/2.4k/2.4k sample pairs for train/validation/test, each of which contains an image-text pair with a binary label {sarcasm, nonsarcasm}. We keep the test set unchanged and randomly select 32 samples from the train/validation set to construct our few-shot dataset. To balance the label distribution, we control the samples for each label to account for half of the total number. To improve measuring robustness, we sample three disjoint datasets and report the mean result on them. The statistics of the MSDT dataset is shown in Table 2.\n\nOur experiments for MSA are based on the MVSA-S (Niu et al., 2016) dataset. Each sample in MVSA-S contains an image-text pair annotated with one of the labels: {positive, neutral, negative}. For few-shot MSA, Yu et al. (2022) performed random sampling on both training and development sets from MVSA-S, constituting 1% of the total. We keep the same setting with them for fair comparison and use Accuracy as a metric."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Dataset and Evaluation Metrics",
            "text": "We conduct experiments on two representative MSU tasks: multi-modal sarcasm detection and multi-modal sentiment analysis. For image-text MSD, the MSDT dataset (Cai et al., 2019) stands as the sole benchmark dataset currently available. MSDT has 29k/2.4k/2.4k sample pairs for train/validation/test, each of which contains an image-text pair with a binary label {sarcasm, nonsarcasm}. We keep the test set unchanged and randomly select 32 samples from the train/validation set to construct our few-shot dataset. To balance the label distribution, we control the samples for each label to account for half of the total number. Following the previous work, we adopt Accuracy as our evaluation metric. To improve measuring robustness, we sample three disjoint datasets and report the mean result on them. The statistics of MSDT dataset is shown in Table 2.\n\nOur experiments for MSA are based on the MVSA-S (Niu et al., 2016) dataset. Each sample in MVSA-S contains an image-text pair annotated with one of the labels: {positive, neutral, negative}. For few-shot MSA, Yu et al. (2022) performed random sampling on both training and development sets from MVSA-S, constituting 1% of the total. We keep the same setting with them for fair comparison and use Accuracy as a metric."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Implementation Details",
            "text": "We choose VLMo-Base-plus as our baseline model. For data pre-processing, we follow the same steps as VLMo. The images are resized to resolution and segmented into patches with RandAugment (Cubuk et al., 2020 ###reference_b6###). We utilize the tokenizer from the uncased version of BERT, limiting the input text to a length of 40 and truncating any exceeding portions. For hyper-parameters, the block number is set to 2, and the prompt length is set to 10 by default. All experiments adhere to the full parameter training paradigm. During training, the model is optimized by AdamW (Loshchilov and Hutter, 2019 ###reference_b31###) with. The peak learning rate is. Weight decay is 0.01. In the 32-shot setting, the batch size is 8, and we use linear warmup over the first 10% of the whole 200 steps. We use 1 Nvidia GeForce 3080Ti card for experiments. One training needs around 8GB GPU memory and takes about 2 hours.\n\n1. Text:<text> Answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:\n2. Text:<text> Based on the image and text, answer the question: Which sentiment does this image-text pair contain, negative, neutral or positive? Answer:\n3. Text:<text> Combining the text, which sentiment does this contain, negative, neutral or positive? Answer:"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Comparing Methods",
            "text": "We conduct a comprehensive evaluation by comparing with the vanilla multi-modal pre-trained methods, the methods specialized for MSD or MSA tasks, and the prompt methods applied to VLMs."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Pre-trained methods",
            "text": "CLIP (Radford et al., 2021) holds a dual-encoder architecture, designed to encode texts and images separately. We further add trainable projection layers at the output of both encoders and perform classification on their concatenated encoding results. InstructBLIP (Dai et al., 2023) is the latest and most advanced general-purpose VLM that introduces instruction tuning techniques to extract informative features tailored to the given instruction. We adopt the Vicuna7B as the base and design three textual prompts for MSD and MSA as the instruction (shown in Table 3) and report their average performance. We use 1 Nvidia A40 card for training InstructBLIP and it takes around 24GB GPU memory."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Prompt Learning Methods",
            "text": "We also consider the multi-modal prompt methods based on CLIP as our baselines. CoOp (Zhou et al., 2022b  ###reference_b56###) replaces the manual-crafted text prompts in CLIP with continuous vectors. MaPLe (Khattak et al., 2023  ###reference_b18###) leverages prefix tuning in both modality encoders and designs a coupling function to enable prompt interactions. CMPA (Liu et al., 2023b  ###reference_b29###) designs prompts for both encoders and employs a cross-modal prompt attention at each layer."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Sarcasm Detection methods",
            "text": "For the MSD task, previous work has only considered a full-shot experimental setup. Based on our few-shot dataset, we replicate some of the studies for which the source code is available. HFM (Cai et al., 2019) leverages a hierarchical strategy to fuse the image, attribute, and text modalities by attention weights. ResBERT (Pan et al., 2020) captures the cross-modal incongruity by inter-modal attention and contradiction within the text by co-attention. HKE (Liu et al., 2022a) learns composition-level congruity based on graph neural networks and introduces auto-generated captions as external knowledge."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Multi-modal Sentiment Analysis methods",
            "text": "MVAN Yang et al. (2021a) utilizes a multi-view memory network to extract single-modal emotion features and interactively capture the cross-view dependencies between the image and text. MGNNS Yang et al. (2021b) learns multi-modal representations by a multi-channel graph neural network based on the global characteristics of MVSA. UP-MPF Yu et al. (2022) adopts the multi-modal prompt-based finetuning paradigm and proposes a pre-training stage to narrow the semantic gap between image and text encoders."
        },
        {
            "section_id": "5.3.x",
            "parent_section_id": "5.3",
            "section_name": "Different Prompting Learning Strategies",
            "text": "Manual Prompt defines a hard template combined with input. The representation of [MASK] is then fed into the verbalizer for prediction. Soft Prompt prepends several virtual tokens before the input and utilizes [CLS] token for classification. P-Tuning Liu et al. (2021b) combines soft prompt with manual prompt. Table 3 lists the templates used in these methods. P-Tuning v2 Liu et al. (2021a) adopts the prefix-tuning idea and expands the prompt parameter space to each transformer layer, using [CLS] token for classification."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Results and Analysis",
            "text": "Generally, a longer prompt correlates with an increase in learnable parameters, while in the few-shot setting, it may exacerbate the over-fitting problem. We vary the prompt length from 5 to 50 in MoPE, and the results concerning different lengths are visualized in Figure 4 (a). Overall, the impact of prompt length on model performance shows a trend of initially increasing, subsequently diminishing, and then stabilizing. Our explanation is that prompts too short are insufficient to bring about qualitative changes to the model, while too long may pose challenges for searching for the optimal solution due to the complexity of the expanded search space.\n\nWe investigate the impact of the block number in the BAF module. Specifically, in the first 21 layers of VLMo that use V-Prompt and L-Prompt experts, we divide them into 2-7 blocks. A configuration with 1 block implies no prompt fusion, and is therefore not displayed here. If the number of model layers cannot be evenly divided by the block number, the excess layers are allocated to the bottom blocks. This ensures that the maximum layer difference between any two blocks does not exceed 1. The results are shown in Figure 4 (b). When the block number is between 3 and 5, the overall performance remains almost similar. However, a noticeable decline can be observed after 6 blocks. We speculate that too many cross-modal cross-attention operations between blocks cause their original specialization to be discarded, thereby contradicting the function of MoPE. An appropriate number of blocks can strike a balance between the specialization and different modalities interaction.\n\nThe number of training shots plays a crucial role in model performance, particularly in the few-shot setting. We conduct experiments to train the VLMo base and our full model with different numbers of samples. The results are presented in Figure 4 (c). We find that the performance of both models improves as training shots increase. This demonstrates the robustness and efficiency of our method."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Evaluation on Multi-modal Sarcasm Detection",
            "text": "We conduct extensive experiments on the dataset MSDT and report the results in Table 4. We implement two settings for our method. The first employs a classification head using [CLS] for prediction, termed MoPE-BAF. The second uses an additional manual template containing [MASK], termed MoPE-BAF + Manual Prompt (our full model).\n\nCompared to previous methods that target full-shot MSD, our model MoPE-BAF achieves significant improvements over HFM and resBERT, and surpasses HKE by a margin of 4.56 points on Accuracy, which leverages external caption knowledge.\n\nAmong the current advanced multi-modal models, our method achieves promising results. Compared with the backbone model VLMo, we realize a 5.14-point improvement on Accuracy. Compared with CLIP, our methods attain consistent improvements across all four metrics. In addition, our model with only 150m parameters surpasses the large model InstructBLIP with 8.2B parameters by a 4.76 points increment on Accuracy. This efficiency highlights the superiority of our methods and suggests our potential for practical resource-saving applications.\n\nIn comparison to the multi-modal prompt learning methods CoOp, MaPLE, and CMPA implemented on CLIP, MoPE-BAF showcases outstanding performance despite being built on a less powerful backbone model VLMo. Besides, under the scope of prompt methods on VLMo, our full model achieves the best results, whether combined with a classification head or an LM head with the verbalizer. We speculate that our method distinguishes prompts for different modalities and facilitates the interaction between them as the layers deepen, which enhances the specialization of single-modal representation and guarantees a thorough modality fusion. Thus, we obtain significant improvements compared with the non-specific prompt methods P-Tuning and P-Tuning v2, even though P-Tuning v2 requires a larger prompt parameter space for prepending soft prompts in each Transformer layer."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Evaluation on Multi-modal Sentiment Analysis",
            "text": "For the MSA task, the experimental results on the MVSA-S dataset are shown in Table 5. There is a huge performance gap between the non-pretrained methods (MVAN, MGNNS) and pre-trained methods. Among the pre-trained multi-modal methods, InstructBLIP demonstrates impressive efficacy, outperforming the current state-of-the-art UP-MPF. VLMo series perform better than CLIP series in this task, perhaps because the unified structure of VLMo, which effectively leverages both text and image input, is well-suited to this type of task, and the abundant pre-training tasks (especially mask language modeling) endows it with the ability to infer sentiment. VLMo performs well in the zero-shot setting, establishing a robust baseline. When integrated with prompts, the performance of VLMs is significantly boosted. VLMo adopting prompt-based finetuning (+MP) delivers the most superior results compared to those without VLMo, even outperforming the large-scale language model InstructBLIP. After combining with our MoPE-BAF method, the performance is further elevated, surpassing UP-MPF significantly. It demonstrates the generality and validity of our proposed model in different MSU tasks."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   Ablation Study",
            "text": "We conduct ablation experiments to investigate the effects of MoPE and BAF, and the results are presented in Table 6. In both settings (with or without using Manual Prompt), MoPE significantly improves the performance of VLMo, and further adding the prompt fusion operation achieves better performance. Please note that we cannot validate the effect of BAF separately since it cannot exist independently of MoPE."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "6.4.   Model Analysis",
            "text": "We also conduct experiments to analyze some controllable factors in the proposed MoPE-BAF model. Generally, a longer prompt correlates with an increase in learnable parameters, while in the few-shot setting, it may exacerbate the over-fitting problem. We vary the prompt length from 5 to 50 in MoPE, and the results concerning different lengths are visualized in Figure 4 (a). Overall, the impact of prompt length on model performance shows a trend of initially increasing, subsequently diminishing, and then stabilizing. Our explanation is that prompts too short are insufficient to bring about qualitative changes to the model, while too long may pose challenges for searching for the optimal solution due to the complexity of the expanded search space.\n\nWe investigate the impact of the block number in the BAF module. Specifically, in the first 21 layers of VLMo that use V-Prompt and L-Prompt experts, we divide them into 2-7 blocks. A configuration with 1 block implies no prompt fusion, and is therefore not displayed here. If the number of model layers cannot be evenly divided by the block number, the excess layers are allocated to the bottom blocks. This ensures that the maximum layer difference between any two blocks does not exceed 1. The results are shown in Figure 4 (b). When the block number is between 3 and 5, the overall performance remains almost similar. However, a noticeable decline can be observed after 6 blocks. We speculate that too many cross-modal cross-attention operations between blocks cause their original specialization to be discarded, thereby contradicting the function of MoPE. An appropriate number of blocks can strike a balance between the specialization and different modalities interaction.\n\nThe number of training shots plays a crucial role in model performance, particularly in the few-shot setting. We conduct experiments to train the VLMo base and our full model with different numbers of samples. The results are presented in Figure 4 (c). We find that the performance of both models improves as training shots increase. Besides, applying MoPE-BAF consistently improves the performance of VLMo with the number of training examples from 4 to 64, which demonstrates the robustness and efficiency of our method."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In this paper, we present MoPE-BAF, a new multi-modal soft prompt framework catering to unified VLMs for few-shot multi-modal tasks. Specifically, we devise two prompt experts to serve the text and image modality separately with a better specialization ability, and further activate the interactions of prompt experts by inserting cross-modal prompt attention between adjacent Transformer blocks. In this way, we reach a harmonious balance in modality specialization and fusion, thus fulfilling the requirement of modeling deep relations between modalities. Experiments on multi-modal sarcasm detection and multi-modal sentiment analysis show that our MoPE-BAF model not only surpasses other widely-used prompt methods, but also outperforms the advanced large language models. Further analysis confirms the effectiveness of MoPE and BAF. In the future, we intend to incorporate task-related external knowledge into our prompt design, and broaden the scope of our method to include other tasks, such as multi-modal content generation and multi-modal reasoning."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Limitations",
            "text": "While our study provides insights into the soft prompt technique on VLMs, it has to be acknowledged that it has some limitations. First, we observe a sensitivity of MoPE during training. The performance of MoPE relies on appropriate hyperparameter selection and the optimal hyperparameters differ across downstream tasks. While performing perform a grid search on hyperparameters may mitigate the issue, we believe that how to effectively control the sensitivity is worth further exploration in future work.\nBesides, although MoPE-BAF can theoretically be applied to any unified VLMs without disrupting the architecture or encoding process of the base architecture, our study was conducted exclusively on VLMo, and we have not yet extended MoPE-BAF to other pre-trained VLMs. This restricts the generalizability analysis of our methods. In the future, we would apply MoPE-BAF on more VLMs, which we think would provide a more comprehensive understanding of the role and impact of our methods."
        }
    ]
}