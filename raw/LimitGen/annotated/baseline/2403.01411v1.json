{
    "title": "OVEL: Large Language Model as Memory Manager for Online Video Entity Linking",
    "abstract": "In recent years, multi-modal entity linking (MEL) has garnered increasing attention in the research community due to its significance in numerous multi-modal applications. Video, as a popular means of information transmission, has become prevalent in people’s daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos’ mentions to entities in multi-modal knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking (OVEL), aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of OVEL, we specifically concentrate on live delivery scenarios and construct a live delivery entity linking dataset called LIVE. Besides, we propose an evaluation metric that considers timeliness, robustness, and accuracy. Furthermore, to effectively handle the OVEL task, we leverage a memory block managed by a Large Language Model and retrieve entity candidates from the knowledge base to augment LLM performance on memory management. The experimental results prove the effectiveness and efficiency of our method.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Videos, showcased by platforms like TikTok and YouTube, have become a dominant medium for communication. As their significance grows, so does the breadth of academic research into understanding them. Beyond the well-studied areas of video retrieval and captioning, scholars are exploring aspects like pre-training, cross-modal fusion, and more, striving for a comprehensive grasp of video content. However, these existing studies mainly concentrate on understanding the holistic content of videos and often overlook the significance of specific entities within them. \n\nConsider a live streaming example: where a video captioning model might merely state “a host explaining a product”, however, for viewers, specific details like “Nike Air Jordan 37th Generation Mid-Top Basketball Shoes” might be the critical information they seek. Therefore, in such scenarios, discerning specific entities can be more vital than a broad overview of the video content. Video entity linking refers to linking mentions that appear in a video to their corresponding entities in a knowledge base. Related research on this task is still relatively limited.\n\nThere have been some studies that have conducted video entity linking, but with certain limitations. On the one hand, they link to coarse-grained entities like “bird” or “human”, which becomes overly simplistic due to the broad granularity. On the other hand, they don’t demand real-time processing. With the rise of network terminals, there is an increasing demand for improved online performance in certain scenarios. For instance, in online sports live broadcasts, if specific athletes can be identified, comments and even real-time explanations can be generated based on the career of the athletes. These scenarios put forward higher requirements for online video entity linking.\n\nIn this paper, we propose the task of Online Video Entity Linking (OVEL) on dynamic video streams. The objective of this task is to link important entities appearing in online videos to a corresponding knowledge base. Furthermore, to advance the research on OVEL, we construct a dataset for LIVE stream product recognition based on live streaming scenarios called LIVE, which includes 82 live streams and nearly 250 hours of video. Based on the LIVE dataset, to better evaluate the accuracy and efficiency of entity linking on video streams, we introduce a time-weighted decay metric named RoFA, which comprehensively considers the accuracy and robustness of model predictions while also imposing requirements on online performance.\n\nConsidering the OVEL task and LIVE dataset, as shown in figure 1, we analyze the OVEL task, which poses several key challenges:\n\n1. Much Noise. Real-time scenarios often exhibit a multitude of visual scenes and various sounds, which can introduce interference in entity recognition. For instance, in live-streaming e-commerce scenarios, hosts tend to use a significant number of interjections, engage in interactions with viewers, or interact with other hosts. These can cause substantial interference in the recognition of entities.\n\n2. Timeliness. In online scenarios, which are characterized by strict time constraints, the prompt identification of salient entities and their timely recommendation to potential users often results in enhanced economic benefits. The expeditious identification of significant entities entails a challenging prerequisite for timeliness.\n\n3. Domain knowledge. Recognizing certain products requires a certain level of domain knowledge, and individuals unfamiliar with the domain may struggle to make accurate identifications. For instance, it might be challenging for some people to distinguish the specific generation and specific superstar’s basketball shoes.\n\nConsidering these challenges of the OVEL task, we propose several methodologies to address these challenges. Firstly, to address the issue of high noise levels in online scenarios, we propose adopting a LLM-based information extraction approach, aiming to extract information from videos that are more relevant to the entities. Secondly, to address the issue of timeliness, we utilize a memory block to store information before the current inference moment. For the subsequent moment, only the information within the time interval and the memory block before this moment need to be inputted, ensuring real-time performance. And we delegate the management of the memory block to the LLM. Furthermore, to tackle the domain-specific nature of live recognition, we propose utilizing a model retrieval to provide examples to LLM, enabling the LLM to possess a broader background knowledge.\n\nIn summary, the main contributions of this paper are as follows:\n- To the best of our knowledge, we introduce the task of online entity linking (OVEL) for the first time, focusing on improving the accuracy and efficiency of entity recognition in online videos.\n- Building upon live streaming scenarios, we have created a dataset for live stream product recognition, comprising 82 live stream videos, approximately 250 hours of video, and nearly 3,000 data instants. And a corresponding metric named RoFA.\n- To better address the task of OVEL, we propose a framework for the comprehensive management of video stream information based on LLM as a memory manager. Additionally, we leverage retrieval for LLM to manage memory better and employ a two-stage approach for entity linking. Subsequent experiments validate the effectiveness of our framework."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Multi-modal Entity Linking",
            "text": "Multimodal Entity Linking (MEL) is an extension of entity linking that links mention in multi-modal information (e.g., images, audio, or videos) to a corresponding knowledge base. Existing research primarily focuses on static image-text pairs. ResearchersAdjali et al. (2020a  ###reference_b1###, b  ###reference_b2###); Zhou et al. (2021  ###reference_b42###); Wang et al. (2022b  ###reference_b34###, a  ###reference_b33###); Gan et al. (2021  ###reference_b11###); Sun et al. (2022  ###reference_b30###); Chengmei et al. (2023  ###reference_b7###); Xing et al. (2023  ###reference_b36###); Shi et al. (2023  ###reference_b29###); Yao et al. (2023  ###reference_b39###); Zhang et al. (2021  ###reference_b40###) constructed multiple datasets for different scenarios or proposed various multimodal representation methods, integrating features from different modalities to facilitate entity mention and entity matching.\nThese studies primarily focus on static textual and graph data and have not been extended to the domain of videos. In the realm of entity linking in videos, Li et al. (2015  ###reference_b16###) introduced a dataset for entity linking in videos and linked prominent entities from the videos to the knowledge base. For example, they linked highlights of Kobe Bryant’s career to the entity “Kobe Bryant”. Venkitasubramanian et al. (2017  ###reference_b31###) established a dataset for documentary video linking, utilizing video descriptions and content recognition to identify corresponding animals such as lions, birds, and others. These methods have two limitations. Firstly, the granularity of entities in videos is often too coarse, lacking fine-grained entity identification. Secondly, they primarily focus on pre-stored videos, linking them to the knowledge base with the whole video information, without considering real-time entity linking for online video streams."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM as Memory Controller",
            "text": "With the development of large language models (LLMs) Devlin et al. (2019  ###reference_b8###); Radford et al. (2018  ###reference_b27###, 2019  ###reference_b28###); Brown et al. (2020  ###reference_b5###), LLMs that have been pre-trained on massive corpora have demonstrated remarkable capabilitiesOuyang et al. (2022  ###reference_b25###); Wei et al. (2022  ###reference_b35###). With the advent of powerful generative models such as GPT-4OpenAI (2023  ###reference_b24###), these models have demonstrated exceptional capabilities in generation, conversation, and the comprehension of human instructions, finding applications across a variety of downstream tasks. Recently, numerous researchers have integrated Memory with Large Language Models (LLMs), proposing frameworks to address resource constraints such as input length limitations inherent in LLMs. Liang et al. (2023  ###reference_b17###) proposed the utilization of memory to enhance the ability of LLMs to handle long texts, while Zhong et al. (2023  ###reference_b41###) introduced a customized memory mechanism specifically designed for LLMs. These frameworks offer potential for downstream applications of LLM-based agents. In this paper, we employ the LLM for memory block management and entity linking."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Retrieval Augment Generation",
            "text": "Despite the impressive capabilities demonstrated by models trained on large-scale corpora, they still suffer from phenomena such as hallucinations, long-tail problems, and knowledge decay. Retrieval augmentation, as a form of external corpora and knowledge enhancement, can alleviate these limitations of large models. In recent years, retrieval augmentationLewis et al. (2021  ###reference_b15###); Guu et al. (2020  ###reference_b12###); Lin et al. (2023  ###reference_b18###) has been employed in various stages of model training, fine-tuning, and inference, leading to improved performance of models on downstream tasks.\nIzacard et al. (2022  ###reference_b14###) utilized knowledge retrieval as a few-shot paradigm to enhance the performance of large language models in tasks such as knowledge question answering and fact-checking. Vu et al. (2023  ###reference_b32###) leveraged search engine retrieval to augment large language models, mitigating the issue of factual inaccuracies resulting from outdated knowledge. Asai et al. (2023  ###reference_b3###) improved the quality and factuality of model-generated outputs through retrieval and reflection.\nIn this paper, we utilize retrieval augmentation to alleviate the issue of insufficient knowledge using LLM in domain-specific scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Benchmark Construction and Evaluation",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "Online Video Entity Linking (OVEL) is a task designed for live video data streams. The goal of this task is to accurately identify salient entities in a live video stream, like the products highlighted by the anchor in the live broadcast scene. Given a live video, for instance, within the first 3 seconds, the host first mentions a specific pair of Nike shoes, followed by another 3 seconds of detailed introduction of it, then 3 seconds of answering questions from the live audience, and another 3 seconds of introduction to Nike shoes, and followed by a 3 seconds of Adidas’s competitive shoes. The prominent entities in these video streams should be Nike shoes, OVEL should predict the Nike shoes for each 3 seconds input accuracy and robustness. This uneven distribution of information poses significant challenges to the OVEL task.\n\nThe input of OVEL should be a sequence of frames that accumulate with time. Given a live video consisting of a list of video clips, where each element represents a clip of the video, a predefined knowledge base is available, where each entity in the knowledge base has corresponding multimodal information. Below is the formal formulation of OVEL at a timestamp:\n\nAn entity should be predicted at each timestamp with the video information before that timestamp. Hence, a list of entities will be predicted in the video. Each entity in the prediction list should be as similar as the ground truth. This places significant challenges on the robustness and accuracy of the algorithm."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Construction and Analysis",
            "text": "To advance the research on OVEL task, we have built an e-commerce video stream entity linking dataset based on live streaming scenarios. The construction of the dataset consists of three main steps. Firstly, the initial raw videos and their corresponding multimodal knowledge base are obtained. The second step involves segmenting the corresponding live videos into data instances and manually annotating the entities in the knowledge base. The third step entails simulating online input by dividing each data instance into a list of video clips based on their playback time. The details of dataset construction and dataset Analysis can be found in Appendix A."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation For OVEL",
            "text": "Evaluating the OVEL task is not inherently straightforward and presents certain challenges. In the domain of live streaming, early identification of entities is increasingly effective for recommendation algorithms, potentially leading to greater economic benefits. The simplest approach involves assigning higher scores to instances where the correct location of real entities is identified earlier in the video. However, there is a possibility of correct recognition in the first minute but misidentification after one and a half minutes, which puts forward requirements for the robustness of the algorithm. Based on these characteristics, we propose a comprehensive metric that considers accuracy, online performance, and robustness, referred to as Robust online Fast Accuracy (RoFA). Below is the formulation of RoFA:\n\nGiven a list of prediction results in the temporal sequence, where the scores for predictions made later should be lower, we have devised a weighted decay mechanism that is proportional to the size of the prediction results. We initialize a linearly decreasing weight. For example, the weight of the first prediction is set to 1, and the weight of the last prediction is set to 0.2. The weights between these two windows decrease linearly, which aims to evaluate the fast and robust performance of algorithms. As we only recommend the best matching product to users, when considering the prediction result for each video clip, if the prediction is correct, the score should be 1. Meanwhile, if the prediction is incorrect, the score is 0. The final metric is calculated as the sum of scores divided by the sum of weights, representing the average score. The calculation method of RoFA is as follows: \n\nwhile the score is calculated as below, with the actual entity denoted by the ground truth of the video and the predicted entity."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "In this section, we will first present the overall framework of the methodology, followed by an introduction to the summary modules that constitute the methodology and an overview of the main components of the LLM as the memory controller. Finally, we will introduce the two-stage entity linking methods."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Overview of the Framework",
            "text": "###figure_2### Figure 2  ###reference_### illustrates the entire workflow of our Framework. When the input is an online video, we initialize the initial memory block using the summary module. Then we leverage the memory block and image information from video clips to perform the initial retrieval of candidate products. At each time t, the LLM manager gets the current video information, accesses the content within the memory, and refers to the results obtained from the retrieval model to make decisions and update the memory from the previous time step. To better use LLM’s capacity, we also employed a two-stage entity linking method. First is the retrieval model to retrieve the candidate entities, and give candidates to LLM for fine-grained entity disambiguation. Below we will provide a detailed description of each module."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Summary Module",
            "text": "For a given input of video clips, while  represents the video clip at time , transcribed speech text , and keyframe sequences  over time. The task of OVEL is to predict ground truth entities at every moment as accurately as possible.\n\nAs a task of multimodal entity linking, the fundamental model should be a multimodal retrieval model. The multimodal retrieval model aims to maximize the similarity between real-time videos and their corresponding entities while minimizing the similarity between non-matching entities. This can be represented by the following equation:\n\nIn the equation,  denotes the similarity calculation, while  represents the encoder component for both the video and the entities in the knowledge base. The video contains two multi-modal information: speech text and images.  contains all the information before time .\n\nHowever, in the context of live streaming, real-time videos present dynamic and evolving information, accompanied by a substantial amount of irrelevant noise, such as the host’s habit of introducing “all girls” and engaging with the audience. To address this issue, we first propose an approach that leverages a LLM for extracting textual content from speech. Equation 7  ###reference_### is replaced with the following formulation:\n\nWe utilize speech text following summaries for multimodal retrieval, which forms the summary module of our proposed method."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Memory Controller Module",
            "text": "However, online entity linking poses a challenge in terms of responsiveness. As the video progresses in time, we encounter a more important challenge. The length of the textual content extracted from speech increases over time, resulting in longer summaries. At this point, using the summary module cannot meet the real-time requirements. To address this issue, we propose utilizing a memory block to store past extracted information.\n\nThe memory block module is designed to record entity-related attributes from previous video clips. When processing new video segments, only the current memory information needs to be updated, thereby avoiding linear growth in the number of tokens required for inference per clip. From the equation, it can be observed that at each time step, only the memory from the previous time step and the textual information of the current clip are required as inputs.\n\nHowever, in the live-streaming scenario, there are limitations. The granularity of products in live streaming is relatively fine, requiring domain-specific knowledge. Additionally, there is a significant amount of irrelevant information present in the videos. If we solely rely on an LLM trained in a general domain to manage the memory block, there is a risk of extracting a large amount of irrelevant information. To ensure that the memory block is primarily filled with information related to the products, we combine it with the retrieval model. The products obtained through multimodal retrieval are simultaneously considered by the LLM, which acts as guidance for better memory block management.\n\nFrom the equation, it can be observed that at each time step, the inputs consist of the memory from the previous time step, the textual information of the current slice, and the retrieval results from the retrieval model. This not only fulfills the requirements of real-time inference but also alleviates the issue of insufficient domain-specific knowledge in LLM."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Two-stage Entity Linking",
            "text": "The LLM demonstrates remarkable capability, which we desire to use for entity linking. However, in real-time scenarios, it is challenging to provide all the candidate entities to the LLM due to its limited context length, and fine-grained non-deterministic generation is also difficult. Drawing from previous approaches, we divide the linking process into two steps: the first step involves the retrieval model to get entity candidates, and the second step involves the entity disambiguation made by the powerful LLM.\n\nThis approach not only leverages the powerful background knowledge of LLM but also reduces the time-consuming inference capacity. Above is the comprehensive presentation of our proposed framework. The following experiments show that our method ensures real-time performance while effectively enhancing overall performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we primarily focus on analyzing the experimental results conducted on the constructed dataset. Firstly, we present the main experimental results of the overall framework. Secondly, we examine the comparative analysis of different approaches in terms of temporal performance. Lastly, we compare the performance of traditional metrics on static videos. Furthermore, several intriguing phenomena emerged during our experimental process, which can be observed in Appendix B."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiments setting",
            "text": "Model selection. We employed Qwen-14B-ChatBai et al. (2023  ###reference_b4###) as the LLM. We also utilized powerful models such as ChatGPT. However, due to a lack of training data in the e-commerce domain, these models performed poorly on the task. We also attempted to translate the Chinese memory block and product database into English and used English text-image retrieval, however, due to the transcription errors in the text converted from voice in the live streaming scenario, including brand names, it is challenging to achieve fine-grained product recognition. Our static experimental results also demonstrate the underperformance of other models on the dataset.\n\nImplement Details. For the method proposed in this article is designed for online performance analysis, all experiments are performed on the same machine. Our local machine has four 3090 GPUs. To facilitate better inference, we deployed Qwen-14B-Chat on an A100 80G machine and used API calls to manage memory blocks through the LLM controller. Due to limitations in local inference memory, we randomly sampled a product database approximately 10 times larger than the test set from the knowledge base. We fixed this subset of 3,000 products as the candidate pool, and the test set consisted of 275 video samples. We assume that the model begins generating outputs after processing 10 video clips, indicating that the model starts linking from the 10th video clip. To better utilize the sequential information in memory, except for the Base method, all other approaches perform inference once every 5 video clip sizes. The inference results are then replicated for all five video clips. All methods are finetuned on the training set."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Main result",
            "text": "In this section, we added our framework to two multi-modal retrieval models. To compare the effectiveness of different modules, we denote the model that directly employs multimodal retrieval as “Base”, and our proposed LLM as memory controller as “Ours”. From the table, it can be observed that the approach combining retrieval model retrieval with LLM achieved the highest performance. Particularly, our method combining the CN-CLIP_L model achieved the best results, likely due to CN-CLIP_L’s superior performance on the test sets compared to the other retrieval models. In most cases, using a single memory management approach yields slightly inferior results compared to using full summaries, as the structure of memory management lacks complete information, leading to some information loss. The results also demonstrate that our method provides substantial improvements when the retrieval model performs poorly. This suggests that in low-resource scenarios where the retrieval model lacks training data, leveraging the combination of the LLM can serve as a viable solution."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Online performance analysis",
            "text": "In this section, we analyze the online performance of different methods. We evaluated the time performance of various methods on the test set. We assessed the time taken to give a predicted entity and recorded the inference time on the test set at intervals of every five video clips. After calculating the average time for each method, the smoothed results are presented in Figure 3.\n\nFrom Figure 3, it can be observed that “Base” utilizes the retrieval model and yields the best time performance. The time cost of the Ours-M and Our Ours-R significantly increases as the number of video clips grows. It needs to be mentioned that when the window size exceeds 200, these methods surpass the recommended inference time, thereby potentially failing to provide meaningful linked entities within the given time interval. On the opposite, Our method initially exhibits a rapid increase in time cost, followed by a tendency toward stability.\n\nAnalyzing the reasons behind this situation: as the number of video clips increases, the length of the memory block also increases, resulting in an information increase in all methods. In the later stages of inference, due to its domain knowledge from the retrieval model, Ours method tends to have content that is more related to specific products and remains fixed. On the other hand, the Ours-R may continue to accumulate irrelevant information as it lacks related knowledge. And Ours-M method exhibits some instability due to variations in the length of text in different video clips, the reason may be the lack of a complete memory, the extracted information may be inconsistent in format, and there may be insufficient or redundant."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Static experiment results",
            "text": "To compare with some of the existing methods for Multimodal Entity Linking (MEL) and Video Entity Linking (VEL), we treat each video as an individual data instant and perform video entity linking across the entirety of the video’s content to ascertain the applicability of our method to static data as well. In this chapter, we have selected a variety of representative approaches for evaluation. These include CLIP4ClipLuo et al. (2022) in the domain of video retrieval, a purely textual entity linking approach BLINKLogeswaran et al. (2019), and the multimodal entity linking method V2VTELSun et al. (2022). The experimental metrics primarily utilized are Recall and Mean Reciprocal Rank (MRR) at K. The experimental outcomes are as exhibited in Table 2.\n\nFrom Table 2, it can be observed that our method achieves the best performance, once again demonstrating the effectiveness of our approach. Furthermore, the performance of the CLIP4clip and V2TVEL approaches compared to pure text-based BLINK is poor, indicating that text plays a more significant role in our scenario."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose an Online Video Entity Linking (OVEL) task for online videos, construct the LIVE dataset based on live streaming scenarios, and introduce the RoFA metric, which considers robustness, timeliness, and accuracy. Based on the dataset, we present a method that combines LLM with a retrieval model for memory management, which handles the OVEL task efficiently. Experimental results demonstrate the effectiveness of our approach. However, OVEL is a highly challenging task, and we earnestly invite researchers to join us in the field of online video entity linking."
        }
    ]
}