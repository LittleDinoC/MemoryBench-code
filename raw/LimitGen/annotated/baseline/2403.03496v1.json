{
    "title": "A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation",
    "abstract": "Knowledge-based, open-domain dialogue generation aims to build systems that converse with humans using mined support knowledge. Various types and sources of knowledge have been proven useful for this purpose. Despite advancements with large language models, generating responses based on knowledge from additional up-to-date sources continues to be important. Prior work with single-source knowledge has demonstrated a positive correlation between knowledge selection performance and response generation. However, there is a lack of multi-source datasets for evaluating support knowledge retrieval. Previous research has often assumed that knowledge sources available during testing are the same as during training, which is not always realistic, as new knowledge can emerge post-training. In this context, we introduce a high-quality benchmark called multi-source Wizard of Wikipedia (Ms.WoW) for assessing multi-source dialogue knowledge selection and response generation. Ms.WoW includes clean support knowledge, grounded at the utterance level, and is divided into several knowledge sources. Moreover, we introduce a new challenge, called dialogue knowledge plug-and-play, aimed at evaluating an already trained dialogue model's ability to utilize new knowledge from previously unseen sources in a zero-shot manner.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Knowledge-based open-domain dialogue generation aims to build chit-chat systems that talk to humans on various domains with mined support knowledge. Many types of knowledge have been shown to be useful as support knowledge, such as encyclopedias Dinan et al. (2019), knowledge graphs Wu et al. (2019); Zhou et al. (2020); Liu et al. (2021), personas Zhang et al. (2018), and commonsense knowledge Zhou et al. (2018); Zhang et al. (2020); Wu et al. (2021); Varshney et al. (2022). Further, Shuster et al. (2022) have demonstrated that multiple knowledge sources are helpful on top of large-scale, pre-trained language models. Even in the era of large language models (LLMs; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023), which use internally learned knowledge from their pretraining corpora for zero- or few-shot predictions, knowledge can become outdated. Thus, response generation grounded in knowledge retrieved from additional up-to-date sources is still a practically important approach. Previous studies using single-source knowledge for response generation have shown a clear positive correlation between the performances of knowledge selection and response generation Dinan et al. (2019); Li et al. (2022). In this work, we aim to extend these results to multi-source knowledge and further propose a new challenge task, dialogue knowledge plug-and-play. We address two major challenges facing multi-source knowledge-based dialogue generation. First, there are no existing multi-source datasets for evaluating support knowledge retrieval. Prior work used non-knowledge-based dialogue datasets with silver support knowledge labeled using unsupervised approaches Liu et al. (2019b); Wu et al. (2021); they could only measure the final response generation performance, without being able to evaluate the support knowledge selection module. As a result, results achieved on these datasets lack interpretability: the relationship between the quality of knowledge selection and response generation is unclear, adding an extra layer of difficulty in improving models’ performance. Second, prior work has assumed that the knowledge sources available at test time are the same as during training. We argue that this is an unrealistic assumption that unnecessarily handicaps models. New knowledge sources can become available after a model is trained: new knowledge graphs are published, or new types of knowledge are shown to be useful for dialogue generation. Information present only in a new knowledge source (for example, about a recent newsworthy event) may be crucial in a conversation with a real user. To make use of such information, it is necessary to ensure that trained dialogue generation models are robust to the addition of new knowledge sources at inference time: the new source should improve, or at the very least not harm, a model’s performance. PepsiCo was formed in 1965 with the merger of the Pepsi-Cola Company and Frito-Lay, Inc. PepsiCo has since expanded from its namesake product Pepsi to a broader range of food and beverage brands, the largest of which included an acquisition of Tropicana Products in 1998 and the Quaker Oats Company in 2001, which added the Gatorade brand to its portfolio. (‘’, ‘formed’, ‘PepsiCo’, ‘in 1965’, ‘’) (‘its’, ‘’, ‘has’, ‘namesake product Pepsi’, ‘’, ‘’) (‘largest of which’, ‘’, ‘have included acquisition of Tropicana Products in 1998’, ‘beverage brands’, ‘’, ‘’) (‘largest of which’, ‘’, ‘have included Quaker Oats Company in 2001’, ‘food brands’, ‘’, ‘’) (‘Quaker Oats Company in 2001’, ‘’, ‘added Gatorade brand to’, ‘portfolio’, ‘’, ‘’) (‘Frito-Lay’, ‘parent organization’, ‘PepsiCo’) (‘Pepsi’, ‘instance of’, ‘cola’)"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Background & Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Open-Domain Dialogue Generation",
            "text": "To the best of our knowledge, no existing open-domain dialogue dataset is well-suited for the study of dialogue knowledge plug-and-play.\nFirst, most knowledge-grounded, open-domain dialogue corpora only provide support knowledge from a single source Wu et al. (2019); Zhou et al. (2020); Liu et al. (2021); Wang et al. (2021); Komeili et al. (2021). For example, the WoW dataset Dinan et al. (2019), which we use as the base for our study, uses a knowledgeable “wizard\" speaker who records the Wikipedia support articles for each utterance during a conversation. However, all of that support knowledge is from a single source: Wikipedia plain text. In this work, we partition the knowledge in WoW to simulate the availability of several “sources\" containing complementary knowledge.\nSecond, existing multi-source, knowledge-based dialogue generation works collect support knowledge after the conversation occurs, so there is no gold support knowledge grounded for each utterance. Liu et al. (2019b) map an unreleased knowledge graph to two existing datasets Moghe et al. (2018); Dinan et al. (2019) to create an augmented multi-source dataset. Wu et al. (2021) build a single-turn dialogue dataset upon three Weibo corpora Shang et al. (2015); Ke et al. (2018); Cai et al. (2019) by extracting post-reply pairs and further augment the utterance pairs with ConceptNet Speer et al. (2017), as well as article text and infobox tables from Chinese Wikipedia. These works are not able to report explicit knowledge selection performance due to the lack of ground-truth support knowledge. Further, their methods produce noisy support knowledge, as erroneous examples are frequently seen in their automatic knowledge-matching process.\nHoll-E Moghe et al. (2018) is the closest multi-source dialogue work to ours; however, they focus only on the movie domain, retrieving plot, review, fact, and comment information for each utterance. Moreover, they do not perform knowledge selection, so the correlation between knowledge selection and dialogue generation performance is unknown.\nIn contrast, the single-source WoW dataset contains gold support knowledge annotated at the utterance level, and because state-of-the-art Transformer-based models are unable to take all candidate knowledge (i.e., all relevant Wikipedia articles) as input due to their length limits, Dinan et al. (2019) perform knowledge selection to filter out unneeded candidates and feed only the selected support knowledge to the dialogue response generator. Thus, dialogue systems evaluated on WoW are able to report knowledge selection performance and observe the positive correlation between the performances of knowledge selection and response generation. For these reasons, we build our study on top of WoW."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Plug-and-play",
            "text": "The concept of plug-and-play has been introduced in the context of studying language models’ ability to adapt to new knowledge. Dathathri et al. (2020) proposed a Plug and Play Language Model (PPLM) for controllable language generation. Xu et al. (2021) proposed K-PLUG, a knowledge-injected, pre-trained language model for e-commerce that handles information such as item category and attributes in key-value pairs, as well as item summaries and descriptions in plain text."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Multi-Source Wizard of Wikipedia Dataset",
            "text": "We use spaCy to extract entities and noun phrases from each sentence, filtered with NLTK555https://www.nltk.org/  ###reference_www.nltk.org/### stop words.\nWe pass each extracted entity or noun phrase, along with its original sentence as context, to a dense retrieval-based entity linker Wu et al. (2020  ###reference_b25###) to obtain the corresponding Wikidata entities. We take the top-1 Wikidata entity candidate for each extracted entity or noun phrase.\nWe retrieve all Wikidata triplets that contain at least one of the linked entities.\nWe keep only triplets whose subjects and objects both match the corresponding WoW knowledge sentence, requiring one of the following conditions:\nboth the subject and object entities in the triplet appear in the set of linked Wikidata entities extracted from the WoW sentence, or\nthe subject and object both match the WoW sentence using a fuzzy matcher666https://spacy.io/universe/project/spaczz  ###reference_### with score higher than .\nFor each WoW knowledge sentence, we keep only triplets whose entities cover more than 75% of the extracted entity set of the sentence.\nIn order to minimize information loss when we replace each WoW sentence with the retrieved tuples, we filter out those sentences whose retrieved tuples can only partially cover their semantics. We tokenize and lemmatize each WoW knowledge sentence and remove punctuation and stop words to create a bag of words  for each sentence. Then we concatenate all retrieved tuple elements from the three sources to create a pseudo-sentence and its corresponding bag of words . We only keep those sets of tuples whose  covers more than 60% of .\nThis filtering step results in the creation of a fourth, supplementary knowledge source: Wikipedia sentences (w.p.) from the original WoW that could not be adequately captured by tuples from the other three sources.\nSince the knowledge tuples are retrieved independently from the three sources, some are redundant with each other. Since our goal is to partition the knowledge into complementary sources, we perform deduplication to remove redundant tuples. Given the full-coverage bag of words , we want to select the minimum number of tuples that maximally cover . We formulate this goal as a set-cover problem, which is NP-Complete, and apply its 2-approximation algorithm to select the minimum set of tuples that covers the semantics of the original WoW sentence to the same extent as the full set of retrieved tuples."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Knowledge Tuple Retrieval or Extraction",
            "text": "We create Ms.WoW by replacing the Wikipedia knowledge sentences in WoW Dinan et al. (2019  ###reference_b6###) with tuples retrieved or extracted from three sources with different emphases. The three sources, described in detail below, provide disjoint partitions that cover the semantics of the original WoW sentences222All filtering thresholds in this section are empirically determined..\nWe use spaCy to extract entities and noun phrases from each sentence, filtered with NLTK555https://www.nltk.org/  ###reference_www.nltk.org/###  ###reference_www.nltk.org/### stop words.\nWe pass each extracted entity or noun phrase, along with its original sentence as context, to a dense retrieval-based entity linker Wu et al. (2020  ###reference_b25###  ###reference_b25###) to obtain the corresponding Wikidata entities. We take the top-1 Wikidata entity candidate for each extracted entity or noun phrase.\nWe retrieve all Wikidata triplets that contain at least one of the linked entities.\nWe keep only triplets whose subjects and objects both match the corresponding WoW knowledge sentence, requiring one of the following conditions:\nboth the subject and object entities in the triplet appear in the set of linked Wikidata entities extracted from the WoW sentence, or\nthe subject and object both match the WoW sentence using a fuzzy matcher666https://spacy.io/universe/project/spaczz  ###reference_###  ###reference_### with score higher than .\nFor each WoW knowledge sentence, we keep only triplets whose entities cover more than 75% of the extracted entity set of the sentence."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   OPIEC",
            "text": "OPIEC Gashteovski et al. (2019  ###reference_b7###) is a large-scale dataset generated using an open information extraction (OIE) system applied to the text of Wikipedia. For each sentence in Wikipedia, OPIEC extracts one or more (subject, negation, relation, object, time, space) tuples. The tuples are dense, structured versions of the original Wikipedia sentences.\nWe retrieve OPIEC tuples by performing soft sentence matching between the knowledge sentences in WoW and OPIEC’s source sentences. There are some mismatches between sentences in these two datasets because the Wikipedia dumps used are not exactly the same, due to the continuous editing of Wikipedia contributors. We use Sentence-BERT 333all-MiniLM-L6-v2 checkpoint from https://www.sbert.net/  ###reference_www.sbert.net/###. Reimers and Gurevych (2019  ###reference_b18###) to encode sentences from WoW and OPIEC that appear in the same Wikipedia article and consider sentence pairs with cosine similarity larger than  as a match."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Semantic frames",
            "text": "Semantic frames (sem. frm. or s.f.) capture the core semantics of sentences, such as who did what, when, and where. Previous work on OIE has studied the use of semantic role labeling (SRL)-based knowledge tuples Christensen et al. (2011  ###reference_b4###). Therefore, we use SRL results as complementary structural knowledge to the OPIEC tuples. We use the semantic roles parsed by spaCy’s444https://spacy.io/  ###reference_spacy.io/### SRL pipeline for each WoW knowledge sentence and use templates to map the results into (subject, relation, object, time, space) tuples."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3.   Wikidata",
            "text": "Wikidata (w.d.) is a large-scale knowledge base containing triplets grounded in Wikipedia articles. It contains (subject, relation, object) triplets that relate one Wikipedia concept to another. We retrieve triplets from Wikidata using the following steps:\nWe use spaCy to extract entities and noun phrases from each sentence, filtered with NLTK555https://www.nltk.org/  ###reference_www.nltk.org/###  ###reference_www.nltk.org/###  ###reference_www.nltk.org/### stop words.\nWe pass each extracted entity or noun phrase, along with its original sentence as context, to a dense retrieval-based entity linker Wu et al. (2020  ###reference_b25###  ###reference_b25###  ###reference_b25###) to obtain the corresponding Wikidata entities. We take the top-1 Wikidata entity candidate for each extracted entity or noun phrase.\nWe retrieve all Wikidata triplets that contain at least one of the linked entities.\nWe keep only triplets whose subjects and objects both match the corresponding WoW knowledge sentence, requiring one of the following conditions:\nboth the subject and object entities in the triplet appear in the set of linked Wikidata entities extracted from the WoW sentence, or\nthe subject and object both match the WoW sentence using a fuzzy matcher666https://spacy.io/universe/project/spaczz  ###reference_###  ###reference_###  ###reference_### with score higher than .\nFor each WoW knowledge sentence, we keep only triplets whose entities cover more than 75% of the extracted entity set of the sentence."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Post-processing",
            "text": "We perform post-processing to ensure the quality of the retrieved tuples and create complementary and disjoint partitions of knowledge sources by filtering out semantically incomplete and redundant tuples.\nIn order to minimize information loss when we replace each WoW sentence with the retrieved tuples, we filter out those sentences whose retrieved tuples can only partially cover their semantics. We tokenize and lemmatize each WoW knowledge sentence and remove punctuation and stop words to create a bag of words  for each sentence. Then we concatenate all retrieved tuple elements from the three sources to create a pseudo-sentence and its corresponding bag of words . We only keep those sets of tuples whose  covers more than 60% of .\nThis filtering step results in the creation of a fourth, supplementary knowledge source: Wikipedia sentences (w.p.) from the original WoW that could not be adequately captured by tuples from the other three sources.\nSince the knowledge tuples are retrieved independently from the three sources, some are redundant with each other. Since our goal is to partition the knowledge into complementary sources, we perform deduplication to remove redundant tuples. Given the full-coverage bag of words , we want to select the minimum number of tuples that maximally cover . We formulate this goal as a set-cover problem, which is NP-Complete, and apply its 2-approximation algorithm to select the minimum set of tuples that covers the semantics of the original WoW sentence to the same extent as the full set of retrieved tuples."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1.   Filtering of Retrieved Tuples",
            "text": "In order to minimize information loss when we replace each WoW sentence with the retrieved tuples, we filter out those sentences whose retrieved tuples can only partially cover their semantics. We tokenize and lemmatize each WoW knowledge sentence and remove punctuation and stop words to create a bag of words  for each sentence. Then we concatenate all retrieved tuple elements from the three sources to create a pseudo-sentence and its corresponding bag of words . We only keep those sets of tuples whose  covers more than 60% of .\nThis filtering step results in the creation of a fourth, supplementary knowledge source: Wikipedia sentences (w.p.) from the original WoW that could not be adequately captured by tuples from the other three sources.\nSince the knowledge tuples are retrieved independently from the three sources, some are redundant with each other. Since our goal is to partition the knowledge into complementary sources, we perform deduplication to remove redundant tuples. Given the full-coverage bag of words , we want to select the minimum number of tuples that maximally cover . We formulate this goal as a set-cover problem, which is NP-Complete, and apply its 2-approximation algorithm to select the minimum set of tuples that covers the semantics of the original WoW sentence to the same extent as the full set of retrieved tuples."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2.   Grounding Gold Knowledge Tuples to Utterances",
            "text": "In the original WoW dataset Dinan et al. (2019  ###reference_b6###), each Wizard utterance has at most one gold support knowledge sentence. However, in our Ms.WoW dataset, each original WoW support knowledge sentence is decomposed into multiple knowledge tuples (see Table 1  ###reference_### for an example). Since the semantics of the sentence is spread among these tuples, some tuples derived from the WoW knowledge sentence may contain extra information not found in the corresponding Wizard utterance. Therefore, we use the same set-cover approach as in Section 3.2.1  ###reference_.SSS1### to select those knowledge tuples that are grounded by their utterances; we refer to these grounded tuples as “gold\" tuples."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Dataset Statistics",
            "text": "Table 2  ###reference_### shows the statistics of our Ms.WoW dataset. OPIEC tuples are parsed from the entire Wikipedia, so they have the most coverage. In contrast, semantic frame tuples are much fewer in number due to our template-based matching rule.\nDespite originating from the same Wikipedia sentences, tuples derived from different sources have significantly different length attributes (see Table 1  ###reference_### for examples). As Table 3  ###reference_### shows, different knowledge sources yield knowledge with different numbers of components: while Wikidata tuples only have (subject, relation, object), OPIEC tuples also have negation, time, and space. The lengths of each knowledge type’s attributes are also significantly different: semantic frame tuples have single-word relations with long subjects and objects, while OPIEC tuples have longer relations with shorter subjects and objects. In addition, while semantic frames retain the original sentence tokens, OPIEC and Wikidata decompose the original sentences into multiple pieces."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Quality assurance via dialogue response generation",
            "text": "Since Ms.WoW is a new dataset derived from WoW but designed for a different purpose, we want to ensure that the knowledge tuples in Ms.WoW sufficiently retain the information in the WoW knowledge sentences. Table 9  ###reference_### shows that response generators using our full Ms.WoW. (all sources) perform comparably to those using the original WoW dataset: our ROUGE scores and utterance F1 are comparable, and our unigram multi-source knowledge F1 is close to that of the original WoW setting (metric descriptions are found in Section 4.1  ###reference_###). This confirms that our Ms.WoW covers the knowledge needed to support a dialogue model to generate high-quality responses with limited information loss compared to WoW."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Dialogue Knowledge Plug-and-Play",
            "text": "Having collected Ms.WoW, we apply it as a test bed to study dialogue knowledge plug-and-play via multi-source dialogue knowledge selection and response generation. We employ a baseline approach to demonstrate the basic characteristics and challenges of dialogue knowledge plug-and-play.\n\nWe create the input sequences by concatenating up to the last five utterances in the conversation history, speaker roles, and the support knowledge sentences and tuples for each Wizard dialogue turn. Each support knowledge subsequence is prepended with a special token <kg>. We feed the input sequence to a pre-trained language model.\n\nWe use the Vicuna-13B-v1.1 LLM from Huggingface. We set the generation temperature to 0.7. It takes approximately 30 hours to perform inference on the test set (seen + unseen) for each configuration using 4 Nvidia Tesla V100S GPUs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Experimental Design",
            "text": "Retraining a dialogue model each time a new knowledge source becomes available is computationally costly and requires extra engineering effort. Dialogue knowledge plug-and-play examines the ability of a pretrained dialogue model to adapt to support knowledge from new sources in a zero-shot fashion.\n\nFor response generation, we compare the knowledge-ablated models to an upper bound. We train a model on the full set of available knowledge tuples, simulating a model that is retrained when the new knowledge source becomes available.\n\nThe following is the conversation between the “Wizard\", a knowledgeable speaker who can access Wikipedia knowledge sentences to chat to with the “Apprentice\", who does not have access to Wikipedia. The conversation topic is {{topic}} and the persona setting of the Wizard is “{{persona}}\".\n\nThis is their conversation history:\n{{speaker 1}}: {{utterance 1.1}}\n{{speaker 2}}: {{utterance 2.1}}\n{{speaker 1}}: {{utterance 1.2}}\n…\n\nHere is some retrieved Wikipedia knowledge for the Wizard. Some of the knowledge is in the tuple form, such as (subject, negation, relation, object, time, space) or (subject, relation, object). The Wizard can choose any subset of the following knowledge. It’s also allowed to not choose any of them.\n{{(subject 1, relation 1, object 1)}}\n…\n\nGiven the knowledge above, make a very brief, such as one sentence, natural response for the Wizard. Not all information in the chosen knowledge has to be used in the response. The Wizard’s response is:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Baseline Approaches",
            "text": "We create the input sequences by concatenating up to the last five utterances in the conversation history (), speaker roles (), and the support knowledge sentences and tuples () for each Wizard dialogue turn. Each support knowledge subsequence is prepended with a special token <kg>. We feed the input sequence to a pre-trained language model. The input sequence can be written as:\nWe use a BART-base Lewis et al. (2019 ###reference_b10### ###reference_b10###) model () to perform a standard response generation using the same input from Equation 1 ###reference_### ###reference_###: ."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1.   Fine-tuned Models",
            "text": "We create the input sequences by concatenating up to the last five utterances in the conversation history, speaker roles, and the support knowledge sentences and tuples for each Wizard dialogue turn. Each support knowledge subsequence is prepended with a special token <kg>. We feed the input sequence to a pre-trained language model. We use a BART-base model to perform a standard response generation using the same input."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2.   Large Language Model",
            "text": "We also prompt an LLM in a zero-shot fashion for the response generation task. We use Vicuna-13B, a 13-billion-parameter LLaMA model fine-tuned on 70k user-shared conversation samples. Table 7 shows the prompt we use."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Experimental Details",
            "text": "We use the BART-base (139M parameters) model from Huggingface. We mostly use the default hyper-parameters. We use a single Nvidia Tesla V100S GPU for model training and testing. Each dialogue knowledge selector and dialogue generator takes approximately 3 hours for training and a few minutes for inference. We use the Vicuna-13B-v1.1 LLM from Huggingface. We set the generation temperature to 0.7. It takes approximately 30 hours to perform inference on the test set (seen + unseen) for each configuration using 4 Nvidia Tesla V100S GPUs."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experimental Results and Analysis",
            "text": "Tables 9  ###reference_### and 10  ###reference_### show the performance of our BART-based response generator; and Table 11  ###reference_### shows the performance of the LLM response generator.\nTo measure the dialogue response generation performance, in addition to ROUGE scores Lin (2004  ###reference_b12###) compared to the gold response, we follow Dinan et al. (2019  ###reference_b6###) in reporting the unigram F1 of the generated response with the gold response, as well as unigram precision, recall and F1 (K-P, K-R & K-F1) of the generated response with all available (i.e. non-ablated) candidate knowledge."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Full-knowledge vs. Zero-shot Adaptation",
            "text": "Unsurprisingly, there is a significant difference between the full-knowledge model (i.e. retrained with each new knowledge source) and the zero-shot adapted models that have one knowledge source ablated during training (Tables 4  ###reference_### and 9  ###reference_###). This performance gap generally increases as the ablated knowledge source occupies a larger proportion of the overall available knowledge.\nThis trend is clearer when we separately examine the knowledge selectors’ performances on each knowledge source in Table 5  ###reference_### (diagonal entries vs. full knowledge). In general, an ablated model’s recall score on the newly available knowledge source is dramatically lower than that of the full-knowledge model; the distribution of the new knowledge source is not recognized as usable support knowledge. This observation clearly shows the significant performance gap between the full-knowledge model and the zero-shot adapted models, which is exactly the gap that our dialogue knowledge plug-and-play challenge aims to highlight as a target for improvement.\nInterestingly, Table 5  ###reference_### (non-diagonal entries vs. full knowledge) also shows that a new knowledge source becoming available improves the model’s knowledge selection performance on some of the previously available knowledge sources, demonstrating that there is some synergy among knowledge from different sources.\nThe only exception to the observations above is the supplementary Wikipedia sentence source, consisting of WoW sentences that could not be adequately covered by our three other sources."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Differences among Knowledge Sources",
            "text": "There is a clear difference in difficulty among knowledge sources. Since semantic frame tuples are extracted using high-precision, human-engineered templates, all models perform significantly better on semantic frame tuples than other knowledge types (Table 5  ###reference_###).\nDifferent knowledge sources also have different usefulness in response generation. As Table 9  ###reference_### and 11  ###reference_### show, Wikidata knowledge triplets are not as helpful as the other knowledge sources. This may be because Wikidata triplets are generally short (Section 3.3  ###reference_###), making them less informative than the other knowledge sources."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   “More is Better” in Zero-shot Settings",
            "text": "Tables 6  ###reference_### and 10  ###reference_### show the knowledge selection and response generation performance of our models when one knowledge source is ablated from both training and testing, simulating a scenario where that knowledge source never becomes available. Comparing these results with Tables 5  ###reference_### and 9  ###reference_###, respectively, we see that introducing additional knowledge sources, even in a zero-shot fashion, mostly benefits, rather than hurts, knowledge selection and response generation, supporting the claim that “more (knowledge sources) is better” Wu et al. (2021  ###reference_b26###). This is a promising result for dialogue knowledge plug-and-play, which challenges models to be robust to new knowledge sources. This phenomenon is also relevant to the in-context-learning Brown et al. (2020  ###reference_b1###) of LLMs, where LLMs learn from new inputs in a few-shot manner."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   LLMs for Dialogue Knowledge Plug-and-Play",
            "text": "LLM-based response generation task can be considered an extreme scenario of zero-shot prediction, where no in-domain training is conducted at all. Compared to the fine-tuned models, which have the opportunity to learn the speaking style and statistical distribution of the conversationalists, the Vicuna-generated zero-shot responses are significantly longer: mean token length of 42.5 vs. 21.3 from the BART-based model on the Ms.WoW full knowledge test set; the mean target response length is 24.5 tokens. The Vicuna-generated outputs have less overlap with the corresponding gold responses, but a higher unigram overlap with the input knowledge in the full-knowledge setting (Table 11  ###reference_###), indicating that the LLM is able to generate utterances relevant to the input knowledge when sufficient knowledge is given. Surprisingly, when provided with gold knowledge only, the LLM seems not to use the provided knowledge to the same extent, and we see a decrease in performance, in contrast with the BART-based model that improved with gold knowledge. However, similar to the BART-based model, we still observe that more knowledge sources provided at test time significantly improves the LLM’s response generation performance across all metrics."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "We introduce a dataset of multi-source support knowledge for open-domain dialogue generation, with knowledge tuples partitioned into disjoint sources and grounded at the utterance level. We further introduce the dialogue knowledge plug-and-play challenge, where a trained dialogue system must adapt to a new knowledge source at test time. Our baseline experiments demonstrate how future works can use this dataset to study how dialogue models generalize to new knowledge sources."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}