{
    "title": "HarmPot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text",
    "abstract": "In this paper, we discuss the development of an annotation schema to build datasets for evaluating the offline harm potential of social media texts. We define “harm potential” as the potential for an online public post to cause real-world physical harm (i.e., violence). Understanding that real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu, to result in targeted physical violence, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, our understanding of the intersectional causes of such triggers focuses our attempt at measuring the harm potential of online content, irrespective of whether it is hateful or not. In this paper, we discuss the development of a framework/annotation schema that allows annotating the data with different aspects of the text including its socio-political grounding and intent of the speaker (as expressed through mood and modality) that together contribute to it being a trigger for offline harm. \n\nKeywords: Offline Harm, Harm Potential, Tagset, HarmPot",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Background and Rationale",
            "text": "India is a country with a rapidly proliferating social media presence with over 700 million users (including 81% of teens). However, despite massive levels of social media usage, digital media literacy remains low in India. A 2020 survey of a “highly educated online sample” of Indians found that roughly 50% of the fake news presented to them was judged as “accurate” or “very accurate” in their control group (Guess et al., 2020). The wide reach of social media content, the high prevalence of false or misleading information online, and the extreme communalism/groupthink on social media have exacerbated long-standing social divisions in India, coupled with low levels of media discernment skills (Froerer, 2019; Banerjee and Ghosh, 2018). \n\nIndia has now become a hotbed for online content spurring real-world physical violence. Online rumours and hate speech leading to physical violence against targeted communities and the subsequent filming of lynching is no longer uncommon. In 2018, for example, rumours and accusations of certain individuals being child-lifters, primarily spread on social media, led to several instances of mob killings. Online hate has compounded pre-existing lines of oppression, incentivizing the publicizing of violence against targeted groups for the sake of gaining public recognition and even praise. Several incidents of lynching have been triggered by misinformation around caste (Staff, 2020; Sajlan, 2021), love-jihad (Muslim men eloping with Hindu women) (NewIndianXpress, 2018), religious desecration, etc. There are additional contextual triggers that often cause increased levels of online content and subsequent real-world harm. This includes elections (Deka, 2019), where fake news, rumours, misleading, and divisive content are typically spiked for political gains, and global crises like the COVID-19 pandemic (Al-Zaman, 2021), which create a context in which users want “someone to blame,” often unjustly.\n\nIn the last few years, over 60 datasets of various sizes and kinds, where a wide variety of abusive language have been annotated, have been released publicly (Vidgen and Derczynski, 2020; Poletto et al., 2021). Existing tools such as Hatebase.org, or the Twitter-backed Hate-Lab or a host of other recent studies have focused on identifying abusive language (Nobata et al., 2016; Waseem et al., 2017), toxic language (Kolhatkar et al., 2020; Kaggle, 2020), aggressive language (Haddad et al., 2019; Kumar et al., 2018b; Bhattacharya et al., 2020), offensive language (Chen et al., 2012; Mubarak et al., 2017; Nascimento et al., 2019; de Pelle and Moreira, 2016; Schäfer and Burtenshaw, 2019; Zampieri et al., 2019a, 2019b, 2020; Kumar et al., 2021; Steinberger et al., 2017), hate speech (several including Akhtar et al., 2019; Albadi et al., 2018; Alfina et al., 2017; Bohra et al., 2018; Davidson et al., 2017; Malmasi and Zampieri, 2017; Schmidt and Wiegand, 2017; Del Vigna et al., 2017; Fernquist et al., 2019; Ishmam and Sharmin, 2019; Sanguinetti et al., 2018), threatening language (Hammer, 2017), or narrower, more specific dimensions such as sexism (Waseem, 2016; Waseem and Hovy, 2016)), misogyny, Islamophobia (Chung et al., 2019; Vidgen and Yasseri, 2020), and homophobia (Akhtar et al., 2019). Some datasets include a combination of these such as hate speech and offensive language (Martins et al., 2018; Mathur et al., 2018)), or sexism and aggressive language (Bhattacharya et al., 2020). However, while most of these datasets and frameworks aim to model whether hate or offensive speech has been used or not, there has been no dataset or framework that could directly model the relationship and interdependence of online content and offline incidents of harm and violence. \n\nIn this paper, we discuss the development of a framework - HarmPot - that could be used for annotating the text with textual and contextual information such that the annotated dataset could be used for training models that could predict the offline harm potential of online content. In the following"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   The HarmPot Framework",
            "text": "“Harm Potential” (HarmPot) could be defined as the potential for an online public post to cause offline, real-world physical harm (i.e., violence). Targeted real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu. As such, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, we focus on marking the harm potential of online content within a specific set of intersectional, contextual factors, irrespective of whether it is hateful or not. The HarmPot framework is designed with the aim of answering the following set of questions with respect to a given text: Who is being talked to, when, how, why, and all this results in what magnitude of harm potential for the addressee? Each of these questions is answered by using a set of parameters, defined in our tagset. We discuss each of these in the following subsections.\n\nA text will be marked as having ‘0’ harm potential in the following cases:\n\n- Texts which are a part of the dataset but do not actually relate to any specific incident of violence or larger narrative campaign.\n- Texts which are blurbs accompanying links to news reports.\n- Texts that criticise public figures and not protected identities.\n\nA text will be marked as having ‘1’ harm potential if it is likely to lead to offline harm in very few, specific contexts but more generally is not expected to trigger incidents of offline harm. The most stereotypical instances of such texts include:\n\n- Texts that target communities by using slurs and pejorative terms.\n- Texts that reinforce negative stereotypes regarding a particular community.\n\nA text that is likely to trigger offline harm in most of the contexts - it is only in very specific contexts that it may not be interpreted as a call to violence - is marked as ‘2’ on the harm potential scale. Some of the most stereotypical instances of such texts include:\n\n- Explicit cases of attack or accusations against communities.\n- Justifying violence or discrimination against communities.\n\nAny text that has a high potential of triggering offline harm, irrespective of the context that it occurs in is marked as ‘3’ on the harm potential scale. Instances of such texts include:\n\n- Explicit and clear calls to violence against communities or people.\n- Explicit and clear attempts to instigate violence against communities or people.\n\nThe magnitude of harm potential is marked at two levels:\n\n- Text Span: It is marked in conjunction with specific spans of text that are used to refer to specific identities. It refers to the potential of that specific span of text to trigger offline harm/violence against specific identities.\n- Document: It is the overall harm potential of the document - generally it is calculated based on the harm potential of individual spans; however, in cases where none of the spans refers to specific identities then an overall harm potential of the document is independently ascertained."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Magnitude of Harm Potential",
            "text": "Depending on what kind of offline harm the text could lead to, we define two broad kinds of harm potential -\n\nPhysical Harm Potential: It defines the potential of a text to lead to acts of physical violence such as murder, mob lynching, thrashing, and beating.\n\nSexual Harm Potential: It defines the potential of a text to lead to acts of sexual violence such as rape (or rape threats), molestation, and sexual harassment.\n\nBoth of these harm potentials are classified on a scale of 0 - 3, defined below.\n\nA text will be marked as having ‘0’ harm potential in the following cases:\n- Texts which are a part of the dataset but do not actually relate to any specific incident of violence or larger narrative campaign.\n- Texts which are blurbs accompanying links to news reports.\n- Texts that criticise public figures and not protected identities.\n\nA text will be marked as having ‘1’ harm potential if it is likely to lead to offline harm in very few, specific contexts but more generally is not expected to trigger incidents of offline harm. The most stereotypical instances of such texts include:\n- Texts that target communities by using slurs and pejorative terms.\n- Texts that reinforce negative stereotypes regarding a particular community.\n\nA text that is likely to trigger offline harm in most of the contexts - it is only in very specific contexts that it may not be interpreted as a call to violence - is marked as ‘2’ on the harm potential scale. Some of the most stereotypical instances of such texts include:\n- Explicit cases of attack or accusations against communities.\n- Justifying violence or discrimination against communities.\n\nAny text that has a high potential of triggering offline harm, irrespective of the context that it occurs in is marked as ‘3’ on the harm potential scale. Instances of such texts include:\n- Explicit and clear calls to violence against communities or people.\n- Explicit and clear attempts to instigate violence against communities or people.\n\nThe magnitude of harm potential is marked at two levels -\n- Text Span: It is marked in conjunction with specific spans of text that are used to refer to specific identities. It refers to the potential of that specific span of text to trigger offline harm/violence against specific identities (refer to Section 2.2 for details).\n- Document: It is the overall harm potential of the document - generally, it is calculated based on the harm potential of individual spans; however, in cases where none of the spans refers to specific identities, then an overall harm potential of the document is independently ascertained."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Who is being talked to?",
            "text": "This parameter is used to identify the specific types of identities that are ‘mentioned/referred’ (and not necessarily targeted) in a particular ‘span of text’. We discuss the various ontological types of identities that can be potentially targeted in a text. Since this parameter works with the magnitude of harm potential, a text span which simply mentions an identity without targeting it will have ‘0’ harm potential.\n\nThere are three broad annotation instructions for this parameter:\n\nIntersectionality: If more than one identity of the same individual is referred to (e.g., Female Dalit or Pakistani Hindu), then the same span is marked with all the identities and the same harm potential is ascribed in all instances. If different identities are mentioned in different spans, then also different spans will carry the same harm potential, considering it an instance of intersectionality.\n\nMultiple Identities: If different identities of different individuals are referred to, they might have different harm potential.\n\nMultiple Spans: If more than one span refers to the same identity of the same or different persons, each span could potentially have different harm potentials.\n\nThe framework itself does not enforce a specific set of identities to be marked. However, for the current project, the following non-exhaustive set of identities have been marked in the dataset. If needed, more, fewer or different kinds of specific subtypes of these categories may also be marked in the text. For each identity and its sub-category, a set of additional guidelines was used for deciding whether its harm potential is ‘0’ or not. If it’s not ‘0’, then the guidelines for marking the magnitude of harm potential are to be used.\n\nCaste: A span is annotated as targeting this identity if there are threats of violence, justification for caste-based discrimination, justification and support for untouchability, and criticism of reservation (affirmative action) policy that questions the intellectual capability of these groups.\n\nReligion: A span is annotated as targeting a member of a religious community if it calls for or justifies violence against them. Propounding or justifying conventional stereotypes associated with the members of such communities or using religious slurs will also have a harm potential greater than ‘0’. For example, Muslims being called terrorists or jihadis, Muslims and Christians being targeted for alleged forced conversions, and Sikhs being called Khalistanis or secessionists.\n\nDescent: For our specific case, descent encompasses all identities based on inherited status. This includes ethnicity, race, and place of origin (including linguistic or cultural minorities) of a victim (but not caste given its prevalence in the Indian context). Spans supporting or justifying attacks based on places of origin are annotated under this category.\n\nGender: This label annotates spans attacking gender minorities (LGBTQIA+ community) and women. Spans propounding or justifying conventional stereotypes or using gendered slurs are also marked with non-zero harm potential.\n\nPolitical Ideology: Political violence, including murder, lynching, thrashing, etc., of opposing party members or people of different political ideologies happens regularly. Spans calling for or justifying violence, supporting discrimination, or furthering stereotypes against the supporters of a political party or ideology are assigned harm potential greater than ‘0’. However, criticism of the political ideologies, political leaders, policies, etc., are assigned a ‘0’ harm potential."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   When is the discourse happening?",
            "text": "This parameter indicates if a text is posted online in relation to or during a major, public event or happening that might add to its harm potential. The harm potential of the content may increase when posted during or before such sensitive occasions and may lead to real-world violence in the form of mob lynchings and even ethnic cleansing. The major categories that we marked under this parameter are discussed below. This parameter is marked at the text/document level and the same text could take multiple labels. Since generally the dates of the events are already well-known, these labels could be mostly assigned automatically and could be seen as a grouping of multiple dates in a single category.\n\nRiots: In general, violent public disorders are referred to as riots. In India, in the past few years, hate speeches in social media have made a significant contribution to the amplification of violence during the riots. As such posts related to riots at the time of riots (or otherwise) are likely to have higher harm potential than otherwise.\n\nElections: Elections in India often see violence by supporters of rival political parties, and they are adopted in various themes such as communalism, terrorism allegations, anti-national, systemized threats and disruption of harmony.\n\nPandemic\n\nExtremist Attack: An extremist attack on state forces or the public may also lead to online hate against particular communities. The Pulwama suicide attack of February 2019 in India led to widespread hate speech and real-world violence against Kashmiri Muslims throughout India.\n\nFestivals: Religious festivals have recently become flashpoints for communal violence with different sides accusing each other of attacking processions or interfering with rituals. Online hate and disinformation often spikes during these situations.\n\nGroup-Specific State Decisions: This context pertains to when the government introduces or implements legislation/decisions affecting a particular community. The government’s decisions may be criticized or protested against by the community followed by online and offline attacks by the government’s supporters. Recent examples in India have been the Citizenship Amendment Act, Farm Laws and the abrogation of Article 370.\n\nGeneric: These refer to the posts related to the incidents that are recurring in nature (like the previous factors) but generally do not have a fixed or predetermined start or end time (viz. mob lynching on the suspicion of being child-lifters or those related to cow vigilantism in India).\n\nOthers: The posts that do not co-occur with any of the above-mentioned contextual factors - seemingly one-off incidents of hate and violence at no specific time - are marked as others."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "2.4.   How is it being said?",
            "text": "Since we focus on the harm potential of social media content, the methodology developed here is sensitive to the fact that the core objects of study are linguistic events themselves. It is essential to model the textual features, including lexical, syntactic, and semantic properties, alongside the contextual features. For the current project, we have defined a set of semantic features, specifically mood and modality, and lexical features such as affective expressions, which are marked in the text. Morphosyntactic features can be marked automatically using existing systems or implicitly learned by modern transformers-based multilingual models, so we have not marked those separately. The labels for this parameter are marked at the span level and generally overlap with the spans of the ‘who’ parameter.\n\nThere is an abstract link between the language a speaker uses to convey harm and how that language is structured to reflect the speaker’s intentions and evaluation of what they say as possible or necessary. Variation in the speaker’s intention and their evaluation of what they say can significantly impact the harm potential. These can be modeled using the linguistic categories of mood and modality.\n\nWe discuss the different subtypes of these two categories used for annotation below:\n\n**Mood Type:** Mood is a grammatical reflection of the speaker’s purpose in speaking or an indication of what the speaker wants to do with the proposition in a particular discourse context. The grammatical form changes depending on whether the speaker discusses a situation that has or will actualize or an event that has not. We annotate three broad kinds of mood types:\n\n- **Realis Mood:** Portrays situations as actualized, knowable through direct perception. Indicative mood usually embodies the realis mood.\n\n- **Irrealis Mood:** Portrays situations as within the realm of thought, knowable only through imagination. It denotes actions not known to have happened, including modality-marked constructions, conditionals, counterfactuals, optatives, hortatives, and subjunctives.\n\n- **Neither:** Includes imperatives, interrogatives, future-tense marked constructions, and negative constructions.\n\n**Illocutionary Mood:** This draws on the idea of illocution acts, encoding speaker intention as a category of mood. Certain illocutionary acts map onto specific grammatical forms. We use these subtypes:\n\n- **Declaratives:** Can be ‘direct’ or ‘indirect,’ such as rhetorical questions, which can assert something indirectly.\n\n- **Interrogatives**\n\n- **Imperatives:** Commands, requests, advice, pleas, permissions, offers, or invitations.\n\n- **Admonitive:** Warnings issued to the addressees.\n\n- **Prohibitive:** Curtail the addressee’s actions.\n\n- **Hortative:** Softened commands or exhortations, often using first-person inclusive reference.\n\n- **Optative:** Express a wish or desire for a situation to occur.\n\n- **Imprecative:** Indicates a wish for an unfavorable proposition.\n\n- **Exclamative**\n\n**Modality:** Modality modifies a state of affairs concerning how the basic event is construed by the speaker, indicating if the event is ‘possible’ or ‘necessary.’ We use these modalities:\n\n- **Epistemic:** Concerns the speaker’s estimation of the likelihood of the state of affairs being true.\n\n- **Deontic:** Indicates the degree of moral desirability of the state of affairs, including societal norms and personal ethics.\n\n- **Dynamic:** Relates to the ability or necessity concerning participants, external circumstances, or inherent state of affairs.\n\n- **Teleological:** Concerns means that are possible or necessary for achieving a goal.\n\nIn addition to these, affective expressions are marked if a word conveys the speaker’s evaluative attitude or emotional state towards the information conveyed by the sentence."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "2.5.   Why is it being said?",
            "text": "This parameter analyses the discursive role of the text placed within its context and checks for the reason or rationale behind posting the text. It is a direct induction of the ‘discursive roles’ by Kumar et al. (2022a  ###reference_b66###) in this framework. We discuss the five categories and their relationship to the magnitude of harm potential here -\n\nAttack: This label is used when any comment/post poses an attack on any individual or group based on any of their identities. Not all attacks are accompanied by a positive harm potential. For example, criticisms, which are not likely to trigger real-world harm against them are tagged as ‘attack’ with harm potential ‘0’.\n\nDefend: This label is used when any comment/post defends or counter-attacks a previous comment/post. Again not all instances of defend have ‘0’ harm potential - in instances where the defense of the perceived ‘victim’ has the possibility of triggering real-world harm against the attacker, they are marked with non-zero harm potential.\n\nAbet: This label is used when any comment/post lends support and/or encourages an aggressive act which has a harm potential.\n\nInstigate: This label is used when any comment/post encourages someone to perform an aggressive act. The comment itself may or may not be aggressive but the purpose must be to instigate an act that is potentially harmful in the real world. Instigation happens before the event and its purpose is to trigger or provoke a harmful act unlike abet which occurs during or after the harmful act and its purpose is to praise, support, and/or encourage that act as well as other such acts in the future.\n\nCounterspeech: Texts that diffuse the potentially harmful situation will be tagged as counterspeech. Just as influential speakers can make violence seem acceptable and necessary, they can also favourably influence discourse through counterspeech."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Data Collection and Annotation",
            "text": "The framework discussed in Section 2 is developed over several stages and iterations. In order to test the reliability and validity of the framework, we collected a dataset from different social media platforms and annotated those using the framework. As a first step towards data collection, we focused on a few incidents of physical harm (riots, lynchings, etc.) that had a link to online disinformation and hate campaigns from 2016 – 2022. This time period was decided given the introduction of low-cost data and smartphones by Reliance Jio in 2016, which led to a manifold increase in per-capita data usage. Finding relevant government-published data related to hate crimes was a challenge as the Indian government stopped collecting data on hate crimes in 2017. Therefore, we decided to use databases from non-governmental organizations like Documentation of the Oppressed (DOTO). This database consisted of a list of over 1,100 incidents of offline hate crimes and violence since 2016. Out of these, we sampled a little over 150 crimes since they had a link to social media discourse. We extracted social media data related to these incidents from different social media platforms viz Twitter, YouTube, Facebook, Telegram, and WhatsApp. We also ensured that we collected data both from before and after the incident separately. This approach ensured that we got data that had links to offline harm incidents so they could be considered as potential triggers for the offline harm incident; at the same time, we also got data that might be triggers for a related post-event incident. We collected a total of over 417,000 data points in Hindi and English using this methodology. The complete dataset, along with the hate crimes they were associated with, will be released publicly for further research."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Inter-Annotator Agreement",
            "text": "Approximately 5 - 10 data points were selected from approximately 50 incidents for running the inter-annotator agreement experiments. Each of these was annotated by 3 annotators and Krippendorff’s Kappa was calculated for the magnitude of harm potential. The first round of experiments with around 500 data points gave a rather dismal Kappa of 0.25. Following this, we made certain changes to the tagset such as merging different categories to reduce overlap across categories (for example, race, ethnicity and nationality under ‘Who’ were combined into a single category of ‘Descent’) and introducing new categories to better classify different kinds of categories (for example, several new categories under mood and modality were added for a better analysis). We also made changes to the annotation guidelines for clarity. These changes led to significant improvements in the alpha - the second round of experiments gave a final value of 0.53.\n\nWhile the Kappa value still remained low, for a highly subjective task such as predicting the magnitude of harm potential as reasonably good. We started conducting focus group discussions to understand the reason behind disagreements. As it has been argued earlier as well (for example, Kumar et al. (2022b  ###reference_b67###) and also the Perspectivist Data Manifesto 444http://pdai.info/  ###reference_dai.info/###), most of these disagreements seemed reasonable. As such we decided not to push for further agreement - instead, we will be making the disaggregated annotations by different annotators publicly available."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Data Annotation",
            "text": "We have annotated a total dataset of approximately 2,000 data points (taking around 15 - 20 data points from over 100 incidents) to demonstrate the validity of the presented framework. We made use of an online app - LiFE App (Singh et al., 2022) - for data annotation since it allowed us to annotate the data simultaneously at the document and the span level. Each data point was annotated by 3 annotators working independently."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   HarmPot and Other Frameworks: A Comparison",
            "text": "As we mentioned earlier, most of the existing frameworks attempt to only model hateful, aggressive, offensive (or one of the other similar flavours) speech but do not attempt to predict the potential of the text to trigger offline harm incidents. However, such language usage is expected to have some correlation with offline harm. Moreover, prior studies have also pointed out the need to flesh out the interrelationship between different frameworks so as to ensure interoperability and cross-use of datasets annotated with different hate speech frameworks (Poletto et al., 2021; Kumar et al., 2022b). In order to understand this relationship, we carried out a comparative study between our framework and three of the other popular frameworks. We took 500 texts annotated with each of these different frameworks, annotated those with the HarmPot framework and carried out a comparative study. The results of these are discussed in the following subsections."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   HarmPot and ComMA",
            "text": "The top level of the framework distinguishes between overtly, covertly, and non-aggressive texts. At the second level, the aggression intensity of the aggressive texts—physical threat, sexual threat, non-threatening aggression, and curse/abuse—are marked. Additionally, bias and threats of four kinds—religious, caste/class, gender, and racial/ethnic—are marked. It also marks the discursive roles—attack, defend, counterspeech, abet and instigate, and gaslighting—of the text. These discursive roles are incorporated in the HarmPot framework. Since social or physical ‘harm’ is inherent to the idea of aggression, we expected a good mapping between the notion of verbal aggression and the harm potential of a text. \n\nThe study showed that most of the non-aggressive texts (NAG) are at Level 0, but the vice-versa is not necessarily true. Also, most of the ‘covertly aggressive’ (CAG) texts are categorized with level ‘1’ harm potential. At the second level, physical and sexual threats were mostly marked as having ‘2’ or ‘3’ harm potential, while non-threatening aggression is mostly marked as ‘1’. Some of the curse/abuse texts were also marked with ‘0’ harm potential. At the level of threat and bias, even though religious, caste, and gender bias have direct parallels in HarmPot since we are marking all mentions of these identities and not just biased or threatening ones, the instances of such spans were higher in our case. However, threats generally carried a harm potential of ‘2’ or ‘3’ while bias carried a harm potential of ‘1’ or ‘2’. Some comments carried a harm potential of ‘1’ or even ‘2’. Moreover, the dataset marks the biases at the document level while HarmPot marks these at the span level."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this paper, we have presented a new framework that could be used for annotating social media text with its potential for triggering offline harm. The framework incorporates contextual information such as the identity of the victim (as mentioned/referred to in the text), the broad socio-political situation in which the post is situated, and the role that the text assumes in the discourse. We have also proposed using mood and modality as relevant categories for marking the speaker’s intention, intended goal, and their own evaluation of whether what they are saying is ‘necessary’ or ‘possible’. These semantic categories have been rarely utilised in NLP but they could prove to be extremely useful in the identification of subjective phenomena like harm potential. We have annotated a total dataset of 4,000 texts - 2,000 related to the possible triggers of offline harm incidents and another 2,000 from datasets available for aggressive and hateful language identification. We are currently annotating some more data and also conducting experiments for the automatic identification of harm potential to understand the practical efficacy of the framework."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Ethical Considerations",
            "text": "The nature of the task - the creation of datasets with high harm potential and its annotation - in itself raises several ethical issues of bias and psychological impact on the annotators working with the data. In order to reduce the impact of working with such data, we took 3 steps - (a) a ‘maximum’ limit of 200 texts per week was set for the annotators - the annotators were barred from going through more than this number of texts in a week; (b) we had made arrangements for psychological counselling of the annotators working on the data; (c) a compulsory weekly ‘venting out’ meeting was organised to enable annotators to talk to each other and other members of the project that allowed them to talk about, discuss and (hopefully) figure out the ridiculousness of the data that they were going through. We made a very conscious decision not to use crowdsourcing or even third-party annotators for data annotation and collection so as to ensure that these mechanisms are put in place.\nIn order to minimise the bias in the annotations and also make different perspectives on the data public, we have decided to release the disaggregated dataset with the annotations of all the annotators (with their disagreements). We were very conscious not to push for an agreement where it was not possible. Moreover, our in-house annotators were from mutually distinct socio-political, religious, cultural, and educational backgrounds, providing an innate cancelling out of any one type of bias overpowering the data analysis and interpretation - we have tried to annotate the data in such a way as to reflect different perspectives on the data (and not propound a single, homogeneous view)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Limitations",
            "text": "One of the primary limitations of the framework and the dataset is the lack of multimodal information being included in it. A large number of hateful and abusive language used on social media, with a high potential for harm, is expected to be accompanied by visuals including images and video. We are working on expanding the dataset to include multimodal data and see how well the framework adapts to that and also what kind of modifications would be needed for handling those cases. The second limitation is the pipeline-based workflow that the framework enforces, which has a greater chance of error propagation - if, for example, the system makes an error in recognising mood and modality, that might ultimately lead to an error in the prediction of harm potential itself. This is a general limitation of the hierarchical frameworks."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ]
}