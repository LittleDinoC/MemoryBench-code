{
    "title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence, thanks to their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements of LLMs limit their widespread adoption. Quantization, a key compression technique, offers a viable solution to mitigate these demands by compressing and accelerating LLMs, albeit with potential risks to model accuracy. Numerous studies have aimed to minimize the accuracy loss associated with quantization. However, the quantization configurations in these studies vary and may not be optimized for hardware compatibility. In this paper, we focus on identifying the most effective practices for quantizing LLMs, with the goal of balancing performance with computational efficiency. For a fair analysis, we develop a quantization toolkit LLMC, and design four crucial principles considering the inference efficiency, quantized accuracy, calibration cost, and modularization. By benchmarking on various models and datasets with over 500 experiments, three takeaways corresponding to calibration data, quantization algorithm, and quantization schemes are derived. Finally, a best practice of LLM PTQ pipeline is constructed. All the benchmark results and the toolkit can be found at https://github.com/ModelTC/llmc.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large Language models (LLMs) such as GPT-4 have demonstrated unprecedented generative capabilities in the field of natural language processing (NLP), achieving widespread applications across various industries. However, their substantial computational and storage costs have impeded their further popularization among users. For instance, BLOOM, an open-access multilingual LLM with 176 billion parameters, requires a minimum of 350 GB of space merely to store model weights in full-precision (FP16) format. At a minimum, it requires 580GB A100 or 940GB A800 NVIDIA GPUs to perform inference with this model. Therefore, reducing their serving cost is paramount to further enhance the application of LLMs.\n\nFor the aforementioned challenge, model quantization can be an effective resolution strategy. It maps weights and/or activations to a lower-bit data format to reduce memory footprints and accelerate model inference. Existing quantization approaches can be categorized into two types: quantization-aware-training (QAT) and post-training quantization (PTQ). Although with prominent high performance, the necessity for QAT to undergo finetuning or retraining with substantial training data and training cost renders it unattainable for the majority of users. Correspondingly, PTQ compresses models without retraining, making it a preferred method for LLMs due to its minimal resource requirements.\n\nCurrent uniform PTQ methods always evaluate across distinct datasets in different quantization configurations and with simulated quantization. This current state would lead to users being unable to accurately assess the configurations that should be selected for the efficient and accurate quantization of LLMs. To provide a comprehensive quantization options menu for users to obtain hardware-friendly quantized LLMs with high performance, we make a fair benchmark, which considers two aspects: factors influencing LLM quantization and inference efficiency under our design principles. The former perspective encompassed three dimensions, e.g., calibration data, algorithm, and target bits. Consequently, we evaluate across various kinds of tasks and find our best practice, encapsulated within an end-to-end pipeline that realizes both high efficiency and accuracy LLM quantization. This best practice has been integrated into our quantization toolkit, LLMC. Notably, LLMC, a user-friendly, plug-and-play quantization tool, incorporates dozens of outstanding PTQ algorithms, provides the freedom to select quantization strategies, and also supports deploying quantized LLMs on different inference backends and hardware.\n\nIn a word, our main contributions can be described as follows:\nWe release a quantization toolkit LLMC supporting dozens of algorithms, models, and hardware. LLMC enables users to perform lossless quantization on 100-billion-parameter LLMs within a matter of hours, utilizing just a single GPU. It notably facilitates the research and production of quantized LLMs.\nWe modularly and fairly benchmark the quantization techniques considering calibration cost, inference efficiency, quantized accuracy. Near 600 experiments on diverse models and datasets provide three insightful takeaways on the calibration data, algorithm pipeline, and quantization configuration selection.\nBased on the takeaways, a best practice of LLM PTQ pipeline is designed, achieving the best accuracy and efficiency performance balance under various scenarios."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Benchmark Overview",
            "text": "In this section, we first provide our benchmark’s design principles, outlining its primary objective. We then detail LLM quantization. After introducing the preliminary of quantization, we overview our exploration in the benchmark, such as factors influencing LLM quantization and inference efficiency. Finally, we exhibit our plug-and-play quantization toolkit within our benchmark."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Design Principles",
            "text": "Our benchmark focuses on four essential aspects for effective and practical LLM quantization: inference performance, calibration cost, quantized accuracy, and modularization.\n\nInference Performance: In our LLM quantization benchmark, we prioritize the importance of selecting a quantization approach that enhances inference performance. This means our chosen setting should either increase throughput or decrease memory requirements, thereby optimizing the efficiency of the model during the inference phase.\n\nCalibration Cost: The process of post-training quantization for LLMs, also known as calibration, involves the resources and time invested, which are crucial factors affecting the practicality of LLM quantization. This benchmark aims to find the best pipeline to produce accurate LLMs in minimal GPUs and time.\n\nQuantized Accuracy: In every method used to create quantized models, it’s crucial to minimize any reduction in accuracy to a tolerable degree. With this fundamental principle in mind, we are dedicated to exploring strategies that reliably preserve the performance of the model within acceptable limits.\n\nModularization: Recent advancements have introduced numerous algorithms aimed at enhancing the performance of quantized LLMs. This benchmark seeks to dissect these algorithms to their most fundamental elements, analyzing the efficacy of each component in isolation.\n\nGuided by these four principles, our goal is to investigate and outline optimal practices for developing quantized LLMs tailored to various scenarios and configurations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM Quantization",
            "text": "Preliminary of Quantization. For an element in a vector to be quantized, the process of quantization can be defined as:\nwhere and are the upper bound and the lower bound of the vector. is the bit-width of the quantized vector, and is the quantized -bit element. If we force , the process can be called symmetric quantization. Otherwise, it is called asymmetric quantization. In this paper, we mainly consider asymmetric quantization. Besides that, in weight-only quantization, we employ per-group quantization, meaning the weights in a group share the same . In weight-activation quantization, we apply per-channel and per-token quantization for weights and activations, respectively. In this paper, the notion “wxay” is employed to represent the bit-widths “x” of weights, and the bit-widths “y” of activations. “gz” means in group-wise quantization the group size is “z.”\n\nFactors Influencing LLM Quantization. We categorize factors influencing LLM quantization into three dimensions: calibration data, algorithms, and target bits.\n\nCalibration data: Calibration data can help to evaluate the range of tensors and then determine the quantization parameters, which is crucial for maintaining model performance post-quantization. Based on that, the impact of different corpora as calibration data warrants further investigation.\n\nTarget bits: The bit adopted for weight, activation, and KV cache impacts the final accuracy. Usually, the hardware-friendly bits are 2-bit, 4-bit, and 8-bit. In this benchmark, we also investigate 3-bit or 6-bit to compare the potential of quantization algorithms. But for the practical deployment, 2/4/8-bit is mainly used.\n\nQuantized inference of LLM. As shown in Figure 1, the quantization mainly targets the Linear layers with weights, i.e., the Q, K, V, and O layers in self-attention modules and the Up, Gate, and Down layer in FFN modules. Figure 1 presents 3 types of quantization including weight-activation quantization, weight-only quantization, and KV-cache quantization. They bring different benefits for reducing the prefill and decode latency."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Quantization Toolkit",
            "text": "To achieve the modular comparison of the different quantization dimensions aforementioned, and to consolidate best practices into an end-to-end pipeline, we have designed and developed a quantization toolkit named LLMC. This toolkit is capable of accommodating multiple quantization configurations using a variety of algorithmic techniques. The models produced by LLMC are designed for seamless deployment across a diverse range of hardware platforms. Presently, LLMC supports over ten algorithms, is compatible with over eight models, is flexible to extend the support of any transformer-based LLMs, and facilitates deployment on three types of inference engines including LightLLM (ModelTC, 2023), TensorRT-LLM (Nvidia, 2023) and PPL-LLM (OpenPPL, 2023)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM-QBench",
            "text": "Under the principles in subsection 2.1 ###reference_###, powered by our quantization toolkit LLMC, in this section, we explore the best practice for quantizing large language models from the aspect of calibration data, quantization algorithm, and target bit."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "We first illustrate our experiment settings, more details can be found in the subsection A.1.\n\nModels. To demonstrate the generability of our benchmark, we access performance on LLAMA-2 family, spanning model sizes from 7B to 70B for general language tasks. To broaden the scope of our evaluation benchmarks, we also benchmark on ChatGLM for long context abilities, CodeLLAMA for coding tasks and WizardMath for mathematical problems.\n\nDatasets. We categorize the datasets into upstream datasets and downstream datasets. For the upstream datasets, we employ WikiText2 and C4 dataset with the perplexity metric for evaluation, since perplexity can stably reflect the LLM’s performance. For the downstream tasks, we select examination tasks including MMLU and ARC-e, knowledge task BoolQ, understanding task Lambada, reasoning tasks including PIQA, HellaSwag and GSM8K, coding tasks HumanEval and MBPP, and the long context evaluation LongBench.\n\nHardware. Benefiting from the versatility of our tool, we can efficiently and conveniently quantize LLMs to support multiple inference backends and hardware platforms. In this paper, we mainly measured the inference efficiency of low-bit kernel on NVIDIA server and edge GPUs with NVIDIA’s TensorRT-LLM framework."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Impact of Calibration Data",
            "text": "Initially, we examine the influence of calibration data on the accuracy of quantization, as illustrated by Table 1. It is evident that calibration data affects all algorithms. To attain optimal accuracy, it is crucial to gather domain-specific data for domain-specific models and collect diverse data for the general models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Quantization Algorithm",
            "text": "Following the principles of modularization, we deconstruct the techniques behind existing algorithms. Through a comprehensive and unbiased experimental comparison, we aim to derive insights critical for developing an optimally combined quantization pipeline. We evaluate these techniques on LLAMA-2 models of 7B, 13B, and 70B sizes, under both weight-only and weight-activation quantization scenarios. Here the 2-bit weight-only experiment of 70B LLAMA-2 is chosen as a representative in the main text. More results are illustrated in subsection A.2  ###reference_###.\n\nCalibration cost for each strategy. In the analysis of calibration costs detailed in Table 4  ###reference_###, we observe that the search-based (v1) strategy requires roughly 10 minutes, making it twice as fast as the (v2) strategy. However, initializing the learning process with pre-searched values can halve the number of epochs required and yield higher accuracy. \n\nTS\n\nTR\nTS\nTS\nTL\nTL\nTL\n\nTS-v1\nTL\nTS-v1 + CL\nTL w/ TS-v1 init.\nRH\n\n-v1\n-v2\nw/ ones init.\nw/ TR init.\nw/ TS-v1 init.\n\nTime\nh\nh\nh\nh\nh\nh\n"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Target Bits",
            "text": "Fixed-precision. In the experimental results presented in subsection 3.3, we observed that both 2-bit weight-only quantization and w4a4 weight-activation quantization experienced over a 20% degradation in accuracy. This significant reduction in performance limits their practical utility. In contrast, 3-bit weight-only and w6a6 weight-activation quantization were primarily evaluated to assess algorithm capabilities and cannot achieve practical hardware acceleration. Consequently, we recommend the 4-bit weight-only, w4a8, or w8a8 weight-activation quantization approaches as they strike a balance between maintaining accuracy and enhancing inference speed. Furthermore, quantization of the Key-Value (KV) cache is proposed as a method to decrease memory usage. In Table 21 and Table 5, we assessed the accuracy impact of 2-bit (per-group quantization with a group size of 8), 4-bit (per-group quantization with a group size of 8), and 8-bit (per-tensor) KV cache quantization. The results indicate that 2-bit KV cache quantization leads to a substantial loss in accuracy, while 4-bit KV cache quantization, with its finer granularity, performs comparably to 8-bit KV cache quantization with a coarser group size. Both the 4-bit and 8-bit configurations closely approximate the performance of FP16 at the code generation task and long-context understanding task. Hence, for KV cache quantization, a 4-bit per-group approach with a group size of 8 is recommended.\n\nMixed-precision. As presented in our experiments, quantizing LLMs into ultra-low precision without significant accuracy loss is difficult. A viable remedy is to employ mix-precision quantization. For mix-precision, we only evaluate accuracy for theoretically hardware-friendly strategies since there are no open-access fast kernels to evaluate inference. As shown in Table 23, Table 23, and Table 24, for weight-only quantization, employing Hessain disturbance as bit allocate strategy outperforms others. High-bit quantization benefits from lower mixture rates, while low-bit requires more full-precision weights in small LLMs for better performance. For weight-activation quantization, dynamic bit allocation with slower inference speed and higher computational overhead during inference gains more accuracy improvements rather than static strategy, even though the latter uses a double mixture rate. Details are presented in the subsection A.6.\n\nInference Speed. To assess the practical benefits of different quantization approaches, we conducted evaluations using NVIDIA’s cloud (SMX 80G A100) and edge (Drive Orin) GPUs, alongside the official inference library, TensorRT-LLM. Part of our results, as depicted in Figure 2, highlight the throughput improvements achieved through TensorRT-LLM-supported quantization schemes for models with 32,000 input tokens and 512 output tokens. The findings indicate that quantization with 8-bit weights and activations enhances the prefill stage’s speed by 20%-30% and the decode stage by 40%-60%. In contrast, 4-bit weight-only quantization reduces the prefill speed by 10% but increases the decode speed by 40%-60%. It’s important to note that these acceleration rates tend to diminish for larger models. Besides, 8-bit KV cache quantization has minimal impact on prefill times and slightly reduces decoding throughput for very large models, such as those with 70B model. Results for more models and hardware can be found in subsection A.5."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Best Practice of LLM PTQ pipeline",
            "text": "Based on the takeaways distilled from the above exploration, we summarize the best practice of PTQ pipeline for LLM. As depicted in Figure 3, first, we should collect the best calibration data according to the task and model under the guide of Takeway 1. Then the bit-width and quantization scheme could be determined considering the Takeway 3. Finally, the calibration process can be conducted using the algorithm pipeline based on Takeway 2. The results in Table 6 and Table 7 of general-purpose model LLAMA-2-70B and specific-domain code model CodeLLAMA-7b and math model WizardMath-7b proved the effectiveness, especially for maintaining high accuracy. More experimental results on other models and datasets to validate our best practice for decent performance and efficient inference can be found in subsection A.3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have undertaken a comprehensive benchmarking of decomposed quantization techniques for large language models (LLMs), leading to the identification of best practices that balance calibration costs, accuracy, and efficiency. Furthermore, we introduce LLMC, a toolkit designed to empower the research and development community. Models optimized through our recommended practices and toolkit are readily deployable across a variety of hardware platforms, enhancing accessibility and applicability in diverse computational environments."
        }
    ]
}