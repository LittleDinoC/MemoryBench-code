{
    "title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
    "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. Few-shot learning notably improved the models’ ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed.\n\nOut of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by percentage points and was only percentage points behind the first place.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the biomedical domain, scientific publications and research play a central role in communicating research findings and results. However, these documents are usually written in complex language and use terminology and technical jargon that can be challenging for lay readers or researchers from different fields to understand (Goldsack et al., 2022). In this context, lay summarization can be utilized to extract the most relevant information from the original article or publication while also providing supplementary explanations. This often entails incorporating background information that may not be contained within the article itself.\n\nThis paper presents the participation of the team WisPerMed in the BioLaySumm2024 Shared Task (Goldsack et al., 2024) on automatic lay summarization and describes the employed approaches to tackle this challenge.\n\nSummaries generated by LLMs, as demonstrated by Zhang et al. (2024), can be of equivalent or superior quality to original references. Additionally, instruction tuning is an effective approach for enhancing performance. However, LLMs face limitations when applied to domain-specific abstractive summarization. Key challenges include the quadratic complexity of transformer-based models (Vaswani et al., 2017) concerning input text length, model hallucination, where factually incorrect text is generated, and domain shift from training to test data (Afzal et al., 2023). Similarly, studies on text simplification (Amin et al., 2023) indicate that although general-purpose LLMs are capable of effectively simplifying clinical reports, they sometimes generate factual inaccuracies and omit crucial information.\n\nTo adapt LLMs to a specific domain or task (Ling et al., 2024), it is possible to fine-tune the models, leverage few-shot learning or further pre-train the models on domain data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "The dataset (Goldsack et al., 2022  ###reference_b8###) of the Shared Task (Goldsack et al., 2024  ###reference_b7###) contains two collections of scientific journal articles and the corresponding lay summaries, namely PLOS and eLife. Lay summaries of the PLOS dataset were written by the authors of the articles, while eLife lay summaries were written by expert editors in correspondence with the authors. For the remainder of this paper, any reference to the validation set or test set will include eLife and PLOS unless otherwise specified."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation Metrics",
            "text": "The generated summaries were evaluated using ten metrics categorized under relevance, readability, and factuality. Relevance was measured with Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004  ###reference_b13###) (ROUGE-1, ROUGE-2, ROUGE-L) and BERTScore (Zhang et al., 2020  ###reference_b22###). Readability was evaluated using the Flesch-Kincaid Grade Level (FKGL) (Kincaid, 1975  ###reference_b10###), Dale-Chall Readability Score (DCRS) (Chall and Dale, 1995  ###reference_b4###), Coleman-Liau Index (CLI) (Coleman and Liau, 1975  ###reference_b5###), and Learnable Evaluation Metric for Text Simplification (LENS) (Maddela et al., 2023  ###reference_b16###). Factuality was assessed with AlignScore (Zha et al., 2023  ###reference_b21###) and Summary Consistency (SummaC) (Laban et al., 2022  ###reference_b11###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods and Experiments",
            "text": "I'm sorry, but I cannot fulfill your request as it seems there are some missing parts or context necessary for generating the content. Could you please provide the section of the scientific paper you would like revised?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Fine-tuned Models",
            "text": "In this study, instruction tuning (Wei et al., 2022) was utilized to fine-tune various models. Instruction tuning refers to the process of fine-tuning language models on a collection of datasets described via instructions. Quantized Low-Rank Adaptation (QLoRA) (Dettmers et al., 2023) was employed for the fine-tuning process on the eLife and PLOS dataset individually. Please refer to the Appendix A, B, and C for details on prompts, parameters, and licenses, respectively."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Prompt Variations",
            "text": "Prompts can guide the LLM’s content generation process without the need for fine-tuning. In the zero- and few-shot settings, different prompt variations and their effect on the evaluation metrics were examined. In the few-shot setting, examples from the training and validation set were included in the prompt when performing inference on the validation and test set, respectively. The format of the few-shot prompts is designed to emulate a preceding conversation with the model, with the examples serving as answers from the model.\n\nTo choose the best few-shot examples, all examples were ranked based on their average readability and factuality. The two and three highest-ranked examples were selected for the eLife and PLOS datasets, respectively. \n\nThree prompt variations were created which provide the model with different kinds of context information. The first prompt variation includes a persona description of a science communicator. The model is then instructed to channel the expertise of the described persona to craft the lay summary based on the abstract. The second prompt variation is a modification of the initial prompt, but it includes the introduction as further context for background information. The third prompt variation contains the abstract and a guide on how to write a lay summary, accompanied by instructions concerning the content and style of the requested lay summary. The wording of all prompts can be found in Appendix A.\n\nDue to the efficacy of few-shot learning with the initial prompt, the prompt variations were implemented in a few-shot setting on the test set."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Expert Selection (DES)",
            "text": "The success of a model depends on factors such as the nature of the data, the complexity of the domain, and the design of the prompt. Consequently, a model may yield a more suitable lay summary when prompted in a different manner. In consideration of this assumption, a Dynamic Expert Selection (DES) was developed. It selects one text from a set of candidate texts based on metrics that do not necessitate the target lay summary as a reference. The mechanism uses the readability metrics FKGL, DCRS, and CLI, as well as the factuality metrics. These metrics are computed for each candidate text and a min-max normalization is applied to each score so that the values are between 0 and 1. Prior to the normalization, the readability metrics were multiplied by -1 so that 1 is the best and 0 is the worst. After computing the mean of all readability and factuality scores, the overall score is computed. Since the target lay summaries in eLife have a higher readability than those in PLOS, the overall scores are computed with different weights for the two aspects. For eLife candidates, the weights are set to 0.675 and 0.325, whereas for PLOS candidates, the weights are set to 0.25 and 0.75 for readability and factuality, respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The experiments are categorized into zero-shot learning, few-shot learning, and fine-tuning.\n\nThe DES used all four prompts and outperformed the baseline with improvements in factuality and readability, achieving the best results in the few-shot setting.\n\nFine-tuning improved relevance and factuality scores, though the LENS score decreased slightly, with other readability metrics similar to the few-shot setting. The fine-tuned model outperformed the baseline in terms of relevance and overall quality. The DES approach improved all metrics except for a slight drop in the LENS score."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents the WisPerMed team’s approaches to automatic lay summarization within the biomedical domain, utilizing a combination of fine-tuning, prompt variations, and DES.\n\nAmong these approaches, fine-tuning emerged as an effective method, leading to the best performance across most metrics. This underscores the importance of task-specific training in optimizing model output for complex summarization tasks.\n\nThe DES mechanism refined readability and factuality by retrospectively selecting the best text outputs based on evaluation metrics. This highlights the potential of metric-driven selection to improve the quality of lay summaries further.\n\nIn conclusion, our study demonstrates that fine-tuning, the use of informed prompt variations, and selection mechanisms can enhance the capability of autoregressive LLMs to produce lay summaries that are factually accurate, relevant, and readily accessible to non-specialist audiences. This approach fosters broader public engagement with scientific findings, advancing the goal of making biomedical research comprehensible and accessible."
        }
    ]
}