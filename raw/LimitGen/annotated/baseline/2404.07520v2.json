{
    "title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination",
    "abstract": "The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning. The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 2.33% in overall performance, by 1% in base-to-novel generalization, and by 2.84% in cross-dataset transfer tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Training Vision-Language Models (VLMs) with large-scale image-text pairs is known for imparting robust generalization capabilities across diverse downstream tasks [31, 20, 44, 41, 42, 1]. However, training these models from scratch for each downstream task is very time-consuming. Moreover, the essence of pre-training with a large-scale dataset is lost when the pre-trained model is not generalizable across downstream tasks. This is due to unexpected changes in data distribution, and the sensitivity to these shifts leads to a decline in performance [16, 32, 30]. To tackle this, there exist three most commonly used techniques: fine-tuning [28], prompt tuning [47], adapter [13], and LoRA [19]. Among these, prompt tuning is the simple, recent, and most widely used technique for foundation models [47, 46, 21, 22, 43]. However, prompt learning/tuning approaches are used during the training phase to learn representative prompts based on the training data for the downstream task. This approach does not specifically address the distribution shift present in the dataset.\n\nIn this work, we demonstrate the multi-modal test-time adaptation of prompts. We propose class-aware prototype alignment to address the distributional shift on a class-wise basis. For instance, in an open world there are 360 different breeds of dogs compared to only 71 for cats, leading to one class having higher variance than the others. For each test sample, we obtain randomly augmented views (for both text and image) that are fed to the model for prompt tuning on both the textual and visual branches. We adapt the learnable prompt tokens by aligning the prototype for test sample and confident augmented views with the pre-computed class prototypes (obtained from the proxy source dataset) weighted by the mean probability of each class obtained from confident augmented views. Before alignment, we update the prompt tokens on both the text and visual branches using prototype discrimination and then use updated prompts to align the test sample and augmented views with class prototypes using mean class probabilities. This is based on the idea that prototype vector can capture the complete information of mean and variance for each class distribution and hence it mitigates the class collapse (during test time adaptation) due to high variance of particular classes. Empirical evaluation of our methods shows state-of-the-art Top-1 accuracy for three tasks: domain generalization, base-to-novel generalization, and cross-dataset transfer. This validates the effectiveness of our method in enhancing zero-shot generalization. Our contributions can be summarized as follows:\n\nWe propose a class-aware prototype alignment technique for individual test samples to align the context of each test sample with the source distribution on a class-wise basis, thereby mitigating the effects of distributional shift between classes.\n\nWe propose class-aware prototype discrimination to discover the class distribution for efficient alignment. Additionally, we propose the offline computation of class prototypes from a proxy source dataset for foundation V-L models.\n\nWe propose multi-modal test-time prompt tuning for both text and visual branches. Empirical evaluation on base-to-novel generalization, domain generalization, and cross-dataset transfer shows the efficiency of our method over existing methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Vision-Language (V-L) foundation models like CLIP and ALIGN have emerged as robust zero-shot generalizable models. They integrate image and text modalities through pre-training on extensive image-text pairs. However, adapting these models to specific downstream tasks with limited data remains challenging. Recent methods explore prompt tuning in CLIP-like models, treating prompts as continuous learnable vectors and fine-tuning them while keeping the model parameters frozen. MaPLe is a deep prompting baseline that tunes prompts on both text and image branches, further conditioning image prompts on text prompts using a V-L coupling function. However, these approaches necessitate training data for prompt learning, limiting adaptation to novel datasets during test time. In contrast, our method, inspired by a multi-modal prompting variant, actively aligns class prototypes by leveraging a proxy dataset as a substitute for unavailable CLIP pre-training data. To our knowledge, our approach is the first to explicitly address class-aware distribution misalignment in V-L foundational models during test time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Revisiting CLIP: Our approach is based on the pre-trained V-L model: Contrastive Language-Image Pre-Training (CLIP). It consists of a text and visual encoder (denoted by  and , respectively, and their pre-trained parameters are represented by , respectively), used for mapping the text and image to the vector representation, respectively. The input image is , which is divided into  patches, and the [CLS] token is prepended to these  patch tokens that are projected to produce , where  is the embedding for the corresponding patch token in . The image encoder produces latent visual feature representation  with transformer blocks from . The class label  is embedded within a text template, such as “a photo of a <CLS>” resulting in , where SOS and EOS are the start and end token embeddings and  and  are the token embeddings corresponding to the text template and the class label, respectively. Similarly, the text encoder  encodes  with transformer blocks to produce latent text feature representation . For zero-shot inference, each text feature for class labels  is paired with an image feature to compute the similarity score  where  denotes cosine similarity. The predicted probability on X for each  is given as , where  is the temperature of softmax.\n\nPrompt Tuning: CLIP integrates a considerable pool of knowledge derived from its training on millions of image-text pairs characterized by varying degrees of noise. Prompt tuning methods aim to extract the rich features learned by the CLIP model. Recent approaches [46  ###reference_b46###, 47  ###reference_b47###, 21  ###reference_b21###, 3  ###reference_b3###, 43  ###reference_b43###] append extra learnable prompts to the input of image and text encoders while keeping them frozen. Modified input prompts with frozen encoders generate undistorted and rich CLIP features, where prompt tuning tries to map the context to the source distribution, i.e., the CLIP pre-training dataset. In our work, we use a recent multi-modal prompting baseline [21  ###reference_b21###] where prompt tuning is performed on both the text and image encoders.\n\nSpecifically, the image and text encoders process the input  and  respectively. The learnable prompts  and  represent the  visual and  textual tokens, respectively. We will call prompts  and  as p only. Our approach is based on deep prompting, as in[21  ###reference_b21###], along with text and image prompts at subsequent transformer blocks. We suggest referring to [21  ###reference_b21###] for more details on baseline architecture."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Proposed Method: PromptSync",
            "text": "Inspired by prototype learning [38] and Extreme-Multi-PatchSSL (EMP-SSL) [39], which establish a prototype/benchmark for each class/sample, we propose class-wise prototype alignment between original and augmented views for both source and test samples. The architecture of PromptSync is shown in Figure 1. We use the parameter update from prototype discrimination to generate the class probabilities for the test sample and its augmented views. We accumulate the average of gradients from prototype alignment loss weighted by class probabilities for confident augmented views. The accumulated gradient over multiple iterations is then applied for prompt tuning during test-time adaptation."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Class-aware Prototype Generation",
            "text": "We generated prototypes for each class for both text and visual branches. The prototype for each class is computed using a proxy source dataset. For a test sample and its random views (generated using a set of augmentations), the prototype vector is generated. Let’s denote the token features of a sample, at the output of the text encoder and visual encoder as  and , respectively. The prototype for a sample from text and visual branches is given as:\n\nwhere  represents the total number of tokens (learnable and non-learnable for both text and visual), excluding EOS, SOS, and CLS.  represents textual branch and visual branch respectively. For the proxy source dataset, the class-aware prototype is obtained as:\n\nwhere  contains all samples for class. The prototypes for augmented views are calculated using the augmented samples for each class denoted as  and the corresponding prototypes are denoted as ,  and  respectively."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Prototype Discriminating Loss",
            "text": "The discriminating loss is responsible for training learnable prompts to distinguish the context of samples from one class compared to other classes. This goal is achieved by pushing the class prototype for both text and visual branches away from the prototype of class where . Likewise, we pull prototypes and for the same class and push away augmented ones for . In this regard, contrastive learning offers a solution to pull prototypes of positive pairs and push away negative pairs. We refer to propose our discriminating loss, formally expressed as:\n\nwhere and . The prototypes and additionally contain when . Resulting prompt update (learnable prompt tokens) is obtained after applying gradients for the discriminating loss. Since the proxy dataset will remain the same for all the test instances, the updated prompt can be saved and restored each time for an incoming test sample. We presented the study on performance and latency with and without saving these updated prompts in Appendix. For the rest of the paper, we generalize our method without requiring to save these updated prompts."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Prototype Alignment Loss",
            "text": "We propose the prototype alignment of the test sample (and its augmented views) with the class prototype obtained from the source distribution. We propose to weigh the prototype alignment by the probability of the test sample lying in the particular class. Let's denote the probability (as mentioned in Eq.1) as the mean of probabilities (for class c) produced with the updated prompt across filtered augmented views (preserved after the confidence selection filter) including the test sample. The amplitude and angle alignment of sample with the class prototypes for both text and visual branches is calculated as follows:\n\nHowever, there is an issue with MSE loss since it gives an equal penalty (e.g. = 0.1) for an increase from 1.2 to 1.3 and 1.7 to 1.8. But we wanted to penalize more for 1.2 to 1.3 since an increase in MSE in the smaller range should be penalized more to preserve the base class performance. Hence we penalize with logarithm, i.e., we use . Similarly, the penalty should be applied to for angle alignment. The updated amplitude and angle alignment loss is and respectively. We combined the amplitude and angle loss with equal importance, and hence the prototype alignment loss is given as:"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Algorithm Details",
            "text": "In order to compute the prototype discriminating loss on the source dataset, we require the pre-training dataset of the CLIP model. However, it was trained on over 400 million image-text pairs, which are not publicly available. Nevertheless, in previous works, CLIP has been heavily tuned on the ImageNet dataset to achieve excellent zero-shot performance. Hence, we use ImageNet as the proxy for the source dataset to compute prototypes for each class. These prototypes are computed offline for both the sample and its augmented views, and they are used directly during test-time adaptation. During each iteration of test-time adaptation, the meta-train stage is entered first. The model starts training using the prototype discriminating objective, and gradients are calculated, resulting in the prompt update. Subsequently, the meta-test stage is executed. Here, the augmented views are first filtered using a confidence threshold over predicted probabilities using the updated prompts. The mean probabilities are computed and used as weights. The model is trained, and the gradient of prototype alignment loss is calculated. We average out the gradients over all samples. Finally, the prompts are updated using the combined objective. For this process, we accumulate the averaged gradients before the final prompt update."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We have evaluated PromptSync on different benchmark settings with different datasets described below:\n\nDatasets: For domain generalisation setting, we evaluated our method on four out-of-distribution (OOD) variants of ImageNet: ImageNetV2, ImageNet-Sketch, ImageNet-A, and ImageNet-R. We also consider the evaluation on a recent and challenging benchmark, namely, Photorealistic Unreal Graphics (PUG) dataset, comprised of different textures, sizes, orientations, and backgrounds. For cross-dataset transfer setting, we evaluate the performance on 10 diverse image classification datasets with varying complexities for visual recognition tasks. This includes Caltech 101 for generic objects. Five fine-grained datasets (spanning images of animals, flowers and transportation) are StanfordCars, Food101, Flowers102, FGVC-Aircraft, OxfordPets. Moreover, four datasets, namely, SUN397, DTD, UCF101, and EUROSAT, comprise scenes, textures, human actions, and satellite imagery, respectively. For base-to-novel generalisation, we evaluate our method on ImageNet and the 10 image classification datasets.\n\nBaselines: MaPLe is a multi-modal prompt learning baseline that adapts CLIP by learning prompts on both text and visual branches.\n\nImplementation Details: We ran all experiments on a single NVIDIA A100 40GB GPU. We trained on ImageNet with 16-shot training data selected at random for each class using 2 prompt tokens for a depth of 3 layers (on CLIP ViT-B/16 backbone architecture). We optimized the prompts on both the text and visual branches using a single test image. We augmented each test image with 127 different views using random resized crops, background substitution, horizontal flip augmentations, and visual corruption. For text augmentation, we used hyponyms, synonyms, and meronyms from WordNet. Moreover, we generated various text prompts from pre-trained LLMs. Additionally, we randomly masked one of the learnable tokens for 15% of augmented views. We computed the gradients of alignment loss for a batch size of 128 images, including the original image. During the meta-train stage, we updated the original parameters (using a single iteration) and then optimized the prompts in the meta-test stage by calculating the gradients of alignment loss w.r.t. the updated parameters accumulated for a single iteration to facilitate the one-to-one comparison with baselines. We obtained the top 10% confident predictions of augmented views based on the lowest entropy. We used the AdamW optimizer and a learning rate for the fine-grained datasets and for the rest of the datasets."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Domain Generalization",
            "text": "We demonstrate that all test-time adaptation methods exhibit better performance compared to the pre-trained CLIP model, highlighting the advantage of tuning V-L models at test time. PromptSync achieves the highest Top-1 accuracy averaged across all the domains of ImageNet variants. Furthermore, we evaluated the ImageNet-trained model on various out-of-distribution (OOD) datasets and observed consistent improvement in performance compared to existing state-of-the-art (SOTA) approaches. This confirms that alignment and discriminative training with augmented views on both the text and visual branches enhance the generalization performance of V-L models like CLIP."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Base to Novel Generalization",
            "text": "Table 3 presents the detailed performance report of PromptSync on base and novel classes across 11 recognition datasets. On average, our strategy outperforms the model performance by 1.29% on base classes and nearly 1% on novel classes. This demonstrates that: 1) test-source alignment is crucial for prompt tuning. 2) Prompt tuning alone in the text branch is not sufficient for zero-shot generalization. Since distribution alignment does not promote discriminative learning and the entropy loss on the test dataset is noisy, PromptSync outperforms with class-aware prototype discrimination and alignment across different augmented views. Averaging the gradients further motivates domain-agnostic prompt tuning on both the text and visual branches. This enhances the zero-shot generalization of the V-L model compared to other state-of-the-art approaches. Moreover, our strategy for prompt tuning does not lose information for base classes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Cross-Dataset Transfer",
            "text": "In Table 4, we compared the transfer performance of PromptSync with existing state-of-the-art methods using prompt learning. We evaluated methods for transfer performance across diverse cross-datasets. PromptSync consistently outperforms previous methods across all cross-datasets, providing an average improvement. This affirms that both text-visual alignment and domain-agnostic parameter updates result in better transfer generalization across cross-datasets in V-L models. As opposed to our method, the previous approaches were not consistent in performance across all datasets, which further affirms the advantage of a domain-agnostic training strategy."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Ablation",
            "text": "Class-Aware Prototype Alignment: Table 5  ###reference_### summarizes the comparison between two alignment strategies: distribution alignment of the test sample with the class-agnostic source distribution. All results are on the ImageNet-A dataset. However, we perform domain-agnostic parameter updates with class-aware prototype alignment for the test sample.  With additional signals from predicted probabilities for each class, the class-aware prototype alignment acts as a geometric regularizer, mitigating class collapse in prompt representation.\nLoss variants: We conducted an ablation study on amplitude and angle loss for the class-aware prototype alignment objective. Table 6  ###reference_### compares three loss choices: 1) amplitude loss, 2) angle loss, and 3) amplitude + angle loss. Clearly, the combination of amplitude and angle performs better than other choices. The formulation for the combination of amplitude and angle loss is the same as in equation 11  ###reference_###. We further investigated other variants, i.e., combining two of them without taking the log: 1) subtraction between amplitude and angle (sub) 2) the summation of exponential of both losses (sum_exp). Clearly, the formulation in equation 11  ###reference_### () performs best among other variants. Ablation on the proxy dataset is given in Appendix 12  ###reference_###, and ablation on performance and latency with and without saving updated prompts is provided in Appendix 10  ###reference_###. We also compared the number of augmented views and prompt updates in Appendix 11  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Performance and Latency",
            "text": "The experiments presented in the Table 7  ###reference_### (Appendix) involve a comparison of different methods, namely PromptSync*, and PromptSync. In these experiments, we evaluated the top-1 average accuracy (%) and latency (in hours for a single prompt update) of each method. Specifically, we investigated PromptSync with and without saving the updated prompt obtained after prototype discrimination, with the variant denoted as PromptSync* indicating the adaptation of prompt tokens for test samples after restoring saved prompt tokens.\nThe results, as shown in Table 7  ###reference_###, include latency measurements represented in hours for a single prompt update, and all evaluations are conducted on the ImageNet-A dataset. Notably, the PromptSync* variant demonstrates a faster processing time compared to the full PromptSync method, with only a marginal drop in performance. This outcome underscores the achieved generalization through prototype alignment. Furthermore, in comparison to previous methods, the PromptSync* variant exhibits only a slight increase in latency (0.03 hours) while still improving overall performance."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Sensitivity Comparison",
            "text": "We further performed the sensitivity comparison of our method as compared to other state-of-the-art baselines. In Appendix, Figure 2  ###reference_###(a) shows the comparison of performance during test time adaptation as the number of views increases. All the\nresults are on ImageNet-A dataset. Their performance almost plateaus around 64 views with insignificant improvement further, while PromptSync shows a consistent improvement with the increase in views and insignificant improvement beyond 128. This proves the generalizability achieved by our method since it optimises base CLIP over a larger number of possible shifts in the dataset, resulting in better performance. Figure 2  ###reference_###(b) shows the performance comparison as the number of prompt update steps increases. All the methods increase their performance with an increase in the number of steps; however, our method shows better adaptation to the test sample with more steps. For apples-to-apples comparison we perform a single-step update (128 views)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "LAION400M Proxy Dataset Analysis",
            "text": "Given CLIP’s impressive zero-shot performance on ImageNet, we opted for ImageNet as a viable proxy source dataset, aligning with prior research [33  ###reference_b33###]. We worked with a subset of LAION400M, comprising 2.5 million images (2 times the size of ImageNet). Furthermore, we carried out an ablation study on the alignment strategy using LAION400M as the source dataset, a dataset known to mirror CLIP’s training dataset [9  ###reference_b9###]. The results for this ablation study is shown in Table 8  ###reference_### (Appendix). Notably, the performance impact remains consistent when utilizing this subset of LAION400M alongside ImageNet. Source class prototypes are computed on the proxy source data to derive the distribution for alignment during test time. As this proxy dataset aligns with the model’s training set, this offline computation remains unchanged despite environmental shifts and only necessitates computation once."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Benchmark Settings",
            "text": "###figure_2### ###figure_3### Base-to-Novel Generalisation: Following MaPLe [21  ###reference_b21###], we evaluate PromptSync on a zero-shot setting. We split the dataset into base and novel classes. The model is trained only on the base classes in a few-shot setting and evaluated on the base and novel classes.\nCross-dataset Transfer: We evaluate PromptSync on the ImageNet[11  ###reference_b11###] pre-trained model on other datasets to determine the transfer performance. Our model is trained on all 1000 ImageNet classes in a few-shot manner.\nDomain Generalisation: We evaluate PromptSync on out-of-distribution (OOD) datasets for domain generalizability. Similar to cross-dataset, we evaluate our ImageNet-trained model directly on OOD datasets, which are described in Section 4  ###reference_###."
        }
    ]
}