{
    "title": "Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models",
    "abstract": "We consider the problem of online finetuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency), sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context learning and finetuning blurs: both are methods to condition the model on previously observed tokens.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformer-based language models can be conceptualized as systems with two distinct memory components: one is given by the model’s parameters, where learning (through gradient descent) encodes information from the training set into this memory. The other is the context, which roughly corresponds to the persistent hidden states in recurrent neural networks. For transformers, context is a non-parametric form of memory: the tokens within the attention window.\n\nLLMs heavily rely on context to condition the model towards the desired behaviour. However, the prompt is a precious resource, and for transformers, the cost of inference grows with the size of the attention window. This becomes more problematic in multimodal systems, where images or short videos can easily exhaust the context tokens we can afford to use.\n\nWith dynamic evaluation as explored by Krause et al. (2018, 2019), the concept of updating model parameters at test time is introduced, with model parameters becoming part of the temporal, changing state of the model. Parameters can capture longer-term information that exceeds the length of the context window and are also suited to adapt to distributional changes that exceed the in-context adaptability of the model. Online learning can be seen as a particular type of memory, especially suited to changes like style or topic, which appear to the model as a distribution shift in the observations.\n\nWe investigate various trade-offs when online-adapting transformer-based LLMs with gradient descent on long text sequences."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methods for SGD online adaptation",
            "text": "In this section, we describe the methods that we put in place to make our study possible. These methods have two main goals:\n\n(i) learn from a sequence of tokens longer than the model context, and\n\n(ii) efficiently update the model parameters and reduce the memory and/or compute footprint of online adaptation.\n\nThe challenge is that transformer implementations operate on a limited, typically fixed number of tokens each time they are invoked. To operate on longer sequences they have to be broken into sub-sequences, and the model implementation operates on such a sub-sequence at a time. \n\nCompared to static evaluation, online adaptation requires additional computational resources for the backward pass and, typically, additional memory for the optimizer state. We investigate an approach to mitigate these costs:\n\nReducing the Update Frequency. In order to vary the computational cost of online learning, and to construct Pareto fronts that highlight the compute vs. performance trade-offs, we update the parameters only every nth forward step. While this approach leads to a suboptimal performance, we show in our experiments that it can strike interesting trade-offs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "To investigate the online adaptability of transformers, we use the books from Project-Gutenberg (the PG-19 dataset) as a source for long and consistent text sequences. The starting point for all experiments is a transformer pretrained on the C4 dataset. We experiment with different model sizes, between 150M and 1B parameters; Appendix B contains the experimental details. The pretrained models are then finetuned on books from the PG-19 training set because the content and style of text in the C4 dataset, which consists mostly of internet-scraped data, and PG-19, which contains books from before 1919, is significantly different.\n\nThe finetuned models are then tested against the sequence of 100 books from the PG-19 test set. For the test sequence, we record the (cumulative) log-losses for all tokens. We concatenate the 100 books from the PG-19 test set, in the order they are stored, to form a fixed sequence of 11.8M tokens.\n\nFigure 1 visualizes typical results: We compare static evaluation vs. dynamic evaluation where the model is reset to the finetuned model at each book boundary. The static model accumulates in total 26.73 M nats log-loss on the test sequence (2.26 nats/token), while the dynamic model accumulates 26.20 M nats (corresponding to 2.20 nats/token).\n\nThe regret plots show the cumulative log-loss relative to the static comparator: a flat curve indicates that a model has on average the same per-token log-loss as the comparator around the position, while positive and negative slopes indicate a locally higher or lower log-loss respectively. It often takes the online adapting model some thousand tokens to show a clear advantage over the static model. Just after book boundaries, a continuously adapted model often underperforms compared to the pretrained (static or dynamic) model. This may not be surprising as the continuously learning model has specialized to the previous books, but also suggests that more advanced adaptation methods should be able to close this gap."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Online Learning - An analysis of compute vs. performance",
            "text": "In order to understand the impact of online learning and its interaction with in-context learning, we conduct a large exploration varying:\n\nThe number of samples used for finetuning: This allows us to control the distribution shift that the model is faced with – the more the model is finetuned on the training set of PG-19, the more it is in-distribution with respect to the test sequence.\n\nThe model context size: We hypothesize that online learning can have similar benefits as larger context windows. We therefore compare online adapted models with short context windows to in-context learning in models with longer context windows.\n\nThe model size: We are interested in understanding how our observations generalize across different scales.\n\nIn Figure 3, we first look at a single model size (1B parameters) and compare the Pareto fronts corresponding to two context sizes (512 and 2048) as we vary the number of samples the model is finetuned on. We observe that when the model is directly updated online on the PG-19 test set without prior finetuning, the models with a smaller context exhibit a better compute to performance trade-off than the models with a larger context (left panel). As the amount of finetuning increases, this advantage is reduced (middle panel) and even inverted (right panel). These observations generalize to other model sizes (figures in the appendix). These results suggest that the models favor memory in weights when faced with a large distribution shift between the pretraining and online adaptation data. When the models are more in-distribution with respect to the online data, the results suggest that at a fixed budget, it is better to use a model with a larger context window. Online adaptation however unsurprisingly always improves the performance of the models. Moreover, models with shorter context and online adaptation can achieve a competitive performance to the models with longer context. While this can be more expensive in terms of FLOPs, it comes at a lower memory requirement.\n\nFigure 4 gathers the results obtained with different model sizes (150M, 400M, and 1B parameters). For this figure, we show only models that are updated at every step (corresponding to an update frequency of 1 in Figure 3). We observe that when we increase the amount of finetuning, the number of the static models that appear on the Pareto front increases. We however similarly observe that online adaptation always improves the performance, smaller models with online adaptation can achieve a competitive and sometimes better performance than larger models.\n\nFinally, Figure 5 shows how the static and dynamic model performance scale with the distribution shift (amount of finetuning data) and the model size. The figure shows that while finetuning reduces the gap between static and online evaluation, this gap does not disappear, but rather becomes constant once the model adapts to the change in distribution. Similarly, increasing model size, while also improving overall performance, does not replace the benefit of online learning."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we offered a new perspective on online adaptation of LLMs (a.k.a. dynamic evaluation). Through extensive experiments, we show that when a model is faced with a significant distribution shift, online learning with a smaller context window and/or a smaller model can lead to a better compute-performance Pareto front, which suggests a superiority of the memory in weights over the memory in activations associated with in-context learning. This advantage is reduced and eventually disappears when the model is fine-tuned to the target distribution before the online adaptation phase. We observe that online learning always improves performance. Moreover, for models that are in-distribution, online learning with a smaller context (and therefore smaller memory requirements) can close the gap with in-context learning with a larger context. It is also worth noting that the best results observed with online adaptation employ a simple strategy to avoid that the models overfit to data in the local context by resetting the weights to their value at the start of the online adaptation phase.\n\nThis study opens up many interesting research avenues, such as improving the efficiency of online learning (in terms of memory or compute), automatic detection of reset points, and a better understanding of the difference between what the weight memory and the activation memory capture. In particular, it is to be expected that weight memory would be better suited to store the style or topic of a discussion, which are perceived by the model as a distribution change. Context and retrieval are better at capturing details, leading to a separation of concerns similar to how memory of biological systems is categorized into different types in cognitive sciences or neuroscience."
        }
    ]
}