{
    "title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
    "abstract": "The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts.\n\nIn this work, we revisited the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conducted further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) (OpenAI, 2022; 2023; Touvron et al., 2023a; b) have demonstrated remarkable capabilities in various NLP tasks, owing to the knowledge memory acquired during pre-training. Despite the generated text often appearing correct, careful observation reveals that they sometimes exhibit factually incorrect statements, i.e., ”hallucinations.” Such hallucinations significantly undermine the reliability of LLMs in real-world scenarios.\n\nFactual hallucinations have received widespread attention due to their significant side effects, as LLMs generate content that deviates from established world knowledge. Furthermore, with the widespread changes in the world’s circumstances and the continuous expansion of large model scales, it’s paramount to efficiently keep factual knowledge up-to-date. Knowledge Editing has been proposed to address this issue, achieving efficient modifications to model facts while ensuring no adverse effects on other unrelated knowledge.\n\nAn excellent text generation without hallucinations demands that an LLM is capable of ”knowing” correctly and ”telling” accurately, which means it needs to keep factual knowledge up-to-date and convey it accurately. Some emerging works focus on narrowing the gap between ”knowing” and ”telling” in LLMs, guiding them to accurately ”tell” the facts they know. In particular, various factuality decoding methods can directly generate answers that better align with factuality, which is highly convenient as it does not require the infusion of extensive new factual knowledge through supervised fine-tuning (SFT) or RLHF.\n\nOur experimental results on the TruthfulQA and FActScore benchmarks have demonstrated the effectiveness of factuality decoding in LLMs. These diverse decoding strategies lead to varying improvements in LLMs’ factual metrics compared to their original decoding methods. Although modifying the decoding methods of LLMs enables them to more accurately ”tell” the factual knowledge they have learned, they overlook a crucial aspect: Can these modified LLMs still be efficiently edited for updated knowledge?\n\nFactuality decoding encourages LLMs to produce answers that better align with pre-training facts, leading them to believe that the facts they have learned are accurate. Therefore, we suspect that current decoding methods for factuality may potentially inhibit the inherent simplicity and generalization ability of pre-trained language models. This overconfidence could result in rigidity in knowledge, making it challenging to update outdated facts using recent efficient knowledge editing methods.\n\nHence, we find that current existing decoding methods still cannot perfectly address the factual hallucinations of LLMs, as they overlook the importance of preserving the flexibility for knowledge editing. Altogether, our study highlights the potential risks associated with current factuality decoding methods and validates their apparent decline in knowledge editing. Consequently, we strongly advocate that a proficient LLM with factual accuracy should prioritize both the efficient update of factual knowledge and the accurate conveyance of factual information, thereby reducing the likelihood of factual hallucinations. Therefore, we recommend that research exploring factual alignment should simultaneously focus on the effectiveness of knowledge editing."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "This section revisits decoding approaches (Section 2.1  ###reference_###) and factual knowledge editing (LABEL:{ssec:factKE}), aiming to delve deeper into the subsequent exploration of the impact of factuality decoding on knowledge editing for LLMs.\nModel editing (Mitchell et al., 2022  ###reference_b21###; Yao et al., 2023  ###reference_b40###) aims to efficiently adjust the behavior of the original base model  on specific editing descriptors , without affecting the model’s behavior on other samples.\nThe editing descriptors  describes a desired change in model behavior and can be represented as , where  is a input-output pair like Who is the president of US? Joe Biden.\nThe ultimate objective of model editing is to generate an edited model, denoted as . Consequently, given an edit descriptor , the post-edit model  is anticipated to predict the edited output answer, formally represented as , where .\nA factual knowledge can be represented using a triplet , where  represents the subject,  represents the relation, and  represents the object (Petroni et al., 2019  ###reference_b26###; Zhong et al., 2023  ###reference_b44###).\nConsequently, if LLMs can predict the masked entity expressing this fact in a cloze-style question, such as in The president of the United States is_ , which is built from the triplet , and the object can be predicted as “Joe Biden,” then it indicates that LLMs possess knowledge of this fact.\nFact editing (De Cao et al., 2021  ###reference_b8###; Zhong et al., 2023  ###reference_b44###) is an indispensable aspect of the continuous development of LLMs. This is because the factual knowledge within models cannot always remain correct over time, which should become outdated as time progresses.\nBased on the preceding context, in fact editing,  can be represented by a tuple  while  can be represented by , denoting the edited factual answer for the original . Thus, the edit descriptor  can be represented as: , and the post-edit model  satisfies  while .\nConsequently, given a collection of fact edits , fact editing involves learning a function  satifying .\nAs the scale of LLMs continues to expand, adjusting model parameters through retraining becomes increasingly challenging. Consequently, efficient  methods without requiring training are receiving more attention. This is also why our work chooses to evaluate using a simple yet efficient knowledge editing method in Section 4  ###reference_###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Decoding Approaches",
            "text": "This paper focuses on decoding strategies used in open-ended language generation tasks, which entail language models receiving input prompts and generating fluent and coherent continuations.\nThe objective is to anticipate the succeeding word within a given contextual sequence, which is a fundamental pre-training goal extensively employed in state-of-the-art large language models (Radford et al., 2018  ###reference_b27###; Brown et al., 2020  ###reference_b4###; Anil et al., 2023  ###reference_b1###; Touvron et al., 2023b  ###reference_b33###).\nFormally, given an prompt sequence of length , denoted as , where  is a token in the vocabulary . We compute the next token probability distribution from a pre-trained autoregressive language model  conditioned on the previous context:\nwhere  represents a temperature parameter regulating the precision of the subsequent-token distribution.\nIn text generation, the language model samples from the conditional distribution  to generate the next token , iterating this process continuously until the sequence generation reaches the end token.\nAt decoding time, various decoding strategies can be applied at each step  to select the next token , with the given predicted distribution of the next token .\nThe most prevalent strategy involves sampling-based decoding, where  is randomly sampled from the distribution. Another prevalent method involves searching for the most probable text sequence through either greedy decoding or beam search (Wu et al., 2016  ###reference_b37###). However, these approaches often leads to repetitive and monotonous outputs, thus giving rise to numerous variants.\nFor instance, nucleus sampling (Holtzman et al., 2019  ###reference_b10###) selects tokens from the top- percentile of the next token distribution, while top- sampling  (Fan et al., 2018  ###reference_b9###) chooses tokens from the top- candidates in the next token distribution."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Factual Knowledge Editing",
            "text": "Model editing (Mitchell et al., 2022  ###reference_b21###  ###reference_b21###; Yao et al., 2023  ###reference_b40###  ###reference_b40###) aims to efficiently adjust the behavior of the original base model  on specific editing descriptors , without affecting the model’s behavior on other samples.\nThe editing descriptors  describes a desired change in model behavior and can be represented as , where  is a input-output pair like Who is the president of US? Joe Biden.\nThe ultimate objective of model editing is to generate an edited model, denoted as . Consequently, given an edit descriptor , the post-edit model  is anticipated to predict the edited output answer, formally represented as , where .\nA factual knowledge can be represented using a triplet , where  represents the subject,  represents the relation, and  represents the object (Petroni et al., 2019  ###reference_b26###  ###reference_b26###; Zhong et al., 2023  ###reference_b44###  ###reference_b44###).\nConsequently, if LLMs can predict the masked entity expressing this fact in a cloze-style question, such as in The president of the United States is_ , which is built from the triplet , and the object can be predicted as “Joe Biden,” then it indicates that LLMs possess knowledge of this fact.\nFact editing (De Cao et al., 2021  ###reference_b8###  ###reference_b8###; Zhong et al., 2023  ###reference_b44###  ###reference_b44###) is an indispensable aspect of the continuous development of LLMs. This is because the factual knowledge within models cannot always remain correct over time, which should become outdated as time progresses.\nBased on the preceding context, in fact editing,  can be represented by a tuple  while  can be represented by , denoting the edited factual answer for the original . Thus, the edit descriptor  can be represented as: , and the post-edit model  satisfies  while .\nConsequently, given a collection of fact edits , fact editing involves learning a function  satifying .\nAs the scale of LLMs continues to expand, adjusting model parameters through retraining becomes increasingly challenging. Consequently, efficient  methods without requiring training are receiving more attention. This is also why our work chooses to evaluate using a simple yet efficient knowledge editing method in Section 4  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Factuality Decoding for LLMs",
            "text": "Our work focuses on potential pitfalls in current factuality decoding strategies for LLMs. Before evaluating the modified LLMs with various decoding methods, it is necessary to thoroughly understand them. Therefore, this section first revisits several strong decoding methods for LLMs’ factuality, then evaluates and analyzes their performance in enhancing the factuality of LLMs.\n\nInference-Time Intervention (ITI) first identifies a sparse set of attention heads with high linear probing accuracy for truthfulness, as defined by the TruthfulQA benchmark. Then, during inference, it shifts activations along these truth-correlated directions. This process is repeated autoregressively until the entire answer is generated.\n\nZhang et al. first construct a factually weak LLM by inducing hallucinations from the original LLMs, and then penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Specifically, they determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Decoding Methods",
            "text": "To avoid the resource-intensive nature of existing methods like RLHF (Ouyang et al., 2022; Bai et al., 2022a; Menick et al., 2022) and RLAIF (Bai et al., 2022b), which require significant annotation and computational resources, factuality decoding aims to modify the decoding architecture of LLMs solely to narrow the gap between \"knowing\" and \"telling\". We have selected several representative strong decoding methods, which are introduced individually as follows.\n\nInference-Time Intervention (ITI) (Li et al., 2024b) first identifies a sparse set of attention heads with high linear probing accuracy for truthfulness, as defined by the TruthfulQA benchmark. Then, during inference, it shifts activations along these truth-correlated directions. This process is repeated autoregressively until the entire answer is generated.\n\nDoLa (Chuang et al., 2023) leverages a modular encoding of knowledge to magnify factual knowledge within an LM through a contrastive decoding approach. In this method, the next-word probability output is derived from the disparity in logits between a higher layer and a lower layer. By accentuating the knowledge from higher layers and diminishing that from lower layers, LoRa aims to reduce factual hallucinations.\n\nZhang et al. first constructs a factually weak LLM by inducing hallucinations from the original LLMs, and then penalizes these induced hallucinations during decoding to enhance the factuality of the generated content. Specifically, ICD determines the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Factuality Evaluation",
            "text": "We evaluate the factual enhancement of the decoding method on the TruthfulQA (Lin et al., 2021) and FActScore (Min et al., 2023) benchmark. Evaluation on both benchmarks adheres to the settings of previous studies. For TruthfulQA, we employ multiple-choice-based metrics, specifically MC1, MC2, and MC3 scores. For FActScore, assessment is conducted through retrieve+chatGPT methodology. We conduct the evaluations on two sizes of llama-2-chat (7B,13B) models as base models. The results are presented in Table 1.\n\nThrough observation of the experimental results, we first note that increasing the model size does not lead to a significant improvement in factual accuracy for the llama2 model. When compared to the base llama-2-chat model, the majority of decoding methods enhanced the factual performance of the llama2 model to some extent across both benchmarks.\n\nIn conclusion, the aforementioned factual decoding methods enhance the factual accuracy of base LLMs, guiding LLMs to infer more authentic facts from existing knowledge."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Multi-hop Knowledge Editing Benchmark",
            "text": "We conduct experiments on the multi-hop knowledge editing benchmark to assess whether factuality decoding methods remain effective in updating knowledge. Changing one fact should result in cascading changes to the model’s associated knowledges. For instance, if we modify the UK Prime Minister to be Rishi Sunak, the response to Who is married to the British Prime Minister? should differ. Therefore, this section introduces the concept of multi-hop facts and presents a multi-hop knowledge editing dataset, along with efficient editing methods tailored to it. In our experiments, we design prompts for fact editing and prompt LLMs to answer multi-hop knowledge questions through a chain of thought approach."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Multi-hop Fact Editing",
            "text": "Multi-hop fact editing aims to edit not only a single-hop fact but also all the facts within the multi-hop context that are affected by this edited fact. Formally, we consider two chains of facts, and , which have the same relation set. When editing a single-hop fact in the first fact chain with an edit descriptor, the factual memory of the large model regarding it should be edited to. For instance, regarding the two-hop question Who is married to the British Prime Minister? mentioned above, the original answer should be Carrie Johnson, and the corresponding chain of facts can be described as follows: (United Kingdom, head of government, Boris Johnson), (Boris Johnson, spouse, Carrie Johnson). With a fact edit and an additional fact chain, the edited LLMs should respond with the new rippling answer: Akshata Murthy.\n\nEdits  \n2-hop  \n3-hop  \n4-hop  \nTotal\n\n1  \n2,454  \n855  \n446  \n3,755\n\n2  \n2,425  \n853  \n467  \n3,745\n\n3  \n-  \n827  \n455  \n1,282\n\n4  \n-  \n-  \n436  \n436\n\nAll  \n4,879  \n2,535  \n1,804  \n9,218"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We conduct experiments on the recent knowledge editing dataset MQuAKE-CF-3k (Zhong et al., 2023) for multi-hop fact editing. MQuAKE-CF-3k comprises 3,000 instances derived from paths extracted from Wikidata (Vrandečić & Krötzsch, 2014), which consists of fact triples associated with millions of entities. Table 2 presents the statistics of the MQuAKE-CF-3k dataset. The dataset provides multi-hop fact questions with fact chains, along with the answers before and after editing, which are used to evaluate knowledge editing on counterfactual edits."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Editing Methods",
            "text": "We consider efficient and convenient knowledge editing methods that can be flexibly applied to all black-box LLMs, sidestepping the computational burden associated with retraining models. In our experiments, we design prompts for fact editing and prompt LLMs to answer multi-hop knowledge questions through a chain of thought approach."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Knowledge Editing Evaluation for Factuality Decoding",
            "text": "In this section, we evaluate the knowledge editing of factuality decoding methods on the benchmark introduced earlier to explore the impact of factuality decoding on the factual updates of LLMs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Implementation Details",
            "text": "We employ two different sizes of LLMs, the llama-2-chat 7B and 13B, with the unchanged decoding strategy as the baseline. Following previous studies, we apply two strong factuality decoding strategies, ITI and DoLa, on the llama-2-chat models in our experiments. During inference with llama2 models, we align all decoding methods with all the baseline, including settings such as temperature=0.9, top-=0.95, and others."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "The experimental results demonstrate that factuality decoding methods lead to a significant decrease in accuracy of the question answering task for knowledge editing, indicating that Factuality Decoding severely impacts the knowledge flexibility of LLMs. Moreover, by comparing the editing results using default decoding and DoLa on llama2 models of different sizes, it is apparent that the impact of model size on knowledge editing is opposite. When the size of the llama2 model is increased, the accuracy of DoLa decreases. This further reveals that as the model size increases, the current factuality decoding exacerbates the detrimental effects on the factual updates of LLMs."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Factuality Accuracy vs Knowledge Flexibility",
            "text": "We consider the performance of LLMs in knowledge editing as indicative of their knowledge flexibility. As shown in Figure 2, we visually present the performance of different factuality decoding methods in knowledge editing alongside the baseline and ChatGPT. It can be observed that ChatGPT, with a particularly large parameter size, achieves the highest accuracy, implying its strong knowledge flexibility. However, the factuality decoding reduces the knowledge flexibility of the llama2 model compared to its original state, suggesting potential pitfalls of factuality decoding for knowledge flexibility in developments of LLMs.\n\nFigure 2 further illustrates the comparison between the decline in knowledge flexibility and the improvement in factuality accuracy. It can be easily observed that the proportion of the decline in knowledge flexibility far exceeds the improvement in factuality accuracy. This prompts us to ponder whether such improvements in factuality accuracy are worth the significant loss in knowledge flexibility. Therefore, we recommend that research on factuality should simultaneously consider both aspects of factuality accuracy and knowledge flexibility."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Case Study",
            "text": "Figure 3 provides a qualitative comparison example for baseline and factuality decoding. Factuality decoding may lead to errors even when knowledge is correctly edited in the baseline. Taking a closer look, the output of factuality decoding consistently reflects answers prior to editing, while the baseline generates correct new answers based on the edits. This indicates that LLMs using factuality decoding exhibit excessive confidence in their own knowledge, thus failing to adjust their answers based on editing prompts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we evaluate existing factuality decoding methods that enhance the factual accuracy of large language models on the knowledge editing benchmark. The results indicate that while these methods enhance factual accuracy to some extent, they lead to significant loss of knowledge flexibility. This excessive confidence in large language models makes it difficult to carry out knowledge editing. Therefore, our work advocates that research into factual alignment should not overlook the importance of the effectiveness of knowledge editing."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion and Future Work",
            "text": "This paper proposes a new concept regarding the\nfactuality of large language models: LLMs with good factuality should simultaneously balance factuality accuracy and knowledge flexibility.\nThis implies that while accurately conveying factual information, it should also ensure that keeping the knowledge up-to-date is easily achievable for LLMs.\nBased on the validation results of existing factuality decoding methods in this paper, achieving the desired balance seems challenging.\nNot only decoding, but we also hold a skeptical stance regarding methods such as RLHF and SFT for injecting new knowledge or self-alignment in terms of knowledge editing.\nIt appears to present a natural paradox where we strive for LLMs to simultaneously maintain a strong belief in existing knowledge while also being capable of facile modification when necessary.\nTherefore, for future work, we plan to incorporate more methods aimed at improving the factuality of LLMs into our repertoire of knowledge editing evaluation criteria.\nWe aim to further validate their factual updating capabilities and, in the process, observe the inherent relationship between factuality accuracy and knowledge flexibility.\nWe envision establishing a comprehensive validation framework for the factuality of LLMs that integrates both accuracy and flexibility, which will be of paramount practical significance for the long-term development of future large language models."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "Ethical considerations are of utmost importance in our research endeavors. In this paper, we conscientiously adhere to ethical principles by exclusively utilizing open-source datasets and employing models that are either open-source or widely recognized in the scientific community. Moreover, our proposed method is designed to ensure that the model does not produce any harmful or misleading information. We are committed to upholding ethical standards throughout the research process, prioritizing transparency, and promoting the responsible use of technology for the betterment of society."
        }
    ]
}