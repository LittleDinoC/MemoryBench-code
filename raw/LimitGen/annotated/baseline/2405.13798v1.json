{
    "title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models",
    "abstract": "We propose a new asymptotic equipartition property for the perplexity of a large piece of text generated by a language model and present theoretical arguments for this property. Perplexity, defined as an inverse likelihood function, is widely used as a performance metric for training language models. Our main result states that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that language models are constrained to only produce outputs from a “typical set”, which we show, is a vanishingly small subset of all possible grammatically correct outputs. We present preliminary experimental results from an open-source language model to support our theoretical claims. This work has possible practical applications for understanding and improving “AI detection” tools and theoretical implications for the uniqueness, predictability and creative potential of generative models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction and Motivation.",
            "text": "Consider a generative model, defined as an algorithm that takes a user input and produces an output that statistically resembles data from a natural source. A specific type of generative model is a large language model (LLM) whose output is a body of text and the input is a text user prompt. State-of-the-art LLMs (Anthropic, 2024 ###reference_b1###; OpenAI, 2024 ###reference_b21###) are now able to produce detailed and information-rich text outputs such as entire screenplays and book-length manuscripts from a short and simple user prompt. Furthermore, LLMs produce text that can imitate human language well enough to pass the Turing test (Pinar Saygin et al., 2000 ###reference_b22###), i.e., resembles text created by humans well enough to be convincing to human observers. In this work, we argue that an LLM is strongly constrained by statistical laws. Specifically, we show that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that any language model is constrained to only output text strings from a typical set, which is an exponentially vanishing subset of all possible grammatically correct strings."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Contribution: Equipartition Property for Perplexity",
            "text": "Perplexity, defined as an inverse likelihood function, is widely used as a performance metric for language models (Meister and Cotterell, 2021). It is closely related to the information-theoretic concepts of surprisal (Levy, 2008) and cross-entropy (Cover and Thomas, 2005b), and it also appears to capture linguistic (Miaschi et al., 2021; Gamallo et al., 2017) and cognitive (Demberg and Keller, 2008; Cohen and Pakhomov, 2020) phenomena at least partially. Many “AI detection” tools for identifying synthetic text from language models are based on observed differences between the perplexity of synthetic and natural text (Mitchell et al., 2023; Gehrmann et al., 2019).\n\nOur main result is a generalization of the well-known Asymptotic Equipartition Theorem (AEP) (Cover and Thomas, 2005a) from information theory. The simplest version of the AEP states that a long sequence of independent and identically distributed (iid) random symbols is likely to be a typical sequence (Csiszar, 1998) defined by the property that the empirical distribution of the occurrence of different symbols within the sequence is very close to the distribution from which the symbols were drawn.\n\nThe AEP itself can be thought of as a variant of the Law of Large Numbers (LLN) (Idele, 2018) applied to a log likelihood function, and just like the LLN, there is an extensive literature on extending the AEP to a sequence of nearly-independent but not identically distributed random variables (Nishiara and Morita, 2000; Timo et al., 2010). The outputs of non-trivial language models, however, cannot be reasonably modeled as a sequence of nearly-independent symbols."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Definitions",
            "text": "Let M be a generative model described by , where the output  consisting of a (potentially infinite) string of tokens  is a deterministic function of user prompt  and a pseudo-random sequence . Each token is chosen from a finite set of tokens . The model M can produce a number of different output strings  for the same user prompt  corresponding to different values of the pseudo-random sequence . This defines a probability distribution , or more formally, a sequence of probability distributions  over  where  is the substring of  consisting of the first  tokens."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sequential LLM",
            "text": "Practical implementations of LLMs specify the probability distribution iteratively:\nand so on. Thus, the model M first draws a random value for the first token by sampling from the distribution. Then the model determines a distribution for the second token as a function of the initial prompt and the randomly chosen first token. Thus, the second token is randomly sampled from a distribution and so on. We can write:\n\nGiven a string, open-source LLMs can be programmed to print out the distributions from which its tokens were selected. Specifically, given a user prompt and a string of tokens, it is possible to get a complete listing of the distributions. Note that complete knowledge of the distributions is not the same as complete knowledge of the full model even for a fixed user prompt. As an example, the former requires knowledge of the distributions for different values of the tokens, but only conditioned on the specific values contained in one specific string.\n\nRemark. The equation is simply an application of the Bayes rule of probability theory and it always holds for any generative model regardless of whether the tokens are sequentially generated. However, the conditional distributions are not in general easily accessible, so while the equation is true for all generative models, it may only be useful for sequential models.\n\nThe perplexity of a (finite length) text string for a model M is defined as the per-token inverse likelihood of the string. It is usually more convenient to work with the log-perplexity."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "A Toy Problem",
            "text": "Let  be two fixed probability distributions over the (discrete) set  of tokens. Consider a toy problem involving two language models A and B, that each generate a string of tokens  where each token is generated iid from the distribution  and  respectively. The iid assumption implies that the tokens  can be thought of as being generated by a stationary and ergodic random process.\n\nConsider a long string  randomly generated from model A. Let  denote the empirical distribution of  in the string :\nwhere  denotes the indicator function and  is the relative frequency of token  in the (long) string . The log-perplexity of string  for model A is:\nwhere  is the cross-entropy between two distributions  over .\n\nIt is well-known with equality when , where  is the entropy of distribution . The simplest version of the classical Asymptotic Equipartition Theorem (AEP) from information theory states that the log-perplexity of a long string  of iid symbols is almost always very close to the entropy  of the distribution  the symbols are drawn from.\n\nSimple AEP. For a long text string  of iid tokens  drawn from a distribution , the log perplexity  as defined is close to the entropy of the distribution  of the distribution  with high probability:\n\nProposition 1 can be thought of as a direct consequence of the (weak) Law of Large Numbers applied to the random variable :\n\nFrom this, we see that the empirical distribution  for long strings  is very close to  with high probability. Putting these observations together, we have: \n\nIntuitively, this states that a long string  generated from model A is likely to have a lower perplexity for model A than for any other model B. This means that a model trained to have low perplexity over a set of reference texts will generate text strings that are statistically similar to the reference texts. This is the theoretical justification for using perplexity as a loss function for training language models, and it is an excellent justification—provided that we accept the Shannon model of language, i.e., the idea that languages can be reasonably modeled as a stochastic sequence of tokens."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "A Naive Method for Detecting Text Generated by a Language Model",
            "text": "Given a text string, we seek to determine whether or not it was generated by a given model M. This is the problem of “white-box” detection of AI-generated text where we assume knowledge of the specific language model that potentially generated the text. A naive method for doing this is as follows: compare the log-perplexity of the text for model M with a threshold: if it exceeds the threshold, we determine that the text is likely to have been generated by model M.\n\nConsider a long string randomly generated from model B. Using the same argument as in Section 2.2, we have:\n\nNote, however, that we have no basis to assert any relationship between two different perplexities. Indeed if one perplexity is significantly lower than the other, it cannot be ruled out that a text string generated by a different model has lower perplexity for a model than a text string generated by itself.\n\nThe mathematical properties of cross-entropy provide a rigorous basis for using perplexity to compare the compatibility of different models with a given text string. However, this theory does not support using perplexity to compare different text strings for compatibility with a given model."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "A Modest Generalization",
            "text": "This theoretical analysis relies on Proposition 1 ###reference_position1### which only applies to toy models that generate strings of iid tokens. \n\nGeneralized AEP. Consider a language model A’ that generates a text string where the tokens are drawn independently from a sequence of distributions. Assuming the entropies of distributions are uniformly bounded i.e., the log perplexity of the string as defined in (3 ###reference_###) is close to the average entropy of the distributions with high probability:\n\nThe regularity condition that the entropies are uniformly bounded is trivially true if the token dictionary is a finite set: for any distribution over.\n\nWhile Proposition 2 ###reference_position2### does not lend itself to an intuitive interpretation in terms of the empirical distribution of the tokens, it too is a direct consequence of the Law of Large Numbers applied to the log-perplexity random variable. Much of the analysis in Section 2.2 ###reference_### can be extended to generalized models like A’ in Proposition 2 ###reference_position2### that allows each token to be drawn from different distributions. However, the tokens in these models must be drawn from fixed distributions independent of past tokens. This is still quite trivial compared to modern language models where the output tokens depend in highly complex and sophisticated ways on past tokens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "An Equipartition Property for Perplexity",
            "text": "We propose a generalization of the theory described in Section 2.2 to a broader class of models. Consider a language model \\( M \\) and a random infinitely long text string \\( S \\) generated by \\( M \\), whose probabilities for a given prompt \\( P \\) are described by a sequence of probability distributions \\( \\{ p_i \\} \\) over the set of tokens \\( V \\). We assume that the prompt \\( P \\) is fixed.\n\nThe empirical entropy \\( H_{emp}(S) \\) of model \\( M \\) for string \\( S \\) is defined as the average entropy over the sequence of distributions. Note that the empirical entropy \\( H_{emp}(S) \\) is a random variable, whereas the entropy \\( H(p_i) \\) is a constant number for a fixed distribution. The expectation of \\( H_{emp}(S) \\) is taken over random text strings \\( S \\).\n\nThe log-deviation of a probability distribution \\( p \\) over tokens \\( V \\) is defined as the standard deviation of the log-likelihood random variable under distribution \\( p \\). The log-deviation for a string \\( S \\) for model \\( M \\) is then the average log-deviation over the sequence of distributions.\n\nWe interpret the mean and standard deviation of the log-likelihood as the empirical entropy and log-deviation, respectively. If the log-deviations of the distributions for a string \\( S \\) are uniformly upper-bounded, the log-deviation asymptotically vanishes.\n\nOur main result suggests that while the log-deviation of a fixed probability distribution is deterministic, the log-deviation for a string is a random variable that can vary for different random text strings. This randomness arises because the distributions differ for each string due to the dependence of future tokens on earlier ones. This dependency is a key feature of modern language models that enables the generation of sophisticated text outputs. We introduce an auxiliary language model as a conceptual device to manage this complexity."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Concept of an Auxiliary Language Model",
            "text": "Let us consider a fixed string . For this particular string  and model M, we have a fixed set of distributions . Now define an auxiliary generative model  that generates a random string  where the tokens  are generated independently from the distributions . This construct of the auxiliary model  for a fixed string  allows us to work with fixed distributions  which are not themselves functions of a random string.\n\nIn particular, note that the empirical entropy  and log-deviation  for model  are both deterministic functions of  independent of the string . Specifically, we have . Similarly, we have  which, since we have fixed our special string , is now also fixed. Indeed, it is easy to show that  is the standard-deviation of the random variable  the log-likelihood of the random string . Then, by the Chebyshev Inequality (Cohen, 2015  ###reference_b4###), we have:\n\nClearly, for substrings  of our special string  that defines the auxiliary model , we have by definition . In addition, the entropy  of the random strings  output by auxiliary model  is by definition .\n\nThe auxiliary model  is much simpler than the general language model M. In particular, because the tokens  are generated independently of other tokens, the AEP in Proposition 2  ###reference_position2### applies to strings  generated by the auxiliary model . Indeed we note from (15  ###reference_###) that the log-deviation  vanishes for large  under some regularity conditions on the distributions , in which case, the AEP for model  follows from (16  ###reference_###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Ensemble of Auxiliary Models",
            "text": "We can think of a generative model M as a statistical ensemble of a large number of (simpler) auxiliary models. Each auxiliary model is capable of generating a random text string consisting of a sequence of independent tokens. We can then imagine choosing randomly from one of the auxiliary models. We now make this idea more precise. Consider an enumeration of the possible text strings. Let denote the auxiliary model associated with model M and string. A sequential generative model can be represented by a probability tree where each branch in the tree represents a new token being generated. The model M as well as each of the auxiliary models can each be represented by such a probability tree. An example of a probability tree representing a generative model and a sample random string represented by a sequence of branches through the tree are shown in Fig. 1, which also shows the probability tree for one possible auxiliary model.\n\nA generative model uses a pseudo-random number generator (PRNG) that produces a sequence of random symbols to randomly choose a sequence of branches through its probability tree. As an example, suppose the symbols are independent and uniformly distributed in and a generative model uses it to generate the n’th token according to the distribution using a sampling algorithm such as the classical inversion method. Once we fix the PRNG and the sampling method, we can in principle write down a deterministic function to describe the generative model M as where recall that is the fixed initial prompt. Each of the auxiliary models can also similarly be described by deterministic functions.\n\nWe can then define a new generative model that we call the ensemble model as follows. Pick a pseudo-random sequence and generate a string from model M. Now consider the auxiliary model corresponding to the string. Generate a pseudo-random string from model where is another pseudo-random sequence. The ensemble model is defined by.\n\nThe following observation is crucial to our main result: a good random number generator will almost always generate pseudo-random sequences that belong to a typical set i.e. that satisfies the simple AEP in Proposition 1.\n\nTypical String. An infinite text string (or more precisely a sequence of text strings) is said to be typical for a model M if the log-perplexity of sufficiently long sub-strings converges to their empirical entropy for model M.\n\nWe illustrate these ideas for a simple Markov language model.\n\nAEP for Perplexity. For a long text string i.e. generated from a language model with a given prompt, with high probability, the log-perplexity is close to the empirical entropy. More precisely:\n\nLet denote the pseudo-random sequence that generated our string from model M. Thus we have. With high probability, is a typical sequence. Consider the ensemble model for model M with pseudo-random sequences and let. The classical AEP in Proposition 2 for model selected by the ensemble model requires that random strings generated by model must almost always satisfy:\n\nSince is almost always a typical sequence, we can set, in which case the output of the ensemble model is exactly the same as the output of model M. Setting we get and\n\nFinally setting gives and\n\nInserting into proves the result.\n\nProposition 3 asserts that with high probability a sufficiently long text string will be typical for the model that it was generated from. Note the crucial role in the above proof of the assumption that the pseudo-random sequence is typical. We can take this construct further to establish some non-asymptotic results as well. We expect from that the log-perplexity will be within a few standard deviations of its mean e.g. even when is not large enough for asymptotics to kick in. Finally, we will show that the set of typical strings is vanishingly small compared to all grammatically correct strings.\n\nLet denote the set of substrings of typical strings for a model M. The size of the “typical set” is an exponentially vanishing fraction of all grammatically correct sequences.\n\nIf we use top-k sampling, the model M generates possible grammatically correct values for each token. The number of grammatically correct strings is then lower-bounded as. Under the (mild) assumption that for a long string, at least some fraction of the token distributions are non-uniform, we have. By definition, the probability of model M generating string is, so we have for the probability of a typical string generated by model M:\n\nThe total number of typical strings cannot exceed.\n\nProposition 4 suggests that the typical sets of two different models, each set being vanishingly small, likely have zero overlap."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments with Open-Source Language Model",
            "text": "We performed a series of experiments to verify the ideas described in Section 3. The basic idea is to evaluate the empirical perplexity of generating a given string using a model’s conditional probability distributions and compare the result with the log-perplexity of the string calculated using the model’s probability distributions. Software code for the experiments is available at Bell and Mudumbai (2024)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results and Discussion",
            "text": "Figures 3(a) and 3(b) show the log-perplexity and empirical entropy for sub-strings for a string not generated by the other model. The string in Fig. 3(a) is a paragraph written by the authors for an early draft of this manuscript, and the string in Fig. 3(b) was generated by the Claude 3 Opus model from Anthropic (2024). While our theory does not require non-convergence of to , we do see from Fig. 3(a) and Fig. 3(b) that the log-perplexity is several standard deviations away from the empirical entropy. The contrast between Figs. 3 and 2 has obvious implications for AI text detection, model fingerprinting, and so on."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Anomalies and Limitations",
            "text": "While the results from our experiments described in Section 4 were always consistent with Proposition 3, they did reveal certain limitations of our setup that we now discuss.\n\nFundamentally, these limitations arise from cost and computational constraints. More powerful LLMs can generate longer texts and be less likely to get stuck in bad states that produce degenerate distributions and so on. In this work, all experiments were ran on a single desktop machine (Windows 11, Intel Core i9-14900K 3.20 GHz, 96.0 GB RAM, NVIDIA GeForce RTX 3090 Ti). Additional plots and more details about these experiments are presented in Section A.1."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We proposed an asymptotic property that must be satisfied by the perplexity of any long string generated by a language model and provided theoretical and experimental arguments in support of the property. While this paper focused narrowly on an exposition of the AEP property, it opens up many questions for further study. For instance, Proposition 4 suggests that different language models have distinct and unique “model signatures.” The contrast between the analyses implies potential applications for understanding and enhancing AI detection algorithms. As reported in our experimental section, our results are limited by computational constraints and can be strengthened by improved hardware and software. A broader question is to explore other statistical laws and constraints that generative models are bound by."
        }
    ]
}