{
    "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
    "abstract": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.\n\nThe rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide “common sense” reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.\n\nLarge language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model’s decision, and detect when it may be hallucinating.\n\nIn this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "It is an exciting time to be a machine learning and robotics researcher. A great deal of progress has been made in the last decade and a half with regards to the efficacy and efficiency of models for perception, decision-making, planning, and control (Soori et al., 2023; Janai et al., 2020). Broadly speaking, approaches to these problems fall under one of two umbrellas: hand-engineered model-based systems and data-driven learning-based models (Formentin et al., 2013). With some deployment scenario in mind, developers may hand-engineer rules (Hayes-Roth, 1985) or tune a controller (Borase et al., 2021) to be tested, or in the case of learning-based models, collect training data and craft some reward function to fit a model to an objective, given said data (Henderson et al., 2018). In practice, these methods work particularly well in the scenarios that they were specifically designed and trained for, but may produce undesirable results in previously unseen out-of-distribution cases (Wen et al., 2023). Designers may choose to add more rules, re-tune their controller, fine-tune their model to a more representative dataset, fix the reward function to handle edge cases, or even add a detector (which may itself be rule-based or data-driven) at test-time to identify out-of-distribution scenarios before calling on the decision-maker (Singer and Cohen, 2021; Schreiber et al., 2023; Chakraborty et al., 2023). However, even with these changes, there will always be other situations that designers had not previously considered which will come about during deployment, leading to sub-optimal performance or critical failures. Furthermore, the modifications made to the model may have unforeseen effects at test-time like undesired conflicting rules (Ekenberg, 2000) or catastrophic forgetting of earlier learned skills (Kemker et al., 2018). Informally, classical methods and data-driven approaches lack some form of common sense that humans use to adapt in unfamiliar circumstances (Fu et al., 2023a).\n\nRecently, researchers have explored the use of large (visual) language models, L(V)LMs, to fill this knowledge gap (Cui et al., 2024). These models are developed by collecting and cleaning an enormous natural language dataset, pre-training to reconstruct sentences on said dataset, fine-tuning on specific tasks (e.g., question-answering), and applying human-in-the-loop reinforcement learning to produce more reasonable responses (Achiam et al., 2023). Even though these models are another form of data-driven learning that attempt to maximize the likelihood of generated text conditioned on a given context, researchers have shown that they have the ability to generalize to tasks they have not been trained on, and reason about their decisions. As such, these foundation models are being tested in tasks like simulated decision-making (Huang et al., 2024b) and real-world robotics (Zeng et al., 2023) to take the place of perception, planning, and control modules.\n\nEven so, foundation models are not without their limitations. Specifically, these models have a tendency to hallucinate, i.e., generate decisions or reasoning that sound plausible, but are in fact inaccurate or would result in undesired effects in the world. This phenomenon has led to the beginning of a new research direction that attempts to detect when L(V)LMs hallucinate so as to produce more trustworthy and reliable systems. Before these large black-box systems are applied in safety-critical situations, there need to be methods to detect and mitigate hallucinations. Thus, this survey collects and discusses current hallucination mitigation techniques for foundation models in decision-making tasks, and presents potential research directions. Existing surveys particularly focus on presenting methods for hallucination detection and mitigation in question-answering (QA) (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023d; Ye et al., 2023) or object detection tasks (Li et al., 2023c). There are also other works that provide examples of current use cases of L(V)LMs in autonomous vehicles (Yang et al., 2023b) and robotics (Zeng et al., 2023; Zhang et al., 2023a). Wang et al. (2023a) perform a deep analysis of the trustworthiness of a variety of foundation models and Chen and Shu (2024) provide a taxonomy of hallucinations within LLMs, but both exclude applications to general decision problems. To the best of our knowledge, we are the first to propose a general definition of hallucinations that can be flexibly tuned to any particular deployment setting, including commonly found applications to QA or information retrieval, and more recent developments in planning or control. Furthermore, there is no"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Foundation Models Making Decisions",
            "text": "Originally coined by Bommasani et al. (2022), the term foundation models refers to models that are “trained on broad data at scale such that they can be adapted to a wide range of downstream tasks.” This approach is in contrast to works that design and train models on a smaller subset of data for the purpose of being deployed to a specific task Yang et al. (2024). The key difference is that foundation models undergo a pre-training procedure on a large-scale dataset containing information from a variety of possible deployment fields, through which they are expected to learn more general features and correspondences that may be useful at test-time on a broader set of tasks Zhou et al. (2023); Zhao et al. (2023). Examples of existing pre-trained foundation models span language Devlin et al. (2019); Brown et al. (2020); Touvron et al. (2023a), vision Caron et al. (2021); Oquab et al. (2024); Kirillov et al. (2023), and multi-modal Radford et al. (2021); Achiam et al. (2023) inputs. In this section, we give a brief overview of existing use cases for foundation models in robotics, autonomous vehicles, and other decision-making systems. We also succinctly point out hallucinations found in these works and leave a lengthier discussion in Section 3.2. Readers should refer to works from Yang et al. (2023b), Zeng et al. (2023), and Zhang et al. (2023a) for a deeper review of application areas."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Autonomous Driving",
            "text": "For the autonomous vehicle domain, researchers have formulated the use of language foundation models as a fine-tuning and prompt engineering problem. An external sub-system is usually designed with (1) a perception module to process signals from raw sensors, (2) a memory bank of prior important experiences and its corresponding similarity function to find alike scenarios, and (3) a prompt generator to convert current sensor data and relevant memories into natural language that can be input to the foundation model. Currently, works either fine-tune LLMs with a few examples, or directly apply the model in a zero-shot manner, on a QA task with driving related questions. By framing the task in a QA form, researchers have been able to provide context to the L(V)LM to probe for high-level natural language decisions Wen et al. (2023), path planning Mao et al. (2023); Sima et al. (2023), vehicle tracking and trajectory prediction Wu et al. (2023b); Keysan et al. (2023), descriptions of the surroundings of the vehicle Chen et al. (2023c); Xu et al. (2024), and low-level control Liu et al. (2023). Wen et al. (2024) propose DiLu, a framework consisting of reasoning, reflection, and memory modules that support an LLM in producing high-level decisions for autonomous driving, and they test their method within a driving simulator environment. Specifically, the reasoning module views the current observation of the vehicle, queries the memory module for any similar situations that were encountered in the past, and converts the experience into a prompt, which is input to the LLM. The prompt is formatted such that it elicits chain-of-thought reasoning Wei et al. (2022) from the LLM, which is shown to improve the accuracy of the model. The generated text output by the LLM is summarized by the reflection module, and is used to update the memory bank of experiences. A separate decision decoder model converts the summary into a discrete high-level decision (e.g., idle, turn right, accelerate, etc.) to take in the simulator. Agent-Driver from Mao et al. (2023) utilizes a tool library of functions that communicate with neural modules that are responsible for object detection, trajectory prediction, occupancy estimation, and mapping. The LLM is asked to reason about what information would be helpful to plan a path of the ego vehicle, and calls on functions from the tool library to iteratively build up relevant context. Like Wen et al. (2024), the authors use a memory bank of prior driving experiences to bolster the context provided to the LLM. With this context, the LLM predicts a suitable path for the ego vehicle to follow. If a collision is detected between the predicted trajectory and surrounding objects in the scene, the LLM undergoes self-reflection, like Reflexion Shinn et al. (2023), another hallucination mitigation technique, to fine-tune its prediction. Through a short study, the authors test the frequency of invalid, hallucinated outputs from the model, and find that their self-reflection approach results in zero invalid generations at test-time. Sima et al. (2023) build up context before predicting a path for the ego vehicle by asking a VLM questions about its perception of surrounding vehicles, predicting their behaviors, planning high-level ego vehicle decisions, converting to lower level discrete actions, and finally, estimating a coordinate-level trajectory. Their method, DriveLM-Agent, predicts paths from raw images in an end-to-end manner using a multi-modal approach, whereas Agent-Driver requires sensor modules to process context separately. Wu et al. (2023b) propose PromptTrack as a method to predict bounding boxes and trajectories of vehicles in multi-view camera scenes conditioned on a text prompt. PromptTrack is another end-to-end method that encodes multi-view images with an image encoder, decodes previously tracked boxes and current detections into new tracks, uses a language embedding branch to predict 3D bounding boxes, and updates the memory of current tracks using past & future reasoning branches. Rather than using ego vehicle point of view images for object tracking, Keysan et al. (2023) propose an approach to convert rasterized images from a birds-eye view of the scene into a prompt describing the past trajectories of vehicles with Bézier curves. This method combines vision and language encoders to generate the Bézier curve-based scene description, and elicits a language model to predict trajectories in a similar format. Works using foundation models for generating scene descriptions given multi-modal information frame the task as a QA problem. For example, Chen et al. (2023c) use a reinforcement learning (RL) agent pre-trained on the driving task in simulation to collect a dataset containing the vehicle’s state, environment observation (assumed to be ground truth from simulator,) low-level action, and the ego’s percentage of attention placed on"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robotics",
            "text": "Foundation models have been used in the robotics domain for object detection, affordance prediction, grounding, navigation, and communication. Ichter et al. (2023) address the issue of misalignment between a robot's capabilities and what a language model believes it is capable of performing. This gap, due to models not being trained with data from the specific robots they are deployed on, can lead to impractical outputs. The authors propose SayCan as a method to integrate the general knowledge of language models with the specific capabilities of a robot. An LLM is tasked with generating a list of smaller actions to complete a task, constrained by (1) the model's token probability distribution and (2) available robot skills, prioritizing actions with the highest combined probability of token generation and successful execution. \n\nPaLM-E, presented by Driess et al. (2023), is a multi-modal model that processes sensor inputs like images into token-space embeddings, integrating them with instruction embeddings for input into a PaLM language model. This setup enables the system to answer questions about the robot's environment or plan action sequences to complete tasks. Driess et al. emphasize the increased risk of hallucinations due to this multi-modal approach. \n\nInspired by success in using foundation models to generate programs, other works focus on deploying models to write low-level code for robots. Liang et al. (2023) introduce Code as Policies, where LLMs hierarchically generate interactive code and functions for robots. The models leverage existing function libraries or dynamically develop custom methods. Hu et al. (2024) propose the RoboEval benchmark to test robot-agnostic LLM-generated code via the CodeBotler platform, which provides abstract functions like “pick” and “place.” RoboEval Temporal Logic (RTL) checks if the generated code meets task and temporal constraints, also testing robustness by using paraphrased prompts. This consistency-checking is discussed further in Section 4.3.1.\n\nIn robot navigation, LM-Nav uses a vision-language model (VLM) to predict a sequence of waypoints based on language commands, as shown by Shah et al. (2023). They apply in-context learning to teach a model like GPT-3 to extract landmarks from instructions. Using CLIP, LM-Nav matches these landmarks to waypoint images. Dynamic programming then optimizes the landmark path, aiming to maximize instruction completion likelihood."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Other Areas",
            "text": "There are also other works that apply foundation models for decision-making outside of the robotics and autonomous vehicle domains. For example, ReAct from Yao et al. (2023b ###reference_b144###) identifies that a key limitation of chain-of-thought reasoning Wei et al. (2022 ###reference_b131###) is that the model does not update its context or action based on observations from an environment. As such, chain-of-thought reasoning relies purely on the internal reasoning of the foundation model itself to predict actions to take, missing a crucial step in grounding its actions with their effects on the environment. Given a prompt, ReAct iterates between an internal reasoning step and acting in the environment to build up context relevant to the task. Yao et al. (2023b ###reference_b144###) showcase the promise of the method in a QA setting where the LLM can take actions to query information from an external knowledge base, as well as an interactive text-based game, ALFWorld Shridhar et al. (2021 ###reference_b114###).\n\nChen et al. (2023b ###reference_b14###) admit that ReAct is a powerful tool for dynamic reasoning and grounding, but is limited by the fact that the updated context from the Act step is only helpful for the particular task the model is currently deployed for. They propose Introspective Tips to allow an LLM to reason about its past successes and failures in a world to generate general tips that will be helpful across diverse instruction-following tasks. Specifically, tips are generated from the past experience of the model from a similar set of tasks, from expert demonstrations, and from several games that differ from the target task. By summarizing these experiences into more concise tips, Chen et al. (2023b ###reference_b14###) show that Introspective Tips outperform other methods in ALFWorld with both few- and zero-shot contexts.\n\nPark et al. (2023 ###reference_b94###) and Wang et al. (2023b ###reference_b128###) apply foundation models in more complex environments to push models to their limits to simulate realistic human behaviors and test lifelong learning. Park et al. (2023 ###reference_b94###) propose generative agents that produce believable, human-like interactions and decisions within a small town sandbox environment. They develop a module for individual agents in the simulation to store and retrieve memories, reflect about past and current experiences, and interact with other agents. Their generative agents use similar methods to ReAct and Introspective Tips to act based on a memory of experiences, but also interact and build relationships with other agents through dialogue. The authors show that the agents are able to effectively spread information, recall what has been said to others and stay consistent in future dialogue interactions, and coordinate events together. Sometimes, however, agents are found to hallucinate and embellish their responses with irrelevant details that may be attributed to the training dataset of outside, real-world knowledge.\n\nKwon et al. (2023 ###reference_b66###) explore the use of LLMs to act as a proxy for a hand-tuned reward function in RL tasks. This application is particularly motivated by decision-making tasks that are difficult to specify with a reward function, but can be explained textually with preferences of how a policy should generally act. Specifically, the LLM evaluator first undergoes in-context learning with examples of how it should decide the reward in several cases of the task that the agent will be deployed to. Then, during RL training, the LLM is provided a prompt with the trajectory of the agent within the episode, the resulting state from the simulator, and the original task objective from the user, and is asked to generate a binary reward for the agent (1 if success, 0 else). The binary reward is added to the experience replay, and the agent can be updated using any RL algorithm. Kwon et al. (2023 ###reference_b66###) find that a baseline in their work that predicts rewards with no in-context learning especially hallucinates with incoherent reasoning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hallucinations",
            "text": "Even with all their success in a multitude of deployment areas, foundation models still produce inconsistent outputs, or hallucinate, at test-time. Here, we provide a general definition for hallucinations that can be applied to any foundation model deployment task, including various autonomous systems. Additionally, we give examples of hallucinations encountered in literature and discuss how they come about during testing.\n\nA work evaluating the frequency at which LVLMs hallucinate in their descriptions of images finds that these models’ outputs may include non-existent objects or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c). For example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage and predicts that the “table is neatly arranged, showcasing the different food items in an appetizing manner.” Although the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022).\n\nChen et al. (2021) explore alignment failures of LLMs applied to code completion tasks. The authors evaluate the likelihood of these models generating defective code given different input prompts and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand. The study also identifies similar model biases towards race, gender, religion, and other representations. Furthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner. These findings have been corroborated by other foundation model code generation works in the robotics domain. For example, Wang et al. (2023b) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo. Similarly, Hu et al. (2024) find that their model has a tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\n\nSeveral works focus on identifying cases of hallucinations in QA tasks. Although this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems. Common hallucinations in QA result in incorrect answers to questions. Another set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1 Mündler et al. (2024); Zhang et al. (2023b). Intuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output. Notice in this example, with relation to Definition 3.1, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks. As such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "What are hallucinations?",
            "text": "Across current literature on foundation models, there exist similar patterns and themes that can be used to develop a unified definition for hallucinations. With the majority of works studying this problem within QA tasks, where ground truth answers are available, several authors explain hallucinations as producing text that includes details/facts/claims that are fictional/misleading/fabricated rather than truthful or reliable. Works making use of a dedicated knowledge-base further describe hallucinations as generating nonsensical or false claims that are unsubstantiated or incorrectly cited. Some authors also present the idea that foundation models may sound syntactically correct, or coherent, while simultaneously being incorrect. Other works, who explore hallucinations of vision-language models (VLMs) in detecting and classifying objects within images, define hallucinations as generating object descriptions inconsistent with target images.\n\nA common theme among existing hallucination definitions for QA, information retrieval, and image captioning domains is that, while the generation may sound coherent, either the output is incorrect, or the model’s reasoning behind the generated text is incorrect. However, these characteristics on their own do not completely encompass the hallucinations found in decision-making tasks, thus requiring additional nuances. Within papers that apply foundation models to decision-making tasks specifically, researchers have encountered similar problems of hallucinations impacting performance. Hallucinations are described as predicting an incorrect feasibility of an autonomous system when generating an explanation behind the uncertainty of an action to take. Studies find that language models may provide incoherent reasoning behind their actions and that these generative models also have a sense of high (false) confidence when generating incorrect or unreasonable plans. In the case of robot navigation and object manipulation, hallucinations refer to attempting to interact with non-existent locations or objects.\n\nMetric frameworks for various tasks can be outlined as:\n\n- Questing-Answering: Generations must align with database facts; tone of answer should be informative; answers should not include references to unrelated topics. \n- Image Captioning: Objects in description must appear in image; censor descriptions for inappropriate images; descriptions should not be embellished with details that cannot be confirmed.\n- Planning: Predicted sub-task must be feasible to solve; plans should maximize expected return; predicted sub-tasks and actions should not stray from the end goal with unnecessary steps.\n- Control: Predicted action must be possible to perform; predict actions to complete plan efficiently.\n\nIn the code generation task, terms like “alignment failure” are used, with similar effects to those of hallucinations discussed above. More specifically, this is informally described as an outcome where a model is capable of performing a task but chooses not to. Issues arise when test-time prompts include even minor mistakes, leading models to continue to generate buggy outputs to match the input prompt.\n\nThese issues present across different applications suggest the need for a comprehensive definition of hallucinations that encompasses these diverse characteristics:\n\nA hallucination is a generated output from a model that conflicts with constraints or deviates from desired behavior in actual deployment, or is completely irrelevant to the task at hand, yet could be deemed syntactically plausible. There are three key aspects to consider:\n\n1. A generated output from a model.\n2. A deployment scenario to evaluate model outputs with:\n   - A list of constraints that must be consistent.\n   - A loose interpretation of a desired behavior.\n   - A set of topics relevant to the task.\n3. Metrics measuring consistency, desirability, relevancy, and syntactic soundness (plausibility) of generations.\n\nThis definition captures the qualities of hallucinations such as inconsistency, undesirability, irrelevancy, and plausibility, which can be applied flexibly to any deployment scenario by choosing appropriate metrics for each characteristic."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Examples",
            "text": "Regardless of the weather and driving conditions, difficulties are identified in detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself. Additional irrelevant (or completely false) details about other agents may be presented when not prompted. Describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle’s perspective also pose challenges. Hallucinations arise in these complex environments due to the high variability in driving scenarios. Even after applying hallucination mitigation techniques like chain-of-thought reasoning, undesired outputs persist.\n\nIn evaluating LVLMs, models’ outputs may include non-existent objects or additional irrelevant phrases that are not possible to test for accuracy. For example, in a picture of food on a table, a model hallucinates a non-existent beverage and predicts that the “table is neatly arranged, showcasing the different food items in an appetizing manner.” Although errors in classification and generation in this case are not critical, there are warnings of potential failures with more severe societal impacts, such as biases leading to marginalizing users.\n\nResearch explores alignment failures of LLMs applied to code completion tasks by evaluating the likelihood of generating defective code given different input prompts. In-context learning with buggy code examples increases the chance of poor generations. Similar model biases towards race, gender, religion, and other representations are identified. Additionally, the model, Codex, can generate code that may aid in the development of insecure applications or malware, though in a limited scope. Such findings are corroborated in code generation works within the robotics domain.\n\nAuthors describe models generating code with references to non-existent items and actions that are inappropriate or contradictory. For example, models might call functions with invalid objects, attempt actions with inappropriate timing, or generate undesired behaviors like asking for help when no one is near.\n\nResearch in identifying cases of hallucinations in QA tasks highlights incorrectly generated answers. Such hallucinations are categorized into closed-domain (irrelevant information given context) and open-domain (incorrect claims without context). Fine-tuning helps reduce but does not eliminate hallucinations completely. Additional insights are drawn from contradictions among several sampled generations from an LLM. If a context leads to conflicting outputs, some form of hallucination is likely occurring. This aspect aligns with the concept of testing for consistency among multiple generations rather than against a fixed knowledge base, allowing flexibility across system setups by describing consistency, desired behavior, and relevancy."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Why do they happen?",
            "text": "There are several speculations as to how hallucinations come about during deployment. First and foremost, like any learning task, foundation models are sensitive to biases in training data (Rawte et al., 2023). Once a model is trained on a given large dataset, some facts may become out-of-date or stale at any point in time (Puthumanaillam et al., 2024). Furthermore, as the training set is embedded into a smaller encoding dimension, the knowledge within a large language model’s frozen parameters is lossy, and models cannot feasibly be fine-tuned every time there is new data (Peng et al., 2023; Elaraby et al., 2023). Zhang et al. (2023b) recommend changing algorithm parameters at runtime, such as temperature (spread of probability distribution of next token), top-k sampling (narrows the set of next tokens to be considered), and beam search (choosing a set of possible beams, i.e., trajectories, of next tokens based on high conditional probabilities), but the process of tuning these parameters is expensive. To combat out-of-date training data, some works provide models with an external knowledge-base of information to pull facts from, with the hope of increasing model accuracy. Even with this up-to-date information, Zhang et al. (2023c) pose that there may exist a misalignment between the true capabilities of a model, and what a user believes the model is capable of, leading to poor prompt engineering. In fact, poor prompting is one of the most significant causes of hallucinations. Chen et al. (2021) find that poor quality prompts lead to poor quality generations, in the context of code completion. This phenomenon is attributed to the reconstruction training objective of large language models attempting to maximize the likelihood of next generated tokens, given context and past outputs, i.e., where context is an input to the model, there is an output sequence of tokens, and any generated token is conditioned on previously generated tokens. As the public datasets these models are trained on contain some fraction of undesirable generations (e.g., defective code), the models become biased to generate similar results under those inputs. Qiu et al. (2023) show that this limitation can actually be exploited to push foundation models to generate toxic sentences, or completely lie, by simply rewording the prompt. While foundation models condition generated tokens on ground-truth text without hallucinations at train time, during inference, the model chooses future tokens conditioned on previously (possibly hallucinated) generated text. As such, Chen et al. (2023d) and Varshney et al. (2023) state that generated outputs are more likely to contain hallucinations if prior tokens are hallucinated as well. Furthermore, Li et al. (2023a) find that, even if prompt context provided to a foundation model is relevant, the model may choose to ignore the information and revert to its own (possibly outdated or biased) parameterized knowledge. Overall, the hallucination detection task is highly complex with several possible sources of failures that need to be considered at test-time. Chen and Shu (2024) validate the complexity of the detection problem with studies identifying that human- and machine-based detectors have higher difficulty correctly classifying misinformation generated from large language models than those written by other people."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Detection and Mitigation Strategies",
            "text": "Hallucination detection and mitigation methods can be classified into three types (white-, grey-, and black-box) depending on the available inputs to the algorithm. Generally, given some context, a foundation model outputs a predicted sequence of tokens, the corresponding probabilities of each token, and embeddings of the generation from intermediate layers in the network. White-box hallucination detection methods assume access to all three output types, grey-box require token probabilities, and black-box only need the predicted sequence of tokens. Because not all foundation models provide access to their hidden states, or even the output probability distribution of tokens, black-box algorithms are more flexible during testing. In this section, we present existing detection and mitigation approaches clustered by input type. While several of these works show promise in QA and object detection settings, many of them require further validation on decision-making tasks, and we will point out these methods as they come about. Works in this section are summarized in Table 2."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "White-box Methods",
            "text": "Methods in this section require access to internal weights of the model for hallucination detection."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Hidden States",
            "text": "Some approaches utilize intermediate embeddings at different network layers. Azaria and Mitchell (2023) empirically find that language models attempt to correct themselves after outputting an untruthful claim. They hypothesize that the internal states of a model must have some understanding of whether the output is correct. Furthermore, the authors stray away from directly using the token probabilities, even though they have correlation with model accuracy Varshney et al. (2023), because the complete output sentence’s probability is dependent on the length of the generation and appearance frequency of tokens.\n\nAzaria and Mitchell (2023) present SAPLMA, a simple classifier trained with supervised learning, that takes the activation values of a hidden layer of an LLM as input, and outputs the probability of the generated claim being true. SAPLMA is shown to be able to identify untruthful outputs, even when trained on a held-out dataset on a completely different topic than evaluated on.\n\nYao et al. (2023a) aim to test the resiliency of foundation models to varying prompts. They propose perturbing an input prompt with additional tokens so as to make an LLM under test produce a desired hallucination (e.g., modify the original query, “Who won the US election,” to get the LLM to generate, “Donald Trump was the victor,” while its original response was correctly stated as, “Joe Biden was the victor”). As the search space of possible tokens to add/replace when developing the adversarial prompt is massive, the work uses a gradient-based token replacing strategy. Specifically, they define an objective that attempts to find trigger tokens in the direction of the gradient of the likelihood that maximizes the probability of the model outputting the desired hallucination. With simple prompt modifications, the authors show that the white-box approach is able to induce the specified hallucinations.\n\nLUNA, introduced by Song et al. (2023), is a general framework that measures the trustworthiness of an LLM output containing four stages of evaluation: model construction, semantic binding, quality metrics, and practical application. The abstract model construction phase attempts to profile the LLM using its hidden states with either a discrete time Markov chain (DTMC) or a hidden Markov model (HMM) architecture. For example, when fitting a DTMC model, the authors encode the hidden states of the language model into a lower dimensional space, cluster them into abstract discrete states, and learn a transition function between said states. Semantic binding is used alongside quality metrics to identify the states and transitions that are trustworthy, and which ones are undesired. Finally, at inference time, as the model generates output tokens to a given prompt, the intermediate network layer embeddings are iteratively passed through the profiling model to identify when undesired transitions occur. The authors evaluate their framework’s capability of detecting hallucinations within QA datasets."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Attention Weights",
            "text": "Attention weight matrices, prominent within transformer model architectures, signify the importance the model places on earlier tokens within a generation when predicting future tokens. OPERA, proposed by Huang et al. (2024a), is a hallucination detection method for LVLMs that utilizes the model’s internal attention weights. When visualizing the attention matrix, the authors find peculiar column patterns that align with the beginning of a hallucinated phrase. These aggregation patterns usually occur on a non-substantial token like a period or quotation mark, but are deemed to have a large impact on the prediction of future tokens. As such, this finding led Huang et al. (2024a) to modify the beam search algorithm Freitag and Al-Onaizan (2017) by applying a penalty term to beams wherever an aggregation pattern is detected, and roll back the search to before the pattern arises. Their method is shown to reduce hallucinations and even eliminate possible repetitions in generations."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Honesty Alignment",
            "text": "In addition to methods that require hidden states or attention matrices, we also include methods that fine-tune foundation models to better communicate their uncertainty to questions under white-box algorithms, as they require access to model weights for training. For example, Lin et al. (2022a) collect a calibration dataset of questions and answers from GPT-3 under multiple types of tasks (e.g., add/subtract and multiply/divide), and record how often each task is incorrectly answered. They aim to fine-tune the LLM to also output its certainty that the prediction is correct. Consequently, Lin et al. (2022a) fine-tune the model with data pairs of a question and the empirical accuracy on the task that the question originates from in the calibration dataset, such that the model is expected to similarly output a probability of accuracy at test-time. The authors show that the proposed verbalized probability in deployment does correlate with actual accuracy on the tasks. Yang et al. (2023a) take the method one step further by also training the model to refuse to answer questions with high uncertainty."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Grey-box Methods",
            "text": "Grey-box approaches leverage the probability distributions of tokens output from the model. These approaches can provide a more fine-grained understanding of model behavior and help in debugging model outputs. By examining token probabilities, researchers and developers can identify unexpected outputs and refine model behavior, ensuring more accurate and reliable language model performance. This method is especially useful in contexts where the model's decision-making process needs to be transparent and interpretable. Overall, grey-box approaches represent a valuable tool in the optimization and evaluation of language models."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Concept Probabilities",
            "text": "Empirically, Varshney et al. (2023) show that there is a negative correlation between hallucination rate and token probability (i.e., as a token’s probability decreases within a sentence, the tendency to hallucinate increases). Thus, the authors rely on token probabilities to estimate uncertainty of concepts within a generated claim, and they check for correctness by cross-referencing a knowledge-base. Whenever a concept is found to be conflicting with a fact through verification questions, their method attempts to mitigate the error by prompting the LLM to replace the incorrect claim with the evidence. Although effective in the QA setting, Varshney et al. (2023) concede that, in the event token probabilities are not available, some form of heuristic must be used to detect hallucination candidates. Zhou et al. (2024) show that external models can be developed to automatically clean hallucinations. The authors tackle the issue of object hallucinations that LVLMs experience when describing the content of images. Through theoretical formulations, the authors show that LVLM responses tend to hallucinate in three settings: when described object classes appear frequently within a description, when a token output has low probability, and when an object appears closer to the end of the response. As such, their model, LURE, is a fine-tuned LVLM trained on a denoising objective with a training dataset that is augmented to include objects that appear frequently within responses, and replacing objects with low token probabilities or appearing close to the end of the response, with a placeholder tag. At inference time, tokens are augmented similarly to how they were changed to generate the training dataset, and the LURE LVLM is prompted to denoise hallucinations by filling in uncertain objects. SayCanPay, proposed by Hazra et al. (2024), builds off of the SayCan framework Ichter et al. (2023) to improve the expected payoff of following a plan specified by a language model. Within our hallucination definition, this goal translates to increasing the desirability of generations by improving the likelihood of the model achieving higher rewards. The authors propose three different strategies for planning: Say, SayCan, and SayCanPay. Say methods greedily choose next actions based only on token probabilities. SayCan approaches also take the success rate of the chosen action into consideration. Finally, SayCanPay additionally estimates the expected payoff from following the plan with some heuristic. Hazra et al. (2024) learn this Pay model with regression on an expert trajectory dataset. Combining all three models together minimizes the likelihood that a generated plan contains conflicting infeasible action calls, while maximizing the efficiency of the task completion."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Conformal Prediction",
            "text": "Another range of works estimate the uncertainty of a model output with conformal prediction so as to provide statistical guarantees on the likelihood of predictions being correct Shafer and Vovk (2008  ###reference_b110###). Quach et al. (2024  ###reference_b100###) propose conformal language modeling to build a set of possible candidate responses to a test prompt, while calibrating algorithm parameters on a held-out dataset of independent prompts and their corresponding admission functions, which check whether a model output meets the criteria of an input prompt. In their algorithm, the authors calibrate thresholds for three separate scoring functions that test for generation quality, similarity with other responses, and model confidence using “Learn then Test” Angelopoulos et al. (2022  ###reference_b3###).\n\nKumar et al. (2023  ###reference_b64###) similarly apply conformal prediction to LLMs, but for answering multiple choice questions. Specifically, the method first collects a calibration dataset of prompts and the normalized token probabilities of the correct token (i.e., A, B, C, or D) being chosen from the model. Then, during deployment, given a user-defined error rate and a prompt, their algorithm chooses the multiple choice answers with token probabilities that fall within the calibrated score on the held-out dataset.\n\nWhile the previous hallucination mitigation works presented using conformal prediction are solely applied to QA settings, Ren et al. (2023  ###reference_b105###) are the first to apply conformal prediction of foundation models to robotic tasks. The authors are motivated by a desire for language-conditioned robots to understand when they are uncertain about the next action to take, such that they can ask for help in those cases (while minimizing frequency of clarifications). Because LLM generations with different length sequences inherently produce different complete sentence probabilities, the authors propose framing the control task as a multiple-choice problem, like Kumar et al. (2023  ###reference_b64###). Their approach, KnowNo, prompts an LLM to generate a possible set of next actions to take in multiple choice form.\n\nLiang et al. (2024  ###reference_b75###) extend the KnowNo methodology by incorporating an introspective planning step using a previously constructed knowledge-base of experiences, which tends to (1) enhance quality of generated plans, and (2) improve interpretability of decisions. Specifically, introspective planning first constructs a knowledge-base containing training pairs of tasks, observations, and valid plans, which the LLM is prompted to generate explanations behind why they are reasonable. Each experience is stored with a key as an embedding of the original instruction. During inference, given a new test instruction, their method queries the database to find the key with the closest embedding to that of the new instruction. This previous experience and reasoning is fed into the model to generate a set of candidate plans to follow. Finally, the remainder of the algorithm follows the same process as KnowNo to calibrate and narrow down the prediction set to fall within a desired error rate.\n\nWang et al. (2024  ###reference_b129###) aim to provide additional guarantees on completing the task provided within the natural language instruction. To do so, the authors propose a novel task specification, LTL-NL, which combines linear-temporal-logic (LTL) descriptions with natural language from a user instruction, which the authors claim is easier to define than classical LTL specifications. Given this specification, a symbolic task planner chooses a sub-task to complete next and an LLM generates plans for each sub-task, respectively. Like Ren et al. (2023  ###reference_b105###) and Liang et al. (2024  ###reference_b75###), Wang et al. (2024  ###reference_b129###) apply conformal prediction to minimize the number possible actions to take next within some desired error rate. However, rather than directly asking a user for assistance when there is high uncertainty in the next action to take (or when there are environmental constraints), their method, HERACLEs, samples a new sub-task to complete from the task planner. If on the other hand, the task planner is unable to provide a new sub-task, HERACLEs requests help from the user. With experimentation, the authors find that their method achieves higher task completion rate on missions requiring more sub-tasks, outperforming baseline planners that do not utilize LTL specifications."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Black-box Methods",
            "text": "Black-box algorithms only rely on the input prompts and output predictions from the model, without making assumptions on the availability of the hidden state, nor the token probabilities."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Analyzing Samples from Model",
            "text": "Several works examine sampling multiple responses from a language model (LLM) and measuring the similarity of information present in all samples. For instance, SelfCheckGPT, by Manakul et al. (2023), evaluates consistency among varied responses through different methods, one of which uses BERTScore to compute similarity between sentences from different outputs. Hallucinations are detected when the similarity is low across samples. Other approaches include using multiple-choice QA systems, training classifiers to predict contradictions, and prompting the LLM to evaluate mutual support among sampled responses. They find that sampling more responses enhances the estimation of a claim's validity but slows computation. Elaraby et al. (2023) focus on computing entailment among responses at the sentence level. Their method, HaloCheck, employs the SummaC entailment estimation method, offering a balanced prediction score. HaloCheck is efficient in score prediction.\n\nDu et al. (2023) propose an ensemble approach, where multiple agents debate the correct answer to a question using an iterative process, incorporating chain-of-thought reasoning to reduce hallucinations and improve accuracy. This method, though tested on static datasets, could be applied in simulation frameworks for debates on planning.\n\nPark et al. (2024) introduced CLARA for predicting ambiguous or infeasible instructions to robotic systems using LLMs. CLARA samples concepts from a prompt to create multiple inputs, examining output action similarity and checking goal feasibility. It requests clarification if uncertain but feasible. CLARA performs well in robotic tasks with real instructions yet occasionally hallucinates during uncertainty reasoning.\n\nSome approaches focus on identifying explicitly contradictory responses, enhancing response validity by removing conflicts. Mündler et al. (2024) propose finding important concepts, generating additional information on them, and evaluating sentence consistency. They revise conflicting sentences before output. Dhuliawala et al. (2023) utilize chain-of-thought reasoning, prompting an LLM to generate verification questions about responses and regenerating outputs if conflicts arise.\n\nIn image captioning, Li et al. (2023c) improve accuracy estimation for LLMs, proposing POPE to handle stability issues in accuracy metrics. POPE curates binary questions about image objects, flagging models hallucinating objects based on conflicting answers.\n\nXiong et al. (2024) explore LLMs expressing uncertainty verbally through prompt engineering, aiming for models to provide confidence levels. They implement chain-of-thought reasoning to predict confidence scores for sub-claims, integrating these for overall confidence assessment. Their hybrid approach combining verbalized uncertainty and self-contradiction detection enhances calibration error performance compared to individual methods."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Adversarial Prompting",
            "text": "Works specializing in adversarial prompting attempt to test the robustness of models to varying inputs that may coerce the model into producing out-of-distribution results. For example, Mehrabi et al. (2023) apply adversarial prompting to text-to-image foundation models, like Stable Diffusion, to generate offensive images. With respect to their framework, FLIRT is essentially testing the tendency of foundation models to hallucinate undesired generations in deployment. FLIRT uses an adversarial language model to predict a prompt to input to the image generator, scores the generated image for the presence of undesirable traits using an external classifier, re-prompts the adversary to produce a new instruction conditioned on the findings of the classifier, and repeatedly generates images until the adversary successfully prompts the test model to output an undesirable result. Mehrabi et al. (2023) define objective functions conditioned on the score output by external classifiers to maximize diversity of adversarial prompts and minimize toxicity to pass text filters that detect malicious inputs, while improving attack effectiveness.\n\nAnother work from Yu et al. (2023) presents the AutoDebug framework for automatically sampling and updating several prompts for use in adversarial testing of the language model. The authors specifically explore adversarial testing under the case that the model predicts a correct response when provided relevant context, but generates an incorrect prediction when the evidence is modified. They apply two different modification approaches: replacing tokens within the context to provide incorrect facts, and adding additional relevant facts to the prompt that may make it difficult to pick out the most important details. All in all, adversarial prompting is an effective method for identifying robustness of models to unseen inputs, which can be used to develop stronger input filters or fine-tune the model for decreased hallucination tendency."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Proxy Model",
            "text": "Certain black-box works rely on an external, proxy model to detect and mitigate hallucinations. One such method is used as a baseline within the SelfCheckGPT article Manakul et al. (2023 ###reference_b85###). As many language foundation models do not provide access to token probabilities, the authors use an open-source proxy LLM that does provide token probabilities as an estimate of the original output’s probability. They find that using proxy LLMs for probability estimation and hallucination detection successfully is highly variable. The accuracy of detection is dependent on the complexity of the LLM itself, as well as the training data of the proxy LLM (i.e., models trained on independent datasets from the original LLM will have different generation patterns).\n\nWithin this section, we also include works that use an external trained classifier to detect hallucinations. For example, Chen et al. (2023d ###reference_b17###) curate a dataset of QA dialogue from LLM generated responses. They apply a composition of metrics to assess quality of responses, including a self-assessment from the LLM comparing the ground-truth and predicted text, human-labeled, and machine metrics (e.g., BERT score, F1 score, BLEU, etc.). Their hallucination discriminator, RelD, is trained on the dataset in multiple separate phases, each using a different objective: regression, multi-class classification, and finally binary classification. Through experiments, they find that RelD closely aligns with human evaluators’ original predictions.\n\nSimilarly, Pacchiardi et al. (2024 ###reference_b92###) develop a black-box lie detector for LLMs. In their case, the authors hypothesize that models that output a lie will produce different behaviors in future responses, like Azaria and Mitchell (2023 ###reference_b4###). As such, at inference time, Pacchiardi et al. (2024 ###reference_b92###) prompt the LLM with several binary questions (that may be completely unrelated to the original response) and collect yes/no answers. All the responses are concatenated into a single embedding that is input to the logistic regression model to predict the likelihood that the response was untruthful. The authors find that the simple detector is mostly task- and model-agnostic once trained on a single dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Grounding Knowledge",
            "text": "In knowledge grounding tasks, a language model is tasked with identifying evidence from an external knowledge-base that supports claims within a summary. Although seemingly irrelevant to decision-making scenarios, similar methods discussed in this section may be applied in planning tasks to identify observations that are most relevant to predicting the next action or to generate reasoning behind a specified plan.\n\nPURR, proposed by Chen et al. (2023a), is a denoising agent, like LURE, that is trained in an unsupervised fashion given evidence from online sources, a clean (correct) summary, and a noisy (hallucinated) summary. The model learns to denoise the incorrect summary to the clean statement. During deployment, given a possibly hallucinated claim, a question generation model queries online sources for evidence about the claim, and PURR generates a cleaned version of the original summary with said evidence.\n\nSome knowledge grounding approaches prompt LLMs to generate code to directly query information from databases. Li et al. (2024) are motivated by the limitations of existing knowledge-based hallucination mitigation methods; namely that (1) they utilize a fixed knowledge source for all questions, (2) generating retrieval questions with LLMs that interface with a database is not effective because they may not be trained on the particular programming language of the database, and (3) there is no correction capability that handles error propagation between knowledge modules. Consequently, the authors propose augmenting LLMs with heterogeneous knowledge sources to assist with summary generation. Specifically, in the event that the model is found to be uncertain about its generated statement through self-contradiction, their framework, chain-of-knowledge (CoK), chooses subsets of knowledge bases that may be helpful for answering the original question. Assuming each database has its own query generator, CoK queries for evidence and corrects rationales between different sources iteratively. Compared to chain-of-thought reasoning, CoK consistently produces more accurate answers with its iterative corrections.\n\nAnother source of potential conflict that leads to hallucinations is misalignment between a model’s capabilities and the user’s beliefs about what it can do. Zhang et al. (2023c) tackle this knowledge alignment problem and categorize alignment failures into four types: Semantic — an ambiguous term maps to multiple items in a database, Contextual — the user failing to explicitly provide constraints, Structural — user provides constraints that are not feasible in the database, Logical — complex questions that require multiple queries. Their proposed MixAlign framework interacts with the user to get clarification when the LLM is uncertain about its mapping from the user query to the database. With the original query, knowledge-base evidence, and user clarifications, the LLM formats its final answer to the user.\n\nPeng et al. (2023) aim to add plug-and-play modules to an LLM to make its outputs more accurate since these large foundation models cannot feasibly be fine-tuned whenever there is new information. Their work formulates the user conversation system as a Markov decision process (MDP) whose state space is an infinite set of dialogue states which encode the information stored in a memory bank, and whose discrete action space includes actions to call a knowledge consolidator to summarize evidence, to call an LLM prompt engine to generate responses, and to send its response to the user if it passes verification with a utility module. The proposed LLM-Augmenter has a memory storing dialogue history, evidence from the consolidator, set of output responses from an LLM, and utility module results. Its policy is trained in multiple phases with REINFORCE Williams (1992) starting with bootstrapping from a rule-based policy designed from domain experts, then learning from simulators, and finally, from real users. The authors find that access to ground-truth knowledge drastically improves QA results, and feedback from the utility module and knowledge consolidator help to provide more accurate answers to users.\n\nEvaluated in actual decision-making settings, Introspective Tips from Chen et al. (2023b) provide concise, relevant information to a language planner to learn to solve more efficiently. Intuitively, summaries that collect information over all past experiences may be long and contain unnecessary information. In contrast, tips are compact information with high-level guidance that can be learned from one’s own experiences, from other demonstrations, and from other tasks in a similar setting. Chen et al. (2023b) show that providing low-level trajectories is less effective than tips on simulated planning tasks. Additionally, with expert demonstrations, the LLM learns faster with fewer failed trials than with just past experience alone. However, one limitation identified in the study is that the LLM underperforms in unseen, low-difficulty missions where it has issues generating general tips for zero-shot testing."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Constraint Satisfaction",
            "text": "There is also additional work in creating black-box algorithms for ensuring decision plans generated by foundation models meet user-defined goal specifications and system constraints, like their grey-box counterpart developed by Wang et al. (2024  ###reference_b129###).  \nBecause these models under test provide their results in text form, it is natural to apply formal method approaches (e.g., satisfiability modulo theory, SMT, solvers) to verify the satisfaction of generated plans.  \nFor example, Jha et al. (2023  ###reference_b54###) prompt an LLM planner with a problem formulated with first order constraints to predict a set of actions to complete the task.  \nThe output plan is input to an SMT solver to check for any infeasibilities in the program, and any counterexamples found are used to iteratively update the prompt and generate new plans.  \nThis counterexample approach is much faster than relying on combinatorial search methods that find a plan from scratch.  \nHowever, the quality of generated plans and the number of iterations before a successful plan is generated are heavily dependent on the LLM generator itself, with similar reasons to the proxy-model used by Manakul et al. (2023  ###reference_b85###).  \nAnother work from Hu et al. (2024  ###reference_b48###) develops a RoboEval benchmark to test generated plans on real robots, in a black-box manner.  \nLike Wang et al. (2024  ###reference_b129###), the authors introduce their own extension of LTL formulations, known as RTL, which specifies temporal logic at a higher, scenario-specific, level, while abstracting away constraints that are not dependent on available robot skills.  \nRTL and LTL-NL are easier to read and define than classic LTL methods.  \nRoboEval utilizes the provided RTL formulation of a problem, a simulator, and evaluator to systematically check whether the output meets requested goals.  \nFurthermore, to check for robustness of the model to varied instructions, Hu et al. (2024  ###reference_b48###) hand-engineer paraphrased sentences within an offline dataset that should ideally result in the same task completion.  \nPrimary causes of failures were found to be a result of generated code syntax/runtime errors, attempting to execute infeasible actions on the robot, and failing RTL checks.  \nLike adversarial prompting approaches, testing generated plans on robots in diverse scenarios enable researchers to design more robust systems that hallucinate less frequently at test-time."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Metrics and Evaluation Platforms",
            "text": "We now present common metrics, datasets, and simulation platforms leveraged when developing and evaluating the hallucination detection algorithms introduced in Section 4  ###reference_###.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###).\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "Here, we list established metrics used for computing language similarity and accuracy of generated image descriptions.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Language Similarity",
            "text": "Given a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Object Detection",
            "text": "CHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Offline Datasets",
            "text": "In this section, we present relevant offline datasets used for evaluating the performance of hallucination detection and mitigation techniques in driving, robotic, and QA tasks.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Driving",
            "text": "BDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Code Generation and Robotics",
            "text": "HumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Question-answering",
            "text": "HotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Simulation Platforms",
            "text": "Finally, we introduce common online simulators used to test hallucination detection methods for decision-making tasks.\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Driving",
            "text": "The developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###  ###reference_b130###)."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Robotics",
            "text": "Ravens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Other Simulators",
            "text": "TextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": "Here, we discuss some possible future directions in hallucination detection and mitigation techniques for foundation models to improve deployments to decision-making tasks.\nMost hallucination detection approaches are currently evaluated in offline QA settings for information retrieval or knowledge alignment, as seen in Table 2  ###reference_###.\nAs foundation models are increasingly used for more complex tasks, researchers should make an effort to adapt and evaluate earlier detection/mitigation approaches that were applied to QA problems in these modern applications.\nAlthough dissimilar in practice from QA settings, planning and control problems may be formulated in such a way that enables these earlier mitigation methods to be evaluated on decision-making tasks.\nFor example, as discussed in Section 2.1  ###reference_###, Chen et al. (2023c  ###reference_b15###) treat the autonomous driving task as a QA problem, which could be naturally extended to test other QA hallucination detection methods in the same setting.\nThis evaluation may lead to greater understanding of the general limitations of these models, as we draw parallels across diverse deployments.\nWhite- and grey-box detection methods may not generally be applicable in situations where the internal state or token probabilities are unavailable from the language model.\nThus, we predict black-box approaches will take precedence in the near future, as state-of-the-art LVLMs like GPT-4V already prohibit access to probability outputs.\nHowever, current black-box methods are limited with simplistic sampling techniques to gauge uncertainty, and proxy models may not be representative of the true state of the model under test.\nWorks like FLIRT (while only applied to image generation models) showcase the promise of black-box adversarial prompting approaches in generating undesirable results from models Mehrabi et al. (2023  ###reference_b87###).\nWe argue developing more aggressive black-box adversarial generative models, which explicitly optimize for producing inputs that may perturb the system outputs, is key to identifying the limits of a foundation model’s knowledge.\nCurrently, foundation models are primarily deployed to decision-making tasks that likely have some relation to its training set.\nFor example, although complex, tasks like multi-agent communication, autonomous driving, and code generation will be present in training datasets.\nOn the other hand, dynamic environments like robot crowd navigation require identifying nuances in pedestrian behaviors which the model may not have explicitly seen during training.\nPushing the limits of foundation model deployments will allow researchers to find areas for growth in other applications.\nWith the explosion of LVLMs, which allow for explicit grounding of natural language and vision modalities, further exploration should be performed in evaluating their effectiveness in decision-making systems.\nWen et al. (2023  ###reference_b133###) take a step in the right direction towards testing black-box LVLMs in offline driving scenarios, but there is still work to be done in deploying these models in online settings.\nThis direction can shed light on the long-standing debate of whether modular or end-to-end systems should be preferred in a particular deployment setting.\nIn this survey, we provide a glimpse into the progress of research into evaluating hallucinations of foundation models for decision-making problems.\nWe begin by identifying existing usecases of foundation models in decision-making applications like autonomous driving and robotics, and find several works make note of undesired hallucinated generations in practice.\nBy referencing works that encounter hallucinations across diverse domains, we provide a flexible definition for hallucinations that researchers can leverage, regardless of the deployment scenario in mind.\nFinally, we give a taxonomy of hallucination detection and mitigation approaches for decision-making problems, alongside a list of commonly used metrics, datasets, and simulators for evaluation.\nWe find that existing methods range in varying assumptions of inputs and evaluation settings, and believe there is much room for growth in general, black-box hallucination detection algorithms for foundation models."
        }
    ]
}