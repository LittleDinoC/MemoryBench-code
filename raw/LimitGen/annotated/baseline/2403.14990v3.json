{
    "title": "MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness",
    "abstract": "This paper presents the MasonTigers’ entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches to semantic textual relatedness across 14 languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from  to  in Track A, from  to  in Track B, and from  to  in Track C. Adhering to the task-specific constraints, our best performing approaches utilize an ensemble of statistical machine learning approaches combined with sentence transformers.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In this modern era of information retrieval and NLP, understanding semantic relatedness is fundamental for refining and optimizing diverse applications. Semantic relatedness refers to the degree of similarity and cohesion in meaning between two words, phrases, or sentences. Semantic relatedness allows systems to grasp the contextual and conceptual connections between words or expressions. Various NLP tasks and applications can benefit from modeling semantic relatedness such as question answering, knowledge transfer, text summarization, machine translation, and content recommendation.\n\nWhile significant research has been conducted on semantic relatedness in English, more recently the interest in semantic relatedness in other languages has been steadily growing. This reflects an increasing awareness of the need for developing models for languages other than English. NLP is evolving rapidly and we have been witnessing the emergence of language-specific transformer models, the release of datasets for downstream tasks in diverse languages, and the development of multilingual models designed to handle linguistic diversity.\n\nSemEval-2024 Task 1 - Semantic Textual Relatedness aims to determine the semantic textual relatedness (STR) of sentence pairs across 14 diverse languages. Track A focuses on nine languages (Algerian Arabic, Amharic, English, Hausa, Kinyarwanda, Marathi, Moroccan Arabic, Spanish, Telugu) using a supervised approach where systems are trained on labeled training datasets. Track B adopts an unsupervised approach, prohibiting the use of labeled data to indicate similarity between text units exceeding two words.\n\nThis track encompasses 12 languages (Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Modern Standard Arabic, Moroccan Arabic, Punjabi, and Spanish). Track C involves cross-lingual analysis across the 12 aforementioned languages. Participants in this track must utilize labeled training data from another track for at least one language, excluding the target language. Evaluation across all three tracks involves using Spearman Correlation between predicted similarity scores and human-annotated gold scores. We conduct distinct experiments for each track using statistical machine learning approaches along with the embeddings generated by transformer-based models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Understanding the level of semantic relatedness between two languages has been regarded as essential for grasping their meaning. Notable studies on the topic including Agirre et al. (2012 ###reference_b7###, 2013 ###reference_b8###, 2014 ###reference_b5###, 2015 ###reference_b4###, 2016 ###reference_b6###); Dolan and Brockett (2005 ###reference_b18###) and Li et al. (2006 ###reference_b30###) have introduced datasets like STS, MRPC, and LiSent. These datasets have been pivotal in advancing research in tasks such as text summarization and plagiarism detection. Finding semantic relatedness and semantic similarity, as well as determining sentence pair similarity using existing datasets or paired annotation, are integral in understanding the nuances of language comprehension. Previous studies describe how words and sentences are perceived to convey similar meanings Halliday and Hasan (2014 ###reference_b23###); Morris and Hirst (1991 ###reference_b33###); Asaadi et al. (2019 ###reference_b11###); Abdalla et al. (2021 ###reference_b1###); Goswami et al. (2024 ###reference_b21###). Methodologies like paired comparison represent the most straightforward type of comparative annotations Thurstone (1994 ###reference_b44###), David (1963 ###reference_b17###). Best-Worst Scaling (BWS) Louviere and Woodworth (1991 ###reference_b32###) a comparative annotation schema, offer insights into methods for evaluating relatedness through pairwise comparisons. The utilization of these methods aids in generating ordinal rankings of items based on their semantic relatedness. Kiritchenko and Mohammad (2016 ###reference_b27###, 2017 ###reference_b28###) highlight the effectiveness of such techniques, emphasizing the importance of reliable scoring mechanisms derived from comparative annotations for understanding the intricacies of semantic relatedness in natural language processing tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "The shared task comprises three tracks: Supervised, Unsupervised, and Cross-Lingual. The dataset Ousidhoum et al. (2024a) is comprised of two columns: the initial column, labeled \"text,\" containing two full sentences separated by a special character, and the second column, labeled as \"score,\" which includes the degree of semantic textual relatedness for the corresponding pair of sentences. In the supervised track (Track A), there are 9 languages, and for each language, train, dev, and test sets are provided. The specifics of the dataset for this track can be found in Table 1.\n\nIn the unsupervised track (Track B), there are 12 languages, and for all the languages, dev and test set is provided. The details of the dataset of this track are available in Table 2.\n\nFinally, in the cross-lingual track (Track C), there are 12 languages, and for all the languages, dev and test set is provided, and they are the same as the unsupervised track. Here the training dataset is not provided. Hence, for each individual language of this track, we select 5 languages from the supervised track (different from the target language) and merge training data of those five languages to create the training dataset for each of the languages of the cross-lingual track. The details of the dataset of this track are available in Table 3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use statistical machine learning to find the sentence embeddings and predict relatedness between pair of sentences. Additionally, we use sentence transformers for the supervised track. Our experiments are described in detail in the next sections."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Track A - Supervised",
            "text": "For each training embedding, we calculate the cosine similarity between the pairs. After that, we apply ElasticNet and Linear Regression separately on these embeddings and predict the relatedness of the sentence pairs in the development phase. We clip the predicted values to ensure the prediction range from 0 to 1. In the development phase, we find the Spearman Correlation Coefficient of these predictions (each by ElasticNet and Linear Regression). Finally, we perform a weighted ensemble depending on the Spearman Correlation Coefficient of the predicted results and get our ensembled Spearman Correlation Coefficient in the development phase. We also perform this approach on the test data and find our best Spearman Correlation Coefficient in the evaluation phase."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Track B - Unsupervised",
            "text": "In the unsupervised track, we calculate cosine similarity Rahutomo et al. (2012  ###reference_b41###) between the pairs of development data embeddings. In the development phase, we find the Spearman correlation Myers and Sirois (2004  ###reference_b34###) and perform an average ensemble of the calculated results to get our ensembled Spearman correlation. We also apply this approach to the test data to determine our best Spearman correlation in the evaluation phase."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Track C - Cross-Lingual",
            "text": "For each language in the cross-lingual track, we select 5 different languages from the Supervised Track to use as training data. The details of the language selection are provided in Table 3. We apply ElasticNet and Linear Regression separately on these embeddings and predict the similarity of the sentence pairs in the development phase. We clip the predicted values to ensure the prediction range from 0 to 1. In the development phase, we find the Spearman correlation of these predictions and perform an average ensemble of the predictions to get our ensembled Spearman correlation in the development phase. We also perform this approach on the test data and find our best Spearman correlation in the evaluation phase."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "For all the tracks, ensemble of the predictions prove helpful in terms of achieving better Spearman correlation.\nFor Track A sentence transformer LaBSE along with Linear Regression performs the best among the eight combinations for all the languages. Then the weighted ensemble improves the result 1% - to 3% in development phase and 1% - 2% in evaluation phase - depending on the languages. For English this method performs the best in terms of ranking with  rank while the worst for Moroccan Arabic with  rank. On test Spearman correlation, English is the best securing 0.84 and Kinyarwanda is the worst with 0.37. Detailed results are shown in Table 4  ###reference_### of Appendix.\nFor Track B, the average ensemble improves the result 0% - to 3% in development phase and 0% - 2% in evaluation phase - depending on the languages. For Kinyarwanda this method performs the best in terms of ranking with  rank while the worst for English with  rank. On test Spearman correlation, English is the best securing 0.77 and Punjabi is the worst with 0.02. Detailed result is shown in Table 5  ###reference_### of Appendix.\nFor Track C embedding generated by language specific (unrelated to target language) models provide the best result among the six combinations for all the languages. Then the average ensemble improves the result 0% - to 2% in both development and evaluation phases depending on the languages. For Punjabi this method performs the best in terms of ranking with  rank while the worst for Hausa and Kinyarwanda with  rank. On test Spearman correlation, Spanish is the best securing 0.56 and Punjabi is the worst with 0.02. Detailed result is shown in Table 6  ###reference_### of Appendix."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Error Analysis",
            "text": "###figure_1### ###figure_2### ###figure_3### For Track A, Algerian Arabic, Moroccan Arabic and Spanish test Spearman Correlation Coefficient decreases in the evaluation phase. This happens because the dev set was around 7.5%-9% and the test set is around 39% - 46% size of the train set.\nFor Track B, amount of dev data was only 20 for Afrikaans which is the reason of a very big difference between the result of development and evaluation phase. Algerian Arabic, Amharic, Modern Standard Arabic, Moroccan Arabic have a very small amount of dev data (less than 100) which is reason of decreased Spearman Correlation Coefficient in the evaluation phase. Hindi also faces the same issue but as it had more dev data the test Spearman Correlation Coefficient is only 4% less than the development period.\nFor Track C, Algerian Arabic, Indonesian, Kinyarwanda, Modern Standard Arabic faced bigger drop of the Spearman Correlation Coefficient from the development phases. Also the diversity of the train and test data make it more challenging to score better Spearman Correlation Coefficient. In addition, due to the unavailability of the text label, only the ensemble performance of Spanish language for all the tracks are shown.\nRegarding the result of the Punjabi language in the both unsupervised and cross-lingual track, it was the most challenging language where the provide baseline was less than zero. Though our system achieves 0.02 Spearman Correlation Coefficient for for this language, the ranking is quite impressive which also proves the struggle of other teams to cope up with this language.\nMoreover, ElasticNet and Linear Regression exhibit limitations as assumption of linearity may not align with the intricate and nonlinear relationships inherent in the textual data. The issue of dimensionality poses a challenge, especially when dealing with a large number of features. The difference between the gold and predicted semantic relatedness scores for the three tracks are shown in Figure 1  ###reference_###, Figure 2  ###reference_###, and Figure 3  ###reference_###."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We experimented with various methodologies on the dataset provided by the organizers, including statistical machine learning approaches. In the supervised task (Track A), with no restrictions on the model or data, we utilized the available training dataset. Conversely, the unsupervised task (Track B), lacking training data, presented challenges, leading us to use statistical machine learning approaches. The cross-lingual track (Track C) imposed more stringent restrictions, requiring us to use training data from other languages in Track A, excluding the target language. We show that our ensemble approach exhibited superior performance compared to individual model experiments. However, the task’s inherent difficulty became evident in instances where relatively small datasets presented challenges for effective model learning. Semantic textual relatedness tasks face challenges like subjectivity, context dependency, and ambiguity due to multiple meanings and cultural differences. Limited data, domain specificity, short texts, and biases hinder accuracy. Ongoing research is crucial to address these limitations and improve model accuracy."
        }
    ]
}