{
    "title": "GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots",
    "abstract": "Geospatial Copilots unlock unprecedented potential for performing Earth Observation (EO) applications through natural language instructions. However, existing agents rely on overly simplified single tasks and template-based prompts, creating a disconnect with real-world scenarios. In this work, we present GeoLLM-Engine, an environment for tool-augmented agents with intricate tasks routinely executed by analysts on remote sensing platforms. We enrich our environment with geospatial API tools, dynamic maps/UIs, and external multimodal knowledge bases to properly gauge an agent’s proficiency in interpreting realistic high-level natural language commands and its functional correctness in task completions. By alleviating overheads typically associated with human-in-the-loop benchmark curation, we harness our massively parallel engine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse multi-tool tasks and across 1.1 million satellite images. By moving beyond traditional single-task image-caption paradigms, we investigate state-of-the-art agents and prompting techniques against long-horizon prompts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "With the advent of generative AI, Large Language Models (LLMs) have the potential to significantly enhance Earth Observation (EO) workflows across a broad range of tasks, from detection to learning from spatio-temporal data to analyzing aerial, UAV, and satellite images and videos. However, existing approaches consider predefined low-level template-based prompts that only capture the textual surface form of the predicted image-caption pairs, while often overlooking the functional agent correctness at completing high-level natural language (NL) commands. While the need for more representative benchmarks has been underscored across several generative AI domains, their significance is even greater in the geospatial domain, as it involves complex multimodal data across diverse spatial and temporal dimensions.\n\nSuch disconnect partly stems from prevailing perceptions that benchmark creation, focused on simplistic single-task prompts, is straightforward. However, the overhead extends beyond merely curating benchmarks with image-caption pairs, a task that can be programmatically accomplished, as evidenced by the rapid release of numerous geospatial benchmarks over recent months. Instead, the challenge lies in establishing an environment equipped with the requisite tools, dynamic UIs, and real-world APIs to form the “engine” for developing complex tasks. In this work, our key insight is that amidst the abundance of “geospatial benchmarking” works, a subtle refocus is necessary, prioritizing the construction of a robust engine as the foundation for benchmark creation, rather than the benchmarks themselves.\n\nWe draw inspiration from novel work that introduces environment-based benchmarking suites for comprehensive agent assessment. While these works highlight potential in their domains, adopting them for geospatial applications necessitates overcoming human-in-the-loop bottlenecks, particularly in manual ground-truth verification and template creation. To mitigate these challenges, we employ formal-language-based verification techniques, recently introduced to expedite labor-intensive Reinforcement Learning from Human Feedback (RLHF) workflows.\n\nIn this work, we introduce GeoLLM-Engine, a highly realistic environment that captures real-world tasks on EO platforms. Our environment comprises various fully operational APIs and dynamic map/web UIs to execute geospatial tasks via high-level NL prompts. More importantly, we employ model-correctness checker techniques that allow our “back-end” engine to autonomously verify the accuracy of generated benchmarks, requiring only a one-off initial validation of task templates. By reducing the necessity for human intervention, we can massively parallelize our benchmark suite across 100 GPT-4-Turbo nodes to create large-scale benchmarks with 100,000 prompts that span half a million tasks over 1.1 million images from open-source EO datasets.\n\nEach GeoLLM-Engine prompt exhibits high-level intent that emulates the nuances and abstract language usage patterns typically employed by human operators, as shown in Fig. 1. Using this benchmark, we follow state-of-the-art evaluation schemes with tool-augmented fine-tuning-free agents in zero-/few-shot in-context learning modality, powered by the latest GPT-3.5 and GPT-4 Turbo (0125) versions. Capturing our key insight, our findings reveal that merely expanding LLM benchmarks with more tasks of uniform complexity (e.g., an excess of visual QA captioning tasks) does not significantly enrich our understanding, as agent performance predictably shows little variation. Conversely, we show that varying levels of complexity better assess agent performance. To this end, we diversify the scope of our benchmark by incorporating various remote sensing (RS) applications over different satellite imagery sources and tasks of escalating intents, ranging from document knowledge retrieval to UI/Web interactions to geospatial data analytics."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "GeoLLM-Engine Environment",
            "text": "Our aim is to establish a realistic environment that clearly advances beyond current benchmarks, featuring a self-contained web UI with a varied array of LLM tools and an integrated benchmarking engine (Fig. 2  ###reference_###). Built upon open-source libraries and APIs, GeoLLM-Engine is designed as a reproducible and scalable platform to support the development and evaluation of geospatial agents.\nEnvironment - “Front-end”:\nA key challenge in creating such an environment is the need for reproducibility and comparability across different systems and methodologies. To address this, we leverage a suite of open-source APIs, enabling the seamless integration of a wide array of tools, datasets, and functionalities, while facilitating transparency and accessibility. We intend to release our codebase and benchmark to foster advancements in geospatial Copilots.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### Tool Space:\nWe equip our environment with a comprehensive array of open-source Python packages, catering to various functionalities from data analytics to LLM-specific tasks, such as employing LangChain for FAISS [8  ###reference_b8###] embeddings in knowledge retrieval applications. The interface leverages Mapbox APIs for interactive mapping, while rasterio and geopandas facilitate advanced manipulation of geospatial data. The complete module inventory, comprising over 175 tools, is presented in Tab. 1  ###reference_###. This diverse toolkit enables us to execute complex tasks ranging from satellite imagery analysis to utilizing vector storage for rapid geographic data querying.\nEngine - “Back-end”:\nTo facilitate large-scale experimentation, GeoLLM-Engine incorporates a Command-Line Interface (CLI) alongside its UI, providing comprehensive tooling and scripting capabilities. This feature is crucial for conducting extensive investigations, allowing us to efficiently run model-verification checks on over half a million tasks and evaluate baseline agents against our benchmarks. Thanks to this setup, these operations are completed within hours, leveraging hundreds of GPT endpoints. The CLI, in tandem with the UI, provides diverse modalities essential for replicating the intricate demands of geospatial analysis tasks which require the integration of disparate data sources and analytical techniques.\nEnvironment Formulation:\nOur environment is represented as , comprising the environment state , the action space , and the tool space  (Tab. 1  ###reference_###). To intuitively understand , consider a user query “zoom into the Indo-Pacific region and show me the vessels during November 2021.” Upon query completion,  will encapsulate not just the visual or textual response, but also the altered state of the map, such as its zoom level or position, the loaded database, and the temporal window of displayed data.\nWhile detailed notation of transition functions extends beyond our study’s scope, previous research [52  ###reference_b52###] has established these functions as deterministic. This implies a critical property: given the same starting state , any two agents executing an identical sequence of actions or tools will invariably arrive at the same final state . This deterministic nature is vital for our purposes, as it allows for the verifiability of any agent’s solution against the benchmark’s “ground truth” specifications [43  ###reference_b43###]. We exploit this feature in our model checker (elaborated in Sec. 3  ###reference_###) in two principal ways. Firstly, given a “golden” ground truth, we can deterministically assess the functional correctness of any candidate solution that follows the same sequence of actions or tools. Secondly, this principle underpins our approach to benchmark (ground-truth) generation: by evaluating multiple agent solutions, a consensus on the final state among the majority indicates a high likelihood of an accurate ground truth, hence eliminating the need for human inspection.\nUser Intent Formulation:\nIntuitively, each user intent is encapsulated by four parts: the question  that triggers the agent, the executed tool sequence , the agent’s textual response  to the user, and the concluding environment state . Thus, we can express each task as . The sequence  is defined by the set of tool , where at each step  the agent invokes tool . As shown later in Sec. 3  ###reference_###, by contrasting the task set  with a gold standard , we can ascertain the functional correctness across our entire benchmark."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "GeoLLM-Engine Benchmark Suite",
            "text": "In this section, we first describe the process for “grounding” high-level natural language instructions into a structured set of task templates and user intents that cover all the GeoLLM-Engine tools. Next, we discuss our GPT-driven, human-out-of-the-loop ground-truth sampling approach.\nIntent Collection:\nThe initial phase involves utilizing human annotators (our team members) to craft a small set of user intents, following the qualitative guidelines set forth in [52  ###reference_b52###]. These intents are designed to be both nuanced and high-level, requiring the agent to perform more than one or two actions, and should be decomposable into a series of interchangeable templates. While this step is manual, please note that it represents the primary (and essentially the only) offline step necessitating human involvement. Through this procedure, we generate the foundational intents and templates, serving as the modular components from which benchmark queries are constructed.\nThe annotators are instructed to input detailed prompts, utilizing zero-shot GPT-4-Turbo to propose solutions (as highlighted in our Results, GPT-4 demonstrates notable zero-shot capabilities). Utilizing “User Feedback” UI buttons, annotators identify and confirm correctly executed model responses, hence collecting the “correct” examples along with the corresponding agent actions. By promoting scenarios that necessitate an average of 7-8 tool interactions, we can cover the entire tool space within 250 instantiated queries. Following the granular intent categories as in [52  ###reference_b52###], we classify the various prompts into the categories shown in Tab. 2  ###reference_###:\nInformation Seeking: Queries for knowledge retrieval aimed at sourcing information from wikis and documents to support EO investigations.\nUI/Web Navigation: Commands designed for UI interaction, such as opening web search results or displaying images corresponding to detections mapped out.\nLoad-Filter-Plot: Operations to load data, apply specific filters, and present geospatial findings in an insightful manner, for example, through change-detection heatmaps or land cover classification (LCC) categories.\nTool Templates:\nGiven the 250 instantiated queries and GPT’s solutions, we programmatically parse all json responses and remove duplicates, obtaining a distinct template for every tool agent-call, i.e.,  we have the GPT json-call . These json responses furnish us with the “building blocks” that can be used for benchmark sampling in a straightforward intuition: leveraging the recently introduced json_mode feature in OpenAI’s APIs, we can initiate queries to a standalone GPT model. Within this context, we present the model with a specific tool interaction template and pose inquiries akin to: “Given your prior action of zooming into X and the corresponding function call, generate the new function call to instead focus on Vienna.” This allows us to dynamically generate tool-specific commands tailored to new, contextually relevant scenarios.\nGPT-Driven Benchmark Creation:\nTo efficiently scale our benchmark generation, we capitalize on two key attributes of GPT-4-Turbo agents: the extended input token lengths and their ability to parse extensive databases described through SQL-like schemas. We leverage the expanded token capacity and we incorporate directly into the model’s context all tool-calling json templates for the 174 tools (manually verified in the previous step). Moreover, we compile the metadata (e.g., coordinates, dates, categories, document titles, etc.) associated with all satellite images and documents into a SQL table. From this, we randomly select 1,000 entries, collecting their categories, coordinates, and dates sets and providing them as SQL schemas to GPT. Last, we append the 250 manually crafted queries as “successfully sampled” examples, and alongside in-context “benchmark creation” instructions, this entire prompt is fed to GPT-4, prompting it to autonomously generate both a suggested task prompt and the corresponding solution, forming a new query task .\nTo circumvent the need for manual verification of each solution’s accuracy, we introduce a novel approach that integrates functional model checking with the principle of LLM self-consistency [40  ###reference_b40###]. Drawing from the concept where an agent repeatedly solves the same prompt, typically converging on the correct solution, we apply this by having GPT-4 propose multiple solutions to its self-generated (original) prompt. After 10 iterations, we execute these solutions in our engine (in-parallel via our CLI tools) to ascertain the final state  for each. If 9 out of 10 solutions converge on the same end state, we consider this to be a verified ground truth and incorporate it into our dataset. Overall, this novel self-consistency sampling scheme allows us to streamline the benchmark generation process without extensive human intervention.\nModel-Checker Formulation:\nGeoLLM-Engine incorporates a rigorous set of model checks to determine both the functional correctness and overall success of an agent’s response to a given prompt. As shown in Tab. 2  ###reference_###, correctness checks the agent’s tool usage  against the corresponding ground-truth , assessing argument accuracy within function calls (e.g., missing a required tool or calling the right tool with wrong parameters). Success checks the final system state produced by the agent’s sequence of actions  against the expected ground-truth state . We leverage our engine to “run” both the sequences starting from the same initial state to confirm whether the agent’s output and ground-truth match. Note the distinction between the two failure cases; they are not mutually inclusive. For example, an agent may erroneously invoke an unnecessary tool (“Function Error”), yet this may not alter the final state, which could still align with the anticipated result.\nAgent Evaluation Metrics:\nBased on the model checks, we can define the appropriate metrics that we subsequently use to evaluate the performance of different agents:\nSuccess rate: the ratio of successfully completed tasks across the entire benchmark as defined by  (Tab. 3  ###reference_###). This ratio informs us of the degree to which the agent is able to complete tasks, irrespective of whether it took incorrect or unnecessary intermediate steps.\nCorrectness rate: the ratio of correct function-call operations across the benchmark as defined by the  error types. Given the total number of errors and ground-truth tools, we compute the correctness ratio  [53  ###reference_b53###, 28  ###reference_b28###], which captures how likely it is for the agent to invoke the correct functions in the expected order.\nROUGE score: ROUGE-L recall score [21  ###reference_b21###] to compare final model replies  with the ground truth .\n###figure_13###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Remote Sensing Datasets",
            "text": "We consider several open-source data sources encompassing tasks like object detection, land cover classification, and visual question answering to build a collective dataset of 1,149,612 images. All data sources have coordinates and time metadata which offer us global spatio-temporal coverage in our benchmark prompts (Fig. 3  ###reference_###):\nxView1 [19  ###reference_b19###]: 846 high-resolution images sourced from WorldView-3 satellites focusing on overhead object detection with 1 million objects and 60 classes.\nxView2 [11  ###reference_b11###]: dataset for building damage assessment with 5,598 images sourced from the Maxar/DigitalGlobe Open Data Program before and after 19 natural disasters with 850,736 building annotations.\nxView3 [31  ###reference_b31###]: a dataset of 23,432 SAR GRD images from the Sentinel-1 mission annotated for (fishing) vessel detections to study illegal fishing practices.\nSARFish [27  ###reference_b27###]: extends the xView3-SAR GRD dataset by providing products from the Sentinel-1 C-band SAR satellite constellation operated by the European Space Agency’s (ESA) Copernicus Program in both real-valued GRD and complex-valued SLC product types.\nFAIR1M [39  ###reference_b39###]: object detection dataset with 24,775 images with over 1 million instances sourced from Gaofen satellites and Google Earth.\nFunctional Map of the World (FMoW) [5  ###reference_b5###]: a multi-label LCC dataset for buildings/land use with 727,144 images from over 200 countries.\nBigEarthNet [38  ###reference_b38###]: 344,385 Sentinel-2 LCC images from the CORINE Land Cover database."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Tab. 4 summarizes the performance of state-of-the-art GPT-based agents with various prompting schemes. In addition to the LLM metrics, we report the agent’s performance with respect to the underlying object detection (F1 score), LCC (Recall), and visual question answering (VQA, Rouge-L) tasks in our benchmark. These metrics provide insights into each agent’s efficiency, accuracy, and responsiveness in executing geospatial tasks.\n\nGPT versions:\nWe observe that there’s a marked difference in performance between models based on the GPT-3.5 Turbo and GPT-4 Turbo frameworks, with the latter generally achieving higher scores across all metrics. This improvement underscores the advancements in model understanding and task execution capabilities. This finding is consistent with most of the work in language guided agents [4]. As anticipated, the few-shot configurations outperform their zero-shot counterparts across all evaluated methods. This trend underscores the value of providing models with a few examples to adapt to specific tasks, significantly enhancing their ability to accurately interpret and respond to complex geospatial queries.\n\nModel Cost: An interesting observation is the lack of a clear correlation between the tokens consumed and the success or correctness rate. This finding suggests token usage does not directly translate to higher performance, challenging the assumption that more extensive responses might yield better results. These insights collectively highlight the nuanced dynamics of model performance within the GeoLLM-Engine benchmark.\n\nSuccess Rate vs. Task Complexity:\nDelving into the relationship between success rate (SR) and task complexity provides further insights into agent performance. As shown in Fig. 4, there is an inverse relationship between SR and the number of tool calls required to answer a prompt. Specifically, note how for tasks requiring a single tool call, SR exceeds 95%, while for more complex tasks involving more than eight tool calls SR is below 70% and even dropping below 27% for certain agents. This finding highlights a critical limitation of current geospatial agents: while agents can handle simpler tasks with relative ease, their performance degrades as the task complexity increases. We emphasize the need for benchmarks that measure agents’ real-world utility against complex geospatial scenarios.\n\nSuccess Rate vs. Benchmark size:\nNext, we conduct ablations varying dataset size from 500 to 10,000 tasks (Fig. 4): for all methods, the success rates remain relatively unchanged across this range. In a significant escalation of our testing regime, we scaled our GeoLLM-Engine benchmark to 100,000 queries covering over half a million tool calls (Tab. 5). For this experiment, which is the largest in this domain to our knowledge, we leverage the massively parallel nature of our engine over 100 GPT endpoints. Despite the tenfold increase, the success rate of GPT-4 showed relative stability, suggesting that merely increasing the benchmark size does not necessarily challenge the agents more or provide a better assessment of their capabilities. Instead, it is the task complexity within the benchmarks that presents a significant factor in evaluating geospatial performance. These findings critique the prevalent approach in recent works where the focus has been on scaling the benchmark size, while our findings highlight that increasing the complexity of tasks is more essential for assessing agent performance.\n\nSuccess Rate vs. Error tolerance:\nLast, we capture GeoLLM-Engine’s parameterizable nature that allows us to evaluate agents based on varying error thresholds by properly updating the model-checkers functions. The incremental increase in success rates with the relaxation of error thresholds reflects scenarios allowing for a certain margin of error or applications where absolute precision is often unattainable (e.g., nuanced definition of what a user means when asking “Show me all car detections in Madrid”; whether this assumes to include suburban areas or not might vary per user)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent advancements in autonomous agents span from reinforcement learning platforms to web-based applications and benchmarks for web interactions. VisualWebArena represents a milestone of LLM benchmarking, offering realistic tasks to evaluate multimodal web agents towards improving performance on complex web pages. The adoption of multimodal models for EO tasks is gaining momentum. Innovations like SkyEyeGPT and Remote Sensing ChatGPT showcase advancements in integrating VQA agents and computer vision models with RS imagery for enhanced multimodal responses. However, existing benchmarks often rely on predefined, single-step text-image prompts. GeoLLM-Engine allows us to assess agents’ ability to execute nuanced EO tasks."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations and Future Work",
            "text": "We recognize limitations within our framework. First, using GPT-4 for both generating and evaluating ground truths could introduce bias risks, as highlighted by benchmarking work [52  ###reference_b52###]. We are currently enhancing sampling diversity by leveraging hybrid strategies that incorporate both GPT-generated outputs and programmatic elements [53  ###reference_b53###]. Second, while our platform emulates complex tasks reflective of EO analysts’ workflows, we emphasized depth (long-horizon tasks) over breadth in task complexity. We are actively expanding the capabilities of our engine to incorporate a wider variety of tasks (e.g., from maritime traffic analysis [2  ###reference_b2###] to illegal fishing [1  ###reference_b1###] to damage assessment [33  ###reference_b33###]) leveraging GeoLLM-Engine’s flexible APIs. Furthermore, we have focused on assessing finetuning-free agents, which is why GeoLLM-Engine doesn’t specify train-val-test splits. Following recent work on tuning LLMs on EO tasks [49  ###reference_b49###], we are expanding our benchmark to tool-agents training [14  ###reference_b14###]. Last, we have utilized standard LLM APIs without explicitly optimizing for cost (e.g., latency or token usage). We are currently orthogonal optimizations to enhance GeoLLM-Engine efficiency, such as LLM compilers [16  ###reference_b16###], dynamic tooling [9  ###reference_b9###], and token compression [15  ###reference_b15###]."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce GeoLLM-Engine, a novel environment for evaluating geospatial Copilots, designed to bridge the gap between simplistic benchmarks and the complex demands of Earth Observation (EO) applications. By leveraging a rich array of geospatial API tools, dynamic interfaces, and a massive parallel processing framework over 100 GPT-4-Turbo nodes, our environment facilitates the execution of over half a million multifaceted tasks across 1.1 million satellite images. This advancement not only highlights the limitations of existing benchmarks but also sets a new standard for the development and evaluation of AI agents in the geospatial domain. Looking forward, GeoLLM-Engine paves the way for future research to explore sophisticated EO tasks, promising significant strides toward realizing the full potential of geospatial Copilots."
        }
    ]
}