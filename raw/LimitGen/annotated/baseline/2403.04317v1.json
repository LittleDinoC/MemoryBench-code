{
    "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
    "abstract": "Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning which substitutes the optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. Code is available at: https://github.com/jihoontack/MAC.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs; Brown et al., 2020; Touvron et al., 2023) have significantly accelerated progress in natural language processing (NLP) and thus become a core technology in various real-world applications used by millions of users, such as coding assistants (Chen et al., 2021), search engines (Xuan-Quy et al., 2023), and personal AI assistants (Gao et al., 2023). However, LMs are typically static artifacts, and as the world changes, the knowledge encoded in their parameters becomes outdated. This becomes especially problematic for large language models (LLMs), as multiple applications (e.g., Chatbots; Kim et al. 2021; OpenAI 2022) require the model to be up-to-date. Unfortunately, updating LLMs with new documents not only requires high computational demands (Jang et al., 2022a), but also carries with it the likely risk of catastrophic forgetting (McCloskey & Cohen, 1989; French, 1999; Kirkpatrick et al., 2017; Schwarz et al., 2018), thus making it a challenging problem.\n\nTo tackle this issue, multiple studies suggested online and continual learning frameworks for LMs, i.e., adapting the LM on a stream of new documents. Here, one line of work proposes to use retrieval-augmented models by saving the stream of documents into the memory and learning a retrieval system to select the most relevant document based on the input (Chen et al., 2017; Karpukhin et al., 2020). However, even large models often fail to update their memorized predictions when the retrieved document consists of counterfactual information (Longpre et al., 2021; Li et al., 2022; Si et al., 2023). Moreover, retrieval-augmented models may not be suited for edge computing as a large number of documents itself poses expensive computation and storage costs for model inference (Hu et al., 2023).\n\nDue to these limitations, another line of recent works suggests updating the model’s knowledge by finetuning on a stream of documents to directly update the knowledge inside the LM (i.e., online finetuning; Lazaridou et al., 2021; Jang et al., 2022b). For instance, recently, Hu et al. (2023) showed that online finetuning can be effective when the model is auto-regressively finetuned with the re-weighted token importance, where they propose to meta-learn a token weighting LM. While effective, online finetuning schemes also face limitations such as a large computation for gradient calculation of LM, sensitivity of the online optimization hyper-parameter (Hu et al., 2023), and the aforementioned catastrophic forgetting problem.\n\nIn this paper, we instead ask: Can we tackle the limitations of retrieval augmented models and the online finetuning techniques by updating the model’s parameters efficiently while retaining the knowledge learned from the online documents?\n\nTo this end, we suggest bridging the gap between the two frameworks through a complementary learning systems approach by introducing an end-to-end differentiable auxiliary retrieval augmentation system which can be run alongside the target LM. This system extracts knowledge from incoming documents, builds a memory approach and learns the automated selection of relevant information from the memory bank which is subsequently passed as additional modulations to the target model.\n\nWhile intuitive, optimizing such modulations also (i) requires a gradient computation and (ii) is non-trivial to define, since the optimization objective at test time must work in the absence of labels (e.g., no ground truth supervision information about the salient aspects of a document). To this end, we take inspiration from the amortization-based meta-learning literature (Garnelo et al., 2018b; Requeima et al., 2019), which learns to predict modulations through amortized optimization (i.e., substituting an optimization process with an encoder forward pass; Amos et al., 2023; Lorraine et al., 2023) and learns to extract the knowledge without the need for label-access at test time.\n\nContribution. We propose Memory of Amortized Contexts (MAC), an efficient and effective online learning framework for static LMs (see the overview in Figure 1). The core idea of MAC is to freeze the parameter of the LM and instead edit the LM by using a predicted Parameter Efficient FineTuning (PEFT) modulation, which captures relevant knowledge from hitherto unseen documents. Specifically, we utilize amortization-based meta-learning to compress a new document’s information into a compact modulation where such modulation maximizes the task performance of the adapted LM (e.g., question-and-answer ability). Then, we learn to aggregate documents represented in feature space into a single modulation based on a given question. During the online adaptation stage (or test-time), we thus"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Amortization-based meta-learning.\nAmortization-based meta-learning, which encodes the given context to directly predict the task-specific model, has gained much attention due to its computational efficiency as it only requires a single encoder forward pass when adapting the model (Santoro et al., 2016  ###reference_b69###; Mishra et al., 2018  ###reference_b51###; Garnelo et al., 2018b  ###reference_b19###, a  ###reference_b18###). These approaches, typically when combined with modulation techniques, have achieved notable success in various applications, such as few-shot visual recognition (Requeima et al., 2019  ###reference_b64###; Bronskill et al., 2021  ###reference_b7###) and 3D reconstructions (Guo et al., 2023  ###reference_b21###; Kim et al., 2023  ###reference_b36###). Recently, such a paradigm has been extended to language domains where prior works facilitate hypernetworks to adapt LMs with given few-shot prompts (Phang et al., 2023  ###reference_b57###; Ivison et al., 2023  ###reference_b29###). In this paper, we extend the use of amortization-based meta-learning to extract the knowledge of the given document into a compact yet informative modulation for online adaptation.\nOnline learning.\nOnline learning, also referred to as continual or lifelong learning, is a task of adapting models to new data or task distributions (Thrun & Mitchell, 1995  ###reference_b76###). Recently, due to the emergence of real-world applications using large models, online learning became important for multiple domains, including vision (Titsias et al., 2020  ###reference_b77###), multi-modal learning (Garg et al., 2023  ###reference_b17###), and RL (Schwarz et al., 2018  ###reference_b70###). In the language domain, there have been various attempts to tackle online learning (Kuhn, 1988  ###reference_b40###; Yogatama et al., 2014  ###reference_b87###; Rei, 2015  ###reference_b62###) where recent studies focus more on online learning of LLMs, e.g., finetuning on a stream of documents (Lazaridou et al., 2021  ###reference_b42###), architectural constraints (Jang et al., 2022b  ###reference_b33###), and the use of replay buffers (Dhingra et al., 2022  ###reference_b13###). Among them, Hu et al. (2023  ###reference_b27###) found that online finetuning can be effective when an LM focuses on important tokens during the adaptation and proposed a gradient-based meta-learning approach to automatically learn a token importance weighting model. However, such gradient-based meta-learning schemes require a compute-expensive second-order gradient calculation (Finn et al., 2017  ###reference_b14###; Ren et al., 2018  ###reference_b63###). Moreover, online finetuning schemes can face multiple issues and challenges, including (i) inevitable forgetting of the learned knowledge, (ii) gradient computation of LLMs during adaptation, and (iii) high sensitivity to the online optimization hyperparameter (e.g., learning rate; Hu et al., 2023  ###reference_b27###). MAC does not suffer from such issues as the amortization is not only efficient but also does not introduce any hyperparameter for the adaptation, and knowledge is preserved in the memory.\nRetrieval augmentation for LMs.\nRetrieval augmentation of LMs with relevant information from external knowledge sources has served as an effective way to improve the performance of LMs on various NLP tasks (Guu et al., 2020  ###reference_b22###; Lazaridou et al., 2022  ###reference_b43###; Izacard et al., 2023  ###reference_b31###) by reducing hallucination and leveraging external knowledge which is not seen during pre-training. However, retrieval augmentation drastically increases computational cost (Xu et al., 2023  ###reference_b84###) as documents often consist of thousands of words, and the computational cost of large LM’s is quadratic in the input length. In addition, its effectiveness is sensitive to the configuration of retrieved information (Liu et al., 2023  ###reference_b46###), and even negatively affects the performance of LMs when the retrieved information is counterfactual (Si et al., 2023  ###reference_b74###). Our approach is more efficient than retrieval augmentation as it amortizes the external knowledge to modulate LMs rather than directly incorporating it.\nMemory augmented LMs. Augmenting the neural network with memory banks has been explored in multiple domains, including supervised learning (Hochreiter & Schmidhuber, 1997  ###reference_b25###; Graves et al., 2008  ###reference_b20###), meta-learning (Santoro et al., 2016  ###reference_b69###; Shan et al., 2020  ###reference_b73###), and reinforcement learning (Oh et al., 2016  ###reference_b54###; Rolnick et al., 2019  ###reference_b66###). More recently, memory augmentation has also shown great promise for LMs where it significantly improves the performance and efficiency in various directions, e.g., extending context length with memory retrieval (Wu et al., 2022  ###reference_b83###; Wang et al., 2023  ###reference_b81###), personalization (Baek et al., 2023  ###reference_b2###), and model editing (Mitchell et al., 2022b  ###reference_b53###). Unlike these methods, which store the raw text or use the memory bank to train new LMs, MAC stores the compact modulation parameter and adapts the frozen-based LM, thereby utilizing large models without the heavy computation of training LMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
            "text": "In this section, we present Memory of Amortized Contexts (MAC), an efficient and effective framework for online learning of Language Models (LMs). We first briefly describe our problem setup (Section 3.1  ###reference_###), and then core components, namely amortization and aggregation framework (Section 3.2  ###reference_###) and finally, efficient training and inference schemes for MAC (Section 3.3  ###reference_###). Algorithm 2  ###reference_### and 3  ###reference_### in the Appendix B  ###reference_### provide detailed training and online adaptation processes for our framework."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem setup: Online Adaptation",
            "text": "We consider a realistic online adaptation scenario from Hu et al. (2023  ###reference_b27###) where an static LM parameterized by  is adapted with an online stream of context documents . We then evaluate the adapted model’s performance with a set of query input  with the corresponding label set , where the  query and label are drawn from a conditional distribution of the context document , i.e., . Here, note that the query  is not accessible during online adaptation; hence, retaining the learned information from  is critical for achieving good results. While the query input and label pair  can be in any format or task, we specifically focus on question and answering (QA) tasks by following Hu et al. (2023  ###reference_b27###), i.e.,  is a question and  is the corresponding answer based on the given information in , as it is straightforward to evaluate the LM’s updated knowledge."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "MAC: Memory of Amortized Contexts",
            "text": "The stated goal of MAC is (i) the efficient adaptation of a given LM to unseen information (ii) while retaining previously learned knowledge, both from its original training stage as well as updates from prior examples in a stream of novel data. To this end, we propose to utilize amortization-based meta-learning (Garnelo et al., 2018a  ###reference_b18###, b  ###reference_b19###) of a memory-augmented system. Amortization-based meta-learning with modulations (Humplik et al., 2019  ###reference_b28###; Requeima et al., 2019  ###reference_b64###; Bateni et al., 2020  ###reference_b4###) learns to predict a task-specific modulation (i.e., a compact representation of a task) through amortizing the given context set sampled from the task distribution. This enables efficient adaptation using the learned amortization network, as it only requires a single forward pass to adapt a model, foregoing the cost of gradient computation. It is worth noting that this is also beneficial as the LM does not have access to the input and label pair  during the online adaptation, where we can design the amortization to find the modulation only with the given document . Furthermore, meta-learned modulations have been found to preserve the task information well (e.g., showing great potential for generating or classifying distributions of tasks; Schwarz & Teh, 2022  ###reference_b71###; Bauer et al., 2023  ###reference_b5###), hence, can effectively extract the document information. Based on this insight, we suggest meta-learning the amortization network so that it directly predicts a compact modulation for a new document rather than requiring a separate optimization step.\nLearning to amortize contexts. For a given context document  sampled from the training document set , we learn an amortization network parameterized by  to predict the modulation parameter  as follows:\nHere, we use a hypernetwork (Ha et al., 2017  ###reference_b23###) for  where we mainly follow the architectural design from Phang et al. (2023  ###reference_b57###) by using the T5 encoder-decoder architecture (Raffel et al., 2020  ###reference_b59###) with learnable tokens for the decoder, to have a consistent number of output tokens. Here, one can design the modulation with any type of PEFT scheme (e.g., LoRA (Hu et al., 2022  ###reference_b26###) or FiLM (Perez et al., 2018  ###reference_b56###)) among which we use P-Tuning v2 (i.e., predictions of the key-value of each attention layer; Liu et al., 2022  ###reference_b47###) by following Phang et al. (2023  ###reference_b57###).\nModulating LMs via aggregating amortized contexts. Given a memory bank of compressed documents in the form of modulations , we now learn to choose relevant information in the form of a modulation  for a given input . While one design choice is to select a single modulation (i.e., retrieve), this has two drawbacks: i) risk of selecting the wrong modulation, and ii) limited utilization of learned knowledge across different modulations. Moreover, it is worth noting that recent studies empirically shown the linear interpolation (or advanced merging) between the modulations trained from the same pre-trained LM can even perform better than individual modulation (coined “model soup”; Wortsman et al., 2022  ###reference_b82###; Zadouri et al., 2023  ###reference_b88###). In this regard, we thus aggregate the memory bank into a single modulation based on the given input. Formally, we learn a set aggregation network  that satisfies permutation invariance (i.e., invariance to the order of a memory bank) by utilizing cross-attention blocks (Vaswani et al., 2017  ###reference_b79###; Kim et al., 2019  ###reference_b37###; Xu et al., 2020  ###reference_b85###) to select :\nwhere  is the input encoder, and we use the same architectural design as the amortization network , albeit resorting to a reduced number of parameters for efficiency reasons.\nEnd-to-end training objective. To learn aggregation and amortization networks, we optimize both networks in an end-to-end fashion based on the loss function , i.e., negative log-likelihood of the given label :\nwhere  is the batch size of training query inputs and labels.\nHere, it is important to state that we make no updates to the static LM , which would carry the risk of catastrophic forgetting by overwriting important parameters.\nOnline adaptation stage. After training amortization and aggregation networks based on a given training set, we now consider the online adaptation scenario. Here, we consider a stream of  documents  given to the LM in a sequential manner, where the task input  is not accessible during adaptation. To this end, we propose to store the compact modulations into a memory bank  and later predict the modulation using the aggregation network to adapt the LM, i.e.,  where ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Memory Efficient Training and Inference for MAC",
            "text": "Due to aforementioned challenges, the training of MACcan quickly become prohibitive. The following sections cover techniques to drastically reduce memory requirements.\nBackpropagation dropout.\nDuring the online adaptation stage, the aggregation network is required to predict the modulation based on the memory bank, which may consist of large numbers of modulations (examples extracted from thousands of novel documents in our experimental setup). To handle large batch inference, it is crucial to present similar examples during training to avoid distribution shift between training and online adaptation stage and ensure that memory selection is robust. To this end, we propose a memory-efficient way to increase the training context size  by computing gradient using only a subset of randomly chosen examples (ensuring unbiased gradient computation), thus allowing training with significantly larger memory sizes. More concretely, with probability , we perform amortization at training time with a stop-gradient operation, i.e., stopgrad\nwhere  is a hyper-parameter, thus reminiscent of backpropagation. It is important to note that this random sub-sampling yields unbiased approximation of the full gradient under amortization-based meta-learning schemes (Bronskill et al., 2021  ###reference_b7###), hence, does not hurt the overall performance.\nInput: , , , , subgroup cardinality\nOutput:\nHierarchical modulation aggregation. In addition, we furthermore use a hierarchical modulation aggregation to deal with the large memory banks. Let  and  be the output token number for each context and the number of amortized contexts, respectively. Then, the memory usage made by a single cross-attention layer becomes  (note that the input  is also mapped into  tokens). This indicates the aggregation process requires a memory cost that linearly scales with the size of the memory bank.\nTo alleviate memory consumption, we propose hierarchical modulation aggregation that uses a divide-and-conquer strategy (see Algorithm 1  ###reference_###). Specifically, for a given memory bank size of  with  tokens, we subgroup the total  tokens into  tokens each, thereby having  groups ( is the ceil function, i.e., the smallest integer which is greater than or equal to the given input). Then, we aggregate the modulations of individual subgroups into a single output to obtain  modulations. We repeat this procedure until it outputs a single modulation. Assuming no parallelization, one can compute this process by only utilizing the memory complexity of  where  is a hyperparameter.\nSomewhat remarkably, we found that hierarchical modulation aggregation can be used without any additional training or modification of the main training objective (Eq. 3  ###reference_###); it shows comparable results to the full set aggregation (i.e., no hierarchical aggregation). We note that similar observations are also highlighted by other hierarchical merging methods in the Transformer literature (Bolya et al., 2023  ###reference_b6###; Song et al., 2024  ###reference_b75###), indicating the useful inductive bias of the attention mechanism (Bahdanau et al., 2014  ###reference_b3###; Vaswani et al., 2017  ###reference_b79###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we provide an empirical evaluation of MAC, systematically verifying claims made throughout the manuscript and thus supporting the suitability of its constituent components. Specifically, we investigate the following questions: How does MAC perform compared to other online learning techniques for LMs? (Table 1  ###reference_### & Table 2  ###reference_###) Is MAC more efficient compared to online finetuning schemes in terms of memory and time? (Figure 3  ###reference_###) Does MAC show effective knowledge retention compared to other finetuning methods? (Figure 3  ###reference_###) Does proposed efficient training and inference schemes save memory usage? (Figure 5  ###reference_### & Figure 5  ###reference_###) Before answering each question, we outline the experimental protocol (see Appendix A  ###reference_### for more details).\n\nDatasets. For the experiment, we utilize three question-and-answering (QA) datasets including StreamingQA (Liška et al., 2022  ###reference_b45###), SQuAD (Rajpurkar et al., 2016  ###reference_b61###), and ArchivalQA (Wang et al., 2022  ###reference_b80###), by following the prior work (Hu et al., 2023  ###reference_b27###). Here, unlike the original use of SQuAD and ArchivalQA (i.e., used for evaluating static LMs), we use these datasets for online adaptation (i.e., adapting on a stream of documents), hence, denote with an additional “-Seq” notation throughout the section.\n\nModels. We mainly consider GPT2 family (Radford et al., 2018  ###reference_b58###) as the static base LM by following the prior work (Hu et al., 2023  ###reference_b27###), where we additionally conduct the experiment on LLaMA-2 (Touvron et al., 2023  ###reference_b78###) to verify the scalability of MAC. For the amortization network, we consider the T5 model family (Raffel et al., 2020  ###reference_b59###) that are relatively smaller than the base LM. It is important to note that the output number of tokens of the amortization and aggregation networks is a hyper-parameter, where we use 24 for all architectures except for Distil-GPT2, which uses 12. Then, we map these tokens into each layer’s modulation through a linear layer where we use P-tuning v2 (Liu et al., 2022  ###reference_b47###) as the modulation design.\n\nOnline adaptation setup. After training MAC (i.e., learning, , and  parameters) on a training dataset that consists of document and QA pairs, we evaluate the online adaptation performance on the stream of documents. Here, we use 1,665 documents to adapt the LM and then perform the evaluation after the adaptation, where QA pairs are sampled from the learned documents. Each document can consist of tokens up to 512 when using the Byte Pair Encoding (Sennrich et al., 2015  ###reference_b72###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Online Adaptation Performance",
            "text": "We first present the main result by comparing the online adaptation performance with other baselines. Here, we mainly compare with online finetuning schemes and additionally show that MAC can be jointly used with a retrieval augmentation method to further improve the performance.\n\nComparison with online finetuning methods.\nIn Table 1, we show the online adaptation performance of MAC and the online finetuning baselines. Overall, MAC significantly outperforms all the prior online finetuning methods by a large margin, leading to a better exact match (EM) and F1 score.\n\nFurthermore, it is worth mentioning that MAC is significantly efficient in both memory and adaptation time compared to other online finetuning methods; we remark that MAC does not require any gradient computation to update the model, while online finetuning needs the gradient to update the model. We emphasize that both types of efficiency are crucial for online learning LMs as i) the document corpus is expanding rapidly, and ii) it enables the user to use a larger model for a better generalization.\n\nImproving MAC with retrieval augmentation. In addition, we show that MAC can be further improved by using retrieval augmentations. Here, we note that the user requires more inference costs to use retrieval augmentations as prepending the retrieved document in front of the question quadratically increases the inference computation based on the document length due to the Attention mechanism (Vaswani et al., 2017).\n\nFor the experimental setup, we compare it with LMs that are pre-trained on QA training set and then appended with top-1 retrieved document for each question. As shown in Table 2, using BM25 with MAC significantly improves the performance by a large margin in all cases.\n\nWe conjecture that the aggregation process of MAC enables the utilization of shared information across the documents, thus improving performance over single document retrieval. We believe further extending MAC for joint usage with retrieval augmentation schemes will be an interesting future direction to explore, where one can extend the amortization and input network to enhance the aggregation of the modulations but also learn to well retrieve the documents."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Knowledge Retention of MAC",
            "text": "We now address one of our primary motivations for this study: a comparison of knowledge retention by analyzing the catastrophic forgetting of each method. To this end, we evaluate the F1 score retention ratio, which is determined by the decline in the F1 score of the initially adapted 200 documents during the optimization on a subsequent stream of documents. As shown in Figure 3, MAC shows a strong knowledge retention compared to other online finetuning methods: when adapting additional 1,400 documents, MAC retains the initial performance by 96.2%. These results indeed highlight i) the benefit of using a memory bank as a tool for preserving knowledge and ii) our aggregation mechanism well predicts the modulation even when the memory bank’s cardinality increases throughout the adaptation process. It is also worth noting that online finetuning schemes somewhat suffer from preserving the newly learned knowledge, especially when the number of adapted documents increases, thus may limit the practical usage for real-world applications."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Memory Efficiency of Backpropagation Dropout and Hierarchical Modulation Aggregation",
            "text": "Next, we verify the proposed memory efficient techniques, namely the backpropagation dropout and the hierarchical modulation aggregation for training and inference, respectively. Here, we report the peak GPU utilization when using the proposed techniques to show the memory efficiency. Furthermore, we re-emphasize that such techniques are important for (i) scaling LMs to larger models and (ii) handling a large number of documents during online adaptation, which are both necessary for scaling.\n\nTraining memory efficiency.\nTo show the memory efficiency of the backpropagation dropout, we increase the number of amortized contexts during training time and vary the dropout ratio. As shown in Figure 5, increasing the dropout ratio can significantly handle more contexts under the same memory constraint. As a result, we found that simply using 0.5 is an effective choice when using large models (# parameters > 1B) as the training context size is small in such cases. For instance, when training LLaMA-2 (7B) model on StreamingQA dataset without this technique, one can only compute the loss with a single document (under 32 GB GPU), thus the aggregation network cannot learn the similarity between the modulations.\n\nInference memory efficiency. Here, we show that the hierarchical modulation aggregation can significantly reduce memory usage while effectively preserving the performance for the inference. To this end, we vary the cardinality of the subgroup and report the peak GPU memory usage and F1 score where we only measure the used memory by the modulation aggregation (i.e., excluding the LM cost). As shown in Figure 5, using the subgroup size of 4 can reduce the memory by 65.6% while still preserving 93.2% of the original accuracy. We remark that this technique can be applied even without additional training trick or regularization, demonstrating similar observations from the prior works that uses hierarchical aggregation (or merging) in the context of Transformers (Bolya et al., 2023; Song et al., 2024), yet MAC is the first to aggregate the modulations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "In this paper, we propose Memory of Amortized Contexts (MAC), an efficient yet effective online adaptation framework for static LMs with strong knowledge retention. MAC extracts the information of each context document into compact parameter-efficient finetuning modulations where we predict such modulations through the meta-learned amortization network. Then, we store the amortized contexts into the memory bank for strong knowledge retention and aggregate the modulations into a single output when the question input is given. We demonstrate that MAC benefits from multiple aspects including performance, adaptation time, and memory efficiency, and more importantly, exhibits superior knowledge retention of the newly learned documents when learning on a stream of documents.\n\nFuture work. We believe MAC can be extended to multiple applications that require online learning in an efficient manner, e.g., federated learning for LMs (Che et al., 2023 ###reference_b9###) and model editing (Mitchell et al., 2022a ###reference_b52###, b ###reference_b53###; Hartvigsen et al., 2023 ###reference_b24###). Furthermore, it would be an interesting future direction to explore when the memory bank size has a constraint for further storage efficiency. In this direction, various ideas can be explored to maximize knowledge retention, such as merging similar modulations (or applying model soup; Wortsman et al., 2022 ###reference_b82###; Zadouri et al., 2023 ###reference_b88###) to perform better in both provided documents while reducing the size of the memory bank."
        }
    ]
}