{
    "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
    "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication.\n\nOur defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against adversarial attacks. The code is available at https://github.com/anudeex/WARDEN.git.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Nowadays, Large Language Models (LLMs), due to their vast capacity, have showcased exceptional proficiency in comprehending and generating natural language and proven effective in many real-world applications. Using them as EaaS in a black-box API manner has become one of the most successful commercialization paradigms. Consequently, the owners of these models, such as OpenAI, Google, and Mistral AI, have initiated the provision of EaaS to aid users in various NLP tasks. For instance, one notable provider, OpenAI, with over 150 million users, recently released more performant, cheaper EaaS models.\n\nGiven the recent success of EaaS, the associated vulnerabilities have started to attract attention in security and NLP communities. As a primary example, model extraction attack, a.k.a. imitation attack, has been proven to be effective in stealing the capability of LLMs. To conduct such attacks, the attackers query the victim model and then train their own model based on the collected data. Attackers usually invest far less cost and resources than victims to provide competitive services. Therefore, it is imperative to defend against them, and the most popular tactic is to implant statistical signals (or watermarks) via backdoor techniques.\n\nBeyond intellectual property (IP) infringement, further vulnerabilities have been exposed, such as privacy breaches, more performant surrogate models, and transferable adversarial attacks. As a result, backdoor watermarks are added to EaaS embeddings enabling post-attack lawsuits because the attack models inherit the stealthy watermarks, which could be utilized by EaaS providers to identify them. The first work of this kind uses a pre-determined embedding (vector) as the watermark, which is then incorporated into text embeddings in proportion to trigger words. The primary requirements for watermarking methods include: (i) they should not lower the quality of the original application, and (ii) it should be difficult for malicious users to identify or deduce the secret watermark vector.\n\nOur first work, CSE attack, challenges the aforementioned second point. It involves creating a framework CSE (Clustering, Selection, Elimination) that selects the suspected embeddings with watermarks by comparing the distortion between embedding pairs of the victim model and a benchmark model, then neutralizes the impact of the watermark on the embeddings. Empirical evidence demonstrates that CSE successfully compromises the watermark while preserving high embedding utility. To mitigate the effects of CSE, our second work introduces WARDEN, a multi-directional Watermark Augmentation for Robust DEfeNse mechanism, which uses multiple watermark embeddings to reduce the chance of attackers breaching all of them. We notice that WARDEN, even with a limited number of watermarks, is successful in countering CSE. Moreover, we design a corresponding verification protocol to allow every watermark the authority to verify copyright violations.\n\nOur main contributions are as follows:\n- We propose CSE (Clustering, Selection, Elimination) framework that breaches the recent state-of-the-art watermarking technique for EaaS, and we conduct extensive experiments to evaluate its effectiveness.\n- We design WARDEN to enhance the backdoor watermarks by considering various watermark vectors and conditions. Our studies suggest that the proposed defense method is more robust against CSE on various datasets."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Imitation Attacks",
            "text": "Imitation attacks (Krishna et al., 2020; Orekondy et al., 2019; Yue et al., 2021; Wallace et al., 2020) can replicate cloud models without needing access to their internal parameters, architecture, or training data. This attack strategy involves querying the victim model and then training a surrogate model that mimics the victim based on the API's responses (Chandrasekaran et al., 2020; Tramèr et al., 2016). Liu et al. (2022) highlighted that publicly available cloud EaaS APIs are susceptible to these imitation attacks. This vulnerability presents a significant threat to EaaS providers, as attackers can quickly duplicate the deployed model with little time and financial resources. Alarmingly, such surrogate models can outperform the original victim models (Xu et al., 2022), especially when factors such as victim model ensemble and domain adaptation come into play. These surrogate models can be used to launch a similar API at a lower price, infringing on intellectual property rights and negatively impacting the market."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Backdoor Attacks and Watermarks",
            "text": "Backdoor attacks involve inserting textual triggers into a target model such that the victim model behaves normally until the backdoor is activated. Recent works have shown that pre-trained LLMs are susceptible to backdoor attacks and transferable to downstream tasks.\n\nRecent research has utilized backdoor as the essential technology to integrate verifiable watermark information in deep learning models, especially LLMs. The reason is that other techniques, such as altering model parameters, need white-box access and are non-transferable in model extraction attacks. Similarly, lexical watermarks do not work on embeddings in the EaaS use case. Drawing inspiration from backdoor attacks, one can correspond EaaS embeddings to a pre-defined watermark when trigger conditions are satisfied."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we present an overview of the conventional backdoor watermark framework to counter model extraction attacks and then proceed to a detailed design of our CSE attack. Next, we explain WARDEN, the multi-directional watermark extension to the previous watermarking technique.\n\nWe employ clustering algorithms to organize the embeddings in the dataset (retrieved by attackers) into groups. This enhances the subsequent selection step by: (i) improving the efficiency of calculating pair-wise distance within smaller sets of embeddings, and (ii) providing distinct groups of poisoned data entries, which facilitates the identification of more anomalous pairs. The K-Means algorithm is used as the primary clustering approach, and the effectiveness of other clustering methods is discussed in Appendix C.1. However, clustering alone is not sufficient for filtering out the watermarked embeddings. For instance, it can be observed from the contour lines in Figure 3 that watermarked samples are spread across clusters and inconspicuous. Furthermore, the centroids of the watermarked samples and overall clusters do not coincide. To counteract this, we propose the selection module to identify the most suspicious embeddings with the watermark.\n\nWe introduce a hold-out standard model (or benchmark model). Within each cluster, we conduct pairwise evaluations on the corresponding embeddings (provided embedding) and (standard embedding). Those with distinctive distance changes are considered suspected samples.\n\nGiven the suspicious embeddings that are potentially watermarked from the previous step, we hypothesize that the watermark can be identified and recovered in suspicious embeddings’ top principal components because the target embedding would be common among them. Following this idea, we propose an elimination algorithm, composed of two steps. First, we apply singular value decomposition (SVD) to analyze and identify the top principal components. Then, the contribution of these components is iteratively eliminated using the Gram-Schmidt (GS) process. The elimination step for each principal component vector is demonstrated as follows: \n\nIn the projection function, is projected onto the -th principal component, denoted as. Here, is initialized with . After each iteration, is acquired by normalizing such that .\n\nTo diversify the possibility of watermark directions (or target embeddings), we introduce multiple watermarks. This strategy increases the difficulty of inferring all of them via the elimination module in the CSE attack. These watermarks remain confidential on servers and can be subject to regular updates. We randomly split the trigger words set into independent subsets for watermarks. The trigger counting function is the frequency of trigger words in the set with a maximum threshold.\n\nWe add watermarks to the original embedding for text to generate the corresponding embedding as follows:\n\nBecause we split for multiple watermarks, the proportion of watermarked samples is independent and is the same as in the single watermark case. Due to weight values being implicitly normalized, where is watermark weight used on a single trigger set.\n\nWe adopt a conservative approach to copyright verification with multiple watermarks, i.e., if any watermark confidently flags IP infringement, we consider it positive. Hence, we build verification datasets, backdoor texts, and benign text as follows:\n\nThe premise is that embeddings for these backdoor texts will be closer to their corresponding target embedding in contrast to benign texts in the case of watermarks. We leverage this behavior of embedding backdoors to verify copyright infringement at each watermark level. We quantify the closeness by computing cosine similarity and squared distance between target embeddings and embeddings of the backdoor and benign texts.\n\nThe copyright detection performance is evaluated by taking the difference of averaged cosine similarity and averaged squared distance. Furthermore, we compute using the Kolmogorov-Smirnov (KS) test, which compares these test value distributions. We aim to reject the null hypothesis: The two cosine similarity value sets are consistent.\n\nWe evaluate these three metrics independently for all the watermarks and then combine them.\n\nThe core idea is that overall infringement can be certified by the infringement of any one of the target watermarking embeddings."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminary",
            "text": "Malicious attackers target the EaaS victim service by sending texts as queries to receive corresponding original embeddings. Considering the threat of model extraction attacks, the victim backdoors original embedding using a watermarking function to inject an additional pre-defined embedding to return provided embedding. Then, the attack model is trained on the embeddings received by querying, and the attacker provides a competitive service based on the model. Copyright protection is feasible when the system adheres to these criteria: (i) the original EaaS provider should be able to query the attack model to verify if it has imitated the original model; (ii) the utility of provided embeddings is comparable to that of the original model for downstream tasks."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "CSE Attack Framework",
            "text": "This section outlines our Clustering, Selection, and Elimination (CSE) attack, as the framework shown in Figure 2. This approach aims at (i) identifying the embedding vectors most likely to contain the watermark and (ii) eliminating the influence of the watermark while preserving the essential semantics within the embeddings.\n\nWe first employ clustering algorithms to organize the embeddings in the dataset (which attackers have retrieved) into groups. This action enhances the subsequent selection step by: (i) improving the efficiency of calculating pair-wise distance within smaller sets of embeddings, and (ii) providing distinct groups of poisoned data entries, which facilitates the identification of more anomalous pairs. K-Means algorithm is used as the primary clustering approach, while we discuss the effectiveness of other clustering methods in Appendix C.1. Nevertheless, clustering solely is not sufficient for filtering out the watermarked embeddings. For instance, we can observe from the contour lines in Figure 3 that watermarked samples are spread across clusters and inconspicuous. Furthermore, the centroids of the watermarked samples and overall clusters do not coincide. To counteract this, we thus propose the selection module to identify the most suspicious embeddings with the watermark.\n\nWe denote the victim model as  and introduce another hold-out standard model (or benchmark model) as . Within each cluster , we conduct pairwise evaluations on the corresponding embeddings  (provided embedding) and  (standard embedding). Those with distinctive distance changes are considered suspected samples.\n\nGiven the suspicious embeddings that are potentially watermarked from the previous step, we hypothesize that the watermark can be identified and recovered in suspicious embeddings’ top principal components because the target embedding would be common among them. Following this idea, we propose an elimination algorithm, composed of two steps. First, we apply singular value decomposition (SVD) to analyze and identify the top principal components. Then, the contribution of these components are iteratively eliminated using the Gram-Schmidt (GS) process. The elimination step for each principal component vector is demonstrated as follows:\n\nIn the projection function, is projected onto the -th principal component, denoted as\n\nHere,  is initialized with . After each iteration,  is acquired by normalizing  such that ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "WARDEN Defense Framework",
            "text": "In response to the successful CSE attack, we propose Watermark Augmentation for Robust DEfeNse (WARDEN) as a counter measurement, which incorporates multiple directions as watermarking embeddings. To diversify the possibility of watermark directions (or target embeddings), we introduce multiple watermarks. This strategy increases the difficulty of inferring all of them via the elimination module in CSE attack. These watermarks remain confidential on servers and can be subject to regular updates. We randomly split the trigger words set into independent subsets for watermarks. Then, the trigger counting function is the frequency of trigger words in a set with a maximum threshold of a level of watermark.\n\nFinally, we add watermarks to the original embedding for text to generate the corresponding embedding. Because we split the trigger words set for multiple watermarks, the proportion of watermarked samples is independent and is the same as in the single watermark case. Weight values are implicitly normalized, where the watermark weight is used on a single trigger set.\n\nWe adopt a conservative approach to copyright verification with multiple watermarks, i.e., if any watermark confidently flags IP infringement, we consider it positive. Hence, we build verification datasets, backdoor texts, and benign text as follows: The premise is that embeddings for these backdoor texts will be closer to their corresponding target embedding in contrast to benign texts in the case of watermarks. We leverage this behavior of embedding backdoors to verify copyright infringement at each watermark level. We quantify the closeness by computing cosine similarity and squared distance between target embeddings and embeddings of backdoor and benign texts.\n\nThe copyright detection performance is evaluated by taking the difference of averaged cosine similarity and averaged squared distance. Furthermore, we compute the statistic using the Kolmogorov-Smirnov (KS) test as the third metric, which compares these test value distributions. We aim to reject the null hypothesis that the two cosine similarity value sets are consistent. We evaluate these three metrics independently for all the watermarks and then combine them. The core idea is that overall infringement can be certified by the infringement of any one of the target watermarking embeddings."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To benchmark our attack and defense, we employ standard NLP datasets: Enron Metsis et al. (2006), SST2 Socher et al. (2013), AG News Zhang et al. (2015), and MIND Wu et al. (2020). We use Enron dataset for email spam classification. AG News and MIND are news-based and used for recommendation and classification tasks. We use SST2 for sentiment classification. The statistics of these datasets are reported in Table 1.\n\nTo evaluate different aspects of our techniques, we adopt the following metrics:\n\n(Downstream) Task Performance: We construct a multi-layer perceptron (MLP) classifier with the EaaS embeddings as inputs. The quality of the embeddings is measured by the accuracy and -score of the classifiers on the downstream tasks.\n\n(Reconstruction) Attack Performance: We measure the closeness of reconstructed target embedding(s) with original target embedding(s) by reporting their cosine similarity.\n\n(Infringement) Detection Performance: Following previous work (Peng et al., 2023), we employ three metrics: p-value, difference of cosine similarity, and difference of squared distance. Their customized variations for WARDEN are defined in Section 3.3. Our findings largely rely on this evaluation as it reflects the performance in real-world applications. Additional details are provided in Appendix A.\n\nIn a successful attack, the principal components removed from the embeddings erase the watermark by recovering the target embedding. To validate this conjecture, we model and solve an optimization problem as defined in Equation 9 where a linear combination results in the recovered target embedding. We then calculate cosine similarity to the target embedding. A high cosine similarity demonstrates the technique’s effectiveness.\n\nAn attacker will not be aware whether the model they are trying to imitate is watermarked. Table 4 shows that our attack leads to only minor quality degradation in such scenarios, demonstrating the suitability of our approach. We perform further extensive quantitative and qualitative sensitivity study to investigate how other factors (such as algorithms, parameters, and models) affect the efficacy of our suggested attack in Appendix C.\n\nFigure 5 illustrates the efficiency of employing multiple watermarks, which demonstrates the outstanding performance (yellow and green line upward trend) of WARDEN with increasing use and marginal degradation (blue line) in the downstream utility. Similar patterns are observed in the results on other datasets as detailed in Appendix D.1.\n\nNow, we investigate the effectiveness of WARDEN against attacks as shown in Figure 6. As observed in the previous section, WARDEN is stealthier with increasing watermarks. As expected, the performance of the attack diminishes, correlating with decreasing attack performance (red line). Due to the usage of more watermarks, there is a natural increase in the likelihood that one of them will detect an infringement. Moreover, in extreme scenarios, a mixture of multiple target embeddings will substitute the watermarked samples, reducing the impact of the attack’s exploitation of the semantic distortion in the embeddings.\n\nTo further strengthen WARDEN, we explore the application of the Gram-Schmidt (GS) process on target embeddings, assuming the orthogonal set of watermark embeddings is more distinguishable. In our experiments, as reported in Figure 7, detection performance is stronger after GS selection. In addition, due to orthogonality, the reconstructed target embedding cosine similarities will be significantly lower, indicating potential ineffectiveness of the attack. The corresponding ablation study in Appendix D.1 supports this observation.\n\nSimilar to the experiments for removal of watermarks, we perform WARDEN on non-watermarked models. With strict verification, at high values of , the p-value could be noisy. This may occur as the verification process might find closeness due to genuine semantics instead of backdoors because of a high pool of watermark directions. This could lead to false positives, i.e., incorrectly classifying models as copied. However, other detection metrics remain reliable, aiding entities in making appropriate decisions. This is further discussed in Figure 18. We conduct a further detailed ablation study dissecting the WARDEN components and demonstrating its stealthiness in Appendix D."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "To benchmark our attack and defense, we employ standard NLP datasets: Enron Metsis et al. (2006), SST2 Socher et al. (2013), AG News Zhang et al. (2015), and MIND Wu et al. (2020). We use Enron dataset for email spam classification. AG News and MIND are news-based and used for recommendation and classification tasks. We use SST2 for sentiment classification. The statistics of these datasets are reported in Table 1.\n\nTo evaluate different aspects of our techniques, we adopt the following metrics:\n\n(Downstream) Task Performance: We construct a multi-layer perceptron (MLP) classifier with the EaaS embeddings as inputs. The quality of the embeddings is measured by the accuracy and -score of the classifiers on the downstream tasks.\n\n(Reconstruction) Attack Performance: We measure the closeness of reconstructed target embedding(s) with original target embedding(s) by reporting their cosine similarity.\n\n(Infringement) Detection Performance: Following previous work (Peng et al., 2023), we employ three metrics, i.e., p-value, difference of cosine similarity, and difference of squared distance. Their customized variations for WARDEN are defined in Section 3.3. Our findings largely rely on this evaluation as it reflects the performance in real-world applications.\n\nis detailed in the Appendix A."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "CSE Experiments",
            "text": "CSE is designed to assist model extraction attack bypassing post-publish copyright verification. Hence, we evaluate whether we are able to bypass the copyright verification using the same watermark detection metrics with an opposite objective, i.e., lower p-value and the absolute values of metrics close to zero.\n\nIn a successful attack, the principal components removed from the embeddings erase the watermark. To validate this conjecture, we model and solve an optimization problem as defined in Equation 9  ###reference_###  ###reference_### where a linear combination of results in the recovered target embedding . We then calculate cosine similarity to the target embedding. A high cosine similarity demonstrates the CSE technique’s effectiveness. For CSE, the reconstructed target embedding is extremely (99+% cosine similarity) close to the original target embedding (more in following Section 4.2  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###).\n\nAn attacker will not be aware whether the model they are trying to imitate is watermarked. Table 4  ###reference_###  ###reference_### shows that our attack leads to only minor quality degradation for such scenarios, demonstrating the suitability of CSE. We perform further extensive quantitative and qualitative sensitivity study to investigate how other factors (such as algorithms, parameters, and models) affect the efficacy of our suggested CSE attack in Appendix C  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "WARDEN Experiments",
            "text": "We illustrate the efficiency of employing multiple watermarks in Figure 5, which demonstrates the outstanding performance (yellow and green line upward trend) of WARDEN with increasing and marginal degradation (blue line) in the downstream utility. The results on other datasets also show similar patterns which can be found in Appendix D.1.\n\nNow, we investigate the effectiveness of WARDEN against CSE (shown in Figure 6). As observed in the previous section, WARDEN is stealthier with increasing watermarks. As expected, the performance of CSE diminishes, correlating with decreasing attack performance (red line). Due to the usage of more watermarks, there is a natural increase in the likelihood that one of them will detect an infringement. Moreover, in extreme scenarios, a mixture of multiple target embeddings will substitute the watermarked samples, reducing the impact of the CSE attack’s exploitation of the semantic distortion in the embeddings.\n\nTo further strengthen WARDEN, we investigate the application of the Gram-Schmidt (GS) process on target embeddings, as we assume the orthogonal set of watermark embeddings are more distinguishable to each other. In our experiments, as reported in Figure 7, the detection performance is stronger after GS selection. In addition, due to orthogonality, the reconstructed target embedding cosine similarities will be significantly lower, indicating CSE might also be ineffective. We observe the same from the corresponding ablation study in Appendix D.1.\n\nSimilar to the experiments for CSE, we perform WARDEN on non-watermarked models. Due to our strict verification, for the high value, the p-value could be noisy. It is because the verification process might find closeness due to genuine semantics instead of backdoors as a result of a high pool of watermark directions. This could lead to false positives, i.e., incorrectly classifying models as copied. However, in such cases, we observe that other detection metrics are reliable, which should aid the entity in making appropriate decisions (refer Figure 18). We conduct a further detailed ablation study dissecting the WARDEN components and showing its stealthiness in Appendix D."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we demonstrate that our new CSE attack can bypass the recent EaaS watermarking technique. CSE cleanses the watermarked dataset by clustering them first, then selecting embedding pairs with disparity, and finally eliminating their top principal components, while maintaining the service utility. To remedy this shortcoming, we propose a simple yet effective watermarking method, WARDEN, which augments the previous approach by introducing multiple watermarks to embeddings. Our intensive experiments show that WARDEN is superior in verifying the copyright of EaaS from prior works. Furthermore, WARDEN is also effective against potent CSE, which shows its resilience to different attacks. We also conduct detailed ablation studies to verify the importance of every component of CSE and WARDEN. Future studies may consider exploring watermark ownership under multi-owner service settings."
        }
    ]
}