{
    "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
    "abstract": "A promising approach to preserving model performance in linearized transformers is to employ position-based re-weighting functions. However, state-of-the-art re-weighting functions rely heavily on target sequence lengths, making it difficult or impossible to apply them to autoregressive and simultaneous tasks, where the target and sometimes even the input sequence length are unknown. To address this issue, we propose Learned Proportions (LeaP) and LeaPformers222https://github.com/OSU-STARLAB/LeaPformer. Our contribution is built on two major components. First, we generalize the dependence on explicit positional representations and sequence lengths into dependence on sequence proportions for re-weighting. Second, we replace static positional representations with dynamic proportions derived via a compact module, enabling more flexible attention concentration patterns. We evaluate LeaPformer against eight representative efficient transformers on the Long-Range Arena benchmark, showing that LeaPformer achieves the best quality-throughput trade-off, as well as LeaPformer to Wikitext-103 autoregressive language modeling and simultaneous speech-to-text translation for two language pairs, achieving competitive results.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Transformers (Vaswani et al., 2017 ###reference_b35###) are dominant in the natural language processing (NLP) solution space, demonstrating state-of-the-art performance for a range of applications. With the advent of widely accessible large language models (LLMs), transformers as a class of models are being studied more closely than ever. Unfortunately, the quadratic complexity of the attention mechanisms of typical transformers limits the lengths of the sequences that they can process, rendering them sub-optimal or even impossible to apply for tasks with long sequences.\n\nNaturally, an active area of possible improvement for classical transformers are efficient attention mechanisms that reduce the sometimes prohibitive quadratic run-time and memory complexity of softmax attention with respect to sequence lengths. Many efficient transformer variants have been proposed, including both sub-quadratic attention mechanisms, usually with key assumptions or experimental bounds surrounding their construction, and truly linear attention mechanisms with no prior environmental assumptions (Katharopoulos et al., 2020 ###reference_b14###; Choromanski et al., 2020 ###reference_b8###; Peng et al., 2021 ###reference_b25###; Chen et al., 2021 ###reference_b6###; Qin et al., 2022b ###reference_b29###). While the aforementioned linear transformers are often effective for specific tasks, they tend to exhibit varying degrees of performance degradation when generalized.\n\nTo address this issue, re-weighting functions have been recently formalized (Su et al., 2022 ###reference_b31###; Qin et al., 2022b ###reference_b29###) in linear transformers and serve to concentrate attention scores. Although promising, state-of-the-art position-based re-weighting functions rely on explicit token positions and sequence lengths (Qin et al., 2022b ###reference_b29###). This reliance on knowing the sequence length beforehand makes it difficult to apply those re-weighting functions and linear transformers to autoregressive tasks without specialized solutions (Agostinelli & Chen, 2023 ###reference_b1###) and renders it impossible to apply them to simultaneous tasks. Furthermore, existing re-weighting functions’ reliance on explicit positional representations usually produce static attention concentration patterns, which can severely limit their generalizability when an attention concentration pattern is ill-suited to a given task.\n\nTo solve this reliance on explicit positional representations and enable linear transformers for a wider range of tasks, we propose a novel approach that we refer to as Learned Proportions (LeaP) and call models we apply it to LeaPformers. This contribution is composed of two major aspects: generalization to proportions and learned behavior. First, we generalize the dependence on explicit positional representations and sequence lengths into an intuitive dependence on proportions of a sequence for re-weighting, removing theoretical dependence on sequence lengths. Second, instead of employing static positional representations, we construct and deploy a compact module that dynamically derives sequence proportions for a given token during training and inference. These straightforward, but critical, contributions ultimately remove any reliance that current position-based re-weighting functions may have on sequence length, enabling them for tasks where the sequence length is not known beforehand (and cannot be estimated) and/or where attention concentration patterns are more complex.\n\nTo validate our proposed approach, we evaluate and compare with eight other representative attention mechanisms on the Long-Range Arena (LRA) benchmark (Tay et al., 2021 ###reference_b33###), a competitive benchmark for efficient attention mechanisms on long sequences. In addition, we validate LeaPformers on autoregressive language modeling on Wikitext-103b (Merity et al., 2016 ###reference_b22###) and on multiple language pairs for simultaneous speech-to-text translation (SimulST) (Ma et al., 2020c ###reference_b21###). When compared to popular, previously proposed efficient attention mechanisms on the LRA benchmark, the proposed LeaPformer achieves the best accuracy-throughput trade-off, balanced performance across tasks, small memory footprint, and notably beats competing mechanisms’ inference quality. During autoregressive language modeling, LeaPformer achieves the lowest perplexity out of a limited set of efficient attention mechanisms, beating out the next closest mechanism by 0.13 perplexity on the test set. Finally, when applied to simultaneous translation, LeaPformer demonstrates competitive results with a reasonable accuracy-throughput trade-off compared to classical softmax attention for critical ablations, with variations achieving quality loss of only 0.26 BLEU-4 (Post, 2018 ###reference_b26###) for English to German and 0.23 BLEU-4 for French to English while being completely linear in complexity. To our knowledge, this is the first time that an explicit position-based re-weighting function for linear transformers is successfully applied to simultaneous tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Motivation",
            "text": "LeaPformers aim to address challenges in transformer models, specifically focusing on efficiency and performance improvements. The traditional transformer model, while highly effective for a range of tasks, often suffers from limitations related to computational complexity and resource requirements.\n\nTo tackle these challenges, LeaPformers introduce innovative mechanisms that reduce the computational load and memory usage without compromising performance. By leveraging efficient algorithms and architectural modifications, these models enable faster processing times and improved scalability.\n\nOur experiments show that LeaPformers consistently achieve superior performance on benchmark datasets compared to standard transformer models. This highlights their potential for a wide range of applications, particularly when computational resources are limited or rapid inference is critical.\n\nIn conclusion, LeaPformers represent a significant step forward in the development of transformer models, offering a balanced solution that meets the growing demand for efficient and effective neural network architectures."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Softmax Attention Mechanisms",
            "text": "Multi-headed self-attention in transformers (Vaswani et al., 2017) can generally be described as follows: where query is , key , and value , with  being the input sequence for each attention head that divides the model embedding space  into some  (denoted as  hereafter for simplicity) and ,  and . In cases where the concatenation of the attention head outputs differs in dimensionality from , an optional output projection layer is commonly applied via . For long sequences, the quadratic complexity of the mechanism can prove to be a throughput bottleneck during training and inference."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Efficient and Linear Transformers",
            "text": "Efficient transformers have emerged over the past few years as an active area of research for particularly resource or latency-constrained environments, exhibiting notable inference speedups and smaller memory footprints. These transformer variants focus on alternative attention mechanisms that reduce the quadratic complexity of typical softmax attention. A plethora of efficient transformer options exist that can be classified into a few groups: sliding-window or localized attention mechanisms (Parmar et al., 2018 ###reference_b24###; Dai et al., 2019 ###reference_b9###; Wu et al., 2020 ###reference_b40###; Beltagy et al., 2020 ###reference_b4###), pattern or sparsity-based attention mechanisms (Child et al., 2019 ###reference_b7###; Zaheer et al., 2020 ###reference_b42###), and some unique outliers (Wang et al., 2020b ###reference_b38###; Kitaev et al., 2020 ###reference_b15###).\n\nWhile many approaches linearize the computations, truly-linear transformers, such as the kernel-based substitutions for the softmax mechanism, do not make any prior assumptions of the environments (e.g., no assumed sparsity or local dependencies). This can be described via row-wise outputs for each attention head with a corresponding similarity function that transforms the product of the query and key matrices. If the similarity function becomes a particular form, it accurately represents softmax attention.\n\nIf we decompose certain terms, computation can be reordered such that the attention complexity reduces from a higher order to a lower order. This corresponds to the sequence length of the query matrix and those of the key and value matrices (a generalization for encoder-decoder cross-attention). When these parameters are significantly larger than others, this rearrangement of the attention calculation leads to linear complexity with respect to the sequence length."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Position-Based Re-weighting Functions",
            "text": "While reducing the computational complexity, linearizing multi-headed self-attention leads to varying degrees of degraded model performance. To address this shortcoming, re-weighting functions have been recently proposed. They introduce an additional function to augment, with the goal of concentrating/adjusting the probability distribution of the normalized (Qin et al., 2022b ###reference_b29###). Re-weighting functions are commonly based on token positions, and we multiply as shown in Equation 6 ###reference_###:\n\nNote that even though is placed at the end of the equation and multiplied, that particular placement and operation can be arbitrary. For example, placing in between or before the transformed query and key matrices would also be valid as a re-weighting function application. can also map to any number of possible concentration methods, such as a matrix modifying by multiplication or element-wise operations (e.g., addition). Elaborate position-based encoding schemes (Raffel et al., 2020 ###reference_b30###; Wang et al., 2019 ###reference_b36###; Wang & Chen, 2020 ###reference_b39###; Liutkus et al., 2021 ###reference_b17###; Press et al., 2022 ###reference_b27###), using absolute or relative token positions, have advanced the scheme utilized by the initial work (Vaswani et al., 2017 ###reference_b35###) and many provide what can be intuited as position-based re-weighting functions. However, those schemes are specifically designed for a formulation and do not work for the decomposed linearized formulation.\n\nRotary Positional Embeddings (RoPE) (Su et al., 2022 ###reference_b31###), with some minor modifications, is closest to being a true position-based re-weighting function for linear transformers by using relative token positions. However, RoPE is unaware of the total sequence length when it is applied, and this can cause potential problems. For example, RoPE would treat two tokens that are 100 tokens apart in a 1k length sequence and a 200 length sequence the same, where the actual relationship of the two tokens could vary drastically between the two sequences. This lack of sequence length awareness renders RoPE’s re-weighting ability inherently limited, especially for sequences that exhibit more than the aforementioned locality characteristic. We elaborate on RoPE’s construction (and its linear attention variant tested in this paper) in Appendix A.6 ###reference_###."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Motivation of Our Study",
            "text": "Unfortunately, the reliance on sequence length makes it difficult to apply certain re-weighting functions towards autoregressive and simultaneous tasks. For instance, it can be challenging to apply position-based re-weighting functions to autoregressive tasks (e.g. text-to-speech translation) where target sequence lengths are usually not known beforehand. Although some effort has been made to address these issues (Liu et al., 2022; Agostinelli & Chen, 2023), mostly via target sequence length prediction based on the full input sequence, proposed solutions are prone to some level of approximation error. Furthermore, none of the prior approaches has discussed the impossibility of applying them to simultaneous tasks, where even the full input sequence is not available at decoding time-steps. \n\nMoreover, the static nature of the state-of-the-art re-weighting functions can cause issues from an inference quality standpoint. In such instances, dynamic flexibility in the re-weighting function to encourage strong, long-range connections would be preferred. An example of when this flexibility may be desirable can be found in a typical translation task, where languages like German that tend to exhibit subject-object-verb (SOV) structures as opposed to subject-verb-object (SVO) structures in languages like English may require diverse attention patterns and long-range dependencies. A verb near the end of a German sentence may attend strongly to the subject near the beginning of the sentence, but static re-weighting functions would likely have trouble enabling this relationship."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LeaPformer",
            "text": "We propose a novel re-weighting function and method for constructing such functions that resolves the issues in applying them to many autoregressive tasks and enables their application to simultaneous tasks. \n\nTo this end, we first generalize the reliance on absolute token position and sequence length into a more direct, intuitive reliance on the relative placement of a token in the sequence which we refer to as a proportion. This generalization allows for easier analysis of re-weighting function behavior and removes theoretical dependence on sequence length. Second, we propose, construct, and deploy a compact module to learn proportional representations derived from each token, a technique that we call Learned Proportions (LeaP) and call the models it is applied to LeaPformers. LeaPformers can be applied to tasks where sequence lengths are unknown and, more importantly, capture dynamic attention patterns over position-based re-weighting functions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Proportion-Based Re-weighting Functions",
            "text": "We introduce proportion-based re-weighting in Equation 8, where  and  represent proportions of sequences from which queries and keys are derived from and  represents the re-weighting function with a reliance on the provided proportions. Technically,  and  can be set in any manner, but for the most straightforward proportion-based re-weighting implementations, they would correspond to the proportion of a sequence that a token is placed (e.g., at 20% of the sequence)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Learned Proportions",
            "text": "In contrast to determining the proportions statically, models can learn to derive these representative proportions via a module containing a compact network embedded within attention blocks. We call this method Learned Proportions (LeaP) and models utilizing this technique LeaPformers. The possible inference quality benefits of LeaP can be understood intuitively. By deriving representative elements via a small module based on the query matrix, the module’s learned behavior could produce derived elements equal to classical positional representations, thus enhancing performance. Alternatively, it could defer inter-token relationships that might otherwise be emphasized in other static methods. \n\nWe redefine the aforementioned proportions in accordance with Equation LABEL:eq:proportion_derivation, where  and  represent the proposed modules that derive proportions based on the query and key matrices, and  and  are redefined as  and .\n\nTo elaborate on potential inference quality benefits further, consider the example of translation to or from German and the SOV structure that could be challenging for certain models. If  is derived from a small LeaP module in self-attention, models could effectively manage locality bias, allowing attention to be concentrated elsewhere in the sequence. If correctly learned, this might facilitate models deferring attention from the verb at the end of the German sequence to the beginning, where a strong attention score might typically be expected. Allowing derivations of both  and  affords maximum flexibility in attention patterns produced by the employed re-weighting function.\n\nBeyond the inference quality benefits of LeaP, our method removes any dependence that proportion-based re-weighting functions have on knowing the sequence length beforehand, widely enabling them for autoregressive tasks without target sequence length prediction and, for the first time, demonstrating the feasibility to apply them to simultaneous tasks."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Optimizing LeaP Module for Throughput and Analyzing Expressivity",
            "text": "It is critical that the addition of LeaP does not significantly affect the throughput of a given model or its memory footprint, as it is intended for resource-constrained and latency-sensitive environments. Given that, we recommend a module composed of a simple, two-layer feed-forward network that steps down the attention head embedding dimension with a sigmoid activation at the end of the network, along the lines of the augmentation highlighted in Figure 2. While a separate LeaP module for each attention head would be straightforward, we found in our experiments that this made a very minor difference in terms of quality. For English to German SimulST, we observed that when replacing the decoder self-attention block with LeaPformer where a separate LeaP module was provided for each attention head the models were of similar quality (measured by validation perplexity, difference of 0.03). Given that and acknowledging that deploying multiple LeaP modules would drastically increase the parameter footprint of the module, we elect to share one LeaP module for all attention heads.\n\nAdditionally, given the activation functions chosen for the LeaP module’s architecture, it is important to examine the expressivity of the module. It is generally desirable that the LeaP module outputs a complex range of values as opposed to saturating to values of 0 or 1, as otherwise it is simply sparsifying the matrix."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Empirical Evaluation",
            "text": "We validate the potential of LeaP by applying it on three major sets of tasks. We first test our approach on the popular Long-Range Arena (LRA) benchmark (Tay et al., 2021), built specifically for validating the capabilities of efficient attention mechanisms. We also engage with basic autoregressive language modeling, employing Baevski & Auli 2019’s adaptive input/output architecture on Wikitext-103b (Merity et al., 2016). Moreover, we evaluate it on speech-to-text simultaneous translation (SimulST) via a wait-k read-write schedule (Ma et al., 2019, 2020b, 2020c) across two language pairs. For our SimulST and autoregressive language modeling experiments, we employ Fairseq (Ott et al., 2019) for training and validation alongside SimulEval (Ma et al., 2020a) for SimulST evaluation. LRA results are compared via accuracy, autoregressive language modeling results are evaluated via validation and test set perplexity, and SimulST results are compared via detokenized BLEU-4 (called BLEU later) using sacreBLEU (Post, 2018). Additional details related to employed hardware and hyperparameters can be found in the Appendix."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Long-Range Arena Benchmark",
            "text": "Instead of the Long-Range Arena (LRA) benchmark provided by Tay et al. 2021, our implementation follows Skyformer’s (Chen et al., 2021) PyTorch framework and reuses their architectures and hyperparameters, which we hold static. We provide baseline results for various architectures, including the classical transformer (Vaswani et al., 2017) and several seminal efficient transformers. Some auxiliary results and details related to the LRA benchmark are provided in Appendices A.4 and A.5 regarding controlling for increased parameter counts and alternatives to efficient transformers.\n\nIn addition to these results, we propose a composite heuristic evaluation metric that we call Relative Composite Performance (RCP) to more concretely evaluate efficient attention mechanisms and their throughput-accuracy trade-offs. We treat softmax attention as an inference quality ceiling and throughput floor for the LRA benchmark as follows: with given efficient attention mechanism and softmax attention and their corresponding accuracy and throughput values. The RCP numerator rewards significant speedups in a proportional manner via a simple ratio. Contrastingly, the RCP denominator is governed by the delta in accuracy between two attention mechanisms normalized by the standard deviation of the entire benchmark’s accuracy, focusing on penalizing for inaccuracy. Adding by one in the denominator smooths out the resulting values. While depicted in Equation 11 favors equally prioritizing accuracy and throughput, one could prioritize one or the other by changing the exponential values of the expressions in the numerator and denominator (they are currently set to 1 as a default for equal prioritization).\n\nEquation 12 is a memory footprint-aware version of the metric, called RCPmem, that splits its reward between throughput increases and memory footprint reductions, where the weights for those rewards are similarly tunable: We set these tunable weights to 0.5 as a default.\n\nRegarding the LeaPformers tested on the LRA benchmark, a minimal setup was initially employed with around a maximum of a 0.2% increase on the number of parameters for the LeaP module. We additionally test a larger module employed with a maximum increase of 1.5% to the number of parameters to investigate the effects of increased size. Some very limited fine-tuning was employed across a few possible module sizes on a per-task basis for the larger LeaPformer, depending on the perceived difficulty of the task.\n\nWe show a holistic view of performance in Figure 1, with kernel-based linear transformers tending to provide an excellent quality-throughput trade-off. It is clear from the figure that LeaPformer provides the best performance trade-off, exhibiting significant quality increases over Performer, Linformer, Reformer, and Skyformer, with a reduced memory footprint. Details on inference quality are showcased in Table 1, where both LeaPformer-0.2% and LeaPformer-1.5% exhibit a balanced performance profile. While classical softmax attention achieves the highest average score by a notable margin, it is beaten on a number of tasks by other methods.\n\nLeaPformer does not seem to specialize nearly as much as other architectures (aside from some difficulty on the pathfinding task), indicating its balanced performance. BigBird is the closest to providing a similarly balanced inference quality profile, but this comes with significant throughput reductions as shown in Table 2 and noticeable increases to memory footprint. Regarding the application of RCP to the results in Table 1 and Table 2, LeaPformer beats out all other options in Table 3 by a wide margin (minimum increase of 0.81 RCP), demonstrating its very effective relative performance on the LRA benchmark compared to softmax attention and its efficient attention peers. Across the board, LeaPformer matches the general inference quality of task-balanced models with a massively reduced memory footprint while still exhibiting a minimum 1.52x throughput increase over those mechanisms."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Autoregressive Language Modeling",
            "text": "While autoregressive language modeling has advanced tremendously with the advent of LLMs, more accessible methods can still serve to validate architectural differences between attention mechanisms. Given that, we’ve employed the adaptive setup of Baevski & Auli 2019 and reuse nearly their exact model hyperparameters for autoregressive language modeling on Wikitext-103b (Merity et al., 2016). Hyperparameter differences are only related to batch sizes and the number of updates due to computational constraints, and are detailed in our Appendix. All sequences during training and evaluation were composed of 512 tokens (i.e., 511 tokens of context where possible for evaluation).\n\nAs observed in Table 4, classical softmax attention outperforms all linear attention mechanisms by a wide margin, but amongst the linear attention mechanisms themselves there are distinctions in terms of quality. Notably, LeaPformer demonstrates significant improvement over its linear attention peers while only requiring a parameter increase of approximately 3.13%, though it still falls significantly short of classical softmax attention by 2.37 perplexity."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Simultaneous Speech Translation (SimulST)",
            "text": "For the purposes of our SimulST related experiments, we employed a model inspired by the ESPnet-ST toolkit (Inaguma et al., 2020) that focused on end-to-end speech-to-text (S2T) translation with a modified cross-attention block for a wait-k and fixed pre-decision paradigm (Ma et al., 2019, 2020b, 2020c). All model encoders were pre-trained on automatic speech-recognition (ASR) and were trained on a wait-k of 5 and a fixed predecision ratio of 9 and were evaluated on a wait-k of 3 with greedy decoding. Models are evaluated via validation set perplexity and by detokenized BLEU-4 (Post, 2018) via SimulEval (Ma et al., 2020a). Two language pairs and two datasets were employed to test the application of LeaPformer to simultaneous tasks. We utilized MuST-C’s (Cattoni et al., 2021) English to German (en-de) split and CoVoST 2’s (Wang et al., 2020a) French to English (fr-en) split. More comprehensive evaluation is provided for the en-de pair, comparing the results of LeaPformer to an ablation without a re-weighting function. The application of LeaP modules resulted in an approximate parameter increase of 0.03% for ablations that included all attention blocks being linearized. We seek to validate it further on en-de SimulST while also providing several ablations for LeaPformer, representing a wide-range of quality-throughput trade-offs. Similar results are provided for the fr-en language pair, with trends from en-de persisting. While our analysis related to SimulST is focused on analyzing the possible translation quality benefits of employing LeaPformers, we also provide some latency analysis, with some qualifications, employing common SimulST latency metrics in Appendix A.7."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we made two concrete contributions. We re-framed reliance on explicit positional representations and sequence lengths to reliance on sequence proportions, removing theoretical dependence on sequence lengths. Additionally, we proposed LeaPformers and applied proportion-based transformers for the first time to simultaneous translation, achieving minimal quality loss compared to softmax attention for two language pairs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Impact Statement",
            "text": "We advance the efficiency of transformers in state-of-the-art deep learning. Any societal consequences or impacts that typically relate to work focused on increased efficiency also apply here, as such work necessarily improves the practicality of deep learning models for an array of applications."
        }
    ]
}