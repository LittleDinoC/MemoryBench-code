{
    "title": "Adaptive Memory Replay for Continual Learning",
    "abstract": "Foundation Models (FMs) have become the hallmark of modern AI, however, these models are trained on massive data, leading to financially expensive training. Updating FMs as new data becomes available is important, however, can lead to ‘catastrophic forgetting’, where models underperform on tasks related to data sub-populations observed too long ago. ††*Work done during internship at MIT-IBM Watson AI Lab.\n\nThis continual learning (CL) phenomenon has been extensively studied, but primarily in a setting where only a small amount of past data can be stored. We advocate for the paradigm where memory is abundant, allowing us to keep all previous data, but computational resources are limited. In this setting, traditional replay-based CL approaches are outperformed by a simple baseline which replays past data selected uniformly at random, indicating that this setting necessitates a new approach. We address this by introducing a framework of adaptive memory replay for continual learning, where sampling of past data is phrased as a multi-armed bandit problem. We utilize Bolzmann sampling to derive a method which dynamically selects past data for training conditioned on the current task, assuming full data access and emphasizing training efficiency.\n\nThrough extensive evaluations on both vision and language pre-training tasks, we demonstrate the effectiveness of our approach, which maintains high performance while reducing forgetting by up to  at no training efficiency cost.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The concept of the Foundation Models (FMs) has recently gained popularity and became ubiquitous in many downstream applications, including language, vision, and other application domains - advocating towards the ‘train-once-and-use-everywhere’ paradigm shift in AI/ML. One of the most attractive features of FMs is their ability for Zero-Shot prompting, few-shot In-Context Learning (ICL), and great transferability to any task. This is due to their massive scale pre-training, often on billions or trillions of data points. However, such power comes with high training costs. The pre-training data size is so large that normally each sample is observed only a few times or, as in language training, once (single epoch). Moreover, a common requirement for FMs is to rather frequently undergo ‘Extended Pre-Training’ (EPT) – a process of updating the model with new (massive) additional data intended to improve the model’s temporal currency. During EPT the original pre-training data cannot be naively replayed as, given the massive size of both pre-training and EPT data, it would effectively double the EPT cost (naturally assuming a single epoch and 50% replay mix for EPT as is customary in such cases). This would be prohibitive both in terms of the high cost (millions of dollars) as well as non-negligible negative environmental impact (extra heat emission).\n\nHowever, neglecting past data during EPT is prone to the issue of catastrophic forgetting, where models updated with new data tend to underperform on previously seen data. This leads to an important question: how can we adapt large-scale FM models to an ever-evolving world without compromising on performance or efficiency?\n\nThe realm of continual learning offers some insights, but also limitations. While current benchmarks effectively highlight the challenge of catastrophic forgetting by training on non-overlapping data tasks sequentially, they are less applicable to (massive scale) EPT, as they either restrict themselves to limited memory storage (while in practical EPT all data, past and current, is usually available) and do not take into account the training cost of replay. For practical EPT, we argue the cost impact needs to be minimal in the sense that the ‘continual’ EPT needs to have similar cost as ‘naive’ EPT (disregarding old data and catastrophic forgetting issue). This is intuitive, as even the tiny overhead fraction due to replay will be applied as a factor to the training cost.\n\nTaking inspiration from prior works that have tried to select memory data intelligently by selecting the most representative samples of each memory replay dataset, we push the boundaries by considering not which past samples are the most representative (which is typically pre-decided before training future tasks), but rather which samples most effectively prevent forgetting conditioned on the current task data (which is decided during the training of future tasks). This notion is based on the intuitive concept that the model retains full access to previously seen data and that the ‘optimal’ replay data may be contingent upon the new data a model encounters during EPT.\n\nWe specifically propose an approach that dynamically adjusts the proportion of replay samples from each past task based on its propensity to be forgotten given the new task data. In such an adaptive memory replay for continual learning, our algorithm efficiently decides on the optimal allocation of memory replay samples among past tasks to minimize overall forgetting, under the vital consideration of how to do this without the requirement for drastic computation. We evaluate our replay strategy for both vision and language large-scale pre-training tasks.\n\nIn particular, we propose and evaluate a zero-cost protocol that includes intelligent selection of both data to replay and reduction in the new EPT data to compensate for the (relatively small) extra cost of the selection algorithm itself.\n\nIn summary, we make the following contributions: We present an adaptive memory replay for continual learning, a novel scheme inspired by a bandit estimation formulation that assumes full memory access and dynamically adjusts replay samples based on the new data, ensuring reduced forgetting. Extensive evaluations demonstrate the efficacy of our method across both vision and language large-scale pre-training tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": "Continual Learning:\nIn the past few years, there has been significant progress in continual learning to alleviate catastrophic forgetting. Regularization-based methods modify the model parameters with additional regularization constraints to prevent catastrophic forgetting. They store no data but explore extra regularization terms in the loss function to consolidate previous knowledge. Rehearsal approaches memorize or generate a small fraction of data points for previous tasks and utilize them to retain the task knowledge. Importantly, what data to retain is decided during the task itself, and subsequently used throughout future tasks. Expansion approaches expand a model’s architecture as new tasks are encountered; these are highly effective for applications where a model growing with tasks is practical. Our work does not consider these methods because the model parameters grow with the number of tasks, but acknowledge that the contributions could be incorporated into these approaches.\n\nRecently, prompt-tuning methods such as those that outperformed rehearsal-based methods without using a replay buffer by learning a small number of insertable model instructions or prompts. Another line of research is the parameter isolation-based approaches, which focus on freezing the task-specific parameters and growing new branches for new tasks. Proposals like adapters add a small number of parameters to the model for training on downstream tasks. Low-Rank Adaptation (LoRA) extends on the above by using low-rank matrix counterparts of the original weights during fine-tuning, and keeps the actual weights frozen to further reduce inference costs.\n\nContinual Learning in Transformers:\nThe recent Vision Transformer (ViT) has made a pure Transformer architecture scalable for large-scale image classification and several works have successfully applied the Transformers architecture for continual learning. Some methods use the model as a teacher model in a distillation phase. Other approaches propose a unified model by building upon new architectures that dynamically expand the tokens processed by the last layer to mitigate forgetting, with some learning new task-specific tokens per head using task-attention-based decoder blocks. Recently, certain methods propose using pre-trained Transformers while maintaining strict control of the memory usage and reaching state-of-the-art predictive performance. However, these methods either train a new transformer or need to fine-tune large pre-trained transformer models which require significant compute, in contrast to our objective of achieving optimal performance with limited compute."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "In continual learning (CL)111In our setting, the model does not have access to the task id during inference., the objective during task , is to find parameters  which minimize the loss  over the current dataset  and all previously seen datasets:\nTypically, CL approaches assume that past data cannot all be stored. These approaches [40  ###reference_b40###] use  to approximate the true objective (Eq. 1  ###reference_###) and minimize:\nwhere  is a hyper-parameter. The memory buffer is updated after each task but the total number of stored items is constant.\nThe resulting method’s computational requirements scale well with the number of learned tasks, but the limited size of  means that it becomes less effective at representing past data, as the number of learned tasks increases.\nThe (stochastic) K-armed bandit problem [5  ###reference_b5###] considers a setting in which there are  available actions, referred to as arms. Performing one of the actions returns a stochastic reward drawn from an unknown distribution. The problem is selecting a number of actions in a way which minimizes the expected regret, defined as the expected difference between the rewards obtained by always choosing the optimal action and the rewards obtained by following our strategy. At each step, a bandit strategy approximates the parameters of the reward distribution of each of the  actions. Thereafter, the strategy needs to select an action to perform. One such strategy is Boltzmann Exploration [23  ###reference_b23###] which computes the mean of the observed rewards for each action, and then uses all means to define a categorical distribution, from which the choice of action is drawn. Finally, if the action’s distributions change between steps, the bandit problem is referred to as non-stationary [59  ###reference_b59###]."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Adaptive Memory Replay",
            "text": "In this section, we modify the typical CL setting by challenging the restrictive assumption that past data cannot be accessed. We also modify the objective which we minimize, in order to better reflect the new CL setting. Finally, we link the resulting problem to that of multi-armed bandit allocation and detail our approach."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Replay: A New Perspective",
            "text": "For FM extended pre-training, we challenge the common CL assumption that past data is unavailable and alter the common CL setting after making two observations. First, data storage is cheap, therefore it is possible to store and sample from any dataset we have seen before. Second, computation is expensive, meaning that we cannot re-train on all of the past data in memory. Following this insight, we modify the CL setting so that all of the past data can be stored, but a CL algorithm’s computational demands to access and use the past data need to be constant and compensated during training to lead to zero extra cost (compared to naive training only on new data).\n\nWe begin developing our approach by modifying the objective function to better reflect our goal. First, as detailed in the Appendix, we express the main objective in terms of its forgetting on past data, compared to the performance of the optimal parameters for the previous task:\n\nThis change reflects the fact that we fine-tune the previously optimal model on the new task and that our focus is on minimizing forgetting, rather than improving our performance on past data.\n\nIn order to implement our approach, it is crucial to select the subset of past data with the highest forgetting. This subset changes as we update the parameters, and it is computationally infeasible to evaluate the forgetting of each of the past data points. Instead, we seek to divide all past data into clusters of items expected to have similar forgetting values.\n\nThis allows us to infer the forgetting values of the data points in a cluster, based on a small number of evaluations, and use this to select data points which exhibit a high amount of forgetting. Currently, we place all of the data from the same previous task into the same cluster and assume that it would exhibit similar forgetting values. We leave more elaborate clustering techniques for future work."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Adaptive Memory as a Bandit Optimization",
            "text": "Formally, we divide all past data into disjoint subsets, such that , where the forgetting of an input is distributed according to a subset-specific distribution: , for . For the parameters at training iteration , we would like to select a subset which exhibits close to the worst forgetting, minimizing the following quantity:\n\nWe frame this as a non-stationary K-armed bandit (KAB) problem, where pulling an arm and receiving a reward corresponds to sampling a data point from a cluster and evaluating its forgetting. Then, at each training iteration, we have to choose which of the K arms to pull to select data points with maximum forgetting. As we select different over all training steps, we would like to reduce the expected regret: , which is the expected difference in forgetting values between and over all training steps.\n\nIn this work, we implement the Boltzmann Exploration approach which, at training step , approximates the mean reward of each arm, denoted by , and then uses all arms’ means as parameters for a categorical distribution over the choice of arms to pull. This distribution is then used to sample arms and in turn sample the memory buffer.\n\nTo approximate the mean forgetting values of cluster at training step , we first sample a small number of data points from the cluster and evaluate the average of their forgetting values — . We would like to compute based on the previously computed mean value and the currently computed forgetting average . However, we note that the forgetting values depend on our model’s parameters, thus change between training iterations. We account for this by using a moving average, which is used for KAB when the underlying distributions are non-stationary:\n\n.\n\nOnce we have approximated the mean forgetting values for all clusters, we use them to create a categorical distribution over the choice of clusters, with the help of the tempered softmax function. We compute: , where is the temperature hyperparameter and is the normalization constant. Afterwards, we use this distribution to sample cluster indices. Finally, we sample one input from each selected cluster, uniformly at random, and combine the samples to create the memory buffer for the current training step.\n\nOur full method is summarized in Algorithm 1."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate the efficacy of our proposed method for continual learning of FM pre-training (i.e., extended pre-training) in both the vision and language domains. We utilize two distinct pre-trained models as our backbones for these experiments: a Vision Masked Autoencoder (MAE) pre-trained on ImageNet-1K for vision-related tasks, and LLaMA with 7 billion parameters for language experiments.\n\nThe evaluation metrics for our experiments are twofold: test data Final Loss and test data loss Forgetting. These metrics are normalized between 0% and 100%, where 0% represents an offline upper bound with all data trained independently and identically distributed (iid), and 100% corresponds to the performance of the pre-trained model without any fine-tuning.\n\nOur experiments are designed to demonstrate the advantages of our approach over traditional iid replay, especially in terms of computational efficiency and reduced forgetting. We consider gains of our approach to be orthogonal to the realms of non-replay regularization-based continual learning methods, and thus these comparisons are not the main focus of our results. From the perspective of computational efficiency, recent work has found such approaches to be impractical for computationally bounded continual learning. However, we discuss the interaction of our approach with different continual learning strategies like regularization methods and knowledge distillation in our Appendix.\n\nThe hyperparameters for our experiments were meticulously chosen based on a series of small task experiments. We update our model on new data examples per task. In the interest of computational resources for the larger Llama model, we approximate the training of all the model parameters with LoRA finetuning in the language modeling experiments. In our experience, conclusions attained for LoRA finetuning reflect the same in full model training. We use a learning rate for full model fine-tuning and for LoRA-based fine-tuning. For our method, we found that a temperature and forgetting mean update ratio performed best. We compose our replay batches for both iid replay and our method with a 1:1 ratio of replay data to new task training data. We conducted evaluations on a hold-out test dataset comprising 500 samples per dataset. Additional training details can be found in our Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Results for Vision SSL",
            "text": "In Tables 1, 2, 3, we benchmark our proposed approach on 3 different continual pre-training sequences composed of vision datasets. Our goal was to demonstrate the robustness of our findings with a variety of unique and practical dataset sequences. The first dataset is the DomainNet dataset (Table 1), containing 6 different domains of common objects. The next is the Medical MNIST dataset (Table 2), from which we sampled 5 standardized biomedical image datasets containing the highest number of samples. Finally, we use 4 attribute splits from the Synthetic Visual Concepts (SyViC) dataset (Table 3).\n\nOur results demonstrate the advantages of our adaptive memory replay method in the vision domain. Our approach consistently achieves lower final loss and forgetting rates. The slight increase in normalized training time is negligible compared to the performance gains. We show a 0-cost result where we reduce the number of training steps of our approach to align with the training time of naive fine-tuning, and show that this result achieves enhanced performance.\n\nWe note that our strongest performance gains come from the DomainNet results. The gains for the medical data sequence and synthetic data pre-training are much more modest, yet remain pronounced. The synthetic data sequence is interesting in that forward transfer (i.e., negative forgetting) appears in all results. In practical terms, our results imply that vision systems equipped with our continual learning strategy would exhibit improved robustness over time, adapting to new data without significant loss of prior knowledge or computational costs. We re-iterate that there is much more room for improvement from our full-memory continual learning perspective - advanced strategies can close the gap between our method and an ideal scenario with fixed time costs by exploring interesting questions such as how to better cluster the data and which new data is more or less favorable to discard."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results for Causal Language Modeling",
            "text": "Our approach is further affirmed through our language experiments using the Llama model. In Table 4, we benchmark on a 5-dataset sequence using datasets from Huggingface. These datasets were chosen based on the significant variations in loss observed post fine-tuning, thus providing a rigorous test for our approach. The datasets encompass a broad range of language tasks, ensuring that our results are representative of diverse language modeling scenarios. Further specifics about these datasets are available in our Appendix.\n\nThe performance of our method in language experiments shows a significant reduction in both forgetting and final loss compared to the iid full-memory replay. The ‘0-cost’ variant of our method is particularly noteworthy, as it manages to retain a high level of performance without additional computational expenditure compared to naive fine-tuning. This aspect is crucial for applications where computational resources are limited, especially fitting LLM extended pre-training where due to high data volumes and enormous model sizes even a tiny fraction of extra cost is intolerable."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "In Figure 4, we present a comprehensive comparison of the final loss versus training time for our adaptive memory replay method against the Oracle, using the Synthetic Visual Concepts dataset sequence [9]. This plot demonstrates how our method converges towards the Oracle’s performance as we increase the compute budget of our method (via using more replay samples and discarding fewer new task samples). With a limited budget, there is a notable difference in the final loss between our method and the Oracle. However, as training progresses, our method steadily approaches the Oracle’s level of performance, matching and even outperforming Oracle (which has a fixed compute budget itself, pre-defined by the number of training steps we use) with a lower compute cost. The horizontal dotted line marks the point at which our approach reaches the Oracle’s normalized loss, showcasing the efficiency of our method in terms of both loss minimization and computational time. This result is significant as it not only validates the effectiveness of our adaptive memory replay in reducing the final loss but also highlights its capability to achieve this with a substantially lower training time."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the importance of adapting machine learning methodologies to the ever-evolving demands of real-world large-scale continual learning. Our findings, rooted in extensive evaluations across both vision and language tasks, validate the advantages of our approach for continual learning. Ultimately, we hope to inspire methodologies that are both computationally efficient and effective in real-world continual learning scenarios, where data access and computational resources are bound by practical constraints. This research trajectory serves as a stepping stone towards the development of continually learning systems that efficiently and intelligently utilize all available data, enhancing their learning and adaptability across a series of tasks throughout their life-cycle.\n\nFuture work should focus on refining the mechanism, particularly exploring more sophisticated strategies to further enhance the balance between retaining old knowledge and accommodating new information. The decision of which data to discard during the learning phase also warrants deeper investigation to avoid potential loss of critical information. There is also a great need to develop advancing techniques to capture the subtleties of data evolution. In addition, bringing greater realism into continual learning models by incorporating real-world constraints and scenarios will be crucial, such as blurred task boundaries and online learning."
        }
    ]
}