{
    "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
    "abstract": "Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size. We systematically investigate the interaction between and spans and propose an improved configuration for selecting important tokens.\n\nKeywords: interpretability, spans, agreement",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Transformer-based models learn to map features in the input to some output. When training an NLP system, the model learns to identify the most important features (in our case tokens) for the final prediction. Post-hoc explanation methods aim to attribute an importance score to the individual features to interpret the model’s decisions. Generally, these methods tend to disagree with each other when ranking token importance on a set of top-tokens based on attribution scores. Given their disagreement, and assuming that explanations that are faithful to the transformer’s inner mechanisms should be agreeable, the faithfulness of these methods comes under question. However, methods might agree more than initially appears.\n\nFor example, Figure 1 shows that none of the methods selects the same top-4 tokens and that 12 of the 13 tokens appear in at least one top-4 selection, indicating a high variance across methods. Intuitively though, methods seem to target the verb phrases are standing and are unloading to a high degree as the vast majority highlights at least one of the tokens in each of these phrases. Similarly, some methods tend to agree on the noun phrases shipyard workers (first occurrence) and the ships, and even more so on different tokenised subwords of the same word, namely un and ##loading. This leads us to hypothesise that agreement between methods is systematically higher when we look at the linguistic spans they are targeting: the constituents to which tokens syntactically belong.\n\nThis example shows that a single method may have a specific preference for one word class over another, e.g. noun over adjective, auxiliary over inflected verb form or modifier over head. However, the extent to which preferences differ across methods remains unclear, as well as its impact on method–method agreement. A methodological aspect that directly affects agreement is the selection of the top-most important tokens for each method to compare. A relatively under-explored parameter is defined as the number of features that are assigned highest scores by the attribution method, relative to all the features in the input example. A common way of picking is by selecting a fixed number. Intuitively, a that is fixed across instances (e.g., 4) is suboptimal, and the selection process of is often overlooked or obtained by an approximation. As an alternative, can be estimated dynamically across instances, but different conceptual settings for this approach and their effect on agreement have not been investigated yet.\n\nInstead of ranking tokens by attribution score and manually setting a, researchers propose to automatically detect tokens that are signal peaks in the input. Hypothesising that spans are better suited for agreement than tokens conceptually overlaps with this dynamic approach. Precisely, the latter suggests that solely focusing on token-level attribution scores, semi-arbitrary importance cut-offs and the consequent agreement measurements between tokens may be undesirable for interpreting model behaviour.\n\nIn this paper, we aim to disentangle the interdependencies between word class preference, span-level agreement, and the determination of . We show that methods systematically select different word classes and that methods that agree most with other methods and with humans exhibit similar word class preferences. We also find that dynamic and spans work well in combination, and that an adapted threshold for dynamically selecting the most important tokens passes our baseline tests for both token- and span-level estimation. Our main contributions are: i) a linguistic analysis of disagreement on the token-level and on the span-level and ii) an improvement to the dynamic-estimation algorithm. All analyses are available at: https://github.com/jbkamp/repo-Span-Pref."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "In this section, we place our work in the context of prior work on interpretability (§2.1  ###reference_###), the patterns of linguistic information that attribution methods reveal (§2.2  ###reference_###) and top- estimation (§2.3  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Model Interpretation",
            "text": "Tracing the decision processes in neural models poses difficulties due to various factors, including their non-linear nature and the absence of explicit human-defined rules to link patterns in the input features with output labels. Different research lines exist to interpret different aspects of the model (Choudhary et al., 2022; Räuker et al., 2023), such as the linguistic information that might implicitly be learned by the model, or the importance that single input features might have had towards the model’s decision (Madsen et al., 2022).\n\nTo address the latter, post-hoc attribution methods in NLP have been developed to assign a score to each token in the input, creating an attribution profile over the tokens. While these methods are often being used in error analyses (Bongard et al., 2022, i.a.), their reliability is questionable. In fact, attribution profiles obtained from different methods can differ strongly even on the same input. This leads to an overall low inter-method agreement (Neely et al., 2022), which has also been found for domains outside of NLP (Krishna et al., 2022). Diverging experimental results of such methods on different models, datasets and tasks provide additional evidence on their inconsistency. For example, when trying to identify the attribution methods that best align with human preferences–the most plausible (Jacovi and Goldberg, 2020) methods–, Atanasova et al. (2020) and Attanasio et al. (2022) come to fundamentally opposing conclusions. Roy et al. (2022) characterise disagreement between methods in a software defect prediction task as being highest in terms of top- feature importance, followed by rank, then sign. Similarly to Pirie et al. (2023), they propose aggregation schemes for different explanation methods that aim to tackle disagreement in real-world use cases.\n\nOne question that, to our knowledge, remains under-explored, is why attribution methods in NLP disagree. A key to answering this would be comparing methods on their faithfulness, i.e. the degree to which methods are reflecting the model’s decision making process, as recent work (Atanasova et al., 2023, i.a.) aims to assess. However, directly measuring faithfulness might only find glimpses of the model’s inner workings rather than providing a conclusive answer (Jacovi and Goldberg, 2020). Therefore, we think that the first step should be explaining disagreement by the observable output of the methods, i.e. the attribution profiles. We aim to provide a linguistic comparison by quantifying the kind of features that are targeted, expecting different methods to consistently target different classes of words."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Linguistic Patterns in Attributions",
            "text": "Identifying the linguistic preferences of models is important in order to pinpoint the cues upon which models depend during inference time. Only a handful of studies have explored POS preference. Especially in a feature attribution setting, there is little evidence that shows certain preferences by different attribution methods and how these preferences differ. Lai et al. (2019) find that different models (i.e. LSTM, XGBoost and SVM) have different POS preferences on the same data and task, but they do not explore preferences for different attribution methods. Ramnath et al. (2020) examine the top-5 most important tokens in each layer and find that BERT (Devlin et al., 2019) primarily focuses on nouns in all 12 layers, followed by verbs and adjectives. Interestingly, both punctuation tokens and stop words each correspond to 10% in the top-5 selections. \nLanguage (and model behavior) can often not be explained by merely highlighting individual tokens. Rather, we would ideally observe how features act in combination with each other and, for example, if they do so hierarchically. As an alternative way of analysing the attributions of tokens in isolation, we find a growing line of research on feature interactions. Jumelet and Zuidema (2023) find evidence of attribution methods faithfully reflecting linguistic structure in language models. Sikdar et al. (2021) combine token-wise attribution scores into scores assigned to syntactic parent constituents. Similarly, Babiker et al. (2023) train a model on intermediate representations in a hierarchical fashion. Pruthi et al. (2022) anticipate that certain spans of tokens should be highlighted by attribution methods in a sentiment analysis task.\nWhile their intuition is on point, the relatively broad expectations found in the latter underscore the relevance of a clear definition of token spans and their role in demonstrating how neighboring features are grouped.\nAs far as we know, there is no prior work that covers a linguistic analysis of the token selections targeted by different attribution methods. To the best of our knowledge, we are also the first to investigate the relation between disagreement on the linguistic level to overall disagreement among methods. We provide a linguistic analysis in terms of individual tokens, and also in terms of spans that have a clear syntactic definition. In particular, we link disagreement to linguistic preference on the token level and within spans."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Top- Estimation",
            "text": "We analyze the factors of disagreement through an additional scope, namely top- estimation, where  represents the number of most important tokens in the attribution profile. Studies reporting on consistent disagreement between methods often do not consider the impact of the number of selected tokens (Pruthi et al., 2022; Krishna et al., 2022; Neely et al., 2022). A common way of selecting  is by approximating it to a low value, e.g., 1 or 2 (Bastings et al., 2022), 5 (Ramnath et al., 2020), 5 or 10 (Camburu et al., 2019), 25% of the average input length (Krishna et al., 2022). However, a fixed  does not account for variability among instances. A  that is too low can exclude important tokens from the comparison, whereas one that is too high will include non-important tokens while artificially boosting agreement between methods. Keeping  relatively low also helps users to more easily digest the explanations in a real-world scenario.\n\nThe value of  has also been estimated dynamically. Pruthi et al. (2022) set  to 10% of the input length, assuming that longer inputs have a higher number of important features than shorter inputs. Kamp et al. (2023) propose a  that varies dynamically based on properties of the attribution profile of each instance, aiming to include features that display above average importance and that focus more on the targeted region of the input instead of the specific token. While their method estimates a value for  that is close to human preference, we find that their algorithm necessitates further experiments and refinement. Different importance thresholds are possible and need baseline benchmarking. Moreover, prior methods for determining dynamic  do not explicitly account for negative attribution scores.\n\nWe adopt and improve the dynamic  estimation by Kamp et al. (2023) throughout our analysis, measuring agreement at the span level compared to the token level. Formally, this dynamic approach defines a strong signal in the attribution profile as a score that is higher than its neighboring scores according to two principles: local importance and global importance. Local importance requires that a score must be higher than its strict neighbors ( window) to reduce redundancy of tokens belonging to the same signal. A set of adjacent tokens with relatively high scores is converted to a single important signal, and the highest attribution in the set is kept as the peak of the signal. Similarly, the global importance principle requires important signals to be minimally above average signal strength, i.e., , where  is the attribution profile. By only adopting the global importance threshold, the inclusion of groups of (redundant) neighboring tokens with high attribution scores is expected to increase , unnecessarily boosting the agreement scores. Therefore, adding a local importance setting is necessary to estimate signal peaks. Regarding global importance, we maintain a constant threshold to compare span-level agreement to token-level agreement and explore different settings in further sections."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Linguistic Analysis",
            "text": "We hypothesize that one of the reasons attribution methods disagree is that different methods have different preferences for the classes of words they target. Following from this, we expect that differences in word class preferences are put under a different light when we look at the syntactic spans they are assigned to."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Setup",
            "text": "To analyze the disagreement problem, we consider different attribution methods on a natural language inference task. For the experimental setup, we follow Kamp et al. (2023) using the e-SNLI dataset to finetune DistilBERT 10 times on different random seeds. We then use the model (0.89 F1) with the least variation in attribution profiles on the test split for analysis. The dataset consists of premises and hypotheses with output labels of contradiction, entailment, or neutral, indicating the relation.\n\nEach instance has words annotated for importance towards the output label, resulting in human rationales. These annotations are aggregated into scores indicating the proportion of annotators who found the words important. These scores are used to compare attribution scores to human preference in a top-selection context. For the attribution methods, we include perturbation-based approaches, including Partition SHAP (Lundberg and Lee, 2017)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Preference for a Word Class",
            "text": "The first step in our analysis compares word class preference of different attribution methods on top- tokens. We set to 4, which corresponds to the average number of tokens highlighted by humans in e-SNLI, as reflected by a comparable value of averaged dynamic and comparable method–method agreement levels as found by Kamp et al. (2023). Figure 2 illustrates the occurrence of different word classes among the tokens with the highest attribution values (i.e., important tokens) for each method and for human aggregated annotations.\n\nWe compare the ratio of important stop words (Figure 2(a)), punctuation tokens (2(b)), and the distribution of the five most preferred POS tags by humans: noun, verb, adj, adp, det (Figure 2(c)). Various methods exhibit a roughly 10% selection of punctuation tokens on average in each top-4 selection. Despite question answering and natural language inference being different tasks, the punctuation preference is notable. This indicates an inherent method preference, not model reliance, as each instance usually contains two sentences ending with a full stop each. Thus, it's unlikely that models use punctuation as shortcut signals to output labels. This observation might imply that some methods capture locality information more than precise lexical information.\n\nWhile punctuation may be a simple symptom of locality, further examination of this phenomenon in the broader context of spans is essential. We perform this through a linguistic analysis of spans of locally adjacent tokens, the use of dynamic , and their intersection in §4. For stop words, the preference does not match previous findings, indicating task-related differences. For other POS tag preferences, there isn't a clear overlap with prior research. However, Figure 2 shows systematic differences in preference for stop words, punctuation, and most frequent POS tags between Group 1 and Group 2 methods compared to humans, inferring two distinct groups showing different word class preferences.\n\nTo test the independence among methods, we apply Chi-Square tests to method–method (and human–method) pairs’ preference distributions. The full Chi-Square tests are detailed in Appendix A. We assess significant differences in stop word distributions, punctuation distributions, and POS tag distributions for each pair. These tests confirm that most distributions in one group significantly differ from the other group (25/36 pairs) with no significant differences within groups. Exceptions mostly involve certain methods, where some non-significant differences suggest that they could be placed between the two groups. Punctuation preferences contribute significantly to non-significant differences, perhaps due to small punctuation frequencies affecting the Chi-Square statistics.\n\nThus, the high similarity in terms of word class preference for methods in one group results in lower agreement. Conversely, the similar preferences for methods in the group close to human preference correlate with higher agreement. From another perspective, methods similar in agreement scores show similar word class preferences."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Span Definition",
            "text": "We obtain syntactic spans by shallow parsing the data with Flair chunker (Akbik et al., 2018), similarly to Zhou et al. (2020) who use parsed constituents as pre-processed spans for a parsing experiment. Chunking is commonly adopted in Named Entity Recognition where usually noun phrases or verb phrases are the focus of interest (Taufiq et al., 2023). For our task, the advantage of this method over full constituency parsing (Kitaev et al., 2019, e.g.) or dependency parsing (Chen and Manning, 2014, e.g.) is that the chunker output of discrete non-overlapping units facilitates direct alignment with attribution values. Punctuation tokens are ignored by the parser; we treat them as separate spans. Sikdar et al. (2021) use constituency parsing (Mrini et al., 2020) as a basis for hierarchically attributing feature importance scores from tokens to phrases (including any subphrases). However, different methods can have different word class preferences (e.g. a noun modifier may systematically be attributed more importance over its head) and it is therefore questionable whether score aggregation of any kind is a sensible approach. Having clearly defined, non-overlapping phrases is instead crucial to our initial hypothesis. In our dataset, each sentence contains on average 24.4 tokens (6–73), which are grouped into 15.3 spans (3–45). The average ratio of spans over tokens is 0.63 (0.23–1.0). A targeted span is a span that contains at least one token included in the top- selection by the attribution method. During agreement evaluation we treat spans as atomic units, meaning that a span is assigned 1 if targeted, otherwise 0 (similarly to tokens in top- selection). For a fixed  set to 4, the average number of targeted spans in a sentence is slightly lower: Partition SHAP 3.5, Grad × Input 3.6, Human 3.3. The average over methods is 3.5."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Head vs. Modifier Preference",
            "text": "We zoom in on this phenomenon and investigate the attribution patterns in noun phrases (NPs), focusing on methods that select the head over its modifier and vice versa. The NPs must span a minimum of two tokens to make the preference analysis for different word classes possible. To add some consensus stability to this subset, the spans under question should also be targeted by highly agreeing methods like Partition SHAP.\n\n This example clearly illustrates how methods do not only target different word classes in absolute terms, but also how that translates to systematic, alternating differences within syntactic spans.\n\nFurthermore, the ratio of targeted tokens in the [det, noun] NPs is significant. This detail strengthens the claim of systematic preference in that the det–noun alternation is usually exclusive. In other words, it is uncommon for the methods to target both tokens from the NPs. This increases the prominence of the preference phenomenon in cases where one selects the det and the other the noun."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Agreement at the Span Level",
            "text": "We showed that different methods have different word class preferences, and these preferences can be strong in the case of syntactic noun phrases. A consistently strong preference by two methods leads to a notable disagreement at the token level. Therefore, the expectation for methods to agree on the token level might be too stringent. Given these insights, we measure method–method and human–method agreement at the span level, expecting a relative improvement compared to token-level agreement."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Setup",
            "text": "The dataset, model configurations, and pool of attribution methods that we use are identical to those described in the linguistic analysis (§3 ###reference_###). In addition, we adopt the definition for spans given in §3.3 ###reference_###. Our data therefore has a version where the instances are divided into tokens and one where instances are split into spans. The details of dynamic correspond to those described in §2.3 ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   The Effect of Dynamic  on Spans",
            "text": "We compare the effect of dynamic  on the span level versus dynamic  on the token level. We measure the effect as the increase in agreement i) versus a baseline to assess overall difficulty of the task and ii) versus fixed  to assess the ability of the dynamic approach to detect important spans. We expect dynamic  to be better suited than fixed  to identify linguistic spans that the model considers important in the instance. Specifically, the local importance setting (in combination with global importance) appears to work as a pooling operator, highlighting the distinct important parts of the instance rather than few concentrated parts. We assume here that for the specific NLI task,  parts of the input should be considered important.\n\nAgreement is measured as follows. We denote an attribution method as .  assigns an attribution profile  to the input sequence of tokens  so that each  indicates the importance of token  towards the inferred class. The subset of  tokens with the highest attribution values are formalized as . We compare  attribution methods  in pairs by calculating sentence-level agreement@. Agreement@ is based on the relevance of a each token. Relevance for a token  is equal to the ratio of methods that include the token in their respective  subsets. Agreement@ ignores perfect agreement on non-important tokens (where relevance ) in order not to inflate the score. For our experiments, we report mean agreement@, the averaged agreement over instances in the dataset .\n\nThe average pair-wise agreement (all method–method combinations) for dynamic  is 0.61 on the token level and 0.69 on the span level. 0.5 indicates perfect disagreement and 1.0 perfect agreement. While agreement seems relatively low, it might still suggest that consistently the same, few types of signals are identified by a pair of methods.\n\nWe compute a baseline to measure how likely methods are to agree on the token and span level with a pseudo-random attribution method. In other words, we measure task difficulty of making two vectors with a subset of important tokens and spans to agree given a low value of . For fixed , 16% of the tokens in a sentence would be highlighted on average; consequently, 23% of the spans would be highlighted on average. For the token-level baseline, we then randomly shuffle two binary vectors of 100 elements, 16 of which 1s, and compute pairwise agreement. We repeat the process  times. For the span-level baseline we adopt the same procedure, with the exception that the 1s in the vector are 23. The resulting baselines are 0.54 and 0.57, respectively, indicating that agreeing on tokens and spans is similarly difficult at low values of . We thus observe the token-wise baseline for fixed  being outperformed by 0.07 (0.540.61), whereas the span-wise baseline is outperformed by a relatively larger increase of 0.12 (0.570.69).\n\nThe comparison results between span agreement on fixed  versus dynamic  are given in Figure 3  ###reference_###. Dynamic  provided marginal boosts on the token level for some methods but proves to have a larger positive effect on the span level. Specifically, the span agreement for method–method pairs that include certain methods remains constant or increases. At the same time, other method–method and human–method agreement scores remain constant or marginally decrease.\n\nWith regards to the largest difference observed between dynamic and fixed , this can also be explained through the concentration levels of targeted tokens within spans. In fact, dynamic  scatters the important tokens so that more spans are targeted compared to selecting an average fixed . While  yields 3.7 and 3.6 spans on average for the two methods, dynamic  yields 6.9 and 6.5. Since it becomes easier for methods to agree when more tokens (and therefore more spans) are targeted, we investigate the settings of dynamic  further (§4.3  ###reference_###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Adjusting Dynamic",
            "text": "How can we validate or improve the dynamic algorithm? A solid global importance threshold should meet two conditions: i) resulting values should be low, preferably close to a human preference average of 43; ii) they should outperform a baseline.\n\nWe explore multiple thresholds: different combinations of typical distances from the mean in a distribution; the median, which is more robust to outliers than the mean. The thresholds are calculated for (a) all scores and (b) positive scores. Thresholds for positive scores should ignore attributions with negative importance towards the inferred class. The influence of negative values and peaks in the attribution profiles is not accounted for by the current threshold set at .\n\nThe resulting values for different thresholds are given in Table 1. We find that for different thresholds, resulting values are comparable across methods, which might indicate that the attribution profiles have overall similar distributions. The three thresholds that yield the closest values to human preference are the mean, mean plus standard deviation, and the median. Closeness corresponds to the averaged Euclidean distance between the mean-standard deviation pairs and human preference of 43, for each threshold column. Among these three, the mean had already proven to keep values low and close to ground truth average.\n\nEven if the estimated values by the three candidate thresholds are relatively low, it could be, for example, that a method-specific value is too high, positively biasing the agreement score. A high value would even give high agreement for a pseudo-random attribution profile, which should not be possible if the threshold is properly set. Hence, we compare each method’s agreement scores with other methods to the method’s agreement with a baseline. This gives us an indication of how well a specific threshold works with different attribution profiles. We do this both on the token level and on the span level. The baseline method operates pseudo-randomly by assigning attribution scores to the tokens without knowledge about token importance. For each method, we randomly shuffle the scores in each attribution profile. Each method has its own baseline so that the different distributional properties of the attribution profiles are preserved. We then compute agreement@dynamic- between original and shuffled attribution profiles, which are consequently averaged over the dataset. If the threshold for estimation is strong, the agreement with the baseline for each method should be lower than the agreement with other methods.\n\nResults for the mean are given in Table 2. We find that for the mean, various methods have higher baseline agreement than others. This can be explained by the higher values of the mean for this threshold. Importantly, both methods have method–method agreement scores that do not beat the baseline (which pseudo-randomly selects tokens), neither on the token level nor on the span level. With regards to the median, multiple methods do not beat their baselines either. The threshold as mean plus standard deviation instead does, for all methods and both on tokens and on spans. This is an indication that the latter might be a better threshold for dynamic estimation. An additional interpretation of why mean plus standard deviation works better than mean is that negative local maxima in the attribution profiles are hereby ignored, leading to less but more important tokens (and spans) to be targeted."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Discussion",
            "text": "Analysing disagreement from a linguistic perspective helps us to better understand the differences between attribution methods. We briefly discuss the implications of token- and span-level analyses on other tasks than NLI. With an eye on the ability and reliability of these methods to reflect the model’s decision process, we also consider the implications for the faithfulness aspect in interpretability research.\nGenerally speaking, an NLI task is sufficiently challenging that it avoids sentences of different classes (e.g. contradiction, entailment) differing by exactly one word. It is therefore fair to expect methods to target the same span and not to penalise them for disagreeing on the token level. However, targeting a modifier instead of its syntactic head can make a big difference for other tasks. Additionally, the span-token ratio should determine the difficulty of assessing span-level agreement compared to tokens.\nThe choice of considering spans rather than tokens should therefore be weighted against the type of task and data.\nOn a similar note, §3.2  ###reference_### describes the systematic differences in punctuation preferences. We may hypothesise that methods that consistently include full stops in their top- are actually catching the signal’s onset (locality information) rather than the full stop being itself a signal (lexical information). To this end, our choice of treating punctuation as separate spans might have influenced the span agreement of such methods. More research is necessary to disentangle locality from lexical information.\nAgreement is linked with both plausibility and faithfulness. We considered plausibility when estimating dynamic  thresholds, as we aimed for s close to human preference. However, a more direct way of testing for plausibility in this context is by assessing human–method agreement, which we mostly left out of scope in this study. To that end, we did find that agreement results are constant on both tokens and spans, possibly suggesting that human–method agreement reaches a ceiling already at the token level (i.e. tokens are targeted that belong to different signals in the sentence). This interpretation might even hold for more faithful methods. In fact, models do often not rely on the same patterns as humans do, instead resorting to shortcut signals.\nMeasuring faithfulness, on the other hand, is less straightforward. Following Jain and Wallace (2019  ###reference_b15###), who state that faithful attention-based explanations should be agreeable, we carefully extend their perspective in that agreement between method-generic explanations can be considered as a proxy for faithfulness. According to the principle of reproducibility in science (Popper, 2005  ###reference_b27###), a finding that is confirmed through different means is, in principle, more likely to be correct. As such, if two attribution methods with distinct means yield similar results, they are likely similarly (un)faithful. If one method disagrees with the majority of the batch, either the one, the majority, or all are unfaithful. Because of the reproducibility principle, however, it is more likely that the majority is more faithful.\n Given that some methods might highly correlate with other methods by design, one must be careful at drawing conclusions. Constructing a batch of methods that is representative of different ways of interpreting the model is, for this reason, not a simple task."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion and Future Directions",
            "text": "In this study, we approached post-hoc explanation disagreement from a syntactic perspective. We found that methods that agree most with other methods and with aggregated scores of human rationales have similar POS tag preferences for the targeted tokens. We then determined that attribution methods agree more at the span level than at the token level, which appear to be similarly difficult tasks at low values. One particular reason for disagreement is the consistent preference by one method to target the determiners instead of the noun head within the same noun phrase. We showed that dynamic works well in combination with spans, as it seeks for non-neighboring important signals in the sentence. Finally, we empirically tested for different thresholds of the global importance setting of dynamic, suggesting a value that accounts for both negative attribution scores and results in low values.\n\nOne issue that dynamic aims to tackle is the targeting of redundant tokens as signals in the same span. To complement this, a more in-depth analysis would provide a better understanding about the way that different methods concentrate their targeted tokens in the same spans. Intuitively, for a fixed value, some methods highlight tokens that are more sparse across the instance, whereas others more quickly concentrate targeted tokens within the same spans. To obtain such a concentration metric, one could measure how rapidly a set of tokens belonging to the most important ground truth span are being targeted at increasing values.\n\nFuture directions of research include the exploration of different local importance criteria in the dynamic algorithm, such as different windows (current 1 versus 2, 3). Another is to exploit (syntactic) span-based information to improve interpretability accuracy at the token level, or to improve explanation aggregation techniques. Finally, we advise future evaluation datasets based on multiple annotators’ rationales to preserve specific instance–annotator mappings in the metadata. This would facilitate new directions in assessing the plausibility of attribution methods, specifically how variations in human subjectivity relate to agreement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Ethical Considerations",
            "text": "We would like to reiterate that attribution scores cannot be blindly relied upon to precisely determine model functioning, as they can be influenced by experimental factors such as task and model performance. To avoid drawing generalised conclusions, it is advisable to employ multiple metrics when studying feature attribution."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Acknowledgements",
            "text": "Jonathan Kamp’s research was funded by the Dutch National Science Organisation (NWO) through the project InDeep: Interpreting Deep Learning Models for Text and Sound (NWA.1292.19.399). Antske Fokkens was supported by the EU Horizon 2020 project InTaVia: In/Tangible European Heritage - Visual Analysis, Curation and Communication (http://intavia.eu) under grant agreement No. 101004825. Lisa Beinborn’s work was funded by the Dutch National Science Organisation (NWO) through the VENI program (Vl.Veni.211C.039). We would like to thank the anonymous reviewers for their valuable contribution."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Bibliographical References",
            "text": ""
        }
    ]
}