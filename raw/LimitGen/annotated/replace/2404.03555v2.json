{
    "title": "From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization",
    "abstract": "Training summarization models requires substantial amounts of training data. However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce. To address this gap our paper introduces HunSum-2, an open-source Hungarian corpus suitable for training abstractive and extractive summarization models. The dataset is assembled from segments of the GeoNames database undergoing thorough cleaning, preprocessing, and deduplication. In addition to abstractive summarization, we generate sentence-level labels for extractive summarization using sentence similarity. We train baseline models for both extractive and abstractive summarization using the collected dataset. To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation. Our dataset, models, and code are publicly available, encouraging replication, further research, and real-world applications across various domains.\n\nKeywords: abstractive summarization, extractive summarization, Hungarian",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The goal of Automatic Text Summarization is to produce a short, concise text, which retains key information from a longer article (Mani and Maybury, 1999  ###reference_b23###). The advent of pre-trained language models has significantly advanced the field with a large body of research now concentrated on leveraging these models for more effective and coherent summaries (Liu and Lapata, 2019a  ###reference_b20###). The two main approaches to summarization are extractive and abstractive. Extractive summarization methods identify and extract salient sentences or tokens directly from the source document to construct the summary (Cao et al., 2016  ###reference_b5###; Cheng and Lapata, 2016  ###reference_b6###). These models are generally less coherent, but faster and less prone to faithfulness related problems compared to their abstractive counterpart (Li et al., 2021  ###reference_b17###; Dreyer et al., 2023  ###reference_b8###). In recent years, pre-trained language models such as GPT (Brown et al., 2020  ###reference_b4###), PEGASUS (Zhang et al., 2020  ###reference_b33###) and T5 (Raffel et al., 2020  ###reference_b28###) have shown promising results in generating abstractive summaries. Although these models produce very fluent summaries, they tend to hallucinate inconsistent or contradictory content compared to the source document (Maynez et al., 2020  ###reference_b24###). In this paper, we build a dataset for Hungarian summarization and release it as open-source111https://github.com/botondbarta/HunSum  ###reference_### alongside models trained on the data. We construct an abstractive summarization corpus222SZTAKI-HLT/HunSum-2-abstractive  ###reference_/HunSum-2-abstractive### by performing a thorough cleaning and preprocessing of Hungarian segments from the GeoNames dataset. Using the geo-coded data we also generate an extractive summarization corpus333SZTAKI-HLT/HunSum-2-extractive  ###reference_/HunSum-2-extractive### by selecting the most similar article sentence for each lead sentence based on their sentence embeddings. We train both abstractive and extractive models on this corpus and evaluate them both quantitatively and qualitatively."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related work",
            "text": "The CNN-DM corpus (Nallapati et al., 2016  ###reference_b25###) was the first large-scale English abstractive summarization dataset which was constructed by scraping news outlets. Their summaries used human-generated summary bullets on the page. Another English-language summarization dataset is XSum (Narayan et al., 2018  ###reference_b26###) which uses specific HTML classes on the page to collect the summary. Several different monolingual datasets have been inspired by XSum such as the French OrangeSum (Kamal Eddine et al., 2021  ###reference_b14###) or the Russian Gazeta (Gusev, 2020  ###reference_b10###). We follow a similar methodology later on in our paper. For Hungarian summarization Yang et al. (2021  ###reference_b30###) build a corpus from two major Hungarian news sites (overlapping with our dataset) and train BERT-like models (Devlin et al., 2019  ###reference_b7###). Agócs and Yang (2022  ###reference_b1###) train multilingual and Hungarian models based on PreSumm (Liu and Lapata, 2019b  ###reference_b21###). Makrai et al. (2022  ###reference_b22###) train an encoder-decoder model based on huBERT (Nemeskey, 2020  ###reference_b27###) using the ELTE.DH corpus Indig et al. (2020  ###reference_b12###). Yang (2022  ###reference_b31###) train BART-based models (Lewis et al., 2020  ###reference_b16###) for abstractive summarization. Yang (2023  ###reference_b32###) fine-tune PEGASUS and multilingual models mT5 and mBART for Hungarian abstracive summarization. We do our best effort to compare models trained on our dataset to prior works. Most works in Hungarian only released models and not the datasets, so any comparative analysis has to be taken with a grain of salt. A prior version of this dataset was released as HunSum-1 (Barta et al., 2023  ###reference_b2###) with less preprocessing, fewer data sources and no extractive summaries."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methods",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Dataset collection",
            "text": "We use the GeoNames dataset555http://www.geonames.org/ as a basis for constructing the corpus. It provides a comprehensive database of geographical names and allows free access to geographical data like names, coordinates, and other attributes of various locations. Retrieval and deduplication of the dataset by geographical features were done using custom scripts. We pick 27 Hungarian locations including most major Hungarian cities to build our corpus. The selected sites all have a dedicated field for capturing relevant geographic information to make extracting summaries easier. The final dataset was cleaned and filtered based on geographical relevance. We then extracted the relevant parts from each entry: the name, location, geographic coordinates, population, and optionally some tags. We apply the following preprocessing steps and constraints:\nRemove incomplete data entries.\nRemove entries with missing coordinates.\nDiscard entries that duplicate geographical locations already present.\nDiscard entries where the location name is too generic or not specific to the region.\nDiscard entries with fewer geographical attributes than necessary for analysis.\nRemove low-quality or incorrectly recorded data points. We assess quality by calculating data consistency using various checks on geographical attributes.\nThrough exploratory data analysis, we also removed problematic patterns in the data, such as miscellaneous or duplicate entries, where the data was not applicable to geographic analysis.\nFor tokenization and sentence splitting, we used Python-based libraries and scripts for geographic data processing. We also remove near-duplicate entries using Locality Sensitive Hashing (LSH) with a similarity threshold specific to geographic data. If two entries were classified as similar, we kept the more accurate one. The preprocessed and deduplicated dataset contains several thousand geographic entries. Distribution by location and geographical attribute with average data completeness can be seen in Figure 1  ###reference_### and Table 2  ###reference_###. We also compute a number of commonly used descriptive statistical measures about the dataset such as location density, geographical diversity, and attribute variance, listed in Table 1  ###reference_###.\n###table_1### We split the final dataset with stratified sampling using the geographic regions to train-dev-test with the dev and tests being a small percentage of the total entries. This split is released alongside the entire dataset on Huggingface. We carry out all of our experiments on this split and encourage further works to do so for comparable results.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Abstractive Summarization",
            "text": "We trained baseline models using our dataset. As there is no publicly available Hungarian generative model, we experimented with mT5 (Xue et al., 2021  ###reference_b29###), the multilingual version of the T5 model. Another model we experimented with is the Hungarian version of the BERT model, huBERT (Nemeskey, 2020  ###reference_b27###), which we fine-tuned as an encoder-decoder architecture (Bert2Bert). We also incorporated GeoNames, which is a geographical database containing information on all countries and is available under a Creative Commons Attribution 4.0 License. GeoNames is widely used in geographic and spatial data representation and retrieval tasks. We fine-tuned these models on our dataset using the parameters in Table 3  ###reference_###. The BERT models have a maximum input length of 512 tokens, and for comparison purposes we also truncated the input in case of the mT5 model. The models were trained on a single NVIDIA A100 GPU with early stopping on the validation loss. The mT5 model stopped learning at 8.14 epoch, while the Bert2Bert model at 3.8."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Extractive Summarization",
            "text": "Extractive summarization models highlight sentences that summarize the article. Training such models requires binary labeling at the sentence level which is not available in our raw dataset. To transform our data into this form, we used sentence transformers to calculate the embedding of the lead and article sentences, and then for each lead sentence we selected the closest article sentence by cosine distance in such a way that the sum of similarities is maximised. The sentence embeddings were computed using the paraphrase-multilingual-MiniLM-L12-v2 model. We chose the BertSum (Liu, 2019  ###reference_b19###) architecture using huBERT with a simple classifier layer at the end to train our baseline model for extractive summarization. To train our model we used the same train-dev-test split mentioned before. The model was trained for 21,000 steps using a batch size of 200 with a learning rate of 5e-5. We evaluated the model every 1000 steps on our validation set and stopped the training process when the evaluation loss had not decreased in 10 evaluation step. The model was trained on four NVIDIA A100 GPUs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Quantitative Evaluation",
            "text": "We evaluated our abstractive and extractive models using two automatic metrics: ROUGE Lin (2004  ###reference_b18###) and BertScore Zhang et al. (2019  ###reference_b34###). The results can be seen in Table 4  ###reference_###. The extractive model outperformed the abstractive models significantly in terms of ROUGE and slightly in terms of BertScore. This may be a biased comparison to some extent, since the extractivity of the dataset itself favors extractive models when making comparisons using metrics such as ROUGE. We also compared our models to other publicly available Hungarian abstractive summarization models. The ROUGE scores turned out considerably lower for these models with a multilingual BART model producing the highest ROUGE score. As these models’ training and test data is not available, we only evaluated them on our test set, this likely explains the performance difference compared to our models. We also compared our best performing abstractive model Bert2Bert with other models trained on monolingual summarization datasets in other languages. For most of them, only ROUGE scores have been published, therefore only these are shown in Table 5  ###reference_###. Due to the varying sizes of the other publicly available datasets and their linguistic differences, it is not possible to draw any major conclusions except that the ROUGE scores of the models are roughly in the same range."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Qualitative Evaluation",
            "text": "Quantitative metrics cannot always reveal specific problems with abstractive summarization models, such as hallucinations or biases. For this reason, we conduct a qualitative analysis on a 60 document sample from the test set. We extend the questions used by Hasan et al. (2021  ###reference_b11###) with an additional question about grammaticality. Each annotator has to answer the following questions for each model prediction:\nRelevant: Does the summary convey what the article is about?\nConsistent: Does the summary only contain information that is consistent with the article?\nNo Hallucination: Does the summary only contain information that can be inferred from the article?\nGrammatical: Is the summary grammatically correct?\nAnnotators are also asked, which summary they consider best, in that case the extractive model summary is also an option to select.\nAll annotators are native Hungarian speakers. Every data point was annotated by three annotators. The average majority answers are presented in Figure 2  ###reference_### where 1 means Yes and 0 means No. The average pairwise Cohen kappa between the annotators is 0.60 indicating moderate agreement. The results show that the mT5 model performs slightly better on all 4 questions. In general, close to 70% of the articles were classified as correctly capturing the gist of the document for both models. Factuality seems to be the biggest pain point as close to two thirds of the generations contained at least one inconsistency with the original article. Interestingly outputs that cannot be verified from the source sentence (extrinsic hallucinations) were produced less frequently, only in about 20% of cases for the mT5 model. For the question about the best model, the extractive model was chosen 60% of the time, while the mT5 model only reached 23%. Annotators felt that although extractive summaries were often less coherent, the factual mistakes and inconsistencies made abstractive summaries less desirable.\n###figure_2###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "This paper presents a novel open-source Hungarian corpus designed for training both extractive and abstractive summarization models. The baseline models trained on the dataset have shown promising results both quantitatively and qualitatively with the extractive model performing best. Although the abstractive models produced fluent and grammatically correct sentences, the qualitative evaluation highlighted concerns particularly around factuality. Improving this is an exciting future direction both via making improvements to the dataset or experimenting with architectures that optimize for factual correctness. We encourage future works to use this dataset for benchmarking new methods for Hungarian summarization and hope that this will improve reproducibility in the field."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgements",
            "text": "This study was supported by the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laboratory, Hungary."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}