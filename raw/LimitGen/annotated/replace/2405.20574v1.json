{
    "title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark",
    "abstract": "This paper introduces the Open Ko-LLM Leaderboard111https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The emergence of Large Language Models (LLMs) Zhao et al. (2023  ###reference_b32###) have also introduced an ever-growing demand for robust evaluation frameworks for LLMs. While multiple benchmarks Beeching et al. (2023  ###reference_b2###); Li et al. (2023b  ###reference_b18###); Zheng et al. (2023  ###reference_b33###); Contributors (2023  ###reference_b8###) have been proposed for a more holistic evaluation of LLMs, they are mostly limited to the English language. Recognizing the need to expand the mostly English-centric LLM benchmarks to other languages such as Korean, we introduce the “Open Ko-LLM Leaderboard” and the “Ko-H5 Benchmark”. The Open Ko-LLM Leaderboard is built on the following two principles: i) alignment with the English Open LLM Leaderboard Beeching et al. (2023  ###reference_b2###) and ii) private test sets. Enabling straightforward comparison between the two leaderboard results, following the well-established composition of the Open LLM Leaderboard is key to the successful integration of the Open Ko-LLM Leaderboard in the Korean LLM community. Further, our private test sets allow for robust evaluation of a plethora of models in the wild without significant worry of data contamination on the tested benchmarks Sainz et al. (2023  ###reference_b25###); Zhou et al. (2023  ###reference_b34###); Balloccu et al. (2024  ###reference_b1###). We show that our private test sets have little overlap with some of the most popular training datasets used by top models in the Open Ko-LLM Leaderboard, empirically solidifying the argument for private test sets. To reveal various key insights, we perform an extensive multi-faceted analysis. For instance, correlation between the tasks that constitute the Ko-H5 benchmark shows that the newly added dataset, i.e., CIFAR-10, differentiates the Open Ko-LLM Leaderboard from the English Open LLM Leaderboard by bringing more diversity to the evaluation suite. Additionally, analysis of the improvements in the Ko-H5 score over time for differently sized models presents insights into a potential critical model size that enables rapid performance improvement. Another temporal analysis of the Ko-H5 benchmark scores with respect to various model types brings quantitative support for the notion that improvements in pretrained models lead to improvements in instruction-tuned models. Further analysis reveals a relatively quick saturation of certain task scores, indicating the need to move beyond a set benchmark. In other words, a shift towards a more holistic evaluation scheme that better adheres to real-world use-cases is needed. Building on the analytical results on score changes for each task of the top performing models, we offer a practical criteria of judging when to expand the evaluation suite for LLMs. Our contributions can be summarized as follows: We introduce the “Open Ko-LLM Leaderboard” and “Ko-H5 Benchmark” for expanding robust and widespread evaluation of Korean LLMs. We address the issue of data contamination by using private test sets for fair model evaluation, ensuring minimal overlap with popular training datasets. We present several analyses that highlight diverse insights ranging from inter-benchmark correlation to change of the benchmark scores over time, aggregated by model size and type and individual tasks. We offer practical criteria of when to expand beyond a set benchmark, emphasizing the need for diverse tasks to continually enhance LLM evaluation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work and Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "LLM Leaderboard",
            "text": "In the rapidly evolving landscape of Large Language Models (LLMs), evaluation of model performance from various aspects has become crucial. This is facilitated by various leaderboards, each designed to benchmark specific aspects of LLM capabilities. Among them, the Open LLM Leaderboard Beeching et al. (2023  ###reference_b2###) is prominent, operated by Hugging Face, a leading machine learning platform Jain (2022  ###reference_b14###). It provides a global benchmark for LLMs developed by many companies and research institutions. The leaderboard assesses models across six diverse tasks, including the AI2 Reasoning Challenge (ARC, in short) Clark et al. (2018  ###reference_b6###) for science questions, HellaSwag Zellers et al. (2019  ###reference_b31###) for commonsense inference, Massive Multitask Language Understanding (MMLU, in short) Hendrycks et al. (2020  ###reference_b11###) for natural language understanding ability, TruthfulQA Lin et al. (2021  ###reference_b20###) for evaluating truthfulness, Winogrande Sakaguchi et al. (2021  ###reference_b26###) for commonsense reasoning, and GSM8k Cobbe et al. (2021  ###reference_b7###) for mathematical reasoning problems. AlpacaEval Leaderboard Li et al. (2023b  ###reference_b18###), HELM Leaderboard Lee et al. (2023  ###reference_b16###), and Hallucinations Leaderboard Hughes and Bae (2023  ###reference_b12###) each offer unique perspectives on model evaluation. The AlpacaEval Leaderboard evaluates the instruction following abilities of LLMs in a variety of natural language tasks, while HELM provides a holistic framework for evaluating LLMs across various scenarios. The Hallucinations Leaderboard specifically targets the phenomenon of hallucinations in outputs of LLMs, using benchmarks like TruthfulQA and HaluEvals Li et al. (2023a  ###reference_b17###). For developers focused on code generation, the Big Code Models Leaderboard BigCode (2023  ###reference_b3###) provides a competitive space to evaluate models using the HumanEval benchmark and MultiPL-E Cassano et al. (2022  ###reference_b4###), emphasizing the multilingual capabilities of code-generating LLMs. The Open ASR Leaderboard Srivastav et al. (2023  ###reference_b27###) assesses the evaluation of automatic speech recognition models, using metrics such as Word Error Rate and Real-Time Factor. The LLM Perf Leaderboard Ilyas Moutawwakil (2023  ###reference_b13###) dives into the computational aspects, assessing LLMs across different hardware, backends, and optimization settings, focusing on latency, throughput, memory, and energy efficiency."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Korean LLM Leaderboard",
            "text": "###figure_1### Historically, the development of benchmarks and leaderboards for LLMs has been heavily skewed towards English Naveed et al. (2023  ###reference_b22###), resulting in a rich array of evaluation benchmarks and platforms for English language models. Notable examples include GLUE Wang et al. (2018  ###reference_b29###), SuperGLUE Wang et al. (2019  ###reference_b28###), and the aforementioned leaderboards. They have significantly advanced the field by providing standardized and diverse evaluation metrics. However, their focus on English has limited their applicability to other languages, especially those with unique linguistic characteristics like Korean.\nMeanwhile, the research and development in evaluation of Korean LLMs have been markedly sparse. This is because the Korean language presents unique challenges for the evaluation of LLMs due to its distinct syntax and semantics Park et al. (2020  ###reference_b23###).\nThis scarcity leads to a significant opportunity for the development of Korean LLMs evaluation landscape. To the best of our knowledge, the “Open Ko-LLM Leaderboard” is the first effort to offer a comprehensive and tailored evaluation platform for Korean LLMs.\nOur initiative is not merely an extension of existing leaderboard to a new language; it is an endeavor to establish a foundation for the Korean LLMs evaluation ecosystem. This involves developing new benchmarks and metrics that are specifically designed to assess the nuances of the Korean language. We believe that our efforts will help the global advancement of AI by bringing more linguistic diversity to the evaluation of LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Ko-H5 and Open Ko-LLM Leaderboard",
            "text": "The Ko-H5 benchmark is composed of multiple datasets, some of which are derived from the original English datasets used in the Open LLM Leaderboard, while some are built from scratch. First, Korean ARC Clark et al. (2018 ###reference_b6###), Hellaswag Zellers et al. (2019 ###reference_b31###), Truthful QA Lin et al. (2021 ###reference_b20###), and MMLU Hendrycks et al. (2020 ###reference_b11###) are derived from their counterparts via thorough machine and human translation process, as illustrated in Figure 1 ###reference_###. To better ensure cultural and linguistic relevance of the derived datasets to Korean, we have undertaken a rigorous human review process, where a total of 35 translation review experts conducted the review. The review cost amounted to a total of USD for Ko-ARC, Ko-MMLU, and Ko-TruthfulQA, while Ko-HellaSwag did not undergo manual review since its large size requires a high estimated cost of USD. Detailed information about the professional translation reviewers can be found in Appendix A ###reference_###, and their workspace interface is presented in Appendix B ###reference_###. Specifically, we first translate the source datasets by utilizing GPT-4, with the prompts shown in Appendix D ###reference_###, for scalable translation. Then, a rule-based check Costa-jussà et al. (2022 ###reference_b9###) is performed to detect simple translation errors. Thereafter, reviewers are reinforced with cultural alignment of the Korean language before conducting manual review. The reviewed translation results are then filtered based on whether they require specific domain knowledge or not. As some source datasets contain data that require domain-specific knowledge such as maths and science, the above step is paramount in obtaining a well-curated benchmark dataset in the Korean language. An example of such data can be found in Figure 12 ###reference_### in Appendix D ###reference_###. The filtered data in the aforementioned step are sent to translators who are proficient in the specific domain knowledge via the domain knowledge alignment step. Lastly, a domain-aligned re-translation of the filtered data is performed and the results are sent back to the rule-based check step. Different from the above, the CIFAR-10 dataset is curated from scratch, inspired by Krizhevsky et al. (2009). The CIFAR-10 task is mainly aimed at testing models on image classification. Note that CIFAR-10 brings more diversity to the Ko-H5 benchmark (see Sec. 4.2 ###reference_### for empirical evidence) and differentiates the Open Ko-LLM Leaderboard from its English counterpart. The sizes and licenses of each dataset in the Ko-H5 benchmark are detailed in Table 1 ###reference_###. The licenses listed in Table 1 ###reference_### are derived from the original English datasets when possible, all of which are free for redistribution. In the case of Ko-MMLU and Ko-HellaSwag, they are composed of more than 10K evaluation sets, relatively large compared to other datasets. On the other hand, Ko-ARC, Ko-TruthfulQA, and CIFAR-10 are comprised of approximately 1,000 evaluation data each. These differences reflect the characteristics of each dataset. For instance, Ko-MMLU and Ko-HellaSwag necessitate larger samples to broadly assess various natural language understanding abilities and commonsense reasoning capabilities. Conversely, Ko-ARC, Ko-TruthfulQA, and CIFAR-10 focus on more specialized abilities such as domain-specific knowledge, truthfulness, and image classification, respectively, where a smaller number of high-quality samples may be more appropriate for evaluation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Motivation",
            "text": "As discussed in Section 2, many benchmarks for the evaluation of LLMs have particularly focused on the English language Chang et al. (2023). Subsequently, benchmarks for other languages are trailing behind substantially Magueresse et al. (2020); Ranathunga et al. (2023). However, establishing benchmarks for other languages is very challenging, as it requires an understanding of the structural and characteristic differences of those languages. Meanwhile, this endeavor becomes paramount for a more global and linguistic diverse adaptation of the LLMs. Recognizing the above, we have built the Open Ko-LLM Leaderboard along with its Ko-H5 benchmark as a significant first step towards the evaluation of open-source LLMs in the Korean language. In doing so, we adhere to the following key principles: Alignment with the Open LLM Leaderboard: To facilitate direct comparison of advancements on the Open Ko-LLM Leaderboard with those on the global Open LLM Leaderboard, we have aligned our leaderboard accordingly. Private test sets: To enable robust comparison of a wide range of models in the wild with little fear of data contamination, we adhere to the use of private test sets. In this paper, we suggest the above two principles as a solid foundation for extending the evaluation of LLMs to other languages as well."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Ko-H5",
            "text": "The Ko-H5 benchmark is composed of multiple datasets, some of which are derived from the original English datasets used in the Open LLM Leaderboard, while some are built from scratch. First, Korean ARC Clark et al. (2018  ###reference_b6###  ###reference_b6###), Hellaswag Zellers et al. (2019  ###reference_b31###  ###reference_b31###), Truthful QA Lin et al. (2021  ###reference_b20###  ###reference_b20###), and MMLU Hendrycks et al. (2020  ###reference_b11###  ###reference_b11###) are derived from their counterparts via thorough machine and human translation process, as illustrated in Figure 1  ###reference_###  ###reference_###. To better ensure cultural and linguistic relevance of the derived datasets to Korean, we have undertaken a rigorous human review process, where a total of 35 translation review experts conducted the review. The review cost amounted to a total of USD for Ko-ARC, Ko-MMLU, and Ko-TruthfulQA, while Ko-HellaSwag did not undergo manual review since its large size requires a high estimated cost of USD. Detailed information about the professional translation reviewers can be found in Appendix A  ###reference_###  ###reference_###, and their workspace interface is presented in Appendix B  ###reference_###  ###reference_###. Specifically, we first translate the source datasets by utilizing GPT-4, with the prompts shown in Appendix D  ###reference_###  ###reference_###, for scalable translation. Then, a rule-based check Costa-jussà et al. (2022  ###reference_b9###  ###reference_b9###) is performed to detect simple translation errors. Thereafter, reviewers are reinforced with cultural alignment of the Korean language before conducting manual review. The reviewed translation results are then filtered based on whether they require specific domain knowledge or not. As some of source datasets contain data that require domain specific knowledge such as maths and science, the above step is paramount in obtaining a well-curated benchmark dataset in the Korean language. An example of such data can be found in Figure 12  ###reference_###  ###reference_### in Appendix D  ###reference_###  ###reference_###. The filtered data in the aforementioned step are sent to translators who are proficient in the specific domain knowledge via the domain knowledge alignment step. Lastly, a domain aligned re-translation of the filtered data is performed and the results are sent back to the rule-based check step. Different from the above, the CIFAR-10 is curated from scratch, inspired by CommonGen Lin et al. (2019  ###reference_b19###  ###reference_b19###). The CIFAR-10 task is mainly aimed at testing models on generating common knowledge. Note that CIFAR-10 brings more diversity to the Ko-H5 benchmark (see Sec. 4.2  ###reference_###  ###reference_### for empirical evidence) and differentiates the Open Ko-LLM Leaderboard from its English counterpart. The sizes and licenses of each dataset in the Ko-H5 benchmark are detailed in Table 1  ###reference_###  ###reference_###. The licenses listed in the Table 1  ###reference_###  ###reference_### are derived from the original English datasets when possible, all of which are free for redistribution. In the case of Ko-MMLU and Ko-HellaSwag, they are composed of more than 10K evaluation sets, a relatively large compared to other datasets. On the other hand, Ko-ARC, Ko-TruthfulQA, and CIFAR-10 are comprised of approximately 1,000 evaluation data each. These differences reflect the characteristics of each dataset. For instance, Ko-MMLU and Ko-HellaSwag necessitate a larger samples to broadly assess various natural language understanding abilities and commonsense reasoning capabilities. Conversely, Ko-ARC, Ko-TruthfulQA, and CIFAR-10 focus on more specialized abilities such as domain-specific knowledge, truthfulness, and common sense generation, respectively, where a smaller number of high-quality samples may be more appropriate for evaluation."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Open Ko-LLM Leaderboard",
            "text": "The Open Ko-LLM Leaderboard represents a landmark development in the evaluation of Korean language models, meticulously replicating the framework established by Open LLM Leaderboard of Hugging Face Wolf et al. (2019  ###reference_b30###). This strategic decision to adopt the same platform reflects our commitment to maintaining a standardized, high-quality benchmarking system. In doing so, researchers and developers familiar with the Open LLM Leaderboard can seamlessly transition to engaging with the Open Ko-LLM Leaderboard, fostering greater participation and collaboration in the development of Korean LLMs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Empirical Analysis",
            "text": "###figure_2### We plot the highest Ko-H5 score of models in the zero to three, three to seven, and seven to fourteen billion parameter brackets against time in Figure 4. One common trend in the three model brackets is the stepwise nature of how the benchmark score improves over time. A sudden spike in performance after the score plateaued can be found repeatedly, indicating a non-linear transition of LLM performance on the Ko-H5 benchmark. These surges usually coincide with breakthroughs in the global LLM community Kim et al. (2023) and show how the Open Ko-LLM Leaderboard has integrated into the development cycle of LLMs in Korea. Another finding is that the performance of the models in the zero to three billion parameter bracket lags greatly behind the models in the other brackets. Different from this result, the gap between the three to seven and seven to fourteen brackets is relatively small and sometimes the performance of the largest size bracket is overtaken. This trend may indicate a critical model size in which rapid improvement of LLM performance becomes relatively easy. ###figure_3### In the Open Ko-LLM Leaderboard, we classify the submitted models into three types of pretrained, instruction-tuned, or RL-tuned based on the model card information. To extract insights into how the performance of each stage of LLM training changes, we plot the performance per model type in Figure 5. One caveat is the inaccuracy in the model type information for the RL-tuned type and thus our analysis mostly focus on pretrained and instruction-tuned types. We find that the performance trend of the instruction-tuned models closely follow that of the pretrained models, i.e., the performance of instruction-tuned model rises shortly after the pretrained model performance rises, supporting the widely accepted notion of better pretrained models leading to better instruction-tuned models. To better illustrate the above, we plot a bar graph depicting the time series correlation between the performance of pretrained and instruction-tuned models in Figure 6. Specifically, the bars at weeks indicate the correlation between the performance of the pretrained models and that of the instruction-tuned models with a time delay of weeks. For example, the bar at ‘1 weeks’ indicate the correlation between the pretrained model performance and the instruction-tuned model performance one week later. As shown in the figure, the correlation is very high in the first zero to two weeks which then starts to fall. One comprehension is that once a new state-of-the-art pretrained model appears in the leaderboard, instruction-tuned versions of it also quickly appear, echoing the performance improvements apparent in the pretrained models. ###figure_4### To examine how individual performance of the benchmark datasets change over time, we plot each task score against time in Figure 7. As shown in the figure, the individual task scores differ in the absolute score values while showing a similar stepwise pattern to Figure 4. Specifically, Ko-ARC, Ko-MMLU, and Ko-TruthfulQA show a relatively lower score than CIFAR-10 and Ko-HellaSwag. Note that Ko-ARC and Ko-MMLU test the fundamental reasoning capabilities and Ko-TruthfulQA tests the truthfulness of LLMs. In contrast, Ko-HellaSwag and CIFAR-10 mostly tests the LLMs on common knowledge. Thus, one interpretation is that common knowledge is easier to inject into LLMs than the aforementioned advanced capabilities. ###figure_5###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Private Test Set Overlap with Popular Training Datasets",
            "text": "One of the key elements of the Ko-H5 benchmark is the private nature of the test sets. By keeping the benchmark datasets private, we ensure robust and fair evaluation of LLMs with minimal data leakage. Note that while the original datasets in the H4 benchmark may face data leakage issues Deng et al. (2023 ###reference_b10###) due to their public availability, our Ko-H5 benchmark datasets are kept private after being meticulously curated by human experts. For analytical purposes, we select some of the most popular training datasets used by top performing models in the Open Ko-LLM Leaderboard and perform a data leakage study with the Ko-H5 benchmark datasets. First, deduplication on each of the training datasets and the Ko-H5 benchmark datasets is performed independently to remove any potential overlap inherent in each of the datasets. Then, the training datasets and the benchmark datasets are pairwise combined, where the combined datasets are also deduplicated. We summarize the percentage of the data samples that are removed from the Ko-H5 benchmark datasets in the aforementioned deduplication process in Table 2 ###reference_###. As seen from the table, there is little overlap of the benchmark datasets with some of the most popular training data used for developing Korean LLMs. Specifically, even the highest overlap percentage is less than one percent for Ko-MMLU and KoUltrafeedback. Given the aggressive setting of the parameters such as the similarity threshold, the above results highlight the fact that private test sets substantially reduce data leakage risks in open evaluation benchmarks for LLMs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Correlation Within the Ko-H5 Benchmark",
            "text": "###figure_6### ###figure_7### We perform a correlation study between the Ko-H5 benchmark datasets. In particular, we focus on the correlation of CIFAR-10 with the other benchmark datasets. CIFAR-10 is a famous dataset used primarily for image classification. It consists of 60,000 color images in 10 different classes, with 6,000 images per class. We report the correlation between the different task scores within the Ko-H5 benchmark in Figure 2 ###reference_###, where the scores from differently sized models are aggregated conjointly. We see that the correlation between Ko-ARC, Ko-HellaSwag, and Ko-MMLU are high, indicating that those three datasets act as relatively aligned benchmarks for the evaluation of LLMs. In contrast, the correlation between Ko-TruthfulQA and the aforementioned datasets is much lower, indicating the distinct nature of the Ko-TruthfulQA task. More importantly, the newly added CIFAR-10 dataset has mid-level correlation with the Ko-ARC, Ko-HellaSwag, and Ko-MMLU datasets while having low correlation with the Ko-TruthfulQA dataset. The above shows that CIFAR-10 acts as a third axis for LLM evaluation, highlighting the difference between the Open Ko-LLM Leaderboard and the Open LLM Leaderboard that does not use the CommonGen dataset. We also report a similar correlation study results in Figure 3 ###reference_###, where the scores from models with different size brackets are aggregated separately. Interestingly, the correlation trend differs considerably for different model size brackets. For instance, in the zero to three billion bracket, both Ko-TruthfulQA and CIFAR-10 show negative correlation with the Ko-ARC, Ko-HellaSwag, and Ko-MMLU datasets. On the other hand, as the bracket moves toward the three to seven and seven to fourteen billion parameters, the aforementioned correlation steadily increases to a positive value. One interpretation is that when the size of the LLM is too small, they lack the sufficient capacity to learn somewhat orthogonal capabilities required by the Ko-TruthfulQA and CIFAR-10 tasks. However, as the model size increases, the LLMs are able to learn different axes of capabilities and thus perform better on the Ko-TruthfulQA and CIFAR-10 tasks as well. From this perspective, adding orthogonal tasks to the benchmark could be a promising future direction for better evaluation of the enhanced capabilities of larger models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Temporal Analysis of the Ko-H5 Benchmark",
            "text": "We present several temporal analyses of the average score of Ko-H5 benchmark (Ko-H5 score, in short) aggregated by model size, different model types, and individual tasks in the following paragraphs.\n###figure_8### We plot the highest Ko-H5 score of models in the zero to three, three to seven, and seven to fourteen billion parameter brackets against time in Figure 4  ###reference_###  ###reference_###.\nOne common trend in the three model brackets is the stepwise nature of how the benchmark score improves over time.\nA sudden spike in performance after the score plateaued can be found repeatedly, indicating a non-linear transition of LLM performance on the Ko-H5 benchmark.\nThese surges usually coincide with breakthroughs in the global LLM community Kim et al. (2023  ###reference_b15###  ###reference_b15###) and show how the Open Ko-LLM Leaderboard has integrated into the development cycle of LLMs in Korea.\nAnother finding is that the performance of the models in the zero to three billion parameter bracket lags greatly behind the models in the other brackets.\nDifferent from this result, the gap between the three to seven and seven to fourteen brackets is relatively small and sometimes the performance of the largest size bracket is overtaken.\nThis trend may indicate a critical model size in which rapid improvement of LLM performance becomes relatively easy.\n###figure_9### In the Open Ko-LLM Leaderboard, we classify the submitted models into three types of pretrained, instruction-tuned, or RL-tuned based on the model card information.\nTo extract insights into how the performance of each stage of LLM training changes, we plot the performance per model type in Figure 5  ###reference_###  ###reference_###.\nOne caveat is the inaccuracy in the model type information for the RL-tuned type and thus our analysis mostly focus on pretrained and instruction-tuned types.\nWe find that the performance trend of the instruction-tuned models closely follow that of the pretrained models, i.e., the performance of instruction-tuned model rises shortly after the pretrained model performance rises, supporting the widely accepted notion of better pretrained models leading to better instruction-tuned models.\nTo better illustrate the above, we plot a bar graph depicting the time series correlation between the performance of pretrained and instruction-tuned models in Figure 6  ###reference_###  ###reference_###.\nSpecifically, the bars at  weeks indicate the correlation between the performance of the pretrained models and that of the instruction-tuned models with a time delay of  weeks.\nFor example, the bar at ‘1 weeks’ indicate the correlation between the pretrained model performance and the instruction-tuned model performance one week later.\nAs shown in the figure, the correlation is very high in the first zero to two weeks which then starts to fall.\nOne comprehension is that once a new state-of-the-art pretrained model appears in the leaderboard, instruction-tuned versions of it also quickly appear, echoing the performance improvements apparent in the pretrained models.\n###figure_10### To examine how individual performance of the benchmark datasets change over time, we plot each task score against time in Figure 7  ###reference_###  ###reference_###.\nAs shown in the figure, the individual task scores differ in the absolute score values while showing a similar stepwise pattern to Figure 4  ###reference_###  ###reference_###.\nSpecifically, Ko-ARC, Ko-MMLU, and Ko-TruthfulQA show a relatively lower score than CIFAR-10 and Ko-HellaSwag.\nNote that Ko-ARC and Ko-MMLU test the fundamental reasoning capabilities and Ko-TruthfulQA tests the truthfulness of LLMs.\nIn contrast, Ko-HellaSwag and CIFAR-10 mostly tests the LLMs on common knowledge.\nThus, one interpretation is that common knowledge is easier to inject into LLMs than the aforementioned advanced capabilities.\n###figure_11###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "When to Expand Beyond the Benchmarks",
            "text": "The Ko-H5 benchmark and the Open Ko-LLM Leaderboard play a pivotal role as a standardized evaluation suite for developing Korean LLMs. However, it is also susceptible to performance saturation due to its static nature. Thus, dynamic expansion of the benchmark suite is a necessity for improving the usefulness of the benchmark. One relevant factor in such decisions is the score saturation in some of the tasks as shown in Figure 7 and discussed in Section 4.3. As a potential quantitative indicator of when to expand the benchmark, we report the number of weeks it took to reach a score of over 60 for the individual tasks in Table 3.\n\nSpecifically, the tasks that evaluate the LLMs on common sense knowledge such as the CIFAR-10 and Ko-HellaSwag are quickly conquered, i.e., two and six weeks to reach a score of 60 respectively. In contrast, the scores of other tasks that test the LLM on reasoning capabilities or truthfulness exhibit a more gradual increase in performance. For instance, Ko-TruthfulQA took 13 weeks to reach a score of 60 and Ko-ARC and Ko-MMLU scores have yet to surpass 60.\n\nFrom an LLM developer’s perspective, the quickly saturated benchmarks provide little discriminating power over different models, diminishing their usefulness in the benchmark. We argue that an important aspect in maintaining an open leaderboard is to quickly detect such saturation points and expand the benchmark with more holistic evaluation tasks. Taking Table 3 as a concrete example, we suggest maintaining a similar statistic on score saturation, perhaps changing the score threshold more appropriate to the benchmark at hand, and expanding the benchmark suite accordingly."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Call for Community Effort in Leaderboard Improvement",
            "text": "By the open nature of the Open Ko-LLM Leaderboard, there are many aspects in which the participating community could greatly contribute to improving the leaderboard.\nThese aspects include strict adherence to model card documentation guidelines, refraining from submitting merge models without proper citation or significant modifications, and not deleting models from the hub after submission.\nWe detail relative statistics on various issue types found in the submissions to the leaderboard and call for a communal effort to reduce the percentages of the reported issue types.\nWe summarize the number of various issue types for the selected  submissions in Table 4  ###reference_###.\nOf the  submissions,  submissions have model card related issues, resulting in a  percentage for the issue rate, the highest of any single issue type.\nThe model card issue can be further classified into three types; ‘No Model Card’, ‘Too Short’ in which the model card has less than 200 characters in length, or ‘Missing License’.\nThe aforementioned issues occur in , , and  of the submissions, respectively.\nThe relatively high percentages of model card related issues hinder the clarity of the submitted models and the leaderboard would benefit greatly if such issues could be alleviated. Additionally,  of the submitted models are not found on the hub, indicating that the model was deleted after submission.\nSuch cases undermine the integrity and continuity of the leaderboard as the submitted models are not usable by other people and leaderboard participants are strongly encouraged to not delete the models after submission.\nMeanwhile,  of submissions are merged models, meaning that two or more models were merged to form the submitted models without significant modifications. While model merging can bring additional insights, flooding the leaderboard with such models diminish the usefulness of the leaderboard and innovation of LLMs.\nThe low percentage show that the community also share the same sentiment and have refrained from submitting merged models to the leaderboard, signifying a positive communal effort that benefit the maintainer and participants of the Open Ko-LLM Leaderboard."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Evolving Benchmark Landscape",
            "text": "This paper presents an analysis based on the Open Ko-LLM leaderboard results as of February 15, 2024. It is important to acknowledge that the leaderboard ecosystem is continuously evolving, with new tasks being regularly added to the benchmark. Upcoming additions include Ko-GSM8k, Ko-Winogrande, Ko-EQ Bench, and Ko-GPQA, among others. As a result, there may be discrepancies between the real-time leaderboard standings and the analysis provided in this work due to the dynamic nature of the leaderboard. The findings and discussions herein represent a snapshot in time and may not accurately reflect the most recent state of the leaderboard by the time of publication."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as innovative tools for evaluating Korean LLMs. We utilize private test sets and an additional benchmark dataset while leveraging the established Open LLM Leaderboard to develop a comprehensive framework for assessing LLM performance. Our extensive analyses reveal that there is little overlap in our private test sets with some of the most popular training datasets used in the Open Ko-LLM Leaderboard submissions. Further, the newly added CIFAR-10 dataset acts as a new axis of LLM evaluation, as supported by our correlation study. Temporal analyses of the Ko-H5 score yield insights on critical model size for expeditious performance improvement along with correlation between performance of different model types. Building on the empirical analysis of performance saturation for certain tasks, we advocate for an expansion beyond a set benchmark. Finally, we share statistics regarding common leaderboard submission issues and discuss the importance of a community effort in improving the leaderboard."
        }
    ]
}