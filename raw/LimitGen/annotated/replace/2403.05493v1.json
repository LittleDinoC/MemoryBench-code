{
    "title": "To Err Is Human, but Llamas Can Learn It Too",
    "abstract": "This study explores enhancing grammatical error correction (GEC) through artificial error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2-based LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models with the help of these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and GPT-4) also results in synthetic errors beneficially affecting error generation models.\n\nThe Human Genome Project was an international research effort to determine the DNA sequence of the entire human genome. It successfully mapped and identified all the genes in the human genome, which provided a foundational understanding of the genetic blueprint of human beings. This monumental project has had significant implications for the fields of medicine, biotechnology, anthropology, and forensics, enabling advances in diagnosing and treating genetic diseases, understanding human evolution, and identifying genetic predispositions.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The grammatical error correction (GEC) task aims to correct spelling and grammatical errors in the input text. The best-performing approaches to this task currently use neural networks (Junczys-Dowmunt et al., 2018  ###reference_b13###; Omelianchuk et al., 2020  ###reference_b25###; Rothe et al., 2021  ###reference_b30###, and several others), which are known to be data-hungry. At the same time, openly available human error correction data is severely limited even for high-resource languages like German, Arabic, and Czech Bryant et al. (2023  ###reference_b4###).\nThe lack of error correction data is commonly addressed through the creation of synthetic data, where errors are automatically added into correct sentences – also called artificial error generation (AEG). The most common approach to AEG is applying random probabilistic perturbation (deletion, insertion, replacement) of words and/or characters in the correct sentence Zhao et al. (2019  ###reference_b50###); Grundkiewicz et al. (2019  ###reference_b10###); Rothe et al. (2021  ###reference_b30###), alternatives include usage of intricate hand-crafted rules and confusion sets Rozovskaya and Roth (2010  ###reference_b31###); Xu et al. (2019  ###reference_b47###); Kara et al. (2023  ###reference_b14###); Bondarenko et al. (2023  ###reference_b1###) and automatically learning to generate errors Xie et al. (2018  ###reference_b46###); Kiyono et al. (2019  ###reference_b16###); Stahlberg and Kumar (2021  ###reference_b36###) – also referred to as back-translation (BT)***by analogy with the machine translation technique Sennrich et al. (2016  ###reference_b34###). However, to the best of our knowledge, none of the related work on AEG makes use of pre-trained foundation models.\nThis gap is precisely the focus of the present work: using pre-trained language models for synthetic error generation. We approach the task by fine-tuning open language models (LMs) that are based on Llama 2 Touvron et al. (2023  ###reference_b42###) and show that this can result in successful AEG results even when very limited amounts of human error data are available. Our analysis shows that the resulting errors are much more similar to natural human errors.\nWe also compare the approach to prompting commercial LMs (GPT-3.5 and GPT-4: OpenAI, 2023  ###reference_b26###) to perform AEG, as well as include other open models commonly employed for GEC and tune them for AEG: mT5 Rothe et al. (2021  ###reference_b30###); Palma Gomez et al. (2023  ###reference_b28###) and NLLB Luhtaru et al. (2024  ###reference_b20###). The details of our proposed methodology are given in Section 3  ###reference_###.\nOur final goal and evaluation setting is improving grammatical error correction for low-resource languages. In particular, we focus on German, Ukrainian, and Estonian GEC. For error correction, we also fine-tuned Llama 2 and compared it to the prompting of variants of GPT-4. Our experimental results show that Llama-based language models with fewer learned parameters can sometimes beat state-of-the-art results achieved with a bigger model. When pre-trained on our LM-generated synthetic errors, the resulting GEC models achieve the best current results on the included benchmarks in all three evaluated cases, including previous state-of-the-art and 4-shot GPT-4.\nIn summary, our contributions are as follows:\nWe show that pre-trained language models can be fine-tuned to generate high-quality synthetic errors.\nWe compare the influence of different models applied to AEG (LLama/GPT/mT5/NLLB) on subsequent GEC models.\nWe achieve new state-of-the-art GEC results across all tested languages with Llama 2-based models outperforming related work as well as GPT-4.\nThe paper is structured as follows. We outline related work in Section 2  ###reference_###, methodology experimental settings in Section 3  ###reference_###, and results in Section 4  ###reference_###. Additional questions on the same topic are discussed in Section 5  ###reference_### and the paper is concluded in Section 6  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The use of synthetic data is a common concept in GEC. The first effective neural method proposed by Junczys-Dowmunt et al. (2018  ###reference_b13###) approaches GEC as low-resource Machine Translation (MT), making it a relatively resource-heavy method encouraging synthetic data generation. Over the years, there have been different approaches to deliberately introducing errors into monolingual text, like rule-based and probabilistic methods, methods based on confusion sets and error patterns, models trained for error generation and using round-trip translation Bryant et al. (2023  ###reference_b4###).\nOne widely adopted approach to generating synthetic data involves the probabilistic addition of errors to monolingual corpora. This technique encompasses inserting, deleting, substituting, or moving characters or words without considering the context, as described by Grundkiewicz et al. (2019  ###reference_b10###), Zhao et al. (2019  ###reference_b50###), and Rothe et al. (2021  ###reference_b30###). Additionally, Grundkiewicz et al. (2019  ###reference_b10###) introduced a \"reverse speller\"approach that suggests word replacements from confusion sets based on the speller’s corrections. This method has been applied to several languages such as German, Czech, Russian, Ukrainian, Icelandic and Estonian Náplava and Straka (2019  ###reference_b21###); Trinh and Rozovskaya (2021  ###reference_b43###); Náplava et al. (2022  ###reference_b22###); Palma Gomez et al. (2023  ###reference_b28###); Ingólfsdóttir et al. (2023  ###reference_b12###); Luhtaru et al. (2024  ###reference_b20###).\nAs we show later, errors generated with the context-free probabilistic method differ from human errors and thus cover a much smaller number of error types, shown by significantly lower GEC recall.\nLearned methods of error generation typically require more resources. Before the widespread adoption of transformers and MT, various studies explored alternative approaches for training models for error generation. For instance, Felice and Yuan (2014  ###reference_b9###) and Rei et al. (2017  ###reference_b29###) utilized statistical machine translation to generate errors, while Xie et al. (2018  ###reference_b46###) and Yuan et al. (2019  ###reference_b49###) experimented with convolutional neural networks (CNNs) for this purpose. Additionally, Kasewa et al. (2018  ###reference_b15###) investigated using RNN-based sequence-to-sequence models with attention mechanisms.\nMoving towards more modern MT architectures, Htut and Tetreault (2019  ###reference_b11###) tested various model frameworks, including transformers, and Kiyono et al. (2019  ###reference_b16###) specifically employed transformer models. Both of the latter studies trained models from scratch, utilizing datasets ranging from approximately 500,000 to over a million error correction examples to train the artificial error generation system. In contrast, our work generates up to 1 million sentences with synthetic error while using between 9k and 33k human error sentences to fine-tune the base models.\nDuring the last few years, there has been no one error-generation method that has proved its superiority. It depends on language and available resources. For English Stahlberg and Kumar (2021  ###reference_b36###) train Seq2Edit models Stahlberg and Kumar (2020  ###reference_b35###) from scratch for learning to create diverse sets of errors. As mentioned in the beginning, synthetic probabilistic errors have found wide use for different languages. For instance, Ingólfsdóttir et al. (2023  ###reference_b12###) combine probabilistic character/word permutations with a rule-based approach for Icelandic and Kara et al. (2023  ###reference_b14###) curate special rules for generating Turkish data.\nIn addition, Oda (2023  ###reference_b24###) shows that generating new targets for synthetic datasets can be beneficial. Fang et al. (2023  ###reference_b8###) argue that translationese can be closer in domain to language learner’s text than traditional monolingual corpora, which can cause domain mismatch problems.\nNext, we present the key methodological details of our work."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology and Experiments",
            "text": "The primary target of our work is to apply generative language models to artificial error generation (AEG) via fine-tuning. Additionally, we experiment with prompting large language models to perform the same task and include two seq2seq models that are fine-tuned to do the same. The efficiency of proposed AEG solutions is evaluated using them to improve grammatical error correction (GEC). Thus, we also fine-tune generative LMs to perform the GEC task and compare the results to prompting-based GEC results and related work.\n\nThe Human Genome Project, a groundbreaking international scientific research initiative, was aimed at mapping and understanding all the genes of the human species. By determining the complete sequence of the DNA in the human genome, the project sought to provide fundamental insights into genetic structures, variations, and functions. It significantly accelerated the development of personalized medicine and has had profound implications in the fields of genetics, biotechnology, and medical research. The project was officially completed in 2003, and it has since served as a foundational resource for researchers worldwide, leading to numerous discoveries and advancements in a wide range of biological and medical sciences.\n\nNext, we describe the technical details of our implementation and the experimental setup."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data",
            "text": "We use two distinct types of data in our work. Firstly, we rely on datasets containing examples of grammatical error corrections to train our error generation systems and correction models. Secondly, we incorporate monolingual data to create synthetic datasets by introducing errors. See an overview of used data in Table 1.\n\nWe use the language learners’ corpus from the Human Genome Project Rummo and Praakli (2017) for gold data in Estonian. In Ukrainian, we use the UA-GEC corpus Syvokon et al. (2023) used in the UNLP 2023 Shared Task on Grammatical Error Correction for Ukrainian Syvokon and Romanyshyn (2023), using the GEC+Fluency data for training. For German, we rely on the widely used Falko-Merlin (FM) corpus Boyd (2018).\n\nFor monolingual Estonian data, we employ the Estonian National Corpus 2021 Koppel and Kallas (2022). We randomly sample equal sets from the latest Wikipedia, Web, and Fiction subsets and shuffle these together. For Ukrainian and German, we use the CC-100 dataset Conneau et al. (2020); Wenzek et al. (2020). Depending on the experiments, we sample the required number of sentences from the larger corpora (i.e., one million or 100 thousand sentences or a set equal to gold corpora sizes)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Models and Training",
            "text": "Llama-2-based models. We fine-tune models that have been enhanced with bilingual capabilities using continued pre-training from Llama-2-7B (Touvron et al., 2023  ###reference_b42###). For Estonian, we use Llammas-base†††huggingface.co/tartuNLP/Llammas-base  ###reference_se###, and for German, LeoLM‡‡‡huggingface.co/LeoLM/leo-hessianai-7b  ###reference_-7b###. For Ukrainian, we apply continued pre-training to replicate the conditions of Estonian LM by training with 5B tokens from CulturaX (Nguyen et al., 2023  ###reference_b23###) with 25% of the documents being in English and the rest in Ukrainian. For Human Genome Project fine-tuning, we formatted the training data with a prompt (see Table 8  ###reference_### and 9  ###reference_###) loosely based on Alpaca (Taori et al., 2023  ###reference_b40###). During fine-tuning, the loss is calculated on the tokens of the correct sentence.\nFine-tuning details (including hyperparameters) are discussed in Appendix B.1  ###reference_###.\nOther models we use are NLLB Team et al. (2022  ###reference_b41###) and mT5 Xue et al. (2021  ###reference_b48###). Specifically, we use the NLLB-200-1.3B-Distilled and mt5-large (1.2B parameter) models for our experiments and train NLLB models using Fairseq Ott et al. (2019  ###reference_b27###) and mT5 with HuggingFace Transformers Wolf et al. (2020  ###reference_b45###). When training in two stages, first with synthetic data and later with human errors, we keep the state of the learning rate scheduler, following the fine-tuning approach rather than retraining as defined by Grundkiewicz et al. (2019  ###reference_b10###). See Appendices B.2  ###reference_### and B.3  ###reference_### for further details."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Generation",
            "text": "Fine-tuned models. We use sampling instead of beam search to generate the synthetic errors and sample from the top 50 predictions with a temperature of . During error correction, beam search with a beam size of 4 is used without sampling as regularly.\nPrompt engineering. We perform iterative prompt engineering, analyzing intermediate qualitative results and updating the prompt. For instance, we initially started with a simple 2-shot prompt (temperature = 0.1) asking GPT-3.5 to add grammatical and spelling mistakes into the input text but noticed that some error types were missing. We then improved the prompt by specifying the missing error types, adding two more examples, and upping the temperature. Our final prompt uses four examples and a model temperature of 1.0. See Appendix A  ###reference_### for the prompts. We randomly pick the examples from each language’s train set for few-shot prompting. When comparing the prompting between GPT-4-Turbo and GPT-3.5-Turbo, we use an identical random set of examples to ensure comparability.\nFinally, we converged on using GPT-3.5-turbo for more massive error generation (100,000 sentence pairs per language). The motivation for that is partially financial (as GPT-4/GPT-4-turbo are several times more expensive) as well as performance-driven (see Figure 1  ###reference_### and description for details).\nWe apply simple post-processing to the resulting set because, in some cases, parts from the prompt are duplicated in the output. If the model didn’t generate a response due to safety model activation or the response was too short or too long compared to the input sentence, we replaced the output with the source text (equivalent to adding no errors).\nThe precise model versions we prompt are gpt-4-1106-preview for GPT-4-Turbo (using the OpenAI API) and gpt-3.5-turbo (GPT-3.5-Turbo) and gpt-4 (GPT-4) (using Azure OpenAI API, version 0613 for both).\nProbabilistic errors. We generate rule-based synthetic errors as done in prior work Grundkiewicz et al. (2019  ###reference_b10###); Náplava and Straka (2019  ###reference_b21###); Palma Gomez et al. (2023  ###reference_b28###); Luhtaru et al. (2024  ###reference_b20###) using the same method and also employing the Aspell speller§§§aspell.net  ###reference_spell.net/### for replacing subwords."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation",
            "text": "We evaluate the performance of our models using test sets and evaluation metrics consistent with those employed in previous works (see datasets in Table 1 ###reference_###). For Estonian, we evaluate our models using the Estonian learner language corpus (EstGEC-L2)¶¶¶github.com/tlu-dt-nlp/EstGEC-L2-Corpus/ ###reference_rpus/###, alongside a modified version of the MaxMatch scorer∥∥∥github.com/TartuNLP/estgec/tree/main/ M2_scorer_est ###reference_in/M2_scorer_est###, following Luhtaru et al. (2024 ###reference_b20###). The Estonian scorer also outputs recall per error category, accounting for both other errors within the word order error scope and not accounting for these. We report the ones that do consider other errors separately. For Ukrainian, our evaluation methodology aligns with that of the UNLP 2023 Shared Task Syvokon and Romanyshyn (2023 ###reference_b38###), utilizing the CodaLab platform for submissions to a closed test set that uses the ERRANT scorer for evaluationBryant et al. (2017 ###reference_b3###). We follow the GEC+Fluency track setting since it encompasses a wider range of challenging errors. For German, we use the test set from the Falko-Merlin (FM) corpus Boyd (2018 ###reference_b2###) that several works have reported their scores on and the original MaxMatch scorer Dahlmeier and Ng (2012 ###reference_b7###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we evaluate the performance of Llama-based models for GEC and AEG tasks. We then compare the AEG effectiveness between NLLB and mT5 models against Llama-based models to see if smaller, more efficient models can generate quality data. Separately, we assess AEG through prompting with GPT-3.5-turbo versus Llama models with trained error generation. Finally, we examine the quality of generated errors against human data and probabilistic reverse-speller errors."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation and Correction with Llama",
            "text": "We compare LLama-based language model (LM) fine-tuning error corrections across three configurations: (1) the baseline approach of training exclusively on human error GEC data, (2) the established related work approach of training on probabilistic reverse-speller AEG data and then continuing training with human error GEC data, and (3) our approach of training on back-translation style AEG data produced by fine-tuned Llama-based models first, followed by fine-tuning on human data.\n\nThe resulting scores are compared in Table 2, along with previous state-of-the-art (SOTA) scores and results of GEC via 4-shot prompting of GPT-4/GPT-4-turbo. Results show that Llama-based models, further enhanced through continued pre-training, exhibit strong correction capabilities across our study languages. Even without synthetic data, these models outperform current state-of-the-art (SOTA) methods in Estonian and Ukrainian error correction, and are not too far behind in German, trailing the best score by less than two points. However, it’s important to note the discrepancy in model sizes for a fair comparison; our 7B Llama model significantly exceeds the NLLB-200-1.3B-Distilled model Team et al. (2022) used for Estonian Luhtaru et al. (2024) and the mBART model Tang et al. (2021) for Ukrainian Bondarenko et al. (2023) in size. At the same time, it is smaller than the 13B mT5-xxl model used for German Rothe et al. (2021).\n\nIncorporating synthetic data as a preliminary step to fine-tuning significantly enhances performance across all languages and synthetic data types. Notably, our back-translation style synthetic data consistently delivers superior precision and recall compared to the probabilistic reverse-speller (or probabilistic) method. This approach results in a 2-2.4 point increase in the F score relative to solely using gold data for fine-tuning. Conversely, the gains from using probabilistic reverse-speller data are more modest, ranging from 0.6 to 1.5 points, highlighting the enhanced utility of our learned AEG errors.\n\nOur systems consistently outperform GPT-4 models regarding precision across all languages studied. However, GPT-4 models exhibit higher recall rates for Estonian and German. This discrepancy indicates that while our systems are more accurate in identifying correct instances, GPT-4 models better retrieve a broader range of relevant errors in these languages. On the other hand, the performance of GPT-4 models on the Ukrainian test set is notably lower compared to other methods and languages."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation with Smaller Models",
            "text": "Since error generation with 7B Llama-based models can be costly and time-consuming and many other architectures have proved useful for correction, we also explore smaller models for AEG: the 1.3B NLLB model and 1.2B mT5-large. The goal here is to see if these can also produce useful errors.\nTable 3 shows the results of the analysis. Both models can learn valuable information that improves performance beyond what is achieved with fine-tuning on gold data alone. Notably, errors generated by the NLLB model are particularly effective, delivering results close to those achieved by LM-generated errors in Estonian and German, almost matching the performance of LLama-based models. However, for Ukrainian, NLLB-generated errors fall behind probabilistic reverse-speller errors. The Ukrainian NLLB zero-shot GEC performance is also significantly lower than for Estonian or German (see more in Appendix C) or English that Luhtaru et al. (2024) also tested.\nThe mT5 models, in contrast, appear less adept at error generation. The errors produced by mT5 lag behind those from probabilistic reverse speller for Ukrainian and German and offer only a minimal improvement for Estonian.\nWe can also see that the scores before gold fine-tuning highlight that Ukrainian scores are notably low across all methods. However, these scores recover well after fine-tuning, suggesting the synthetic data may not align well with the text domain or error types specific to the Ukrainian language. Estonian and German models show higher scores for models trained with just AEG data and improve less drastically with fine-tuning."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Artificial Error Generation with Prompting",
            "text": "To assess the capability of generating errors without additional LM training, we utilize advanced commercial models, specifically exploring the efficiency of error generation through prompting GPT-3.5-turbo with datasets comprising 100,000 sentences. We later also explore the effectiveness of GPT-4-turbo in a more limited setting (see Section 4.4  ###reference_###). The generation cost depends on the sum of input and completion tokens. Ukrainian, our most expensive language, had the highest number of tokens per 100,000 sentences: 98 million input and 12 million completion tokens. The cost for input tokens with GPT-3.5-Turbo in USD is $147, and for completion tokens, it is $25 – in total, $172 for generating 100,000 Ukrainian sentences. In comparison, the costs with GPT-4-Turbo would have been $983 and $370, respectively******openai.com/pricing  ###reference_openai.com/pricing###. Table 4  ###reference_### shows the results of continued pre-training Llama-based models on the same amount of sentences (100,000) with synthetic errors from prompting or fine-tuning. In terms of error correction quality after gold fine-tuning, employing GPT-3.5-turbo for prompting and fine-tuning Llama-2-based models are both viable strategies for artificial error generation, as they lead to very close F scores in all three languages (with a slight difference in favor of fine-tuning errors for German: 75.58 vs 76.25). Analyzing the performance before gold fine-tuning reveals distinct differences between the two methods. For Estonian and German, recall rates are significantly higher with fine-tuning than prompting, though precision is slightly compromised. Conversely, Ukrainian exhibits the reverse pattern. However, it’s important to note that any disparities observed before gold fine-tuning are greatly diminished after training on actual error correction examples. The most considerable remaining difference is under 0.7 points for German, with smaller discrepancies for Estonian and Ukrainian. When comparing LLama model scores for 100k to the ones with only gold tuning (see Table 2  ###reference_###), we can see that although scores increase more modestly, only 100k examples of synthetic data increase the scores more for German (almost 2 F-score points), a bit for Estonian (around 0.4 points) and stay the same for Ukrainian with higher precision and lower recall. This shows a possible text domain mismatch between the human error train/test data and our choice of monolingual sentences. This negative effect is alleviated with higher numbers of pretraining AEG data in the 1M sentence experiments."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Error Generation Quality",
            "text": "Finally, we run a direct comparison between human errors and artificial ones. To do so we train models using the same number of sentences as the respective human error set sizes: 19k sentence pairs for German, 33k for Ukrainian, and 9k sentence pairs for Estonian. We include comparing these models to ones based on one million probabilistic sentences. Our findings indicate that the precision of all synthetic data closely matches that of high-quality (gold) data in both Estonian and German, as illustrated in Figure 1  ###reference_###. A notable distinction, however, is observed in recall rates. For Estonian and German, the recall for errors generated by LMs is more comparable to human-generated (gold) data than errors produced through probabilistic methods. Ukrainian scores with synthetic data are substantially worse than gold data, regardless of the AEG method. Still, recall for LM-generated errors is significantly higher than for simple probabilistic errors. This might be due to a larger mismatch in the text domain or error frequency. Ukrainian UA-GEC data predominantly contains punctuation errors (43%) and has a two times smaller error rate than German (8.2 vs 16.8) Syvokon et al. (2023  ###reference_b37###). Comparing GPT-3.5-turbo with GPT-4-turbo, we find similar performance overall. However, for Estonian, GPT-4-turbo exhibits higher recall but lower precision. For German, GPT-4-turbo shows reductions in both precision and recall. Performance is nearly identical for Ukrainian between the two models. Overall, the F scores of GPT-4-turbo are slightly lower for Estonian and German and marginally higher for Ukrainian compared to GPT-3.5. When analyzing the recall for various error categories in Estonian, it is evident that our models trained with AEG data particularly face challenges in inserting missing punctuation marks and correcting errors related to word order, as depicted in Figure 2  ###reference_###. Errors generated probabilistically excel in identifying spelling mistakes and can correct certain errors in noun and verb forms. However, they generally perform poorly in addressing issues beyond spelling errors. This comparison suggests that our learned and prompted synthetic errors are much more similar to naturally made human errors."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We investigated contemporary methods for generating artificial errors (AEG) for Estonian, German, and Ukrainian languages with relatively scarce resources. These languages have approximately 10k, 20k, and 30k error correction examples derived from corpora with varied error type distributions. The Estonian and Ukrainian corpora notably include language learner texts, characterized by a high frequency of errors, whereas the Ukrainian corpus contains more native-speaker texts with less edits.\nAcross these languages, our primary approach (AEG using Llama-based models and grammar error correction with Llama-based language models) demonstrated consistent efficacy after fine-tuning with error correction examples. This success underscores the value of the learned error generation method over the probabilistic reverse-speller approach, as evidenced by improved precision and recall based on reference metrics. The other methods – prompting and smaller models – also consistently prove useful.\nHowever, before fine-tuning with gold-standard GEC examples, we observed divergent language behaviors, raising questions about potential overfitting to these test sets and the generalizability of methods trained on specific datasets. For instance, our Ukrainian test set presented challenges for all methods lacking specific training data, including those involving GPT-4 models. It remains unclear whether methods that tend towards paraphrasing and fluency edits, including GPT models Coyne et al. (2023  ###reference_b6###), fail to align with the precise edits needed, overcorrect, or generate incorrect corrections. Critiques of current GEC metrics, which are argued to poorly correlate with human judgments Sakaguchi et al. (2016  ###reference_b33###); Östling et al. (2023  ###reference_b51###), suggest that true quality assessment may require human evaluation — a step beyond the scope of our study.\nThe observed performance with generated errors may also relate to mismatches between the chosen monolingual sentences and the original GEC human data. While our study utilizes web texts, resembling the essay-like texts typically employed in GEC and differing from native speaker constructions, these sentences might be simpler than those the model is accustomed to handling."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our research demonstrates the significant potential of Llama-based LMs in addressing the challenges of GEC for low-resource languages. We have successfully developed state-of-the-art systems for Estonian, Ukrainian, and German by leveraging these models as both correctors and synthetic data generators. We also explore other methods for AEG and show that prompting stronger commercial LMs is another way of generating high-quality data, and fine-tuning smaller models also has potential when the resources are more limited.  \nPotential directions for future work include tuning LMs to perform AEG and GEC multilingually (like  Rothe et al., 2021  ###reference_b30###; Luhtaru et al., 2024  ###reference_b20###), applying our proposed AEG methods to monolingual data of a more similar text domain to the benchmarks Oda (2023  ###reference_b24###). An interesting direction would be to test these methods with high-resource GEC languages (English, Chinese)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work focuses on three languages, recognizing that numerous other languages with grammar error correction (GEC) datasets exist outside our study’s scope. We selected languages based on recent relevant research activities: Ukrainian due to its recent Shared Task; Estonian, a newly emerging language in GEC research; and German for comparison with a robust 13B model. To comprehensively validate our method, further exploration across additional languages is necessary.\nOur objective was not to devise the optimal system exhaustively. Therefore, several avenues remain unexplored, such as varying generation methods, testing different temperatures, and adjusting parameters. Moreover, we capped the generation of synthetic sentences at one million, below the volume utilized in many (though not all) synthetic data studies. Questions about the ideal amount of data needed its dependency on the quality of synthetic and gold examples, remain unanswered.\nFurthermore, our study lacks human evaluation, a component for more reliably assessing the real-world efficacy of GEC systems."
        }
    ]
}