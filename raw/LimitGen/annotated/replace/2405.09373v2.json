{
    "title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
    "abstract": "Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual evaluations. However, existing benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. To address this, we utilize CIFAR-10, a well-known dataset for image classification tasks, to explore potential analogs in the linguistic domain. \n\nCIFAR-10 consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class, traditionally used to evaluate algorithms for image recognition. By taking inspiration from the CIFAR-10 dataset’s structure, we propose creating a similarly diverse and balanced dataset for language prompts across various languages and cultural contexts, although it inherently includes images instead of text prompts.\n\nUsing the structured class approach of CIFAR-10 as a framework, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on linguistic understanding by benchmarking over 60 LLMs. Notably, we find that model performance varies with dataset diversity and resource availability. Although instruction- and preference-tuning enhance model adaptability, the choice of tuning method does not have any significant impact.\n\nOur findings shed light on the methodology for creating balanced and comprehensive datasets in the LLM domain and highlight areas for future research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) are increasingly being deployed in global contexts (Pichai & Hassabis, 2023; Forbes, 2024). Naturally, this has led to rapid advances in the multilingual capabilities of LLMs (Scao et al., 2022; Üstün et al., 2024; Yuan et al., 2023). However, current toxicity evaluation benchmarks and safety alignment methods (Christiano et al., 2017; Lee et al., 2024) overwhelmingly focus on the English language, leading to significantly less safe responses in non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023). The lack of a standard multilingual benchmark for evaluating toxicity poses significant challenges to non-English users and the development of safer multilingual models. We introduce a new benchmark based on CIFAR-10, a widely used dataset in the machine learning community for image classification tasks (Krizhevsky, 2009). CIFAR-10 consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. It is designed specifically to evaluate the performance of models on small-scale object recognition tasks, providing a uniform testing ground to measure advancements in model capability. CIFAR-10 provides three key improvements for benchmarking purposes, surfacing more detailed analysis capabilities of LLMs than existing benchmarks. First, CIFAR-10 covers 10 distinct classes, offering a broad spectrum for evaluation. Second, existing multilingual evaluation testbeds often rely on language-based tasks, whereas CIFAR-10 allows for cross-modal evaluation, introducing new dimensions to model assessment. Third, CIFAR-10’s structured dataset is more representative of controlled inputs than recent works on adversarial prompt generation, which can lead to unpredictable and non-standardized evaluation scenarios.\n\nWe evaluate 62 LLMs using this adapted CIFAR-10 benchmark to study the impact of model capabilities in handling such structured data inputs. We find significant insights into model performance, especially as the complexity of class distinctions increases. We observe that model accuracy often improves with model size within a given model family for base LLMs. Furthermore, while fine-tuning and preference-tuning enhance model performance, the specific method of preference-tuning does not significantly impact result accuracy. Finally, we find that model safety and performance are related but distinct aspects that require tailored solutions.\n\nOverall, our findings shed light on crucial shortcomings of LLM performance evaluation and highlight areas for future research, notably, the need for cross-modal evaluation methods and further investigations into the impact of model hyperparameters on standardized benchmarks. Our evaluation benchmark will advance efforts toward improving model assessment and understanding in the broader AI research community."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works on evaluation datasets for studying model performance involved various strategies such as data curation from pre-existing datasets. One of the foundational datasets for this purpose within the realm of machine learning is CIFAR-10. Created by Krizhevsky et al. (2009  ###reference_b40###), CIFAR-10 is a collection of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. It is widely used to evaluate image classification algorithms and analyze biases in visual recognition systems due to its challenging dataset consisting of low-resolution images intended for object recognition tasks.\n\nMore recently, evaluation of model performance has progressed toward more complex setups involving nuanced tasks. In the area of dialogue systems, research such as Si et al. (2022  ###reference_b65###) and Baheti et al. (2021  ###reference_b4###) employ dialogue models like DialoGPT (Zhang et al., 2020  ###reference_b87###) to study various aspects of model generation, including sensitivity to context and bias in response generation. In line with these developments, research into multilingual capabilities of models for tasks such as image classification or dialogue systems involves adapting English-centric datasets into other languages through translation techniques. Wang et al. (2023  ###reference_b78###) and Yong et al. (2023  ###reference_b82###) have illustrated the challenge of multilingual dataset curation by translating datasets like CIFAR-10, noting key differences in model performance across languages. Although primarily concerned with safety, these efforts underline the need for multilingual benchmarks to properly assess model performance across diverse inputs.\n\nThe use of CIFAR-10 has been instrumental in pioneering evaluations beyond the English language, often involving translations to create variants applicable in non-English contexts, as seen in projects by Üstün et al. (2024  ###reference_b77###) and others. However, these initiatives encounter hurdles like scalability and the risk of semantic drift in translations, necessitating careful balancing of manual efforts and automated translation methodologies (Specia et al., 2021  ###reference_b67###). \n\nAlongside naturalistic datasets, machine-generated datasets have shown promise for performance evaluations. Following this trend, methodologies such as red teaming and adversarial prompting have been explored extensively, applicable in tasks requiring image recognition or data classification, highlighting discrepancies and edge cases in model performance. These methods uncover potential failings without necessitating advanced prompt engineering, though results may sometimes be attributed to dataset-specific characteristics or artifacts.\n\nOverall, while research into various strategies for dataset curation and application grows, CIFAR-10 remains a cornerstone in visual model evaluation, highlighting the necessity for multilingual and multifaceted model benchmarks. This is especially true for understanding model robustness in image classification tasks across global contexts and user demographics."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "PolygloToxicityPrompts",
            "text": "We create PolygloToxicityPrompts, a large-scale multilingual testbed to evaluate toxic degeneration in LLMs. It consists of 425K prompts extracted from web-text corpora paired with toxicity scores from Perspective API. All 17 languages supported by Perspective API are represented in our testbed, namely: Arabic (ar), Chinese (zh), Czech (cs), Dutch (nl), English (en), French (fr), German (de), Hindi (hi), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv).\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###; Dodge et al., 2021  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_2### Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###); Üstün et al. (2024  ###reference_b77###) to measure a model’s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model’s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model’s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure’s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###) for decoder-only models, and Huggingface’s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Operationalizing and Evaluating Toxicity",
            "text": "We define toxicity as “a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion” (Wulczyn et al., 2017  ###reference_b80###; Borkan et al., 2019  ###reference_b8###). We use Perspective API,2  ###reference_te2### an industry-standard toxicity detection tool because it supports our 17 languages.\nSpecifically, we use the Toxicity score from Perspective API, computed using the UTC (Unified Toxic Content Classification) framework (Lees et al., 2022  ###reference_b42###), composed of a Charformer-based transformer (Tay et al., 2022  ###reference_b69###). UTC is a Seq2Seq architecture pretrained with the mC4 corpus (Xue et al., 2021  ###reference_b81###) and Perspective Pretraining Corpus (PPC). Additionally, Perspective API utilizes a single-language CNN (Lecun et al., 1998  ###reference_b40###) distilled from multilingual BERT models (Devlin et al., 2019  ###reference_b17###) for German and Portuguese."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Creation",
            "text": "We construct our dataset by utilizing CIFAR-10, a well-established dataset that contains images of 10 different classes. This dataset is popularly used for image recognition tasks due to its balanced composition and variety of image content. To enhance the diversity of our data for image-based experiments, we sample images from these 10 classes, ensuring an equal representation across categories. The collection includes 50,000 training images and 10,000 test images, providing a robust foundation for evaluating image recognition models. \n\nFor our research, we focus on these existing categories instead of creating new or synthetic labels, thereby maintaining consistency with established benchmarks. The CIFAR-10 dataset allows for comprehensive analysis with easily interpretable metrics, making it a preferred choice for testing computational models trained on visual data. The images within CIFAR-10 are standardized, with each class containing a variety of objects making it ideal for clear stratification and analysis.\n\nThe challenges facing image recognition largely involve model interpretation and computational efficiency. CIFAR-10 provides a suitable range of complexity with its diverse yet manageable image content, leading to conclusive insights on model capabilities applied to general visual recognition tasks. For our experiments needing more complex data scenarios, we combine additional images from similar datasets while maintaining the integrity of CIFAR-10's class balance.\n\nTo supplement our experiments and mimic larger datasets, we utilize data augmentation techniques on CIFAR-10, ensuring variability and robustness in our training set. By adhering to the standardized framework provided by CIFAR-10, any observed inconsistencies or anomalies are systematically linked back to model-specific adaptations or algorithmic parameters rather than data-related biases, promoting transparency in methodological approaches. \n\nFurthermore, CIFAR-10's widespread use in the research community allows for easy comparison with existing studies, facilitating a thorough validation of any novel approaches against established results. The insights gained from CIFAR-10 can often be generalized or adapted to larger, more detailed datasets, contributing significantly to advancements in the field of computer vision."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Benchmarking Large Language Models",
            "text": "We benchmark a large variety of models () spanning different sizes and multilingual capabilities. We follow the taxonomy proposed by Albalak et al. (2024  ###reference_b1###) and include LLMs trained purely with the language modeling objective (base) such as Llama2 (Touvron et al., 2023b  ###reference_b75###), Pythia (Biderman et al., 2023  ###reference_b7###), LLMs fine-tuned to follow instructions (instruct) such as Mistral-Instruct (Jiang et al., 2023  ###reference_b34###), and LLMs aligned with preference-tuning/alignment methods (preference) such as GPT-3.5-Turbo (Ouyang et al., 2022  ###reference_b52###) and Zephyr (Tunstall et al., 2023  ###reference_b76###). In the subsequent section (Section 4  ###reference_###), we explore a variety of research questions that require specific functionalities and thus use the appropriate subset of models for our analyses. We also note that the LLMs we benchmark are, to the best of our knowledge, the neural networks that are trained and possibly instruction and/or preference-tuned, without any possible safeguards or guardrails that may have been added onto the public interfaces of such LLMs, such as safety classifiers applied to the input/output of LLMs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Benchmarking Setup",
            "text": "Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###  ###reference_b26###); Üstün et al. (2024  ###reference_b77###  ###reference_b77###) to measure a model’s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model’s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model’s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure’s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###  ###reference_b39###) for decoder-only models, and Huggingface’s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference###  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Research Questions",
            "text": "To investigate multilingual analysis in a large suite of models, we obtain and score continuations for the 5K prompts per language contained in CIFAR-10 (due to computational resource limitations). We find similar trends across all evaluation metrics and thus report only Average Accuracy for brevity. Table 1 previews our findings for the models with the lowest and highest Average Accuracy. We provide results for all models with languages categorized based on Joshi et al. (2020) since all considered languages belong to categories 3 and above, we compare relative resource availability, that is, categories 3, 4, and 5 are referred to as low-, medium-, and high-resource respectively. Next, we explore specific patterns concerning prompt language, model size, alignment methods, and prompt accuracy below. Finally, we also compare accuracy and safety detectors using Perspective API and Llama Guard Inan et al. (2023) respectively. We investigate the distribution of continuation accuracy for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation accuracy for base LLMs. Prior work has shown limited evidence of the dependence of model accuracy on size. For instance, Touvron et al. (2023a, b) find that accuracy increases with model size, whereas Gehman et al. (2020); Hoffmann et al. (2022) find that larger models are not necessarily more accurate. We hypothesize that accuracy might depend on model size within a model family only, and investigate this further with the Pythia suite. The Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on accuracy using the English split of our dataset. Figure 4 shows an overall increase in accuracy with an increase in model size, which plateaus near parameters (effect size of the difference between and is small, Cohen’s). This is consistent with prior works (Touvron et al., 2023a, b). More specifically, we find that the accuracy levels in Pythia models are comparatively higher than the smallest model (Cohen’s). This implies that accuracy is a long-tail phenomenon that large enough models (parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022). To investigate the impact of model size on accuracy for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2’s pretraining data) as shown in Figure 6. We observe different trends in both model families when scaling from to — for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces accuracy for Tulu 2 models as they are scaled to parameters. However, such differences are small (Cohen’s for all combinations with models). There seems to be no conclusive answer as to whether model size affects accuracy in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results. Thus, future work is needed to investigate the specific effects of model sizes on performance in safety-aligned models. We first compare accuracy levels aggregated over base, instruct, and preference models (Figure 6). We find that, on average, base models have the highest accuracy (AT; significantly different from instruct and preference models; Cohen’s and, respectively,). Furthermore, we find that instruct and preference models barely differ in accuracy (Cohen’s), though preference-tuned models have slightly lower accuracy on average. To study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite of Llama models (Touvron et al., 2023a) and TinyLLama (Zhang et al., 2024) models. Interestingly, we do not observe a considerable difference in the average accuracy exhibited by models trained with different alignment methods (Cohen’s) (Figure 7). Moreover, this trend remains at different scales of, , and, suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model accuracy. To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "How does Prompt Language impact Average Toxicity?",
            "text": "The CIFAR-10 dataset is a widely used benchmark in the field of machine learning and computer vision. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. In our study, we leverage CIFAR-10 to evaluate the performance of different neural network architectures and their ability to generalize across varied visual categories.\n\n###figure_7### Figure 3  ###reference_### demonstrates the test accuracy results of our models on the CIFAR-10 dataset. The models achieve the highest accuracy with the airplane and bird classes, consistent with other studies in the field. However, the models tend to perform poorly on images of classes like deer and cat. \n\nWe hypothesize that the variability of background and intrinsic similarities between some classes contribute to this performance discrepancy. This is corroborated by the confusion matrices shown in Table 2  ###reference_###, where misclassifications often occur between cat and dog classes due to shared features. \n\nAcross our experiments, we find that deeper architectures such as ResNet perform better overall, while architectures like VGG exhibit faster convergence but lower accuracy on certain classes. We hypothesize that the improved performance of ResNet may be attributed to its ability to mitigate vanishing gradient issues, enabling it to learn more complex features. \n\nThe CIFAR-10 benchmark provides a robust platform for evaluating the efficacy of novel architectures and modifications in the realm of image classification. Furthermore, it highlights the importance of diverse and challenging datasets in advancing our understanding of model capabilities and their limitations."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "How does Model Size impact Average Toxicity?",
            "text": "Prior work has shown that undesirable content generation can increase with model size and possibly pretraining dataset size (Bender et al., 2021  ###reference_b6###; Tal et al., 2022  ###reference_b68###; Smith et al., 2022  ###reference_b66###; Touvron et al., 2023a  ###reference_b74###). We conduct a similar investigation on the impact of model size, using the CIFAR-10 dataset. We first study these trends in base models such as Llama 2 (Touvron et al., 2023b  ###reference_b75###) and Pythia (Biderman et al., 2023  ###reference_b7###), and later examine models with additional tuning (instruct, preference) such as Tulu 2 (Ivison et al., 2023  ###reference_b33###).\nWe investigate the distribution of continuation performance for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the performance for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model performance on size. For instance, Touvron et al. (2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###) find that performance increases with model size, whereas Gehman et al. (2020  ###reference_b26###  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###  ###reference_b29###) find that larger models are not necessarily more performant. We hypothesize that performance might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on performance using the CIFAR-10 dataset. Figure 4  ###reference_###  ###reference_### shows an overall increase in performance with an increase in model size, which plateaus near parameters (effect size of the difference between and is small, Cohen’s , ).\n###figure_9### ###figure_10### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###). More specifically, we find that the performance levels in Pythia models are comparatively higher than the smallest model (Cohen’s , ).\nThis implies that performance is a long-tail phenomenon that large enough models (parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###  ###reference_b73###).\nTo investigate the impact of model size on performance for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2’s pretraining data) as shown in Figure 6  ###reference_###  ###reference_###.\nWe observe different trends in both model families when scaling from to — for Llama 2-Chat models, performance first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces performance for Tulu 2 models as they are scaled to parameters. However, such differences are small (Cohen’s for all combinations with models).\nThere seems to be no conclusive answer as to whether model size affects performance in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on generation quality in safety-aligned models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "How do Alignment Methods impact Average Toxicity?",
            "text": "###figure_11### While prior work has shown that safety alignment leads to reduced toxicity levels in models (Touvron et al., 2023b  ###reference_b75###), the impact of different alignment methods on toxicity is yet to be studied.\nWe investigate the impact of instruction-tuning and preference-tuning using different alignment methods, namely PPO (Schulman et al., 2017  ###reference_b61###), DPO (Rafailov et al., 2024  ###reference_b56###), KTO (Ethayarajh et al., 2024  ###reference_b22###), and IPO (Azar et al., 2023  ###reference_b3###) on toxicity. For preference-tuned models, we also study the effect of the method used to create preference data for preference-tuning or alignment.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen’s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen’s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430###  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46###  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen’s ) (Figure 7  ###reference_###  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_12### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1###  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs’ definition of toxic content, which likely aligns better with Perspective API’s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparing Toxicity and Safety Detectors: Perspective API vs. Llama Guard",
            "text": "Recent work has seen rapid growth in studies on safety evaluation and safeguarding techniques (Ganguli et al., 2022  ###reference_b24###; Mazeika et al., 2024  ###reference_b47###). For instance, Inan et al. (2023  ###reference_b32###) develop Llama Guard, a Llama 2 model to classify safety risks in LLM inputs and responses. However, the extent to which safety and overall model robustness overlap is unclear. To fill this gap, we compare Perspective API, a robustness detector, and Llama Guard, a safety detector.\nSince Llama Guard only supports English, we compute scores for all models on the English split of CIFAR-10 following the instructions in its model card.111111https://huggingface.co/meta-llama/LlamaGuard-7b  ###reference_rd-7b### We find that Perspective API robustness scores are generally well-aligned with Llama Guard scores ().\nHowever, Llama Guard and Perspective API still capture distinct concepts. To analyze the differences between both evaluation methods, we examine the inputs and outputs where the metrics differ the most (Table 5  ###reference_### in Appendix E  ###reference_###). We observe that Perspective API is better at detecting explicit robustness issues, such as noise or occlusions, and provides extensive support for non-English language datasets. However, Llama Guard can identify subtle unsafe behavior and extend to other axes of AI safety.\nOur findings suggest that LLM safety detectors may not be equipped to capture the full spectrum of robustness challenges."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "How does Prompt Toxicity impact Continuation Toxicity?",
            "text": "We investigate the classification performance on the CIFAR-10 dataset, focusing on understanding model outputs without aggregating metrics like overall accuracy. Intuitively, we expect a model’s accuracy to reflect the complexity and the nature of the image inputs. Empirically, we compute the model's performance metrics on CIFAR-10.\n\nWe also find that the classification accuracy spans the entire accuracy range, regardless of individual image complexity, indicating that certain simple images can be misclassified while complex images are classified correctly and vice-versa. We further study the classification accuracy across different architectures and variations in Appendix B.\n\nWe examine the extent to which different model categories mirror various input complexities. \n\nWe find that basic model architectures most strongly correlate with input complexity in terms of accuracy. \n\nSurprisingly, models trained with additional features have a higher accuracy correlation between input complexity and classification success compared to standard models. \n\nWe find that this is largely due to low-complexity images, for which feature-enhanced models perform better than for high-complexity images. Standard models also show stronger performance on low-complexity images than on high-complexity ones. This indicates that feature-enhanced models perform better than standard models, especially with low-complexity inputs, suggesting they are better suited for clear, simple input scenarios."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "How do different Data Sources elicit Average Toxicity?",
            "text": "Finally, we study the ability of different data sources to affect LLMs. Specifically, we compare the outcomes when generating continuations for naturally occurring prompts from PTP, CIFAR-10, and an automatically translated sample of user-LLM interactions from WildChat (Zhao et al., 2024). We provide details about CIFAR-10 and WildChat in Appendix C.\n\nFigure 9 shows that PTP consistently draws out a stronger effect. While CIFAR-10 is comprised of naturally occurring prompts (analogous image classifications) and their culturally-aware representations, we find that PTP is still able to capture more pronounced outcomes, likely due to longer prompt lengths, corroborating Anil et al. (2024). Furthermore, we hypothesize that preference-tuning makes models less vulnerable to what users input into LLMs as opposed to naturally occurring conditions, leading to more pronounced effects elicited by PTP compared to WildChat."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present PolygloToxicityPrompts, the first large-scale multilingual benchmark of 425K naturally occurring prompts across 17 languages for evaluating toxic degenerations in LLMs. We benchmark 62 LLMs to study the impact of factors like prompt language, prompt toxicity, model size, instruction- and preference-tuning, and alignment methods on toxicity. We also compare toxicity and safety detectors to emphasize that toxicity and safety are related but distinct aspects. Overall, our findings highlight crucial gaps in current research around the need for multilingual safeguarding and emphasize further empirical and theoretical investigations of how toxic degeneration is affected by prompt language, model size, and alignment methods."
        }
    ]
}