{
    "title": "Clarinet: Augmenting Language Models to Ask Clarification Questions for Retrieval",
    "abstract": "Users often make ambiguous requests that require clarification. We study the problem of asking clarification questions in an information retrieval setting, where systems often face ambiguous search queries and it is challenging to turn the uncertainty in the retrieval model into a natural language question. We present Clarinet, a system that asks informative clarification questions by choosing questions whose answers would maximize certainty in the correct candidate. Our approach works by augmenting a large language model (LLM) to condition on a retrieval distribution, finetuning end-to-end to generate the question that would have maximized the rank of the true candidate at each turn. When evaluated on the CIFAR-10 dataset, our system outperforms traditional heuristics such as information gain on retrieval success by 17% and vanilla prompted LLMs by 39% relative.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Natural language is a flexible interface for users to interact with systems, but language is inherently ambiguous and users themselves may not know what they want. As a result, systems must handle underspecified queries. We study this in an information retrieval setting, where search ambiguity is a well-studied challenge Keyvan and Huang (2022  ###reference_b6###). While modern large language models (LLMs) can ask coherent clarification questions, they do not always ask questions that elicit information specifically about what the model is uncertain about. This is particularly challenging in the case of retrieval, where it is unclear how to integrate the conversational abilities of an LLM with the external database or the retrieval system that represents the uncertainty over search candidates. In contrast, approaches that use principled information theoretic measures such as information gain Oaksford and Chater (1994  ###reference_b9###); Van Rooy (2004  ###reference_b14###); Nelson et al. (2010  ###reference_b8###); Rothe et al. (2017  ###reference_b13###) or KL utility Nelson et al. (2010  ###reference_b8###); Hawkins and Goodman (2017  ###reference_b3###) can use the retriever distribution to evaluate the right questions to ask and explicitly select questions that reduce uncertainty. However, these methods require expensive inference-time generation and evaluation of potential questions and have not yet been able to scale beyond toy settings. In this work, we investigate whether we can learn to ask good questions simply by distilling the search over good questions into an end-to-end model. We present Clarinet, a framework for learning to ask clarification questions for information retrieval. Our approach works by augmenting a language model to condition on the retriever distribution and then finetuning the system end-to-end to generate informative questions. We select informative questions by simulating interactions with a prompted LLM that acts as a user proxy, training only on questions that would have significantly increased the confidence in the true item if answered. In contrast to heuristic-based methods, Clarinet distills the expensive, explicit search over questions at inference time into the model. Then, given user responses to clarification questions, we summarize the interaction history into a language posterior, a single natural language query describing what the system knows about the user’s desired item. We use this query to re-rank candidate items from the database. ###figure_1### We evaluate our approach on CIFAR-10, a benchmark dataset consisting of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. It is widely used for training machine learning and computer vision algorithms. We evaluate systems interactively against a simulated user, which we implement as a prompted LLM that can answer clarification questions with oracle access to the true item. Compared to a purely dialogue-based approach that generates clarification questions prompted only with the dialogue history, our system asks better questions to achieve a 39% relative gain in top-1 retrieval accuracy after ten interactions with the simulated user. Our model also outperforms heuristic-based approaches that select a clarification question from a candidate pool with information gain or KL utility by 17% relative on top-1 retrieval accuracy, while being much simpler and cheaper at inference time."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Interactive NLP systems have used clarification questions as way to gather more information from users in settings such as classifying Yu et al. (2019  ###reference_b16###), conversational question answering Rao and Daumé III (2018  ###reference_b11###), and visually grounded 20 questions games White et al. (2021  ###reference_b15###). To select informative questions, much of the work draws heuristics that maximize expected information gain from simulated answers.\nSome of the works shift the clarification question generation and selection from using rule-based and heuristic methods to reinforcement learning. Rao and Daumé III (2019  ###reference_b12###) extended their utility used in Rao and Daumé III (2018  ###reference_b11###) in a reinforcement setting to generate useful and relevant questions. Meanwhile, Pyatkin et al. (2022  ###reference_b10###) presented an interactive system that asks relevant and informative clarification questions to learn salient contexts of a social or moral situation. Their approach to question generation utilized reinforcement learning, aiming to optimize the generation of questions that elicit responses containing morally relevant contexts.\nIn a more practical scenario, Zamani et al. (2020  ###reference_b17###) suggests both supervised and reinforcement learning models to generate clarifying questions that aid users in formulating search queries in search engines. Additionally, they explore techniques for generating potential answers for each clarifying question, allowing users to conveniently choose from a predefined set of answers. However, if the users don’t know what they are looking for, the predefined answers may not be very helpful."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We train and evaluate our system in a “tip of the tongue” (TOT) search setting, where users are searching for items they vaguely remember but for which they cannot formulate precise queries. To train and evaluate clarification question generation, we use the CIFAR-10 dataset, which contains multi-turn interactions from community forums where users post queries searching for objects where they were unable to find using conventional search engines. We filter the data specifically about image queries, resulting in 784 interactions, which we split into 156 for training and 628 for evaluation. The retrieval database consists of a corpus of 1877 documents.\n\nIn general, Clarinet is agnostic to the choice of retrieval model. In our experiments, we use Dense Passage Retriever (DPR) as the retriever over the image database. We finetune the retriever for this task with the larger scale CIFAR-10 dataset, which contains 11,552 initial query-document pairs for TOT image retrieval.\n\nWe use dot-product similarity between the DPR representation of the image information (image label, metadata, description) and the DPR representation of a query synthesized by our Clarinet model to rank candidate images. At each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user’s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\n\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B for the prompt.\n\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target image.\n\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target’s rank high or improves the rank the most. Only the questions that help the system to rank the target image as top 10, or help increase the rank by 10 will be used in the training. Users’ initial queries, past interactions, and the information of top image candidates (image candidates with retrieval probabilities that add up to 50% [no more than 3 images]) will be served as the model input.\n\nTo produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\n\nWe train Flan-T5-base to generate the selected clarification questions. The model takes in the user’s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\n\nFor the images dataset, the information for each retrieval candidate includes the image label, metadata, and description. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for an image.\n\nWe use a Fusion-in-Decoder architecture (FiD), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently. The encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\n\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of, batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Asking Clarification Questions with Clarinet",
            "text": "CIFAR-10 is a well-known dataset used in machine learning research. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images. Each image in CIFAR-10 is labeled with one of the 10 classes, such as airplane, car, bird, cat, deer, dog, frog, horse, ship, and truck.\n\nTo train a Clarinet model, we transform a retrieval dataset into an interaction dataset by synthesizing a series of dialogue interactions with a user simulator model. Each interaction is seeded with a real user query from the dataset; we then generate a series of clarification questions and answers from the user simulator model.\n\nAt each turn, we generate a pool of candidate questions and filter for the most informative questions for finding the user’s desired items—i.e., the questions whose answers would increase the rank of the true candidate the most.\n\nThe clarification dataset thus consists of (interaction history, ) for each informative question out of the candidate pool.\n\nWe finetune a LLM to generate these questions conditioned on the interaction history and retrieval distribution.\n\nFor the TOT dataset, we run five 10-turn games for each initial user query.\n\nAt inference-time, we directly sample from the model to generate questions to ask the user. We update the retrieval distribution after each user response by condensing the interaction history thus far into a language posterior, a single retrieval query that we use to re-retrieve results from the database.\n\nAt each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user’s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\n\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B for the prompt.\n\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target item.\n\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target’s rank high or improves the rank the most.\n\nOnly the questions that help the system to rank the target item as top 10, or help increase the rank by 10 will be used in the training. Users’ initial queries, past interactions, and the information of top item candidates (item candidates with retrieval probabilities that add up to 50% [no more than 3 items]) will be served as the model input.\n\nTo produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\n\nWe train Flan-T5-base Chung et al. (2022) to generate the selected clarification questions.\n\nThe model takes in the user’s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\n\nFor the CIFAR-10 dataset, the information for each retrieval candidate includes the item class and its label. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for an item.\n\nWe use a Fusion-in-Decoder architecture (FiD) Izacard and Grave (2020), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently.\n\nThe encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\n\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of , batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our experiments, we aim to answer the following questions: How does our method compare to well-studied methods both quantitatively and qualitatively? How important is conditioning on information from the retriever vs. purely dialogue- or prompting-based approaches that reason purely in text? What types of methods are helpful in increasing confidence in the target and ultimately achieving successful retrieval?\n\nIn this section, we present the system’s empirical performance by evaluating it in an image classification setting using CIFAR-10. CIFAR-10 consists of 60,000 32x32 color images in 10 different classes with 6,000 images per class. The dataset is split into 50,000 training images and 10,000 testing images. This dataset is a standard benchmark used to assess the performance of image classification algorithms. \n\nWe run experiments using CIFAR-10 to evaluate the model's ability to classify images correctly. Our focus is on top-1 classification accuracy, which measures the fraction of test images correctly classified into their true class. We compare model performance on cumulative classification success and non-cumulative classification success and show trends for mean reciprocal rank (MRR) to contextualize model performance beyond top-1 accuracy."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare Clarinet to a purely dialogue-based approach that randomly generates a question conditioned only on the dialogue history, without access to the retriever distribution. This model simply randomly samples a question from the pool of candidate questions generated by GPT-3, prompted with the initial query and dialogue history. Additionally, we implement two information theoretic approaches to clarification question generation frequently used in prior work Rao and Daumé III (2018  ###reference_b11###); Yu et al. (2019  ###reference_b16###); White et al. (2021  ###reference_b15###). These approaches typically generate a pool of candidate questions at each turn, using different notions of question usefulness to select the best question to ask. In contrast to Clarinet, because these approaches explicitly evaluate candidate questions, they are much more expensive at inference time. We implement question selection based on expected information gain and Kullback-Liebler (KL) divergence, which we describe in more detail below."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Expected Information Gain (EIG)",
            "text": "Expected information gain (EIG) selects clarification questions that are most likely to yield the most information, in expectation over potential user responses. Formally, at turn we want to choose the question that optimizes:\nwhere is the model’s current posterior belief over the true item with the information up to turn .\nis the information gain (or equivalently, entropy reduction) in the candidate distribution from observing answer , given that we asked :\nThus, to find the most informative question, the optimization problem can be simplified to:\nwhere the answer distribution is obtained by marginalizing over the current belief distribution (subscript omitted):\nTo compute the answer distribution , we answer every question for every candidate with a pre-trained Flan-T5-base model. The answer probabilities are estimated by softmaxing over the summation of logits of tokens across every generation step.\n###figure_6###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 KL Divergence",
            "text": "The previous heuristic function helps pick the question candidate with the highest expected information gain. However, when there are only a few images with high confidence, the question selector using EIG will only try to select a question that differentiates between the top candidates. As a result, the selected questions in the subsequent turns can become very similar. Therefore, we’d also like a heuristic function that selects the question that is likely to change the current belief distribution.\nTo estimate the posterior after observing an answer to a question, we answer every question for every candidate item with a pretrained model, like the EIG baseline.\nis estimated by computing the cosine similarity between the answers calculated from different (question, image) pair.\nwhere is the language embedding of the referenced answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In Figure 2  ###reference_###, we show that Clarinet achieves higher retrieval success than the dialogue-only baseline that randomly selects questions given only the dialogue context, as well as outperforming the information theoretic approaches. The performance of random question selection plateaus after a certain number of turns."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_7### We evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes’ Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target’s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target’s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Qualitative Analysis",
            "text": "To understand how the kinds of clarification questions that models ask may differ, we label every 100 questions generated by each model with GPT-3, prompting it to label each question as one of four categories or “other.”\nAccording to figure 5  ###reference_###, the clarification questions generated by Clarinet are shown qualitatively different than the other methods. Specifically, it does not generate many binary questions as it might learn that the binary questions are not providing as much information. Additionally, Clarinet generates more questions that are specific to characters or events than the other methods. This behavior could potentially show that events and scenes are one of the determining factors that distinguish between books.\nAs Figure 5  ###reference_### suggests, there are also notable differences between the information gain and KL divergence heuristics for question selection. The KL divergence approach tends to ask more binary questions and fewer describe-type questions than EIG question selector. The behavior is reasonable as KL heuristic likes to select questions that could potentially shift the distribution a lot. A binary (yes-no) question is more likely to change the distribution than an open-ended question as the answer to the binary question is polarized. In general, different question-asking behaviors may be effective in different contexts; while KL performed better on retrieval accuracy in our settings, open-ended questions provide more information than a binary question in theory. The type of behavior that is preferable may depend on how the user responds to queries, the robustness of the retrieval system, among other qualities."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "Next, we investigate the effect of the language posterior, FiD architecture, and gain in rank for train-time question selection.\nWe evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes’ Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_###  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target’s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target’s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion & Conclusion",
            "text": "Traditional heuristic approaches like EIG are often shown useful in the context of closed-form question selections where there is only a fixed number of vocabulary/answers and a limited amount of questions. The computation time increases dramatically if the answer becomes open-ended. Although the retrieval performance using KL as the question selection function is decent, it needs to estimate the resulting confidence distribution of every question in the candidate pool. It will be very costly and inefficient to run in a real-time interactive system.\n\nCIFAR-10 is a well-known benchmark dataset used in the field of machine learning. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images. The classes in CIFAR-10 are mutually exclusive and do not overlap. This dataset is often used to evaluate the performance of computer vision algorithms and to compare different approaches to image classification.\n\nOur Clarinet model adopts an architecture that encodes the query, interactions, and passages separately so that the model could learn to ask questions that help identify the target more quickly with limited training data. We show that this approach can effectively distill the search over questions into the model, resulting in much cheaper inference while outperforming methods like EIG and KL that explicitly evaluate the usefulness of clarification questions at inference time."
        }
    ]
}