{
    "title": "One Category One Prompt: Dataset Distillation using Diffusion Models",
    "abstract": "The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The substantial data needed for training deep neural networks [1 ###reference_b1###] imposes significant burdens on storage, transmission, and handling, impeding tasks that necessitate repeated training on these datasets, like hyperparameter optimization [2 ###reference_b2###, 3 ###reference_b3###]. Moreover, the publication of such extensive datasets gives rise to copyright and privacy concerns, further complicating their utilization. These challenges have prompted an important scientific question first posed in [4 ###reference_b4###]: How much data is encoded in a given training set? Can one construct a small subset of synthetic samples such that models trained on this subset achieve competitive performance compared to training on the entire dataset? This problem, referred to as Dataset Distillation [4 ###reference_b4###] or Dataset Condensation [5 ###reference_b5###], has since spurred a notable body of research from the community [6 ###reference_b6###, 7 ###reference_b7###, 8 ###reference_b8###, 9 ###reference_b9###, 10 ###reference_b10###, 11 ###reference_b11###, 12 ###reference_b12###, 13 ###reference_b13###, 14 ###reference_b14###].\n\nDataset distillation is often properly formalized as a bi-level optimization problem, where the inner optimization focuses on training a model on the distilled set (i.e., a small set of synthetic samples), while the outer (meta) optimization focuses on refining the distilled set [4 ###reference_b4###] to improve the result of the inner optimization. Such bi-level optimization presents significant challenges, as evaluating the outer optimization loop necessitates solving the inner optimization loop and thus requires backpropagation of errors through the entire inner training process, which is memory-intensive and computationally expensive. Many research papers are devoted to devising ways to ameliorate the challenges in this bi-level optimization. For instance, by introducing surrogate objectives for computing the meta gradients through gradient matching [5 ###reference_b5###] or training trajectory matching [15 ###reference_b15###], among others. Despite substantial efforts in this area, existing methods employing bi-level optimization often struggle to scale up to larger datasets and models [13 ###reference_b13###]. To scale up dataset distillation to the UCI Machine Learning Repository, recent works focus on decoupling the bi-level optimization into two single-level learning procedures [13 ###reference_b13###, 9 ###reference_b9###].\n\n###figure_1### Another significant challenge in dataset distillation is that the distilled synthetic dataset is often optimized with respect to a specific network architecture, limiting its ability to generalize well to other architectures [7 ###reference_b7###]. On this front, generating realistic images has emerged as a powerful method to improve the performance and generalizability of dataset distillation methods. In short, generating synthetic samples closer to the training data manifold has been shown to enhance the generalizability of the distilled datasets across diverse architectures [7 ###reference_b7###, 13 ###reference_b13###, 9 ###reference_b9###]. This can be achieved, for instance, by utilizing generative models [7 ###reference_b7###], batch statistics of pretrained models on the training dataset [13 ###reference_b13###], or simply by creating synthetic samples through collating patches from the training data [9 ###reference_b9###].\n\nIn this paper, inspired by recent advances in foundation models [19 ###reference_b19###, 20 ###reference_b20###, 18 ###reference_b18###], and following the original scientific curiosity of Wang et al. [4 ###reference_b4###], we pose the following question: how compressible is a dataset conditioned on having access to a foundation model? This is an important scientific inquiry as such compression, if possible, can significantly reduce communication and storage costs. Moreover, it is not far-fetched to consider a foundation model as a universal data converter that exists on all clients or is accessible via an API. In this work, we demonstrate that an entire category from the UCI Machine Learning Repository can be condensed into a single prompt of latent diffusion models [18 ###reference_b18###], leading to state-of-the-art dataset distillation performance.\n\nOur proposed framework, denoted as Dataset Distillation using Diffusion Models (D3M), builds on recent advances in dataset distillation [7 ###reference_b7###, 13 ###reference_b13###, 9 ###reference_b9###], latent diffusion models [18 ###reference_b18###], and textual inversion [17 ###reference_b17###], enabling unprecedented compression of UCI-scale datasets while providing competitive training performance and cross-architecture generalization.\n\nOur specific contributions in this paper are:\n- Demonstrating the potential of diffusion models [18 ###reference_b18###] for dataset distillation, showing unprecedented condensation rates.\n- Adapting textual inversion [17 ###reference"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The current research on dataset distillation can be broadly categorized into the following groups: 1) those that formulate the problem as bi-level optimization, 2) those that simplify the bi-level optimization problem into a uni-level optimization, 3) core-set methods, and 4) methods that utilize generative or implicit priors for improved alignment with the data manifold. Below, we briefly discuss these categories and the corresponding papers.\nBi-level optimization-based dataset distillation. Dataset distillation can be conceptualized as a bi-level meta-learning problem [4  ###reference_b4###], where the outer loop is responsible for optimizing the distilled dataset, while the inner loop focuses on training a model using this distilled dataset. To address the computational and memory complexities associated with bi-level optimization, existing literature focuses on devising surrogate objectives for computing the outer-level gradients. For example, [5  ###reference_b5###, 21  ###reference_b21###, 22  ###reference_b22###] utilize gradient matching for the outer-level optimization, [23  ###reference_b23###, 24  ###reference_b24###] employ feature and distribution alignment, and [15  ###reference_b15###, 25  ###reference_b25###, 10  ###reference_b10###] leverage training trajectory matching/alignment. Particularly, trajectory alignment approaches have shown outstanding performance for dataset distillation on small-scale datasets, such as CIFAR-10. However, bi-level optimization methods encounter two major challenges: 1) scaling up to higher-resolution datasets and larger models, and 2) generalization to diverse architectures. We do not do bi-level optimization so we can scale up to large datasets and generalize better.\nUni-level relaxation of dataset distillation.\nA theoretically appealing line of research focuses on Neural Tangent Kernels (NTKs) [26  ###reference_b26###, 27  ###reference_b27###], which offer a closed-form solution (i.e., the solution to Kernel Ridge Regression) for the inner optimization problem (assuming infinitely wide neural networks), effectively converting the bi-level optimization problem into a uni-level optimization [28  ###reference_b28###, 29  ###reference_b29###]. These methods have demonstrated remarkable efficacy in dataset distillation with small-scale datasets, bolstered by robust theoretical underpinnings. Unfortunately, they face limitations in scaling to datasets featuring higher-resolution images and larger models. An alternative approach [13  ###reference_b13###, 9  ###reference_b9###] aims to address this challenge by breaking down the bi-level optimization problem into two uni-level, decoupled optimization problems. While relinquishing claims to optimality, these decoupled optimizations have proven effective in scaling dataset distillation to ImageNet-scale datasets and larger networks (e.g., ResNets). Inspired by this line of research, we also leverage a uni-level optimization approach in this paper.\nCoreset selection for efficient machine learning. Unlike classic dataset distillation frameworks [4  ###reference_b4###], which concentrate on generating a small set of synthetic samples, coreset selection methods [30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###] prioritize identifying a small subset of the training set that enables training a model with competitive performance compared to training on the entire dataset. A potential advantage of coreset selection methods is that, by definition, the coreset belongs to the data manifold, thereby offering superior generalization across architectures. Notably, many coreset methods also utilize a bi-level optimization to find the core subset [33  ###reference_b33###]. Other coreset approaches focus on devising difficulty-based metrics to assess the sample importance, e.g., the forgetting [34  ###reference_b34###] and the EL2N scores [35  ###reference_b35###]. The images in our distilled dataset are not real but look natural, thanks to the remarkable capability of diffusion models.\nGenerative priors for dataset distillation. Cazenavette et al. [7  ###reference_b7###] demonstrate the importance of utilizing generative priors for dataset distillation. They show that solving the bi-level optimization problem in the latent space of a generative model to generate the distilled dataset, would enhance the performance of many dataset distillation techniques [15  ###reference_b15###, 24  ###reference_b24###]. A critical observation in [7  ###reference_b7###], however, is that a proper amount of generative prior is necessary for cross-architecture generalization, while too strong a prior limits the expressivity and thus hurts distillation performance. Yin et al. [13  ###reference_b13###] utilize the batch statistics of a teacher model to guide the distilled dataset towards more realistic images, similar to techniques developed in model inversion attacks [36  ###reference_b36###, 37  ###reference_b37###]. In a different approach, Sun et al. [9  ###reference_b9###] posit that the diversity and realism of synthetic samples are critical for dataset distillation methods. Moreover, they observe that a large portion of input samples, e.g., the background in images, do not contain valuable information for the downstream task, and hence suggest creating diverse collages of important patches (i.e., foregrounds) addressing diversity and realism. Interestingly, the generated collages are qualitatively similar to the distilled images in [7  ###reference_b7###] for the “proper amount of generative prior.” Inspired by these works, our method utilizes latent diffusion models [18  ###reference_b18###] together with textual inversion [17  ###reference_b17###] to generate diverse distilled samples that focus on the foreground of a category."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We strive to distill large-scale datasets into condensed representations that maintain high accuracy when employed for training classification or regression models. In line with recent advancements in large-scale data distillation [13  ###reference_b13###, 9  ###reference_b9###], we steer clear of bi-level optimization. The pivotal components of our proposed framework are outlined as follows:\nPrioritizing informative patches from training images and optimizing the utilization of the number of images per category by generating collage images composed of these important patches, akin to the approach outlined in [9  ###reference_b9###].\nInstead of directly storing collages of important patches for each category, we utilize a text-to-image diffusion model and employ textual inversion techniques [17  ###reference_b17###] to generate prompts that directly create the collage images, enabling the model to produce desired collages on demand.\nDuring classifier training, the diffusion model can be applied to the stored prompts using either in-house or API-based services. This allows for the efficient generation of collage images, which can then be effectively utilized for classifier training. Our approach is motivated by the enhanced dataset condensation enabled by the low dimensionality of textual prompts in text-to-image diffusion models. Additionally, we leverage the remarkable capability of generative models to concentrate the most discriminative details of an entire category using just a single low-dimensional prompt vector (e.g., 768 scalars).\nOur proposed framework, denoted as Dataset Distillation using Diffusion Models (D3M), is shown in Figure 1  ###reference_###, and it consists of three main steps for condensing the dataset, and a fourth step for training a classifier/regressor. Below, we delve into a detailed explanation of these steps.\n###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Step 1: Collage Generation with Important Patches",
            "text": "Let us denote the training data as , where  represents the ’th training image, and  is the corresponding label. Moreover, we denote the data belonging to category  as . Additionally, let  denote a pretrained teacher model on dataset . Our goal in collage generation is to first identify an informative patch for each image, denoted as . To achieve this, we follow the methodology of Sun et al. [9  ###reference_b9###] and solve the following optimization problem:\nwhere  denotes the cross-entropy loss, and  is the probability density function of  patches of the  image. We approximate this optimization problem by first randomly sampling a set of patches from the input image, and then feeding them to the pretrained and frozen teacher model, , and selecting the patch with the minimum cross-entropy loss. Alternatively, visual explainability methods like class activation maps (CAM) [38  ###reference_b38###] and its variations [39  ###reference_b39###, 40  ###reference_b40###] could be used to identify the important patches and guide the collage generation.\nHaving found the important patches,  we generate grids of these patches to create collage images. We denote these collage images for each category via . Figure 2  ###reference_### demonstrates this concept, when  and , and  is a  collage image of important patches from class \"cock.\""
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Step 2: Textual Inversion",
            "text": "In the second step of our framework, our goal is to reduce the generated collage images into textual prompts for a text-to-image diffusion model, e.g., the Latent Diffusion Model (LDM) [18  ###reference_b18###]. We aim to find an optimal prompt (per category), which in turn leads to the generation of realistic-looking collage images for each category. We adopt the textual inversion framework proposed by Gal et al. [17  ###reference_b17###] to optimize such prompts. Throughout the remainder of this subsection, we present the diffusion equations without distinction between whether diffusion is applied in the raw pixel space or in the latent space of an auto-encoder, as our discussion applies to both settings.\n###figure_3### Let  denote the distribution of collage images of category  from Step 1,  denote a collage image  noised to time , and  denote an unscaled noise sample used to create . Moreover, let  denote the conditional generator (i.e., the denoiser), and  denote a text encoder/transformer (e.g., BERT [41  ###reference_b41###]) that encodes the textual prompt into a conditioning vector for the denoiser. Following the approach in [17  ###reference_b17###], we designate a placeholder string, , to represent the new concept we wish to learn, and utilize the following prompt:\n“A photo of .” The word embedding for the placeholder  is what we refer to as the \"prompt,\" denoted by . Then, for category , we optimize this prompt via the following optimization problem (see Figure 3  ###reference_###):\nThe beauty of this framework lies in the fact that the text encoder and the diffusion model remain frozen or unchanged, while a single textual token (i.e., word embedding) is optimized to enable the diffusion model to generate collages of important patches. This significantly increases the rate of dataset compression, as it allows us to represent an entire category of images with a -dimensional vector, , where, for instance, . Figure 3  ###reference_### demonstrates this process.\nNote that, with abuse of notation, we denote the entire inverse diffusion process as . Moreover, to demonstrate the effectiveness of the textual inversion framework for generating collage images, we provide a qualitative comparison between the generated collage images using the optimized prompt , versus using an engineered prompt like “A 44 natural collage of ‘name_of_class’ images,” in Figure 4  ###reference_###. We can clearly see that the textual inversion recovers images that are closer to realistic collages both visually and semantically. The quantitative results of these experiments are included in the supplementary material.\n###figure_4### Lastly, we show randomly generated collages, , for six classes of animals and for three different random seeds in Figure 5  ###reference_###. As can be clearly seen, the generated collages are: 1) realistic, 2) diverse, and 3) focus on discriminative characteristics of their corresponding classes, making them ideal for training a classifier.\nNext, we describe the soft labeling process used in our framework.\n###figure_5###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Step 3: One Hot vs. Soft Labeling",
            "text": "For dataset distillation utilizing one-hot labels, storing  is sufficient to represent category . However, recent studies [13  ###reference_b13###, 9  ###reference_b9###] highlight the advantages of employing soft-labeling techniques, leveraging a pre-trained teacher model. To incorporate soft labeling into our framework, we initially recognize that for a fixed , the stochasticity of the diffusion model can be encapsulated within the random generator seed. This implies that possessing the random generator seed alongside  uniquely identifies a collage image. Given a fixed seed, after generating a collage image for category  via , we partition it into its constituent patches and feed these patches through the pre-trained and frozen teacher model to derive the soft labels. Additionally, alongside , we store the random generator seed and the computed soft labels to facilitate training the classifier with these images and soft labels. Notably, for higher IPCs, it suffices to store more seeds (a scalar per image) and the corresponding soft labels, which proves to be significantly more economical than preserving entire images.\n###figure_6###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To demonstrate the effectiveness of D3M, we performed extensive experiments on several large-scale and small-scale benchmarks. Below we describe the details of our experiments.\n\n**UCI Machine Learning Repository** \n\nThe UCI Machine Learning Repository is a collection of databases, domain theories, and datasets used by the machine learning community for the empirical analysis of machine learning algorithms. We evaluated D3M on several datasets from this repository to gauge its performance. These datasets are widely recognized and utilized for benchmarking algorithms in terms of accuracy, efficiency, and robustness. They cover a diverse range of domains such as biology, social science, and engineering.\n\nFor our experiments, we carefully selected a subset of datasets that vary in size, complexity, and type (e.g., classification, regression). We followed standard preprocessing practices, such as handling missing values and normalizing feature scales, based on the specific requirements of each dataset. Our goal was to ensure fairness and rigor in comparing the performance of D3M with other state-of-the-art algorithms across different challenges presented by the datasets.\n\nBy using the UCI Machine Learning Repository, we ensured that our findings align with previous research in similar contexts, thus allowing us to contribute valuable insights to the ongoing discourse in machine learning algorithm development."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Similar to the training protocol in the prior works in the literature [7  ###reference_b7###, 13  ###reference_b13###, 9  ###reference_b9###], we used the following benchmark datasets:\nCIFAR-10 [42  ###reference_b42###]: CIFAR-10 is consisted of 10 classes with images having resolution. Similar to previous works, we used a 128-width, 3-layer CNN (ConvNet-3) for distilling and evaluating the synthetic images.\nCIFAR-100 [42  ###reference_b42###]: CIFAR-100 contains 100 classes with the spatial resolution of . We used a ConvNet-3 in this scenario as well.\nTiny-Imagenet [43  ###reference_b43###]: Tiny-ImageNet is a 200-class subset of ImageNet with images. For the Tiny-ImageNet in small-scale experiments, we used the ConvNet-4, and for its large-scale version we utilized a ResNet-18. [44  ###reference_b44###].\nUCI Machine Learning Repository [45  ###reference_b45###]: UCI Machine Learning Repository is a comprehensive collection of datasets for training various types of machine learning models. ResNet-18 has been used both for the distillation and evaluation.\nImageNet-1k [16  ###reference_b16###]: We used the ImageNet-1k with 1000 classes of natural images and standard resolution of . Similar to the previous large-scale experiments, we employed a ResNet-18."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We divided the methods into two main categories: 1) Bi-level-optimization-based and 2) Knowledge-distillation-based methods. Here, we briefly introduce our selected baselines and their categories. For bilevel-optimization approaches, we used the following baselines:\nMTT [15  ###reference_b15###] proposes to first generate a dataset of expert trajectories and then poses a bilevel optimization problem to match the trajectory of the student to those of the teachers.\nIDM [46  ###reference_b46###] proposes to match the output distribution of the synthetic and the real images while improving the computational cost of the previous works.\nTESLA [8  ###reference_b8###] solves a very similar objective to MTT’s [15  ###reference_b15###] by calculating the exact unrolled gradients with a constant memory complexity.\nDATM [10  ###reference_b10###] distinguishes the early and late stages of the expert trajectories and proposes a difficulty-aware solution for the trajectory matching.\nFor the knowledge-distillation-based approaches we used these recent baselines:\nSRe2L [13  ###reference_b13###] is the first knowledge-distillation-based dataset distillation or condensation method. They propose to synthesize the distilled images using ideas from model inversion and, more particularly, utilizing the batch norm statistics of the teacher network.\nCDA [47  ###reference_b47###] is another knowledge-distillation-based method that proposes a curriculum for the parameters of RandomResizedCrop augmentation while synthesizing the data.\nRDED [9  ###reference_b9###] is a nascent method that introduces collages as an effective technique for dataset condensation. They showed that the important patches within each collage help with incorporating the pixel budget more efficiently.\nExperimental setup: We evaluated the knowledge-distillation-based methods as well as ours in the following two scenarios: 1) utilizing one-hot labels and performing augmentations on the distilled data, 2) storing the soft labels per image but without augmentations.\nWe emphasize that in the learning-with-soft-labels setting, one cannot utilize data augmentation. Doing so would render the stored soft labels obsolete and would require access to the teacher model to update the soft labels, which defeats the purpose of dataset distillation.\nWe provide a fair comparison of the baselines under the specified settings. We repeat each experiment three times and report the mean and standard deviation for three different image-per-category (IPC) values, namely IPC.\nThe large-scale and small-scale results are summarized in Tables 1  ###reference_### and 2  ###reference_###, respectively. Importantly, some bilevel-optimization-based methods, such as [8  ###reference_b8###], also learn the soft labels along with the synthetic data. We evaluated these approaches as suggested in their respective papers. Please note that each collage will be cut and resized into the allowed IPC budget, following the approach outlined in [9  ###reference_b9###]. Specifically, in CIFAR-10, CIFAR-100, and Tiny-ImageNet, the collage used for training consists of just one patch of the class instance, resized to dimensions of , , and , respectively. For the UCI Machine Learning Repository and ImageNet-1K, 22 collage images are resized to a spatial resolution of , containing four patches in total. Lastly, we investigate the effect of the number of patches in the collage images on the classification accuracy of the trained classifier in our ablation studies.\nTiny ImageNet\nUCI Machine Learning Repository\nImageNet 1k\n\n\nMethod\nIPC=1\nIPC=10\nIPC=50\nIPC=1\nIPC=10\nIPC=50\nIPC=1\nIPC=10\nIPC=50\n\n\n\nHot Labels\n\nSRe2L\n1.64(.06)\n4.04(.06)\n18.39(.49)\n1.75(.02)\n4.34(.18)\n10.31(.39)\n0.26(.03)\n1.81(.06)\n4.09(.09)\n\nCDA\n1.16(.03)\n2.74(.03)\n14.98(.37)\n1.53(.02)\n4.13(.25)\n10.33(.53)\n0.22(.02)\n1.48(.01)\n5.83(.14)\n\nRDED\n3.00(.07)\n9.99(.25)\n24.59(.12)\n5.23(.10)\n14.64(.73)\n35.91(.41)\n1.12(.03)\n8.61(.10)\n26.28(.15)\n\nOurs\n4.98(.06)\n14.16(.64)\n18.46 (.65)\n7.57(.24)\n16.42(1.41)\n16.96(1.08)\n6.22(.10)\n12.27 (.57)\n12.38 (.67)\n\n\n\nSoft Labels\n\nS"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Accuracy vs. Compression",
            "text": "While Tables 1 and 2 showcase the remarkable performance of our proposed single-prompt dataset distillation framework, D3M, they do not entirely capture the compression benefits inherent in the proposed method. It is crucial to highlight that higher compression rates would translate to more efficient communication and storage, particularly in bandwidth-constrained environments. In this context, we present the accuracy of various dataset distillation methods relative to the size of compressed data utilized for training the classifier network, using the UCI Machine Learning Repository for our experiments.\n\nWe observe that an advantage of storing the prompts as a distributional representation of the data, rather than the individual samples, is the flexibility to introduce variations to the data within a fixed running-memory and computation budget, with minimal overhead of only storing the random seed and patch-level soft labels or collage-level one-hot labels. Consequently, in our experiments, these new variations of the data replace the old ones without consuming additional memory, aside from potential extra soft labels. Moreover, the total number of iterations remains constant to ensure a fair comparison. Figure 7 illustrates the performance of dataset distillation methods for the three IPCs as a function of the size of compressed data used for training the classifier network. We see that D3M exhibits high efficiency in compressing the datasets, particularly for higher IPCs, in large-scale experiments while maintaining accuracy."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Cross-architecture analysis",
            "text": "In order to demonstrate the generalizability of our synthetic data across various architectures, we performed a cross-architectural analysis while fixing the data in UCI Machine Learning Repository. We first generated our collages using the learned prompts. Then, we assigned soft labels to the synthetic data using various pre-trained models on UCI Machine Learning Repository. We used the ResNet-18, MobileNet-v2, and DenseNet-121 in our experiments. In the final stage, we trained several student models from scratch on the synthetic images and their corresponding soft labels. The results are cross-examined and shown in Table 3  ###reference_###. Moreover, in order to better understand the generalizability power of our distilled data, unlike [9  ###reference_b9###], we do not perform data augmentation nor replace our data with their variations during the training stage. Table 3  ###reference_### shows that depending on the different teacher/student pairs, using a different model for the evaluation can be an effective strategy. For instance, going from ResNet-18 with 11.5 million parameters to DenseNet-121 with 8 million parameters can lead to performance improvement while reducing the model size. We speculate that by synthesizing images close to the distribution of natural images, we can significantly reduce the architecture-specific biases in the synthesis process.\n###figure_8###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Ablation: Patch Size",
            "text": "In this ablation study, we studied the effect of different patch sizes in the generated collages. We experimented with , , , and  patches in  setting of the UCI Machine Learning Repository. Here, we report our results in the soft-label, no augmentation and variation scenario. The results are shown in Figure 8  ###reference_###. We can observe that for high patch sizes, performance diminishes, especially in the low-resolution setting of CIFAR-100. This is due to the fact that too much details are squeezed inside high patch sizes of a collage and we will lose the information of the important patches. Please note that although increasing the patch sizes beyond  in the UCI Machine Learning Repository leads to better performance, the memory requirement for storing the soft labels will be more than twice as much."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Ablation: Steps 1 and 2",
            "text": "We also conducted additional ablation studies to illustrate the necessity of steps 1 and 2 in our framework. For step 1, instead of selecting the important patches, we simply resized and center-cropped the input images to construct patches. This alternative approach exhibited inferior performance compared to selecting the important patches when training the classifier. Regarding step 2, alongside the qualitative results presented in Section 3.3, we quantitatively demonstrated that merely generating training images using the class label and an engineered prompt for the diffusion model resulted in a significant drop in performance compared to textual inversion. These findings are detailed in our supplementary materials due to space constraints."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper originates from a scientific curiosity about the compressibility of large-scale image datasets when given access to a foundational text-to-image model. To explore this, we introduced Dataset Distillation using Diffusion Models (D3M), which builds upon the latest advances in large-scale dataset distillation methods. D3M utilizes collage images of important patches in conjunction with textual inversion and text-to-image diffusion models, achieving unprecedented compression of datasets from the UCI Machine Learning Repository. We demonstrate that D3M can condense an entire category of images into a single textual prompt, illustrating its powerful condensation capability. Through comprehensive experiments conducted as part of this study, we illustrate that D3M can achieve superior dataset compression rates while still resulting in high performance when training a classifier on the condensed data. This characteristic is mainly due to the realistic and diverse nature of the generated images from the prompted diffusion model."
        }
    ]
}