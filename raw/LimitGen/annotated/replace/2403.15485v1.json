{
    "title": "MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection",
    "abstract": "Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. To ensure that our model can capture authentic symptoms of depression, we utilized the COCO dataset, known for its extensive collection of various media types, which are crucial for analyzing emotional expressions. To leverage the diverse features in COCO, we adopt a multimodal approach and collect additional metadata such as categories and annotations available within the dataset. To effectively aggregate these multimodal features, we employed a cross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score of 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its performance with a benchmark dataset and achieved comparable results with prior studies (0.61 F1-score). In conclusion, we believe that the proposed model, MOGAM, is an effective solution for detecting depression in social media, offering potential benefits in the early detection and treatment of this mental health condition.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Depression is one of the most increasingly severe mental illnesses worldwide. According to the World Health Organization (WHO), about 5% of adults globally suffer from depression WHO (2023). Unfortunately, the situation has been exacerbated by the COVID-19 pandemic, which has led to a 25% global increase in the prevalence of depression. Compounding the problem is the serious issue that not only the majority of individuals do not receive suitable treatment services, but also they are mainly unaware of their conditions (Pramanik et al., 2022).\n\nThe most representative traditional way of diagnosing depression is a series of face-to-face interviews conducted by psychiatrists, utilizing screening instruments such as CES-D (Radloff, 1977) or PHQ-9 (Kroenke et al., 2001). However, the COVID-19 pandemic has significantly restricted face-to-face interviews, adding further notable challenges to the diagnostic process (Self et al., 2021). To tackle this problem, a number of researchers have explored the use of social media as an effective man of diagnosing depression (Kim et al., 2021).\n\nSocial media provides an interactive platform for individuals to share their thoughts, assertions, experiences, and emotions, as well as to connect and communicate with other users. This identified characteristic has allowed social media to be a popular source for analyzing each user’s psychological state (Lin et al., 2020; Liu et al., 2021), particularly in the context of depression detection. Several scholars have utilized various data-driven approaches and technologies, including natural language processing (NLP) and computer vision (CV) techniques, to explore and analyze the user’s psychological state from social media (Kim et al., 2020).\n\nRepresentatively, text-based social media platforms like Twitter (Orabi et al., 2018; Cha et al., 2022), and Reddit (Kim et al., 2020; Ren et al., 2021) have been extensively explored for this purpose. However, the recent advancements in CV techniques have led to a surge in research emphasized on both image-based and video-oriented social media platforms such as Instagram (Maxim et al., 2020) and YouTube (Yoon et al., 2022). These platforms allow scholars to have new opportunities to examine users’ mental states through visual content.\n\nAmong various content types in social media platforms, common objects captured on videos, as described in the COCO dataset (Lin et al., 2014), refer to images containing everyday objects in context, which serve as a rich resource for training machine learning models to understand and analyze visual information. The COCO dataset includes images of people, animals, and objects engaged in various activities, which provides a comprehensive basis for research in detecting and understanding visual content. Researchers utilize datasets like COCO to assess how individuals present their environments and activities through video logs, focusing on the context of surroundings and interactions rather than just the individual themselves.\n\nIn the context of depression detection, prior research has focused on exploring clinical interview videos of individuals with depression (Gratch et al., 2014). Facial expressions (Girard et al., 2013), acoustic signals (Ray et al., 2019), and body movements (Joshi et al., 2013) have been utilized as significant features to capture their psychological state. However, several challenges still exist in this approach. First, datasets based on clinical interviews are costly to obtain, resulting in a limited number of samples available for analysis. Additionally, models trained on such datasets may not be applicable to real-world scenarios or datasets. For instance, if a person’s face or body is not detected or obscured by other objects, it becomes difficult to extract significant features from the videos.\n\nTo address these challenges, we propose a novel approach called MOGAM, a multimodal object-oriented graph attention model, for depression detection using COCO. We collected depression and non-depression videos from YouTube utilizing COCO concepts. It allows us to create high-risk depression and depression datasets consisting of videos that effectively represent the object context within which users operate. That is, our research question (RQ) is presented as follows:\n\nRQ: Can we accurately detect depression and high-risk depression using our proposed method with COCO?\n\nConsidering RQ, in addition to employing specific features like facial expressions, we employed a unique approach. We extract objects presented in each video (e.g., person, cup, bed), and create an object network by computing the co-occurrence count between pairs of objects. This forms the basic framework of MOGAM, which leverages the object co-occurrence network to extract features from COCO-related content.\n\nOur model utilizes a graph neural network (GNN), which is designed to learn representations of nodes or graphs based on their structural relationships. In this case, GNN operates on the object co-occurrence network, allowing it to capture the interactions among different objects in the context"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "High-risk mental disorders detection",
            "text": "Proactively identifying individuals at risk of mental disorders is crucial because early diagnosis is one of the most important issues in effective treatments  (Conus et al., 2014  ###reference_b22###). To achieve such a goal, data science and machine learning came to play an essential role in identifying potential symptoms and risk factors, as they are strongly correlated to mental disorders (Thieme et al., 2020  ###reference_b23###). For example, Hao et al. (2013  ###reference_b24###) aimed to identify users’ mental health status through social media, leveraging machine learning to detect at-risk individuals. Similarly, Wang et al. (2017  ###reference_b25###) built a predictive model for eating disorders using Twitter data, analyzing social status, behavioral patterns, and psychometric properties of individuals in the disorder group and non-disorder group.\nNumerous studies have also investigated early detection and risk prediction for several notable mental disorders since the impact of depression on individuals and society is growing significantly, and early detection and diagnosis remain crucial for effective treatment (Halfin, 2007  ###reference_b26###). Xu et al. (2019  ###reference_b27###) and King et al. (2008  ###reference_b28###) have conducted extensive research on detecting high-risk groups for depression by analyzing symptoms and patterns associated with the disorder. However, they commonly face challenges such as time-consuming experiments, limited sample sizes, and relatively high costs. Several researchers have turned to social media platforms to overcome these limitations as a valuable potential source for detecting depression. This approach takes advantage of these platforms’ vast amount of user-generated content, providing critical opportunities for more scalable and cost-effective detection methods (Cha et al., 2022  ###reference_b11###; Kim et al., 2020  ###reference_b9###; De Choudhury et al., 2013  ###reference_b29###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Depression Detection in Social Media",
            "text": "Utilizing social media datasets in terms of detecting depression provides some distinct advantages, such as enabling the researchers to make use of large datasets and to apply more sophisticated data-driven approaches. These features greatly help the understanding of an individual’s mental health, which leads to the vast usage of social media in depression detection research (De Choudhury, 2013  ###reference_b30###, 2015  ###reference_b31###; Balani and De Choudhury, 2015  ###reference_b32###). For example, Al Asad et al. (2019  ###reference_b33###) and Lin et al. (2020  ###reference_b7###) employed Facebook and Twitter datasets to distinguish depression users using machine learning and deep learning approaches, respectively. Some other scholars used Reddit, the topic-oriented social media services, to apply various approaches such as linear SVM with bag-of-n-gram features (Pirina and Çöltekin, 2018  ###reference_b34###) or MLP on LIWC, LDA, and bi-gram features (Tadesse et al., 2019  ###reference_b35###). Most of these studies highlighted the data-driven approaches for detecting depression in social media, indicating the potentiality of social media as a valuable resource.\nVideo is one of the primarily modality for detecting depression, offering diverse features such as visual cues. Especially, user-oriented features presented in video content, including facial appearance and pose, are commonly adopted for depression detection. For example, Guo et al. (2022  ###reference_b36###) fed 2D landmarks and head pose features from the DAIC-WOZ dataset to the CNN-based model, achieving an accuracy of 0.857. Wang et al. (2018  ###reference_b37###) focused on Chinese individuals and employed clinical video samples to detect depression, achieving an accuracy of 0.789 using SVM with facial expression and eye movement features. Another mainstream of utilizing video data on depression detection is the multimodal approach, which incorporates various features such as visual, audio, and metadata information from given videos. Yoon et al. (2022  ###reference_b14###) and Chen et al. (2021  ###reference_b38###) also applied a multimodal fusion model on facial visual and acoustic features to identify people’s depression.\nWhile prior studies primarily rely on human-oriented features like facial expressions, eye movement, or pose, one of the challenges is their limited applicability to videos without human presence. This poses a limitation for real-world applications since many vlogs may not necessarily involve people in their content. In contrast, the proposed object-based graph method is not restricted to human-appearing videos and can be applied to any type of video. To the best of our knowledge, your study is the first to examine depression detection in vlogs using an object-based Graph Neural Network (GNN) approach, which provides a novel perspective for addressing this important problem."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "MOGAM: Multimodal Object-oriented Graph Attention Model",
            "text": "In this section, we introduce the construction process of our vlog dataset and the classification method. Specifically, we cover (1) the procedure for collecting and preprocessing YouTube vlogs, and (2) the introduction of our multimodal object-oriented graph attention model: MOGAM."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Collection and Preprocessing",
            "text": "To collect vlogs, we utilized the YouTube API (Application Programming Interface). We searched vlogs using two specific hashtags: “#우울증브이로그” (depression vlog) and “#일상브이로그” (daily vlog). All daily vlogs resulting from the search using the “#일상브이로그” hashtag were collected without any additional filtering procedures. However, for the depression vlogs indicated by the “#우울증브이로그” hashtag, we manually inspected the results because the hashtag alone could not guarantee the vlogs’ relevance to a medical depression diagnosis. Thus, the following steps were conducted.\n###figure_1### All vlogs uploaded by users, who wrote the “#우울증브이로그” hashtag, were collected.\nTwo separate researchers carefully reviewed the content of each user’s vlogs to identify the first collected vlog, which indicated a diagnosis vlog ().\nBased on the upload time of each depression diagnosis vlog , we divided the user’s vlog list into two groups: high-risk depression vlogs, which were uploaded before , and depression vlogs, which were uploaded after . Figure 1  ###reference_### provides the representative cases demonstrating the vlog collection and division procedures.\nBased on these procedures, the resulting dataset is organized by 1888 daily, 2237 depression, and 642 high-risk depression vlogs. The dataset consists of three groups: daily, depression, and high-risk depression. The daily group includes vlogs uploaded by non-depressed individuals unrelated to depression. The depression group consists of vlogs from individuals clinically diagnosed with depression. The high-risk depression group includes vlogs created by individuals who have not received a clinical diagnosis of depression but may exhibit symptoms associated with depression. The average duration times of these vlogs are 903.39, 416.03, and 515.74 seconds, respectively (Table 1  ###reference_###).\nTo analyze vlogs for depression detection, we utilized both image frames and metadata. Initially, we split each vlog into frames at a rate of single frame per second (FPS). It resulted in transforming the vlogs into a collection of individual images. In addition to the image frames, we gathered relevant metadata from the vlogs, including the title, description and duration. It is worth noting that while providing a description for a vlog is not mandatory on Youtube, we encountered cases where no description was available. In such instances, we replaced the missing description with empty string as dummy input for ensuring consistency in the metadata collection procedures."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Depression Detection on the vlog",
            "text": "The proposed model, MOGAM, is organized by three key components for depression detection in vlogs as presented as follows: 1) our object-oriented graph neural network, 2) the extraction of additional visual and metadata features, and 3) the aggregation of multimodal features to detect depression in vlogs. Through the integration of the object-oriented GNN, visual, and metadata features, our proposed model provides a comprehensive approach for depression detection in vlogs, leveraging both the inherent patterns within the vlogs and the multimodal information available."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Object-oriented Graph Neural Network",
            "text": "###figure_2### To construct the vlog object adjacency matrix, we conducted the following steps:\nDetection of objects in frames: We employed YOLOv5 (Jocher et al., 2020  ###reference_b39###), one of the widely-employed open source object detection models, to detect objects in each frame of the vlogs.\nDefining nodes and edges: We defined the detected the objects and co-occurrence count of each object pair in a frame as nodes and edges, respectively. If multiple objects of the same class were found in a frame (e.g. two different cup), we considered them as separate nodes.\nConstruction of adjacency matrix: To build the adjacency matrix, we utilized the co-occurrence counts of the object pairs.\nNormalization: Because the duration of vlogs may vary, the adjacency matrix was normalized by dividing each entry by the total number of frames. It ensures that the graph structure remains consistent irrespective of the length of vlogs.\nBased on these steps, we got a weighted undirected adjacency matrix which represents the object co-occurrence in the vlog. This matrix can capture the relationships among different objects, and serve as a basis for comparing the graph structures of other vlogs. The construction of the vlog object adjacency matrix is on of the fundamental steps in our approach to investigate the vlog depression detection. That is, the following equation is presented for building the proposed vlog object adjacency matrix ():\nwhere  denote the number of frames in the th vlog, the function  returns the co-occurrence count of two objects () in th frame of th vlog, respectively. Next, we fed  into three off-the-shelf GNN models: GCN (Kipf and Welling, 2016  ###reference_b40###), GraphSAGE (Hamilton et al., 2017  ###reference_b41###), and GAT (Veličković et al., 2018  ###reference_b42###). We compare the results of our proposed framework, MOGAM with the existing GNN models. The detailed description of the procedures are presented as follows:\nwhere  are nodes and  denote the nonlinear function, node ’s feature vectors at th layer, neighbors of node , information of neighbor’s and myself in the previous layer, attention weight, and aggregation function, respectively. Next, we fed  and node feature matrix made up of one-hot vectors into a number of GNN layers. Then, to obtain graph-level representation, we applied a global mean pooling, which involves averaging the node features across the node dimension. This pooling operation makes sure that the output representation can effectively capture the overall vlog information."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Visual Feature",
            "text": "To extract visual features, we utilized a pre-trained ResNet (He et al., 2016  ###reference_b43###), which is commonly employed for transfer learning and image feature extraction. We extracted feature vectors with a dimension of 1,000 and reduced them to the same size as the object-oriented graph feature using a fully connected layer. To ensure that the extract features are independent of the vlog length, we averaged all feature vectors."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Metadata Feature",
            "text": "For metadata feature extraction, we chose the title, description and duration from various metadata, which is available on Youtube, one of the globally used social media platforms. These features are selected by the eusers during the vlog uploading procedures. To prepare the metadata, we removed unnecessary textual information such as email addresses, URLs, and non-Korean text via data pre-processing steps. Consequently, we utilized pre-trained KoBERT (SKTBrain, 2023  ###reference_b44###).\nNote that if other multilingual language models such as M-BERT (Pires et al., 2019  ###reference_b45###) or XLMs (Lample and Conneau, 2019  ###reference_b46###) were adopted, MOGAM would be applicable to various languages. The feature vectors obtained from the title, description, and duration of the vlog were concatenated into a single feature vector."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Aggregation & Detection",
            "text": "Each encoder (, , and ) generates features  and  where  denotes the hidden dimension (Figure 2  ###reference_###). Then, we concatenated visual and metadata features to build additional integrated features.\nTo enhance the model’s ability to capture the inherent patterns of vlogs, which may not be captured by the baseline models, we incorporated a cross-attention mechanism within a transformer architecture (Vaswani et al., 2017  ###reference_b47###). In this mechanism, we utilized  as the query (Q) and  as the key (K) and value (V) for the cross-attention module.\nTo prevent an over-fitting tendency, we applied a dropout regularization. We employed several fully connected layers to reduce the dimensionality of the integrated multimodal features, , for vlog classification. The reduced features were then passed through a sigmoid function. The output represents the logit of the corresponding label."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conducted three conditional experiments as follows: Daily versus Depression Daily versus High-risk Depression Scalability Evaluation In addition to conducting a series of experiments using the multimodal approach, we implemented the experiments not only using the unimodal () approach, but also employing multimodal information for examining the effects of features on the model performance. All experiments were conducted using a single NVIDIA RTX A6000 48GB GPU and Python 3.7. We used the Yolo v5 model, which was pre-trained on the COCO dataset (Lin et al., 2014  ###reference_b48###), which consists of 80 objects. We split both datasets into the train, validation, and test sets in an 8:1:1 proportion (five times). The exact number of each set is presented in the appendix. We used the Adam optimizer (Kingma and Ba, 2014  ###reference_b49###) and set batch size, epochs, and hidden dimensions to 32, 500, and 1024, respectively."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Daily & Depression",
            "text": "The performance of the proposed model on both daily and COCO vlogs is summarized in Table 2. Except for GraphSAGE, all the proposed models exhibited significant accuracy and F1-score, providing evidence for the effectiveness of GNN-based models in classifying COCO vlogs. Notably, MOGAM with GAT achieved the highest F1-score among the baselines, indicating that the cross-attention mechanism is a suitable approach for learning representations based on the relationships between objects. This finding underscores the importance of capturing inter-object relationships in effectively detecting depression patterns. Consistent with the findings of prior work (Yoon et al., 2022), the models incorporating multimodal features demonstrated superior performance compared to the baselines. This finding suggests that the integration of object-based graph features with visual and metadata features creates a robust framework for effectively identifying depression in vlogs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Daily & High-risk Potential",
            "text": "To learn the symptoms and patterns of the high-risk depression state prior to clinical diagnosis, we trained our models using the daily and high-risk potential datasets. The performance comparisons on daily and high-risk depression data are also presented in Table 2. The data structure and classification model architecture for the daily datasets are identical to those described in the previous section. We observed that incorporating multimodal features enables the model to capture the distinctions between daily and high-risk depression data. Importantly, with the implementation of MOGAM, all models achieved accurate discrimination between daily and high-risk depression data (F1-score of 0.997)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Multiclass Classification",
            "text": "Finally, a multiclass classification experiment was conducted within the same experimental environment as the previous sections, aiming to simultaneously learn and distinguish symptoms and patterns across all three states. The models were trained on datasets representing daily, depression, and high-risk potential states, and the comparison of results is presented in Table 3  ###reference_###. In contrast to the binary classification, the GraphSAGE model combined with MOGAM exhibited the best F1-score of 0.815 and 0.750 for daily and high-risk potential states, respectively while the GAT model with MOGAM presented superior performance in the depression group with an F1-score of 0.783. However, the GCN, GraphSAGE, and GAT models without MOGAM exhibited limited ability to distinguish high-risk potential states, mostly misclassifying them as daily and depression (F1-score: 0.061, 0.083, 0.182). In contrast, the proposed MOGAM ensured comprehensive classification of high-risk potential states across all models (F1-score: 0.713, 0.750, 0.736). Consequently, we observed that MOGAM is suited for capturing and classifying image-specific features of each state simultaneously, even in an imbalanced data environment."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Scalability Evaluation",
            "text": "To address the scalability of the proposed methods, we conducted experiments on COCO (Lin et al., 2014), which is used in several prior research for image recognition tasks. It consists of over 200,000 images with more than 80 object categories. To conduct a fair evaluation, we followed the same procedures to extract object-based graph, visual, and metadata features from COCO. Subsequently, we applied the models trained on our image dataset, including both baseline and multimodal features, to the COCO dataset. Among these models, the GAT-based model trained with multimodal features achieved the highest F1-score of 0.612, which is comparable to the reported results in prior research (0.635). Figure 3 shows the comparison results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Object Distribution",
            "text": "To investigate gaps in the frequency of object appearance across different state vlogs, we conducted one-way analyses of variance (ANOVA) and post hoc tests. Employing an ANOVA involving 80 objects, we identified 47 objects exhibiting statistically significant differences in their mean values among the three groups. Subsequently, the results of post hoc tests present that the object “fork\" exhibited significant differences in all groups, while only eight objects (knife, cake, handbag, sandwich, apple, wine glass, banana, and vase) exhibited differences between the daily and non-daily group (depression and high-risk potential). The objects associated with food demonstrated notably low p-values, indicating a strong association between food-related objects and the user’s states in vlogs. Moreover, the frequency of object appearance is higher in the daily group compared to the non-daily group. The mean and standard deviation (SD) normalized appearance counts for the five objects with the lowest p-value are presented in Table 4. The entire results of analysis and the post hoc test are shown in the appendix."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this study, we proposed MOGAM, a new multimodal object-oriented graph attention model for depression detection on YouTube vlogs. To facilitate our approach, we collected a vlog dataset including vlogs of daily, depression, and high-risk potential depression groups. Then, we extracted three key multimodal features (graph, visual, and metadata) from the vlogs and combined them using a cross-attention mechanism. By leveraging these integrated multimodal features, our proposed model aims to determine whether the vlog uploader is experiencing depression. The results on the dataset demonstrate that our model effectively captures and detects depression symptoms and patterns in the vlogs by leveraging the power of multimodal features.\nBased on our findings, we can draw several implications. Many researchers utilize social media platforms for depression detection, leading to the development of various depression-related datasets (Gratch et al., 2014  ###reference_b18###; Yoon et al., 2022  ###reference_b14###). However, there is often a trade-off between dataset size and reliability. Building a dataset with clinical diagnoses can be prohibitively expensive, making it challenging to create large-scale datasets of this nature. In contrast, our vlog dataset comprises vlogs from individuals who have undergone clinical diagnosis, providing valuable insights into real depression patterns and symptoms. Moreover, our study validates the effectiveness of the object-oriented graph encoder and multimodal features for depression detection in YouTube videos. The scalability experiment results suggest that our proposed MOGAM can be applied to other datasets, including those in different languages and environments.\nAlthough we present several implications, notable concerns remain. First, the extraction of object-oriented graph features depends on the performance of the employed object detection model (e.g., Yolo v5). If the object detection model poorly performs, the constructed object co-occurrence network may not accurately represent the entire vlog, that can lead to the lower performance level of MOGAM. Second, because additional features we employed can depend on off-the-shelf models, we extracted them using pre-trained encoders (ResNet and KoBERT), which were trained on other datasets. Therefore, the quality and performance of these off-the-shelf models can impact the effectiveness of the additional features in our framework.\nIn future research, we aim to extend our method. We could apply state-of-the-art (SOTA) object detection methods. The SOTA object detection model can take advantage of a large number of objects and accurate detection performance, letting the model to capture complex relationships among objects. In addition, by utilizing encoders such as Transformer, we can effectively encode multimodal features and generate integrated representations of vlogs, leading to the improved performance. Furthermore, since depression is not the only mental disorder, we could be valuable to collect vlogs related to other mental disorders (e.g. anxiety, BPD, bipolar disorder) and apply our models to them.\nWe believe that our dataset and method as valuable tools for gaining insights into potential depression symptoms in vlogs. Consequently, wee hope that our model can assist individuals who are suffering from depression but may be unaware of their symptoms, as well as those who are not receiving adequate treatment. By effectively detecting inherent depression patterns in their vlogs, our model has the potential to offer support and guidance to those in need, ultimately leading to improved mental health outcomes."
        }
    ]
}