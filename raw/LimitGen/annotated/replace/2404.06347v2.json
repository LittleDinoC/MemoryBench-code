{
    "title": "RAR-b: Reasoning as Retrieval Benchmark",
    "abstract": "Semantic textual similarity (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release C-STS, a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. C-STS is available at https://github.com/gowitheflow-1998/RAR-b.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Semantic textual similarity (STS) and information retrieval tasks (IR) have been two principle measures to record the progress of dense representation models (Agirre et al., 2013 ###reference_b1###; 2014 ###reference_b2###; 2015 ###reference_b3###; 2016 ###reference_b4###; Cer et al., 2017 ###reference_b14###; Thakur et al., 2021 ###reference_b54###). Despite still heavily being evaluated in sentence representation research, STS is known for its limited alignment with real-world use cases (Neelakantan et al., 2022 ###reference_b40###; Muennighoff et al., 2023b ###reference_b38###), ambiguity (Deshpande et al., 2023 ###reference_b20###), and performance orthogonality with IR and other downstream tasks (Reimers et al., 2016 ###reference_b47###; Wang et al., 2021 ###reference_b57###; Xiao et al., 2023a ###reference_b64###).\n\nIn the LLM era, Retrieval-augmented Generation (RAG) (Lewis et al., 2020 ###reference_b30###; Neelakantan et al., 2022 ###reference_b40###; Xu et al., 2023 ###reference_b67###; Gao et al., 2023 ###reference_b21###) has become a go-to alternative method to vanilla end-to-end generative language models (OpenAI, 2023 ###reference_b43###; Touvron et al., 2023 ###reference_b55###). This shift is motivated by the inherent weaknesses of LLMs towards factual errors, due to hallucinations (Ji et al., 2023 ###reference_b26###), knowledge outdatedness (Vu et al., 2023 ###reference_b56###), rarity in long-tailed knowledge (Kandpal et al., 2023 ###reference_b28###; Malaviya et al., 2023 ###reference_b34###), and reasoning failure such as on logical deduction (Berglund et al., 2023 ###reference_b11###).\n\nRetrieval-Augmented Generation (RAG) is employed differently across various NLP tasks: For knowledge-intensive tasks, RAG is employed to retrieve the most up-to-date and reliable knowledge references (Vu et al., 2023 ###reference_b56###; Malaviya et al., 2023 ###reference_b34###), serving as new prompts for LLMs to extract information and formulate responses. This method mitigates models’ natural tendencies to hallucinate (Ji et al., 2023 ###reference_b26###) and reduces the need for frequent fine-tuning of LLMs. In reasoning-dependent tasks, RAG aims to fetch the most relevant chunks from extensive inputs to guide the focus of LLMs, e.g., in multi-hop question answering scenarios where reasoning across chunks from multiple documents is required. Reasoning with such long context is not only impossible for LLMs with built-in short context windows, but also challenging for LLMs with long context capabilities (Xu et al., 2023 ###reference_b67###).\n\nDespite the promise shown by dense retrievers in fetching references for knowledge-intensive tasks, these systems still fall short in retrieving reliable and cite-worthy references (Malaviya et al., 2023 ###reference_b34###), compared to state-of-the-art proprietary LLMs (OpenAI, 2023 ###reference_b43###) in a standalone manner, highlighting the undesirable behavior of retrievers in assisting LLMs (BehnamGhader et al., 2023 ###reference_b10###; Asai et al., 2024 ###reference_b7###). This discrepancy is more pronounced in reasoning-intensive tasks, where retrieval-augmented generation methods present inconsistent gains, or even performance degradation to LLMs (Bai et al., 2023 ###reference_b9###; Xu et al., 2023 ###reference_b67###).\n\nWith the complexities of the role that dense representation models play in the LLM era, the need for accurately assessing their advanced language understanding abilities becomes crucial. We advocate for evaluating these models’ capabilities beyond mere factual recall or semantic matching, focusing on their proficiency in complex thought processes and logical deduction.\n\nThis paper introduces the C-STS Benchmark, a novel framework that reframes reasoning tasks as retrieval tasks, offering a comprehensive reevaluation of “the actual reasoning representation compressed”111we use “C-STS” to convey the connotation of “compression” in dense representation models, and challenges the status quo of reasoning-oriented RAG. ###figure_1### Historically, lexical-based retrieval methods have long been seen as baselines of reasoning tasks (Clark et al., 2016 ###reference_b17###; Rogers et al., 2020"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "RAR-b",
            "text": "In this work, we construct and release RAR-b: Reasoning as Retrieval Benchmark. Deviating from evaluating on reasoning tasks with a full RAG pipeline (retriever+LLM), we instead focus on evaluating only the retrievers. By constructing reasoning tasks into retrieval tasks, we investigate how good retrievers are on solving them in a standalone manner, and use this as a proxy of the upperbound of retrievers’ capabilities in assisting LLMs, in a standard RAG system. We design three levels of tasks, resulting in the integration of 12 tasks derived from 17 datasets. We convert the original datasets into retrieval formats with both multiple-choice retrieval and full-dataset retrieval settings. We first benchmark the performance of state-of-the-art bi-encoder models, spanning across three model categories: unsupervised dense retrieval models, supervised dense retrieval models, and instruction-aware dense retrieval models. We evaluate both representative open-source models and proprietary models such as Cohere and OpenAI Embedding Models. We further benchmark the performance of representative re-ranking models, both on using them to solve multiple-choice retrieval setting independently, and on further re-ranking the documents retrieved by bi-encoders in the Full-dataset retrieval setting. The datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020  ###reference_b12###), HellaSwag (Zellers et al., 2019  ###reference_b70###), PIQA (Bisk et al., 2020  ###reference_b13###), SocialiQA (Sap et al., 2019  ###reference_b50###), ARC-Challenge (Clark et al., 2018  ###reference_b18###), Quail (Rogers et al., 2020  ###reference_b48###), Winogrande (Sakaguchi et al., 2021  ###reference_b49###), and C-STS (Deshpande et al., 2023  ###reference_b20###). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023  ###reference_b20###) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021  ###reference_b54###) (detailed explanation in Appendix B  ###reference_###). Apart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with C-STS and SpartQA (Mirzaee et al., 2021  ###reference_b35###). For C-STS, we evaluate each of its sub-level tasks separately, and construct the pure, fact and context settings to assess different aspects of retrievers’ behaviors (detailed analyses in Section 4.1  ###reference_###). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021  ###reference_b22###) and GSM8K (Cobbe et al., 2021  ###reference_b19###), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023  ###reference_b69###), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH). With the common user cases of code retrieval and that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) and MBPP (Austin et al., 2021  ###reference_b8###) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) is an extended version of HumanEval (Chen et al., 2021  ###reference_b16###), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al.,"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Formulation",
            "text": "We propose the novel framework of evaluating reasoning tasks as retrieval tasks, assessing whether representations of reasoning expressions are semantically well-encoded by retrievers. In other words, we assess whether the groundtruth answer to a reasoning problem is encoded to have the highest similarity with the query in the semantic space. C-STS allows us to examine these concepts in depth. Given a retriever, a query, and a groundtruth answer hidden in a corpus that is intentionally made very large, the most ideal scenario would be:\nHowever, this is typically not possible given the complexity of reasoning language and the fact that current sentence encoders are typically not yet well-trained to model such expressions. Therefore, we in turn are interested in whether:\nFor the Full-dataset retrieval setting, we can quantitatively measure C-STS's performance with commonly-adopted information retrieval metrics such as nDCG@n and recall@n, respectively concerning how well the top-n retrieved answers are ranked (nDCG@n), and if the correct answers are even retrieved as the top n (recall@n).\nFor the Multiple-choice setting, we simply assess whether the query is perceived by the retrievers to be more similar to the groundtruth answer than to other candidate answers in its original dataset format. Models’ performance in this setting can be easily quantified by accuracy."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Datasets",
            "text": "The datasets we consider can all be seen as reasoning problems. Firstly, we include commonsense reasoning datasets, NLI (Bhagavatula et al., 2020  ###reference_b12###  ###reference_b12###), HellaSwag (Zellers et al., 2019  ###reference_b70###  ###reference_b70###), PIQA (Bisk et al., 2020  ###reference_b13###  ###reference_b13###), SocialiQA (Sap et al., 2019  ###reference_b50###  ###reference_b50###), ARC-Challenge (Clark et al., 2018  ###reference_b18###  ###reference_b18###), Quail (Rogers et al., 2020  ###reference_b48###  ###reference_b48###), Winogrande (Sakaguchi et al., 2021  ###reference_b49###  ###reference_b49###), and C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###). For all datasets in the full retrieval settings, we pool all the candidate answers across all splits to serve as the corpus, aligning with the setting in most retrieval benchmarks, and evaluate them using test set questions as the queries. All commonsense reasoning datasets are suitable to test in Full-dataset retrieval setting, except for C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###) due to the potential sparse annotation problem when constructing it into Full-dataset retrieval settings (Thakur et al., 2021  ###reference_b54###  ###reference_b54###) (detailed explanation in Appendix B  ###reference_###  ###reference_###).\nApart from commonsense reasoning abilities, we further envision the possession of temporal, spatial, numerical, and symbolic reasoning abilities in dense retrievers. Temporal and spatial reasoning abilities are evaluated respectively with C-STS (Deshpande et al., 2023  ###reference_b20###  ###reference_b20###) and SpartQA (Mirzaee et al., 2021  ###reference_b35###  ###reference_b35###). For C-STS, we evaluate each of its sub-level tasks separately, and construct the pure, fact and context settings to assess different aspects of retrievers’ behaviors (detailed analyses in Section 4.1  ###reference_###  ###reference_###). For numerical reasoning abilities, we concatenate MATH (Hendrycks et al., 2021  ###reference_b22###  ###reference_b22###) and GSM8K (Cobbe et al., 2021  ###reference_b19###  ###reference_b19###), two datasets commonly used to evaluate LLMs, using questions as queries, the pool of all answers as the corpus. Because of their small scales, we enlarge the corpus with MetaMathQA (Yu et al., 2023  ###reference_b69###  ###reference_b69###), which is created synthetically with LLMs, using the training set of MATH. Therefore, it is assured that no examples in MetaMathQA can act as the groundtruth answer for any of our evaluated queries (which are from the test set of GSM8K and MATH).\nWith the common user cases of code retrieval and that code understanding can serve as a proxy of the understanding of symbolic language, we further include code retrieval tasks. We concatenate HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) and MBPP (Austin et al., 2021  ###reference_b8###  ###reference_b8###) to form the evaluation queries, because of their validated quality, ubiquitously seen in the evaluation of LLMs. Notably, HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) is an extended version of HumanEval (Chen et al., 2021  ###reference_b16###  ###reference_b16###), by translating the original Python split to cover Javascript, Java, Go, C++, and Rust. To enlarge the corpus, we further sample 200k code from CodeSearchNet (Husain et al., 2019  ###reference_b24###  ###reference_b24###) and 100k answers from a synthetic dataset TinyCode222https://huggingface.co/datasets/nampdn-ai/tiny-codes  ###reference_tiny-codes###  ###reference_tiny-codes### to cover pure code text and the mixture of natural language and code. We extensively explore the optimal setting to construct the code corpus and evaluate the code tasks (see analyses in Section 4.2  ###reference_.SS"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "We first benchmark the performance of state-of-the-art bi-encoders, spanning across unsupervised dense retrieval models (U-DR), supervised dense retrieval models (S-DR), supervised instruction-aware dense retrieval models (I-DR). For U-DR models, we have Contriever (Izacard et al., 2022 ###reference_b25###). For S-DR models, we include two Sentence Transformers (Reimers & Gurevych, 2019 ###reference_b46###) models that have been the most popular in the last few years, all-mpnet-base-v2 and all-MiniLM-L6-v2; Dragon+ (Lin et al., 2023 ###reference_b31###), an S-DR model that is progressively distilled with different views of other SOTA models, and BGE-M3 (Chen et al., 2024 ###reference_b15###). For I-DR models, we include Instructor (Su et al., 2022 ###reference_b51###), BGE (v1.5) (Xiao et al., 2023c ###reference_b66###), and consider all of their model sizes (base, large, XL for Instructor and small, base, large for BGE) to understand the scaling laws (if there’s any) between model size and reasoning abilities. We further include E5-Mistral (Wang et al., 2023a ###reference_b58###), the latest state-of-the-art instruction-aware embedding model that is finetuned from Mistral (Jiang et al., 2023 ###reference_b27###) with {instruction, query, answer} pairs distilled from GPT-4. We recognize the recent advancements in unifying generative and representation abilities into one model, and benchmark GritLM (Muennighoff et al., 2024 ###reference_b39###), whose data and training process of the embedding abilities are similar to Jiang et al. (2023 ###reference_b27###), but differing in its full parameters finetuning and joint learning with the generative objective. Lastly, we benchmark popular proprietary models Cohere Embed-English-v3.0 and OpenAI text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large through API encoding.\n\nFor C-STS (cross-encoders), we benchmark a few representative models such as BGE-reranker-large (Xiao et al., 2023c ###reference_b66###), TART-Full (Asai et al., 2022 ###reference_b5###), and Cohere rerank-english-v2.0."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Reasoning as Retrieval",
            "text": "Instead of solving these reasoning tasks via text-to-text generation (Raffel et al., 2020  ###reference_b45###; Lourie et al., 2021  ###reference_b32###), we explore the possibilities of solving them as a unified representation-to-representation matching problem. That is, all tasks, contexts, and possible answers are all understood through a shared representation space, and the search of the answer, given a task and the context information, falls back to be a simple similarity match problem. We explore two settings: RARw/o Instructions and RARw/ Instructions. For RARw/o Instructions, we simply use the original question/query/context in the datasets as the query, without describing the task with an instruction prompt, and use the pool of all possible choices across splits as the candidate documents. For RARw/ Instructions, we prepend the query with an instruction that describes the task. For instance, for NLI, an (imperfect) instruction can be “Given the following start and end of a story, retrieve a possible reason that leads to the end”. We construct two task settings: Multiple-choice Retrieval Setting (MCR), and Full-dataset Retrieval Setting (Full). For multiple choice retrieval, we utilize the choices available to each question in the original dataset. For this setting, the performance can be easily measured by accuracy, as each input has only 2-4 candidates. For Full-dataset Retrieval Setting, we construct a pool of all candidate choices with all the unique candidates from the same dataset, and investigate whether the groundtruth answer can be retrieved from a candidate pool of a much larger order of magnitude. In line with typical IR benchmarks, the corpus is constructed with candidates from all splits, and it is only shared across them in inference. For Full-dataset Retrieval, we measure the performance with commonly-adopted information retrieval metrics, including nDCG@n and Recall@n. ###figure_2### The multiple-choice setting and the full retrieval setting serve as simulacra of real-life RAG scenarios. Full retrieval performance assesses whether correct answers can be retrieved as top candidates, providing a basis for the later LLMs to make informed decisions. Multiple choice performance gauges whether hard-negative candidates might be erroneously prioritized over the groundtruth answer, introducing noise into the references presented to LLMs. From a pure retrieval perspective on the other hand, multiple-choice setting evaluates the extent to which a dual dense encoder can substitute a cross-encoder in reasoning-intensive late interactions - if dense encoders are already good at MCR, no re-ranking models (cross-encoders) are needed to rerank the top candidates retrieved by dense retrievers, before them being presented to LLMs. As a comparison, we also benchmark the performance of representative re-rankers, both as standalone solves in the MCR setting, and as re-rankers to re-rank candidates retrieved by bi-encoders in the Full setting. For Multiple-Choice Retrieval, we evaluate both retrievers and re-rankers, on the datasets that are originally multiple-choice problems (NLI, HellaSwag, PIQA, SIQA, WinoGrande), and ones that can be constructed to a MCR problem (C-STS). For Full-Dataset Retrieval, we evaluate both performance of using only retrievers, and retrieval+reranking performance as in a typical retrieval pipeline. We evaluate the FULL setting for every dataset (NLI, HellaSwag, PIQA, SIQA, WinoGrande, C-STS, SpartQA, Math-pooled, Code-pooled) except C-STS, which is not suitable for full retrieval due to potential sparse annotation effect - its original correct answer might not be the most suitable one throughout the corpus."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Table 2 presents the results for nDCG@10 performance. Figure 1 outlines the relative performance of the evaluated models. Because of the different scales of nDCG@10 across tasks due to different task difficulties and corpus sizes, we take the geometric mean across tasks to represent each model’s average performance, which is given by , where  is the number of tasks and  represent the performance of each task. We find geometric mean to be more reflective of model’s average performance, compared to harmonic mean and Z-scored mean. Figure 2 provides insights into the performance gain/degrade brought by prepending task instructions before queries. Similarly, due to the vastly different scales of performance across tasks, simply taking the arithmetic gain () is misleading (by biasing towards a low base). Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base, given by .\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models’ language understanding abilities is relevant to the diversity of training data and instruction-tuning (Wang et al., 2023a), with the current widely-adopted paradigm of instruction-aware information retrieval (Asai et al., 2022; Su et al., 2022; Wang et al., 2023a; Muennighoff et al., 2024). Injecting instructions understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks (Wei et al., 2021; Wang et al., 2022a; b), but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\nWe observe a scaling behavior through the varied versions of models of same classes. The pattern is consistent for Instructor (Su et al., 2022), BGE (Xiao et al., 2023c) and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor’s training through the SuperNI collection (Wang et al., 2022b), its performance gain on these datasets is more relevant to a stronger fitting ability to training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models (Xiao et al., 2023c) is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seem these tasks according to the technical report (Xiao et al., 2023c). Although the finding is consistent with (Ni et al., 2022) who find that larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks (Kaplan et al., 2020; Hoffmann et al., 2022; Wei et al., 2022), but less known for embedding tasks (Muennighoff, 2022).\nIt is observed that instructions generally distract the retrievers’ from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models such as OpenAI-embedding-3-small.\nLatest instruction-aware decoder representational models (Wang et al., 2023a; Muennighoff et al., 2024) seem to start bridging this behavioral gap, pointing the promising direction of scaling decoder models for next-level representational abilities. Cohere’s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time (Nov 02, 2023) compared to latest models that yield better absolute performance (OpenAI-v3, E5-Mistral, GritLM) on RAR-b, this behavior is surprising, and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available whether Cohere’s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities can not add to our main narrative of the potentail for decoder representation"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Full-dataset Retrieval Setting",
            "text": "Table 2 presents the results for nDCG@10 performance. Figure 1 outlines the relative performance of the evaluated models. Because of the different scales of nDCG@10 across tasks due to different task difficulties and corpus sizes, we take the geometric mean across tasks to represent each model’s average performance, which is given by , where  is the number of tasks and  represents the performance of each task. We find geometric mean to be more reflective of the model’s average performance, compared to harmonic mean and Z-scored mean. Figure 2 provides insights into the performance gain/degrade brought by prepending task instructions before queries. Similarly, due to the vastly different scales of performance across tasks, simply taking the arithmetic gain () is misleading (by biasing towards a low base). Therefore, we opt for using logarithmic scaling to dampen the effect of large percentage gains from a low base, given by .\n\nOverall, it is seen that newer models tend to outperform older ones on RAR-b. We believe the enhancement in embedding models’ language understanding abilities is relevant to the diversity of training data and instruction-tuning, with the current widely-adopted paradigm of instruction-aware information retrieval. Injecting instruction understanding abilities into retrievers are intuitively beneficial for modeling more nuanced semantics in natural language, such as intents, improving the alignment of query and groundtruth answer in reasoning tasks in the semantic space, especially for the ones that make less sense without specifying what the task is doing. Further, attaining a more accurate understanding of queries from diverse tasks allows generalizing to retrieving correct answers for unseen tasks. Generalization achieved from task diversity and decoder architecture is known for generative tasks, but more rigorous studies need to confirm the pattern for embedding models, especially for decoder models, which present the best potential on RAR-b.\n\nWe observe a scaling behavior through the varied versions of models of the same class. The pattern is consistent for Instructor, BGE, and OpenAI-Embedding-3. Models display a performance gain as their size increases. Since the level-1 datasets are more or less seen in Instructor’s training through the SuperNI collection, its performance gain on these datasets is more relevant to a stronger fitting ability to the training set due to larger model capacities, but not necessarily a stronger generalizability. On the other hand, the same pattern observed in BGE models is more indicative of a stronger generalizability achieved with larger model sizes, because BGE models have not seen these tasks according to the technical report. Although the finding is consistent with prior research that finds larger-capacity models are more generalizable retrievers, it is unclear whether this comes from the difference in the pretraining data of the base model or the finetuning data when adapting to representation models, especially in our case of reasoning tasks. While Instructor and BGE are encoder models, the same trend observed in OpenAI v3 models confirms the scaling pattern for decoders, which are well-known to scale for generative tasks, but less known for embedding tasks.\n\nIt is observed that instructions generally distract the retrievers from focusing on the content, even for the I-DR models. However, this is more observed for encoder models like Instructor and BGE, and smaller variants of decoder models, such as OpenAI-embedding-3-small.\n\nLatest instruction-aware decoder representational models seem to start bridging this behavioral gap, pointing in the promising direction of scaling decoder models for next-level representational abilities. Cohere’s proprietary Embed-English-v3.0 presents a strong gain brought by instructions. Considering its relatively early release time compared to the latest models that yield better absolute performance on RAR-b, this behavior is surprising and might be relevant to the efforts put in relevance feedback from LLMs. However, no public information is available on whether Cohere’s current embedding model is an encoder or a decoder, and thus its strong instruction-following abilities cannot add to our main narrative of the potential for decoder representation models.\n\nFor models that degrade with instructions, it is more severe in full-dataset retrieval settings compared to the Multiple-choice Retrieval setting, intuitively because of a larger candidate pool to be distracted by. This phenomenon is most pronounced for older-generation embedding models such as Sentence Transformer model all-mpnet-base-v2. Though sharing the same training set with all-MiniLM-L6-v2 while both without training to follow instructions, we suspect it’s more sensitive than the latter to instructions because of its larger capacity. The key takeaway here is, if a model is not trained to follow instructions, its retrieval performance might be largely distracted by instructions, hypothetically as the model capacity gets larger.\n\nWith Sentence Transformer’s earliest models being the pioneer in sentence embeddings in the contextualized LM’s era, the field has now entered a stage where sentence embedding models need to model more complex nuances in human language.\n\nIn general, the nDCG@10 performance in Full setting is not as bad as the slightly-higher"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Does Reranker Help?",
            "text": "In a standard two-step retrieval pipeline, reranking models are utilized to further rerank the first-round results retrieved with bi-encoder embeddings. As we will show in the next sub-section through multiple-choice retrieval (Table 3), nowadays’ reranker models are not trained to understand reasoning-level language, and are already outperformed by recent bi-encoders with decoder backbones trained with instruction tuning (Wang et al., 2023a; Muennighoff et al., 2024).\n\nBut can we fine-tune the reranker to improve performance on such tasks? Note that it is non-trivial to fine-tune a bi-encoder on each task: with varied sizes of each training set and the de-facto usage of InfoNCE loss (Oord et al., 2018), we do not want to overfit a bi-encoder to an isotropic distribution for one reasoning task (Xiao et al., 2023b). And intuitively, cross-attention enables faster convergence to tasks with deeper language understanding requirements.\n\nIn this section, we fine-tune Cohere’s proprietary rerank-english-v2.0 model through API fine-tuning because of its well-configured infrastructure. For each task, we construct a training set using its original MCR-format dataset, with the ground truth being the positive pair for each question, and all non-groundtruth candidates as hard negatives. We evaluate NLI, HellaSwag, PIQA, SocialiQA and Winogrande to understand the pattern, and fine-tune a reranker model for each task. The average performance is outlined in Figure 3 (without-instruction nDCG@10 presented), where we can easily achieve state-of-the-art performance by re-ranking the top 100 documents with a fine-tuned Cohere reranker. Note that the first-round retrieval result puts a large cap on the later re-ranking upperbound.\n\nAlthough recent papers have confirmed generative models’ potential as rerankers (Sun et al., 2023; Pradeep et al., 2023), they are neither trivial to train nor cheap (yet) in inference. Considering the release time of Cohere’s rerankers, we confirm using previous established cross-encoder frameworks is still the simple, robust and strong solution for such tasks. However, we envision decoder models to still be the future for scaling for complicated tasks, and position the effective training of decoder-based rerankers and resource-efficient inference of them as a valuable research direction."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Multiple-choice Retrieval Setting",
            "text": "Table 3 presents the results of the Multiple-choice Retrieval (MCR) setting, where each dataset has a baseline characterized by random predictions. The MCR setting provides an in-depth check of retrievers’ nuanced understanding. Although we evaluate MCR with a separate implementation, we note that in essence, MCR is equivalent to checking whether the correct choice ranks above all wrong choices in the Full setting.\n\nThe findings could be summarized as follows:\n\nFrom Table 3, we can easily find that models struggle with Winogrande (Sakaguchi et al., 2021) and C-STS (Deshpande et al., 2023) at around chance level. Although some models perform well on Winogrande in the Full setting, we find that this performance might be more of a mirage. For example, Contriever can achieve an nDCG@10 of 0.471 on Winogrande without instruction in the Full setting (Table 2), outperforming even the OpenAI-Embedding-3-large model. However, its performance on MCR is stuck at chance level. We find that the two candidate answers in Winogrande are both words that exist in the queries (e.g., entities), and it requires only finding these two words and ranking them at the top to get good performance in the Full setting, making this task fall back on being a name entity matching task for the Full setting, without the models needing to develop a fine-grained understanding to distinguish them.\n\nMultiple-choice retrieval settings serve as a proxy measurement of how well the correct reasoning can be found through representation matching, given a few selected possible answers already. From the MCR experiments, we can conclude that current dual-dense retrievers fail to serve as late interaction reasoners.\n\nA special case in the models is Instructor, which is known to have seen the formats of the first 6 tasks through finetuning on Super-NI (Wang et al., 2022b), enabling it to achieve better performance on these tasks compared to traditional S-DR models. Yet, its performance on the two settings of C-STS that we construct is stuck at chance level. Without solid control experiments, we hypothesize that this pattern stems from the subpar generalizability of encoder models. Although the pattern can currently be empirically supported by the better performance achieved by similarly instruction-tuned decoder models including E5-Mistral, GritLM, and OpenAI models, the generalization gap between encoders and decoders has not yet been systematically investigated on information retrieval, and we position this as a valuable research topic.\n\nSurprisingly, Contriever, as an unsupervised model only trained on different spans of text without labeled document pairs, serves as a strong baseline for all datasets. We speculate this pattern is due to the formats of question-answer pairs in reasoning datasets resembling the way that Contriever is trained on. For instance, a question and its answer in HellaSwag essentially compose a consecutive document, while being similar to a span pair that Contriever is trained on. For this reason as well, Contriever is extremely robust to instructions, because a prefix instruction does not make the following question and answer look less like a consecutive span in the document."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Behavioral Analysis",
            "text": "In this section, we present more in-depth understanding of retrievers’ behaviors through the tasks that have multiple settings.\n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like “who is the president of United States in 2003?”, and the retrievers thus need to encode “George W. Bush” to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama’s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. In the setting, we are essentially evaluating retrievers’ reading comprehension abilities. Intuitively, the “fact” is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model’s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges (Xiao et al., 2023a  ###reference_b64###). However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedidng models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average C-STS scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting.\n\nThe translated nature of HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###) grants us the convenience to inspect models’ familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions (e.g., for entries that require “retrieve a Python snippet”, a Javascript snippet of same content is instead ranked above the groundtruth Python snippet).\n\n###figure_6### Here, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the “semi-groundtruth” is placed over the groundtruth, indicating the model’s failure of perceiving about programming languages through instructions.\n\nFigure 4  ###reference_### depicts the pattern. If the queries are not distracted by code of same content from different languages (i.e., same content from different languages is ranked above the groundtruth), the average performance of the single-language settings should actually match with the performance of the pooled setting, because in the pooled setting, the nDCG@10 is essentially nDCG@10 performance averaged across different languages.\n\nAnother property we present is the Entity Shortcut. In Table 5  ###reference_### (left), we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the “instruct” setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the “original” setting) with a self-contained instruction.\n\nWith the Instruct setting, we"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Temporal Reasoning",
            "text": "With the 7 sub-settings we construct for C-STS (Table 4  ###reference_###), we find that it provides a multi-faceted breakdown of retrievers’ behaviors, providing understanding of their parameterized knowledge, reading comprehension abilities, and robustness against irrelevant context, and thus we provide an analysis of it in this sub-section. \n\nC-STS-L1 (C1) in the original dataset concerns evaluating the built-in time perception of models, using time arithmetic questions. We evaluate this task with only one setting - the standalone setting (denoted as Pure). Query and groundtruth document in C1 looks like “When is 7 months after Nov, 1998?” and “June, 1999”.\n\nFor C-STS-L2 (C2) and C-STS-L3 (C3), we respectively construct three settings: Pure, Fact, and Context+Fact. These two levels of tasks evaluate question answering situated in time, providing us the understanding of the three aspects outlined above. \n\nPure setting here evaluates the built-in knowledge stored in retrievers. The question here can look like “who is the president of United States in 2003?” (note that questions in C-STS are in a similar format but are much harder and niche than this), and the retrievers thus need to encode “George W. Bush” to be among having the most similar embedding. We can see the Pure setting evaluates factual knowledge parameterized in retrievers without references.\n\nFor the Fact setting, we concatenate the facts (a paragraph describing the answer to the question in different times - such as the history of US presidential tenures) after the question as query. Concretely, the query can look like: Question: Who was the president of the United States in 2003? Facts: The US president from 2001 to 2009 was George W. Bush. Barack Obama’s tenure began on January 20, 2009, and ended on January 20, 2017. Donald Trump began his presidency in 2017, and his tenure ended in 2021. and ideally, George W. Bush has an embedding close to the above long passage. Note that this above example is for illustrating the concept with the same format as C-STS, and is not from the C-STS dataset, as C-STS is made of much less well-known people and facts, and thus more difficult. In the setting, we are essentially evaluating retrievers’ reading comprehension abilities. Intuitively, the “fact” is laid out in the query and the embedding of the query is expected to be guided towards that of the groundtruth document, by the facts. Another way to describe this setting is that we let the retrievers play the role of LLM in a RAG system in this setting.\n\nLastly, for the Context+Fact setting, we further concatenate the context (the background knowledge of the entity in the question that is going to be asked) after the facts. On top of the Fact setting, the Context+Fact setting further reflects a model’s robustness towards irrelevant information (since the answer can be extracted only from facts, but not from contexts), and encoding capabilities for texts of varied length ranges (Xiao et al., 2023a  ###reference_b64###  ###reference_b64###). However, there potentially is unfair comparison in here because of the max sequence length each model is capable of encoding. For instance, OpenAI embedidng models are able to encode sequence up to 8192 tokens and are faced with more noise to be robust against. On the other hand, models with max sequence length of 512 need to deal with less noise. Therefore, the Context-Fact setting here is only for analysis purposes, and we do not average the scores from this setting into the average C scores in the main table. For E5-Mistral and GritLM, we set its max sequence length to 8192 here to match with OpenAI models, for attaining a comparable understanding on the Context+Fact setting."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Behavioral Analysis through the lens of Code Retrieval",
            "text": "Another area that we can get a more fine-grained understanding about retrievers’ behaviors is through code retrieval, given the extensive settings we explored in the construction of the final pooled evaluation dataset.\nThe translated nature of HumanEvalPack (Muennighoff et al., 2023a  ###reference_b37###  ###reference_b37###) grants us the convenience to inspect models’ familiarity with the same content of different programming languages, and when pooling them together, to what extent do the hard negatives come from the failure of distinguishing across languages through instructions (e.g., for entries that require “retrieve a Python snippet”, a Javascript snippet of same content is instead ranked above the groundtruth Python snippet).\n###figure_7### Here, we inspect the results of 7 settings, respectively with only queries from each of the 6 languages, and the pooled queries from all languages. For each setting, we enlarge the corpus with the same 200k documents sampled from CodeSearchNet and 100k documents from TinyCode, aligning with our final setting. However, we see a performance gap between averaging individual settings and the pooled setting, meaning that for a lot of queries, the “semi-groundtruth” is placed over the groundtruth, indicating the model’s failure of perceiving about programming languages through instructions.\nFigure 4  ###reference_###  ###reference_### depicts the pattern. If the queries are not distracted by code of same content from different languages (i.e., same content from different languages is ranked above the groundtruth), the average performance of the single-language settings should actually match with the performance of the pooled setting, because in the pooled setting, the nDCG@10 is essentially nDCG@10 performance averaged across different languages.\nAnother property we present is the Entity Shortcut. In Table 5  ###reference_###  ###reference_### (left), we present the results of two settings we construct for evaluating HumanEvalPack. For the original setting, we simply follow the continuation setting, where the query is the import, function name and the docstring of the function, and the aim is to retrieve the rest of the code. We construct another setting, the “instruct” setting, where we would like to retrieve the complete code (the concatenation of the query and the groundtruth in the “original” setting) with a self-contained instruction.\nWith the Instruct setting, we see a huge jump of nDCG@10 from the original setting (e.g., 0.374  0.915 for Instructor-XL), indicating that the easiness of this setting for the model. An example for this setting is given in Appendix C  ###reference_###  ###reference_###, demonstrating the entity matching shortcut. Although this setting forms a more self-contained scenario that would happen in real-life question answering, the function declaration presents in both the query and the document, enabling the retrieval to rely solely on the function declaration matching. We speculate this pattern to be highly relevant to the “entourage effect” towards dominant tokens (function declaration in this case) after contrastive learning found in (Xiao et al., 2023b  ###reference_b65###  ###reference_b65###). Due to this reason, we opt for the original format in the final evaluation.\nWe further conduct an experiment by constructing the queries and corpus with only TinyCode, a synthetic dataset of code question answering. This dataset provides the natural mixture of natural language and code. We sample 15k queries and 150k documents including the groundtruth to form the corpus. The result is shown in Table 5  ###reference_###  ###reference_### (right). Although hard to construct controlled experiments, it is seen that models generally perform less well on the mixture of natural language and code."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the critical need of evaluating retrievers’ next-level language understanding abilities, moving beyond the conventional scope of semantic textual similarity and topical matching. We introduced the C-STS framework, a novel problem that casts reasoning tasks as retrieval tasks. Through extensive evaluation, we revealed retrievers’ behaviors in understanding reasoning-level language and depicted the behavioral discrepancies between retrievers and LLMs, suggesting promising directions in future research in retrieval-augmented generation. We assert that representation models excelling in C-STS-b are well-suited to enhance LLMs within RAG systems, positioning C-STS-b as a vital benchmark for tracking advancements in the field."
        }
    ]
}