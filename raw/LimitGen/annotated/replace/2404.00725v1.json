{
    "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation",
    "abstract": "It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time).\nTo address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a B model once vs. generating five outputs from a B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to % across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The common wisdom in deep learning, and language modeling in particular, is that investing more compute leads to improved performance (Kaplan et al., 2020  ###reference_b21###). The standard way of implementing this principle is training larger models. A simpler, yet often overlooked way to increase compute budget is to run a smaller model multiple times, and select the best output using some metric (Chen et al., 2021  ###reference_b4###). In this work we systematically compare these two approaches: we ask whether, given a fixed compute budget, it is best to run a large model once, or a smaller model multiple times (Figure 1  ###reference_###). Our results show that, perhaps surprisingly, given the same compute budget, running 7B or 13B models can not only match the performance of a 70B model, but also substantially surpass it. Addressing our research question requires a method for selecting the best LLM output from a given set of candidates. In this work we focus on execution-based code-generation tasks, which assume the availability of unit-tests (Chen et al., 2021  ###reference_b4###; Austin et al., 2021  ###reference_b2###; Hendrycks et al., 2021  ###reference_b17###). We adapt the widely-used pass@ metric (Kulal et al., 2019  ###reference_b23###), which evaluates a model’s performance on code generation problems by generating outputs and assigning a point if any of them passes all tests. To adapt this metric for our purposes, we consider models of different sizes, and for each generate as many outputs as possible given a fixed compute budget, e.g., floating point operations (FLOPs) or wall-time. \n\n###figure_1### We apply this setup to evaluate the Code Llama (Roziere et al., 2023  ###reference_b30###) model family (B, B, B, and B) across five tasks: COCO (Lin et al., 2014  ###reference_b3###), MBPP (Austin et al., 2021  ###reference_b2###), and the three splits of APPS (Hendrycks et al., 2021  ###reference_b17###). Surprisingly, we find that for the two popular tasks, COCO and MBPP, the smaller models (B and B) outperform the larger ones (B and B) by a margin of up to %. Importantly, this is observed using both budget types (FLOPs and wall-time) and across all computation budgets. When considering the challenging APPS benchmark, we find that the B model performs best across almost all budgets, with a consistent margin of % when considering the hardest split—competition.\n\nWe then proceed to examine the scenario where unit-tests are unavailable, such as in an IDE code-completion setup. In such cases, an efficient policy is required to select a single solution from all generated ones. We consider a simple LLM ranking policy, which ranks solutions based on the negative log likelihood of the LLM. We experiment with the B model, and rank its outputs using each of the models. Our results show that, as expected, our ranking-based selection improves with the increase in compute budget, and with the size of the ranking LLM. Nonetheless, this procedure still falls short of the performance achieved by running the larger model independently with the same budget. Our results highlight the potential of using smaller models instead of larger ones, a practice that has many benefits. First, small models are far computationally cheaper to pre-train.111E.g., Llama- B was faster to pre-train compared to the B variant (Touvron et al., 2023  ###reference_b35###). Further, at inference time, they are considerably more hardware-friendly: a B model can be accommodated on a single A GPU, a feat unachievable for a B model (Dettmers et al., 2022  ###reference_b10###). Finally, as we have shown, when controlling for the compute budget, smaller models may actually outperform larger ones.\n\nOur findings also emphasize the importance of developing effective ranking approaches for LLM outputs. This is especially important in cases where no unit-tests or other verification methods are available (Zou et al., 2021  ###reference_b40###; Uesato et al., 2022  ###reference_b36###; Sun et al., 2023  ###reference_b33###). To support such research direction, we release Code Llama B outputs for each example in COCO and MBPP—a total of more than M outputs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Evaluation under Compute Restrictions",
            "text": "To study our main research question—what is the optimal way of using a given LLM compute budget—we consider a code-generation setup with unit-tests (Chen et al., 2021  ###reference_b4###; Austin et al., 2021  ###reference_b2###; Hendrycks et al., 2021  ###reference_b17###).\nBelow we discuss our methodology for code generation evaluation under computational restrictions. We begin by describing pass@ (Kulal et al., 2019  ###reference_b23###), the current main approach for evaluating code generation tasks (Section 2.1  ###reference_###). We then transition to describe our variant of code generation metrics under computational restrictions (Section 2.2  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Standard Code Generation Evaluation",
            "text": "To evaluate LLM code-generation abilities, a common setup assumes a set of coding questions, each with a set of unit-tests. The LLM is fed with each question, and a fixed number of output generations (labelled ) are sampled. The evaluation protocol considers each question for which at least one output passes all unit-tests as correct. To estimate the performance of a model that generates  outputs, it is common to generate a larger number of outputs  () and compute:\nwhere  is the number of examples that pass the unit-tests. The above mentioned metric results in an unbiased estimator as was shown by Chen et al. (2021  ###reference_b4###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Comparing LLMs of Different Sizes with a Fixed Budget",
            "text": "Our goal is to compare between LLMs of different sizes, which all have access to the same compute budget. To do so, we allow smaller models, which consume fewer resources, to generate more outputs. As a result, the generation process of both models requires roughly the same amount of compute.\nWe consider two types of compute budgets: the number of FLOPs and wall-time. For each type, a specific resource limit is set (e.g., k Tera-FLOPs or  seconds), and the model generates examples up to the point where the compute limit is reached.\nThat is:\nwhere flops() and time() are functions that return the FLOPs/wall-time usage of a given model that generates  outputs.\nNotably, the FLOPs restriction is a more theoretical computational restriction, as it assumes perfect utilization of the hardware. On the other hand, the wall-time restriction is more realistic, but is hardware specific, and thus not directly comparable across different machines."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "In this section we describe our experimental setup, focusing on the code benchmarks used (Section 3.1  ###reference_###), our metrics (Section 3.2  ###reference_###), and our experiments (Section 3.3  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Benchmarks",
            "text": "We experiment with three python code benchmarks: COCO, MBPP (Austin et al., 2021  ###reference_b2###) and APPS (Hendrycks et al., 2021  ###reference_b17###).\nThe COCO benchmark is a large-scale object detection, segmentation, and captioning dataset that includes a wide variety of common objects in complex scenes. The task involves object detection and segmentation in varied contexts.\nMBPP consists of test examples, each one is an instruction for a code function. Here, the Code-LLM is required to generate the full function.\nLastly, the test subset of APPS is composed of k programming problems at various levels of difficulty: introductory (k), interview (k) and competition (k). In the APPS tasks, the Code-LLM is required to generate the complete python file, which includes import declarations, class definitions, and so on."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Metrics",
            "text": "Computing the pass@ and pass@ metrics requires an estimation of the flops() and time() functions from Equations 2  ###reference_### and 3  ###reference_###. To estimate the FLOPs usage of each model we use the calflops library (xiaoju ye, 2023  ###reference_b38###), with input sequence length of . For the estimation of wall-time usage of a model, we measure wall-time while assuming optimal throughput utilization of the hardware. Specifically, we use a node of  A GPUs, optimize the batch size per model and measure the time it takes each model to generate a subset of k examples from our datasets. We report the results in Table 1  ###reference_###, for readability we also report the normalized factor with respect to the B model."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experiments",
            "text": "We use the Code Llama family (Roziere et al., 2023  ###reference_b30###), a finetuned version of Llama (Touvron et al., 2023  ###reference_b35###) for code generation. Code Llama comes in various sizes, which we use for our experiments: B, B, B and B.\nWe follow Roziere et al. (2023  ###reference_b30###), and use a zero-shot setting for COCO, a -shot prompting strategy for MBPP and -shot prompts for APPS, and limit the generation length to // tokens for COCO/MBPP/APPS.\nFor the sampling process, we use nucleus sampling (Holtzman et al., 2019  ###reference_b20###) with top- and a temperature of  for COCO/MBPP/APPS, with all models sizes (Roziere et al., 2023  ###reference_b30###). Additionally, we provide the pass@1 results using a greedy decoding method for all models.\nTo compare models in varying sizes, we select the maximal number of generations for each model with respect to the values in Table 1  ###reference_###. Specifically, for the smaller benchmarks, COCO and MBPP, we generate  answers for the B/B/B/B models, respectively. For the larger benchmarks, the three splits of APPS, we use . To get a robust estimation of these measures, we follow Chen et al. (2021  ###reference_b4###) and Roziere et al. (2023  ###reference_b30###), and report for all benchmarks a maximal value of  for the pass@ metric."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Small Models Outperform Large Ones under a Fixed Compute Budget",
            "text": "Results for COCO and MBPP are presented in Figures 2  ###reference_### and 3  ###reference_###, respectively. We first note that, as expected, the pass@ metric improves both with model scale, and with the number of generations  (Figures 2(a)  ###reference_sf1### and 3(a)  ###reference_sf1###).\nHowever, perhaps surprisingly, when considering the pass@ and pass@ metrics (Figures 2(b)  ###reference_sf2###, 2(c)  ###reference_sf3###, 3(b)  ###reference_sf2### and 3(c)  ###reference_sf3###), we see a different trend—given a fixed compute budget, smaller models yield better results than larger ones. Specifically, the B/B models outperform the larger models across all compute budgets. Particularly, in the small budget regime (up to  normalized FLOPs units and  wall-time units) the performance gap widens to —.\nAnother way of looking at our results is by observing that smaller models match the performance of larger ones using substantially lower budgets. For instance, in COCO, the B and B models achieve a score of  using one quarter of the time it takes the larger models to reach that score. Finally, we compare small models to greedy decoding with larger models, which generally performs better than sampling. We observe that even in this setup, using the smaller models several times is equivalent or preferable in all cases.\n###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### We next turn to discuss the results over the three splits of the APPS benchmark (Figures 4  ###reference_###, 5  ###reference_### and 6  ###reference_###). We first consider the B model, and observe the same trends as in COCO and MBPP: this model achieves the best performance in almost all fixed compute budgets. Specifically for the competition split (Figures 6(b)  ###reference_sf2### and 6(c)  ###reference_sf3###), the most challenging APPS split, the B model outperforms all other models in all compute budgets, with a consistent margin of  from the B model when considering the wall-time budget.\nWe further observe that the B model achieves similar or better performance as the greedy approach of all models in all three splits. Finally, when fixing the performance, we observe that the B model is – times more efficient than the B model (both for FLOPs and wall-time).\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16### We next observe that the B model is also competitive with larger models in small budget regimes (up to  normalized FLOPs units and  wall-time units). Nonetheless, it slightly underperforms the other models on larger budgets.\nThis can be attributed to the B model’s inability to generate a sufficient number of correct answers for the task, and may suggest that there is a minimum size requirement for a certain level of task difficulty.\nAnother intriguing aspect of our research question is the performance of LLMs under a fixed budget when unit-tests are unavailable, and a single selection among several generations must be made. We delve into this topic in the following section."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluating Code Generation without Unit-tests",
            "text": "We examine the scenario where unit-tests are not available (e.g., IDE code-completion setup). In this case, an efficient selection policy strategy may be used to select one answer from the model’s generations. In the previous cases (Section 2  ###reference_###), unit-tests served as this policy. Here we investigate using ranking as a selection policy. In Section 5.1  ###reference_### we show how to estimate the performance of a model given a ranking strategy, and in Section 5.2  ###reference_### we analyze the performance of larger models as rankers for a small model."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Evaluating Rankers",
            "text": "We assume a model that generates outputs, and a policy that ranks them. To estimate the performance of such setup, we count the number of groups containing generations where the highest-ranked generation within them is a correct one. That is:\nwhere is the number of answers generated for the estimation, and are the pass scores sorted according to the ranking policy. That is, passi is if the example ranked according to the policy is correct, and otherwise. See Figure 7 ###reference_### for a python implementation of rank-score@.\nSimilarly to Equations 2 ###reference_### and 3 ###reference_###, we also define:\nwhere flops() and time() are the same functions as in Section 2.2 ###reference_###.\nNext, we conduct an overview of COCO, a large-scale object detection, segmentation, and captioning dataset. COCO provides several evaluation metrics to assess model performance, such as mean average precision (mAP) and the intersection over union (IoU) for object detection tasks. These metrics allow us to comprehensively evaluate how well our models can rank and detect objects within images using the policy described earlier.\n\nWe evaluate the performance of large models as rankers using the above metrics on the COCO dataset."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Large Language Models as Rankers",
            "text": "We examine the usage of LLMs as rankers. To produce a ranking order over a set of generations, we use the averaged Negative Log Likelihood (NLL) the LLM assigns to each generation (excluding the prompt), and rank the generations according to that score. It should be noted that extracting the NLL of a model over a given generation can be done in a parallel manner (i.e., non-autoregressively), which is substantially more efficient compared to traditional token-by-token generation. The score given by a model to a generation given a prompt is:\nTo study the performance of LLMs as rankers we use the COCO and MBPP benchmarks.\nWe use generations produced by Code Llama B as described in Section 3.3 ###reference_###. As rankers we use all four Code Llama model sizes. We discard any generation that fails to complete, i.e., reached the maximal number of generated tokens without producing an end-of-sequence token. We also report the performance of running each model independently with one generation budget (both greedy and sampling).\nOur results are presented in Figure 8 ###reference_###. As can be seen, using LLMs as rankers over generations obtained from smaller models improves performance.\nInterestingly, we observe that using a B model as a ranker for itself can enhance its generation even further than the greedy approach, albeit with the cost of generating several outputs.\nWe also find that using larger models as rankers results in better performance.\n###figure_17### ###figure_18### When considering a fixed compute budget, we find that it is sometimes comparable to use LLMs as rankers instead of sampling from them, as can be seen with the B and B models. However, this is not the case for the greedy approach which consistently outperforms ranking multiple generations from a smaller model given a fixed compute budget. In summary, there remains a gap to bridge between using LLMs as rankers for smaller models and using them as generators. To further promote this line of research, we release the generations per example produced by the B model for both COCO and MBPP (a total of generations)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "was found to be one of the key elements in the success of LLMs (Dehghani et al., 2023  ###reference_b9###; Gu et al., 2023  ###reference_b14###; Hassid et al., 2024  ###reference_b15###; Rae et al., 2021  ###reference_b29###; Chowdhery et al., 2023  ###reference_b5###; Touvron et al., 2023  ###reference_b35###), with Wei et al. (2022  ###reference_b37###) demonstrating how specific abilities emerge mainly after reaching a specific scale. The way language models behave when they are scaled up and their ability to adjust have been a significant factor in the creation of LLMs (Hernandez et al., 2021  ###reference_b18###). Kaplan et al. (2020  ###reference_b21###) investigated the optimal model size to train for a given compute budget, while Hoffmann et al. (2022  ###reference_b19###) demonstrated how scaling both model and dataset sizes improves performance across various tasks. Clark et al. (2022  ###reference_b6###) analyzed the scaling properties of mixture-of-experts models, showing that scaling with the number of experts diminishes as model size increases. Recently, Gadre et al. (2024  ###reference_b13###) provided a scaling laws analysis considering downstream tasks rather than next-token prediction loss. They related the perplexity of a language model to its downstream task performance via a power law and used it to predict the top-1 error averaged over the evaluated downstream tasks. Our work differs from all of the above, as we do not claim to provide new scaling laws but rather suggest that when fixing the budget, smaller models can provide comparable or superior results to larger ones. Finally, a very recent work by Shi et al. (2024  ###reference_b32###) showed that small vision models can outperform larger models if they are fed with multiple scales of the image. Our approach, which generates multiple text outputs from a small model, is similar in spirit to theirs.\nis a growing trend, which leverages LLMs to verify and rank generations obtained from weaker and smaller models (Cobbe et al., 2021b  ###reference_b8###; Uesato et al., 2022  ###reference_b36###; Saha et al., 2024  ###reference_b31###; Havrilla et al., 2024  ###reference_b16###). Both Cobbe et al. (2021b  ###reference_b8###) and Uesato et al. (2022  ###reference_b36###) leveraged an external classifier to rank LLM outputs. Specifically in both setups the authors proposed to generate many candidate solutions and select the one ranked highest by the verifier. The authors demonstrated the applicability of using such verifiers in solving math word problems (Cobbe et al., 2021a  ###reference_b7###). Qin et al. (2023  ###reference_b28###) demonstrated that LLMs can serve as efficient text rankers when considering pairwise ranking. Another line of work leveraged LLMs to evaluate the quality of smaller models (Saha et al., 2024  ###reference_b31###; Dubois et al., 2023  ###reference_b12###; Zheng et al., 2023  ###reference_b39###; Oren et al., 2024  ###reference_b27###). Although providing promising alternative, such evaluation suffers from biases in the larger model (Zheng et al., 2023  ###reference_b39###) and reliance on hand-designed evaluation plans that impact the method’s ability to generalize (Liu et al., 2023  ###reference_b26###). Large models also serve as verifiers of small ones in a speculative decoding setup, with the goal of speeding-up LLM generation (Leviathan et al., 2023  ###reference_b24###; Kim et al., 2023  ###reference_b22###; Chen et al., 2023  ###reference_b3###). In this work we explore the potential of LLMs as selectors of the best output of a smaller model in a fixed budget setup. Similarly to ours, Li et al. (2024  ###reference_b25###) found that smaller sized LMs (B parameters) already exhibit strong mathematical abilities when selecting the best response from  different generations. When considering code generation models, AlphaCode Team (2023  ###reference_b1###) presented impressive results on challenging coding contests tasks while generating M samples, and later on filtering and ranking them using Gemini-Pro LLM (Team et al., 2023  ###reference_b34###). Lastly, Dou et al. (2024  ###reference_b11###) proposed a method to improve code-generation models by learning a policy model using reinforcement learning methods. This is done by sampling several solutions from the LLM and scoring them using a feedback obtained from the compiler."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion & Limitations",
            "text": "Our results show that using smaller models with the same amount of compute can improve LLM code-generation performance. An interesting question we do not fully address is whether, given enough compute, the larger models will overtake the smaller ones, or perhaps they will all saturate at a similar performance level at some point. Our COCO and MBPP results seem to slightly support the latter hypothesis (as all models begin to saturate, see Figures 2  ###reference_### and 3  ###reference_###). However, unfortunately, due to compute constraints, our setting is restricted to exploring only a limited number of generations per model.222For instance, generating  answers for the  examples of the APPS benchmark with a B model takes about  days using a node of  A GPUs. We note that despite this limitation, in practice, due to these costs our conclusions apply to most practical use-cases. We defer more expensive experiments to future work."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we compared large language models with smaller-sized models under fixed budget constraints (i.e., FLOPs and wall-time). We evaluated the models using execution-based code-generation tasks, which provide access to unit-tests. Our findings reveal that generating multiple outputs from a B model may lead to gains of up to 15% over a single generation from a B model across five tasks. This highlights the potential of using smaller models instead of larger ones. In scenarios where unit tests or other solution verifiers are unavailable, we explored a simple ranking-based approach for candidate selection. We found the proposed ranking approach falls short in performance compared to a single output from the larger model. Our findings emphasize the importance of studying approaches for ranking LLM outputs, which hold great potential to not only improve model performance but also improve budget allocation. To further enhance this research direction we release over M samples from the Code Llama 7B models considering both COCO and MBPP benchmarks."
        }
    ]
}