{
    "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
    "abstract": "With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20% and does not require any additional prompt engineering or prior declaration of “you may be attacked“ to the LLMs’ context window.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) trained on large datasets and with significant computational resources have shown unprecedented capabilities (Brown et al., 2020  ###reference_b9###; Kaddour et al., 2023  ###reference_b31###; Zhao et al., 2023  ###reference_b87###; Bubeck et al., 2023  ###reference_b10###).\nFor instance, Claude-3’s latest release exhibits tentative self-awareness111https://www.anthropic.com/news/claude-3-family  ###reference_mily### in the Needle In A Haystack eval222https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main  ###reference_InAHaystack/tree/main###, highlighting the importance of further enhancing the security, user-friendliness, and controllability of LLMs (See et al., 2019  ###reference_b53###; Wang et al., 2023a  ###reference_b61###; d  ###reference_b70###).\nA straightforward way to achieve these goals is to align LLMs’ behavior with human feedback (Stiennon et al., 2022  ###reference_b57###; Ouyang et al., 2022  ###reference_b46###).\nExisting alignment methods first allow humans (or LLMs) to select preferred model responses based on specific criteria. Then, reinforcement learning techniques, e.g., RLHF (Ziegler et al., 2020  ###reference_b92###; Ramamurthy et al., 2022  ###reference_b50###; Stiennon et al., 2022  ###reference_b57###), RLAIF (Bai et al., 2022b  ###reference_b5###; Lee et al., 2023  ###reference_b35###; Chu et al., 2023  ###reference_b14###),\nand Direct Preference Optimization (DPO) (Rafailov et al., 2023  ###reference_b48###)\nmethods are used to train LLMs towards preference-specific behaviors. However, utilizing the existing widely used helpful, honest, harmless (3H) preference criteria to align LLMs poses potential risks as follows:\n(i) In terms of the model performance, over-optimizing 3H rewards\naccording to Goodhart’s law\n333https://www.lesswrong.com/tag/goodhart-s-law  ###reference_aw###\nmay hamper truthfulness performance (Gao et al., 2022  ###reference_b22###), e.g., balancing harmlessness vs usefulness (Bai et al., 2022a  ###reference_b4###; Dai et al., 2023  ###reference_b16###);\n(ii) 3H models tend to imply that LLMs overly\nprefer human inputs in favor of self-positions\n(Xie et al., 2024  ###reference_b76###; Xu et al., 2023a  ###reference_b77###), rendering them susceptible to camouflaged red-team attacks or poisoned text in the context window (Liu et al., 2023c  ###reference_b40###; Zou et al., 2024  ###reference_b93###). The phenomenon that LLMs are highly receptive to external information is referred to as Adaptive Chameleon by Xie et al. (2024  ###reference_b76###).\nIn this paper, we introduce Dialectical Alignment (DA) to address the challenge that 3H LLMs frequently alter their answers when encountering knowledge conflicts due to their tendency to trust external input (by humans) easily. DA empowers aligned LLMs to think dialectically upon conflicting knowledge so that they can spontaneously decide whether to trust the external information or reject the poisoned contexts. We first explore two tasks representing two sides of a coin in dealing with knowledge conflicts: In-context Knowledge Editing (IKE) (Zheng et al., 2023a  ###reference_b88###) and Poisoned Context Attack (PCA) (Zhong et al., 2023  ###reference_b91###; Liu et al., 2023d  ###reference_b41###; Zou et al., 2024  ###reference_b93###) (see more details in Section 2  ###reference_.SSS0.Px2###). The existing studies mentioned above have investigated these two tasks separately, without recognizing their correlation: if an LLM is more susceptible to the external information in the context, the effectiveness of IKE improves while the success rate of PCA also increases; conversely, if a model tends to adhere to its own parametric memory, Poisoned Context Attack will be defended while the model also rejects IKE. We give intuitive examples of this fact in Figure 1  ###reference_### and Figure 2  ###reference_###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Shayegani et al. (2023  ###reference_b54###) proposed an important mode for the failure of LLMs’ safety training - competing objectives. This mode points to the difficulty of balancing the safety and helpfulness of a model when the LLMs’ function (e.g., should always follow instructions and be helpful) conflicts with their safety objectives (Wei et al., 2023a  ###reference_b71###; Shen et al., 2023b  ###reference_b56###). An overly security-trained LLM can easily reject innocuous user instructions (Ganguli et al., 2022  ###reference_b21###) (e.g., role-playing), while complex jailbreak instructions can easily attack a model that lacks security training. On top of this, we presented a more indistinguishable example of knowledge editing and poisoned context attacks in Figure 1  ###reference_###, which both exploit the model’s property of following user instructions as well as trusting the human input but achieve different goals (edit vs. attack). Existing alignment work does not focus on the balance between these two, while our Dialectical Alignment trains models to have the ability to spontaneously make judgments for trust the input or not.\n###figure_1### In-context knowledge editing (IKE) is a novel strategy for LLMs’ knowledge editing without retraining (Zheng et al., 2023a  ###reference_b88###). Compared to other parameter-updating approaches (Yao et al., 2023  ###reference_b84###), it effectively adapts the factual knowledge in language models without parameter updating and with fewer unwanted side-effects (Onoe et al., 2023  ###reference_b45###; Cohen et al., 2023  ###reference_b15###).\nDespite the effectiveness brought by in-context learning (Brown et al., 2020  ###reference_b9###), uncurated external information also introduces hazards to LLMs. When encountering knowledge conflicts between context and parametric memory (Xu et al., 2024a  ###reference_b78###), studies by Qian et al. (2023  ###reference_b47###) and Xie et al. (2024  ###reference_b76###) reveal that LLMs are more inclined towards external evidence, especially when it appears coherent and convincing. Consequently, similar to various context-based attacks (Liu et al., 2022  ###reference_b38###; Mei et al., 2023  ###reference_b43###; Liu et al., 2023c  ###reference_b40###; Toyer et al., 2024  ###reference_b60###; Schulhoff et al., 2023  ###reference_b52###), malicious users could easily exploit IKE to attack LLMs with false information (Zou et al., 2024  ###reference_b93###), which is namely poisoned context attack (Lukas & Kerschbaum, 2023  ###reference_b42###).\nIn an effort to mitigate the risk of data poisoning attacks, some defense methods are proposed for pre-trained language models (Zhang et al., 2022  ###reference_b86###; Wang et al., 2022a  ###reference_b67###; Jia et al., 2022  ###reference_b29###; Wang et al., 2022b  ###reference_b68###; Wang & Feizi, 2023  ###reference_b66###). Chen et al. (2022  ###reference_b11###) conduct a calibration study to discourage models from providing a single answer when confronted with multiple conflicting pieces of evidence. However, there is limited research addressing the potential danger associated with IKE. Our work aims to leverage the reasoning capability of LLMs to dialectically reassess the information factuality in their context window, thereby offering an aligned approach to resolving knowledge conflicts in the retrieval augmentation of LLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "###figure_2### RLHF pipeline (Ziegler et al., 2020  ###reference_b92###; Stiennon et al., 2022  ###reference_b57###) is effective at aligning LLMs to human preferences, but gathering high-quality human preference labels is a key bottleneck. Bai et al. (2022b  ###reference_b5###) proposed Constitutional AI for defining a set of principles to guide LLMs in self-criticizing and improving and thus collecting preference data. RLAIF (Lee et al., 2023  ###reference_b35###)(Reinforcement Learning from AI Feedback) trains language models using preferences labeled by an AI system instead of humans. Lee et al. (2023  ###reference_b35###) indicate that RLAIF achieves comparable performance to RLHF. More proximately, AI feedback data solves the data bottleneck of LLM preference learning, but first training a reward model and then reinforcement learning is still a complex and unstable process. Rafailov et al. (2023  ###reference_b48###) propose DPO, which directly optimizes for the policy best satisfying the preferences with a simple classification objective.\nXu et al. (2024a  ###reference_b78###) categorized existing conflicts in large model knowledge into three types: context-memory, inter-context, and intra-memory conflict. Retrieval augment generation has become mainstream in existing applications of LLMs (Lewis et al., 2020  ###reference_b36###; Gao et al., 2024  ###reference_b23###). However, external information is often erroneous and noisy (Chen et al., 2023a  ###reference_b12###; Liu et al., 2023b  ###reference_b39###; Ali et al., 2020  ###reference_b2###; 2021  ###reference_b3###; 2019  ###reference_b1###), thus context-memory and inter-context conflicts increasingly impact the credibility of LLMs generated content. In our experiments, we emphasize both of these conflicts. In our scenario, inter-context conflict manifests when both correct factual information and poisoned contexts are simultaneously input into the LLM’s context window. Context-memory conflict, on the other hand, arises in our experiments due to conflicts between prior knowledge memorized by the model parameters and external information, Figure 2  ###reference_### offers an intuitive example."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To address the challenge of getting LLMs to balance IKE and PCA defense mentioned above, unlike prior alignment work that focuses on making the model helpful and harmless, and using offensive or disturbing topics as red team behaviors (Bai et al., 2022a  ###reference_b4###; Ganguli et al., 2022  ###reference_b21###), our goal is to defend more insidious red team behaviors, where they exploit the human-friendly and instruction-following nature of the aligned LLMs for the attacks (Wei et al., 2023b  ###reference_b72###; Zou et al., 2024  ###reference_b93###). Specifically, we attempt to align the LLMs to dialectically recognize the user’s purpose (attack or not) and thus selectively choose to believe or reject the input in their context window. Below, we provide precise details regarding our end-to-end Dialectical Alignment framework. See Figure 3  ###reference_### for an illustration.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Base Response Evaluation",
            "text": "Base Generate with Extra Contexts (Base)\nGenerate with Tips (Tips)\nGenerate with Base CoT (Base CoT)\nGenerate with Given No Prior Knowledge CoT (CoT-NoPK)\nGenerate with Given Prior Knowledge CoT (CoT-PK)\nPart 1: Entity Extract Extract the key concepts in the given {question} by format \\[’concept1’,’concept2’,...]\nPart 2: Long Context Generate Tell me what you know about {concept}\nPart 3: CoT Generate\nRetrieved Contexts: {context}  Dialectical KEY:Follow the steps below: 1.Judge the accuracy of the content based on context generated in Part 2; 2.Decide whether or not to refer to this content; 3. Give the\ncorrect answer\nQuery: {question}\nIn STEP 1 in Figure 3  ###reference_###, the initial LLM responds to external information and corresponding questions by the instruction with named Base in Table 1  ###reference_###, which may include poisoned context or factual evidence, or a combination of both in LLMs’ context window. At the same time, a more robust LLM serves as the evaluator for these responses. In our experiments, the evaluator is GLM-4 from Zhipu AI 444https://zhipuai.cn/devday  ###reference_zhipuai.cn/devday###.\nSee Appendix  A  ###reference_### for more discussion on LLM in evaluation. The selection process identifies poisoned responses, comprising those influenced by poisoned context, resulting in incorrect answers and responses containing factual data but yielding incorrect answers. The goal of this step is to establish a baseline of not providing any hints to the model that the external information may be incorrect."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dialectical Path Testing and Revision",
            "text": "Next, we aim to inject different reasoning strategies into the instructions to help the model verify the trustworthiness of the external information. Specifically, in STEP 2 in Figure 3  ###reference_###), we test various reasoning strategies, which are referred to as Dialectical Paths for the same external information as in STEP 1. These paths, detailed in Table 1  ###reference_###, are designed to find the most effective strategies for improving the LLM in terms of generating correct answers in different situations, i.e., better IKE and PCA defense capabilities. We follow the logic of designing these paths from simple to complex, from single to multiple turns of dialogs. Specifically, we first introduce only Tips, suggesting that external context may be incorrect first. Building on the previous research that Chains-of-Thought have been encoded in the model’s parameters (Wei et al., 2023c  ###reference_b73###), we aim to investigate whether providing such tips can prompt LLMs’ dialectical thinking. Then, we use the Base CoT prompt “let’s think step by step” proposed by  Kojima et al. (2023  ###reference_b32###), without referring to explicit thinking steps. Following this, we describe CoT-NoPK, where the reasoning steps that the model is prompted to follow are clearly described, including fact-checking and filtering the poisoned context.\nHowever, in our experiments, we find that under the aforementioned dialectical paths, the LLM still tends to engage in lazy thinking, i.e., directly paraphrasing contexts from the external evidence without referencing the knowledge memoried in its parameters. Therefore, we devise a multi-turn dialog dialectical path, CoT-PK: first, we prompt the model to extract entities from the question, then based on its memory, output knowledge regarding these entities (referred to as prior knowledge in our study); finally, we prompt the model to assess the credibility of the external contexts based on this knowledge. Finally, we prompt the model to perform reasoning using the same strategy as CoT-NoPK.\nIt is worth noting that responses generated by LLMs following our specific dialectical path testing tend to be less natural (e.g., always in the format of “step 1…, step 2…, step 3…”). Thus we use another SOTA LLM to revise them to be more readable in STEP 3 (e.g., “First, based\non the…, so…, lastly…”). We use different SOTA LLMs in the LLM feedback and revision step to prevent LLMs from self-serving bias over their own generated answers (Xu et al., 2024b  ###reference_b79###) and affecting the fairness of accuracy evaluation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Model Supervised Fine-Tuning (SFT)",
            "text": "In STEP 4, we use the revised optimal dialectical paths’ responses in the previous steps (named Dialectical Response in Figure 3  ###reference_###) to construct the dialectical supervised dataset. Subsequently, we conduct supervised fine-tuning (SFT) with the objective of imparting the model with foundational dialectical reasoning skills. Specifically, we construct a dataset using the Alpace (Taori et al., 2023  ###reference_b59###) format, i.e., {\"instruction\"; \"output\"}, where the instructions are in the format of the Base path in Table 1  ###reference_###, and the outputs use the revised responses corresponding to the cases with different external information. An example sample of the constructed dataset is illustrated in Table 5  ###reference_### in the Appendix. Through the steps above, we eschew costly human feedback in favor of utilizing AI feedback (Bai et al., 2022b  ###reference_b5###; Lee et al., 2023  ###reference_b35###) to navigate the most efficient path toward dialectical thinking and aligning LLMs to acquire this skillset."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we describe our experimental details. First, based on the analogous nature of IKE and PCA in utilizing external evidence, as demonstrated in Figure 1 ###reference_###, we design a unified experimental framework to study these two tasks. Second, we compare in detail the effects of different reasoning strategies in Table 1 ###reference_### on IKE and PCA defense. Finally, we construct SFT datasets and use them for Dialectical supervised fine-tuning."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Settings",
            "text": "Tasks. Our experiments explore the ability of LLMs to give correct answers grounded in external information within the context window. Our experiments are for tasks: (1) PCA Defense: LLM needs to defend the attack of the poisoned information in their context window; (2) IKE: LLM needs to update its answer based on the factual information in the context and thus generates the correct answer. Referring to our description in Section 2 and Figure 1, these two tasks are essentially two sides of the same coin of knowledge conflicts. Therefore, we tailor these tasks simply by controlling the ratio of factual and poisoned information in the external context.\n\nModels. We selected TinyDolphin-2.8-1.1B (shortened as TinyDolphin in the paper) and Mistral-7B-Instruct-v0.2 (shortened as Mistral-7B in the paper) in our experiments. TinyDolphin is trained from TinyLlama on the Dolphin 2.8 dataset, a dataset that filters out samples such as alignments, rejected answers, etc., to fine-tune TinyLlama into an unaligned and uncensored model. Mistral-7B is a model that outperforms all other 7B models on MT-Bench and stands out as a model comparable to the 13B chat model. We select gpt-3.5-turbo-16k as the revision LLM in STEP 3 of DA and GLM-4 as the scoring model for all experiments.\n\nDatasets. We follow the poisoned dataset format of previous studies. These datasets are sampled from ImageNet, MS-MARCO (MS), and Natural Questions (NQ), and each sample consists of one question, one correct and one incorrect answer, five poisoned contexts that support the incorrect answer, and one to two factual contexts that support the correct answer, see an example in Appendix B. We utilized 300 samples to identify efficient inference paths and build the training data. Another 300 non-overlapping samples were set aside for the test set to prevent model memorization of correct answers during training. Unless specified otherwise, the results presented in the paper pertain to the test set.\n\nEvaluation. We employ GLM-4 to assess the accuracy (ACC) of the LLMs’ responses based on the correct answers. We use the template in Table 7 to instruct GLM-4 to make the judgment.\n\nExperimental Variables. LLMs are vulnerable to various factors when utilizing external information to answer questions, including the temperature used during inference, the length of information in their context window, the noise in content, etc. Our experiments manage these variables. Specifically, our experiments are evaluated at two temperatures (T), 0.1 and 0.7, where higher temperatures represented higher creativity and diversity of responses. The number of poisoned contexts (PCN) ranges from 0 to 5, accompanied by whether the context window has factual contexts supporting the correct answer (FC in Table 6) and whether the factual context is located at the beginning or end of LLM’s context window. Details of the experimental variables are introduced in Table 6 in Appendix C.\n\nUnified Experimental Framework for IKE and PCA Defense. Our above variable setup allows us to experiment uniformly with IKE and PCA defense tasks. Specifically, when LLM simply answers based on its memory; when LLM answers based on the factual contexts in its context window. In this process, LLM deals with knowledge conflicts between its own parametric memory and external context (called context-memory conflict by Xu et al., which is consistent with the IKE setting. When we focus on scenarios where LLM handles context-memory conflict and PCA defense. Finally, when, we focus on scenarios where LLM handles both inter-context conflict and context-memory conflict to defend PCA. In addition, we use RO to stand for reorder, which means putting the factual evidence in front of the poisoned contexts."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Finding Paths to Motivate LLMs’ Dialectical Thinking",
            "text": "In this section, we explore in detail the dialectical thinking paths of LLMs when dealing with Inter-context conflict and Context-memory conflict, and we analyze the most efficient paths under different variable settings. In Table 2, we present the accuracy of the model in generating answers directly, without utilizing any external data. We provide the average ACC of the two models under the PCA defense task (i.e., ) when answering using different paths in Figure 4 and Figure 5. The results for the IKE task (i.e., only factual context provided) are detailed in Table 3. Our findings on path selection strategy are as follows:\n\nAligned LLM is more receptive to external evidence than unaligned LLM. As shown in Table 2, TinyDolphin exhibits significantly lower ACC than Mistral-7B when no external information is available. Additionally, in Figure 4 and 5, we observe that across all paths, TinyDolphin consistently has lower ACC than Mistral when and factual evidence is present in the context (represented by green and blue lines in these figures). Despite this, TinyDolphin still maintains an ACC of about when the external information contains only poisoned data (indicated by the yellow line with the ”Without FC” tag), while Mistral-7B’s ACC is very close to zero, especially when . These results appear to contradict the findings of Xie et al. (2024), which hypothesized that larger LLMs would be more stubborn to their own parameter memories due to their enhanced memory and reasoning abilities and greater sensitivity to poisoned datasets.\n\nOur results suggest that even though a larger aligned model possesses better memory and reasoning abilities, it could be more susceptible to a poisoned data attack because of the high trust in (human) inputs.\n\nThink less but become more dialectical? The path that excels in both IKE and PCA defense seems to be non-existent. From Table 3, we observe that in the IKE task, employing complex multi-turn dialog CoT paths (CoT-PK) leads to a notable decrease in the model’s answer accuracy compared to other paths. Furthermore, there are no substantial disparities among other paths with no prior knowledge. This suggests that when the model encounters a knowledge conflict and emphasizes the knowledge memoried in its parameter first, it tends to become more stubborn, which can be particularly detrimental in knowledge editing. However, upon comparing subfigures 5 and 4 with the other subfigures in Figure 4 and 5, it becomes evident that CoT-PK allows the model to better resist the attack of poisoned contexts. Conversely, the effectiveness of defense diminishes for the other paths as the amount of poisoned evidence increases. In particular, for Mistral-7B, when the number of poisoned contexts and the number of factual contexts in the model context are almost equal (PCN = 1), the paths with no prior knowledge exhibit a higher ACC (as depicted in subfigures 5-5). However, when PCA significantly surpasses the factual contexts (PNC 2), these paths result in a lower ACC, which is consistent with previous research indicating that LLMs tend to choose the side supported by more evidence (Xie et al., 2024; Xu et al., 2024a). That’s why it’s crucial to train LLMs to learn to dialectically adopt the optimal path based on the distribution of poisoned and factual information in the context window. We provide more detailed results in Appendix D to analyze the effects of different temperatures and the order of external evidence."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Finetuning LLM with Dialectical Data",
            "text": "Based on the previous findings, we construct the Dialectical SFT dataset using different dialectical path responses on different external evidence distributions. Specifically, when , we opt for the responses generated by the four paths without prior knowledge. When , we choose the CoT-PK responses to compile the SFT data. Notably, due to the consistently low ACC of TinyDophin, we exclusively utilize revisoned Mistral-7B’s responses for constructing the training data. We constructed a total of 9,012 data in Alpaca format (see an example in Table 5  ###reference_###) and fine-tuned the model using LoRA (Hu et al., 2022a  ###reference_b24###). Figure 6  ###reference_### and Table 3  ###reference_### display the results of these fine-tuned models on the test dataset, showcasing their enhanced ability to defend PCA while maintaining strong performance in IKE tasks. Moreover, the improvement is particularly notable when PCN significantly exceeds factual context (i.e. when)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Disscussion",
            "text": "In this work, we propose Dialectical Alignment to address the challenge that helpful, honest, and harmless LLMs are highly receptive to human input, which can result in vulnerability to poisoned data attacks (PCA). Unlike previous research, our goal is to maintain the In-context knowledge editing (IKE) performance while simultaneously enhancing the model’s resilience against such attacks. To strike such a balance, we design five dialectical paths ranging from simple to complex to observe the effective reasoning strategies of LLMs when dealing with poisoned and factual contexts with their context window. Building upon this insight, we construct a dialectical dataset and conduct supervised fine-tuning of the model.\nThe resulting fine-tuned model effectively defends against PCA while ensuring IKE’s effectiveness remains intact. We anticipate that our approach will provide actionable insights and solutions for enhancing retrieval augmented generation systems of LLMs."
        }
    ]
}