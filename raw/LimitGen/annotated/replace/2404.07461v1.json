{
    "title": "“Confidently Nonsensical?”: A Critical Survey on the Perspectives and Challenges of ‘Hallucinations’ in NLP",
    "abstract": "We investigate how hallucination in large language models (LLM) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research.\nThrough a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term ‘hallucination’. Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The utilization of Natural Language Processing (NLP), particularly in language generation, has witnessed significant advancement in recent months, owing to the emergence of highly proficient large language multi-modal models like GPT-4 and Gemini Gautam et al. (2024  ###reference_b31###). These models have evolved beyond NLP tools used in technical domains to become sociotechnical systems, amalgamations of social and technical elements that collectively contribute to goal-driven behavior, exerting influence on both social and technical stakeholders involved in the system Cooper and Foster (1971  ###reference_b16###); Narayanan Venkit (2023  ###reference_b80###). NLP applications are now prominently employed in diverse domains ranging from health and medicine Lee (2018  ###reference_b56###) to policy-making Jin and Mihalcea (2022  ###reference_b49###) and entertainment Werning (2024  ###reference_b117###), thereby solidifying their pivotal role in everyday life.  \n###figure_1### The proliferation of language models has also drawn substantial attention to its limitations and potential risks like the propagation of misinformation and the exacerbation of biases Bender et al. (2021  ###reference_b4###); Gupta et al. (2024  ###reference_b37###). Concurrently, there has been a surge in research focusing on examining the phenomenon of ‘hallucinations’ in language models Ji et al. (2023a  ###reference_b44###). This trend is evident in the recent rise in peer-reviewed publications investigating hallucinations in NLP and language models, as depicted in Fig 1  ###reference_###, sourced from SCOPUS.  \nIn the domain of NLP, numerous frameworks have been proposed to conceptualize hallucination, with a predominant focus on its negative connotation. The negative notion commonly revolves around a model’s tendency to exhibit specific errors, particularly evident in tasks such as image captioning and text generation. Hallucination, in this context, denotes instances where the model generates references to non-existent objects or statements, despite lacking corresponding examples in the training dataset Ji et al. (2023a  ###reference_b44###). Despite the increase in studies on hallucination, few works have underscored the need for a cohesive framework and precise definitions within NLP and language model research Filippova (2020a  ###reference_b26###).  \nAcknowledging ‘hallucination’ as a social construct is imperative. Given the evolution of these models into sociotechnical systems, a pressing need arises to grasp the interdisciplinary nuances surrounding this phenomenon to understand how the field addresses it. This necessity is further underscored by research illustrating the societal repercussions of hallucinations on society Dahl et al. (2024  ###reference_b17###). Therefore we see an increase in the need to understand how NLP defines and conceptualises hallucination. Based on this requirement, the following questions guide this study:  \nRQ1: What are the definitions and common frameworks used to explain hallucinations in NLG-published articles?  \nRQ2: What is the current understanding of researchers about hallucinations, and how do they encounter them in their work?  \nTo answer the RQ1, we first conduct an audit of the field of hallucinations in NLP by surveying 103 peer-reviewed articles111github.com/PranavNV/The-Thing-Called-Hallucination. Subsequently, we conduct a practitioner survey involving 171 researchers and academics within the field to explore their viewpoints on this phenomenon and answer RQ2.  \nThrough this work, we aim to understand the challenges posed by hallucinations in NLP. Finally, based on our findings, we construct an ethics framework to guide future endeavors to understand and address hallucinations in language models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Evolution of Hallucination in NLP",
            "text": "The term ‘hallucination’ has a long history in machine learning and has been used in various contexts prior to the LM era. Its earliest documented usage can be traced to the 2000s when Baker and Kanade (2000  ###reference_b3###) applied it in the context of image resolution enhancement, referring to the generation of new pixel values. Subsequently, \"hallucination\" has been frequently employed in computer vision research, including notable works such as Hsu et al. (2010  ###reference_b39###) on face hallucination.\nIn the modern deep learning era, hallucination was used first by Andrej Karpathy in his blog focusing on Recurrent Neural Networks Karpathy (2015  ###reference_b51###). He used the term within the context of language models by illustrating how an LSTM could generate non-existent URLs, effectively ‘hallucinating’ data. The term then gained major traction with the launch of ChatGPT Wu et al. (2023  ###reference_b119###), where it referred to inaccuracies and factual mistakes produced by models Ji et al. (2023a  ###reference_b44###). However, the field lacked a unified definition, leading to a spectrum of interpretations Filippova (2020b  ###reference_b27###). In one of the earlier works, Maynez et al. (2020  ###reference_b73###) divides term usage into intrinsic and extrinsic hallucination. Intrinsic hallucinations are consequences of synthesizing content using the information present in the input. Extrinsic hallucinations are model generations that ignore the source material altogether.\nInitial attempts at detecting hallucination in LMs try to calibrate model probabilities to tell itself if it is hallucinating Tian et al. (2023  ###reference_b109###); Lin et al. (2022  ###reference_b60###); Kadavath et al. (2022  ###reference_b50###). Some attempts have been made to use transferable adversarial attacks to detect hallucination Yu et al. (2023  ###reference_b125###).\nInitial works on reducing hallucination aimed to retrain LMs with extra knowledge Zhang et al. (2019  ###reference_b129###); Sun et al. (2021  ###reference_b105###).\nRecently, self-checking, wherein models check their outputs for factual correctness, has shown progress as well Manakul et al. (2023  ###reference_b69###); Luo et al. (2023  ###reference_b64###). Another direction to reduce hallucination in LMs is by using external knowledge for producing outputs, commonly known as Retrieval Augmented Generation Systems (RAGs) Lewis et al. (2020  ###reference_b58###); Guu et al. (2020  ###reference_b38###); Zheng et al. (2023  ###reference_b131###) whereby the model reduces hallucination using a controlled setting, such as querying on internal data.\nHowever, there is a rise in discussion around terminology that reflects a deeper inquiry into the phenomena, with recent discourse advocating for ‘confabulation’ Millidge (2023  ###reference_b77###) or ‘fabrications’ McGowan et al. (2023  ###reference_b74###) as a more precise descriptor. This reflects the lack of consensus on the term and highlights the importance of looking at the use of hallucination with a more critical lens."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Social Perspectives on Hallucinations",
            "text": "HHHTHTHHHHTTHHHTHTTTHHHHTTTTHTHHTTTHTHTTHHHTHHTTHHTHHTHTTHTHHTHTTHHTHTTHHTHHHTHTHTTHTHHTHHTTTHHTTHHTHTHTTHHTTTHHHTTHHTHTHHTTHHHTTTHHTHTHTTHHHTHTTHHHTHHTHHTHHTTHTHTHHTHHTHTHHTHHTTTHTTHHHTTHTHHTTHHTHHTTHHHTTHTTHTHHTTHTHHHTTHTTHTHHTHTHTHTHHTTHHTH_HTHTHTHTTTHTHHTTTHHHTHTTHHTTHTHHTHTHHTHTTHHHTHTTHTHTTHTHTTTTTHTTHTTHHTHTHTHHTHTTHTHTHHTHHHHTHHTHHTHTHTHHTHTTHTTTTHHHHHTTHTHHTHTHHTTHTHTTHHTTHTHTHHTHTHHTHTTHTHHTHTTHHTHTTHTHHTHTHHTTHTHHHTHHTTHTHTHTTHHTTHTHHTHHTHHTHTHTHTHHTHHTHTHHTTTHTHHTHHTHTHTHTHTHHTTTHTHTHTHHTHTHHTHTHHTTTTTHTHHTHTHTTHHHTTHTHTHTHHTTHTHHTTTHTHTHHTHTTHHHTHTTHTHHTTHTTHTHHTHTHTHTTHHTTHTTHTTHHTHTHTTHHHTHTTHTHHTHTHHTTTTHTHHTHTHHTHHTHTHTHHTHTTHTHTHTHHTTTHTHHHTHTHHHTHTHHTTTHTHHTTHHTTHTHHTHTTTHTHHTHTTHHHTHTTTHTHHTTHTHTHTTHHTHHTHTHTHTHTTTHTHTTHTTHHHTHHTHHHTHHTHHHTHTHTHTHTHHTHTTHHTTTHTHTH_THTHHTHTTHHTHTTTTHHHTTHTHTHTHTHHTHTHTHHTHHTTHTHTHTTHTHHHTTTHTHHTHTTHHTHTTHTHTTHHTTHHTTHHTTHTHTHTHTHHHTHTTHTTHTHTTHTHTHTHTHTHHTHHTTHTHTHTHTTTHHHTHTHTTHHTHTTHTHTHTTTHHTHHHTTHTHTHHTTTHTHHTHTTTHTTHTTHTHTHHTHTTHTTHHTHTHTHTHTHHTHTHTTHHTTHTHTTHTHHTHTHTHTTHTHHHTHTHTHTTHHHTTTHTHTHTHTHTHHTHTHTHHTHTHHTHTHTHTHTHHTHTTTHTHTTTHTTHTHHHTHTHTHTHTHTHTHTHHTHHHTHHTHHTHTHHHTTHTHTHTTHTHHTHTHHTTHTHTHTTHTHHHTHHTHTTHHTHTHHTHHTTTHTHHHTTTHTTHTHTHTTHTHHTHTHTHHTTTHTHTHHTTHTHTHTHTHTHTHTHTHHTHTHHHTTTHTHHTHTTHTHHTHTHTHHHTHTHTTHHTHTTHHHTHHTHTTTHTHHTHTTTHHTTHHTHTHTHTHHHTHTHHTTTHTHHTHTTHHTHTHHTHTHTHTHHHTTHHTHTHTHHHTTTHTTHHHHTHHTHHTHTHTHSHTHHTHTTHHHTTTHHTHTHTHTHTHTHTHTHHTHHTHTHTHTTHTHTHTHTHTHHHTHTHHTTHHTHHTTHHTHTHTTHHTHTHTHHTTTHTHTHTHTHHTHHHTTTHTHTTHTHTHHTTHTHTHTHHTHHTHTTHHTHTHHTHTHTHTHHTTHTHHTHTHTHHTTHTHHTHTHTTHTHHHTHTHTTHTTHHTHTHHTTHTTHTHHTHHTHTHHTTHHTTHTHTHHTTHTHTTHTHTHTHHTTHTHTHTTHTHTHTHTHTHTHTHHTHTHHTHTHTHHTHHTHTTHTTHTHTHHTTHTHTHTTHHTHTHTHHTHTHHHTHTHHTTTHTTTHTTRHTHTHHTHTHT"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Critical Analysis of Hallucination in NLP",
            "text": "Given the discourse outlined in the preceding sections, it becomes evident that the framework employed for hallucination in AI substantially deviates from established social frameworks. This discrepancy presents challenges, as mentioned previously, when examining hallucination within sociotechnical systems Venkit et al. (2023  ###reference_b112###), which strive to devise interdisciplinary solutions for a nuanced understanding of and for society. Therefore, conducting a critical analysis of this phenomenon becomes necessary to understand how the field of NLP conceptualizes and measures hallucination.  \n0101011011101010010110100010110101010110010111110110100111101010010100101  \nTo accomplish this, we conducted an audit of works from the ACL anthology using specific keywords such as ‘hallucination’, ‘NLP (OR) AI’ AND ‘hallucinations’, ‘fabrication’, and ‘confabulations’. We surveyed papers released on and before March 19th, 2024. From this search, a total of 164 papers were retrieved. After filtering out papers that were not directly related to hallucination research or those that merely mentioned the term without substantial focus on the topic, we arrived at a corpus of 103 papers. This corpus forms the basis for our audit and analysis of hallucination research, specifically within the NLP domain."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Conceptualization of Hallucination",
            "text": "We performed an iterative thematic analysis Vaismoradi et al. (2013) to uncover the various applications of hallucination research in NLP. To ensure accuracy and prevent misclassification, this recursive process was employed. This resulted in the identification of seven distinct fields that address research on hallucination (as shown in Table 1).\nThis taxonomy affords insights into the pervasive nature of hallucination within this domain of literature. Notably, it reveals that hallucination in NLP transcends beyond text generation, extending its conceptualization to encompass broader domains such as Image-Video Captioning, Data Augmentation, and Data-to-Text Generation tasks. This underscores the significance of hallucination both within and beyond the realm of NLG. Moreover, our classification framework provides us with a faceted analysis of how each of these tasks defines the concept of hallucination. Using thematic categorization, we come across definite attributes across the definitions of hallucination. One set of attributes elucidated how hallucinations are associated with the style/language generated by the model: Fluency, Plausibility, and Confidence. The next set of attributes falls under the effects of hallucinations: Intrinsic, Extrinsic, Unfaithfulness and Nonsensical. The definition of each of these attributes is elaborated in Table 2. In each paper analyzed within the survey scope, hallucination is defined based on a combination of the set of attributes identified. Our survey revealed 31 unique frameworks for conceptualizing hallucination, illustrating the diverse approaches and perspectives used. This diversity underscores the ambiguity in the term’s usage. To illustrate this phenomenon, we present some examples showcasing the diverse approaches commonly observed in the literature: Hallucination refers to the phenomenon where the model generates false information not supported by the input. - Xiao and Wang (2021a) Large Language Models often exhibit a tendency to produce exceedingly confident, yet erroneous, assertions commonly referred to as hallucinations. - Zhang et al. (2023a) Models generate plausible-sounding but unfaithful or nonsensical information called hallucinations - Ji et al. (2023c) Hence, within the domain of NLP, a notable deficiency persists in grasping coherent characteristics of hallucination. This shortfall underscores the risk of potential misappropriation of the term when employed in divergent contexts. We now analyze what aspects of the definitions of hallucination most commonly occur within each of our identified sub-fields of NLP. The breakdown of all the works associated with each of the subfields is in our Appendix. (Table 1). Conversational AI: In this sub-field, hallucination encompasses fluency, non-factuality, and both intrinsic and extrinsic hallucinations. The definitions’ facets highlight that dialogue systems must balance conversational fluency with factual consistency, aligning both with prior conversation and real-world truths. Abstractive Summarization: Works in this sub-field mainly focuses on extrinsic and intrinsic hallucinations in defining it. Some definitions also mention the faithfulness of the generation. Despite the challenges of aligning with real-world facts and source consistency, prioritizing alignment and adherence to the original material has been shown to be essential in these works. Data2Text Generation: Hallucinations are classified into extrinsic and intrinsic types, similar to abstractive summarization. Here, alignment with the underlying data is emphasized as the more critical factor when compared to the language used in generating the text. Machine Translation: Definitions of hallucination predominantly concentrate on extrinsic hallucination, with rare mentions of intrinsic hallucinations. This observation suggests a lesser concern for stylistic nuances in text generation within this field, with a greater emphasis on comprehending and conveying translated content accurately. Image and Video Captioning: Models are expected to maintain consistency with the source while also incorporating real-world knowledge to address gaps and apply common sense. Consequently, the definition of hallucination in this context encompasses intrinsic, extrinsic, and non-factual elements, highlighting these requirements. Data Augmentation: Works from this domain often omit explicit definitions of hallucination, indicating a divergence in emphasis or a nascent exploration of this construct within this sub-field. Miscellaneous: Encompassing tasks such as language inference and factuality detection, this category’s definitions of hallucination encompass aspects like factuality, intrinsic and extrinsic hallucination, fidelity, and nonsensicality. It’s evident that within these subfields, hallucination addresses both the stylistic aspects of model output and the fidelity and accuracy of generated content. From the analysis of different subfields, it is evident that each perceives hallucination differently, emphasizing specific attributes such as factuality, fidelity, or linguistic styles like confidence, while potentially overlooking others. This diversity indicates that hallucination as a concept is still in its early stages in the field, with various frameworks emerging and a general lack of consensus regarding its definition and application. Furthermore, the lack of social aspects in hallucination discussions in these subfields contrasts with the broader understanding and research in fields like healthcare."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Audit of Frameworks",
            "text": "The preceding section delved into examining hallucination within the various domains of NLP, exposing a need for coherence in the prevailing approaches within the field. Building upon this revelation, our analysis now pivots to underscore the pressing need for a unified understanding of hallucination, particularly in light of the pervasive use of language models in both public discourse and interdisciplinary research endeavors. We now scrutinize existing literature through a critical lens to unravel the dominant frameworks employed in defining hallucination while also assessing the extent to which these models accurately capture the essence of this phenomenon. This examination facilitates the outlook on transparency in the approach to hallucination in NLP, as well as the potential challenges it poses. We start by looking at how many of the selected works explicitly define hallucination. Out of the 103 papers under consideration, just 44 (42.7%) provide a definition of the term, leaving the majority—59 papers or 57.3%—either altogether omitting their understanding of hallucination in the context of their research or providing no definition of a framework for the same. This lack of transparency is not only concerning but also underscores the need for clarity, especially given the varied interpretations of hallucination across different research domains. Taking our scrutiny a step further, we investigate whether the works defining hallucination reference and acknowledge preexisting frameworks. It emerges that only 29 papers or 27% of the selected works explicitly acknowledge and adhere to established frameworks of hallucination, while the remainder 73% either loosely define the term or devise new definitions tailored to their specific research scope. This trend within the field underscores a lack of consensus on the conceptualization of hallucination, leading to disparate interpretations and a shortage of discourse on the subject. We also audit the sociotechnical nature of the definitions of hallucination in NLP. Hallucination, as elucidated in Section 3, is inherently imbued with social dimensions, manifesting varied perspectives across different social contexts. Recognizing this complexity, it becomes important to leverage and integrate these social nuances into the shared lexicon of hallucination within machine learning and NLP. Moreover, given the evolution of language models into sociotechnical systems, adopting a sociotechnical approach becomes necessary, given that the term ‘hallucination’ is inherently a shared vocabulary within these domains. Unfortunately, out of the 103 works examined, only 3 acknowledge the sociotechnical nature of hallucination, with none utilizing this framework to inform their approach. This oversight underscores a need for research to explore the sociotechnical dimensions inherent in hallucination, showcasing the current limited depth of understanding within the ML and AI communities."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Audit of Metrics",
            "text": "In the analysis of the 103 papers, we observed that 87 of these works dedicate efforts to measuring ‘hallucination’. This observation depicts the prevailing trend within NLP, emphasizing the significance of quantifying the concept of hallucination across diverse research endeavors. Building upon prior studies such as Ji et al. (2023a  ###reference_b44###), our analysis categorizes the common approaches in NLP for quantifying hallucination into four major themes: Human Evaluation, Data-driven Metrics, Statistical Metrics, and Mixed Methodologies.\n\nStatistical Metrics approach entails the analysis of lexical features to assess information overlap and contradictions between generated and reference texts Ji et al. (2023a  ###reference_b44###). This method calculates a hallucination score based on the degree of mismatch, with higher discrepancies indicating lower accuracy, factuality or faithfulness and hence, higher hallucination. Statistical scores such as BLUE, ROUGE, and Error Rate metrics are commonly used in this approach. Our findings reveal that 35.2% of the works that quantify hallucination opt for statistical metrics, employing 25 distinct metrics (e.g., BLEU, BERTScore, ROUGE1/2, F1, Perplexity, Cosine Similarity) developed for this purpose. This variability in measurement underscores the lack of a standardized approach within the research community for quantifying hallucination using statistical methods. Random sequences like HTTHHTT, THHTTHT, or HTHTHTH can provide analogous variations in approaches, offering a simplistic yet illustrative metaphor for the randomness in selecting metrics.\n\nData-driven Metrics utilizes curated datasets or neural models to gauge hallucination in generated text. This methodology, accounting for curated knowledge/content mismatches, is adopted by 26.1% of the works, resulting in the development of 18 distinct datasets or models tailored for hallucination measurement, such as HTTHHTT and SelfCheckGPT Manakul et al. (2023  ###reference_b69###).\n\nHuman Evaluation offers a complementary perspective by employing human annotators to assess hallucination levels, compensating for apparent errors in automated indicators Ji et al. (2023a  ###reference_b44###). This approach, used by 10.2% of the works, encompasses scoring and comparison methods, where annotators rate hallucination levels or compare output texts with baselines or ground-truth references. Notably, one outlier paper introduced an innovative approach utilizing eye tracking for hallucination detection in NLP tasks Maharaj et al. (2023  ###reference_b66###).\n\nMixed Method approach is deployed by 28.4% of the works, combining human evaluation with statistical metrics to offer a holistic perspective on hallucination quantification. This trend reflects a concerted effort within the research community to address the limitations of individual methodologies and provide insights into the presence and nature of hallucination in generated texts.\n\nThis metrics audit reveals significant knowledge gaps and challenges across various approaches. Notably, established research highlights areas for improvement in standard methods for measuring hallucination. For instance, methodologies like HTTHHTT and metrics such as ROUGE scores exhibit instability in measuring hallucination due to the need for complex human-crafted parsing rules for exact matching, rendering them susceptible to errors Li et al. (2023  ###reference_b59###). Criticisms also extend to human evaluation methods, which are prone to inaccuracies in gauging hallucination within these models Smith et al. (2022  ###reference_b101###).\n\nBeyond methodological criticisms, our survey uncovers a trend of employing numerous distinct metrics and approaches within these frameworks to categorize hallucinations. Over time, this has led to a diverse set of parameters for measuring hallucination, with a general lack of consensus on a standardized measurement approach. This issue is exacerbated by the multitude of diverse approaches to perceiving hallucination in NLP, further highlighting the absence of a unified method to address this challenge, especially as these models have now shifted to become a sociotechnical solution."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Practitioner Survey of Hallucination",
            "text": "In this section, adopting a community-centric approach Narayanan Venkit (2023  ###reference_b80###), we conduct a survey to gain insights into researchers’ perceptions on hallucinations in NLP. We aim to understand how frequently researchers utilize these models, how often they encounter hallucinations, and their firsthand experiences with model hallucinations.\nThe primary goal is to demonstrate how researchers and practitioners within the field perceive the concept of ‘hallucination’ and to expand our findings beyond the limitations of existing literature. This survey-based approach allows us to delve into the practical aspects of hallucination research and gather real-world perspectives from individuals actively engaged in NLP and AI research."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Survey Recruitment and Data Collection",
            "text": "For our survey, we employed a multi-faceted approach to reach a diverse population of respondents. We utilized direct emails, direct messages, and social media platforms such as LinkedIn and Twitter to distribute the survey. Our target audience included graduate students and professors from academic backgrounds as well as individuals from the industry who work in NLP, aiming to capture a wide range of perspectives on hallucinations.\nTo ensure a comprehensive view, we specifically targeted researchers familiar with AI and ML, primarily from disciplines such as computer science and information science. However, we also welcomed participants from other domains to explore their perceptions of whether they had the literature understanding of the concept of hallucination. The survey was examined and approved by the Institutional Review Board (IRB) for ethical practices.\nFor the survey, we employed a systematic approach by randomly selecting 15 universities from the top 100 in the USA as per the 2023 US News and World Report rankings News (2023  ###reference_b82###), to then reach out to potential participants from the field of NLP and AI, including faculty members and graduate students, to provide their insights through the survey.\nAdditionally, we expanded our outreach efforts by leveraging social media platforms to reach out to practitioners within the networks of all the authors involved in the survey.\nWe received a total of 221 responses, out of which 171 were complete and usable for analysis."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Survey Structure",
            "text": "The survey employed a combination of 12 open-ended and close-ended questions. For the analysis of open-ended questions, we utilized thematic analysis, drawing from established methodologies outlined in Blandford et al. (2016  ###reference_b6###); Terry et al. (2017  ###reference_b107###). The analysis involves identifying patterns, themes, and categories within the free-text responses provided by participants allowing us to uncover nuanced perspectives and key themes related to the research questions. The close-ended questions were analyzed using descriptive statistics to summarize and analyze the numerical data obtained from respondents. Throughout the analysis process, the research team made collective decisions regarding the retention, removal, or reorganization of themes derived from open-ended responses."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Survey Findings",
            "text": "Here, we summarize insights from our 171 survey responses to explore various perspectives on hallucinations in LLMs, including perceptions, weaknesses, and preferences. The breakdown of responses indicates that 76.54% of participants were from academia, 20.98% from the industry, and 2.47% selected both.\nParticipants were also asked about their research area’s direct relation to AI and NLP. The analysis revealed that more than 68.52% of researchers indicated that their work is directly related to AI, while the remaining respondents either exhibited familiarity with or indirectly incorporated NLP and AI methodologies in their work. This highlights the substantial involvement of AI experts and practitioners in addressing issues related to LLM model hallucinations."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Weaknesses of LLM",
            "text": "Before delving into inquiries about hallucinations in LLMs, it is crucial to gain insights into the perceived weaknesses of these models from the participants’ perspective, as well as understand how frequently they utilize these models in their work.\nThe survey results indicate that a significant portion of researchers heavily utilize LLMs in their daily life. Specifically, 67.28% of respondents reported using these models atleast once a day, while 20.37% mentioned using them all the time, highlighting the ubiquity of these models.\nUpon analyzing the themes derived from participants’ responses on the weaknesses of generative AI tools, it was observed that a substantial majority (55%) of researchers perceive the main weaknesses to be the generation of misinformation and hallucinations, despite both phenomena being essentially similar in nature. For instance,\nI have been exploring these models to see what they get right and wrong. They get a lot of things wrong – what some people call “hallucinations”.—Emeritus Professor, NLP\nSome of the other important weaknesses mentioned by the respondents are: biases, not following the prompts correctly, complex language, and not having a long memory. For example,\nThey produce a lot of inaccurate replies with great confidence. These models also tend to be very biased toward many socio-demographic groups.—Graduate student, GenerativeAI\nIt is hard to distinguish whether the information provided by them is accurate or not. Sometimes, the models generate text with reasoning making it sound convincing enough to be true - but ends up being incorrect ultimately.—Industry, GenerativeAI\nThe responses highlight a critical concern within the research community regarding the reliability and accuracy of outputs generated by LLMs, with potential implications for various applications and domains, providing us with a strong motivation behind this study.\nThe widespread use of LLMs, particularly prominent models such as GPT 3, 3.5, and 4, highlights their importance and impact on research and industry practices. However, it’s noteworthy that respondents also mentioned other LLM models that they use or are familiar with. These include Mistral, BERT, LLaMA2, Midjourney, ClaudeAI, Gemini, Vicuna, t5, Falcon, PaLM, Imagen, Dolly, Perplexity, among others."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Familiarity with Hallucination",
            "text": "The survey included the question on participants’ familiarity with the concept of ‘hallucinations’ in AI-generated text, measured on a 5-point Likert scale. The analysis revealed that 24.07% of researchers reported being extremely familiar with the concept, while 33.33% indicated being very familiar with it (Figure 3  ###reference_###).\nParticipants who indicated not being familiar with the term ‘hallucination’ (7.96%) also demonstrated implicit concerns with this phenomenon by highlighting issues such as generating incorrect responses, making mistakes, and crafting stories autonomously.\n###figure_3###"
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Frequency of Encountering Hallucination",
            "text": "The survey included a question regarding the frequency of encountering ‘hallucinated’ content, defined as content that is factually incorrect or unrelated to the input, assessed on a 5-point Likert scale ranging from ‘Never’ to ‘Very frequently’ (Figure 4  ###reference_###). The analysis revealed that 46.91% of respondents reported encountering hallucinated content occasionally, while 29.01% indicated experiencing it frequently. The results suggest that a substantial portion of researchers and practitioners encounter instances of hallucinated content in AI-generated outputs, indicating a prevalent issue in generative NLP models.\n###figure_4###"
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Perceptions of Hallucination",
            "text": "The survey findings revealed that more than 92% of respondents perceive hallucination as a weakness of LLMs. Subsequently, participants were asked to provide their own definitions of ‘hallucination’ in generative AI models through an open-ended question. To analyze these responses, we applied thematic categorization based on attributes generated from the literature audit (Table 2  ###reference_###).\nThe thematic categorization revealed that the majority of respondents categorized hallucination as pertaining to the factuality and faithfulness of input, with relatively lesser emphasis on the extrinsic and intrinsic nature of hallucination concerning the input. This trend reflects a common perception of how hallucination is understood within the context of larger-scope generative AI models.\nMoreover, the analysis identified 12 distinct frameworks regarding how hallucination is defined by respondents. For example:\nResponse that appears syntactically and semantically believable, but is not based on actual fact—Academic Researcher, NLP\nWhen the model confidently states something that is not true—Academic Researcher, AI\nThe diversity of viewpoints underscores the inconsistency within the field regarding the conceptualization and understanding of hallucination in the context of generative AI models."
        },
        {
            "section_id": "5.3.5",
            "parent_section_id": "5.3",
            "section_name": "5.3.5 Alternative Terms for Hallucination",
            "text": "The survey included a question asking participants if they prefer an alternate term to describe the phenomenon of ‘hallucination’ in AI-generated content and to provide an explanation if they do. The analysis revealed that 54.32% of respondents preferred the term hallucination or had no other term to provide. However, among the remaining responses, 40.46% of participants mentioned ‘Fabrication’ as a better term to describe the phenomenon.\nThis indicates that while the majority of respondents did not propose an alternative term, a notable proportion sees fabrications as a more suitable descriptor for the phenomenon of hallucination in AI-generated content. For example\nFabrication makes more sense. Hallucination makes it feel like AI is human and has the same sensory perceptions that could lead to hallucinations.—Academic Researcher, AI & Education\nIt’s interesting to note that a few researchers also prefer to use the term ‘Confabulations’ instead of ‘hallucinations’ when referring to AI-generated content. Their rationale likely stems from the nuanced difference in meaning between the two terms. While hallucinations generally convey the idea of perceiving something that is not based on reality or fact, confabulations specifically refer to the creation of false memories or information without the intention to deceive.\nBy opting for the term ‘Confabulations,’ researchers may be emphasizing the unintentional nature of the inaccuracies or false information generated by AI models, as opposed to implying deliberate deceit. For example,\nI think confabulation works better because it means creating a false memory without deceit. Fabrication gives the idea that it is intentional, which in the case of generative AI models, it is not.—Academic Researcher, AI & HCI\nIt’s also insightful to see that respondents proposed various alternative terms to describe the phenomenon of hallucination in AI-generated content such as incorrect information/misinformation, Non-factual information, Cognitive gap, hyper-generalization, Overconfidence, and Randomness. These alternatives highlight different aspects and nuances of the inaccuracies or distortions present in the generated content. Participants also mentioned how they prefer multiple terms based on the application in which they are used.\nAs I mentioned there are different types of hallucinations. For instruction and context hallucinations, I would refer to them as inconsistency instead. For factually incorrect hallucinations, the word hallucination is fine.—Academia, NLP"
        },
        {
            "section_id": "5.3.6",
            "parent_section_id": "5.3",
            "section_name": "5.3.6 Creativity and Positive Applications",
            "text": "Not all researchers view hallucinations in AI-generated content through a negative lens. While the majority may associate hallucinations with inaccuracies or distortions, a notable minority (10% in our survey) provided insights into how they believe hallucinations in these models can be correlated with creativity rather than negatively impacted behaviors. In fields such as story narration and image generation, researchers often value the creative behaviors exhibited by AI models. Hallucinations, when viewed in this context, may be seen as manifestations of the model’s ability to think outside the box, generate novel ideas, and explore unconventional possibilities. These creative outputs can inspire new approaches to storytelling, art, and problem-solving, contributing to innovation and artistic expression. For example:\nHallucinations are just what is needed for models to be creative. In truth, unless AI text-generators are factually grounded with external knowledge for a specific field, they are just story generators which aim to be creative, hence“hallucinate.\"—National Lab Researcher, NLP"
        },
        {
            "section_id": "5.3.7",
            "parent_section_id": "5.3",
            "section_name": "5.3.7 Social Ramifications of Hallucination",
            "text": "Participants were prompted to explain the effects of hallucination on their work/daily life.\nThe resulting themes, from our qualitative analysis of their inputs, are outlined below:\nNot Good for Education:\nRespondents raised concerns about the extensive use of these models by students for homework, indicating potential negative impacts on their performance and learning abilities. The respondents believe that such reliance on these models can lead to a degradation in students’ learning. Additionally, respondents express skepticism about the suitability of these models for checking homework assignments.\nI don’t actually use AI for my work; I just want to be aware of what it can do because my students are probably using it for their homework. It could have an impact on students’ mastery of the material.—Associate Prof, Biotechnology\nNot Good for Scholarly Work:\nSeveral respondents noted that these models are not effective for scholarly purposes, citing instances where the models generated information that was not present in the original paper. They express concerns that if researchers rely on these models for tasks like literature summarizing, it could lead to a deterioration in scholarly processes. For example:\nThey tend to generate a lot of misinformed facts about certain groups or cultures that I have seen happen often. They also generate ’facts’ from scholarly works where the papers would not have mentioned the same.—Graduate student, NLP\nStruggle with Code Generation:\nThe models were deemed inefficient for code generation by multiple respondents, often producing code that lacks utility due to hallucinations. Respondents highlighted mismatches between the generated code and its intended purpose, emphasizing the need for thorough review before utilization. Various concerns were raised, including the loss of context during prolonged interactions, inaccuracies in complex coding tasks leading to erroneous outputs, fabrication of functions or attributes, inaccuracies in both code and associated theoretical concepts, necessitating extensive debugging and corrections, and a tendency to cycle back to previously incorrect suggestions despite error notifications.\nI was asking an AI to generate me a piece of code. It ended up picking some code from one website and some from another and combining it. However those two websites (they were linked by chatgpt) we’re using different versions of the library so the resulting code couldn’t be executed.—Industry, Network and Security\nIncrease in Time for Task:\nA common sentiment among respondents is that these models frequently produce errors or false information, resulting in potential time wastage. While they acknowledge occasional helpfulness, there’s a consensus that reliance on these models can often lead to unfavorable outcomes, particularly when verifying outputs. This dependency on verification contributes to increased task duration, adding extra work and time toward the project’s conclusion, as noted by several respondents.\nI use GPT API to conduct analysis for some of my work and accuracy and consistency would be good in my context, and I have to find ways to finetune it before I can trust the results of the analysis, which added more work on my end.—Graduate Student, HCI\nMisleading and Distrust:\nGenerating incorrect outputs with confidence can lead to the dissemination of non-existent knowledge, such as misleading information in the literature that may confuse individuals with incorrect concepts. Most of our respondents mentioned this concern. Moreover, it poses challenges in differentiating between accurate AI responses and hallucinations, particularly for users lacking expertise in the relevant subject matter.\nIt leads to problems if even I do not have any idea about the work. It is hard to differentiate if it is a genuine output or hallucination.—Graduate Student, Data Science"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "An External Viewpoint",
            "text": "Additionally, our survey of 51 researchers who do not specialize in AI revealed that all except 3 have used text-generation models like various versions of ChatGPT. Despite their fields not being directly related to AI, a significant number integrate these tools into their workflow, with 19.6% using them multiple times daily and 11.76% using them several times per hour. Their extensive usage has allowed them to identify several limitations in the models; they are: mathematical inaccuracy, outdated information, misinformation, poor performance with complex tasks and creative thinking, lack of specificity in-depth, overconfidence, lack of transparency, bias, and irrelevant responses.\nBased on the definitions provided, it is observed that there is a lack of clarity among the respondents regarding what constitutes a ‘hallucination’ in generative AI models, with perspectives varying widely. Thematic analysis of their responses indicates that the predominant view associates ’hallucination’ with the generation of nonfactual content and misinformation by AI systems. That means these models are generating facts that are not real and misleading. The remaining themes are factually incorrect, biased outputs, incompleteness, misinformation with confidence, and nonsensical but good-looking texts.\nThe results demonstrate the unclear comprehension and significance attributed to hallucination in language models beyond the field of NLP and AI. There is a pressing need to enhance public understanding of the concept of hallucination, emphasizing its meaning and strategies for mitigation. Given the increasing prominence of language models as sociotechnical systems Narayanan Venkit (2023  ###reference_b80###), it is crucial to grasp their social interactions and potential societal ramifications."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Additional Impacts and Concerns",
            "text": "We analyzed perceptions when participants were asked about any additional concerns during the survey. Participants emphasized the necessity for greater control and more nuanced mechanisms to address and manage AI hallucinations effectively. Presently, the detection and rectification of hallucinations rely heavily on meticulous human review, highlighting the need for tools designed specifically to identify and mitigate such occurrences. The presence of hallucinations can significantly impact the credibility and acceptance of generative models among the general public. These issues arise due to the inherent limitations of generative algorithms and the absence of access to real-time external knowledge.\nTransparency regarding the limitations of generative AI is deemed essential through our findings, and user education is seen as a key factor in mitigating risks associated with the unchecked use of AI-generated content, as the responsibility for identifying hallucinations often falls on the user. While inaccuracies in non-critical applications, like movie suggestions, may be tolerable, according to our survey, they are deemed crucially problematic in contexts such as business decision-making, law, or health Dahl et al. (2024  ###reference_b17###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Research Challenges and Recommendations",
            "text": "Finally, based on our audit and survey analysis, we outline the key weaknesses encountered in hallucination within NLP and potential recommendations motivated by the weaknesses. Our focus is to utilize a community-centric and literature-based approach to define the primary weaknesses of the field currently and a path forward."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Challenges",
            "text": "Wide range of vague and absent definitions: The literature and the practitioner’s survey show diverse and conflicting definitions and frameworks, often lacking clarity or omitting explicit definitions for hallucination and how it is perceived in various fields of NLP.\nLack of standardization in measurement: The absence of standardized metrics to quantify hallucination results in the use of multiple scales and categorizations. This lack of standardization makes it challenging to compare and interpret results across different models and studies, leading to a proliferation of diverse approaches for evaluating hallucinations.\nLimited awareness of hallucination in a sociotechnical context: Hallucination research often lacks the understanding of how the concept of hallucination is conceptualized beyond its technical purview (discussed in Section 4). When such analysis is employed in sociotechnical systems like healthcare and policy making Dahl et al. (2024  ###reference_b17###); Pal et al. (2023  ###reference_b84###), it is important to define the socially relevant framework of hallucination as well. There is no motivation shown to understand the social, political, and psychological considerations of hallucination in these works.\nLack of consensus between various subfields There are multiple frameworks of hallucination created across various subfields in NLP. For instance, hallucination in image captioning exhibits distinct characteristics compared to abstract summarization and conversational AI. These frameworks have been adopted based on individual usage without reaching an accord among other existing works.\nMultiple sentiment towards Hallucination\nThe perception of hallucination in generative AI varies depending on the context. For instance, it is often positively regarded as creativity in image generation, whereas in text generation, it is viewed negatively as errors or mistakes. Consequently, future research efforts should aim to better address this disparity to develop a more nuanced framework for understanding hallucination.\nLack of Standardized Nomenclature: Both our literature audit and practitioner survey revealed that the term ‘hallucination’ is inadequate to fully capture the behavior exhibited by NLG models. There is a need for further investigation into which terms are more appropriate and why they are necessary. For instance, terms like ‘confabulation,’ ‘fabrications,’ and ‘misinformations’ are increasingly being used to describe the same phenomenon. A more precise understanding is required to distinguish between these terms and how they are utilized in various fields within NLP.\nEthical and Social Challenges: Given that researchers and users often rely on these models for critical decision-making processes, there is a need for well-defined policies regarding the extent of their utilization and the transparency surrounding their use and the impact of hallucinations within such models. Currently, such policies are lacking in industries and academic settings.\nAddressing these issues requires careful consideration of the categorization approach, integration of contextual information, and, efforts towards robust evaluation methodologies in hallucinations."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Recommendations",
            "text": "Prior works like Blodgett et al. (2020  ###reference_b7###) & Venkit et al. (2023  ###reference_b112###) have formulated thematic recommendations to address AI issues. Expanding on this, we examine strategies for NLP practitioners studying ‘hallucination’ to overcome these challenges. We propose two overarching themes with four associated recommendations."
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Author-Centric Recommendation",
            "text": "These recommendations prioritize actionable steps for both the author and developers, emphasizing transparent and accountable development in conceptualizing hallucinations.\n[R1] Ensure explicit documentation of the hallucination framework and analysis methodology employed during the development of NLP models. Provide guidelines that outline the expected measurements and quantifications for the model to enhance interpretability and applicability.\n[R2] Explicitly state the use cases and user profiles intended to interact with the NLP system. By considering the specific applications and targeted users, it is easier to construct the required framework of hallucination that is sensitive to the community in consideration. Raise awareness about potential ramifications introduced by NLP models, emphasizing the importance of fairness and equity."
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Community-Centric Recommendation",
            "text": "These recommendations prioritize actionable steps for the research community to enhance frameworks and understanding related to hallucinations.\n[R3] Develop clear and standardized definitions for terms such as ‘confabulations,’ ‘fabrications,’ ‘misinformation,’ and ‘hallucinations’ within the context of NLP. Establish frameworks that provide clarity and consistency in understanding these concepts, particularly regarding hallucinations. This requirement is crucial due to the widespread misunderstanding of hallucination and the misnomers that have arisen as research progresses.\n[R4] Promote the creation of methods that offer visibility into the model’s decision-making process, enabling users to comprehend how hallucinations or fabrications can occur within the system, thus fostering trust in its use. Facilitating research discussions for transparency through workshops and conferences is one approach to achieving this goal."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Our work delves into the conceptualization of hallucination within the scope of NLP. Our approach involved two key methodologies: first, an exhaustive audit of 103 peer-reviewed papers in the NLP domain, and second, a practitioner survey of 171 researchers and academics to gauge the field’s perception and understanding of hallucination. Through this analysis, we have gained insights into how the NLP community conceptualizes and defines hallucination. Additionally, our thematic and community-based approach highlights potential weaknesses within the field, particularly in addressing misrepresentations and inaccurate characterizations associated with hallucination. Our work finally contributes to a deeper understanding of the challenges and gaps in research related to hallucination within NLP, paving the way for future advancements in this area."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our study encompasses a selection of 103 papers, incorporating works from primarily the ACL Anthology. While our intention was not to provide an exhaustive collection of all published works on hallucination, we aimed to include diverse sources within NLP that cover various aspects of the field. Our intent was to curate peer-reviewed literature commonly found in the NLP domain, encompassing models, applications, survey papers, and frameworks.\nWe, therefore, did not scope the utility of hallucination and its impact beyond NLP to other fields of research, such as Computer Vision. Regarding the creation of the challenges and recommendations, it is important to note that the themes presented are not meant to be exhaustive but rather serve as a foundational framework to spark additional inquiries and foster further engagement.\nOur survey was designed to capture the viewpoints of researchers and practitioners in the AI and ML field, potentially limiting various experiences. As such, our analysis is centered on this perspective. While we did gather additional insights from participants outside this field, our focus was not comprehensive in that regard. Our future work intends to explore the public’s perspective on hallucination."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We are aware of the ethical considerations involved in our research and have taken measures to ensure responsible practices throughout the study.\nData Publication: All the papers used in our research are listed in the Appendix. However, we recognize the importance of transparency and accountability. Therefore, we publish the complete collection of papers along with our qualitative classification and annotation, allowing for public scrutiny and examination 333github.com/PranavNV/The-Thing-Called-Hallucination.\nMitigating Qualitative Bias: We acknowledge the potential for bias when performing qualitative coding of the papers regarding their applications. To address this concern, we ensured that at least three different individuals independently reviewed and verified the coding to minimize the possibility of misclassification. Additionally, we followed the same approach to verify the presence of various definitions in each paper, enhancing the reliability and validity of our analysis.\nBy disclosing these ethical considerations, we emphasize our commitment to conducting research in an ethical and accountable manner."
        }
    ]
}