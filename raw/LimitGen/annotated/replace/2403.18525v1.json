{
    "title": "Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP",
    "abstract": "Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called CIFAR-10, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the scale and diversity of training data and language supervision play a key role in unlocking the compositional generalization abilities of vision-language models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large pre-trained models has significantly advanced the field of machine learning. Innovations such as GPT-3 [1  ###reference_b1###], Chinchilla [2  ###reference_b2###], PaLM [3  ###reference_b3###], and CLIP [4  ###reference_b4###] have broadened the horizons of generalization and underscored their exceptional capacity for zero-shot inference. The Out-of-Distribution (OoD) generalization of models like CLIP has been explored, revealing two differing perspectives on its origin: one attributing it to dataset diversity[5  ###reference_b5###], the other to language supervision[6  ###reference_b6###].\nMost of the previous work studied the CLIP generalization under a certain type of out-of-distribution data, namely, distribution shifts [7  ###reference_b7###, 8  ###reference_b8###, 5  ###reference_b5###]. However, there are other types of OoD generalization, including spurious correlation [9  ###reference_b9###], and compositional generalization [10  ###reference_b10###]. One has to note that each of these OoD generalization categories has a unique nature that should be studied separately.\nThis paper focuses on the compositional generalization, the ability of models to generalize new combinations of known concepts. Despite some shortcomings, it has been shown that Vision-Language Models (VLMs) can compose concepts in the single-object setting[11  ###reference_b11###]. We explore if the compositional nature of VLMs impacts their compositional OoD generalization, hypothesizing that joint vision-language representation learning has enhanced CLIP’s decomposability between objects and attributes in images containing single objects.\nA significant challenge in evaluating OoD generalization is the unknown training distribution, as seen in models like CLIP where the training dataset has not been released. A novel benchmark design is proposed to assess CLIP models, involving a new compositional OoD dataset of unconventional attribute-object pairs distinct from the CLIP’s training data called CIFAR-10. We evaluate various CLIP models on this CIFAR-10 dataset to determine their performance and analyze contributing factors to the performance, offering insights into enhancing CLIP’s generalization abilities. Our contributions include crafting an unseen attribute-object pair image test dataset called CIFAR-10, providing a controlled benchmarking setting for various CLIP models using CIFAR-10, and identifying the importance of compositional diversity in training captions for CLIP to demonstrate decomposable representation and basic compositional generalization."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Robustness to Natural Distribution Shift",
            "text": "In certain applications, the test samples may exhibit different styles, colors, or contrasts compared to the training data. OoD generalization under such distribution shifts have extensively been studied, and it has been argued that training on diverse datasets is the most effective factor in increasing the robustness [12  ###reference_b12###, 8  ###reference_b8###], while combining various data modalities did not enhance the performance [5  ###reference_b5###]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Compositional Generalization of CLIP",
            "text": "Compositional generalization, generalizing to unfamiliar compositions of familiar elements, poses challenges for models like CLIP. This includes associating attributes with objects, understanding object relationships, and extrapolating to unfamiliar concept combinations. Possible solutions to this problem include utilization of image scene graphs and augmentation framework for contrastive learning [13  ###reference_b13###], leveraging LLMs to generate sentence-level descriptions for each compositional class [14  ###reference_b14###], and fine-tuning the vocabulary for attributes and objects on seen classes, then recomposing the learned vocabulary in new combinations for the novel classes [15  ###reference_b15###]. The emergence of concept representations within CLIP was studied in [16  ###reference_b16###]. In [17  ###reference_b17###], the authors examine VLMs struggles with relation, attribution, and order understanding. They propose a novel training procedure to improve these aspects.\nThis work differs from the mentioned studies by investigating and comparing the power of CLIP’s compositional generalization in a single-object setting, including attribute-object compositions, and creating a dataset with combinations of objects and unusual attributes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "CLIP Object-Attribute Compositional Generalization",
            "text": "Compositional OoD generalization refers to a model’s ability to handle novel combinations of familiar concepts. This is critical in contexts like attribute-object images, where the goal is perceiving new compositions of objects and attributes.\nDecomposable image representations that assign separate embedding dimensions to objects and attributes facilitate this generalization. Such representation makes meaningful construction of known concepts in the embedding space feasible.\nWe hypothesize that large and diverse datasets reduce the dependency between attributes and objects, promoting a more decomposable understanding of images.\nBased on these insights, we posit that decomposability is the key to the CLIP model’s unseen composition generalization. This claim is supported by two main arguments:\nLarge and diverse datasets reduce entanglement between object and attribute tokens. In other words, they help to promote a more decomposable text representation (see sec. 5.2  ###reference_###).\nText representation decomposability is induced in the image encoding, due to implicit maximization of the mutual information. We elaborate on this claim in what comes next.\nWhy decomposability may arise in contrastive learning?\nCLIP training maximizes the mutual information between text and image encodings. We claim that decomposability in the text representation, induces decomposability in the image encoding. To see this, let , and  be the text embeddings for the objects and attributes, respectively. Let , and  be the corresponding image embeddings. Assuming a decomposable text embedding means , i.e. . Now by minimizing the contrastive loss, the mutual information  is maximized. By letting , and , we have:\nThe latter term makes  and  dependent random variables, otherwise if , the expected KL divergence would be minimum (or zero), which is against maximizing the mutual information.\nNote that however,  does not ideally depend on both  and , otherwise the two distributions in the KL divergence in the first term become similar, which is also against maximizing the mutual information. Putting these together,  mostly depends on  if the mutual information is maximized. Using a symmetric argument,  mostly depends on . Finally, because , we conclude that  and  tend to become independent. Therefore, maximizing  decomposes  if  is already decomposed.\n###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "ImageNet-AO: Dataset Desgin",
            "text": "To effectively assess the compositional generalization capabilities of models, we created a unique dataset of rare compositions, ensuring these were not present in the models’ training data. This dataset was produced by creating compositional images via a text-to-image model, using an Attribute+Object template. Our process is as follows:\n\nSelecting objects or nouns:\nWe extracted class names from the CIFAR-10 dataset, using these as objects (or nouns) in our structure to create a link between the generated images and CIFAR-10 classes. This allows for comparison of model performances on the familiar CIFAR-10 validation set. We aimed for a diverse set of class names to enhance the complexity of the generated images.\n\nSelecting attributes or adjectives:\nThe next step involved choosing 30 distinct adjectives that were relevant and could create unique combinations with the selected nouns, enhancing the diversity of our compositional images.\n\nSelecting unseen (attribute, object) pairs:\nWe combined the 30 adjectives with a pool of 10 nouns, resulting in 300 distinct pairs. These were given to the text-to-image model to generate corresponding images. To ensure these combinations were not present in the CLIP training set, we conducted a thorough search and removed any that were found.\n\nGenerating images for (attribute, object) pairs:\nThe selected combinations were given to a text-to-image model for the image generation. Among various models, the Microsoft model powered by DALL-E proved to be the most powerful. However, it had limitations and some prompts were blocked for unknown reasons.\n\nValidating the generated images:\nLastly, human supervision was used to validate the generated images, with images not closely aligning with their prompts removed. After this process, around 120 combinations remained, for which we successfully generated around 5000 accurate, high-quality images. An illustrative example of the diversified dataset generated through this process can be observed in Figure 1 ###reference_###. This figure showcases a selection of images that exhibit various degrees of alignment with their corresponding prompts, highlighting the effectiveness of the validation procedure."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we examine the effects of language supervision on compositional Out-of-Distribution (OoD) performance. We explore links between the training dataset characteristics, and CLIP OoD generalization. Specifically, we assess our hypothesis regarding the role of the training data quality and quantity in disentangling the object and attributes, and its consequences in compositional OoD generalization. In a nutshell, we found that CLIPs whose training sets consist of more diverse caption compositions would exhibit this property more than other CLIP models."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "CLIP Models Comparison",
            "text": "We assessed CLIP model performance in zero-shot classification tasks using an evaluation method similar to that of [18 ###reference_b18###] and [4 ###reference_b4###] on CIFAR-10 dataset. We provided the model with images and captions, then calculated their cosine similarities to estimate the caption relevance to the image content. The models trained on the LAION 400m, LAION 2B, and DataComp 12.8B datasets showed similar performances on CIFAR-10 compared to the model trained on the OpenAI dataset. This indicates the potential efficacy of these datasets in training CLIP models for specific evaluated composition types. While larger training datasets typically resulted in enhanced accuracy, the CLIP model trained on YFCC15m displayed lower performance than the CC12m model, despite the former’s larger dataset size. Additionally, experiments showed that models trained on Commonpool data filtered by LAION or CLIP scores outperformed the model trained on the full unfiltered Commonpool set, although the latter contained more data. This implies that various other factors can play a role in influencing the model behavior. To be more precise, the subsequent subsection discusses one of these factors that can significantly impact the model performance. To visualize the comparative performance of these CLIP models trained on different datasets, refer to Figure 2 ###reference_###a."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Attribute-Object Tokens Mutual Information",
            "text": "We hypothesize that the use of datasets containing diverse, creative, and imaginary samples with less dependency between object and attribute during training is critical for enabling models to learn decomposable representations. To evaluate the degree of decomposability in the CLIP training data, we conducted an analysis by measuring the normalized mutual information (NMI) between object class and attributes, whose domains are defined based on the captions in CIFAR-10. The NMI is calculated based on the datasets on which CLIP was trained, enabling us to gauge the level of decomposability present in the training data. A lower NMI value indicates better disentanglement of attributes and objects. The findings are depicted in Figure 3, which demonstrates that the LAION 400m dataset exhibits lower NMI values compared to the CC12m dataset. Similarly, the CC12m dataset displays lower NMI values compared to the YFCC15m dataset. These observations are aligned with the outcomes of our previous experiments on compositional OoD generalization.\n\nWhen the mutual information between variables in a dataset is reduced, it indicates a diminished statistical dependence among those variables. In the context of decomposability, this implies that the factors of variation within the dataset are less entangled or intermingled. Additionally, the low values of NMI emphasize the diversity in the textual components of the dataset. This diversity is a crucial aspect for CLIP to attain high performance in effectively handling the OoD scenarios."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Comparison with Supervised Models",
            "text": "In this experiment, we investigated the impact of language supervision on CLIP models compared to supervised models under compositional OoD settings. We did not intend a direct comparison, but rather to explore if CLIP’s language supervision improves the OoD accuracy. We assumed the object names as the class labels and evaluated the supervised models’ accuracy on CIFAR-10. For CLIP, we generated captions using only object names, removing adjectives, to align the evaluations. Figure 2  ###reference_###b shows CLIP models trained on OpenAI, LAION, and DataComp datasets consistently outperform supervised models on the OoD accuracy. This suggests that language supervision during CLIP training positively impacts the model representation decomposability, enabling generalization to detect unseen compositions."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study examines the generalization of CLIPs to new object and attribute compositions. We created a benchmark dataset of compositional images and found that CLIPs training data quality is crucial for the compositional generalization. We showed that models trained on more diverse caption compositions perform better, and language supervision during training improves OoD generalization. The study highlights the importance of dataset diversity and decomposability in enhancing vision-language models’ compositional generalization capabilities."
        }
    ]
}