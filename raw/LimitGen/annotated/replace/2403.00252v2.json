{
    "title": "Europa: A Legal Multilingual Keyphrase Generation Dataset",
    "abstract": "Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present COCO, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Keyphrases are short phrases that describe a text, and have been shown to be useful for many applications, from document indexing (Medelyan and Witten, 2006  ###reference_b49###) to opinion mining (Berend, 2011  ###reference_b9###). While heavily researched for the STEM (science, technology, engineering, and mathematics) domains, there has been little investigation on its application to law. This is surprising, as keywords can reduce the workload of legal experts by allowing them to get the gist of lengthy documents Mandal et al. (2017  ###reference_b47###); Sakiyama et al. (2023  ###reference_b62###); Cérat et al. (2023  ###reference_b22###). The scope of this work is to assess to what extent keyphrases can be automatically generated in the legal domain. Our contributions are the following: (1) We collected and curated COCO, the first open benchmark for legal KPG (keyphrase generation), spanning 24 languages and extracted from real-world European judgments.111The dataset can be downloaded from the Hugging Face hub at https://huggingface.co/datasets/NCube/coco while evaluation scripts and model outputs are available at https://github.com/rali-udem/coco. (2) We provide in-depth analysis of our corpus, highlighting its differences compared to other existing corpora. (3) We report performances achieved by multilingual generative models on this benchmark and point out areas where performances could be improved."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "While English KPG has been studied fairly extensively in technical and academic domains, our work bridges two less common fields: legal KPG and multilingual KPG. In this section, we first review the literature surrounding keyphrase extraction (KPE) and generation before covering legal KPE and KPG. For a more complete survey on KPG, we refer to Xie et al. (2023 ###reference_b76###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Keyphrase Extraction and Generation",
            "text": "Many extractive methods have been suggested and used over the years, either using unsupervised heuristic rules and pre-trained machine learning (ML) models to detect and extract keyphrases (Liu et al., 2010 ###reference_b44###; Gollapalli and Caragea, 2014 ###reference_b31###; Meng et al., 2017 ###reference_b53###; Yu and Ng, 2018 ###reference_b79###) or using ML/deep learning (DL) classifiers trained on labeled data to learn to identify them (Yang et al., 2018 ###reference_b78###; Chen et al., 2018 ###reference_b17###; Wang et al., 2018 ###reference_b72###; Basaldella et al., 2018 ###reference_b5###; Alzaidy et al., 2019 ###reference_b2###; Sun et al., 2019 ###reference_b69###; Mu et al., 2020 ###reference_b54###; Sahrawat et al., 2020 ###reference_b61###). Extractive methods are however incomplete as, for most domains, a large portion of the target keyphrases are absent from the document, hence an increased interest in generative models in more recent works. While research on KPG initially used seq-to-seq models in the form of RNNs Meng et al. (2017 ###reference_b53###, 2019 ###reference_b51###, 2021 ###reference_b52###); Yuan et al. (2020a ###reference_b80###), modern KPG uses mostly fine-tuning of pre-trained transformer models (Vaswani et al., 2017 ###reference_b71###), such as BART (Lewis et al., 2020 ###reference_b42###), which showed itself to be as efficient as previous, more complex RNN models Chowdhury et al. (2022 ###reference_b18###). Current English state-of-the-art results are currently obtained by using BART models which are further pre-trained for scientific KPG (KeyBART by Kulkarni et al. (2022 ###reference_b39###) and SciBART by Wu et al. (2022 ###reference_b74###)). KPG has been conducted on limited domains, such as academic and STEM papers (Hulth, 2003a ###reference_b34###; Nguyen and Kan, 2007 ###reference_b56###; Kim et al., 2010 ###reference_b36###; Meng et al., 2017 ###reference_b53###; Krapivin et al., 2009 ###reference_b38###) or news Gallina et al. (2019 ###reference_b28###); Koto et al. (2022 ###reference_b37###), although the need for KPG extends beyond those domains. Of particular interest is KPG for long documents, which has benefited from a few studies, especially in recent years. Ahmad et al. (2021 ###reference_b1###) proposed a two-step approach of this problem, where they first select salient sentences before using those to generate keyphrases, while Garg et al. (2022 ###reference_b29###) tested multiple ways of including information from the main body (generated summary, citation sentences, random sentences, etc), showing that adding a generated summary was the most efficient strategy. Mahata et al. (2022 ###reference_b46###) proposed a corpus for scientific KPG using the whole document instead of only the title and abstract, which is what is currently used in KPG. They do not however run any baseline on that corpus."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Keyphrase Generation in the Context of Legal Domain",
            "text": "Legal documents are more complex and longer than those commonly used in NLP, as illustrated by the European Court of Human Rights corpus (Chalkidis et al., 2019). This motivates a high demand from legal professionals for automatic digests of such documents (Bendahman et al., 2023), mostly from the angle of legal summarization. This task, more widespread than legal KPG, was first formalized as an extractive task in which the most salient segments from the document are returned as a summary (Farzindar and Lapalme, 2004a, b; Saravanan and Ravindran, 2010; Polsley et al., 2016; Aumiller et al., 2022). Similarly to KPG tasks, abstractive summarization models based on transformer architecture (Vaswani et al., 2017) became more widespread than extractive ones. However, open legal summarization benchmarks such as BigPatent (Sharma et al., 2019), Multi-LexSum (Shen et al., 2022), and EUR-Lex-Sum (Aumiller et al., 2022), are still scarce. Legal KPG may be considered as a specialized type of summarization but, to the best of our knowledge, no open benchmark is available for such a task. Moreover, most existing works have been focusing on keyphrase extraction (Le et al., 2013; Audich et al., 2016; Mandal et al., 2017; Daojian et al., 2019), ignoring absent keyphrases. The very first legal KPG experiments addressing abstractive keyphrases were conducted by Cérat et al. (2023) and Sakiyama et al. (2023), but within a monolingual setting and with no public dataset release. This is a major issue as generative models are particularly data-greedy. Furthermore, language-wise, most of the open legal datasets are in English (Chalkidis et al., 2023) and, when it comes to existing open multilingual legal benchmarks Chalkidis et al. (2021); Savelka et al. (2021); Aumiller et al. (2022); Niklaus et al. (2023), none are related to KPG. Overall, our contribution aims at fulfilling these gaps by providing an open multilingual dataset for legal keyphrase generation."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Creation of the Europa Dataset",
            "text": "Cases in the Court of Justice of the European Union (CJEU), which aims at ensuring the consistent interpretation and application of the EU law across all EU institutions (Court of Justice of the European Union, 2023  ###reference_b21###), can be processed in any of the 24 EU official languages, depending on the Member State involved, making drafting and translation pivotal and complex tasks. Stakeholders and judges communicate in their respective languages and rely on lawyer-linguists for document exchange. Once a judgment is rendered, it is translated into other EU languages to ensure consistency in the judgment text, keyphrases, and their legal implications (Domingues, 2017  ###reference_b24###). Through private correspondence, the CJEU explained that keyphrases, drafted by the Registry and completed by the reporting judge’s cabinet or the advocate general, aim at providing concise case descriptions. Their meticulous construction establishes them as high-quality gold references for KPG evaluation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Collection & Processing",
            "text": "For the sake of clarity, we define a judgment as a collection of documents which refer to the same ruling provided in up to 24 languages. Each judgment has a unique CELEX identifier that remains the same for every language version available. Therefore, all language versions with the same CELEX ID are semantically and legally equivalent and can be considered as parallel. Within a judgment, each language version is named an instance, a pair comprising the target keyphrases and the input text from the judgment, with both being in the same language. For example, the judgment shown in Table 1 has 22 instances spanning 22 languages.\n\nIn May 2023, we performed a first pass on the EUR-Lex database, the main legal EU online database (Bernet and Berteloot, 2006), scraping 304,426 query results corresponding to 19,319 judgments released by the CJEU. These query results (each being a potential instance for our corpus) consist of small snippets containing the multi-line title and meta information of the case including the document identifier, but not the judgment text. Then, we made a second pass in order to scrape the plain text of the judgment using the document identifier collected during the first pass. The plain text is available as PDF and/or HTML, but we used the latter for convenience. A total of approximately two weeks was required for collecting the query results snippets and the judgments’ HTML files in all 24 languages. An overview of the corpus collection and curation process is presented in the following paragraphs (more details in Appendix E).\n\nThe structure of a case HTML file generally consists of a mix of keyphrases and meta-information at the top of the document followed by paragraphs that will be merged into the input text. Separating the top of the document from the paragraphs is crucial in order to ensure that the input text is not contaminated by target keyphrases. However, identifying keyphrases from the judgment plain text was infeasible as they are surrounded by quotation marks, symbols, and HTML tags that vary across languages and time. Therefore, the keyphrases were obtained from the small snippet multi-line title.\n\nStill, in such snippets, the keyphrases are mixed with meta information about the case which we need to get rid of as they are redundant and of low value from a KPG standpoint (e.g. sequences such as “Reference for a preliminary ruling” followed by the referring court and the country are so frequent that they would bring noise during training). Using the BeautifulSoup library, language-specific regular expressions, and domain-aware engineered heuristics, we filtered out the sequence of keyphrases that had the highest number of meaningful phrases. An additional sanity check was also performed to ensure that keyphrases are properly split (e.g. missing whitespace beside a phrase delimiter) and that the number of phrases remains consistent across languages for each judgment. Our approach thus ensures the quality of the target keyphrases that will be used in the KPG task during training and evaluation. For the sake of evaluation consistency with the existing KPG literature, keyphrases are lowercased and separated by semicolons.\n\nAnother critical part of corpus preparation is the input text from the judgment HTML files. These raw documents are delicate to process as they begin with a mix of meta-information (e.g. stakeholders identities) and target keyphrases (which must be excluded from input text), followed by numbered paragraphs. Moreover, since the HTML tags are inconsistent and vary across years and languages, BeautifulSoup cannot be used to extract the input text from the judgment. Therefore, we manually designed several language-specific regular expressions to reliably split the documents by matching delimiters in all languages (e.g. “Judgment”, “Sentencia”, “Urteil”). By doing so, only the paragraphs of the judgment are retained as a single input string, while the section containing superficial meta-information and target keyphrases is taken away, thus preventing any data leakage. This was confirmed by a manual examination of 100 random instances, equally distributed across the 24 languages.\n\nAfter removing instances with empty input or target texts, our final corpus is composed of 17,833 judgments, in 16 languages on average and spanning cases from 1957 to 2023. This amounts to a total of 284,957 instances (input/keyphrases pairs). Expectedly, these instances are unevenly distributed across all 24 languages, with languages from older EU Member States being more represented in the dataset. For instance, French (the most represented language) amounts to 17,461 instances (6.13% of all instances) while Croatian and Irish (a significant outlier) contain 5,153 (1.81%) and 92 (0.03%) instances, respectively."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Chronological Split",
            "text": "###table_2### It is a common practice to randomly split data in a NLP task Gorman and Bedrick (2019  ###reference_b32###). However, training and evaluating a model on data from overlapping time periods with similar distributions fails to assess the actual model’s temporal generalization (Lazaridou et al., 2021  ###reference_b40###).\nThis is why legal NLP generally uses a chronological split of documents instead of combining random shuffle with random split (Chalkidis et al., 2019  ###reference_b13###; Medvedeva et al., 2021  ###reference_b50###; Chalkidis et al., 2021  ###reference_b14###).\nIn our case, we tried both splits with a mBART50 model.\nOn the test set, the performance in terms of for present keyphrases differs by 14.1 percentage points (11.1 for absent keyphrases) in favour of random split.\nResults in Table 2  ###reference_### confirm that random split, by ignoring real-world temporal concept drifts, tends to overestimate true performance.\nThis is consistent with Søgaard et al. (2021  ###reference_b68###); Mu et al. (2023  ###reference_b55###).\nTherefore, we choose a chronological split for a proper assessment: The training set covers judgments from 1957 to 2010 (131 076 instances), the validation those from 2011 to 2015 (63 373 instances), and the test set the ones from 2016 to 2023 (90 508 instances).444The temporal split is available at https://huggingface.co/datasets/NCube/coco  ###reference_pa### and a random split version for those who wish it can be found at https://huggingface.co/datasets/NCube/coco-random-split  ###reference_pa-random-split###\nFull details about documents distribution across these splits are shown in Appendix F  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset Analysis",
            "text": "As shown in Table 3, compared to previous works and KPG benchmarks, COCO covers more languages and is highly positioned in matters of number of instances, ratio of absent keyphrases, and average input length. The distribution of keyphrases in our dataset varies across languages due to the fact that the most recent judgments tend to have a higher number of keyphrases attached to them (the average number of keyphrases per language can be found in Appendix F). Consequently, instances in languages from the most recent Member States tend to be biased towards having more keyphrases. Another consequence of the temporal evolution of the average number of keyphrases coupled with the chronological split, is that COCO’s validation and test sets contain more keyphrases per instance on average (8.3 and 10.5 respectively) compared to its training set (5.4 keyphrases on average). This creates a discrepancy between the sets that practitioners should be aware of. However, we advocate that a higher number of keyphrases in the test set of COCO will be beneficial to the evaluation of the competing models by providing a larger number of potential gold keyphrases, and by assessing the capacity of models to generalize across time when target keyphrases follow different patterns with respect to the past."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Models",
            "text": "As the majority of COCO’s target keyphrases are absent from input documents, we focus on generative models, as extractive ones such as YAKE Campos et al. (2020  ###reference_b12###) are ill-suited here. Recent state-of-the-art models in English KPG rely heavily on pretrained models, such as KeyBART (Kulkarni et al., 2022  ###reference_b39###) or SciBART (Beltagy et al., 2019  ###reference_b6###), which are pre-trained and fine-tuned on a massive corpus of English scientific documents. However, such models are not available in a multilingual format, nor are they specific to the legal domain. We therefore choose to use mBART50 (Tang et al., 2020  ###reference_b70###), a variant of mBART (Liu et al., 2020  ###reference_b43###) with support for 50 languages instead of 25. We also test mT5 (Xue et al., 2021  ###reference_b77###), which covers 101 languages. Most models, including mBART50, have a maximum input sequence length typically set to 1024 tokens. While mT5’s input length can be set arbitrarily, it was originally pretrained with a context of 1024. The main caveat is mT5’s memory complexity that increases quadratically with input length, hence a prohibitive computing cost. While there exists some models whose maximum input length reaches up to 16k tokens, such as Longformer (Beltagy et al., 2020  ###reference_b7###), BigBird (Zaheer et al., 2020  ###reference_b82###) or LongT5 (Guo et al., 2022  ###reference_b33###), these remain computationally expensive to run with our current resources and are only suitable for English data. Therefore, similarly to Cérat et al. (2023  ###reference_b22###), we implemented a mBART variant with LSG attention (Condevaux and Harispe, 2023  ###reference_b19###) such that the maximum input length reaches 8192 tokens instead of 1024. Doing so makes the memory complexity increase linearly with respect to the input length, instead of quadratically as it is the case with a traditional attention mechanism. All models are trained with early stopping and a maximum epoch number of 10, except mBART50-8k and mT5-large with only 5 epochs, as their training is more time-consuming. Details about the hyperparameters and training process are provided in Appendices A  ###reference_### and D  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Evaluation Protocol",
            "text": "As has become common practice in recent works on KPG (Meng et al., 2017  ###reference_b53###; Garg et al., 2023  ###reference_b30###; Shen and Le, 2023  ###reference_b66###; Chen and Iwaihara, 2023  ###reference_b16###), model evaluation relies on two F1 measures: F1@ () and F1@, calculated separately for present and absent keyphrases. F1@ is computed using the entirety of the generated keyphrases while F1@ is computed using the best generated keyphrases (by truncating if necessary). In both cases, the number of target keyphrases remain untouched. F1@ is calculated using only the top best scoring keyphrases among the model’s predictions, hence an upper boundary below 1 whenever the number of target keyphrases exceeds, which occurs in most of COCO’s instances. This is why F1@ tends to better reflect the model performance as all candidate keyphrases are taken into account without being truncated. For more details about these metrics, we refer to Yuan et al. (2020b  ###reference_b81###). Following the literature, target and predicted keyphrases are lowercased and stemmed (e.g. Meng et al. (2017  ###reference_b53###) applied Porter Stemmer for English) before conducting an exact match. Stemming is a critical step without which a candidate keyphrase could be erroneously considered wrong because of the morphological nature of the language. That is why we applied stemming for all languages for treating them as fairly as possible. For most of them, the Snowball stemmer Porter (2001  ###reference_b60###) was used. In addition to F1 measures, we also computed MAP@50 (Mean Average Precision). ###table_3###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ",  and  scores for present and absent keyphrases over all languages are reported in Table 4  ###reference_###.\nSince the number of instances varies across languages, we report average scores that are weighted and unweighted.\nThe former is an average score across all instances without taking language into consideration.\nConsequently, it can be highly influenced by high-resource languages covering more instances.\nThe latter is an unweighted average among languages’ scores.\nIn other words, all languages have equal importance, thus reflecting the ability of a model to perform equally well in high- and low-resource languages.\nUnsurprisignly, the YAKE extractive model performs poorly, finding none of the absent keyphrases.\nWith a fixed input length of 1024 tokens, the three mT5 variants dramatically underperform mBART50.\nThis is surprising as mT5 covers twice as many languages than mBART50.\nAlso, as mT5-large has 59% more parameters than mBART50, it would have been expected to outperform the latter, but the results reveal otherwise.\nWhen comparing mBART50 with mBART50-8k, the increase in the maximum context length brings significant improvement across all metrics.\nWhile an average gain of around 2% (for  over present phrases) may not seem significant, KPG evaluation often greatly underestimates the true performance of the models, due to the difficulty of correctly evaluating whether a keyphrase is correct or not (Wu et al., 2023  ###reference_b75###).\nAs such, the improvement shown by the mBART50-8k model is significant and reflected in the generated keyphrases, thus emphasizing the benefits in enlarging the maximum input length.\nStill, Table 5  ###reference_### shows that input context enlargement does not have uniform effects across languages.\nFor instance, for present keyphrases, some languages get small gains (Greek, Swedish), and some degrade (Lithuanian, Latvian).\nFor high-resource languages such as English and Italian, performance improves on present phrases, but seems stagnant on absent ones.\nFor low-resource languages, Croatian has the most dramatic improvement on present phrases while Irish gets one score above  with mBART50-8k.\nHowever, the performance for these languages still lags behind that of moderate-resource ones.\nThis is understandable as these languages have almost no training instances in our temporal split setting, thus revealing the difficulty of conducting KPG for unseen languages.555With a random split dataset,  achieved by mBART50 for Croation/Irish reaches 46.9/20.5 for present keyphrases and 13.9/10.2 for absent ones"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Analysis and Discussion",
            "text": "The first striking observation is that mBART models, despite covering less languages compared to mT5s, succeed in outperforming the latter models.\nOne explanation is that mT5 small, base and large variants generated on average significantly fewer phrases per instance (2.1, 2.3 and 2.3, respectively) with respect to mBART50 and mBART50-8k (5.5 and 6.0).\nConsequently, mT5 models are less likely to achieve high scores.\nThe other noticeable result is that the mBART models even succeed in outperforming mT5s in languages that only mT5s support such as Bulgarian or Greek.\nThis suggests that Conneau et al. (2020  ###reference_b20###)’s corpus used for pretraining mBART50 gave it a capacity to deal with much more languages than stated by Tang et al. (2020  ###reference_b70###).\nThe improvement from mBART50 to mBART50-8k is significant and emphasizes the importance of larger input length.\nLess that 14% of all instances fit into a 1024-tokens context window.\nAround 58% do when that length reaches 8k tokens.\nBuilding models that can efficiently generate keyphrases from larger documents is therefore crucial in order to achieve further progress (the length reaches 9160 tokens on average, and 17k at the 90th percentile, with mBART50 tokenizer).666We tried a sliding-window-based model that was not conclusive, thus the need to find better ways to capture context.\nMoreover, KPG models struggle for keyphrases with more tokens (split by whitespaces).\nIn the case of mBART50-8k, a matched target keyphrase contains on average 3.1 tokens, while an unmatched target phrase contains 5.7 tokens.\nUnmatching generated keyphrases contain 7.7 tokens on average. Upon manual inspection of generated keyphrases, most models succeed in generating simple phrases made of up to 3 terms, but they indeed struggle for longer noun phrases that refer to very specific or technical concepts such as “Market for antidepressant medicines (citalopram)” in Table 6  ###reference_###.\nAlso, although some candidate keyphrases are still relevant from a reader’s standpoint, they are penalized by the exact matching evaluation approach, although stemming is applied.\nFor instance, the output “Concept of ‘restrictions of competition by object’\" could be a decent generation for the target “Restriction by object” despite the lack of matching.\nIn order to mitigate this issue, we evaluated the models again with a semantic matching metric (Wu et al., 2023  ###reference_b75###) allowing us to compare predictions and targets without having to apply any sort of post-processing or stemming (the tool is however only available for English).\nResults in Table 7  ###reference_### have a correlation of  with F1@ scores obtained for English for both present and absent keyphrases.\nThis seems consistent with the ranking among models observed previously in Table 4  ###reference_###: mBART50-based models outperform mT5-based ones, and mBART50-8k is the front-runner."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we present COCO, a novel and open multilingual keyphrase generation dataset in the legal domain. We believe this dataset can help alleviate two current shortcomings of the keyphrase generation task: the lack of data in domains other than STEM; and the lack of multilingual non-English datasets. Our dataset is available at https://huggingface.co/datasets/NCube/coco  ###reference_pa###. We furthermore provide an analysis of COCO with key statistics, thus giving insights on the particularities of our dataset. Finally, we run multiple models in various settings in order to give an initial point of comparison for future works on COCO. Our corpus also highlights the need to efficiently capture larger input context, and will be a suitable testbed for models designed to do so."
        }
    ]
}