{
    "title": "Towards a Zero-Data, Controllable, Adaptive Dialog System",
    "abstract": "Conversational Tree Search Väth et al. (2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree.\nThe agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users.\nHowever, the need for additional training data hinders deployment in new domains.\nTo address this, we explore approaches to generate this data directly from dialog trees.\nWe improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU.\nWe further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms.\nFinally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.\n\n\n\nKeywords: Conversational Systems/Dialogue/Chatbots, Corpus, Usability, User Satisfaction",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "While the breakthroughs of modern Large Language Models  ###reference_id1### have made the creation of new dialog systems much easier, controlling their generated output remains an open challenge.\nThis makes LLMs  ###reference_id1### especially unsuitable for sensitive domains, e.g., legal or medical domains, where users must be able to implicitly trust the system’s output.\nIn such domains, dialog designers usually have the choice between implementing an FAQ-retrieval system or a hand-crafted dialog system.\nFAQ systems directly match user queries to question/answer pairs curated by domain experts, allowing close control of outputted texts (Wu et al., 2005  ###reference_b37###).\nHowever, as they are single-turn systems and cannot ask clarifying questions, they are only able provide general answers, rather than personalized content for a specific user and their situation.\nIncluding information for multiple cases in one answer would make them unapproachably long, while adding FAQs for each case, would make retrieval challenging.\nRetrieval accuracy itself is an open challenge (Thakur et al., 2021  ###reference_b31###),\ncreating a trade-off: Either providing a single, possibly incorrect answer to a user’s question,\nor providing multiple answers and shifting the burden of selecting the correct one to the user,\nwhich might be challenging for users unfamiliar with the domain.\nDialog systems, in contrast, allow for turn-based interactions, which can provide shorter, personalized answers, as well as support users new to a domain without enough experience to formulate precise questions.\nHowever, such systems either suffer from longer interactions (for handcrafted systems),\nor require large amounts of training data (Raghu et al., 2021  ###reference_b23###) and lack transparency and controllability (Gao et al., 2018  ###reference_b9###) (in the case of machine learning approaches),\nmaking them less suitable for low-resource settings (Zhang et al., 2020  ###reference_b40###) or sensitive domains (Cohen, 2020  ###reference_b5###).\nVäth et al. (2023  ###reference_b33###) address this problem by proposing a new type of hybrid dialog task bridging these two interaction styles, called  Conversational Tree Search  ###reference_id4### (CTS  ###reference_id4###).\nIn this task, dialog experts first define a dialog tree.\nAn agent then learns to either walk the user through each node in the tree, or to skip over parts not required to answer a user’s more specific question.\nIn this way, the agent is able to adapt its behavior to the user’s preferred interaction style,\nsupporting both specific and vague user queries,\nwithout sacrificing the controllability required in sensitive domains.\nHowever, CTS  ###reference_id4### still requires that dialog designers collect a corpus of real-user utterances, which poses a barrier to scaling this approach to new domains, especially for large and complicated domains.\nThe goal of this paper is to remove this barrier by exploring how CTS  ###reference_id4### can scale to new domains through the use of synthetically generated training data.\nConcretely, we seek to answer the following research questions:\n(RQ1) How can we effectively generate data for a zero data approach to training CTS  ###reference_id4### agents?\n(RQ1.1) How can we analyze the quality of generated data?\n(RQ1.2) How do agents trained on generated data perform in simulation, compared to agents trained on human data?\n(RQ1.3) How well do the data generation techniques transfer to new domains?\n(RQ2) How does a CTS  ###reference_id4### agent trained on generated data perform with real users compared to an agent trained on human data?\nTo address these questions, we investigate how LLMs  ###reference_id1### can be leveraged to automatically generate training data for new domains,\nwhile at the same maintaining the controllability aspect of the CTS  ###reference_id4### task.\nWe compare the quality of different data generation schemes by evaluating the performance of  Reinforcement Learning  ###reference_id2### (RL  ###reference_id2###) agents trained on the synthetic data.\nThen, we test scalability of our approach to new domains in simulation using multiple generative LLMs  ###reference_id1###.\nFinally, we perform user testing to verify the transferability to real-world use cases. All code and data is publicly available.111https://github.com/DigitalPhonetics/conversational-tree-search/tree/generated_v3  ###reference_sational-tree-search/tree/generated_v3###\nOur main contributions are:\n1) Creating two new datasets, ONBOARD and DIAGNOSE.\n2) Improving the training procedure for the CTS  ###reference_id4### agent, increasing absolute dialog success by more than .\n3) Introducing a new prompting method for generating diverse data, and demonstrating that automatic diversity and answerability metrics can provide insights for downstream dialog performance.\n4) Demonstrating that our generation techniques scale to new domains, where agents trained on synthetic data show comparable (no statistically significant difference) or better dialog success than agents trained on human data.\n5) Showing that success of agents in simulation translates to successful interactions with real users, with no statistically significant differences."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Task-oriented Dialog Systems",
            "text": "While open-domain dialog systems allow users to freely talk about any topic without a concrete goal, task-oriented dialog systems focus on helping a user reach a specific goal. Many task-oriented dialog systems use a slot-filling approach, where the dialog system tries to fill values for a selection of slots, e.g. cuisine type, that are necessary to reach that goal from the user Bobrow et al. (1977  ###reference_b2###). While slot filling approaches can allow hand-crafted dialog policies to follow pre-defined dialog flows Lucas (2000  ###reference_b15###), or can help efficiently narrowing down searches across e.g. database rows, such as finding restaurants or getting trip recommendations Louvan and Magnini (2020  ###reference_b14###), they are usually unable to perform semantic searches over the dialog domain and in cases of learned systems, unable to follow a dialog-designer controlled flow. Research into adaptive dialog systems aims to misalign dialog system output with user expectations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Adaptive Dialog Systems",
            "text": "Research into adaptive dialog systems aims to better align dialog system output with user expectations, aiming for a consistency that is difficult to achieve. Much research in this area uses generative models to adapt linguistic style, e.g., adjusting utterances depending on users’ emotional states Ma et al. (2020 ###reference_b16###) or personalities (Yang et al., 2018 ###reference_b38###; Firdaus et al., 2023 ###reference_b8###). However, generative models are by their nature difficult to control (Dušek and Kasner, 2020 ###reference_b6###). Some approaches even adapt the complexity of language (Janarthanam and Lemon, 2014 ###reference_b10###). In order to adapt underlying system behavior, however, additional cues have usually been required, e.g. social cues like laughter (Ritschel and André, 2018 ###reference_b27###), or explicit fine-tuning by the user Chen and Pu (2012 ###reference_b3###); Narducci et al. (2018 ###reference_b18###). However, eliciting such social cues is difficult for text-based systems and asking for explicit feedback places extra burden on the user."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Controllable Dialog Systems",
            "text": "In sensitive domains, it is crucial subject-experts maintain control of dialog flow to ensure correctness of system outputs. However, purely handcrafted systems struggle to handle the breadth of possible user inputs. To this end, several hybrid approaches have been investigated. Early approaches involved hand-crafting the set of actions allowed at a given dialog turn Williams (2008  ###reference_b35###). More recent approaches expand on this idea for neural systems (Williams et al., 2017  ###reference_b36###; Liang and Yang, 2018  ###reference_b13###; Razumovskaia and Eskenazi, 2019  ###reference_b25###), where the action space can be constrained using masks, e.g., by automatically converting expert designed dialog trees into hybrid code networks (Shukla et al., 2020  ###reference_b29###). Research into adaptive dialog systems aims to better misalign dialog system output with user expectations. While such approaches help control dialog agent behavior, they do not provide a mechanism for skipping portions of a dialog irrelevant to a user, which leads to longer interactions that can be frustrating for users with more domain familiarity."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "2.4.   Data Generation and Augmentation",
            "text": "Common data augmentation approaches include lexical substitution Wei and Zou (2019  ###reference_b34###), where tokens are inserted, deleted or substituted with semantically similar replacements, as well as back-translation Sennrich et al. (2016  ###reference_b28###) where data is automatically translated into other languages before being translated back to the source language.\nWhile such approaches can help to expand an existing dataset, they still require seed data, which may not exist for new domains.\nTo address this, research in, e.g., the field of low-resource  Question Answering  ###reference_id3### (QA  ###reference_id3###) has started exploring the role of LLMs  ###reference_id1### in data generation Puri et al. (2020  ###reference_b21###); Chen et al. (2023  ###reference_b4###).\nGiven a text, LLMs  ###reference_id1### can be prompted to generate questions about it, e.g., by asking the model to generate a question for which a given named entity is the answer (Li et al., 2023  ###reference_b12###).\nHowever, LLMs  ###reference_id1### are black-box algorithms and suffer from hallucination Azaria and Mitchell (2023  ###reference_b1###); Peng et al. (2023  ###reference_b20###); Manakul et al. (2023  ###reference_b17###).\nAs such, it is difficult to guarantee that the generated questions are logical, natural, or answerable by the original text.\nMoreover, commonly used automatic evaluation metrics for text generation do not necessarily correlate with human judgment Nema and Khapra (2018  ###reference_b19###).\nIn light of this, we explore different generation strategies and techniques for analyzing the artificial data quality, rather than trusting a single metric.\nA recent approach in the dialog community trains a model for generating synthetic dialog acts and user utterances for flowchart-grounded troubleshooting dialogs Zhan et al. (2023  ###reference_b39###).\nWhile this method also relies on the domain representation in form of a structured graph, our generation approach does not require any model training, nor any training data besides the domain graph itself.\nAdditionally, CTS  ###reference_id4### is not limited to the specific task format of trouble-shooting dialogs. Research into adaptive dialog systems aims to better misalign dialog system output with user expectations."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "2.5.   Conversational Tree Search",
            "text": "Research into adaptive dialog systems aims to better align dialog system output with user manuals."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Datasets",
            "text": "To investigate the scalability of our data generation techniques, we examine the performance of the CTS  ###reference_id4### agent on three new datasets, and compare to the original REIMBURSE dataset from Väth et al. (2023  ###reference_b33###).\nIn contrast to the REIMBURSE dataset, the goal of all new datasets is to serve as a zero-data test-bed for testing training and testing models on data generated directly from the nodes themselves.\nWhile we do provide a test and a train set, like that in REIMBURSE, the goal of this is to allow for the training of reference models to act as a benchmark for models trained entirely on generated data."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   REIMBURSE",
            "text": "The REIMBURSE dataset as proposed by Väth et al. (2023  ###reference_b33###) is a German language dataset for the CTS  ###reference_id4### task.\nIt is a challenging real-world dataset in the travel reimbursement domain, created with domain experts.\nAlong with the dialog tree, questions and answer paraphrases were collected from real user interactions.\nThese questions and answer-paraphrases have been split into a train and test set which can each be used by the provided user simulator to generate an arbitrary number of simulated dialogs.\nA breakdown of the dataset statistics can be found in Table 1  ###reference_###.\nAlthough we do not train any new models on this dataset, we use it as a benchmark to compare the performance of our agents to."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   REIMBURSE-En",
            "text": "In order to make the CTS  ###reference_id4### task more accessible to a wider audience, we choose to translate the REIMBURSE dataset to English.\nAdditionally, this opens up more options for language models and resources, which might not have been available for the original German data.\nThis dataset represents a direct translation of the REIMBURSE dataset, sharing all of the same characteristics, in order to allow for comparisons to the findings of the original CTS  ###reference_id4### paper.\nThe translation was performed manually by a bilingual domain-expert in order to obtain a faithful and factually correct English equivalent.\nDataset statistics are shown in Table 1  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   DIAGNOSE",
            "text": "The DIAGNOSE dataset was created for the medical domain.\nIt was designed to help users identify different medical conditions based on symptoms, as well as to find out more about treatment options and risk factors.\nThe dataset is based on a small subset of Wikipedia articles about conditions related to scalp and head symptoms.\nDIAGNOSE was designed to be comparatively easy.\nEven though the node texts contain a large amount of domain-specific vocabulary, the dialog tree has a lower maximum node degree and a shallower tree depth than REIMBURSE-En.\nAdditionally, the dialog graph for this domain does not contain any variable- or logic nodes.\nA breakdown of dataset properties can be found in Table 1  ###reference_###.\nAn example node and associated questions can be seen below:\nNode Text Anemia symptoms include fatigue, pale skin and gums, blue color in the whites of the eyes, brittle nails, irritability, dizziness, sore tongue, shortness of breath, unusual food cravings, and headache.\n\\speakQuestion 1 What are symptoms of anemia?\n\\speakQuestion 2 How do I know if I have anemia?\n\\speakQuestion 3 Is a sore tongue a common symptom of anemia?"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   ONBOARD",
            "text": "The ONBOARD dataset provides users with information about moving to a new city in a foreign country, and the legal and financial steps they will need to undertake, i.e., setting up bank accounts, acquiring health insurance, applying for required visas or residence permits, etc.\nThis domain presents an additional challenge as it contains code-switching for topics related to legal issues, in order provide users with official names for documents, concepts, and institutions.\nSimilar to the REIMBURSE dataset, the dialog tree for ONBOARD contains multiple variable nodes and several logic nodes.\nA breakdown of the dataset statistics can be found in Table 1  ###reference_###.\nAn example of a a dialog node and test questions is given below.\nNode Text The registration office will provide you with a confirmation of your registration [Meldebestätigung], which you will need for opening a bank account and for obtaining a residence permit (if applicable).\n\\speakQuestion 1 Where do I get confirmation that I’ve registered my address?\n\\speakQuestion 2 What do I need the confirmation of registering my address for?"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Dialog Agent Implementation",
            "text": "For our RL  ###reference_id2### dialog agent, we follow the architecture and training process outlined in Väth et al. (2023  ###reference_b33###) with the following changes:\n1) We swap the original language model for an MPNET Song et al. (2020  ###reference_b30###) based Sentence-Transformer Reimers and Gurevych (2019  ###reference_b26###), as the new datasets we introduce are in English, and it reports the highest average performance of pretrained Sentence-Transformers for English.\n2) In contrast to free mode, rewards for guided mode only considered whether the agent moved to the correct next node, rather than checking that a global goal was reached by the end of the dialog.\nAfter analyzing conversations between CTS agent and user simulator obtained by the original implementation, we believe it is more realistic that, even in guided mode, users would have a consistent question they wanted answered.\nTherefore, we now draw global goals for guided mode users (a node anywhere in the graph) instead of choosing one of the immediate neighboring nodes as the next goal each turn.\nWe then assign a large reward to reaching the global goal.\nAt the same time, we keep a small positive reward for skipping to the correct follow-up node along the sampled trajectory, as a sequence of locally correct decisions (reaching a correct immediate neighbor) implies global correctness (reaching the correct goal node).\nThese changes result in a harsher evaluation metric for dialog success, since e.g. in a 5-step dialog, following a correct trajectory, but missing the final goal in the last turn, will now result in a failed dialog ( success) instead of a partially successful dialog ( success), which we consider to be more realistic.\n3) Finally, the original CTS agent was trained jointly on navigating the graph and on predicting the appropriate interaction style (intent).\nHere, we scale the loss of the interaction style prediction objective down to  to emphasize learning Q-values as the main task:\n.\nWe found this had no significant impact on the interaction style prediction F1 score.\n4) We tune several other hyperparameters, increasing the batch size from  to , and the training steps from  to .\nAll hyperparameters for training the dialog agent are listed in Appendix A  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Data Generation Methods",
            "text": "As the user simulator from Väth et al. (2023  ###reference_b33###) requires both, initial user questions and per-node user responses, we explore methods for generating both of these types of utterances.\nWe test these generation methods with a small LLM  ###reference_id1###, and with a large commercial one, both of which can process separate system and user input directives.\nThe first method, GenV1, is a naive prompt instructing an LLM  ###reference_id1### to generate diverse, FAQ-style questions about a given dialog node’s text via the system directive.\nThe amount of questions to generate and the node context are then given via user input (see Table 2  ###reference_###).\nFor GenV2, we use the same user input, but change the system directive to explicitly generate shorter questions (Table 2  ###reference_###).\nFor the last method, GenV3, we were inspired by Li et al. (2023  ###reference_b12###) and Chen et al. (2023  ###reference_b4###), who use  Named Entity Recognition  ###reference_id5### (NER  ###reference_id5###) to steer question generation.\nHowever, these approaches only generate cloze questions, where the named entity is the answer, severely limiting the diversity of generated questions (Puri et al., 2020  ###reference_b21###).\nTherefore, we develop a novel mixed method to increase question diversity.\nWe first generate 3 questions about the whole node text using the Method 2, to get a basic coverage of the node.\nThen, we perform NER  ###reference_id5### and explicitly prompt the LLM  ###reference_id1### to generate three questions about each entity –instead of forcing the entities to only be the answer– using a second set of prompts (see Table 2  ###reference_###).\nIf the total number of generated questions is lower than 10, we generate the difference using Method 2."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Question Generation",
            "text": "The first method, GenV1, is a naive prompt instructing an LLM  ###reference_id1###  ###reference_id1### to generate diverse, FAQ-style questions about a given dialog node’s text via the system directive.\nThe amount of questions to generate and the node context are then given via user input (see Table 2  ###reference_###  ###reference_###).\nFor GenV2, we use the same user input, but change the system directive to explicitly generate shorter questions (Table 2  ###reference_###  ###reference_###).\nFor the last method, GenV3, we were inspired by Li et al. (2023  ###reference_b12###  ###reference_b12###) and Chen et al. (2023  ###reference_b4###  ###reference_b4###), who use  Named Entity Recognition  ###reference_id5###  ###reference_id5### (NER  ###reference_id5###  ###reference_id5###) to steer question generation.\nHowever, these approaches only generate cloze questions, where the named entity is the answer, severely limiting the diversity of generated questions (Puri et al., 2020  ###reference_b21###  ###reference_b21###).\nTherefore, we develop a novel mixed method to increase question diversity.\nWe first generate 3 questions about the whole node text using the Method 2, to get a basic coverage of the node.\nThen, we perform NER  ###reference_id5###  ###reference_id5### and explicitly prompt the LLM  ###reference_id1###  ###reference_id1### to generate three questions about each entity –instead of forcing the entities to only be the answer– using a second set of prompts (see Table 2  ###reference_###  ###reference_###).\nIf the total number of generated questions is lower than 10, we generate the difference using Method 2."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Response Generation",
            "text": "To generate responses, we extract all nodes requiring user input from the dialog graph.\nThen, we instruct the LLMs  ###reference_id1### to generate 5 paraphrases for each possible answer prototype, in the context of the full node text (Table 3  ###reference_###; A).\nAdditionally, to mimic different user interaction styles, we instruct the LLMs  ###reference_id1### to generate 5 paraphrases of the the responses using only keywords (Table 3  ###reference_###; B)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Experimental Setup",
            "text": "We asked each participant to interact with either a CTS agent trained on real data or one trained on generated data in the REIMBURSE domain.\nApart from demographic information, we ask for previous experience with dialog systems and with business travel.\nDuring the experiment, participants were asked to complete three conversations with their assigned dialog system.\nEach conversation, they were randomly assigned a new goal, covering one of three expected interaction styles: 1) “open” goals representing a general/vague information need, 2) “easy” goals representing a concrete information need, and 3) “hard” goals representing a concrete information need requiring personalized information to correctly answer.\nPersonalized information refers to the user’s specific circumstances, e.g. trip duration or funding organization, which can change the dialog flow.\nBetween each dialog, users were asked to rate their subjective perception of dialog length and how well their question was answered.\nAfter the interaction, they were asked rate the usability of the dialog agent, how much they trusted it, and its reliability.\nFor more details see Appendix B  ###reference_###."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   RQ 1.1: Analysis of Generated Data",
            "text": "We generate data using the methods described in sections 5.1  ###reference_### and 5.2  ###reference_###.\nWe use two different LLMs  ###reference_id1###: ChatGPT (gpt-3.5-turbo, via API) 222https://platform.openai.com/docs/models/gpt-3-5  ###reference_t-3-5### and a LLAMA-based Touvron et al. (2023  ###reference_b32###), instruction fine-tuned and quantized model 333https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GPTQ  ###reference_ama-30b-instruct-2048-GPTQ### that fits onto a single NVIDIA GeForce RTX 3090 graphics card. GenV3 uses Stanza Qi et al. (2020  ###reference_b22###) for NER  ###reference_id5###.\nTo calculate question similarity, we use the Sentence-Transformer model from section 4  ###reference_###.\nAnswer confidence scores are calculated with a QA  ###reference_id3### model 444https://huggingface.co/deepset/roberta-large-squad2  ###reference_ge-squad2### pretrained on the SQUAD2.0 dataset Rajpurkar et al. (2018  ###reference_b24###), using a generated question and associated node text that is supposed to contain the answer as inputs.\nFinally, we measure diversity using Self-BLEU Zhu et al. (2018  ###reference_b41###) scores."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   RQ 1.2: Human Data vs. Synthetic Data",
            "text": "For automatic evaluation, we use the updated CTS  ###reference_id4### user simulator (section 4  ###reference_###) with 500 randomly chosen dialog goals on the REIMBURSE-En test split.\nWe evaluate not only the combined success rate (average between guided and free mode success), but also present a metric representing the user’s perceived dialog length, which counts only the nodes shown to the user."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   RQ 1.3: Method Generalizability",
            "text": "To evaluate how well our data generation method generalizes to new domains, we perform additional evaluation in simulation, analogous to (section 6.2  ###reference_###), using the test splits of the new datasets ONBOARD and DIAGNOSE."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "6.4.   Human Evaluation (RQ 2)",
            "text": "To understand how performance of an agent trained on generated data translates to real-world users, we recruit 44 participants from the crowdsourcing platform Prolific555https://www.prolific.com  ###reference_www.prolific.com### to take part in human evaluation.\nParticipants were native English speakers with varying experience with business travel (self-rating between 2 and 5 on a 5 point Likert-scale).\nThey were compensated at the platform recommended rate of 9 /hour.\nThe experiment took roughly 20 minutes.\nWe asked each participant to interact with either a CTS agent trained on real data or one trained on generated data in the REIMBURSE domain.\nApart from demographic information, we ask for previous experience with dialog systems and with business travel.\nDuring the experiment, participants were asked to complete three conversations with their assigned dialog system.\nEach conversation, they were randomly assigned a new goal, covering one of three expected interaction styles: 1) “open” goals representing a general/vague information need, 2) “easy” goals representing a concrete information need, and 3) “hard” goals representing a concrete information need requiring personalized information to correctly answer.\nPersonalized information refers to the user’s specific circumstances, e.g. trip duration or funding organization, which can change the dialog flow.\nBetween each dialog, users were asked to rate their subjective perception of dialog length and how well their question was answered.\nAfter the interaction, they were asked rate the usability of the dialog agent, how much they trusted it, and its reliability.\nFor more details see Appendix B  ###reference_###  ###reference_###."
        },
        {
            "section_id": "6.4.1",
            "parent_section_id": "6.4",
            "section_name": "6.4.1.   Evaluation Metrics",
            "text": "The perceived dialog length was measured on a 5-point scale from 1 (much too short) to 5 (much too long).\nPerceived success was measured on a 4-point scale, where users were asked to rate how well their question had been answered from 1 (not at all) to 4 (completely).\nAdditionally, the objective dialog length and success condition were logged for each dialog.\nUsability of the dialog agent was measured using the Universal Measure of User Experience scale developed by Finstad (2010  ###reference_b7###).\nUser trust was measured using the reliability and trust subscales from Körber (2018  ###reference_b11###))."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Results & Discussion",
            "text": "Before testing performance of agents trained on generated data, we first verify our changes to the CTS  ###reference_id4### agent.\nAs the hyperparameters for the original agent were tuned on the German dataset, for fairness, we report the original CTS  ###reference_id4### agent’s performance on both English and German (Table 5  ###reference_###).\nOur changes to the CTS  ###reference_id4### agent improve the combined success rate by over  compared to the original agent on the German REIMBURSE dataset and  for the English REIMBURSE-En.\nIt should be noted that the actual improvement over the German agent is likely larger, as the success metric reported for German comes from (Väth et al., 2023  ###reference_b33###), rather than the new and harsher metric we use for English (section 4  ###reference_###).\nLooking at the question lengths between human data and data generated by GENV1 (Figure 2  ###reference_###),\nwe observe that the generated questions seem to be longer than human questions.\nWhen manually inspecting the generated questions, we also find them to be much less natural than those from the human data.\n###figure_2### We amend the original prompt used, creating GENV2, to explicitly ask for short outputs (subsection 5.1  ###reference_###) in an effort to align the syntax of the generations better with the human data.\nThis change to the prompt shifts the distribution of question lengths more towards the human training distribution, and qualitatively yields more natural utterances.\nHowever, it still does not ensure that the artificial data is semantically similar to human data.\nTo investigate how semantically similar the generated questions are to human data, we calculate the pair-wise similarities between all human and generated questions for each node from the dialog graph, and then average the similarities across all nodes (Figure 3  ###reference_###).\nHere, we see that the GenV2 data is still quite distinct from the human data.\n###figure_3### When manually inspecting the generations, we find that generated questions tend to focus only on one part of the node text, making them lack diversity and omit topics real users might ask about.\nTo address this, we develop the novel two-step GenV3 prompt, steering the model to explicitly ask about all named entities in a node (subsection 5.1  ###reference_###).\nWe see that doing so significantly () increases the similarity of the generated (avg.: ) to the human training data than GenV2 (avg.: ), as measured with a standard t-test.\nWe also look at the diversity of the generated questions.\nThe self-BLEU scores (Table 4  ###reference_###) show that the GenV3 data are the most diverse.\nThis metric can be used to analyze the quality of the generated data even in the absence of human comparison data.\nIn conjunction with diversity, we estimate the average “answerability” via QA  ###reference_id3### confidence scores of the generated questions, given the node text as answer.\nHere, we also see that the improvements from GenV3 and GenV2 together also significantly () increase the average answerability, from an average of  with naive prompt to  with GenV3, according to a t-test.\nWhen looking at downstream performance (Table 5  ###reference_###), we see that improvements in these metrics also lead to higher dialog success, suggesting they can be used as an indicator of generation quality.\nTo investigate whether synthetic data can be a viable alternative to human data, we compare agent performance in simulation.\nFrom Table 5  ###reference_###, we see that the best performing agent trained on artificial data (GenV3: 69.44% success) performs comparably to the best performing agent trained on human data (CTSours: 73.86% success).\nUsing a standard t-test, we find no statistically significant difference.\nTo test of the scalability of our generation methods, we analyze model performance on two new domains.\nAs each of these has their own challenges (section 3  ###reference_###), we compare each model trained on generated data to a baseline trained on human data.\nWhen looking at Table 6  ###reference_###, the agent trained on data generated by LLAMA is again nearly able to match the performance of the model trained on human data for the DIAGNOSE dataset, while the model trained on data generated by ChatGPT surpasses it.\nOn the other hand, the ONBOARD dataset may present a more challenging domain, due in part to the code-switching present in the dialog nodes.\nDespite this, the model trained on data from ChatGPT nearly reaches the performance of models trained on human data.\nBased on this, we find that the generation techniques do appear to scale to new domains, as t-tests show no statistically significant differences between the best synthetically trained agents and the agents trained on real data in any domain."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "7.1.   RQ 1: Transitioning to a Zero Data Approach",
            "text": "Looking at the question lengths between human data and data generated by GENV1 (Figure 2  ###reference_###  ###reference_###),\nwe observe that the generated questions seem to be longer than human questions.\nWhen manually inspecting the generated questions, we also find them to be much less natural than those from the human data.\n###figure_4### We amend the original prompt used, creating GENV2, to explicitly ask for short outputs (subsection 5.1  ###reference_###  ###reference_###) in an effort to align the syntax of the generations better with the human data.\nThis change to the prompt shifts the distribution of question lengths more towards the human training distribution, and qualitatively yields more natural utterances.\nHowever, it still does not ensure that the artificial data is semantically similar to human data.\nTo investigate how semantically similar the generated questions are to human data, we calculate the pair-wise similarities between all human and generated questions for each node from the dialog graph, and then average the similarities across all nodes (Figure 3  ###reference_###  ###reference_###).\nHere, we see that the GenV2 data is still quite distinct from the human data.\n###figure_5### When manually inspecting the generations, we find that generated questions tend to focus only on one part of the node text, making them lack diversity and omit topics real users might ask about.\nTo address this, we develop the novel two-step GenV3 prompt, steering the model to explicitly ask about all named entities in a node (subsection 5.1  ###reference_###  ###reference_###).\nWe see that doing so significantly () increases the similarity of the generated (avg.: ) to the human training data than GenV2 (avg.: ), as measured with a standard t-test.\nWe also look at the diversity of the generated questions.\nThe self-BLEU scores (Table 4  ###reference_###  ###reference_###) show that the GenV3 data are the most diverse.\nThis metric can be used to analyze the quality of the generated data even in the absence of human comparison data.\nIn conjunction with diversity, we estimate the average “answerability” via QA  ###reference_id3###  ###reference_id3### confidence scores of the generated questions, given the node text as answer.\nHere, we also see that the improvements from GenV3 and GenV2 together also significantly () increase the average answerability, from an average of  with naive prompt to  with GenV3, according to a t-test.\nWhen looking at downstream performance (Table 5  ###reference_###  ###reference_###), we see that improvements in these metrics also lead to higher dialog success, suggesting they can be used as an indicator of generation quality.\nTo investigate whether synthetic data can be a viable alternative to human data, we compare agent performance in simulation.\nFrom Table 5  ###reference_###  ###reference_###, we see that the best performing agent trained on artificial data (GenV3: 69.44% success) performs comparably to the best performing agent trained on human data (CTSours: 73.86% success).\nUsing a standard t-test, we find no statistically significant difference.\nTo test of the scalability of our generation methods, we analyze model performance on two new domains.\nAs each of these has their own challenges (section 3  ###reference_###  ###reference_###), we compare each model trained on generated data to a baseline trained on human data.\nWhen looking at Table 6  ###reference_###  ###reference_###, the agent trained on data generated by LLAMA is again nearly able to match the performance of the model trained on human data for the DIAGNOSE dataset, while the model trained on data generated by ChatGPT surpasses it.\nOn the other hand, the ONBOARD dataset may present a more challenging domain, due in part to the code-switching present in the dialog nodes.\nDespite this, the model trained on data from ChatGPT nearly reaches the performance of models trained on human data.\nBased on this, we find that the generation techniques do appear to scale to new domains, as t-tests show no statistically significant differences between the best synthetically trained agents and the agents trained on real data in any domain."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "7.2.   RQ 2: Human Evaluation",
            "text": ""
        },
        {
            "section_id": "7.2.1",
            "parent_section_id": "7.2",
            "section_name": "7.2.1.   Generated vs. Real Data",
            "text": "After performing human evaluation, we find that there are no statistically significant differences (using a standard t-test) between either subjective or objective measures of success or dialog length (Table 7  ###reference_###).\nAdditionally, we find no difference in the reported trust, reliability, or usability scores between either group.\nThis suggests that there is no human-observable loss in performance when using generated data compared to real data, either in terms of objective metrics or subjective metrics."
        },
        {
            "section_id": "7.2.2",
            "parent_section_id": "7.2",
            "section_name": "7.2.2.   Human Evaluation vs. Simulator",
            "text": "Finally, to validate our updated user simulator, we additionally compare the objective performance metrics from the human evaluation (Table 7  ###reference_###) to those obtained in simulation (Table 5  ###reference_###).\nWe find that the success rates between the simulated and human dialogs are very comparable ( and  respectively for the model trained on human data, and  and  for the model trained on generated data).\nWe perform statistical analysis using Welch’s t-test to account for the difference in sample size, and find no significant difference, regardless of the source of training data.\nBased on this, we conclude that results from simulation translate well to real human interaction, suggesting the simulator can be a good proxy for real user evaluation.\nWe therefore expect the results reported in (Table 6  ###reference_###) will translate to similar performance with real users."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Conclusion",
            "text": "In this paper, we present two new and publicly available datasets, ONBOARD, providing help for moving to a new city in a foreign country, and DIAGNOSE, a medical domain.\nThe datasets each consist of a dialog tree and human-collected text inputs.\nWe apply a harsher, more realistic evaluation metric and improve on the agent training method from the original CTS  ###reference_id4### (Väth et al., 2023  ###reference_b33###), increasing dialog success by over .\nGiven a dialog tree, we explore several zero-data prompting-based methods for generating user utterance data to train a CTS  ###reference_id4### agent, developing a novel two-stage prompting approach to increase question diversity.\nThrough this process, we find that automatic scores for diversity and answerability can be indicative of downstream dialog task performance.\nFurthermore, we show that there is no statistically significant difference in objective metrics between agents trained on human data or on generated data in the REIMBURSE-En domain.\nWe verify this both through simulation and through testing with real users.\nUser evaluation further reveals no statistically significant differences on subjective metrics (trust, reliability, usability, subjective length, or subjective dialog success) either.\nThis suggests that we can effectively generate training data from a dialog tree, such that CTS  ###reference_id4### agents can be trained in zero data settings with negligible performance loss.\nWe also find that the size of the tested LLMs  ###reference_id1### does not result in significant differences in task performance.\nTo evaluate how well our techniques scale to new domains, we further tested agent performance on both new datasets we introduced.\nFor ONBOARD, we again find that performance of agents trained on generated data is comparable to that of agents trained on human data.\nFor DIAGNOSE, performance can even exceed that of the agent trained on human data.\nThis suggests that our methods scale well to new domains."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Ethical Considerations",
            "text": "To ensure that users could give informed consent, we provided a detailed description of the task and research objectives both on the crowdsourcing platform and once they had accepted the task.\nIn respect of participant privacy, we specifically did not collect personally identifying data from any users.\nTo this end, we store all logs and survey responses using an anonymous hash generated based on a given username, rather than with the username itself.\nIn this way, users could log in again if they needed to take a break in the middle of the interaction, but we had no way of directly linking any recorded results to, e.g., users’ Prolific account identifiers.\nTo ensure that participants were fairly compensated, we followed best practices recommended by the crowdsourcing platform paying users at 9 /hr.\nWe additionally used our pilot study to verify that our estimated time was below the median time we selected when advertising the task."
        }
    ]
}