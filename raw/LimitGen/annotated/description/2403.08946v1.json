{
    "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
    "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models.\nRecently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency.\nThis extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity and advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the “black box” to actively enhancing the productivity and applicability of LLMs in real-world settings.\nMeanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI.\nTherefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations.\nThe code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Explainability holds great promise in understanding machine learning models and providing directions for improvement.\nIn practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expectations. For example, does the model leverage reliable evidence and domain knowledge in its decision-making? Does the model contain bias and discrimination? Does the model show any vulnerabilities to potential attacks? Will the model output harmful information?\nSecond, in recognition of model imperfections, we aspire for explainability to inform the development of better models. For example, how to adjust the behaviors of a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the performance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met?\nIn recent years, the body of literature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a  ###reference_b62###; Murdoch et al., 2019  ###reference_b175###; Tjoa & Guan, 2020  ###reference_b225###; Došilović et al., 2018  ###reference_b61###; Rudin et al., 2022  ###reference_b195###), encompassing a wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018  ###reference_b282###), textual (Danilevsky et al., 2020  ###reference_b50###), graph (Yuan et al., 2022  ###reference_b279###), and time-series data (Zhao et al., 2023c  ###reference_b288###). Some literature delves into specific techniques, such as attention methods, generalized additive models, and causal models. Additionally, some offer reviews on general principles and categorizations or initiate discussions on evaluating the faithfulness of explanations (Yang et al., 2019  ###reference_b266###).\nDespite the progress, the last mile of XAI – making use of explanations – has not received enough attention.\nIn many cases, we seem to be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by qualitative analysis of the model’s strengths and weaknesses. While these explanations can reveal a model’s imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete steps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold.\nFirst, there is an inherent conflict between AI automation and human engagement in XAI.\nOn one hand, humans need to define explainability that the model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the scalability and practical implementation of model debugging and improvement in AI workflows.\nSecond, many of the current approaches view explainability as a purely technical matter, ignoring the needs of practitioners and non-technical stakeholders.\nExisting XAI methods are mainly developed as statistical and mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the expectations of practitioners across various application domains (Malizia & Paternò, 2023  ###reference_b164###). An explanation that satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer perceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models (LLMs) (Brown et al., 2020  ###reference_b26###; Achiam et al., 2023  ###reference_b1###; Touvron et al., 2023b  ###reference_b228###; Chiang et al., 2023  ###reference_b40###) appear to have exacerbated the challenge we are facing.\nFirstly, LLMs typically possess a significantly larger model size and a greater number of parameters. This increased model complexity intensifies the difficulty of explaining their inner workings.\nSecond, different from traditional ML models that primarily focus on low-level pattern recognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation, reasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges for XAI techniques.\nConsidering the transformative impact of LLMs across various applications, ensuring the explainability and ethical use of LLMs has become an imminent and pressing need.\nMeanwhile, the emergent capabilities of LLMs also present new opportunities for XAI research. Their human-like communication and commonsense reasoning skills offer prospects for achieving explainability in ways that could potentially augment or replace human involvement.\n###figure_1### Defining “Usable XAI”. In light of the above considerations, in the context of LLMs, we define Usable XAI which includes two aspects as follows.\n(1) Utilizing Explainability to Enhance LLM and AI Systems. Beyond just producing explanations or enhancing the transparency of LLMs, we explore whether these explanations can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large, such as accuracy, controllability, fairness, and truthfulness.\n(2) Utilizing LLMs to Enhance XAI Frameworks. The human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness, by converting the numerical values into understandable language. Also, the commonsense knowledge stored in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans and alleviating the need for real human involvement in AI workflows.\nContribution of this paper.\nIn this paper, we investigate 10 strategies towards usable XAI techniques in the context of LLMs.\nThese strategies are organized into two major categories: (1) Usable XAI for LLMs; (2) LLM for Usable XAI, as shown in Figure 1  ###reference_###. Additionally, we conduct case studies to substantiate the discussion on selected techniques.\nFor each strategy, we also explore the open challenges and areas that require further investigation in future work.\nUsable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines, including LLMs and small models.\nFirst, we investigate how explanations could be utilized to diagnose and enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, targeting LLM predictions (Section 2  ###reference_###), LLM components (Section 3  ###reference_###), and training samples (Section 4  ###reference_###), respectively.\nSecond, we focus on how explanations could be leveraged to scrutinize and boost model trustworthiness (Section 5  ###reference_###), including security, fairness, toxicity, and truthfulness, which is crucial to achieving human alignment.\nThird, we discuss how explainability could guide the augmentation of data, including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of crafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6  ###reference_###) and knowledge-enhanced prompts (Section 7  ###reference_###). Furthermore, we introduce leveraging LLM explanations to augment training data for improving small models (Section 8  ###reference_###).\nLLM for Usable XAI. In this part, we investigate strategies for leveraging the advanced capabilities of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in practice.\nFirst, we examine ways to enhance the user-friendliness of explanations through the generative capabilities of LLMs (Section 9  ###reference_###). Second, we introduce how to automate the design of interpretable AI workflows by leveraging the planning abilities of LLMs (Section 10  ###reference_###).\nThird, we introduce how to facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human cognition processes (Section 11  ###reference_###).\nDifferences between this paper and existing surveys.\nMany surveys have been conducted to examine Explainable AI (Du et al., 2019a  ###reference_b62###; Tjoa & Guan, 2020  ###reference_b225###; Došilović et al., 2018  ###reference_b61###) or Interpretable Machine Learning (Murdoch et al., 2019  ###reference_b175###). This paper differs from existing work as we focus on explanation methods for large language models.\nMeanwhile, different from the existing survey (Zhao et al., 2023b  ###reference_b285###) that mainly reviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024  ###reference_b159###), which also discusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable workflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmentation, improving user-friendliness, XAI evaluation).\nFinally, our paper contributes further by providing detailed case studies and open-sourced codes, fostering future research in applying explanations effectively within the LLM context."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM Diagnosis via Attribution Methods",
            "text": "This section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover model defects with attribution scores.\nWe start with revisiting existing attribution methods, and then discuss which methods are still suitable for explaining LLMs.\nSince LLMs widely serve both classification and generation tasks, our discussion categorizes the attribution methods accordingly.\nAfter that, we explore case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss future work of designing novel post-hoc explanation methods for LLMs.\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Literature Review of Attribution Methods",
            "text": "The attribution-based explanation quantifies the importance of each input feature that contributes to making predictions.\nGiven a language model  with a prediction  according to the -words input prompt , the explainer  assesses the influence of input words in  as . Typically, the sign of  indicates word  positively or negatively influences , and a greater value of  indicates a stronger impact.\nIn text classification,  denotes a specific class label. In text generation,  represents a varying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly applied to the generation task.\nThe primary distinction between them is that: classification is limited to a specific set of predictions, while generation encompasses an endless array of possibilities.\nFor instance, in sentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates the positivity of input text by adding a linear layer and a sigmoid function on top of the language model.\nHowever, in the generative setting, the model can express this positivity in numerous expressions, such as “the reviewer definitely loves this movie” and “it is a strong positive movie review”.\nThis distinction poses a unique challenge in adapting explanation methods from classification to generation tasks.\nIn the following, we review related works based on the scenarios they are applicable to."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Attributing Inputs for Label Classification",
            "text": "Common attribution methods (Du et al., 2019a  ###reference_b62###; Murdoch et al., 2019  ###reference_b175###) developed for traditional deep models include gradient-based methods, perturbation-based methods, surrogate methods, and decomposition methods.\nWe introduce the general idea and representative examples for each category, followed by the analysis of their suitability for explaining large language models.\nPerturbation-based Explanation.\nPerturbation-based methods assess the importance of input features by perturbing them and monitoring changes in prediction confidence, i.e., , where  refers to the input sequence with the -th feature being perturbed.\nEach feature could refer to a word (Li et al., 2016a  ###reference_b132###), a phrase (Wu et al., 2020b  ###reference_b259###), or a word embedding (Li et al., 2016b  ###reference_b133###).\nThe underlying principle is that perturbing a more important feature should result in a more pronounced alteration in the model’s prediction confidence.\nHowever, this method has limitations, particularly in its assumption that features are independent, which is not always the case with textual data due to word inter-dependencies. Additionally, it is computationally intensive for explaining LLMs, requiring  inferences for an input of  words.\nGradient-based Explanation.\nGradient-based methods offer a computationally efficient approach for estimating model sensitivity to input features based on gradients , where  refers to the embedding of word .\nSome methods employ the -norm of gradients to assess word importance (Li et al., 2016a  ###reference_b132###), i.e., . This approach only requires a single inference and one backpropagation pass.\nSome extended methods multiply the gradient with the word embedding (Kindermans et al.,  ###reference_b116###; Ebrahimi et al., 2018  ###reference_b66###; Mohebbi et al., 2021  ###reference_b172###), i.e., .\nThese methods may yield explanations with limited faithfulness for deep models (Shrikumar et al., 2017  ###reference_b208###), as gradients only reflect the local relationship between input variation and output variation.\nTo address this, Integrated Gradients (IG) has been proposed (Sundararajan et al., 2017  ###reference_b219###; Sikdar et al., 2021  ###reference_b210###; Sanyal & Ren, 2021  ###reference_b199###; Enguehard, 2023  ###reference_b69###), which accumulates gradients as input transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-based Explanation.\nSurrogate-based explanation methods understand complex models by constructing a simpler model  trained on , where  denotes a dataset constructed for the target instance ;  is usually obtained by perturbing , and . The surrogate model , ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the decision boundary of the target model  for a specific instance .\nNotable examples include LIME (Ribeiro et al., 2016  ###reference_b191###), SHAP (Lundberg & Lee, 2017  ###reference_b158###), and TransSHAP (Kokalj et al., 2021  ###reference_b119###), where the first two are designed for general deep neural networks and the last one is tailored for Transformer-based language models.\nNevertheless, a significant limitation of them is their intensive reliance on repeated interactions with the target model, a process that is impractical for LLMs.\nDecomposition-based Explanation.\nDecomposition-based methods assign linearly additive relevance scores to inputs, effectively breaking down the model’s prediction. Layer-wise Relevance Propagation (Montavon et al., 2019  ###reference_b174###) and Taylor-type Decomposition (Montavon et al., 2017  ###reference_b173###) are well-known techniques for computing these relevance scores. These methods have been adapted for Transformer-based language models in various research (Voita et al., 2019  ###reference_b232###; 2020  ###reference_b233###; Wu & Ong, 2021  ###reference_b258###). However, a primary challenge in implementing decomposition-based explanations is the need for tailored decomposition strategies to accommodate different model architectures. Although many large language models are based on the Transformer framework, there are key variations between them, such as LLaMA (Touvron et al., 2023a  ###reference_b227###) and GPT (OpenAI, 2023  ###reference_b179###), particularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a limitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods are not always suitable for LLMs.\nIn particular, the perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM responses to the input prompts, while the surrogate-based and decomposition-based methods become significantly challenging to do so.\nSpecifically, surrogate-based methods suppose that an explainable small model could approximate the decision boundary of the target model around a local example, but there are limited explainable models for the text generation task.\nMeanwhile, decomposition-based methods require designing decomposition strategies for different layers, which is challenging for big LLM architectures.\nAnother primary concern is their significant demand for computing resources.\nGiven an -words input prompt and an -words output response, we present the time complexity of several representative explanation methods in Table 1  ###reference_###.\nIt demonstrates that existing methods either require a large number of forward operations or backward operations.\nTherefore, improving the efficiency of the attribution-based explanation is an important direction for future research and development."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Attributing Inputs for Text Generation",
            "text": "###figure_2### The explanation of generative models can be defined as attributing the overall confidence  to the input , where  denotes the generated response  with  words.\nOne method to achieve this is by treating the text generation process as a sequence of word-level classification tasks. This perspective allows for the application of existing classification-based explanation techniques to assess the influence of each input word  in relation to each output word , resulting in a corresponding attribution score .\nAfter gathering the attributions  for , we perform an aggregation to determine the overall contribution of each input word . This is accomplished by aggregating the individual attributions for all output words corresponding to the input word, denoted as .\nThe simplest approach for this aggregation is to average the attributions assigned to each input word across the different output words (Selvaraju et al., 2016  ###reference_b204###).\nHowever, Wu et al. (2023  ###reference_b257###) observe that attribution scores from different output words are not inherently comparable. For example, the attribution scores for function words (e.g., “the”, “is”, “have”) are often disproportionately larger than the scores for content words with clear semantic meaning (e.g., verbs and nouns).\nTherefore, it is necessary to normalize the scores prior to the aggregation, so that the scores  become comparable for .\nFigure 2  ###reference_### plots the normalized scores of an example case, where each index in the Y-axis refers to an input prompt token, while that in the X-axis is an output response token.\nA greater normalized attribution score is brighter.\nIn this example, the user attempts to direct the model to output information that does not exist, namely the French president in 1250. The model successfully realizes that this thing does not exist and refuses to answer. The model response can be realized as three parts, “There was no”, “president in France”, and “in 1250”. According to the figure, the first span is generated heavily because of the tokens “Who” and “president”, while the model uses both “France” and “1250” to respond to the second span “president in France”. Finally, the model emphasizes the date “1250” again by referencing the same information from the prompt. Overall, these explanations align with human understanding and highlight the usage of this method in the future.\nHowever, current research on attribution-based explaining for generative LLMs is still in its early stages, and only a limited number of methods have been proposed."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Case Studies: Usability of Attribution Methods for LLMs",
            "text": "###figure_3### The attribution map offers a partial insight into the operational mechanics of LLMs (Chandrasekaran et al., 2018  ###reference_b32###; Hase & Bansal, 2020  ###reference_b88###; Ye & Durrett, 2022a  ###reference_b272###). Accordingly, we propose a general pipeline that leverages attribution scores to analyze LLM behaviors, as shown in Figure 3  ###reference_###.\nFirst, given the target LLM and an input prompt, we compute attribution scores of input tokens relative to the output tokens.\nSecond, we extract a feature vector from the attribution map, tailored to the requirements of the diagnostic task at hand.\nThird, we train a light-weight predictor (e.g., a classifier) to diagnose whether the model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how attribution scores could be utilized to assess LLM response quality (Adlakha et al., 2023  ###reference_b3###).\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_###  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 LLM Response Quality Evaluation with Explanations",
            "text": "This case study explores the use of attribution-based explanations as evidence for assessing the quality of LLM-generated responses.\nHere, “quality” is specifically measured by the responses’ accuracy.\nWe hypothesize that responses generated from correct rationales are likely to be more accurate.\nOur method involves comparing the model’s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such as medical question answering.\nWe employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###  ###reference_b112###) in this case study.\nMultiRC presents a more significant challenge than other datasets: it requires the system to answer questions based on multiple sentences from a given paragraph. The answers may not directly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each MultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant for answering the question. Our study specifically employs its test subset with 950 samples in total. We consider 80% in which are the training set and we report the results evaluated on the rest samples.\nWe apply the Importance Density Score method (Wu et al., 2023  ###reference_b257###  ###reference_b257###  ###reference_b257###) to estimate the importance of each word in the provided paragraph. Specifically, given -word paragraph  and the -word response , the attribution from each input word  to each output word  is defined as , where  indicates the formatted input prompt concatenated with the first  response word,  is the language model, and  indicates the input static embedding of . This pairwise attribution score is then normalized as . Any normalized attribution  is equal or less than  will is forced to 0. The overall attribution score on word  is defined as its attribution density, i.e., . In our experiments, we let , , , and consider Vicuna-7B-v1.1 as our language model . By averaging these word attribution scores, we reach the importance of each sentence. The top-K sentences with the highest importance scores are selected as the explanation for each instance.\nThe explanation is then concatenated with the output response and fed to a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-base (Sanh et al., 2019  ###reference_b197###  ###reference_b197###  ###reference_b197###), and train it over 3 epochs with learning rate  and weight decay . To compare with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to train the classifier.\nFollowing previous studies (Khashabi et al., 2018  ###reference_b112###  ###reference_b112###  ###reference_b112###; DeYoung et al., 2019  ###reference_b55###  ###reference_b55###  ###reference_b55###), we evaluate the accuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2  ###reference_###  ###reference_###  ###reference_### reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nIn Table 2  ###reference_###  ###reference_###  ###reference_###, we observe that isolating the rationales from the full context could best help the classifier identify the response quality.\nIt is evident that explanations align more closely with human-annotated rationales when associated with correct responses. In particular, the precision and recall metrics for explanations derived from correct answers surpass those associated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness of attribution-based explanations in estimating the accuracy of responses generated by LLMs."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Hallucination Detection with Attribution Explanations",
            "text": "This case study explores a different aspect of LLM generation quality, focusing on the presence of hallucinations in LLM-generated responses. We show that attribution-based explanations can serve as indicators to detect LLM hallucinations.\nHallucinations are defined as responses that contain information conflicting with or unverifiable by factual knowledge (Li et al., 2023c  ###reference_b134###; Ji et al., 2023  ###reference_b107###).\nFor instance, if a model is asked about a fictitious entity like “King Renoit” and responds with a narrative about “The Three Musketeers”, claiming it pertains to the nonexistent king, it illustrates a hallucination.\nThis tendency, particularly pronounced in instruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises when direct commands (“tell me a story”) significantly influence the generation process, while the instruction’s subject (“about King Renoit”) is neglected.\nBased on this insight, we develop a hallucination detector according to the distribution of attribution scores over different types of prompting words.\nWe use the Hallucination Evaluation Benchmark (Li et al., 2024a  ###reference_b135###  ###reference_b135###  ###reference_b135###) in this case study. Each instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al., 2022  ###reference_b180###  ###reference_b180###  ###reference_b180###), and the knowledge related to the response. Each piece of knowledge has a human annotation about whether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered a hallucination response. This study focuses on 632 less confused examples from the benchmark, each of which has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form a training set and the rest form the testing set.\nGiven a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density Scores (Wu et al., 2023  ###reference_b257###  ###reference_b257###  ###reference_b257###) to compute the attribution scores of Vicuna.\nWe then use the NLTK package to identify the part-of-speech (POS) tag of each query word.\nFinally, each query-response pair is represented with an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain type of POS tagging.\nWe develop a Support Vector Machine classifier based on the POS tagging attribution scores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well as accuracy over all samples in Table  3  ###reference_###  ###reference_###  ###reference_###.\nTo compare with this, we also leverage a fine-tuned model and a prompting-based method to serve as the hallucination detector baselines.\nIn the table, we first observe that all methods have demonstrated a greater performance than the Random strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4, the attribution score for our method is calculated using a smaller 7-billion-parameter language model. This demonstrates the practicality and efficiency of our approach in weak-to-strong generalization as we could diagnose large language models with smaller models.\nFuture work could consider extracting more effective features and using more powerful classifiers."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 How to Identify and Explain the Semantics of Output?",
            "text": "The attribution function  is no longer faithfully attributing the model predictions in the human-interested semantic level since the model could express the same semantic meaning with various responses.\nSpecifically, the model could assign a lower confidence to its original response than the new one, while both responses share the same semantic meaning.\nThis is a significant difference compared with the traditional classification problem, where target label sets are manually designed so that a lower  indicates the model is less confident in predicting a specific semantic concept.\nTaking the sentiment analysis task as an example, an LLM may generate two different responses sharing the same predicted concepts, such as “it is a positive review” and “the audience thinks the movie is great”.\nCurrent attribution-based explanations concentrate on the literal changes in generated responses, but they do not study how the semantic meanings of these responses change.\nTherefore, they do not provide sufficient explanations of model-generated responses at a semantic level. In this case, the semantic level is which words of the input review lead the model to believe it is positive.\nFuture work may tackle this challenge by proposing metrics to evaluate the semantic differences in responses.\n###figure_4###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Explaining LLM Predictions Beyond Attribution",
            "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attribution methods.\nAttribution methods aim to explain model output with the contribution of input features. This explanation task is meaningful for conventional machine learning (ML) models whose outputs are usually individual decisions with clear formats (e.g., classification, regression, object detection). The decisions are highly dependent on the input features.\nHowever, LLMs differ from traditional ML models in two aspects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running an LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters, which are independent of inputs.\nThese unique properties give rise to novel explanation paradigms.\nFor the first aspect, an explanation task of interest would be to understand the uncertainty of LLM generation. For example, researchers (Ahdritz et al., 2024  ###reference_b4###; Varshney et al., 2023  ###reference_b230###; Su et al., 2024  ###reference_b217###) leverage the prediction perplexity to check whether the LLM is confident during generation, identifying potential errors in less confident predictions.\nSecond, attributing LLM predictions to their encoded knowledge instead of input patterns could provide a new perspective. Some researchers (Yin et al., 2024a  ###reference_b274###) propose the knowledge-boundary detection task to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute the prediction to specific knowledge, so humans cannot verify the prediction process with their results yet."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM Diagnosis and Enhancement via Interpreting Model Components",
            "text": "This section discusses the XAI methods that interpret the internal components of large language models. Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and enhancing the design of language models.\nLLMs adopt transformers as the basic architecture, which typically comprises two types of major components: self-attention layers and feed-forward layers. In the following, we review the research that focuses on interpreting each of these components respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Understanding the Self-Attention Module",
            "text": "A multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-word relations, which are modeled with weights .\nSpecifically, the relation of words  and  is computed as , where  are contextual embeddings of the words.\nThe most straightforward interpretation is analyzing the attention score matrix  given an input sequence to study the relations between words (Vig, 2019  ###reference_b231###; Hoover et al., 2020  ###reference_b96###).\nIn practice, these intuitive explanations would be majorly used to present case studies via visualization.\nWith this strategy, Wang et al. (2023b  ###reference_b240###) conduct case studies on in-context sentiment analysis, where they find that the label words from the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate information from the examples to generate informative representations from the lower layers, while the deeper layers utilize these representations to make final predictions.\nThis insight motivates them to reweight the attention scores of these anchors to achieve better inference accuracy.\nSome researchers (Dar et al., 2023  ###reference_b51###; Wu et al., 2023  ###reference_b257###) extend this framework to globally analyze the attention weights  and  by feeding the static word embeddings of words from an interested vocabulary, instead of their contextual embeddings.\nFor example, with this approach, Wu et al. (2023  ###reference_b257###) find that instruction tuning empowers LLMs to follow human intentions by encouraging them to encode more word-word relations related to instruction words.\nOn the other hand, some mathematical models are proposed to theoretically explain the self-attention mechanism, such as Sparse Distributed Memory (Bricken & Pehlevan, 2021  ###reference_b24###) and Transformer Circuits (Elhage et al., 2021  ###reference_b67###). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based models, which breaks models down into human-understandable pieces.\nAlthough these theoretical analyses on self-attention solids a foundation for future research, their direct application is largely underexplored."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Understanding the Feed-Forward Module",
            "text": "A feed-forward network is formalized as , where  is the intermediate contextual representation of an input word,  is a non-linear operation, and  are model parameters.\nFeed-forward networks can be understood as key-value memories (Sukhbaatar et al., 2015  ###reference_b218###; Geva et al., 2021  ###reference_b75###), where each key or value is defined as  and , respectively. That is, each feed-forward network obtains  key-value pairs, called memories.\nOne simple way to interpret the semantic meaning of memory is collecting the words that could maximally activate the key or value vector of that piece of the memory (Geva et al., 2021  ###reference_b75###; Dar et al., 2023  ###reference_b51###), which has demonstrated strong interpretability of the extracted word lists.\nHowever, it is critical to be aware that the key or value vectors are polysemantic (Arora et al., 2018  ###reference_b10###; Scherlis et al., 2022  ###reference_b200###; Bricken et al., 2023  ###reference_b25###), indicating that this simple approach might not provide concise explanations for each key-value pair.\nIt has been shown that the word list of each key-value pair has 3.6 human interpretable patterns on average (Geva et al., 2021  ###reference_b75###).\nTo alleviate the limited interpretability caused by the nature of polysemantic, Wu et al. (2023  ###reference_b257###) propose to interpret the principal components of these key or value vectors, leading to a more concise explanation for each word list, such as “medical abbreviations” and “programming tasks and actions”.\nOther work examines individual memories by measuring the changes of predictions after perturbing their corresponding activations, where it reveals that some memories encode specific knowledge (Dai et al., 2022  ###reference_b46###) and some others capture general concepts (Wang et al., 2022b  ###reference_b244###).\nBy leveraging the explanations of key-value memories, we could locate and update the memories associated with a specific piece of knowledge to perform model editing (Dai et al., 2022  ###reference_b46###; Meng et al., 2022a  ###reference_b167###; b  ###reference_b168###; Hase et al., 2024  ###reference_b89###), i.e., modifying outdated or incorrect knowledge.\nAnother usage of these weight explanations is model pruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020  ###reference_b49###).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,  or ) to interpret their functionality.\nThe probing technique is the most popular way for this purpose (Belinkov et al., 2018  ###reference_b18###; Tenney et al., 2018  ###reference_b223###; Jawahar et al., 2019  ###reference_b106###; Rogers et al., 2021  ###reference_b193###), identifying whether a specific concept is encoded within the representations. The basic idea is developing an auxiliary classifier  to map from the representations  to the interested concept space , such as syntax and part-of-speech knowledge, and the performance of  interprets how much information encoded in  is related to the concepts in .\nThis technique motivates developing better parameter-efficient (Chen et al., 2022  ###reference_b35###), domain-specific (Das et al., 2023  ###reference_b52###), and robust (Bai et al., 2021  ###reference_b13###; Wang et al., 2023a  ###reference_b238###) LLMs.\nRecent studies (Chen et al., 2023a  ###reference_b34###; Ahdritz et al., 2024  ###reference_b4###) also apply the probing method to detect the knowledge boundary of a LLM so that the hallucinated responses could be reduced.\nSome researchers (Bricken et al., 2023  ###reference_b25###; Cunningham et al., 2023  ###reference_b44###) point out another direction to interpret the model hidden activations, called dictionary learning, which is motivated by the assumption of superposition (Elhage et al., 2022  ###reference_b68###; Sharkey et al., 2022  ###reference_b205###).\nThe superposition assumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending the limitations imposed by the dimensionality of the representation space.\nTherefore, the researchers aim to reconstruct and interpret these features to understand the internals of the model.\nPractically, they develop a sparse auto-encoder  to reconstruct the representations , which shows that humans could well interpret the learned sparse features of  according to their most activation words.\nTheir research shows that this method could be used for more controllable generation.\nSpecifically, if forcing a sparse feature to be activated, then the language model  would change its response to perform the particular behavior of that sparse feature.\nFor example, given “1,2,3,4,5,6,7,8,9,10” as input, the model originally generates numbers as output.\nHowever, when they are forced to magnify the activations of a sparse feature called “DNA”, the model changes its output to “AGACCAGAGAGAAC”.\nIn general, while the explanation techniques for feed-forward networks primarily offer insights for model development, they have also demonstrated promising applications in areas such as model editing and controllable generation."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Challenges",
            "text": "Interpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be tackled in this direction."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Complexity of Individual Models and Their Interactions",
            "text": "The transformer-based language model contains two types of modules that collaborate based on the residual mechanism (He et al., 2016  ###reference_b90###), which enables later modules to utilize, enhance, and/or discard outputs from preceding modules.\nFormally, the output of the -th module is denoted as , where  could be a self-attention module or a feed-forward network.\nResearch in this area aims to interpret how different modules  and  work together for .\nPilot studies (Elhage et al., 2021  ###reference_b67###; Olsson et al., 2022  ###reference_b178###) find that stacked self-attention modules could form Induction Heads, which demonstrate a strong correlation with the in-context learning capability.\nSpecifically, the induction head encourages the model to predict the word “B” followed by a sequence “AB…A”.\nTheir study finds a specific phase during pre-training LLMs where both induction heads and the in-context learning capability emerge from the model.\nFollowing this track, researchers observe diverse functional heads within LLMs for different tasks, such as “Name Mover Head” and “Duplicate Token Head” for the object identification task (Wang et al., 2022a  ###reference_b239###), “Single Letter Head” and “Correct Letter Head” for the multiple-choice question answering task (Lieberum et al., 2023  ###reference_b146###), and “Capitalize Head” as well as “Antonym Head” for the general purpose tasks (Todd et al., 2023  ###reference_b226###).\nAlthough these studies have indeed deepened our understanding of cross-module effects, their analyses are grounded on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Nature of Polysemantic and Superposition Assumption",
            "text": "Interpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in analyzing large language models since a single neuron could be activated by multiple and diverse meanings, called polysemantic (Arora et al., 2018  ###reference_b10###; Scherlis et al., 2022  ###reference_b200###; Bricken et al., 2023  ###reference_b25###).\nThis nature leads to poor interpretability: explaining a single neuron usually does not reflect a concise human concept.\nSome researchers (Elhage et al., 2022  ###reference_b68###; Sharkey et al., 2022  ###reference_b205###) assume that this phenomenon is caused by the superposition of an over-complete set of features learned by the models.\nBased on this assumption, we may reach another level of explanation by decomposing the model weights to reconstruct a large number of features.\nHowever, the two critical problems of this approach are still unclear: (1) How do we ensure our reconstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our reconstructed features with human language?"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Debugging with Sample-based Explanation",
            "text": "In this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The utility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to the training samples can provide evidence for the generation results, which facilitates model debugging in cases of errors and increases the trustworthiness of the model from users when the outcomes are accurate. In addition, it can also help researchers understand how LLMs generalize from training samples. If the outputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might suggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing training samples are abstractly related, it could indicate that LLMs can understand the concepts and generate responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, including gradient-based methods and embedding-based methods, as well as some preliminary explorations to generalize them to LLMs. We then analyze the challenges associated with generalizing the above strategies to LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss the insights to address the challenges, as well as open challenges worthy of further investigation."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Literature Review of Sample-based Explanation",
            "text": "In this section, we denote the input space and output space as  and , respectively. In the context of large language models (LLMs),  is the space of token sequences known as the prompts, and  could be the space of discrete labels in classification tasks or the space of token sequences as output in generation tasks***Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked tokens in  (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in . Therefore, in some works,  is omitted, and only  is included for discussions..\nAccordingly, we have a training dataset  with  samples drawn from the joint space , on which an LLM model  is trained with pretrained parameters . We also have a test sample  of interest, where we want to explain the generation of  from  based on training samples in  (which can be viewed as the information source). The goal of sample-based explanation is to measure the influence of a training sample  or a certain segment within , such that the generation of LLMs can be well-explained and backed up by the selected training samples."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Influence Function-based Methods",
            "text": "One strategy to quantify the influence of a training sample  in the dataset  to a test sample  is through the influence function (Koh & Liang, 2017  ###reference_b117###; Han et al., 2020  ###reference_b86###). It measures the change of the prediction loss  for the test sample , when the training sample  undergoes a hypothetical modification in the dataset  during model training. This modification results in an altered set of optimal model parameters, denoted as . The most common modification of a training sample is to remove it from the dataset, where the influence of the removal of a training sample  on the loss at test sample  can be computed as follows:\nwhere  is the gradient of the loss function  on the test sample  evaluated at the optimal parameters , and  denotes the Hessian matrix of the LLM model at parameter .\nIf we denote the number of parameters in  as , the naïve inversion of the Hessian matrix  leads to  time complexity and  space complexity (Schioppa et al., 2022  ###reference_b201###), which is clearly infeasible for large models. To improve efficiency, Koh & Liang (2017  ###reference_b117###) adopt an iterative approximation process, i.e., LiSSA (Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq. (1  ###reference_###), where the memory complexity can be reduced to  and time complexity to  ( is the number of iterations). To further reduce the complexity, Pruthi et al. (2020  ###reference_b183###) propose an alternative to Eq. (1  ###reference_###), i.e., TracIn, which measures the influence of  on  by calculating the total reduction of the loss on  whenever  is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nwhere  is the -th mini-batch fed into the model during training,  is the parameter checkpoint at the -th step,  is the step size, and  is the size of the mini-batch. According to the above equation, TracIn only leverages gradient terms, where Hessian  is removed from the influence measurement. This substantially improves the efficiency. However, such complexity is still prohibitive for large models from both the computational and memory perspectives. In addition, TracIn can only estimate the influence of adding/removing the sample to the loss, where variants of the vanilla influence function defined in Eq. (1  ###reference_###) can measure the influence of other modifications of the training sample , such as perturbation (e.g., masking out a segment of a document ). To adapt the vanilla influence function of Eq. (1  ###reference_###) to explain transformers, Schioppa et al. (2022  ###reference_b201###) propose to use Alnordi iteration (Arnoldi, 1951  ###reference_b9###) to find the dominant eigenvalues and eigenvectors of the Hessian matrix on randomly sampled subsets , with . In such a case, the diagonalized Hessian can be cheaply cached and inverted, where the computational and memory complexity can be substantially reduced.\nPrevious work mainly focuses on reducing the complexity of calculating the influence of a single training sample. Observing that finding the most influential training sample on  needs to iterate Eq. (1  ###reference_###) overall  training samples, Guo et al. (2021  ###reference_b80###) propose to use fast KNN to pre-filter a small subset of influence-worthy data points from  as candidates to explain small pretrained language models, whereas Han & Tsvetkov (2022  ###reference_b85###) propose to iteratively find a small subset  whose gradient is the most similar to that of the downstream task examples.\nRecently, Grosse et al. (2023  ###reference_b79###) propose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by the multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are fixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al., 2023d  ###reference_b242###). In addition, based on the assumption that weights from different MLP layers are independent, the EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nwhere  denotes the weights of the -th MLP layer, and  is the EK-FAC approximated Gauss-Newton Hessian for . Since the inversion of  small  matrices (i.e., ) is substantially more efficient than the inversion of a large  matrix (i.e., ),  can be adaptable to very large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence function has been used to select a small subset of training samples given few-shot validation samples for a specific downstream task, where the training overhead can be substantially improved (Xia et al., 2024  ###reference_b260###)."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Embedding-based Methods",
            "text": "Another strategy for sample-based explanation involves leveraging the hidden representations within the transformer architecture, which is recognized for encoding low-level semantics from textual data, to calculate the semantic similarity between training and test samples. The similarity can also be used to measure the influence of  on  as explanations (Rajani et al., 2019  ###reference_b188###). Specifically, Akyurek et al. (2022  ###reference_b5###) propose to represent the training sample  and test sample  by concatenating the input and output as , . The concatenation is feasible for generation tasks where the output  lies in the same token sequence space as the input prompt . The similarity between  and  can then be calculated as follows:\nwhere  is the sub-network that outputs the -th layer intermediate activation of the pretrained LLM . The Eq. (4  ###reference_###) has a similar form as the vanilla influence function defined in Eq. (1  ###reference_###) as well as its TracIn alternative defined in Eq. (2  ###reference_###), which assigns a score  for the explainee  for each training sample  in the dataset  as the explanation confidence of the sample .\nCompared with the influence function methods introduced in the previous part, embedding-based methods are computationally efficient, as for each explainee , the explanation score from a training sample  requires only one forward pass of the transformer network. In addition, the calculation can be easily paralleled for different training samples. However, the disadvantage is also evident: These methods lack a theoretical foundation and may fail to identify important training samples that may not be semantically similar to the test sample. Consider the following toy example: Training samples  = (“1+1=”, “2”) and  = (“2+2=”, “4”) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the model with  = “100+100” gives the results =“200”. However, the embeddings between the test sample  and the two training samples  and  can be very different when calculated via Eq. (4  ###reference_###) (Akyurek et al., 2022  ###reference_b5###). Therefore, embedding-based methods may not be able to faithfully find the training samples where the explanations require generalization ability beyond semantic similarity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Case Study: EK-FAC-based Influence Estimation",
            "text": "In this part, we implement the EK-FAC-approximated influence function proposed in Grosse et al. (2023  ###reference_b79###), and verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford et al., 2019  ###reference_b185###), LLaMA2-7B (Touvron et al., 2023b  ###reference_b228###), Mistral-7B (Jiang et al., 2023  ###reference_b108###), and LLaMA2-13B."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Experimental Design",
            "text": "We use the SciFact dataset (Wadden et al., 2020  ###reference_b235###) as the corpora, which contains the abstract of 5,183 papers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018  ###reference_b155###) is used as the optimizer, and the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from the corpora to estimate the (i) uncentered covariance matrices of the activations and pre-activation pseudo-gradients , , and (ii) the variances of the projected pseudo-gradient  for each selected dense layer , and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023  ###reference_b79###)). We select the c_fc layer for GPT2-1.5B, and gate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B***All the implementation and layer names are based on the huggingface transformers, where the details can be found in https://huggingface.co/docs/transformers/en/index  ###reference_n/index###..\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name SciFact-Inf. Specifically, for the -th selected sample  (here  as label equals the input in language modeling), we use the first three sentences in , i.e., , to generate a completion  with the finetuned LLM (here,  does not equal the remaining sentences in ), and we aim to explain the generation of  from  with the finetuned LLM with the training samples via EK-FAC approximated influence scores defined in Eq. (3  ###reference_###). Ideally, the -th training sample  itself should be the most influential sample w.r.t. the generation of  for test sample , which facilitates quantitative analysis of the effectiveness of Eq. (3  ###reference_###).\nIn our implementation, for each test sample , we first calculate the EK-FAC approximated HVP part of the influence , i.e., , which is shared for all training samples . Specifically, we record the layer-wise gradient  and calculate the HVP with the cached ,  as Eq. (21) in Grosse et al. (2023  ###reference_b79###). We then go through candidate training samples (1 positive and 99 negative), calculate the gradient , and take inner-product with the approximate HVP as the layer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3  ###reference_###) as the total influence . We rank the influence and calculate the top- hit rate of the positive training sample."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Results and Analysis",
            "text": "The experimental results are summarized in Table 4  ###reference_###. From Table 4  ###reference_### we can find that, the EK-FAC approximated influence function achieves a good accuracy in finding the training sample that has the greatest influence on the generation of a test sample, even if only the influences mediated by a small part of dense layers are considered. In addition, we find that the main computational bottleneck in calculating the EK-FAC-based influence is to estimate the covariances ,  and variance , which can take hours when 500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to calculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B LLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming the independence of different dense layers and using EK-FAC to simplify the computation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Challenges",
            "text": "Overall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area. Open questions need to be addressed to further advance the field. In this section, we identify three main challenges as follows, which can serve as directions for future explorations."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Strong Assumptions for Scalability",
            "text": "The unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1  ###reference_###) induces both high computational and space complexity. To address the bottleneck, strong assumptions are usually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020  ###reference_b183###) simplifies the second-order term in Eq. (1  ###reference_###) via first-order approximation. Schioppa et al. (2022  ###reference_b201###) assume the Hessian to be low rank. Grosse et al. (2023  ###reference_b79###) that assume that the weights from different layers of the LLMs are independent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to approximate the influence function. From the above analysis, we can find that while the method from Grosse et al. (2023  ###reference_b79###) has the best scalability, it also has the strongest assumption, which may fail to hold in practice. While highly efficient to compute, embedding-based methods make the implicit assumption that semantics similarly implies explainability, which we have demonstrated may not always be the case. Therefore, how to improve the scalability with weak assumptions needs to be investigated in the future."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Explainability v.s. Understandability",
            "text": "Despite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific training sample as the explanation for LLM generation, the understandability of the identified sample can still be weak, where the connection between the selected training samples and the generation may not be understandable to human beings. Specifically, Grosse et al. (2023  ###reference_b79###) cautions that the sign of influence score of the training tokens may be difficult for humans to connect to the positive or negative influence on the generation results. This severely jeopardizes the usability of the identified training samples. In addition, Grosse et al. (2023  ###reference_b79###) also found that, since LLMs are usually not trained to the minimum to avoid overfitting (and due to overparameterization, the number of local minimums may be large), the connection between influence defined in Eq. (1  ###reference_###) with the counterfactual loss of removing the sample  at  is also weak. For the embedding-based methods, since most LLM models are black box transformer models, the similarity of embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the interpretability of the identified training samples, such that tracing back becomes more meaningful."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 LLM-Oriented Sample-based Explanations",
            "text": "Finally, we observed that both gradient-based and embedding-based methods are loosely connected to the LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020  ###reference_b183###) are designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the embedding-based method proposed in Akyurek et al. (2022  ###reference_b5###) is applicable to most machine learning models with latent representations. Grosse et al. (2023  ###reference_b79###) considers the specialty of LLMs by utilizing the knowledge neuron assumption of the backbone transformers (Wang et al., 2023d  ###reference_b242###) to simplify the influence function, where the weights considered are constrained to the MLP layers, which may not fully utilize the property of transformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to design LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead or to improve the explanation quality) is highly promising for future work."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Explainability for Trustworthy LLMs and Human Alignment",
            "text": "In previous sections, we explore the use of explanation techniques for assessing and improving the performance of LLMs.\nIn this section, we shift the focus towards examining LLM trustworthiness.\nAs LLMs are increasingly integrated into various applications of daily life, including high-stakes areas like healthcare, finance, and legal advice, it is crucial that their responses not only are accurate but also align with human ethical standards and safety protocols (Liu et al., 2023b  ###reference_b152###; Li et al., 2023f  ###reference_b144###).\nThus, the need arises to extend the scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness. Herein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in assessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty.\nIt is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving as a foundational tool for addressing other trustworthiness concerns."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Security",
            "text": "LLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching phishing attacks, and poisoning training data (Derner et al., 2023  ###reference_b54###).\nFor enhanced safety, LLMs are designed to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding prompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent these restriction measures and manipulate LLMs into producing malicious contents.\nMalevolent users (i.e., attackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over rejections (Liu et al., 2023c  ###reference_b153###; Li et al., 2023a  ###reference_b130###).\nFor example, through Prefix Injection, attackers can use out-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022  ###reference_b236###; Wei et al., 2023  ###reference_b251###). Another approach, called Refuse Suppression, involves directing or persuading models to ignore established safety protocols (Wei et al., 2023  ###reference_b251###; Zeng et al., 2024  ###reference_b281###), where the instruction following ability is then employed to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack success rates and significant time costs (Li et al., 2024c  ###reference_b141###).\nThus, by understanding and engineering latent representations of LLMs, explanation methods provide a viable way to design advanced attacks and discover the potential vulnerabilities of LLMs (Liu et al., 2021  ###reference_b150###).\nFor example, a recent work extracts “safety patterns” via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be captured from the activation differences between malicious queries and benign queries. The salient portion of difference vectors’ dimensions is localized and utilized to generate features of safety patterns. The safety patterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c  ###reference_b141###).\nBesides, a deeper understanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain et al. (2023  ###reference_b104###) use networking pruning, attention map activation, and probing classifiers to track the changes of model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the capabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks. This finding casts doubt on the robustness of current safety alignments in LLMs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Privacy",
            "text": "Recent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the model away from its standard chatbot-style generation (Nasr et al., 2023  ###reference_b177###). The risk of private data exposure through such means poses a serious challenge to the development of ethically responsible models. This issue is compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs into operating in an unconventional “developer mode” via out-of-distribution prompts (Li et al., 2023a  ###reference_b130###).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021  ###reference_b147###), are impractical as defenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing sensitive data, and (2) establishing safeguards against the release of sensitive information during content generation.\nThe latter can employ techniques used in jailbreak defenses, treating prompts that solicit private information as potentially malicious.\nThe former approach requires identifying whether LLMs possess specific knowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can provide answers. However, this approach faces limitations due to LLMs’ sensitivity to the phrasing of QA prompts, while the optimal prompt is usually unknown.\nTo tackle the challenge, explanatory techniques can serve as a tool to confirm whether LLMs have internalized certain knowledge.\nFor instance, via explaining the relation between factual knowledge and neuron activations (Meng et al., 2022a  ###reference_b167###; Dai et al., 2022  ###reference_b46###; Hase et al., 2024  ###reference_b89###), we may investigate whether and where a piece of factual knowledge is stored within transformers.\nIn addition, Yin et al. (2024a  ###reference_b274###) recently proposes the concept of “knowledge boundary” and develops a gradient-based method to explore whether LLMs master certain knowledge independent of the input prompt."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Fairness",
            "text": "Despite LLMs’ powerful generation capabilities, their widespread applications also bring concerns about exacerbating bias issues in society, as LLMs are able to learn social biases within human-generated corpus (Gallegos et al., 2023  ###reference_b71###). For example, in a gender bias case, “[He] is a doctor” is much more likely than “[She] is a doctor” because of the gender stereotype. In this subsection, we focus on fairness issues that refer to biases related to race, gender, and age within human communities (Li et al., 2023g  ###reference_b145###).\nThere is a rich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023  ###reference_b2###). Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing on unraveling the mechanisms through which biases are embedded into LLMs.\nA research direction within this domain is the examination of biased attention heads. For instance, Ma et al. (2023  ###reference_b162###) detect stereotype encodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg & Lee, 2017  ###reference_b158###). The results unveil that approximately 15% to 30% of attention heads across six transformer-based models are linked to stereotypes. These attention heads tend to specialize in maintaining various stereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring head biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a  ###reference_b267###).\nFurthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a  ###reference_b295###). Typically, specific templates relevant to a given concept or function are designed beforehand. Then, representations closely aligned with the concepts or functions are examined using principal component analysis (PCA). From this analysis, a vector is derived from the first principal component to predict a certain bias.\nTo achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias models. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through various approaches. For example, a recent work attempts to alter biased embeddings with minimal alterations to make them orthogonal to neutral embeddings (Rakshit et al., 2024  ###reference_b189###). Additionally, some studies concentrate on removing biases at the level of attention heads. Ma et al. (2023  ###reference_b162###) address this by pruning attention heads that significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh et al., 2020  ###reference_b198###) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022  ###reference_b109###). Beyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these neurons (Yu et al., 2023  ###reference_b277###). Besides, bias mitigation can also be approached from a data-centric perspective using a few training samples (Thakur et al., 2023  ###reference_b224###). This work uses a pre-trained model to find the most biased training examples, and then modifies these examples to fine-tune the model."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Toxicity",
            "text": "Toxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are often trained on extensive online text corpora that have not been thoroughly filtered, containing elements of toxicity that can hardly be fully eliminated.\nToxicity can be identified by interpreting LLM components like the feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes relevant dimensions through singular value decomposition (Lee et al., 2024  ###reference_b124###).\nFurthermore, the exploration of geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al. (2023  ###reference_b14###) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by the finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024  ###reference_b124###) develops a method called direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices during the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can reduce toxicity. Built on the observation that LLMs’ representations are updated by outputs from attention layers (Elhage et al., 2021  ###reference_b67###), another work attempts to reduce toxicity by identifying the “toxicity direction” and then adjusting representations in the opposite direction (Leong et al., 2023  ###reference_b127###)."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Truthfulness",
            "text": "One prominent drawback of LLMs is their tendency to confidently produce false statements. These statements fall into two main categories: 1) statements that contradict learned knowledge within models, a problem often related to models’ honesty; 2) statements that are factually incorrect and appear to be fabricated by models, a phenomenon commonly referred to as hallucination. In the following, we delve into various approaches that aim to understand aforementioned two behaviors by leveraging explainability tools."
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Honesty",
            "text": "Honesty of LLMs describes models’ ability to produce true statements based on their learned information, where dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous studies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs. One notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of statements (Azaria & Mitchell, 2023  ###reference_b12###). The classifier is simply trained on top of activations from the hidden layers of LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their outputs (Azaria & Mitchell, 2023  ###reference_b12###). Furthermore, research by Campbell et al. (2023  ###reference_b29###) localizes dishonesty behaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest responses, and then trains logistic classifiers on models’ activations over true/false statements. It also employs activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches have witnessed the importance of layer 2329 in flipping dishonesty behaviors. Besides, another popular method tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023  ###reference_b166###). Typically, these structures are visualized by projecting representations of statements onto two principal components. A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Hallucinations",
            "text": "Hallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of explicit knowledge (Xu et al., 2024  ###reference_b265###; Zhu et al., 2023b  ###reference_b293###). However, whether LLMs are aware of their hallucination behaviors remains an open question. Recent work investigates this question by examining models’ hidden representation space (Duan et al., 2024  ###reference_b65###). It examines three hidden states involving a question, its correct answer and its incorrect answer, which are used to compute an “awareness” score. This metric quantifies the uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can increase models’ awareness.\nAdditionally, Li et al. (2024b  ###reference_b136###) illustrates the major differences between models’ output and their inner activations, identifying these discrepancies as a potential source of hallucination. By training linear probing classifiers on each attention head’s activations, the most specialized attention head is identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experiments reveal that “truth” might exist in a subspace instead of a single direction (Li et al., 2024b  ###reference_b136###).\nAnother work investigates the source of hallucination by analyzing patterns of source token contributions through perturbations (Xu et al., 2023c  ###reference_b264###). Their findings suggest that hallucinations may stem from the models’ excessive dependence on a restricted set of source tokens. Besides, the static distribution of source token contribution, termed as “source contribution staticity”, can be used as another indicator of hallucinations.\nBuilding on the above insights into LLM hallucinations, Duan et al. (2024  ###reference_b65###) apply PCA to derive the direction of the correct answer’s final hidden state, and enhance the hidden representations with this direction to reduce hallucinations. In contrast, Li et al. (2024b  ###reference_b136###) adopts a different approach, by intervening on top- specialized attention heads, while minimizing the influence of the rest attention heads within models. Different from PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple directions of intervention. First, they use orthogonal vectors of each probe’s hyperplane, which is similar to PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al., 2024b  ###reference_b136###). The vectors derived from mean shift has been demonstrated more effective than those from probe classifiers, which presents another feasible strategy for identifying directions of truth."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Challenges",
            "text": "We discuss the challenges in employing explanations to improve models’ trustworthiness and enhance alignment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation strategies based on explanations."
        },
        {
            "section_id": "5.6.1",
            "parent_section_id": "5.6",
            "section_name": "5.6.1 Challenges of Existing Detection Methods",
            "text": "Current detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and representations. However, we still lack a finer-grained understanding of how knowledge is encoded within LLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and robust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads might be examined and then the related heads pruned (Li et al., 2024b  ###reference_b136###). This approach requires analyzing each model individually, rather than adopting a general approach. Moreover, existing localization approaches rely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers, the pre-designed biases used to train these classifiers are crucial to their performance. On the other hand, casual cleaning usually introduces new variables that complicate the analysis."
        },
        {
            "section_id": "5.6.2",
            "parent_section_id": "5.6",
            "section_name": "5.6.2 Challenges of Mitigation Strategies",
            "text": "Since LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trustworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner mechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are typically developed based on explanations. Existing explanations are implemented using techniques from mechanistic interpretability and representation engineering (Zhao et al., 2024  ###reference_b286###). While both streams of methods can alleviate these issues, they fail to fully address them. For example, principal component analysis (PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace those demonstrated to be responsible for specific issues. However, the identified directions and patched activations can only mitigate issues to a certain extent. Moreover, the changes to either representations or activations could also influence other aspects of models’ capabilities, which we are yet unable to evaluate."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "LLM Enhancement via Explainable Prompting",
            "text": "A key distinction between LLMs and traditional machine learning models lies in the LLMs’ ability to accept flexibly manipulated input data, namely prompts, during model inference (Liu et al., 2023a  ###reference_b151###).\nLLMs generally give precedence to the information presented in these prompts when generating outputs.\nTherefore, to mitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which is then prioritized over the LLMs’ inherent and implicit knowledge.\nThese enriched prompts can include domain-specific insights, contextual information, or a step-by-step reasoning chain.\nIn response, LLMs might reveal their decision-making processes during inference, which improves the explainability of their behaviors."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Chain of Thoughts (CoT) Prompting",
            "text": "The Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al., 2022  ###reference_b252###). While LLMs are adept at generating human-like responses, they often lack transparency in their reasoning processes. This limitation makes it difficult for users to assess the credibility of the responses, especially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations directly into prompts (Wei et al., 2022  ###reference_b252###; Huang et al., 2023a  ###reference_b98###; Yao et al., 2023b  ###reference_b271###; Besta et al., 2023  ###reference_b19###). Among these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning process. Formally, we define the language model as , and input prompt as , where  denote the example question-response pairs for in-context learning, and  is the actual question. In a standard question-answering scenario, we have the model output as . This approach, however, does not provide insights into the reasoning process behind the answer . Therefore, the CoT method proposes to include human-crafted explanations  for the -th in-context example, resulting in a modified input format . Given the input, the model will output not only  but also the generated explanation :\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is also practically useful as it augments LLMs’ functionality by opening a window for users to control the models’ thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\nReducing Errors in Reasoning: By breaking down complex problems into a series of smaller tasks, CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution of intricate problems (Wei et al., 2022  ###reference_b252###; Qin et al., 2023  ###reference_b184###; Zhang et al., 2023  ###reference_b283###; Wang & Zhou, 2024b  ###reference_b246###).\nProviding Adjustable Intermediate Steps: CoT enables the outlining of traceable intermediate steps within the problem-solving process. This feature enables users to trace the model’s thought process from inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu et al., 2023  ###reference_b161###; Wang et al., 2023d  ###reference_b242###).\nFacilitating Knowledge Distillation: The step-by-step reasoning processes derived from larger LLMs can serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex problem-solving by following explanations, effectively teaching them to tackle intricate questions with enhanced reasoning capabilities (Magister et al., 2022  ###reference_b163###)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Extended Methods of Explainable Prompting",
            "text": "Advanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths available to LLMs towards enhancing the transparency and understandability of the decision-making process (Yao et al., 2023b  ###reference_b271###; Besta et al., 2023  ###reference_b19###; Yao et al., 2023a  ###reference_b270###; Dhuliawala et al., 2023  ###reference_b57###; Lyu et al., 2023  ###reference_b161###).\nWe introduce several notable examples below.\nTree-of-Thoughts (ToT).\nProposed by Yao et al. (2023b  ###reference_b271###), ToT advances beyond the traditional linear Chain of Thought reasoning, offering a more versatile structure that allows models to navigate through multiple reasoning paths.\nToT makes the reasoning process of LLMs more interpretable by closely aligning it with human thought processes, as humans naturally consider multiple options and possible outcomes in both forward planning and retrospective analysis to reach conclusions (Sloman, 1996  ###reference_b213###; Stanovich, 1999  ###reference_b216###).\nThis capability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and reevaluate different strategies, such as devising game strategies or generating creative content. By simulating the way humans think and make decisions, ToT not only makes their thought process more understandable to human users, but also improves the models’ effectiveness in handling complex tasks.\nGraph of Thoughts (GoT).\nProposed by Besta et al. (2023  ###reference_b19###), GoT transforms the output of LLMs into a graph format. This format visualizes information pieces as nodes and their connections as edges, enabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT.\nBy organizing data into nodes (individual concepts or pieces of information) and edges (relationship between these concepts), GoT makes the logical connections within complex systems more understandable (Yao et al., 2023a  ###reference_b270###).\nThis graphical representation brings several benefits for understanding complex information. Firstly, it enables dynamic modification of relationships between concepts, offering a clear visualization of how changing one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023  ###reference_b42###; Boche et al., 2024  ###reference_b22###), scientific research (Ding et al., 2023  ###reference_b59###; Choudhury et al., 2023  ###reference_b41###), and policy analysis (Chen et al., 2023c  ###reference_b38###), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT enables an assessment of the significance of each node within the graph, providing insights into which pieces of information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally powerful for analyzing and navigating complex information networks."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Case Study: Is CoT Really Making LLM Inferences Explainable?",
            "text": ""
        },
        {
            "section_id": "6.3.1",
            "parent_section_id": "6.3",
            "section_name": "6.3.1 Background and Experimental Settings",
            "text": "Despite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered: Does CoT really make LLM inferences explainable? In other words, can the information provided through CoT faithfully reflect the underlying generation process of LLMs?\nWe use multi-hop question-answering (QA) as the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single information source, multi-hop questions require synthesizing information from multiple pieces or sources of data into a coherent and logical sequence.\nWhile LLMs show good performance in single-hop QA tasks (Radford et al., 2019  ###reference_b185###), their efficacy significantly declines in multi-hop situations (Tan et al., 2023  ###reference_b220###; Kim et al., 2023a  ###reference_b114###; Zhong et al., 2023  ###reference_b289###). This discrepancy highlights the need for more advanced methods to effectively handle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT technique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example as below. Here,  denotes the test question. The “Thoughts” following each “Question” are step-by-step problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation process of LLMs with human cognitive problem-solving patterns.\nCoT Faithfulness for Explanation:\nTo quantitatively measure the faithfulness of CoTs, we select fidelity as the corresponding metrics (Zhao et al., 2023b  ###reference_b285###; Wachter et al., 2017  ###reference_b234###):\nwhere  denotes the ground truth label,  denotes the original model output with CoT, while  denotes the model output with misleading information inserted in the \"Thoughts\" section.\nIn the following, we give an example. Given the target question, the correct step-by-step thoughts should be: “Ellie Kemper is a citizen of the United States of America. The president of the United States of America is Joe Biden.”\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the model to generate a new answer based on incorrect thoughts. If the model still generates the correct answer after the modification, we believe that the CoT information does not faithfully reflect the true process of the answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts, then we claim the thoughts are faithful.\nExperimental Settings.\nWe evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023  ###reference_b289###), which includes 1,000 cases for each -hop questions, , which totally consists of 3,000 questions. Our evaluation applies various language models, including GPT-2 (Radford et al., 2019  ###reference_b185###) with 1.5 billion parameters, GPT-J (Wang & Komatsuzaki, 2021  ###reference_b237###) with 6 billion parameters, LLaMA (Touvron et al., 2023a  ###reference_b227###) with 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023  ###reference_b40###) with 7 billion parameters, LLaMA2-chat-hf (Touvron et al., 2023b  ###reference_b228###) with 7 billion parameters, Falcon (Almazrouei et al., 2023  ###reference_b7###) with 7 billion parameters, Mistral-v0.1 (Jiang et al., 2023  ###reference_b108###) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023  ###reference_b108###) with 7 billion parameters.\nThese models have demonstrated proficiency in both language generation and comprehension."
        },
        {
            "section_id": "6.3.2",
            "parent_section_id": "6.3",
            "section_name": "6.3.2 Experiment Results",
            "text": "Performance Improvement.\nThe performance reported in Table 5  ###reference_### for multi-hop question answering highlights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement, particularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT’s coherent reasoning greatly enhances LLMs’ question-answering ability. While GPT-2 shows modest gains, the performance of GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2, indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2 display considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting this observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in advancing the question-answering capabilities of LLMs across different model architectures and sizes.\nFaithfulness Evaluation of CoT.\nTable 6  ###reference_### illustrates the impact of accurate versus misleading CoTs on the performance of LLMs. The Fidelity metric indicates how faithfully the model’s output reflects the reasoning process described in the CoT. Ideally, a high Fidelity score suggests that the model’s final response is directly based on the provided CoT, validating it as a faithful explanation of the model’s reasoning pathway. However, as we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model’s reasoning, which calls for developing more effective evaluation methods in future research.\nGPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence to the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we observe that these models usually rely on their own generated thoughts instead of using incorrect information provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest misleading accuracy scores, suggesting a potential self-defense ability against false information. The lower fidelity scores of later models may be attributed to their improved training processes on more diverse and high-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result, they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer. While high fidelity scores generally indicate a model’s adherence to the provided CoT, low fidelity scores do not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject misleading information. Further research on CoT faithfulness and the development of more sophisticated evaluation metrics could contribute to the advancement of interpretable and reliable language models."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Challenges",
            "text": "Within machine learning, explanation faithfulness refers to the degree to which an explanation accurately reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c  ###reference_b142###). An explanation is considered as faithful if it causes the model to make the same decision as the original input.\nIn this context, the challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language models to generate explanations that are genuinely representative of the models’ internal decision-making processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate answers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately represent the decision-making process within these models. Some efforts have been made to bolster the CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim et al., 2023b  ###reference_b115###; Ho et al., 2022  ###reference_b95###). These methods can help improve the explanation faithfulness of CoT for small language models, thereby addressing this issue to some extent.\nNevertheless, it remains a challenging problem of how to ensure the generated explanations (i.e., “what the model says”) are faithful to the internal mechanism (i.e., “what the model thinks”) of language models.\nRegarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced by the introduction of biasing prompt templates into model input (Turpin et al., 2024  ###reference_b229###). This is because existing CoT requires carefully designed templates to prompt language models to produce explanations. If incorrect or biased information is encoded in such templates, the generated explanations could be misleading. Recently, Wang & Zhou (2024a  ###reference_b245###) propose a novel decoding strategy to implement CoT with prompting, which could mitigate this issue. However, how to effectively help language models get rid of the template reliance still remains to be underexplored."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "LLM Enhancement via Knowledge-Augmented Prompting",
            "text": "Enhancing models with external knowledge can significantly improve the control and interpretability of decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale data, this knowledge is embedded implicitly within the model parameters, making it challenging to explain or control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass the unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in the world.\nTo address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for the explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield more interpretable predictions."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Preliminaries: Retrieval-Augmented Generation",
            "text": "By fetching relevant information from external databases or the internet, RAG ensures that LLM outputs are accurate and up-to-date. It addresses LLMs’ limitation of relying on fixed and potentially outdated knowledge bases.\nRAG operates in two steps: (1) Retrieval: It locates and fetches pertinent information from an external source based on the user’s query; (2) Generation: It incorporates this information into the model’s generated response. Given an input query  and the desired output , the objective function of RAG can be formulated as (Guu et al., 2020  ###reference_b83###):\nwhere  stands for the external knowledge retrieved from a knowledge base . Thus, the target distribution is jointly modeled by a knowledge retriever  and an answer reasoning module .\nThe knowledge  serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to retrieve relevant knowledge  and to produce correct answers  based on  and . As LLMs possess stronger text comprehension and reasoning abilities, they can directly serve as the reasoning module  without further training. In this case, RAG can be treated as a data-centric problem:\nwhere the goal is to find appropriate knowledge that supports the desired output.\nThe interpretability of RAG-based models comes from the information in : (1)  usually elucidates or supplements the task-specific information in ; (2)  could explain the generation of output .\nUnlike other deep models that directly estimate  in an end-to-end manner, where the decision process is not comprehensible, the RAG process provides justification or rationale  that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they integrate external knowledge into the model’s workflow.\nThe first category incorporates external knowledge at the inference stage. For instance, Karpukhin et al. (2020  ###reference_b111###) employ dense vectors to identify related documents or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020  ###reference_b128###) refine the data retrieval process to ensure only the most pertinent information influences the model’s output.\nThe second category integrates external knowledge during the model tuning stage. Some representative approaches include Guu et al. (2020  ###reference_b83###); Borgeaud et al. (2022  ###reference_b23###); Nakano et al. (2021  ###reference_b176###). Generally, these methods embed a retrieval mechanism into the model’s training phase, enabling the model to utilize external data more efficiently from the outset."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Enhancing Decision-Making Control with Explicit Knowledge",
            "text": "The incorporation of explicit external knowledge through RAG enhances the precision and controllability of decision-making in LLMs. This method leverages real-time information from external databases to produce responses that are not only accurate but also tailored to the specific requirements of each query. Below, we explore the mechanisms by which RAG achieves a more controllable and directed content generation process, with references to key papers that have contributed to these advancements."
        },
        {
            "section_id": "7.2.1",
            "parent_section_id": "7.2",
            "section_name": "7.2.1 Reducing Hallucinations in Response",
            "text": "“Hallucination\" in the context of LLMs refers to instances where these models generate information that, while coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang et al., 2023c  ###reference_b101###). This issue can lead to the production of misleading or entirely fabricated content, posing a significant challenge to the reliability and trustworthiness of LLMs’ outputs.\nRAG offers a powerful solution to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external knowledge at the point of generating responses, RAG ensures that the information produced by the model is anchored in reality. This process significantly enhances the factual basis of the model’s outputs, thereby reducing the occurrence of hallucinations. Shuster et al. (2021  ###reference_b209###) applies neural-retrieval-in-the-loop architectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as confirmed by human evaluations. Siriwardhana et al. (2023  ###reference_b212###) introduces RAG-end2end, which joint trains retriever and generator components together. Their method demonstrates notable performance improvements across specialized domains like healthcare and news while reducing knowledge hallucination."
        },
        {
            "section_id": "7.2.2",
            "parent_section_id": "7.2",
            "section_name": "7.2.2 Dynamic Responses to Knowledge Updating",
            "text": "RAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-making processes aligned with the latest developments. This feature is especially vital in fast-evolving fields such as medicine and technology, where the need for timely and accurate information is paramount (Meng et al., 2022b  ###reference_b168###). For example, research by (Izacard & Grave, 2020  ###reference_b103###) demonstrates significant enhancements in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023  ###reference_b87###) suggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently. Additionally, Wang et al. (2023e  ###reference_b243###) introduce a method for integrating newly retrieved knowledge from a multilingual database directly into the model prompts, facilitating updates in a multilingual context."
        },
        {
            "section_id": "7.2.3",
            "parent_section_id": "7.2",
            "section_name": "7.2.3 Domain-specific Customization",
            "text": "RAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models tailored to specific domains. Research by Guu et al. (2020  ###reference_b83###) illustrates how integrating databases specific to certain fields into the retrieval process can empower models to deliver expert-level responses, boosting their effectiveness in both professional and academic contexts. Shi et al. (2023  ###reference_b207###) have applied this concept in the medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge into query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen et al. (2023  ###reference_b165###) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity, finding that simply increasing model size does not significantly enhance the recall of such information. However, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly for questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps. Similarly, Kandpal et al. (2023  ###reference_b110###) highlights LLMs’ challenges with acquiring rare knowledge and proposes that retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing nuanced, less common information."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Challenges",
            "text": "We discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage , does the retrieved information  always elucidate the task-specific information contained in the input ? (2) In the generation stage , does  effectively serve as an explanation for the generation of output ?\nPlease note that our goal is not to exhaustively discuss all the limitations of RAG in this paper as RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations of RAG, we direct readers to other reviews (Gao et al., 2023  ###reference_b72###)."
        },
        {
            "section_id": "7.3.1",
            "parent_section_id": "7.3",
            "section_name": "7.3.1 Retrieval Accuracy Bottlenecks",
            "text": "Existent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020  ###reference_b128###; Gao et al., 2023  ###reference_b72###), which represents a substantial improvement over basic keyword searches (Robertson et al., 2009  ###reference_b192###). However, these methods may struggle with complex queries that demand deeper comprehension and nuanced reasoning.\nThe recent “lost-in-the-middle” phenomenon (Liu et al., 2024  ###reference_b148###) has revealed that an ineffective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting the generation quality.\nTo address this challenge, recent RAG approaches have integrated adaptive learning processes (Asai et al., 2023  ###reference_b11###). This advancement enables the retrieval system to refine their performance over time through feedback, adapting to evolving language use and information updates, ensuring their responses remain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems."
        },
        {
            "section_id": "7.3.2",
            "parent_section_id": "7.3",
            "section_name": "7.3.2 Controllable Generation Bottlenecks",
            "text": "In-context learning stands out as the premier method for incorporating external knowledge to boost the capabilities of LLMs such as GPT-4 (Asai et al., 2023  ###reference_b11###; Gao et al., 2023  ###reference_b72###). Despite its effectiveness, there’s no surefire way to ensure that these models consistently leverage the provided external knowledge within the prompts for their decision-making processes.\nIn practice, to achieve thorough coverage, commonly used dense retrieval usually returns a large volume of content, including both relevant and redundant information to the input question. Unfortunately, redundant information in the model prompt raises the computational cost and can mislead LLMs to generate incorrect answers. Recent research shows the retrieved information can degrade the question-answering task performance (Yoran et al., 2023  ###reference_b276###; Petroni et al., 2020  ###reference_b182###; Li et al., 2022a  ###reference_b129###). Some recent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However, such approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran et al., 2023  ###reference_b276###; Xu et al., 2023b  ###reference_b263###).\nThe challenge of optimizing the use of external explanations to achieve more precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Training Data Augmentation with Explanation",
            "text": "This section explores the generation of synthetic data from explanations using large language models, a technique poised to enhance various machine learning tasks.\nIn machine learning, limited data availability often constrains model performance, presenting a significant challenge across many domains. A viable solution is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data augmentation (Whitehouse et al., 2023  ###reference_b255###), such as transforming existing text samples into new variants (Dai et al., 2023  ###reference_b47###).\nNevertheless, there are several challenges to be tackled for effective text augmentation. First, for utility, the generated samples need to exhibit diversity compared to the original data. Second, these samples should be exhibit useful patterns relevant to the downstream tasks.\nTo address these challenges, explanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts and useful rationales (Carton et al., 2021  ###reference_b30###).\nUsing LLMs for explanation-guided data augmentation is a nascent but promising field.\nIn this section, we aim to outline feasible frameworks and discuss potential applications, offering directions for future research in this field.\nExplanations can be particularly beneficial in data augmentation within two scenarios.\nIn the first scenario, explanations are used to delineate desired model behaviors or to identify existing deficiencies, which effectively guides the data augmentation process of LLMs. The second scenario involves employing LLMs to directly produce explanatory texts, which serve as supplementary information to enrich the dataset."
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Explanation-guided Data Augmentation for Mitigating Shortcuts",
            "text": "Machine learning models are prone to make predictions with spurious correlations, also known as shortcuts (Geirhos et al., 2020  ###reference_b73###), which are misaligned with human reasoning processes. This dependency on shortcuts underlies various challenges in machine learning, notably diminishing a model’s ability to generalize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020  ###reference_b262###).\nThe extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Essentially, if a model’s predictions are predominantly based on such unreliable features, it indicates that the underlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations between input and predictions within deep models (Liu et al., 2018  ###reference_b149###; 2021  ###reference_b150###). For example, Du et al. (2021  ###reference_b64###) adopt Integrated Gradient (IG) to attribute a model’s predictions to its input features, showing that the model tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these shortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features. Explanatory information such as counterfactuals (Wang & Culotta, 2021  ###reference_b247###) has been incorporated in data augmentation to improve model robustness. It generates counterfactual samples by first identifying critical features (e.g., word tokens) and then replacing these features with their antonyms, along with reversing their associated labels. Subsequently, the generated samples are combined with the original ones to train downstream models.\nFurthermore, these techniques can be extended to enhance the out-of-distribution performance of smaller models (Sachdeva et al., 2023  ###reference_b196###; Wen et al., 2022  ###reference_b254###). Namely, large language models could serve as an effective tool to augment data. For example, LLMs are able to synthesize examples that represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen data (Xu et al., 2023a  ###reference_b261###). This could be helpful in building robust models in scenarios where data are scarce or confidential (Tang et al., 2023a  ###reference_b221###).\nBesides, LLMs are promising in improving models’ safety by generating adversarial examples that are more valid and natural compared to conventional approaches (Wang et al., 2023f  ###reference_b249###). First, the most vulnerable words are identified with attribution-based methods. Then, these words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of these examples can be examined with an external classifier. Subsequently, these adversarial examples are employed to train downstream models, effectively fortifying them against potential attacks and boosting their security.\nSimilarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al., 2023  ###reference_b92###). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal augmentation strategy that avoids hurting other groups. New group examples are generated using LLMs with human-providing labels. The experiments observe improvements on both underrepresented groups and overall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a certain demographic, thereby potentially promoting fairness in society."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Explanation-enhanced Data Enrichment",
            "text": "As a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations as augmented data. This strategy relies on LLMs’ understanding abilities to assist smaller models in their learning tasks. One objective in such work is to add natural language explanation generated by LLMs to training data, so as to enhance the performance of small models. Li et al. (2022b  ###reference_b140###) introduce explanations from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and acquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are utilized to train smaller models, including (1) explanations generated through chain of thought prompting, (2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement on accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b  ###reference_b140###). It is worth noting that LLMs including ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for limited languages (Whitehouse et al., 2023  ###reference_b255###).\nExplanations from LLMs have also been utilized to mitigate spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c  ###reference_b241###). This study proposes using LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c  ###reference_b241###). These explanations provide reasoning grounded in contextual semantics rather than relying on spurious correlations between words and labels. The explanations are integrated into the training of aspect-based sentiment analysis models through two methods: augmenting the training data with the explanations or distilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations instead of superficial textual cues, the models can better learn the true associations between text and sentiment and become more robust, improving both in-domain performance and generalization ability (Wang et al., 2023c  ###reference_b241###).\nAnother line of work involves integrating LLM rationales as additional supervision to guide the training of smaller models. Experiments have shown that this approach not only requires fewer training data but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023  ###reference_b97###).\nIn addition to the existing application of augmentation techniques summarized above, we envision that incorporating additional text information can also be practical and efficient in enhancing the performance of various models. For example, one promising application lies in the realm of guiding the parameter learning process of small models by using automatically generated explanations. Previous research has investigated this avenue by directing the attention of natural language inference models towards human-crafted explanations (Stacey et al., 2022  ###reference_b215###). As human-crafted explanations are both arduous and non-transferable, utilizing LLMs as generators presents a more economic and versatile alternation. Another potential application is to enhance model performance on complex tasks using natural language explanations from LLMs. For instance, code translation generation tasks incorporate explanations as an intermediate step, improving model performance by 12% on average (Tang et al., 2023b  ###reference_b222###). The result shows that explanations are particularly useful in zero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve their own reasoning abilities by generating reliable rationales (Huang et al., 2022  ###reference_b100###).\nFurther, Krishna et al. (2023  ###reference_b120###) embed post-hoc explanations, attributing scores to all input features, into natural language rationales. This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another study explicitly investigates LLMs’ ability to generate post-hoc explanations in natural language. The experiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023  ###reference_b121###). These studies present a novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to enrich training data so as to bolster model performance."
        },
        {
            "section_id": "8.3",
            "parent_section_id": "8",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "8.3.1",
            "parent_section_id": "8.3",
            "section_name": "8.3.1 Computational Overhead",
            "text": "Conventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The first scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues. This process typically requires multiple rounds of model training and applying interpretability methods to develop fair and robust models. Consequently, the crafting process can be both time and energy-consuming. Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics can offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods. By focusing on these data-centric measurements, data issues can be diagnosed and fixed before training. The number of training rounds needed is then significantly reduced. This shift not only streamlines model development but also helps reduce computational overhead, making the whole process more practical and efficient."
        },
        {
            "section_id": "8.3.2",
            "parent_section_id": "8.3",
            "section_name": "8.3.2 Data Quality and Volume",
            "text": "Despite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche contexts. For example, one of the most prominent issues is “hallucination”, where models generate plausible but incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of LLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual accuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality and relevance of these generated data relative to the original tasks. Determining the precise amount of data required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated data is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021  ###reference_b287###). This stems from LLMs’ limited ability to accurately control the quantity and distribution of generated data. Moreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs’ potential for data augmentation and related tasks."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Generating User-Friendly Explanation for XAI",
            "text": "Previous sections mainly focused on quantitative explanations with LLM via numerical values. For example, sample-based explanation discussed in Section 4  ###reference_### aims to assign each training sample an influence score (see Eqs.1  ###reference_###-4  ###reference_###) that measures the confidence that we can use that training sample to explain the prediction of a test sample. However, using numerical values for explanations is not intuitive, which can be difficult to understand by practitioners with little domain knowledge (Latif & Zhai, 2024  ###reference_b123###; Lee et al., 2023  ###reference_b125###; Li et al., 2020  ###reference_b137###). User-friendly explanations, on the contrary, aim to generate human-understandable explanations, e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain predictions, or what role a neuron plays in the network, such that the generated explanations can be well-understood by both researchers and practitioners.\nGiven an explainee , which can be a data sample , a neuron  from a pretrained model , or a prediction result  based on the input , generating user-friendly explanation aims to map the explainee  to a sequence of natural language tokens as the explanation for the explainee , such that the generated explanations can be easily comprehended by human beings."
        },
        {
            "section_id": "9.1",
            "parent_section_id": "9",
            "section_name": "User-friendly Data Explanation with LLMs",
            "text": "Data explanation refers to the process of translating difficult materials (e.g., program codes, long documents) into concise and straightforward language so that they are easy to understand by humans.\nLanguage models have long been used to generate explanations for textual data (Dai & Callan, 2019  ###reference_b48###). Since modern LLMs are trained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure textual content. For example, Chen et al. (2021  ###reference_b36###) have demonstrated that pretrained GPT models possess the ability to understand and generate codes, where explanatory comments are generated simultaneously that facilitate the understanding of programmers. In addition, Welleck et al. (2022  ###reference_b253###) propose to explain math theorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs have also been used to elucidate academic papers (Castillo-González et al., 2022  ###reference_b31###), making difficult content to be easily understood by individuals with little domain knowledge."
        },
        {
            "section_id": "9.2",
            "parent_section_id": "9",
            "section_name": "Explaining Small Models with LLMs",
            "text": "Recently, there has been growing interest in leveraging LLMs to generate free-text explanations for small models. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023  ###reference_b20###) propose a prompting-based strategy to identify keywords  in the input texts  with pretrained LLMs that are informative for the label , and ask LLMs to substitute them with another set of keywords , such that changed text  changes the label prediction to . They view the textual mapping rule “if we change  into  in , then  will be classified as ” as the counterfactual explanation for the model. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023  ###reference_b21###) propose to summarize the neuron activation patterns into textual phrases with a larger language model (e.g., GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs. To verify the identified patterns, they generate activation patterns according to the phrases via the same LLM and compare their similarity with the true activation patterns of the neuron, where the phrases with high scores are considered more confident to serve as the explanation for the neuron.\nThe explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a  ###reference_b284###) propose using pretrained vision-language models to generate explanations for a neuron  of an image classification model. Specifically, for each class , they first find regions in images with label  that have maximum activation of the neuron  as the surrogate explainees for , and prompt LLMs such as ChatGPT to generate candidate explanations (words, short phrases) for the class label . Then, they use the pretrained vision-language model CLIP (Radford et al., 2021  ###reference_b186###) to match the candidate explanations with the surrogate explainees as the explanations for the neuron . Recently, LLMs have also found applications in explaining recommender systems Zhu et al. (2023a  ###reference_b291###). Specifically, Yang et al. (2023c  ###reference_b269###) found that LLMs can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al. (2023  ###reference_b126###) propose to align user tokens of LLMs with the learned user embeddings of small recommendation model to generate explanations of user preferences encoded in the embeddings. Recently,  Schwettmann et al. (2024  ###reference_b203###) propose a unified framework to explain all models where inputs and outputs can be converted to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model by iteratively creating inputs and observing outputs from the model, where the textual explanations are generated by viewing all the interactions as the context."
        },
        {
            "section_id": "9.3",
            "parent_section_id": "9",
            "section_name": "Self-Explanation of LLMs",
            "text": "Due to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by human experts. Based on whether the LLM needs to be retrained to generate explanations for themselves, the self-explanation of LLM can be categorized into two classes: fine-tuning based approach and in-context based approach, which will be introduced in the following parts.\nFine-tuning based approaches.\nGiven sufficient exemplar explanations on the labels of the training data (e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017  ###reference_b91###) or the Yelp dataset (Zhou et al., 2020  ###reference_b290###), users have provided explanations on why they have purchased certain items, which can be viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions as an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022  ###reference_b74###), which fine-tunes the pre-trained language model T5 (Raffel et al., 2020  ###reference_b187###) on both the rating and explanation data to generate an explanation alongside the recommendations. Recently, several works have improved upon P5 (Cui et al., 2022  ###reference_b43###; Zhu et al., 2024  ###reference_b292###), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna, etc., and propose different prompt learning strategies (Li et al., 2023d  ###reference_b138###) with generating explanation as the auxiliary task.\nWith explanations introduced as additional supervision signals to fine-tune pretrained LLMs for recommendations, the performance can be improved with good explainability.\nIn-context based approaches. In many applications, there is often a lack of sufficient exemplar explanations. However, the unique capability of modern LLMs to reason and provide answers through human-like prompts introduces the potential for in-context based explanations. Here, explanations for predictions are crafted solely based on the information within the prompt. A leading approach in this domain is the Chain-of-Thoughts (CoT) prompting (Wei et al., 2022  ###reference_b252###), which provides few-shot examples (with or without explanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the intermediate reasoning steps that provide more context for generating the final answer can be viewed as explanations.\nHowever, CoT generates reasoning first and then based on which generates predictions, where the reasoning steps can influence prediction results (Lyu et al., 2023  ###reference_b161###). If explanations are generated after the prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful post-hoc explanation of why the model makes certain decisions (Lanham et al., 2023  ###reference_b122###).\nThe application of in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d  ###reference_b102###) explore generating zero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations alongside the predictions. In addition, Huang et al. (2023a  ###reference_b98###) propose a chain-of-explanation strategy that aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022  ###reference_b157###) find that CoT can generate well-supported explanations for question answering with scientific knowledge."
        },
        {
            "section_id": "9.4",
            "parent_section_id": "9",
            "section_name": "Challenges",
            "text": ""
        },
        {
            "section_id": "9.4.1",
            "parent_section_id": "9.4",
            "section_name": "9.4.1 Usability v.s. Reliability",
            "text": "Many existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as numerical methods with good theoretical foundations. Ye & Durrett (2022b  ###reference_b273###) find that the explanations by CoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the predictions are right or wrong).\nHowever, the validity of viewing CoT explanations as post-hoc justifications has been questioned by recent findings from Turpin et al. (2024  ###reference_b229###), which uses biased datasets (e.g., the few-shot examples in the prompt always answer “A” for multiple choice questions) to show that the generated explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the LLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there’s a growing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility."
        },
        {
            "section_id": "9.4.2",
            "parent_section_id": "9.4",
            "section_name": "9.4.2 Constrained Application Scenarios",
            "text": "Currently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal with data with rich textual information (Bhattacharjee et al., 2023  ###reference_b20###; Lei et al., 2023  ###reference_b126###). Although Zhao et al. (2023b  ###reference_b285###) propose a strategy to explain image classifiers, the ability to match candidate textual explanations with image patterns still relies on the pretrained vision-language model CLIP. This method may not be applicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series analysis (recurrent neural networks), where large pretrained models have demonstrated little progress compared to natural language processing and computer vision. Therefore, there is a compelling need to devise more versatile strategies for explaining models across a wider range of fields. This endeavor could depend on the fundamental research on combining LLM with other domain-specific tasks, such as the development of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner."
        }
    ]
}