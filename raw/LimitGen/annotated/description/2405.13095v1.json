{
    "title": "Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution",
    "abstract": "Automatically generating a presentation from the text of a long document is a challenging and useful problem. In contrast to a flat summary, a presentation needs to have a better and non-linear narrative, i.e., the content of a slide can come from different and non-contiguous parts of the given document. However, it is difficult to incorporate such non-linear mapping of content to slides and ensure that the content is faithful to the document. LLMs are prone to hallucination and their performance degrades with the length of the input document. Towards this, we propose a novel graph based solution where we learn a graph from the input document and use a combination of graph neural network and LLM to generate a presentation with attribution of content for each slide. We conduct thorough experiments to show the merit of our approach compared to directly using LLMs for this task.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Presentations are a very effective medium of communication in several day-to-day and business workflows.\nCompared to a flat summarization, generating a presentation is more complex because it should have a nice narrative and coherence in the content along with the ability to convey the core ideas to the audience. This makes generating presentation a very tedious process for humans Reynolds (2011  ###reference_b19###).\nDocument-to-slide generation using automatic methods has been garnering attention for several years.\nThese methods include using handcrafted and pre-defined heuristics or web schemas Al Masum et al. (2005  ###reference_b1###); Winters and Mathewson (2019  ###reference_b27###). There are approaches which generate the agenda (i.e., sequence of slide titles) of a presentation based on the sections present in the document (Hu and Wan, 2013  ###reference_b5###; Wang et al., 2017a  ###reference_b24###) or require users to provide an agenda (Sun et al., 2021  ###reference_b21###; Li et al., 2021  ###reference_b9###) and subsequently generate the slides as a single-document query-based summarization. However, manually coming up with an agenda is a difficult task, particularly for documents in the range of 10s of pages.\n###figure_1### This issue can be mitigated to an extent given the recent advances in generative language models Brown et al. (2020  ###reference_b3###); Touvron et al. (2023  ###reference_b23###) with increasing context limits, in which we can prompt a large language model (LLM) to ingest the entire document context and generate an outline and the text content for a presentation. However this approach has three major limitations. First, the LLMs tend to hallucinate more and they often ignore the middle portion of a context as the context length grows Liu et al. (2023a  ###reference_b12###). This can be a serious issue if we want to generate slides from large documents and ensure good coverage of all the important concepts in the presentation. Second, processing the entire document in its reading order by an LLM often results in a summary-like overview of the document, as opposed to capturing a narrative-centric view that is required for a presentation.\nNarratives (flow of information in the form of a story) (Xie and Riedl, 2024  ###reference_b28###) in presentations can be non-linear in nature, i.e., paragraphs from across multiple sections of an input document contribute to a slide, and this is not necessarily in the linear reading order of the document (refer to Figure 1  ###reference_###). As shown in Section 4.6  ###reference_###, non-linearity in presentations generated by the authors for the set of research papers in SciDuet dataset (Sun et al., 2021  ###reference_b21###) is 38.6%, whereas the ones generated by a GPT-based clustering baseline (GDP-GPT in Section 4.2  ###reference_###) has only 1.2%.\nTo generate these narratives, the non-linear relationship between the various pieces of content in the given document needs to be captured. Moreover, LLMs do not attribute the source content (e.g., a paragraph) for each part (e.g., a slide) of the generated content. This attribution is necessary to improve the reliability of the generated presentation and for further editing.\nOne can potentially think of posing the problem of non-linear way of generating presentation as a classification task of classifying a sequence of text elements (say, paragraphs) to one of the  classes where each class represents a slide; or a clustering task of cluster the paragraphs to  clusters. However, the number of slides needed from a document cannot be fixed over a set of documents. It can even vary for the same document depending on the audience, intent and the duration of presentation. Thus, it is not possible to pose it as a -class classification task. Also, clustering as an unsupervised task can cluster text based on multiple aspects such as frequency of common words, common sub-topics, etc., where each generated cluster does not match to a slide.\nTo address the research gaps mentioned above, we propose a novel method of generating text presentation from a long input document as shown in Figure 2  ###reference_###. Our motivation is to infer the structure present between the text elements (i.e., paragraphs) of a document via corresponding latent slides (as shown in Figure 1  ###reference_###) by a learnable graph.\nFollowing are the contributions made in this paper: \n1. Automatically generating a non-linear presentation integrated with the content attribution from a given long document is a novel task to the best of our knowledge. \n2. We propose a novel approach, referred as GDP (Graph based automated transformation of Documents to Presentation), which uses a combination of graph neural network (GNN) and LLM. Our method by design is able to capture the non-linearity in generating the presentation and it attributes the source paragraphs for each generated slide within the presentation. \n3. We propose an evaluation framework which includes both automated and human evaluated metrics for document to presentation transformation. Our analysis shows the merit of GDP over the approaches that directly use SOTA LLM along with intelligent prompting techniques."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Problem Formulation",
            "text": "We are given with a training dataset which is a set of documents and the corresponding presentation slides. Let us denote this dataset as , where  is a document consisting of a sequence of  text paragraphs as , . These paragraphs are indexed following the reading order in the document. Similarly,  is a human-generated presentation from the document . A presentation is a sequence of slides. So, . Please note that different documents can have different number of paragraphs and the corresponding presentations can have different number of slides. As discussed in Section 1  ###reference_###, we consider both the input document and the generated slides to contain only text. Ideally, the presentation should cover all the important aspects of the input document, with a nice flow of information such that it is easy to follow by a broader audience. Given this data, our goal is to generate a presentation for each document present in a test set . The number of slides  to be generated from the test document ,  is a user input during the inference time and we can not assumed this to be a constant over all the test documents. The training set and the test set of documents may come from the same distribution or from different distributions to test the generalizability of our proposed approach.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Solution Approach",
            "text": "Following are the details of different components of our proposed approach GDP. Figure 2  ###reference_### shows the overall architecture.\nWe assume the input documents are in pdf format. We use a publicly available PDF Extract API ***https://developer.adobe.com/document-services/apis/pdf-extract/  ###reference_ces/apis/pdf-extract/### to extract the text content from the documents. The output of extract is processed in such a way that we have the section and subsection titles and the text in the form of paragraphs within each section or subsection. The sequence of text elements is the same as the reading order of text in the pdf. We are not considering images, tables and other multimodal information present in the document in this work."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Classifier: Dataset and Training",
            "text": "The first step in our approach is training a classifier that can predict the probability of a pair of paragraphs from the given document going into the same slide.\nWe use SciDuet dataset (Sun et al., 2021  ###reference_b21###) which consists of document-presentation pairs, as discussed in section 4.1  ###reference_###. We leverage this dataset to create a synthetic dataset for training our classifier.\nWe use sentence embedding Reimers and Gurevych (2019  ###reference_b18###) for this. Let  be the embedding of -th slide from -th training presentation, and  be the embedding of  paragraph from the corresponding document.\nThe set of selected paragraphs  for a given slide  is determined as .\nWe use a simple heuristic to define the threshold () for paragraph selection as ,\nwhere  is a list of cosine similarity between  and the paragraphs in , and  is standard deviation of that. Paragraphs with a cosine similarity of less than  are discarded to ensure a high-quality dataset. Additionally, we select a maximum of  paragraphs per slide to ensure a balanced dataset. After this exercise, we get the lists of paragraphs that contribute together in the same slide within our training dataset.\n###table_1### Each pair of paragraphs in the list corresponding to a slide forms a positive sample for the classifier. For creating negative samples, for each paragraph, we sample ten random paragraphs from the document that never occurred with that paragraph. Note that by this approach, negative samples will be more than positive ones reflecting the real world scenario. After creating a dataset like this, we select  samples for training and  for testing and validation. Please note that the documents used to create training, test, and validation datasets are mutually exclusive to prevent leakage. Please refer to Table 1  ###reference_### for the dataset details.\nWe fine-tuned a RoBERTa-base model Liu et al. (2019  ###reference_b14###) on this dataset. Since the dataset is imbalanced, we use the standard weighted binary cross-entropy loss function. Best hyperparameters are found using a grid search on the validation set which gives an accuracy of  and an F1 score of  on the test set."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Learning the Graph Structure",
            "text": "For the simplicity of notation, we will use  as the current document from which the presentation needs to be generated. The trained classifier from Section 3.1  ###reference_### will generate a pairwise probability  for any two paragraphs  which determines their chance of contributing to the same slide.\nIdeally, this probability is high when they actually contribute to the same slide and the probability is low when they do not contribute to the same slide.\nWith this intuition, we aim to form a graph  using these pairwise probabilities, where  is the set of nodes,  is the set of edges (undirected and unweighted) and  is a node feature matrix.\nAs a simple heuristic, initially we create a node for each paragraph of the input document . We connect two nodes  and  by an edge if the probability of them contributing to the same slide , where  is a hyperparameter. It is well-understood that not all the paragraphs in a long document are covered in a presentation. To match this intuition and keep the graph small, we remove the isolated nodes from the graph. Each node (paragraph) is also associated with a vector (node feature)  which is the corresponding paragraph embedding as discussed in Section 3.1  ###reference_###. Thus, the structure of the graph is heavily dependent of the trained classifier above. Next, we use a graph neural network to cluster the nodes of this graph so that each cluster can contribute to a slide."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Slide Attribution via GNN and Clustering",
            "text": "Given the graph  from the document , we want to develop an unsupervised graph neural network which can obtain the vector representations (embeddings) of the nodes in such a way that when two nodes are directly connected in the graph, they have similar embeddings than two nodes which are far apart. Subsequently, a clustering algorithm is used on the generated node embeddings to find the clusters. Let us use  to denote the binary adjacency matrix of , where  if there is an edge between the nodes  and , otherwise . We use a 2-layered graph convolution encoder (Kipf and Welling, 2017  ###reference_b7###) to obtain representation of each node as shown below:\nwhere each row of  is the corresponding node embedding. We compute , where  is the identity matrix and the degree diagonal matrix  with , . We set .\n and  are the trainable parameters of this GCN encoder.\nSince the number of slides required for the same document can vary during the inference time depending on the need of a user, we cannot rely on any dataset to give a direct supervision to generate a presentation from a document. So,\nWe use an unsupervised loss function to train the parameters of the proposed GNN architecture. As our aim is to generate similar embeddings for the node pairs which are connected in the graph than any random pairs of nodes, we use the following strategy. For a given graph , node embeddings are obtained by passing node feature matrix  and the graph structure  through the GCN encoder. Next, we randomly add a set of negative edges , with  in the graph. We pose the training task as to minimize the following binary cross entropy loss on positive and negative edges of the graph as shown below:\nHere  is the sigmoid function and  is the -th row of , i.e., the node embedding of the  node in the graph. We use standard ADAM optimization technique (Kingma and Ba, 2014  ###reference_b6###) with a learning rate  to minimize the loss function above.\nOnce the unsupervised training is complete, we obtain the node embedding matrix  from the graph where each row is a paragraph embedding of dimension  from the given document. Since our main goal is to cluster the paragraphs in such a way that each cluster can correspond to a slide, we use spectral clustering (Ng et al., 2001  ###reference_b16###) on these paragraph embeddings from GNN. The number of clusters is kept as the number of slides  required for the document , and this number varies over the documents during inference. We have observed empirically that spectral clustering is able produce more balanced clusters compared to other algorithms such as KMeans on these node embeddings. At the end of this step, we obtain a clustering of paragraphs (nodes) of the current document as ."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Generating the Presentation",
            "text": "Please note that the clusters obtained above are unordered. To be able to generate slides from the clusters, we first order them using a simple heuristic. For any cluster , where , we consider all the paragraphs that belong to that cluster and take the minimum of their indices (please note that paragraph indices follow the reading order in the document as mentioned in Section 2  ###reference_###). Mathematically, . We use this  number to sort the clusters in increasing order and then associate the first cluster (with the smallest  number) as the one corresponding to the first slide, and so on. Since, paragraph indices follow the reading order of the document, we wanted to roughly follow that in the generated presentation. One can see that non-linearity is there in mapping the paragraphs to slides since paragraphs from any part of the document can contribute to any slide of the presentation. Let us reorder the clusters and denote the clustering as , where they are sorted according to the  discussed above.\nNow, let us discuss the generation of the presentation  for the given document . For a slide , we know the corresponding cluster , and the paragraphs forming that cluster. We use GPT-3.5 ***Please note that GDP can support any LLMs even with lesser context length since we feed only a few paragraphs at a time. The choice of GPT-3.5 was to support some of the baselines in Section 4.2  ###reference_### which need to see the entire document within a single prompt. to generate the slides in sequence. To generate a slide , we provide the texts present in the paragraphs , along with the titles of the previous slides . Experimentally, we found that providing information about the previous slides help GPT to maintain a good flow in the presentation. The prompt for the generation of th slide of the presentation is shown in Appendix E  ###reference_###. To generate the whole presentation , we made  such calls in sequence where  is the required number of slides."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Anonymous link to the selected documents for human evaluation and the corresponding generated presentation can be found in the Appendix. We show the average and standard deviation of ratings by 2 reviewers for five research papers in Table 3  ###reference_###. GDP significantly lags behind baselines and its variant across all metrics. For the GPT based algorithms, a few common concerns were “title matches the text in the slide, but the slides go off topic often”, “There are several details that have not been touched at all, like data collection, annotation, unigram, part-of-speech tag..” and “there was no significant narration that was conveyed in the ppt”. This aligns with our intuition from Section 1  ###reference_### that GPT struggles with lengthy input contexts. Particularly for research papers, where discussions focus on a single topic with repeated words and concepts, GPT-based algorithms struggle to produce quality output despite various prompting techniques. Whereas for GDP, reviewers appreciate the coverage (“The reason is simply because all the data was covered by the slides”), non-repetition (“data provided in the slides wasn’t repeated), consistency and attribution (“no hallucination ”). There was also some concern on GDP about the depth of the generated presentation (“The deck covers a lot of content but doesn’t deep dive”). The human evaluation results for business documents are presented in Table 4  ###reference_###. The results highlight GDP’s ability to generalize to a new domain, outperforming all algorithms in all metrics except slide uniformity. Unlike research papers, business documents have shorter text on average. All the algorithms perform good on building narratives and maintaining information consistency in presentations. However, the reviewers are not satisfied with the language, coverage and utility of the presentations generated by the GPT based algorithms (“The presentation is not an ideal first draft as it very briefly summarizes the content of the input document with limited accuracy and consistency”). But they do appreciate GDP for these metrics (“The presentation is an efficient first draft ”)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We use the SciDuet dataset proposed in Sun et al. (2021  ###reference_b21###), which has research papers and their presentations. The papers are from ICML, NeurIPS, and ACL. We use papers from ICML and NeurIPS as their PDFs are available. We split 500 papers for training, 80 for validation, and 100 for testing. This dataset is used to train the classifier (§3.1  ###reference_###), find the right hyperparameters, and perform final testing. GDP marginally outperforms baselines and its variant across all metrics."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baseline Algorithms and Model Ablation",
            "text": "We conducted experiments using the D2S model (Sun et al., 2021 ###reference_b21###) and various prompting methods for GPT as baseline algorithms. The prompts for the baseline models are detailed in the Appendix. All baseline experiments employ GPT3.5-turbo-1106 with a 16K context length. \n\n1. D2S: We employed the D2S model (Sun et al., 2021 ###reference_b21###) as a semi-automatic baseline. D2S utilizes ground truth slide titles from the test set to extract content and generate presentations. Due to the absence of checkpoints, training scripts, or datasets for their BERT-based IR model ***https://github.com/IBM/document2slides/issues/3 ###reference_ues/3###, we used a pre-trained BERT model to encode queries. \n\n2. GPT-Flat: In this model, a simple prompt delivers the full input text from the document to GPT to generate the presentation.\n\n3. GPT-COT: Citing Wei et al. (2022 ###reference_b26###), chain-of-thought (COT) prompting enhances LLM performance. This method uses a COT prompt for GPT-3.5 to generate presentations.\n\n4. GPT-Constrained: To optimize text per slide, this method uses a modified COT prompt and includes precise instructions on bullets and word count for slides.\n\n5. GDP-KMeans: This serves as a model ablation for our suggested method. Here, KMeans replaces the graph learning and clustering section on paragraph embeddings (all-mpnet-base-v2) according to Reimers and Gurevych (2019 ###reference_b18###), with the remainder of the pipeline unaltered.\n\n6. GDP-Agglo: Identical to the model ablation study above but uses agglomerative clustering rather than KMeans.\n\n7. GDP-GPT: Another model ablation of GDP, replacing the GDP process which classifies paragraph pairs, constructs a graph, and performs node clustering, with GPT-3.5. This model has GPT-3.5 directly determine which paragraphs contribute to each slide, followed by final presentation generation using the prompts in Section 3.4 ###reference_###.\n\nGDP performs comparably to its baselines across all metrics."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We use a RoBERTa base model for a classifier with batch size , a learning rate of , and a dropout of . RoBERTa was finetuned on Nvidia A10 G for 3 hours. We use the all-mpnet-base-v2 sentence transformer model for all our experiments. We use the gpt-3.5-turbo-1106 model for all our experiments with a temperature of  and top_p of . For G-Eval-based evaluation, we use the gpt-4 model with a temperature of , top_p of , and the number of generations as . GDP significantly underperforms baselines and its variant across all metrics."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Automatic Evaluation Metrics",
            "text": "GDP marginally outperforms baselines and its variant across all metrics."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "We have presented the results of the baseline algorithms and GDP with its model variant on the test set (discussed in §4.1 ###reference_###) in Table 2 ###reference_###. We can make the following observations: (1) We can see that recall of GDP for ROUGE-1 is significantly less compared to GPT based baselines. However, GDP is able to achieve very high ROUGE-1 Precision, beating all baselines but GDP-KMeans. As also discussed in Sun et al. (2021 ###reference_b21###), ROUGE is not the best metric to evaluate presentations as multiple correct presentations differ at the lexical level, thus having different ROUGE1 scores. Our results built trust that GDP outputs important words in the final presentation. (2) GDP performs the best in terms of both paragraph-level and sentence-level coverage. This shows that GDP covers the entire document and does not miss out on some sections, a problem that human annotators also identified with the baselines. (3) GDP and its variant GDP-KMeans and GDP-GPT achieve a very low score of PPL (which indicates better performance) compared to baselines. This means clustering the paragraphs, generating a slide from each cluster, and using suitable prompts ensures a smooth flow of text and information in the presentations generated by GDP. (4) Finally for G-Eval, performance of all the GPT based algorithms, GDP-GPT and GDP are very close. GDP-KMeans perform poorly on G-Eval, providing trust in our algorithm of clustering paragraphs. However, contrary to our expectations, GDP significantly underperforms baselines and its variant across all metrics. Appendix A ###reference_### shows some qualitative analysis of a presentation generated by our proposed approach and compare that with the one generated by a baseline from the same input document."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Evaluation of Non-linearity",
            "text": "Following are the observations from this study:\n(1) Human generated presentations are highly non-linear () in nature.\n(2) GDP-KMeans had a very high non-linearity of , even higher than human-generated presentations. On manual inspection, we found that it is clustering some very random paragraphs together, which is undesirable.\n(3) Since, GDP-GPT uses GPT-3.5 to cluster the paragraphs and it is known that GPT tends to follow the ordering of the text present in the context (Liu et al., 2023a  ###reference_b12###), GPT based approaches are inherently quite linear in nature (with a Non-linearity of  for GDP-GPT).\n(3) The construction of graph using the results of the classifier and the subsequent use of GNN and clustering makes GDP quite non-linear () in nature. Graphs are indeed very good to handle non-linearity. Thus, the presentations generated by GDP is less close to human made presentations."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Human Evaluation",
            "text": "Presentation quality is subjective, and there is no universally defined best presentation for a given document (Sun et al., 2021  ###reference_b21###). We conduct a comprehensive human evaluation to further understand the presentations generated by our approach and by some selected baselines based on their performance on manual inspection and the available budget. For this task, we discussed with subject matter experts and selected two sets of documents: (1) Five research papers (2) Seven business documents comprising of technical manuals, reports, and news articles. This is a domain shift from the training set. Following are the metrics we have used for human evaluation: 1. Quality of Language; 2. Slide Uniformity to check the alignment between the title and main text, as well as coherence and uniformity within the slide’s content; 3. Coverage of the content; 4. Non-repetition of content across the slides; 5. Quality of Narrative (the flow of information) in the presentation; 6. Consistency by not including any content (or / and hallucination) outside of the given document; 7. Attribution Quality; and 8. Utility to check how easily a user can use / update a generated presentation. More details can be found in the Appendix. We use a Likert scale from 1 to 5 for all the metrics. We hired professional human reviewers ***https://www.upwork.com/, two with research backgrounds for evaluating presentations on research papers and two with experience in professional writing for business documents. We explained the metrics and the evaluation process to them over multiple sessions. They had no knowledge of the algorithms used to prevent preconceived bias. Each reviewer rated each presentation on a scale of 1 to 5 for each metric while also providing explanations. The cohen kappa score for inter annotator agreement is 0.386. Anonymous link to the selected documents for human evaluation and the corresponding generated presentation can be found in the Appendix. We show the average and standard deviation of ratings by 2 reviewers for five research papers in Table 3  ###reference_###  ###reference_###. GDP significantly outperforms baselines across most metrics but not all. For the GPT based algorithms, a few common concerns were “title matches the text in the slide, but the slides go off topic often”, “There are several details that have not been touched at all, like data collection, annotation, unigram, part-of-speech tag..” and “there was no significant narration that was conveyed in the ppt”. This aligns with our intuition from Section 1  ###reference_###  ###reference_### that GPT struggles with lengthy input contexts. Particularly for research papers, where discussions focus on a single topic with repeated words and concepts, GPT-based algorithms struggle to produce quality output despite various prompting techniques. Whereas for GDP, reviewers appreciate the coverage (“The reason is simply because all the data was covered by the slides”), non-repetition (“data provided in the slides wasn’t repeated), consistency and attribution (“no hallucination ”). There was also some concern on GDP about the depth of the generated presentation (“The deck covers a lot of content but doesn’t deep dive”). The human evaluation results for business documents are presented in Table 4  ###reference_###  ###reference_###. The results highlight GDP’s ability to generalize to a new domain, outperforming all algorithms in all metrics except slide uniformity. Unlike research papers, business documents have shorter text on average. All the algorithms perform good on building narratives and maintaining information consistency in presentations. However, the reviewers are not satisfied with the language, coverage and utility of the presentations generated by the GPT based algorithms (“The presentation is not an ideal first draft as it very briefly summarizes the content of the input document with limited accuracy and consistency”). But they do appreciate GDP for these metrics (“The presentation is an efficient first draft ”)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions and Conclusion",
            "text": "This paper presents an end-to-end novel approach, GDP, for transforming a long document into a text presentations. GDP employs a classifier to build a document graph, followed by graph neural networks and clustering. It then uses an LLM to generate slides from each paragraph cluster. We propose evaluation frameworks, and the results indicate several drawbacks of directly using GPT-based approaches with different prompting techniques. The evaluation shows that GDP can automatically generate a presentation that serves good as a first draft."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In this work, we focus only on the text part of an input document and generate only text presentation. Multimodal content such as images, diagrams and tables carry important information present in a document. Handling such multimodal content and also extract or generate more of them in the output presentation is an interesting problem. We seek to use a vision language model such as CLIP (Radford et al., 2021  ###reference_b17###) or a large multimodal model such as LLaVA (Liu et al., 2024  ###reference_b11###) in our pipeline for this task in some future work.\nAnother important aspect of a presentation is the selection of a relevant template and layout that goes well with the content. For example, a presentation with a formal content should have a different background and colours than the one with a very casual content. Currently in our implementation, we use a default vanilla template for all the generated presentations. Selection or recommendation of a suitable template and layout for a presentation is out of scope for this work and can be addressed in future."
        }
    ]
}