{
    "title": "Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning",
    "abstract": "Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity.\nIn response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). The spatiotemporal patterns of brain neural activity are the excitation of different wavelength characteristic patterns of its geometric structure. MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix  to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model’s performance in downstream tasks. The transformation-matrix  contains four different structures, each designed to simulate the geometric feature patterns of the brain at different levels. In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks and reduces the standard deviation by 0.7% in the Corpus of Linguistic Acceptability (CoLA) task; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.56% in the DART and WebNLG tasks, respectively.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, with the rapid development of large pre-trained language models (LPLMs) such as BERT (Devlin et al., 2019  ###reference_b9###) and GPT-3 (Brown et al., 2020  ###reference_b3###), these models have shown exceptional performance in many downstream tasks of natural language processing (NLP), including text generation, machine translation, sentiment analysis, and question-answering systems (Radford et al., 2019  ###reference_b34###; He et al., 2020  ###reference_b17###; Raffel et al., 2020  ###reference_b35###; Devlin et al., 2019  ###reference_b9###; Liu et al., 2019  ###reference_b22###; Peters et al., 2018  ###reference_b32###; Brown et al., 2020  ###reference_b3###). However, training and deploying LPLMs from scratch face significant resource challenges, as training a GPT-3 model with 175 billion parameters requires running hundreds of NVIDIA A100 40GB GPUs for approximately 200 days. Such a level of resource investment is unaffordable for most research institutions and enterprises.\nFine-tuning techniques based on LPLMs have been proven to effectively enhance model performance (Ouyang et al., 2022  ###reference_b30###; Wei et al., 2021  ###reference_b48###; Min et al., 2021  ###reference_b25###; Wang et al., 2022  ###reference_b46###; Liu et al., 2022  ###reference_b21###), enabling the model to acquire desired capabilities and discard unwanted ones (Ouyang et al., 2022  ###reference_b30###; Askell et al., 2021  ###reference_b1###). The rise of open-source LPLMs globally, such as GPT (Radford et al., 2019  ###reference_b34###), BERT (Devlin et al., 2019  ###reference_b9###), GLM (Du et al., 2022  ###reference_b11###; Zeng et al., 2022  ###reference_b52###), LLaMA (Touvron et al., 2023a  ###reference_b41###), and LLaMA2 (Touvron et al., 2023b  ###reference_b42###), offers new opportunities for a wide range of research organizations and enterprises. These entities can use these open-source LPLMs as a foundation, combined with their industry knowledge and data, for full-parameter fine-tuning (Full Fine-Tuning) (Qiu et al., 2020  ###reference_b33###; Raffel et al., 2020  ###reference_b35###). However, even full-parameter fine-tuning based on open-source LPLMs may require significant computational and storage resources. For instance, according to a study by Dettmers et al. (2024  ###reference_b8###), full-parameter fine-tuning of the LLaMA 65B model could require up to 20 NVIDIA A100 40GB GPUs, which remains prohibitively expensive for many organizations. Moreover, considering the diversity of application scenarios, each downstream application needs to be trained separately, occupying the same space as the original LPLM, which not only increases deployment costs but also challenges the reliability and stability of online services.\nRecent research advancements have proposed many parameter-efficient fine-tuning methods based on LPLMs, aimed at reducing the demand for GPU computational power and storage resources during the training of downstream tasks. In the full-parameter fine-tuning process, all trainable parameters of the LPLM need to be updated. However, during the training process of parameter-efficient fine-tuning methods, the original parameters of the LPLM are frozen, and only a small number of trainable parameters are updated through gradients. This change is comparable in performance to full-parameter fine-tuning and significantly reduces the demand for computational power and storage.\nIn the field of parameter-efficient fine-tuning technology research for LPLMs, there are mainly three types of methods: First, Addition-based methods insert small trainable extension structures into the layers of the LPLM. During fine-tuning, only the trainable parameters within the extension structures are updated, while the original model’s structure and parameters remain unchanged. A representative work in this category is Adapter Tuning (Houlsby et al., 2019  ###reference_b18###; Rebuffi et al., 2017  ###reference_b37###; He et al., 2022  ###reference_b16###), which may increase the model’s depth and complexity. Second, Specification-based methods designate certain parameters within the LPLM as trainable and freeze the rest. BitFit, for instance, updates only the model’s bias parameters during fine-tuning (Zaken et al., 2021  ###reference_b51###). Lastly, Reparameterization-based methods transform existing parameters into trainable ones. A pivotal work is LoRA (Low-Rank Adaptation), proposed by Hu et al. (2022  ###reference_b19###). LoRA approximates the increment matrix  of the pre-trained model parameters  through the product of low-rank decomposition matrices  and . Since the low-rank dimension  of  and  is much smaller than the dimension of , and the parameter matrix  is frozen during training, updating only the  and  parameter matrices significantly reduces the amount of trainable parameters. For instance, fine-tuning the GPT-3 175B model with LoRA can reduce the number of training parameters by a factor of ten thousand and the GPU memory requirement by a factor of three. There are mainly two types of methods based on LoRA improvements: one is performance optimization, such as AdaLoRA (Zhang et al., 2023b  ###reference_b54###), IncreLoRA (Zhang et al., 2023a  ###reference_b53###), and DELTA-LoRA (Zi et al., 2023  ###reference_b55###), which optimize LoRA in various aspects to enhance model performance. The other is functional expansion, like LongLoRA (Chen et al., 2023  ###reference_b6###) and QLoRA  (Dettmers et al., 2024  ###reference_b8###), which expand the model’s applicability in long contexts and quantization based on LoRA as a foundational component. The LoRA fine-tuning technique, with its advantages of excellent performance, simple structure, efficient training, and no inference delay, is expected to continue to drive the development and industrial application of large models.\nDespite significant progress in fine-tuning with reparameterization methods (Hu et al., 2022  ###reference_b19###; Chen et al., 2023  ###reference_b6###; Zhang et al., 2023b  ###reference_b54###; Zi et al., 2023  ###reference_b55###) represented by LoRA, it still faces several key challenges: First, the structure of the parameter decomposition matrix is too simple and singular, making it difficult to dynamically represent various semantically complex downstream tasks. Second, performance fluctuation is significant. According to experiments by Hu et al. (2022  ###reference_b19###), LoRA has high performance fluctuation in the Corpus of Linguistic Acceptability (CoLA) task, with a standard deviation reaching 1.2%. Lastly, there is the issue of computational complexity. AdaLoRA (Zhang et al., 2023b  ###reference_b54###) and IncreLoRA (Zhang et al., 2023a  ###reference_b53###) optimize the size of the parameter incremental matrix rank () through a mechanism that measures the importance scores of the singular value decomposition triplets of the parameter incremental matrix, allowing for different amounts of trainable parameters in incremental matrices at different positions. However, this requires iterative calculations of the importance scores of the triplets in more parameter decomposition matrices to dynamically adjust the size of the incremental matrix rank (), significantly increasing the computational complexity. These challenges limit the application of LPLMs in a broader domain, and addressing these issues is crucial for advancing the development of large language models.\nThis study is inspired by the idea that the brain’s functionality is shaped by its geometric structure (Pang et al., 2023  ###reference_b31###) and integrates this concept into LoRA technology. We propose a novel matrix transformation-based reparameterization method for efficient fine-tuning named MTLoRA. The spatiotemporal patterns of brain neural activity are the excitation of different wavelength characteristic modes of its geometric structure (Pang et al., 2023  ###reference_b31###). MTLoRA employs a transformation-matrix  to perform linear transformations on task-specific parameter matrices, such as rotation, scaling, and translation, dynamically altering their spatial geometric structure to generate new matrix feature patterns (eigenvectors). This mimics the fundamental impact of complex geometric structure characteristic patterns in the brain on functionality, enhancing the performance of the fine-tuned model, thereby alleviating the aforementioned issues. The  transformation matrix contains four different structures, each designed to simulate the characteristic patterns of geometric structures at different levels of the brain.\nIn this paper, through empirical experiments on two categories of 11 tasks in NLU and NLG, the effectiveness of the MTLoRA method is verified. Compared to the LoRA method, it is evident from the experimental results table 1  ###reference_### that MTLoRA improves performance by about 1.54% (=0.1%) on the CoLA (Warstadt et al., 2019  ###reference_b47###) task, effectively reducing the standard deviation; it enhances performance by about 3.61% (=0.8%) on the RTE (Dagan et al., 2005  ###reference_b7###; Haim et al., 2006  ###reference_b15###; Giampiccolo et al., 2007  ###reference_b14###; Bentivogli et al., 2009  ###reference_b2###) task; it increases performance by about 2.45% (=0.3%) on the MRPC (Dolan and Brockett, 2005  ###reference_b10###) task; and it boosts performance by about 0.88% (=0.0%) on the QQP (Quora Question Pairs) task. From the experimental results table 2  ###reference_###, it is known that MTLoRA achieves an average performance increase of about 0.95% (=0.1%) on the DART (Nan et al., 2020  ###reference_b26###) task and about 0.56% (=0.1%) on the WebNLG (Gardent et al., 2017  ###reference_b13###) task.\nOverall, the MTLoRA fine-tuning method proposed in this paper demonstrates significant advantages in downstream tasks based on LPLMs, which include:\n1) The rich matrix transformation structure significantly enhances model performance in many tasks while maintaining the simplicity of the algorithm;\n2) Effectively reduces performance fluctuations in certain tasks;\n3) After training is completed, the incremental parameter matrix  can be merged with the original parameter matrix , achieving no additional latency during the inference stage;\n4) Compared to full fine-tuning, the MTLoRA method significantly increases training speed, drastically reduces the amount of trainable parameters by 99%, and lowers the hardware requirements by several times.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "The emergence of parameter-efficient fine-tuning methods provides a new direction for model fine-tuning by updating all trainable parameters, achieving more efficient and flexible fine-tuning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Method",
            "text": "Our proposed MTLoRA fine-tuning approach is inspired by the idea that the brain’s functionality is shaped by its geometric structure (Pang et al., 2023  ###reference_b31###). Specifically, the neural activities of the brain are incited by the inherent resonance modes of its geometric topological structure, and the task-induced activations are the excitation of whole-brain modes, with wavelengths exceeding 60mm. Furthermore, the dominant role of wave-like activity explains the close connection between the brain’s geometric structure and its function, and wave dynamics are utilized to reconstruct the spatiotemporal characteristics of spontaneous and task-induced brain activity recordings.\nThe intrinsic resonance modes of the geometric structure of the brain’s neocortex can be characterized by its geometric feature patterns (Melrose and McPhedran, 1991  ###reference_b24###; Nozari et al., 2020  ###reference_b29###). These geometric feature patterns can be obtained by solving the eigenvalue problem of the Laplace–Beltrami Operator (LBO) (Chavel, 1984  ###reference_b5###; Seo and Chung, 2011  ###reference_b39###) constructed on the basis of the group average template mesh representation (Fischl et al., 1999  ###reference_b12###) of the neocortex. The geometric feature patterns obtained in this manner include physical properties such as the curvature of the neocortex surface and the spatial relationships between vertices in the mesh (Wachinger et al., 2015  ###reference_b44###), analogous to how the resonance frequencies of a violin string are determined by the string’s density, tension, and length. In essence, geometric feature patterns represent the vibrational modes of the system’s dynamics (Lévy, 2006  ###reference_b20###), and the spatiotemporal patterns of neural activity in the neocortex are the excitations of these geometric feature patterns, just as the harmonics produced by plucking a violin string are vibrations of its own resonance modes.\nThe spatiotemporal patterns of neural activity in the neocortex can be decomposed into a weighted sum of geometric feature patterns of different wavelengths (Nowack, 1995  ###reference_b28###; Robinson et al., 2016  ###reference_b38###). According to the experimental results of  (Pang et al., 2023  ###reference_b31###), this decomposition can reconstruct the functional magnetic resonance imaging (fMRI) experimental data of neocortical activity obtained under spontaneous and task-induced conditions with more than 80% accuracy, thereby confirming that the brain’s geometric structure shapes its function. This relationship between the brain’s geometric structure and function can also be extended to subcortical activities, such as those in the thalamus, striatum, and hippocampus, indicating that the close connection between geometric structure and function is a ubiquitous presence in the brain.\nWe believe LPLMs should also possess rich and complex spatial geometric structure feature patterns. We aim to find a fine-tuning method that can flexibly change the geometric structure of parameter matrices based on the characteristics of downstream tasks, generating new matrix feature patterns (eigenvectors), to simulate the impact of complex geometric structure feature patterns in the brain on function, thereby improving model performance.\nThis section will introduce MTLoRA fine-tuning method in detail, using large pre-trained language models based on the Transformer architecture as an example. Transformer-based models, such as the RoBERTa model, contain multiple Blocks, each with numerous dense parameter matrices, including word embedding projection matrices (), query/key/value projection matrices (, , ), intermediate layer projection matrices (), output layer projection matrices (), and weight matrices in the MLP layer (), among others. These parameter matrices carry rich semantic information, which is our optimization target.\nMTLoRA aims to use a transformation-matrix  to perform linear transformations on parameter matrices specific to a task, changing their spatial geometric structure, and generating new matrix feature patterns, to mimic the fundamental impact of complex geometric structure feature patterns in the brain on function, thereby improving the model’s performance after fine-tuning training. Specifically, MTLoRA uses the product of low-rank parameter increment matrices , , and , i.e., , to approximate the increment matrix  of the pre-trained parameter matrix  in the LPLM, where rank  <<  or . During training,  is frozen, and  performs task-specific adaptive linear transformations on  and , including scaling, rotation, translation, reflection, skewing, and projection.  contains four different structures, each designed to simulate different levels of geometric feature patterns in the brain, where structure 1 is the most basic and can handle most downstream application scenarios, while the other structures are more inclined to handle specific application scenarios with scarce or abundant corpora:\nStructure 1 SHIM (Spatial Harmonic Integration Matrix): This structure is designed to integrate different spatial feature transformations, similar to how various frequencies of harmonics combine in music to produce rich and complex timbres. By introducing the transformation matrix , it applies linear transformations such as spatial rotation, scaling, translation, and shearing to the  and  parameter matrices based on specific tasks, forming a comprehensive feature representation. This integration not only enhances the model’s adaptability to different tasks but also provides a richer expression capability for adjusting model parameters, enabling the model to better adapt and optimize the processing effects of specific tasks. Its forward propagation mathematical expression in the model is shown as 4  ###reference_###:\nwhere .  represents the input vector, and  represents the output vector. Because  <<  or , the number of parameters in the  transformation matrix is very small, adding minimal overhead. The  and  matrices are initialized using random Gaussian initialization, while the  matrix is initialized as a zero matrix, as illustrated in Figure 1  ###reference_### (A).\nStructure 2 ICFM (Intrinsic Correlation Feature Matrix): This structure, by introducing the transformation matrix  and its transpose , results in a positive semi-definite matrix after multiplication, which can simulate a covariance matrix capturing the intrinsic correlation structure within the feature space. This positive semi-definite matrix applies linear transformations such as rotation and scaling to the  and  parameter matrices, highlighting task-specific features within the feature space, thus enhancing the model’s representational ability by increasing the differentiation between patterns. This structure shares similarities with the covariance matrix in Mahalanobis distance and spatial rotation in Principal Component Analysis (PCA) (Wold et al., 1987  ###reference_b50###). Its forward propagation mathematical expression in the model is shown as 5  ###reference_###:\nwhere , , representing the transpose of matrix , , as illustrated in Figure 1  ###reference_### (B).\nStructure 3 CTCM (Composite Transformation Coupling Matrix): By introducing two different matrices,  and , and forming a composite transformation matrix through matrix multiplication, this structure enhances the expressiveness of the parameter increment matrix, thereby fostering the model’s adaptability to complex tasks. Its forward propagation mathematical expression in the model is shown as 6  ###reference_###:\nwhere , , as illustrated in Figure 1  ###reference_### (C).\nStructure 4 DTSM (Dual Transformation Superposition Matrix): This structure utilizes the additive operation of matrices  and  to form a superposition transformation matrix, facilitating complex interactions between model parameters. This structure allows for detailed scaling and modulation of features, thereby providing enhanced expressiveness and flexibility to the model to address complex task requirements. Its forward propagation mathematical expression in the model is shown as 7  ###reference_###:\nwhere , . This approach allows for more complex interactions between  and , enhancing the model’s expressiveness and flexibility, as illustrated in Figure 1  ###reference_### (D).\nThe structural schematic of the MTLoRA fine-tuning method, as shown in Figure 1  ###reference_###.\nThe MTLoRA method can be applied not only in Dense Layers based on the Transformer architecture model but also in any neural network structure with Dense Layers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "This experiment aims to validate the effectiveness of the MTLoRA method in natural language processing tasks. Specifically, the experimental design includes a total of eleven tasks in two major categories: NLU and NLG. GPT-2 Medium (Radford et al., 2019  ###reference_b34###) and RoBERTa-base (Liu et al., 2019  ###reference_b22###) are selected as the base pre-trained models, upon which fine-tuning is performed using MTLoRA and LoRA techniques. To comprehensively evaluate the performance of the MTLoRA method, the experimental results of MTLoRA will be compared and analyzed against existing fine-tuning methods, including Full Fine-Tuning, Adaptive Tuning, Bitfit, and LoRA."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baseline",
            "text": "This study selected recently representative fine-tuning methods for comparative validation to evaluate the performance of each method on specific tasks. Below is a detailed description of each fine-tuning method:\nFull Fine-Tuning involves updating all trainable parameters during the fine-tuning process of downstream tasks. Full fine-tuning typically achieves better performance but requires more GPU computational resources, larger storage space, and longer training time (Qiu et al., 2020  ###reference_b33###; Raffel et al., 2020  ###reference_b35###).\nAdapter Tuning is an addition-based fine-tuning method that introduces extra trainable extension structures within the blocks of a Large Pre-trained Language Model (LPLM). During the fine-tuning process, the original parameters of the LPLM are frozen, with only the trainable parameters in the newly added extension structures being updated. Compared to full-parameter fine-tuning, adapter tuning significantly reduces the number of trainable parameters in the model and can achieve similar or even superior performance (Houlsby et al., 2019  ###reference_b18###; Rebuffi et al., 2017  ###reference_b37###; He et al., 2022  ###reference_b16###).\nBitFit is a method that specifies the updating of Bias parameters in LPLM while freezing the rest of the parameters. As described by Zaken et al. (2021  ###reference_b51###), BitFit significantly reduces the amount of parameter updates during the fine-tuning process and is able to maintain good performance.\nIn the replication experiments, the LoRA fine-tuning method maintained all hyperparameter configurations consistent with those described by Hu et al. (2022  ###reference_b19###).\nAda-LoRA is typically applied to all linear layer parameter matrices in LPLM (Zhang et al., 2023b  ###reference_b54###), but for effective comparison, we utilized the RoBERTa-base pre-trained model and applied the Ada-LoRA method to the query/value projection matrices  and , with experimental data derived from the Delta-LoRA (Zi et al., 2023  ###reference_b55###) experiments.\nDelta-LoRA uses the hyperparameter settings of LoRA as its baseline configuration to facilitate direct performance comparisons across various models (Zi et al., 2023  ###reference_b55###). In natural language understanding (NLU) tasks, Delta-LoRA reduces the input sequence length from 512 to 256 to decrease GPU memory demand and speed up the training process. Moreover, the batch size for different tasks is increased, for instance, from 16 to 120, with the update rate () set to 0.5. Apart from these changes, the remaining hyperparameter configurations are consistent with LoRA. For natural language generation (NLG) tasks, the update rate () is adjusted to 2. The Delta-LoRA method is applied to the query/value projection matrices  and  in LPLM."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Natural Language Understanding Tasks",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Base Model and Dataset",
            "text": "In this experiment, we employed RoBERTa-base as the foundational pre-trained model, which was introduced by  Liu et al. (2019  ###reference_b22###) through the Facebook AI Research Lab. RoBERTa is an advancement over the BERT pre-training model, designed to enhance the model’s performance on downstream tasks. RoBERTa implemented several improvements over BERT, including: (1) training the model with larger batch sizes, over longer periods, and with more data; (2) removing the Next Sentence Prediction (NSP) from the optimization objectives; (3) employing dynamic masking techniques during training; and (4) utilizing longer sentences in the training corpus. Due to its superior performance and moderate parameter size, the RoBERTa pre-trained model is widely utilized in performance testing for various downstream tasks based on pre-trained models.\nTo validate the effectiveness of the MTLoRA fine-tuning method in natural language understanding tasks, we conducted empirical experiments using the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018  ###reference_b45###), which includes eight tasks, specifically:\nCoLA (Corpus of Linguistic Acceptability): Created by Warstadt et al. (2019  ###reference_b47###), this dataset comprises a series of English sentences sourced from books and journals on linguistic theory, aimed at assessing the grammatical acceptability of English sentences. The classification results are categorized as \"acceptable\" and \"unacceptable.\" CoLA contains 8,551 training samples, 1,043 validation samples, and 1,063 test samples. The Matthews correlation coefficient (Matthews, 1975  ###reference_b23###) is used as the performance evaluation metric, suitable for imbalanced binary classification tasks.\nMNLI (Multi-Genre Natural Language Inference): Introduced by Williams et al. (2017  ###reference_b49###), each sample consists of a premise sentence and a hypothesis sentence, with the primary task being to judge the textual entailment relationship between these two sentences. MNLI sentences cover ten genres, including transcripts of speeches, fiction, news reports, and government reports, requiring the model to handle texts of various styles, types, and domains. MNLI includes 392,702 training samples, 19,647 validation samples, and 19,647 test samples.\nMRPC (Microsoft Research Paraphrase Corpus): Proposed by Dolan and Brockett (2005  ###reference_b10###), the dataset contains sentence pairs automatically extracted from news sources, primarily to determine whether two sentences have a paraphrasing relationship. The distribution of these two categories is imbalanced, with about 68% being positive samples, where the semantic similarity of sentence pairs is manually annotated. MRPC includes 3,668 training samples, 408 validation samples, and 1,725 test samples.\nQNLI (Question Natural Language Inference): Provided by Wang et al. (2018  ###reference_b45###), samples are derived from the Stanford Question Answering Dataset (Rajpurkar et al., 2016  ###reference_b36###) and converted into a classification task. The main task is to determine whether a given statement sentence contains the answer to a given question sentence, with sentences collected from Wikipedia and questions manually written. QNLI contains 104,743 training sample pairs, 5,463 validation sample pairs, and 5,463 test sample pairs.\nQQP (Quora Question Pairs): Offered by the question-and-answer website Quora, the main task is to determine whether two question sentences are semantically similar. The distribution of these two categories is imbalanced, with about 63% being negative samples. QQP includes 363,846 training samples, 40,430 validation samples, and 390,965 test samples (Wang et al., 2018  ###reference_b45###).\nRTE (Recognizing Textual Entailment): Combines results published by multiple research organizations (Dagan et al., 2005  ###reference_b7###; Haim et al., 2006  ###reference_b15###; Giampiccolo et al., 2007  ###reference_b14###; Bentivogli et al., 2009  ###reference_b2###) between 2005 and 2011 during the RTE challenges. The sample data, composed of Wikipedia and news data, has a primary task similar to MNLI but on a smaller scale, including 2,490 training samples, 277 validation samples, and 3,000 test samples.\nSST-2 (Stanford Sentiment Treebank): Introduced by Socher et al. (2013  ###reference_b40###), the main task is to determine the sentiment orientation of a given sentence, whether positive or negative. The sample data mainly comes from movie review data, with reviews manually annotated for sentiment classification. SST-2 includes 67,349 training samples, 872 validation samples, and 1,821 test samples.\nSTS-B (Semantic Textual Similarity Benchmark): Proposed by Cer et al. (2017  ###reference_b4###), the main task is to calculate the semantic similarity score between two sentences. Samples are extracted from sentence pairs collected from videos, image captions, news headlines, etc., with sentence pair similarity manually rated on a scale from 1 to 5. Model performance is evaluated using the Pearson and Spearman correlation coefficients. STS-B includes 5,749 training samples, 1,500 validation samples, and 1,379 test samples."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Experimental Details",
            "text": "This experiment was conducted on the basis of the RoBERTa-base as the pre-trained model, applying both MTLoRA and LoRA methods across eight tasks on the GLUE benchmark. The following settings were employed during the training and testing process:\nAdamW was chosen as the optimizer for the model, along with a linear learning rate decay strategy;\nThe GPU processor used was NVIDIA A100-PCIE-40GB;\nThe hyperparameter configuration was consistent with the settings by Hu et al. (2022  ###reference_b19###), including learning rate, maximum sequence length (MaxSeqLength), rank () size, batch size (BatchSize), and epochs. Detailed hyperparameter settings can be found in Appendix A.1  ###reference_###;\nFor each task, the reported experimental results are based on the median and standard deviation obtained from training and testing with three different random seeds;\nThe MTLoRA method was applied to the query/value projection matrices (, ), intermediate layer projection matrix (), and output layer projection matrix (), while the LoRA method was applied to the query/value projection matrices (, )."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Experimental Results",
            "text": "In this experiment, we explored the impact of different  transformation matrix structures of the MTLoRA method on natural language understanding tasks. The test experiment results of the LoRA and MTLoRA methods based on the RoBERTa-base model across eight tasks of the GLUE benchmark are shown in Table 1  ###reference_###:\nWe use the results of LoRA (Hu et al., 2022  ###reference_b19###) as the primary benchmark for comparison. From the results in Table 1, it can be seen that MTLoRA achieves an average performance improvement of about 1% across all tasks and controls the standard deviation fluctuations well. Specifically:\nThe SHIM structure exhibits enhanced performance and stability across a majority of tasks, with observed performance gains of roughly 1.29% (=1.4%) on CoLA, 0.87% (=0.0%) on QQP, and 3.61% (=0.8%) on RTE tasks. This performance indicates its effectiveness and wide-ranging utility in a variety of general tasks.\nIn contrast, the ICFM structure shows robust performance across numerous tasks, particularly excelling in tasks related to semantic similarity and entailment when there is a lack of comprehensive corpus data. For example, it achieves an average performance boost of approximately 1.22% (=0.6%) on the MRPC task and 1.08% (=0.6%) on the RTE task.\nMeanwhile, the CTCM structure delivers consistent performance across a spectrum of tasks, standing out particularly in inference tasks with extensive corpus data. It demonstrates a performance increase of 1.0% (=0.6%) on the CoLA task, indicating a noteworthy reduction in standard deviation when compared to the SHIM structure.\nLastly, the DTSM structure continues to show steady performance in various tasks, particularly highlighting an improvement of around 0.88% (=0.0%) on the QQP task when there is ample corpus data, and a mean performance enhancement of about 1.54% (=0.1%) on the CoLA task. This significant decrease in standard deviation underscores the DTSM structure’s efficiency in managing semantic similarity tasks with abundant corpora.\n###figure_2### ###figure_3### During the training process, the changes in loss for different transformation matrix structures of the MTLoRA model and the LoRA model across the eight tasks of the GLUE benchmark are illustrated in Figures 2  ###reference_### and 3  ###reference_###. From the figures, it can be observed that compared to LoRA, the MTLoRA method exhibits lower loss, faster convergence speed, and lower standard deviation of fluctuations, indicating that MTLoRA indeed can enhance model performance on multiple tasks.\nOverall, the different transformation matrix structures in the MTLoRA method may demonstrate superior performance on various tasks. Therefore, in practical applications, an appropriate transformation matrix structure should be selected."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Natural Language Generation Tasks",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Base Model and Datasets",
            "text": "In this experiment, we employ the GPT-2 Medium model released by OpenAI in 2019 as the pre-trained base model. This model is widely used in various text generation tasks due to its strong natural language generation capabilities (Radford et al., 2019  ###reference_b34###). We conducted experiments on three broadly recognized datasets: the E2E NLG Challenge, WebNLG, and DART.\nThe E2E NLG Challenge dataset was introduced by Novikova et al. (2017  ###reference_b27###), primarily aiming to advance the research of end-to-end, data-to-text natural language generation systems. This dataset poses multiple challenges for the performance of natural language generation systems, including a large number of samples, complex syntactic structure variations, rich vocabulary, and diverse sentence structures. The sample content pertains to the restaurant domain, requiring the system to generate detailed and fluent sentences based on given structured information. The dataset contains approximately 42,000 training samples, 4,600 validation samples, and 4,600 test samples.\nThe WebNLG dataset was introduced by Gardent et al. (2017  ###reference_b13###), and its primary task is to convert structured RDF (Resource Description Framework) triple data into fluent natural language text, facilitating the training and performance evaluation of generation systems. WebNLG thoroughly examines the system’s capabilities in micro-planning, which includes several sub-tasks such as sentence segmentation, lexicalization, referring expression generation, information aggregation, and surface realization. The dataset focuses on 14 categories within the DBpedia domain and specifically notes that 5 of these categories are not included in the training set but only appear in the test set, to assess the model’s generalization ability on unseen data. The dataset comprises approximately 22K samples.\nThe DART dataset was introduced by Nan et al. (2020  ###reference_b26###), with the primary task of converting structured triple data into high-quality natural language text for training and performance evaluation of generation systems. DART is characterized by its large scale, broad domains, and a robust structured data ontology semantic representation framework. Unlike the E2E and WebNLG datasets, which utilize a flat slot-value ontology representation structure, DART employs a unique tree-shaped ontology semantic representation framework. This framework more effectively encodes the rich semantic dependencies between ontologies in structured data."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Experimental Details",
            "text": "This experiment conducts tests on the E2E NLG Challenge, WebNLG, and DART tasks using the MTLoRA and LoRA methods, with the GPT-2 Medium model serving as the pre-trained base. The following settings were adopted during the training and testing phases:\nThe AdamW optimizer was chosen for the model, along with a linear learning rate decay strategy.\nThe GPU used was the NVIDIA A100-PCIE-40GB.\nThe hyperparameter configuration was consistent with the settings of Hu et al. (2022  ###reference_b19###), including learning rate, BeamSize, rank () size, batch size (BatchSize), training cycles (Epoch), etc. Detailed hyperparameter settings can be found in Appendix A.2  ###reference_###.\nFor each task, the reported experimental results are the averages and standard deviations based on three sets of training and testing with different random seeds.\nThe MTLoRA method was applied to the weight matrices in the query/key/value projection matrices (, , ) and the MLP layer (), while the LoRA method was applied to the query/value projection matrices (, ).\nSince the query/key/value projection matrices (, , ) of the GPT-2 Medium model are generated in a merged manner, this experiment utilized the MergedLinear stepwise convolution structure to address this situation. MergedLinear implements stepwise convolution through a transformation matrix, . Specifically, first, the parameter matrix  uses the transformation matrix  as a convolution kernel to perform joint convolution across the , ,  channels, obtaining the transformed matrix  after the first convolution step. Matrix  can integrate the associated information within the  structure, beneficial for enhancing the model’s representational capability. Subsequently, matrix  undergoes independent convolution on the , ,  channels using the parameter matrix  as a convolution kernel, resulting in the parameter increment matrix  after the second convolution step, where  <<  or . The structural diagram is illustrated in Figure 4  ###reference_###.\n###figure_4###"
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Experimental results",
            "text": "In this experiment, we explored the impact of different  transformation matrix structures of the MTLoRA method on natural language generation tasks. The test experiment results of the LoRA and MTLoRA methods, based on the GPT-2 Medium pre-trained model on the E2E, DART, and WebNLG datasets, are presented in Table 2  ###reference_###:\nWe use the experimental results of LoRA (Hu et al., 2022  ###reference_b19###) as the primary benchmark for comparison. From the results in Table 2  ###reference_###, it is evident that the four transformation matrix structures of the MTLoRA fine-tuning method effectively enhance the model’s performance on the DART and WebNLG tasks. Specifically:\nThe incorporation of the SHIM structure led to an enhancement in the model’s efficacy on the DART and WebNLG tasks, with an improvement of approximately 0.83% (=0.0%) and 0.56% (=0.1%), respectively. Similarly, the implementation of the ICFM structure resulted in a performance increment of roughly 0.65% (=0.1%) for DART and 0.22% (=0.5%) for WebNLG. The application of the CTCM structure contributed to a performance uplift on the DART and WebNLG tasks by approximately 0.88% (=0.0%) and 0.27% (=0.2%), respectively. Lastly, the adoption of the DTSM structure facilitated a performance boost on the DART and WebNLG tasks by an estimated 0.95% (=0.1%) and 0.31% (=0.3%), respectively.\nThe experimental results of MTLoRA on natural language generation tasks once again demonstrate that different transformation matrix structures of the MTLoRA fine-tuning method may achieve optimal performance on different tasks. Particularly, when the query/key/value projection matrices (, , ) of the pre-trained model are generated jointly, the stepwise convolutional transformation design of MTLoRA can effectively integrate and share the associated information of these three components, better supporting downstream tasks."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Experimental Analysis",
            "text": "To further validate the effectiveness of the MTLoRA method, we adapted LoRA to the query/value projection matrices (, ), the intermediate layer projection matrix (), and the output layer projection matrix () of the RoBERTa-base model. The other experimental details remain consistent with the description in section 4.2.2  ###reference_.SSS2###. The experimental results on the GLUE benchmark for natural language understanding are shown in Table 3  ###reference_###:\nFrom the experimental results in Table 3  ###reference_###, LoRA shows a slight improvement in performance on the QNLI and MRPC tasks of the GLUE benchmark. However, there is a significant fluctuation in performance on the QQP task, with a standard deviation as high as 12.5%, and a 2% performance fluctuation on the RTE task. This indicates that merely increasing the amount of trainable parameters does not always lead to performance improvements and can sometimes result in performance degradation.\nPerformance of LoRA on E2E Tasks. To verify whether the MTLoRA method actually reduces the model’s performance on the E2E task in NLG, we re-executed the performance test of LoRA on the E2E task using the same computational environment as MTLoRA. The experimental results show that LoRA achieves an average BLEU score of 69.52% on the E2E task, with a standard deviation of 0.5%. Its average BLEU score is comparable to that of MTLoRA, but with an increased standard deviation. This indicates that the experimental computational environment also has an impact on the results, suggesting that MTLoRA does not actually reduce the model’s performance on the E2E task in NLG.\nSensitivity Analysis of Rank (). This study employs the MTLoRA and LoRA methods based on RoBERTa-base, conducting experiments on the COLA task with varying sizes of rank () to assess the specific impacts of rank () on model efficacy and stability. All experiments were carried out with the same random seed, and the specific settings of the experiments are consistent with those described in Section 4.2.2  ###reference_.SSS2###, with the corresponding results displayed in Figure 5  ###reference_###:\n###figure_5### Several observations can be made from the experimental results in Figure 5  ###reference_###: Firstly, the MTLoRA method continues to exhibit good performance and stability on the COLA task across different rank sizes without evident overfitting. Particularly, the ICFM structure in MTLoRA demonstrates exceptional performance and stability, indicating that the ICFM structure can effectively capture the task’s key features, thereby efficiently preventing overfitting. Secondly, compared to the traditional LoRA, MTLoRA can achieve performance improvements with larger ranks while adding fewer parameters. Lastly, the effectiveness of MTLoRA is not solely due to the increased parameters at the same rank, as the performance curve within the examined range suggests that MTLoRA can outperform some higher-ranked LoRAs even at lower ranks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this paper, we explored fine-tuning techniques based on LPLMs, which efficiently adjust a small number of parameters to significantly reduce the computational power and space requirements of the model while maintaining its performance on downstream tasks. Among these, the reparameterization-based fine-tuning technique represented by LoRA stands out, being widely applied to various task scenarios. However, LoRA and its improved methods still have shortcomings in complex task adaptability, performance, stability, and algorithmic complexity.\nTo mitigate these issues, we proposed MTLoRA. This method applies linear transformations to task-specific parameter matrices through the  transformation matrix, altering their spatial geometric structure to generate new matrix feature patterns. This mimics the fundamental impact of different geometric structural feature patterns in the brain on functions, thereby alleviating the aforementioned problems. Through a series of experimental validations, the MTLoRA method has improved performance in multiple task scenarios compared to the LoRA method while also reducing standard deviation. Notably, the MTLoRA method not only enhances performance but also maintains the simplicity of the algorithm, without increasing inference stage latency or reducing the length of the input sequence. Furthermore, the MTLoRA method is applicable not only to the Transformer architecture but also to other neural network structures.\nFuture research will focus on continuously optimizing the MTLoRA method by integrating more in-depth principles of brain neuroscience. It will be applied in the field of safe alignment for large models, ensuring that the models not only perform exceptionally in specific tasks but also possess robust security capabilities. This will contribute to the sustainable development of large model applications. Moreover, considering the effectiveness of the SHIM, ICFM, CTCM, and DTSM transformation matrix structures, we plan to study more brain-inspired transformation matrix structures to further enhance the model’s performance, reduce the standard deviation, and improve the model’s adaptability in various downstream tasks. MTLoRA can also serve as a fundamental component to strengthen the model’s capabilities in handling long contexts and parameter quantization."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "This work was supported by the National Science and Technology Major Project (Grant No. 2022ZD0116202)."
        }
    ]
}