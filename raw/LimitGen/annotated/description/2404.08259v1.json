{
    "title": "Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024",
    "abstract": "Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using combined metrics: BLEU, chrF and TER. Statistical significance results with Bonferroni correction show surprisingly high baseline systems, and that Back-translation leads to significant improvement. Furthermore, we present a qualitative analysis of translation errors and system limitations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Neural Machine Translation (NMT) has progressed so far to reach human-level performance on some languages [1  ###reference_b1###] and has become one of the most prominent approaches within the research area of Machine Translation (MT). Its easy-to-adapt architecture has achieved impressive performance and high accuracy. Promising methods that fall under NMT include Transfer Learning [2  ###reference_b2###, 3  ###reference_b3###], pre-trained language models [4  ###reference_b4###, 5  ###reference_b5###], and multilingual models [6  ###reference_b6###, 7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###] etc.\nHowever, existing NMT resources focus overwhelmingly on high-resource languages, which dominate a great portion of contents on the Internet and Social Media. Low-resource languages are often spoken by minorities with minimal online presence and insufficient amount of resources to achieve comparable NMT results [10  ###reference_b10###, 11  ###reference_b11###], but they might even have a very large population of speakers and still be under-resourced (such as Hindi, Bengali and Urdu). Growing interest in low-resource MT is evident through the annually held Conference on Machine Translation (WMT). In 2021, WMT featured tasks to promote MT in low-resource scenarios by exploring similarity and multilinguality [12  ###reference_b12###]. Among all tasks, the objective of the Very Low Resource Supervised Machine Translation task [13  ###reference_b13###] focused on Transfer Learning between German and Upper Sorbian. The task examined effects of utilizing similar languages and results show that combining Transfer Learning and data augmentation can successfully exploit language similarity during training.\nWe introduce our experiment to develop bidirectional state-of-the-art NMT systems for German and Bavarian, a classic high-resource to/from low-resource language pair. Inspired by WMT21, our experiment explores the generalizability of Back-translation and Transfer Learning from the highest-ranking approach from [14  ###reference_b14###]. Our approach covers the following: First, a simple Transformer [15  ###reference_b15###] is trained as the baseline. Secondly, we use the base model for Back-translation and take the extended corpus to train our second model. Lastly, we experiment with Transfer Learning [3  ###reference_b3###] by introducing German-French as the parent model. For evaluation we opt for a combination of three metrics: BLEU [16  ###reference_b16###], chrF [17  ###reference_b17###] and TER [18  ###reference_b18###]. Recent studies have argued that using BLEU as a single metric neglects the complexity of different linguistic characteristics. Using combined metrics and having various penalization standards may be able to capture translation errors more diversely [19  ###reference_b19###, 20  ###reference_b20###].\nBy choosing the language pair Bavarian / German we offer one exemplar for a low-resource language (combined with a high-resource one) that can serve as a reference point for further experimental work applied to other low-resource MT. This will ultimately help addressing the imbalance that still prevails between a handful of well-resourced languages and the many others that are not.\nThis paper makes the following contributions:\nWe offer a systematic evaluation of state-of-the-art NMT approaches for a language pair involving a low-resource language that has attracted little attention so far. This investigation explores both translation from as well as into the low-resource language. We focus on a Transformer baseline against Back-translation and a Transfer Learning approach.\nTo foster reproducibility and replicabilty (which is in the very spirit of SIGUL, LREC and COLING) we make all code available via a GitHub project repository111https://github.com/whher/nmt-de-bar  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Non- and semi-parametric methods have rarely been applied to MT tasks in recent years. [33  ###reference_b33###] demonstrate a powerful combination of neural networks and non-parametric retrieval mechanisms to improve translation. kNN-MT follows the retrieval principle and proposes a more efficient non-parametric translation method, which augments the decoder of a pre-trained NMT model with a nearest neighbor retrieval mechanism, allowing direct access to data store of cached examples [34  ###reference_b34###]. This approach scales the decoder to an arbitrary amount of examples at test time, particularly strengthening decoderâ€™s translation capability. However, the big drawback is high computational cost and low decoding speed due to word-by-word generation. Chunk-based kNN-MT [35  ###reference_b35###] solves this problem by processing translation in chunks of words instead of passing single tokens through the data store.   \nin MT is often done by training a high-resource language pair and using this parent model to initialize parameters in a child model with low-resource languages. For example, [3  ###reference_b3###] achieved translation improvements for Hansa, Turkish and Uzbek into English by using French-English as a parent model. Experiments from [36  ###reference_b36###] showed improvements using Transformers [15  ###reference_b15###] to train low-resource languages such as Estonian and Slovak. Their results pointed out key factors for a successful transfer include the size of the parent corpus and sharing the target or source language. For instance, Estonian-English as a child gained up to 2.44 BLEU with Finnish-English as a parent.  \nIn Dual Transfer [2  ###reference_b2###], two parent models are used to initialize one child. Monolingual and parallel parent data were trained separately so that inner layers and embeddings can be transferred separately. Another recent study extends conventional transfer learning by additionally transferring probability distributions from parent to child. The Consistency-based Transfer Learning [37  ###reference_b37###] argues that parent prediction distribution is highly informative and can be useful to guide child translation. Their experiment showed that using German-English as a parent can achieve BLEU improvement up to 6.2 for Indonesian-English. Furthermore, the study from [6  ###reference_b6###] investigated a technique to incrementally add new language pairs to a multilingual MT model based on knowledge transfer, without posing the original model at risk for catastrophic forgetting.  \n(PLMs) can be fine-tuned on low-resource languages. For instance, MT quality between Spanish and Quecha was shown to improve by leveraging Spanish-English and Spanish-Finnish PLMs [4  ###reference_b4###], with the latter yielding better results. Furthermore, [38  ###reference_b38###] combined a BERT [39  ###reference_b39###] encoder with a vanilla NMT decoder. Evaluation on low-resource languages like English-Vietnamese show that their two-stage training improves performance significantly compared to simple fine-tuning. XLM extends the features of BERT by using Cross-Lingual Masked Language Modeling [40  ###reference_b40###]. It has not only been reported to be beneficial for general unsupervised learning, but also for low-resource supervised MT such as English-Romanian. [41  ###reference_b41###] acknowledged the success of PLMs and presented their granulated study of fine-tuning, which showed that cross-attention layers are crucial to continue training downstream tasks and that they are powerful when adapting to new languages.  \nTranslation data for low-resource languages are very difficult to come by and the primary source are often from the Web, making the data noisy and of poor quality [42  ###reference_b42###]. Extra analysis and text normalization are often required to prevent overfitting. For instance, inaccurate translations, noisy data and a large amount of text-overlap was found in the parallel data for African languages collected from large crowd-sourced platforms [22  ###reference_b22###]. Comparative results showed that an English-Zulu model trained with noisy data leads to unreliable results and a reduction of 7 BLEU. Research from [28  ###reference_b28###] corroborated this and provided guidelines for removing low-quality translations. They presented translation filtering by way of n-gram models trained on monolingual data and sentence-level char-BLEU score [43  ###reference_b43###] below 15 or over 90. Another novel filtering approach was proposed by [42  ###reference_b42###], where cosine similarity is determined based on available parallel (good quality) data, which is then used as the threshold to filter out pseudo-parallel (noisy) sentences.  \nPrevious findings have pointed out that one-to-many models with middle-sized parallel corpora have achieved better results than one-to-one models [44  ###reference_b44###]. The multilingual model consisting of seven Asian languages developed by [9  ###reference_b"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Low-Resource Languages",
            "text": "Non- and semi-parametric methods have struggled in application to MT tasks in recent years."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Machine Translation",
            "text": "Non- and semi-parametric methods have been unsuccessfully applied to MT tasks in recent years. [33  ###reference_b33###  ###reference_b33###] demonstrate a powerful combination of neural networks and non-parametric retrieval mechanisms to improve translation. kNN-MT follows the retrieval principle and proposes a more efficient non-parametric translation method, which augments the decoder of a pre-trained NMT model with a nearest neighbor retrieval mechanism, allowing direct access to data store of cached examples [34  ###reference_b34###  ###reference_b34###]. This approach scales the decoder to an arbitrary amount of examples at test time, particularly strengthening decoderâ€™s translation capability. However, the big drawback is high computational cost and low decoding speed due to word-by-word generation. Chunk-based kNN-MT [35  ###reference_b35###  ###reference_b35###] solves this problem by processing translation in chunks of words instead of passing single tokens through the data store.\nin MT is often done by training a high-resource language pair and using this parent model to initialize parameters in a child model with low-resource languages. For example, [3  ###reference_b3###  ###reference_b3###] achieved translation improvements for Hansa, Turkish and Uzbek into English by using French-English as a parent model. Experiments from [36  ###reference_b36###  ###reference_b36###] showed improvements using Transformers [15  ###reference_b15###  ###reference_b15###] to train low-resource languages such as Estonian and Slovak. Their results pointed out key factors for a successful transfer include the size of the parent corpus and sharing the target or source language. For instance, Estonian-English as a child gained up to 2.44 BLEU with Finnish-English as a parent.\nIn Dual Transfer [2  ###reference_b2###  ###reference_b2###], two parent models are used to initialize one child. Monolingual and parallel parent data were trained separately so that inner layers and embeddings can be transferred separately. Another recent study extends conventional transfer learning by additionally transferring probability distributions from parent to child. The Consistency-based Transfer Learning [37  ###reference_b37###  ###reference_b37###] argues that parent prediction distribution is highly informative and can be useful to guide child translation. Their experiment showed that using German-English as a parent can achieve BLEU improvement up to 6.2 for Indonesian-English. Furthermore, the study from [6  ###reference_b6###  ###reference_b6###] investigated a technique to incrementally add new language pairs to a multilingual MT model based on knowledge transfer, without posing the original model at risk for catastrophic forgetting.\n(PLMs) can be fine-tuned on low-resource languages. For instance, MT quality between Spanish and Quecha was shown to improve by leveraging Spanish-English and Spanish-Finnish PLMs [4  ###reference_b4###  ###reference_b4###], with the latter yielding better results. Furthermore, [38  ###reference_b38###  ###reference_b38###] combined a BERT [39  ###reference_b39###  ###reference_b39###] encoder with a vanilla NMT decoder. Evaluation on low-resource languages like English-Vietnamese show that their two-stage training improves performance significantly compared to simple fine-tuning. XLM extends the features of BERT by using Cross-Lingual Masked Language Modeling [40  ###reference_b40###  ###reference_b40###]. It has not only been reported to be beneficial for general unsupervised learning, but also for low-resource supervised MT such as English-Romanian. [41  ###reference_b41###  ###reference_b41###] acknowledged the success of PLMs and presented their granulated study of fine-tuning, which showed that cross-attention layers are crucial to continue training downstream tasks and that they are powerful when adapting to new languages."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Refined Solutions",
            "text": "Translation data for low-resource languages are very difficult to come by and the primary source are often from the Web, making the data noisy and of poor quality [42  ###reference_b42###  ###reference_b42###]. Extra analysis and text normalization are often required to prevent overfitting. For instance, inaccurate translations, noisy data and a large amount of text-overlap was found in the parallel data for African languages collected from large crowd-sourced platforms [22  ###reference_b22###  ###reference_b22###]. Comparative results showed that an English-Zulu model trained with noisy data leads to unreliable results and a reduction of 7 BLEU. Research from [28  ###reference_b28###  ###reference_b28###] corroborated this and provided guidelines for removing low-quality translations. They presented translation filtering by way of n-gram models trained on monolingual data and sentence-level char-BLEU score [43  ###reference_b43###  ###reference_b43###] below 15 or over 90. Another novel filtering approach was proposed by [42  ###reference_b42###  ###reference_b42###], where cosine similarity is determined based on available parallel (good quality) data, which is then used as the threshold to filter out pseudo-parallel (noisy) sentences.\nPrevious findings have pointed out that one-to-many models with middle-sized parallel corpora have achieved better results than one-to-one models [44  ###reference_b44###  ###reference_b44###]. The multilingual model consisting of seven Asian languages developed by [9  ###reference_b9###  ###reference_b9###] using the Asian Language Treebank [45  ###reference_b45###  ###reference_b45###] is a great example. The presence of multiple in-domain aligned languages was argued to have contributed to better learn joint representations, hence leading to intra-language improvements. However, low-resource languages often face the risk of being overfitted in multilingual setups [46  ###reference_b46###  ###reference_b46###].\n[7  ###reference_b7###  ###reference_b7###] investigated the extent of multilinguality for low-resource languages. Their corpus consists of Bible texts in 1,108 languages, all aligned by verse. Results show that BLEU increase/decrease with respect to the number of training languages is not uniform across languages. Although the 5-language models outperform bilingual baseline models for Turkish and Xhosa, accuracy decrease can be found in Tagalog. The negative correlation between number of languages and translation quality is found to start at 10 languages, and maximal degeneration is observed at 100 languages, where addition of languages does not affect translation fluency anymore. This complication and pattern of degeneration can be explained by [47  ###reference_b47###  ###reference_b47###], where text repetition harms the likelihood function during decoding. Furthermore, the errors in sequence modeling are more obvious for multilingual corpora, indicating that increased number of languages leads to increased destructive interference.\nLeveraging similarities between low-resource languages has been a growing interest in the MT community and is evident through the Similar Language Translation task (SLT) and Very Low Resource Supervised Machine Translation task at WMT21 [48  ###reference_b48###  ###reference_b48###]. Regardless of level of closeness and degree of mutual structures, similarity between languages has shown to have positive interactions with MT quality [49  ###reference_b49###  ###reference_b49###]. The goal of using language relatedness is similar to leveraging multilinguality. The major difference is they often do not use English as the pivot language, but translate between closely-related languages.\nIn the Very Low Resource Supervised Machine Translation task at WMT21 [13  ###reference_b13###  ###reference_b13###] between German and Upper Sorbian, the participants were encouraged to make use of Czech and Polish datasets (languages closely related to Sorbian). Results pointed out the importance of including related languages, and that carefully applying tricks can compensate for using smaller datasets substantially. For example, NoahNMTâ€™s [50  ###reference_b50###  ###reference_b50###] approach entails a Dual Transfer [2  ###reference_b2###  ###reference_b2###] model that was initialized using German and Czech monolingual data as a parent model. The NRC-CNRC teamâ€™s [14  ###reference_b14###  ###reference_b14###] high-performance was attributed to the combination of minor tricks such as Back-translation [51  ###reference_b51###  ###reference_b51###], monolingual data selection by way of consine similarity, Moore-Lewis filtering [52  ###reference_b52###  ###reference_b52###] and BPE dropout [32  ###reference_b32###  ###reference_b32###].\nThe technique Back-translation is further backed up by the study from [30  ###reference_b"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Motivated by the current findings, we present our experiment to develop bidirectional state-of-the-art NMT systems between German and Bavarian (ISO codes are de and bar respectively) - a language pair consisting of high- and low-resource languages.\nWhile Bavarian and Upper Sorbian are very different languages, they are both spoken by communities which are geographically located within or near Germany. We expect that applying the NMT methods that were found to be effective as part of WMT21 might result in similar findings for our setting.\nWe formulate the following three research questions (applied to the exemplar language pair Bavarian / German):\nRQ1: Does translating between similar languages achieve generally higher BLEU scores?\nRQ2: How well does Back-translation perform for (bidirectional) German-Bavarian?\nRQ3: Does cross-lingual transfer lead to improved results for German-Bavarian? More specifically, does the child model profit from related parent languages (i.e. German-French)?"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data Acquisition",
            "text": "The Tatoeba Challenge222https://github.com/Helsinki-nlp/tatoeba-challenge  ###reference_allenge### [53  ###reference_b53###] is one of the most active projects advocating low-resource MT. It maintains a leader board to compare submitted MT system performance from the community. To our knowledge, we are the first to conduct MT for German-Bavarian systems. We discovered parallel and monolingual sources on OPUS333https://opus.nlpl.eu/  ###reference_opus.nlpl.eu/### [21  ###reference_b21###], which we used for our experiments. More information about data sources can be found in our repository."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Framework",
            "text": "Inspired by the WMT21 Very Low Resource Supervised Machine Translation task [13  ###reference_b13###], our experiment revisits solutions that have been proven to work effectively with low-resource languages.\nFirst, a simple Transformer [15  ###reference_b15###] model using preprocessed parallel data is trained as the baseline model.\nSecondly, Back-translation is used to generate silver-paired parallel data to increase corpus size.\nLastly, we experiment with Transfer Learning [3  ###reference_b3###] by introducing German-French as the parent model.\nFor evaluation, we opt for an ensemble of automated MT metrics consisting of BLEU, chrF and TER for our systems. This is backed up by recent argumentation from [19  ###reference_b19###] and [20  ###reference_b20###], which states that multiple metrics instead of a single metric can diversify the evaluation based on different linguistic characteristics. This approach is a growing trend and has also been adopted by WMT21. Moreover, the study from [30  ###reference_b30###] pointed out BLEU is insufficient in word matching due to ununified orthography."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Implementation",
            "text": "In total we found 99.7K parallel sentences between Bavarian and German on OPUS (details can be found in our repository). After extensive preprocessing, the corpus size was reduced to 42K. To conduct data augmentation for the second system, we downloaded an extra 258K of German and 295K Bavarian monolingual text, mainly from Wikipedia and Wikinews. For German-French, we collected a total size of 184K of parallel data from Tatoeba and WikiMedia, which was reduced to 165K after preprocessing. We argue that the amount of in-domain data could contribute positively to Transfer Learning. Text preprocessing removes special symbols and noisy annotation, as proposed in previous studies [14  ###reference_b14###, 23  ###reference_b23###].\nIn addition to conventional text preprocessing, we took two further measures to de-noise the data. The additional measures entail check and remove misaligned texts by way of cosine similarity between source and target languages and smart sentence truncation. Based on the knowledge that Bavarian and German share common script and that many morphemes are alike, cosine similarity is a great way to support misalignment removal. We assume that a low cosine correlation indicates a low relevance in context between source and target. Following exploratory experiments, we set the correlation threshold at 0.48 and treat anything that falls below 0.48 as misalignment and remove this. We leave a systematic investigation into this aspect as future work.\nOur consideration for smart truncation comes from the long-tailed distribution of sentence lengths (outliers span up to 8000). Having long sentences in the corpus therefore poses potential threat that could damage MT performance [54  ###reference_b54###]. However, if all longer sequences were simply removed, we might lose a significant amount of precious parallel data. Therefore, we implemented smart truncation to deal with longer sequences in the parallel corpus. The truncation is set at the sequence length of 90.\nIn low-resource MT training, it is important to implement Cross Validation (CV) to ensure robust predictive performance and address problems like overfitting. In this case, where the training corpus is small, CV can provide insights on the variability. We opt for 5-fold CV to compare training results. After text preprocessing, the cleaned text are randomly shuffled and split into 5 chunks. The subsets are then concatenated respectively before training. For our baseline systems, 4 of 5 iterations have the subset size of 33813 for training and 8453 for test. The last iteration has the size of 33812 and 8454 respectively.\nof all three systems is carried out as explained in Section 3.2  ###reference_###. We utilized the MT development toolkit Sockeye [55  ###reference_b55###] for BPE encoding, model training and evaluation.\nFor statistical significance analysis, our experimental setup needs to take the multiple comparison problem into account. When testing multiple hypotheses simultaneously, the increased number of statistical inferences leads to increased probability of inexact inferences and Type I errors, making the conventional p threshold of 0.05 less reliable. This is a well-known problem, e.g. in the Genome- and Public Health-related research [56  ###reference_b56###, 57  ###reference_b57###].\nMethods that counteract multiple testing generally adjust  so that the chance of observing inaccurate significant result is reduced. The Bonferroni correction is the simplest (and fairly conservative) approach to cut off the  value. Bonferroni corrects the  by considering the set of n comparisons, causing the  threshold to become . With the Bonferroni correction, the p-value is set to 0.017 as opposed to 0.05."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "Despite the popularity of BLEU, recent studies from [19  ###reference_b19###] and [58  ###reference_b58###] questioned the phenomenon of using BLEU as a single metric, especially in low-resource scenarios, where language structures and scripts are complex and different from many high-resource languages. For example, the meta evaluation on Indian languages by [59  ###reference_b59###] reported higher human judgement correlation using COMET [60  ###reference_b60###] as opposed to BLEU. The limitation of BLEU also lies in the strong dependence on reference translation, whose quality can be highly unstable, especially when data is noisy. Issues such as translationese and poor reference diversity [20  ###reference_b20###] might also jeopardize the entire evaluation. We therefore include chrF and TER for a more diverse evaluation. ChrF is language-independent and has been reported to better capture complex morpho-syntactic structures in MT evaluation [17  ###reference_b17###]. TER (Translation Error Rate) quantifies the amount of edit operations it takes to change the system output to match the reference translation [18  ###reference_b18###]. This intuitive technique avoids knowledge-intensive calculations and focuses on matching hypothesis with reference. The main advantage of TER as opposed to BLEU is the lower penalty for phrasal shifts. TER has also been reported to correlate highly with human judgement and has been implemented in recent WMT tasks [12  ###reference_b12###, 61  ###reference_b61###]."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "System 1: Baseline",
            "text": "Despite the lack of sufficient amount of parallel data, baseline models in both translation directions exceed 60 BLEU (see Table 1  ###reference_###). For bar-de baseline, BLEU scores have an average of 66, chrF has an average of 78 and TER 33. We want to point out little variation between the folds - indicating that the results are robust. However, we observe relatively lower scores on the opposite direction, namely an average of 61 BLEU, 74 chrF and 36 TER. Variation are also small for the de-bar base systems."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "System 2: Back-translation",
            "text": "Back-translation (BT) was applied to the best performing baseline folds with monolingual data. Significant improvements can be observed in all three metrics for bar-de, whereas de-bar systems show subtle increase. In contrast to baseline systems, we observe a systematic increase of standard deviation. Where SD was between 0.3 and 0.6 for base systems, 0.7 to 2.2 SD was found in back-translated systems."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "System 3: Transfer Learning",
            "text": "In contrast to surprisingly high baselines, both parent models perform similarly moderate, the fr-de model scored 29 BLEU, 52 chrF and 65 TER, whereas the de-fr parent reached 30 BLEU, 53 chrF and 65 TER. Given the fact that the German-French corpus size is significantly bigger than the German-Bavarian corpus, we had expected better performance of the parent models. However, our results are comparable with available German-French models on Hugging Face, for instance the one from Helsinki-NLP444https://huggingface.co/Helsinki-NLP/opus-mt-fr-de  ###reference_t-fr-de###.\nDespite the parentsâ€™ BLEU scores are only a half of our baseline models, Transfer Learning improves childrenâ€™s performance considerably. For bar-de, the best system has 54 BLEU, 71 chrF and 42 TER, which is an increase of 25 BLEU and 19 chrF and decrease of 23 TER. For de-bar, the best model scored 51 BLEU, 65 chrF and 43 TER, which has a performance leap of 21 BLEU, 12 chrF and 22 TER from parent. We note that Transfer Learning improved translation capacity from parent to child with an enhancement of more than 20 BLEU. This corroborates with the recent studies on the use of Transfer Learning for low-resource languages. However, these improvement cannot compare with the very high baseline systems and their back-translated extensions."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Statistical Analysis",
            "text": "Two-tailed pairwise t-tests were conducted on all pairs with Bonferroni correction (p threshold is 0.017). Test statistics are shown in Tables 2  ###reference_### and 3  ###reference_###. For bar-de models, the BLEU results from baseline (M = 65.7, SD = 0.2) and BT (M = 70.5, SD = 2) indicate that Back-translation leads to significant improvement, t = -4.89, p = 0.0036. BT also performs significantly better than transferred systems (M = 52.8, SD = 0.7), t = 17.25, p < 0.0. Further statistics from the metrics chrF and TER corroborate these findings.\nFor de-bar models, the tendency is similar. ChrF results show a positive enhancement from baseline (M = 74.1, SD = 0.4) to BT (M = 75.5, SD = 0.7), t = -3.84, p = 0.149. The improvement of BT over transferred systems (M = 64.2, SD = 0.6) is significant as well. TER statistics also verify these findings. Interestingly, while chrF and TER successfully rejects the null hypothesis between baseline and BT performance, BLEU does the opposite. We argue that the results are nevertheless significant based on chrF and TER, and consider this disagreement between metrics as an occurrence derived from linguistically-different perspectives and computations."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Qualitative Analysis",
            "text": "We argue that the surprisingly high baseline results come from the similarity of the source and target languages. This corresponds to findings from [49  ###reference_b49###] that language relatedness contributes positively to MT quality. The analysis of [23  ###reference_b23###]â€™s multilingual NMT on Indo-Aryan languages lists linguistic characteristics such as word-order construction, degree of inflection, amount of similar word root, meaning and conjunct verbs as the key drivers for improving training. Our experiments corroborate these argumentation, thus answering RQ1.\nThe significant improvement from Back-translation, which can be seen with all metrics, aligns well with previous findings. Especially in the submitted systems for WMT21 Very Low Resource Supervised MT between Upper Sorbian and German by [14  ###reference_b14###], Back-translation boosted the training corpus size and contributed to performance increase. However, we are aware of its limits. For instance, the augmented text includes many errors, which were inherited from the baseline systems. This issue of Translationese [62  ###reference_b62###] is widely discussed, especially in the context of using silver-paired data for MT. In our case, we have opted for a smaller amount of augmented data, with the aim to reduce Translationese as much as possible while still allowing model improvement. We therefore answer RQ2 that Back-translation contributes positively.\nRegarding RQ3, we point out that while Transfer Learning did improve performance from parent to child, its final performance was not sufficient to exceed the other two systems.\nWe note that our results are similar to the ones from the German - Upper Sorbian translation task from WMT21. Our baseline and back-translated models have an accuracy range between 60 to 73 BLEU and 74 to 82 chrF, comparable with the final scores from the German - Upper Sorbian task. However, it is interesting to note that their chrF scores are substantially higher than ours (by 10), while our BLEU scores are similar. This brings us back to the notion that all metrics work linguistically different and these variations reflect through different languages.\nFurthermore, a common finding can be observed between our experimental results and the WMT21 experiments we comapre against, namely the result discrepancy between high-to-low and low-to-high directions. In our study, de-bar is ca. 10 BLEU and 10 chrF behind bar-de. Similarly but not as extreme, Upper Sorbian - German also performs better than its high-to-low counter direction. This performance gap on the same corpus but different translation directions raises attention, with possible reasons due to the multiple orthographic standards and sub-dialects in our case.\nTable 4  ###reference_### depicts two translation examples. We translate the German phrase â€œSie hat heute Abend im Restaurant Fisch bestellt\" (English meaning â€œshe ordered fish in the restaurant tonight.\") into Bavarian using all of our systems. We observe that while Base and BT outputs look similar, their differences could come from various sub-dialects in the corpus. For instance, the term â€œheute\" was translated into â€œheit\" and â€œheid\", with only the last consonant different. However, in the Germanic linguistics, these consonants â€œt\" and â€œd\" differ themselves in voice. The linguistic notion of Fortis and Lenis555https://en.wikipedia.org/wiki/Fortis_and_lenis  ###reference_enis### differentiates oral pressure that is given to these consonants. Thus, we suspect these differences come from various dialects."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we presented experimental work in Neural Machine Translation with the aim to push forward our understanding of how to best address the gap between a handful of well-resourced languages and the long tail of languages for which no sufficient resources are available.\nMore specifically, we focused on methods and case studies that have shown promising results for languages with limited resources. We conceptualized the problems of noisy data and data shortage by way of recent studies. We revisited creative solutions designed to combat these challenges such as Back-translation, multilingual training and language relatedness.\nOur own low-resource implementation utilized data augmentation and cross-lingual transfer on German and Bavarian. We report our steps to preprocess the corpus and carry out training for three bidirectional systems. 5-fold cross validation was carried out on each system to compare robustness. We opted for a combined metric system using BLEU, chrF and TER to evaluate translation from different perspectives. For multiple hypothesis testing, pairwise t-tests with Bonferroni correction were conducted to test for statistical significance.\nResults show that translation between similar languages performs generally better and that augmented data contribute positively. However, even though cross-lingual transfer showed huge improvement from parent to child, it was not able to exceed baseline and back-translated models. We recognize that Transfer Learning is an effective approach for low-resource languages, but note that in our study language similarity played a more important role.\nTo support reproducibility and replicability all code is made available via GitHub."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "The Bavarian orthography has been a known problem for decades, as it is mostly a spoken language and has not been properly standardized. For example, the word â€™Bavarianâ€™ alone can be written in two ways: Boarisch or Bairisch. The investigation by [63  ###reference_b63###] illustrates that there are multiple Bavarian orthographic conventions. From a computational perspective, the issue is â€œdeciding which representation should be given precedence\", as stated in the Bribri case study by [11  ###reference_b11###]. Overcoming dialectal variations is also a problem of politics that can carry on for years. In light of the findings by [64  ###reference_b64###], we would add that the automated translation of Bavarian should - like other under-sourced languages - be carefully planned with ethical considerations, and that purely using web-scraped data to deploy translation systems might neglect the concerns of speakers.\nAnother challenge lies in multiple sub-dialects. This phenomenon can be observed in our corpus, which is mined from the Bavarian Wikipedia, where articles are written in different regional dialects. We argue that these sub-dialects in the parallel corpus lead to translation confusion, resulting in translation outputs which consist of mixed accents. Nevertheless, should there be a more refined and organized corpus of a particular sub-dialect, our systems can serve as baselines for fine-tuning.\nAnother, more general limitation is the fact that throughout our work we conducted purely technical evaluations. The strength of such an experimental setup is that it can be reproduced and offers objective results. However, it is clearly necessary to involve native speakers to gain more insights into the quality of any translation process. We mitigated against the problem by choosing not just a single evaluation metric (such as BLEU), but no matter how many different metrics are chosen they are no substitute for user studies."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Future Work",
            "text": "Following our findings and the limitations stated above, we propose further research directions to inspire future work: First, the curation of a more refined and organized parallel corpus for modern German-Bavarian to help establish a high quality benchmark for training and evaluation. An example to achieve this is through recruiting native speakers in both Bavarian and German who have an adequate amount of linguistic knowledge. This annotation could include not only translation of parallel sentences, but also the sub-dialects or Bavarian regional variations the speakers associate themselves with. This human-annotated dataset could furthermore be split into two parts, one for training and another for evaluation.\nAdditionally, identification of dialects would be an approach to counter translation confusion and mixed accents. This could help unify and isolate non-standardized languages or dialects. As mentioned in the previous section, a great way to start modelling sub-dialect detection is to automatically analyze the Wikipedia articles with their corresponding sub-dialects. This would greatly reduce the training corpus size, but additional measures to increase the corpus size could be taken, such as acquiring diverse datasets (i.e. open-source subtitles of Bavarian TV-programs or historical documents). More generally, we see our work as a reference benchmark for future work â€“ be it to explore the same language pair further or other work into the general problem of low-resource language translation efforts."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Ethical Considerations",
            "text": "Ethical concerns arise whenever natural language is being sampled and used to train machine learning systems. For this experimental work we used existing test collections and other freely accessible data. All the experiments are conducted within the ethical framework imposed on us by our institution. In this context we did not identify a specific ethical issue.\nHowever, it is clear that once any automated translation system is on its way to be deployed that care must be taken to (a) train it on representative samples, (b) mitigate against common biases, and (c) make sure no personal information is included in the training data. If trained on social media data there is also a risk that toxic content might surface. Care must be taken to take these issues seriously (rather than treating this as a box-ticking exercise), but we would argue that there are no ethical concerns arising from this work that have not already been identified previously."
        }
    ]
}