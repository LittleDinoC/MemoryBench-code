{
    "title": "Beyond Performance: Quantifying and Mitigating Label Bias in LLMs",
    "abstract": "Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit label bias—an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model’s predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.111We release our code at https://github.com/schwartz-lab-NLP/label-bias.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated impressive abilities in adapting to new tasks when conditioned on a context prompt, containing task-solving instructions Wei et al. (2022  ###reference_b32###) or few examples of input-output pairs Brown et al. (2020  ###reference_b2###).\nStill, recent work has shown that predictions of LLMs exhibit label bias—a strong, undesirable preference towards predicting certain answers over others (Zhao et al., 2021  ###reference_b35###; Chen et al., 2022  ###reference_b4###; Fei et al., 2023  ###reference_b9###, see Fig. 1  ###reference_###).\nSuch preferences were shown to be affected by the choice and order of in-context demonstrations Liu et al. (2022  ###reference_b16###); Lu et al. (2022  ###reference_b18###), the model’s pretraining data Dong et al. (2022  ###reference_b8###), or textual features of the task data Fei et al. (2023  ###reference_b9###). Consequently, several approaches were proposed to address this problem, mostly by calibrating the model’s output probabilities to compensate for this bias Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###).\n###figure_1### Despite these efforts, label bias evaluation relies on performance metrics such as accuracy, rather than metrics designed to directly quantify the bias.\nIn doing so, we might inadvertently overlook crucial aspects of model behavior.\nIndeed, although a given method could effectively improve performance, substantial bias might still persist in the model’s predictions—deeming the method insufficient and the model unreliable.\nAlternatively, performance could remain relatively unchanged, but with the bias mostly removed.\nIn this work, we take a step towards a more comprehensive understanding of the extent of label bias in LLMs and the effects of mitigation approaches.\nUsing metrics to directly measure the label bias in model predictions, which we derive from previous work on fairness and label bias estimation, we evaluate ten LLMs on 279 diverse classification and multiple-choice tasks from Super-NaturalInstructions Wang et al. (2022  ###reference_b31###). We examine both performance and bias along axes such as scale and number of in-context demonstrations.\nWe also evaluate the impact of label bias mitigation methods, such as calibration and few-shot LoRA fine-tuning Hu et al. (2022  ###reference_b13###).\nOur investigation reveals substantial label bias in the predictions of LLMs across all evaluated settings, indicating that raw LLM output scores often represent simple, heuristic solutions. While increasing model size, providing in-context demonstrations, and instruction-tuning all contribute to reducing bias, ample bias persists, even after applying mitigation methods.\nSurprisingly, these results also hold for tasks where the labels are all semantically equivalent (e.g., in multi-choice question answering).\nFurther, although the examined calibration methods can reduce bias and improve performance, we also find cases where they negatively impact both bias and overall performance.\nMotivated by these findings, we propose a novel calibration method for few-shot prompting\nthat more accurately estimates a model’s label bias, using only its predictions on the in-context demonstrations. Compared to existing LLM bias calibration methods, our method improves performance while also removing considerably more bias.\nOur findings highlight the necessity of considering and measuring biases in the predictions of LLMs when evaluating their performance. Moreover, adjusting models to their tasks through more accurate and effective estimation of biases holds promise for improving the reliability of LLMs and their applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "LLM Label Bias",
            "text": "Our objective is to broaden the understanding of label bias in LLMs and the effectiveness of mitigation strategies, focusing on classification tasks.\nIn this section, we define metrics designed to quantify bias in model predictions, providing a nuanced examination of label bias that extends beyond traditional performance metrics.\nWe describe the setting of label bias in in-context learning (§2.1  ###reference_###), briefly outline methods to mitigate it (§2.2  ###reference_###), and finally review approaches to evaluate label bias as well as define the metrics we use in this work (§2.3  ###reference_###).\nPrevious work used qualitative assessments to visualize model output distributions on selected datasets Zhao et al. (2021  ###reference_b35###); Han et al. (2023  ###reference_b11###). However, these cannot be used to rigorously evaluate models at larger scales.\nRecently, Fei et al. (2023  ###reference_b9###) proposed to measure a model’s label bias by considering two sets of inputs: a set of synthetic, content-free task inputs , and inputs consisting of random vocabulary words . For each input, they compute the output probabilities on every label , and finally compute the model’s mean predicted probabilities across both sets,  and :\nThe model’s bias is then defined to be the total variation distance  between the two distributions:\nImportantly, since Fei et al. (2023  ###reference_b9###) also use the model’s predictions on the content-free inputs  to calibrate it, this metric cannot be used to quantify the label bias remaining after calibration.\nIn this work, we simplify the computation of this metric and adapt it to be used after calibration.\nFirst, we hold-out a set of inputs to be used exclusively for measuring bias.\nSecond, when estimating the model’s average output probabilities, instead of using synthetic inputs, we use in-distribution examples held-out from the test set, . This setup allows to account for label imbalance in the data used for bias estimation , as the instances in the test set are all labeled. To do so, we first estimate the model’s output distribution individually on each subset of examples with gold label , , by computing:\nand then set  to be the average of these estimates.222In case examples for an infrequent label  are not found in , we exclude it from the computation of .\nInstead of , we use the uniform distribution over all answer choices , which recent mitigation approaches considered as the “ideal” and unbiased mean output distribution Zhao et al. (2021  ###reference_b35###).\nFinally, we define the model’s bias score as the total variation distance between these two distributions:\nWhen considering the effects of label bias on model predictions, strong label bias will likely result in disparities in task performance on instances of different classes. However, metrics to assess such disparities were not used in previous analyses of label bias.\nWe propose to use the Relative Standard Deviation of class-wise accuracy (RSD; Croce et al. 2021  ###reference_b6###; Benz et al. 2021  ###reference_b1###),\na metric used for studying fairness in classification.\nRSD is defined as the standard deviation of the model’s class-wise accuracy , divided by its mean accuracy acc on the entire evaluation data:333The goal of this normalization is to enhance the metric’s interpretability across tasks of varying difficulty.\nIntuitively, RSD is low when model performance is similar on all classes, and high when it performs well on some classes but poorly on others.\nWe note that each evaluation approach could detect biases that the other does not. For example, a slight bias in the model’s average output probabilities (e.g., 55% vs. 45%) could render dramatic bias in actual outcomes if the model always assigns higher probability to some label. Conversely, when the output probabilities are biased on average but the model’s class-wise performance is balanced, this hidden bias could result in actual performance disparities on more difficult instances. We therefore suggest reporting both measures."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Label Bias",
            "text": "When employing LLMs for classification tasks through prompting, the model is given a test example , preceded by a context . This context can contain a (potentially empty) set of examples of the task’s input-output mapping , henceforth demonstrations, and may also include task instructions. To determine the model’s prediction from a set of answer choices , the likelihood it assigns to each continuation  is computed, and the highest probability option is taken as the model prediction:\nThese output probabilities often exhibit label bias, where the model tends to assign higher probability to certain answers regardless of the input test example  (Fig. 1  ###reference_###).\nMultiple factors were posited to influence this bias, including the choice of verbalizers , the choice and order of in-context examples in , and the overall textual features of task input  Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Bias Mitigation",
            "text": "The predominant approach to alleviate label bias is to calibrate the model’s output probabilities post-hoc, for a specific context prompt .\nSuch methods typically first estimate the model’s label bias using its output probabilities on a set of inputs, which can be content-free (e.g., “N/A” or random words from the task’s domain; Zhao et al. 2021  ###reference_b35###; Fei et al. 2023  ###reference_b9###) or ordinary task inputs Han et al. (2023  ###reference_b11###). Next, calibration parameters are chosen based on this estimate, and used to adjust the original output probabilities during inference to generate the (hopefully unbiased) output."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Measures",
            "text": "Most LLM label bias analysis relies on indirect assessments. For instance, some work inspected improvements in overall performance gained after applying techniques to mitigate it Fei et al. (2023  ###reference_b9###); Holtzman et al. (2021  ###reference_b12###); Zhao et al. (2021  ###reference_b35###). However, these do not indicate the extent of bias originally present, or that which remains after mitigation.\nWe next examine approaches to measure this bias more directly, and define the metrics we use in this work.\nImportantly, we focus on label bias measures that could be used effectively both before and after applying mitigation techniques such as calibration.\nDrawing from previous research on fairness and bias in machine learning, we observe that there are two distinct yet related aspects in which label bias can be measured in LLM predictions: through the probabilities assigned by the model to different answers, e.g., assigning the label “yes” with an average output probability of 0.55, while “no” with 0.45; and through the model’s predictions for different labels, e.g., achieving a recall of 0.50 for instances labeled “yes”, compared to 0.40 on “no” Mehrabi et al. (2021  ###reference_b20###).\nBelow we describe methods to measure each of these notions of bias.\nPrevious work used qualitative assessments to visualize model output distributions on selected datasets Zhao et al. (2021  ###reference_b35###  ###reference_b35###); Han et al. (2023  ###reference_b11###  ###reference_b11###). However, these cannot be used to rigorously evaluate models at larger scales.\nRecently, Fei et al. (2023  ###reference_b9###  ###reference_b9###) proposed to measure a model’s label bias by considering two sets of inputs: a set of synthetic, content-free task inputs , and inputs consisting of random vocabulary words . For each input, they compute the output probabilities on every label , and finally compute the model’s mean predicted probabilities across both sets,  and :\nThe model’s bias is then defined to be the total variation distance  between the two distributions:\nImportantly, since Fei et al. (2023  ###reference_b9###  ###reference_b9###) also use the model’s predictions on the content-free inputs  to calibrate it, this metric cannot be used to quantify the label bias remaining after calibration.\nIn this work, we simplify the computation of this metric and adapt it to be used after calibration.\nFirst, we hold-out a set of inputs to be used exclusively for measuring bias.\nSecond, when estimating the model’s average output probabilities, instead of using synthetic inputs, we use in-distribution examples held-out from the test set, . This setup allows to account for label imbalance in the data used for bias estimation , as the instances in the test set are all labeled. To do so, we first estimate the model’s output distribution individually on each subset of examples with gold label , , by computing:\nand then set  to be the average of these estimates.222In case examples for an infrequent label  are not found in , we exclude it from the computation of .\nInstead of , we use the uniform distribution over all answer choices , which recent mitigation approaches considered as the “ideal” and unbiased mean output distribution Zhao et al. (2021  ###reference_b35###  ###reference_b35###).\nFinally, we define the model’s bias score as the total variation distance between these two distributions:\nWhen considering the effects of label bias on model predictions, strong label bias will likely result in disparities in task performance on instances of different classes. However, metrics to assess such disparities were not used in previous analyses of label bias.\nWe propose to use the Relative Standard Deviation of class-wise accuracy (RSD; Croce et al. 2021  ###reference_b6###  ###reference_b6###; Benz et al. 2021  ###reference_b1###  ###reference_b1###),\na metric used for studying fairness in classification.\nRSD is defined as the standard deviation of the model’s class-wise accuracy , divided by its mean accuracy acc on the entire evaluation data:333The goal of this normalization is to enhance the metric’s interpretability across tasks of varying difficulty.\nIntuitively, RSD is low when model performance is similar on all classes, and high when it performs well on some classes but poorly on others.\nWe note that each evaluation approach could detect biases that the other does not. For example, a slight bias in the model’s average output probabilities (e.g., 55% vs. 45%) could render dramatic bias in actual outcomes if the model always assigns higher probability to some label. Conversely, when the output probabilities are biased on average but the model’s class-wise performance is balanced, this hidden bias could result in actual performance disparities on more difficult instances. We therefore suggest reporting both measures."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setting",
            "text": "During their instruction tuning, Llama-2 chat models were initially fine-tuned on the Flan data collection Chung et al. (2022  ###reference_b5###); Longpre et al. (2023  ###reference_b17###).\nAs roughly 20% of Flan consists of examples from Super-NaturalInstructions, our evaluation of Llama-2 instruction-tuned models is likely affected by data contamination Magar and Schwartz (2022  ###reference_b19###). Still, our results show both 7B and 13B chat models exhibit extensive label bias, possibly due to later fine-tuning on other data. As it is unclear from the implementation details of Touvron et al. (2023  ###reference_b28###) which exact instances in Super-NaturalInstructions were included in training, we do not take extra steps in attempt to reduce possible overlap and contamination.\nZhao et al. (2021  ###reference_b35###) proposed to use calibration in order to remove the label bias arising from the context prompt  and the model’s pretraining. Inspired by confidence calibration methods Guo et al. (2017  ###reference_b10###), they define a matrix  that is applied to the model’s original output probabilities  during inference to obtain calibrated, debiased probabilities .\nTo determine the calibration parameters , they first estimate the bias by computing the model’s average predicted probabilities  on a small set of “placeholder” content-free input strings, such as “N/A”, which replace the ordinary task input that follows .444As in the original implementation, we use “N/A”, “[MASK]”, and the empty string.\nFinally, they set , which ensures that the output class probabilities for the average content-free input are uniform, aiming to reduce bias on unseen examples.\n###figure_2### ###figure_3### ###figure_4### Following CC, Fei et al. (2023  ###reference_b9###) proposed to estimate and mitigate the label bias arising from the textual distribution of the task’s domain, by using task-specific content-free inputs to compute .\nThey construct such inputs by sampling and concatenating  random words from the test set, where  is the average instance input length in the data. They repeat this process  times, and set  to be the average output probabilities over all examples. Given a test example with original output probabilities , they then use the calibrated probabilities .\nFinally, we experiment with few-shot, parameter-efficient fine-tuning for adapting LLMs to a given task’s label distribution, thus potentially mitigating label bias. We fine-tune task-specific models for each context prompt using Low-Rank Adapation (LoRA; Hu et al., 2022  ###reference_b13###), training adapters on 16 held-out training examples for 5 epochs. Importantly, we use the same context  during both fine-tuning and evaluation. Due to computational constraints, we only run LoRA on Llama-2 7B and Mistral 7B, only consider values of , and average\nacross two sets of demonstrations. See App. A  ###reference_.SSS0.Px3### for more details."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "We evaluate models on 279 diverse tasks from the Super-NaturalInstructions benchmark Wang et al. (2022  ###reference_b31###). We select all available classification and multi-choice question answering tasks where the output space is a set of predefined labels, such as “yes/no” or “A/B/C”.\nWe sample 1,000 evaluation examples for all tasks with larger data sizes, and additionally sample 32 held-out examples for computing the bias score metric (§2.3  ###reference_###), and 64 more examples to use as a pool of instances for choosing in-context demonstrations and LoRA fine-tuning examples.\nWe only include tasks with at least 300 evaluation examples in our experiments. For details on the selected tasks, see App. B  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Models and Evaluation Setup",
            "text": "We experiment with models of different sizes from three LLM families: Llama-2 7B and 13B Touvron et al. (2023  ###reference_b28###), Mistral 7B Jiang et al. (2023a  ###reference_b14###), and Falcon 7B and 40B Penedo et al. (2023  ###reference_b23###). We use both the base and instruction fine-tuned versions of each model. We evaluate models using context prompts with  demonstrations, and average the results across  different sets of demonstrations for each .\nTo control the evaluation budget, we run the more expensive Falcon 40B experiments with  averaged across  sets of demonstrations.\nWe use the task instructions and prompt template defined in Super-NaturalInstructions.\nFor tasks where the answer choices  have unequal token lengths, we use length-normalized log-likelihood to compute the output probabilities Holtzman et al. (2021  ###reference_b12###).\nFor additional implementation details, see App. A  ###reference_###.\nDuring their instruction tuning, Llama-2 chat models were initially fine-tuned on the Flan data collection Chung et al. (2022  ###reference_b5###  ###reference_b5###); Longpre et al. (2023  ###reference_b17###  ###reference_b17###).\nAs roughly 20% of Flan consists of examples from Super-NaturalInstructions, our evaluation of Llama-2 instruction-tuned models is likely affected by data contamination Magar and Schwartz (2022  ###reference_b19###  ###reference_b19###). Still, our results show both 7B and 13B chat models exhibit extensive label bias, possibly due to later fine-tuning on other data. As it is unclear from the implementation details of Touvron et al. (2023  ###reference_b28###  ###reference_b28###) which exact instances in Super-NaturalInstructions were included in training, we do not take extra steps in attempt to reduce possible overlap and contamination."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Bias Mitigation Techniques",
            "text": "We evaluate the effects of three label bias mitigation methods: two calibration methods designed to correct a model’s label bias by adjusting its output scores; and few-shot LoRA fine-tuning Hu et al. (2022  ###reference_b13###), which adapts the model to the task and its label distribution.\nWe describe the methods below.\nZhao et al. (2021  ###reference_b35###  ###reference_b35###) proposed to use calibration in order to remove the label bias arising from the context prompt  and the model’s pretraining. Inspired by confidence calibration methods Guo et al. (2017  ###reference_b10###  ###reference_b10###), they define a matrix  that is applied to the model’s original output probabilities  during inference to obtain calibrated, debiased probabilities .\nTo determine the calibration parameters , they first estimate the bias by computing the model’s average predicted probabilities  on a small set of “placeholder” content-free input strings, such as “N/A”, which replace the ordinary task input that follows .444As in the original implementation, we use “N/A”, “[MASK]”, and the empty string.\nFinally, they set , which ensures that the output class probabilities for the average content-free input are uniform, aiming to reduce bias on unseen examples.\n###figure_5### ###figure_6### ###figure_7### Following CC, Fei et al. (2023  ###reference_b9###  ###reference_b9###) proposed to estimate and mitigate the label bias arising from the textual distribution of the task’s domain, by using task-specific content-free inputs to compute .\nThey construct such inputs by sampling and concatenating  random words from the test set, where  is the average instance input length in the data. They repeat this process  times, and set  to be the average output probabilities over all examples. Given a test example with original output probabilities , they then use the calibrated probabilities .\nFinally, we experiment with few-shot, parameter-efficient fine-tuning for adapting LLMs to a given task’s label distribution, thus potentially mitigating label bias. We fine-tune task-specific models for each context prompt using Low-Rank Adapation (LoRA; Hu et al., 2022  ###reference_b13###  ###reference_b13###), training adapters on 16 held-out training examples for 5 epochs. Importantly, we use the same context  during both fine-tuning and evaluation. Due to computational constraints, we only run LoRA on Llama-2 7B and Mistral 7B, only consider values of , and average\nacross two sets of demonstrations. See App. A  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3### for more details."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Quantifying Label Bias in LLMs",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "LLMs are Label-Biased",
            "text": "We begin by examining the performance and label bias of models with and without instruction-tuning. We report averaged results across all tasks for Llama-2 models in Fig. 2  ###reference_###. Results for other models show similar trends (see App. C.1  ###reference_###).\nWe first verify that, as expected, model performance (Fig. 2(a)  ###reference_sf1###) substantially improves with scale, with instruction tuning and with the number of demonstrations.\nWe then consider the two bias metrics—RSD (Fig. 2(b)  ###reference_sf2###) and BiasScore (Fig. 2(c)  ###reference_sf3###).\nWe observe that label bias is substantial across most evaluated settings: When prompted with two or no demonstrations, all models obtain high RSD values of 0.6 or more, with base models obtaining even higher values around 0.9.\nThis implies a widespread disparity in model performance across classes in many of the evaluated tasks, and indicates that for most tasks, models primarily succeed on instances of certain classes, while consistently failing on others. Increasing the number of demonstrations to 8 helps reduce the bias, but RSD remains substantial at around 0.4, and adding further demonstrations results in little to no improvement.\nSimilarly, we find BiasScore improves considerably when using sufficient demonstrations, with models obtaining values as high as 0.25 when using no demonstrations, to around 0.05 for the best model and setting.\nHigh BiasScore values indicate the model is uncalibrated, and tends to make overly confident predictions on certain labels regardless of the input.\nAlthough BiasScore can be relatively small for some models—indicating their average output distribution is close to uniform—when observed together with high RSD, it implies that the model subtly but persistently assigns more probability mass to the preferred labels, resulting in substantial bias.\nfont=small,labelfont=small\n7B\n###figure_8### ###figure_9### ###figure_10### 13B\n###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Differences between the Bias Measures",
            "text": "We further observe that, interestingly, both bias metrics show divergent trends. Although RSD values, much like model performance, sharply improve after instruction-tuning, the resulting models’ BiasScore is often higher than their base counterparts.\nSimilarly, while RSD improves with scaling, the BiasScore of smaller models is lower.\nWe note that higher performance together with lower RSD means that the model’s performance has improved across most classes. In contrast, higher BiasScore indicates that its average predicted probabilities grew farther than uniform. Taken together, this implies that the scaled-up and instruction-tuned models are making more confident predictions on some classes, but not on others.\nThis could mean more confident correct predictions on the preferred classes, or more confidently wrong predictions on others (or both).\nAltogether, this suggests\nmore subtle forms of bias persist after instruction-tuning or scaling up Tal et al. (2022  ###reference_b27###).\nOverall, we find the two metrics to be complimentary due to their measurement of different aspects of label bias. We hence use both in further experiments to provide a more comprehensive understanding of label bias in model predictions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Label Bias Persists after Mitigation",
            "text": "We have seen that LLMs demonstrate extensive label bias across different models, scales and tasks (§4.1  ###reference_###). We next examine techniques aimed at mitigating such bias, and assess the extent of label bias remaining after their application.\nWe report our results for Llama-2 models in Fig. 3  ###reference_###, and observe similar trends for other models (App. C.2  ###reference_###).\nWe first consider the effect of bias mitigation on model performance (Fig. 3(a)  ###reference_sf1###) using the three methods described in §3.3  ###reference_###: contextual calibration (CC), domain-context calibration (DC), and few-shot fine-tuning with LoRA.\nCompared to standard prompting (black lines), we find that applying CC (orange) provides little to no gains. Moreover, it can even undermine model performance, especially for instruction-tuned models, as previously observed by Fei et al. (2023  ###reference_b9###).\nIn contrast, DC (purple) can provide substantial performance gains, especially when using few or no in-context demonstrations, where baseline performance is relatively low. However, when calibrating instruction-tuned models prompted with a higher number of demonstrations, we find that DC mostly fails to improve performance.\nFinally, LoRA considerably improves performance in all cases (green in Fig. 3  ###reference_###, upper row), vastly outperforming both CC and DC.\nWe next turn to measure label bias (Fig. 3(b)  ###reference_sf2### and 3(c)  ###reference_sf3###).\nNotably, here we observe that for the two calibration methods, changes in both RSD and BiasScore are correlated with changes in performance.\nWe find that CC substantially worsens label bias in instruction-tuned models, and can also increase bias for base models. Conversely, while DC alleviates bias in many of the evaluated settings, it is largely unsuccessful in mitigating it when prompting instruction-tuned models with  or more demonstrations. LoRA proves effective for improving RSD in all settings, but RSD values still remain relatively high. In contrast, BiasScore noticeably increases after LoRA fine-tuning, indicating that more subtle bias persists.\nOverall, our results indicate that existing bias calibration approaches are insufficient for diminishing label bias in essential cases, particularly for instruction-tuned models. Further, while LoRA fine-tuning is effective in both improving performance and mitigating certain aspects of bias (though not others),\nit is also considerably more computationally expensive than calibration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Mitigating Label Bias by Calibrating on Demonstrations",
            "text": "Motivated by the challenges of existing calibration approaches on instruction-tuned models (§4.3  ###reference_###), we aim to develop an effective calibration method for such scenarios.\nWe hypothesize a possible cause for such difficulties is that the inputs used for calibration in CC and DC are very distinct from the more curated, high-quality inputs models observe during instruction-tuning Touvron et al. (2023  ###reference_b28###).555Specifically, nonsensical task inputs made up of random words as in DC, or placeholder-like strings as in CC, are less likely to be observed during instruction tuning.\nSeeking to use more naturally-occurring inputs, and to avoid any reliance on additional held-out examples, we propose to calibrate models using the in-context examples readily available in few-shot prompts.\nWe therefore need to obtain the model’s output probabilities on these inputs to estimate its bias.\nHowever, as these examples appear alongside their labels in the context provided to the model, it could simply copy the correct answer from the prompt, leading to unreliable bias estimates.\nWe introduce a simple method to alleviate this concern.\nOur goal is to estimate the model’s average output probabilities  at test-time by using the  demonstrations  provided in the context , and then use it for calibration.\nDrawing from leave-one-out cross-validation, when evaluating the model on the -th demonstration’s input , we prompt it with an edited context  comprised of the original context  after removing the current demonstration , resulting in  demonstrations.666We leave all other demonstrations in their original order.\nWe thus obtain  output probabilities:\nTo reliably estimate , we further need to account for the demonstrations’ labels : for imbalanced choices of demonstrations (e.g., class imbalance), using the average of ’s could lead to an underestimation of the probability assigned to infrequent labels.\nWe therefore compute the average output probabilities  by taking into account the labels , as we do for computing BiasScore (§2.3  ###reference_###): We first average ’s associated with the same label , , and then set  as the mean of these intra-label averages:\nFinally, we use  to compute calibration parameters and score new examples using the same methodology as Zhao et al., 2021  ###reference_b35### (§3.3  ###reference_###).\nWe refer to our method as Leave-One-Out Calibration (LOOC).\nWe use LOOC to calibrate models in the same setup of §4.3  ###reference_###. We report our results for Llama-2 models in Fig. 3  ###reference_### (cyan lines), finding similar trends in other models (App. C.2  ###reference_###).\nComparing our method to other calibration approaches, we find LOOC surpasses CC and DC by a wide margin in both performance and bias metrics for prompts with  demonstrations. Importantly, using LOOC to calibrate instruction-tuned models in this setting dramatically improves upon the uncalibrated model, whereas other calibration methods fail to achieve meaningful gains (§4.3  ###reference_###).\nFurther, LOOC nearly closes the gap with LoRA-level performance while improving upon it in both bias metrics, yet uses substantially less compute.\nAs LOOC relies on the in-context demonstrations for bias estimation,  needs to be sufficiently large for calibration to succeed. Surprisingly, we find that with as few as  demonstrations, our method is often comparable to the next best calibration method on all metrics.\nFinally, we note that while our method can substantially reduce label bias compared to other approaches, the remaining RSD is still considerable, indicating that model performance is still biased on some tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "We study the effect of different factors on the extent of label bias in model predictions: the semantic meaning of the task labels (§6.1  ###reference_###), the level of label imbalance in the demonstrations (§6.2  ###reference_###), and the choice of demonstrations (§6.3  ###reference_###)."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Semantically Equivalent Labels",
            "text": "The output space for classification tasks often consists of labels with strong semantic meaning (e.g., “Positive” vs. “Negative”). Recent work has indicated that, when faced with such labels, models are affected by semantic priors from their pretraining or instruction-tuning Wei et al. (2023  ###reference_b33###); Min et al. (2022b  ###reference_b22###) that could affect label bias Fei et al. (2023  ###reference_b9###).\nWe examine whether models exhibit lower label bias when the task’s labels are semantically equivalent and interchangeable.\nWe extract all multi-choice QA tasks—with label spaces such as “A/B/C/D” or “1/2/3”—and all sentence completion tasks, where models choose a logical continuation for an input text between two options, usually labeled A and B.\nThis results in  tasks with semantically equivalent labels.\nWe compare label bias on this subset of tasks and the entire evaluation suite for Llama-2 models in Fig. 4  ###reference_###, with results for other models largely following similar trends (App. C.3  ###reference_###).\nWe find that, in most cases, models demonstrate lower label bias on tasks with semantically equivalent labels. This is especially evident in settings with few or no demonstrations, where models are typically strongly biased (§4.1  ###reference_###). Still, RSD levels for such tasks remain relatively high across all evaluated settings. Further, we observe that instruction-tuned models prompted with 8 or more demonstrations are often more biased on this subset of tasks. In summary, although using semantically equivalent labels may potentially mitigate bias in scenarios with limited demonstrations, LLMs still exhibit substantial label bias when faced with such labels."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Imbalanced In-context Demonstrations",
            "text": "Label imbalance in the in-context demonstration set was previously shown to amplify label bias Zhao et al. (2021  ###reference_b35###) as well as decrease model performance Min et al. (2022a  ###reference_b21###), but such results were derived on a restricted set of tasks.\nWe use our evaluation suite to investigate the observed label bias and performance of models when varying the level of imbalance in the demonstrations. To establish a consistent definition of label imbalance across different tasks, we use the subset of binary classification tasks () with  demonstrations.\nGiven a task with labels  and a context , we define  as the proportion of the most frequent label in the demonstrations of , such that  attains values in . Specifically,  means the labels are perfectly balanced, and  means the demonstrations only include examples for one of the labels.\nfont=small,labelfont=small\n###figure_14### ###figure_15### font=small,labelfont=small\n###figure_16### ###figure_17### ###figure_18### For every task, we prompt Llama-2 (7B/13B) and Mistral (7B) models using 10 different sets of demonstrations, with 2 sets for each value of : one where  is the most frequent label in , and another where  is the most frequent,\nas well as two different balanced sets ().777To build each set, we randomly select and permutate 8 demonstrations from a pool of 16 held-out examples, while controlling for the selected number of examples per label.\nWe group measurements taken across different tasks and demonstration sets by their level of label imbalance , and inspect the average results per level.\nWe report our results in Fig. 5  ###reference_###. Examining the two bias metrics, RSD (Fig. 5(a)  ###reference_sf1###) and BiasScore (Fig. 5(b)  ###reference_sf2###), we observe that both pretrained and instruction-tuned models are resistant to label imbalance:\nIncreased imbalance does not result in notable gains in bias, unless the imbalance is very extreme—specifically, when the demonstrations include only a single or no demonstrations for one of the labels ().\nInterestingly, model performance follows the same trends (Fig. 5(c)  ###reference_sf3###).\nOverall, our results indicate that for most tasks, the impact of label imbalance in the demonstrations set is minimal, except for cases of severe imbalance."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Choice of Demonstrations",
            "text": "The performance of LLMs in in-context learning was shown to be sensitive to the exact choice of demonstrations used to prompt the model Liu et al. (2022  ###reference_b16###); Chang and Jia (2023  ###reference_b3###). We examine whether such choices also impact the extent of label bias in model predictions.\nWe assess the performance and bias of Llama-2 (7B/13B) and Mistral (7B) models across 5 different sets of  demonstrations for each task in our evaluation suite. In addition to reporting the mean and standard deviation of each metric, we use several oracle methods to aggregate and choose a specific demonstration set per task when computing the overall cross-task performance and bias metrics. Specifically, we select the demonstration sets that attain the following, per task: best performance; worst performance; median performance; least bias; and most bias.\nWe report our results for Llama-2 7B base in Tab. 1  ###reference_###, with other models showing similar trends (App. C.4  ###reference_###).\nWe find that label bias, similarly to model performance, is highly sensitive to the choice of demonstrations, as indicated by the high variance across sets. Interestingly, the set of demonstrations that attains the worst performance also leads to strong bias, and vice-versa. In fact, we find that performance and bias are anti-correlated, with strong Pearson correlation for RSD () and moderate for BiasScore (),\nindicating that when LLMs underperform in classification, it is often due to prompts that exacerbate bias. We leave further research into demonstrations that lead to biased and unbiased predictions to future work."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent work has revealed various blunders in the predictions of LLMs. Wang et al. (2023a  ###reference_b29###) showed that models exhibit positional bias when presented with several texts for evaluation and ranking. Pezeshkpour and Hruschka (2023  ###reference_b24###) and Zheng et al. (2024  ###reference_b36###) exposed a similar bias in multi-choice QA. Si et al. (2023  ###reference_b26###) studied inductive biases in in-context learning. Complimentary to these works, we study label bias and seek to improve its evaluation and mitigation. Recent work introduced calibration methods to mitigate label bias in LLMs Zhao et al. (2021  ###reference_b35###); Fei et al. (2023  ###reference_b9###). Han et al. (2023  ###reference_b11###) proposed to fit a Gaussian mixture to the model’s output probabilities and use it for calibration, but their approach requires hundreds of labeled examples. Concurrently to our work, Jiang et al. (2023b  ###reference_b15###) proposed to generate inputs for calibration by conditioning models on the context prompt, and Zhou et al. (2023  ###reference_b37###) calibrate models using model output probabilities on the entire test set. While the motivation for both methods is similar to ours, our approach does not require access to the test set, or any compute to obtain inputs for calibration. Importantly, unlike previous work on bias calibration, our main focus is the evaluation of label bias in LLMs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The label bias of LLMs severely hinders their reliability. We considered different approaches for quantifying this bias. Through extensive experiments with ten LLMs across 279 classification tasks, we found that substantial amounts of label bias exist in LLMs. Moreover, we showed this bias persists as LLMs increase in scale, are instruction-tuned, are provided in-context examples, and even when they are calibrated against such bias.\nWe proposed a novel calibration method, which outperforms existing calibration approaches and reduces label bias dramatically.\nOur results highlight the need to better estimate and mitigate LLM biases."
        }
    ]
}