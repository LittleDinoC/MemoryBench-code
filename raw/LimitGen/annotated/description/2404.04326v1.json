{
    "title": "Hypothesis Generation with Large Language Models",
    "abstract": "Effective generation of novel hypotheses is instrumental to scientific progress.\nSo far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment).\nIn this paper, we examine the potential of large language models (LLMs) to generate hypotheses.\nWe focus on hypothesis generation based on data (i.e., labeled examples).\nTo enable LLMs to handle arbitrarily long contexts,\nwe generate initial hypotheses from a small number of examples\nand then update them iteratively to improve the quality of hypotheses.\nInspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process.\nOur algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets.\nWe also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets.\nFurthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Hypothesis generation drives scientific progress.\nMendel’s hypothesis on allele pairs lays the foundation for modern genetics;\nEinstein’s hypothesis in general theory of relativity led to the prediction and subsequent confirmation of gravitational waves.\nIn the context of language modeling, the hypothesis on scaling law inspires recent progress in large language models (LLMs) (Chowdhery et al., 2022  ###reference_b6###).\nDespite the importance of hypothesis generation, as Ludwig & Mullainathan (2024  ###reference_b16###) points out, science has been curiously asymmetric.\nWhile many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers.\nIn order to generate novel hypotheses, researchers may read literature, analyze data, pick the brain of each other, and even “hallucinate” (see Kekulé’s discovery of the structure of the benzene molecule (Rothenberg, 1995  ###reference_b25###)).\nGiven the rise of large language models (Brown et al., 2020  ###reference_b5###; Anthropic, 2023  ###reference_b2###; OpenAI, 2023b  ###reference_b20###), we examine their potential of providing much needed assistance in hypothesis generation in this work.\nIn particular, we focus on hypothesis generation based on data, a common approach in empirical sciences.\nOur main question is how we can enable LLMs to generate hypotheses of high-quality.\nWhile one can easily prompt LLMs to generate hypotheses, LLMs may not be able to effectively leverage the input examples in a single long prompt.\nMoreover, it is important to have measures of quality in the generation process so that we can filter bad hypotheses and come up with better ones.\nThese two observations motivate us to start with a setup analogous to supervised learning.\nWe can iteratively prompt an LLM to generate hypotheses based on the training examples and use training accuracy as a measure of quality to guide the generation process.\nConveniently, we can also evaluate the quality of the final generated hypotheses with their performance on held-out examples, similar to supervised learning.\nTo generate high-quality hypotheses with LLMs, we propose an algorithm\ninspired by the upper confidence bound algorithm in multi-armed bandits (Auer, 2002  ###reference_b3###) (HypoGeniC, Hypothesis Generation in Context; see Figure 1  ###reference_###).\nGiven initial hypotheses generated from a small number of examples, we need to assess their quality and propose new hypotheses to address their deficiencies.\nTo navigate this exploration-exploitation tradeoff,\nwe introduce a\nreward function\nand evaluate the top  hypotheses for each\ntraining example.\nWe maintain a wrong example bank to capture the gap in knowledge of the current hypotheses pool, and\ngenerate new hypotheses based on the wrong example bank to close the gap.\n###figure_1### The generated hypotheses naturally enable an interpretable hypothesis-based classifier.\nWe propose a suite of inference strategies given a set of hypotheses.\nWe apply our method to one synthetic task where there is a single known valid hypothesis and three real-world tasks (Deceptive reviews,  and Tweet popularity).\nThe real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans (Ott et al., 2011  ###reference_b21###; Salganik et al., 2006  ###reference_b27###).\nOur algorithm can recover the hypothesis in the synthetic task and also provide useful hypotheses for the real-world tasks.\nIn fact, our generated hypotheses\nconsistently outperform few-shot in-context learning baselines across all four tasks (31.7% in Shoe sales, 13.9% in Deceptive reviews, 3.3% in  and 24.9% in Tweet popularity).\nThe predictive performance matches and even outperforms oracle supervised learning with RoBERTa except in Deceptive reviews.\nIt is important to emphasize that although the utility of hypotheses in assisting downstream classification serves as an indicator for LLMs’ ability to generate hypotheses,\nour primary interest lies in the quality of the hypotheses.\nThus, it is critical for the hypotheses to be interpretable beyond the LLM used to produce the hypotheses.\nWe show that hypotheses generated by one LLM (e.g., GPT-3.5-turbo) can be used to make accurate inference by another LLM (e.g., Mixtral).\nOn an out-of-distribution dataset for Deceptive reviews, we can even outperform the oracle fine-tuned RoBERTa.\nSuch cross generalization provides strong evidence that we are able to generate hypotheses of high quality.\nFurthermore,\nthrough a qualitative analysis, our generated hypotheses\nnot only confirm theories from existing literature but also provide new insights about the task.\nFor instance, one novel hypothesis is that “reviews that mention personal experiences or special occasions, such as birthdays, anniversaries, or weddings, are more likely to be truthful”.\nWe encourage future research on deception detection to explore these novel hypotheses.\nOur work is connected to many recent studies on using LLMs to propose “hypotheses”, notably, Qiu et al. (2024  ###reference_b24###) and Zhong et al. (2023  ###reference_b34###).\nQiu et al. (2024  ###reference_b24###) is motivated by testing the ability of LLMs to perform human-like induction reasoning,\nand Zhong et al. (2023  ###reference_b34###) aims to support open-ended exploration.\nWhile similar in spirit, we examine the case of generating theories between input and labels for challenging problems where even researchers may struggle with proposing new hypotheses.\nTo summarize, we highlight the following contributions:\nWe propose a novel computational framework for generating and evaluating hypotheses with large language models.\nOur generated hypotheses enable interpretable hypothesis-based classifiers that outperform in-context learning and even supervised learning for one synthetic and three real-world datasets. These hypotheses are also robust across different LLMs and out-of-distribution datasets.\nWe find our generated hypotheses to corroborate existing findings while also providing new insights for the tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We begin with a description of the problem formulation.\nGiven a set  where  is an example and  is the corresponding label, the goal is to learn a set of hypotheses  that describe theories of relationships\nbetween  and .\nTo this end, we prompt an LLM to summarize demonstration examples into high-level hypotheses (§ 2.1  ###reference_###).\nThen, during inference, the LLM makes inference based on the generated hypothesis (§ 2.2  ###reference_###).\nOur code is available at https://github.com/ChicagoHAI/hypothesis_generation  ###reference_eneration###."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Hypothesis Generation",
            "text": "Our hypothesis generation algorithm (Algorithm 1  ###reference_###) is inspired by the upper confidence bound (UCB) algorithm (Auer, 2002  ###reference_b3###).\nGiven a set of initial examples , we first prompt an LLM to generate hypotheses for , which serve as our initial hypothesis bank .\nWhile initialized hypotheses may explain some portions of data, they often fall short of encompassing the full scope of the examples.\nWe thus introduce an update stage which serves a dual purpose:\n1) it increases the percentage of data explainable by the hypotheses and 2) it replaces any hypotheses that are found to be inaccurate.\nIn the update stage, for a training example\n, we select the top  high-reward hypotheses from the hypothesis bank .\nThe LLM is prompted to make a prediction with each of the top  high-reward hypotheses on .\nThen we compute the accuracy of the inference and accordingly update the reward for each of the hypotheses.\nIf  hypotheses predict incorrectly for the example ,\nthen  is added to a wrong example pool .\nOnce the wrong example pool reaches a max size of , the wrong examples in  are used to generate new hypotheses, as shown in Algorithm 1  ###reference_###.\nThe wrong example pool represents the gap in knowledge that the current pool of hypotheses has for the dataset. Thus by generating new hypotheses, the algorithm fills in these gaps.\nWe update  with the newly generated hypotheses according to the rewards.\nReward.\nAs mentioned above, each hypothesis has an associated reward.\nIn our algorithm, we\nuse the reward function in the UCB algorithm due to similarities between the multi-arm bandit problem and our problem formulation.\nIn particular, we consider each hypothesis to be an arm and each training example to be a “pull”. We note, however, that unlike the multi-arm bandit problem, multiple hypotheses are tested for a singular train example.\nMoreover,\nthere can be new arms after hypotheses are updated, altering the setting from the standard static arms scenario to a dynamic arms scenario.\nFormally, the reward is defined as\nwhere  is the set of examples that have been used to evaluate the hypothesis ,  is train time step, and  is a hyperparameter that controls the exploration term.\nThe first term in the reward function denotes the accuracy of the hypothesis for all .\nThe second term is the exploration term, which is computed based on the number of times the hypothesis has been selected and the number of training examples visited so far.\nThe accuracy term urges the algorithm to use well-performing hypotheses, whereas the exploration term encourages the algorithm to explore hypotheses that have not been selected many times.\nThus, the reward function strikes a balance between exploration and exploitation.\nFor more details on implementation of HypoGeniC, refer to § B.1  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Hypothesis-based Inference",
            "text": "For efficiency purposes, we use each hypothesis on its own without accounting for their combinatorial effect during training; however, we should leverage the set of hypotheses as a whole during inference for at least two reasons.\nFirstly, some hypotheses may only apply to a subset of examples. Second, competing theories may require head-to-head comparisons.\nHence, we develop multiple inference strategies to account for these different styles of reasoning (see Appendix A  ###reference_### for prompts and § B.2  ###reference_### for implementation details).\nBest-accuracy hypothesis.\nThe hypothesis  with the highest accuracy from the hypothesis bank  is included in the prompt to guide the model to perform inference.\nFilter and weighted vote.\nOne hypothesis may not be enough to explain the data.\nThus, this approach uses a combination of relevant hypotheses to make predictions for a single example.\nFor each example,\nwe first filter hypotheses by prompting an LLM to judge which hypotheses are relevant to the example.\nNext, an LLM is prompted to generate predictions for each of the relevant hypotheses, and these predictions are aggregated with weighted vote, where the weight is the training accuracy of the corresponding hypothesis.\nSingle-step adaptive inference.\nSimilar to filter and weighted vote, this approach leverages contextual information to choose hypotheses. The difference, however, is that it selects the most applicable hypothesis for each test example.\nSpecifically, for a given test example, the LLM is tasked with identifying the most applicable hypothesis from a set of options.\nFor each hypothesis, we provide instances from the training set where the hypothesis was accurate. Then, the LLM selects the most relevant hypothesis by comparing the test example to these training examples and evaluating their similarity.\nThereafter, we apply the hypothesis to the test example to perform inference. Please note that this is all done in one step with a long prompt.\nTwo-step adaptive inference.\nWe divide the previous inference strategy into two steps:\nThe LLM determines the most relevant set of examples by comparing the test example with the corresponding examples of the hypotheses.\nThen, the corresponding hypothesis is provided to the LLM, which it uses to perform inference on the test example in a second prompt."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiment Setup",
            "text": "Next, we introduce the experiment setup to evaluate HypoGeniC."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks and Datasets",
            "text": "The choice of appropriate tasks is critical for evaluating the ability of LLMs to generate hypothesis.\nThe focus of our work is on generating hypotheses based on observed data.\nA prerequisite is that potential hypotheses do exist.\nIn the context of classification, it implies that the classification performance is non-trivial.\nIn addition, we need to ensure that the hypotheses describing the data are likely not a priori known by LLMs, which rules out standard tasks such as sentiment analysis.\nTherefore, we use four datasets that satisfy these requirements: a synthetic task where we know the true hypothesis and three real-world datasets that exhibit complex underlying patterns and constitute widely studied social science problems.\nShoe sales is a synthetic task we created to investigate the scenario where there is only one single valid hypothesis.\nThe task is to predict the color of the shoe that the customer will buy based on their appearance.\nThe input provides appearance features, namely, age, height, gender, color of the hat, color of the shirt, color of the bag, and size of the bag.\nWe construct this dataset such that the color of the shoe must match the color of the shirt.\nSince there are six colors in total, this becomes a 6-class classification problem.\nDeceptive review detection is an instance of deception detection, a widely studied phenomenon in psychology and other social sciences (Granhag & Vrij, 2005  ###reference_b9###).\nThis particular task (Deceptive reviews) requires distinguishing genuine reviews from fictitious ones (Ott et al., 2011  ###reference_b21###), where human performance is about chance (Lai & Tan, 2019  ###reference_b12###).\nThe dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.\nPredicting popularity is a notoriously challenging task in social sciences because it is known to be affected by seemingly random factors (Salganik et al., 2006  ###reference_b27###).\nWe use two datasets in this work: nd Tweet popularity.\ns derived from a dataset in the Upworthy Research Archive (Matias et al., 2021  ###reference_b17###).\nThe original dataset was collected through A/B testing, where each user was shown pairs of a headline and image for multiple packages (articles).\nEach user was exposed to only one of these pairs per package, and the clicks were recorded for each pair per package.111The Upworthy Research Archive only provides the image IDs instead of the graphics. We thus only use the headlines for our dataset.\nThis process resulted in a total of 150,816 headlines across 22,666 packages.\nWe construct a binary classification dataset by choosing for each package the headline that received the most clicks and the one that received the fewest.\nWe remove all sets of duplicate headlines, which results in our version of the ataset.\nThe task for this dataset is to deduce which headline had more clicks in a pair.\nTweet popularity uses a dataset of 13,174 tweet pairs (Tan et al., 2014  ###reference_b29###), which are matched by the topic and the author.\nSimilar to  the task is to predict which one received more retweets."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baselines, Oracles, and Evaluation Metrics",
            "text": "We use three different LLMs in our experiments (Mixtral (Mistral, 2023  ###reference_b18###), GPT-3.5-turbo (OpenAI, 2023a  ###reference_b19###), and Claude-2.1 (Anthropic, 2023  ###reference_b2###)).\nWe compare our approach with the following methods.\nZero-shot and few-shot prompting.\nWe provide LLMs with task-specific instructions (zero-shot), optionally accompanied by three demonstration examples (few-shot).\nNo updates.\nTo assess the value of the update stage in our algorithm, we evaluate the performance of the initialized hypotheses.\nIn particular, we pick the best-performing hypothesis on the training set and use it for inference on the test set.\nSupervised Learning.\nWe fine-tune RoBERTa on each of the datasets to serve as a non-interpretable oracle.\nWe include results for training on 200 examples and 1000 examples.\nSince fine-tuning update model weights, we expect RoBERTa to set the upper bound on in-distribution datasets.\nWe randomly sample 200 training examples and 300 test examples for each dataset.\nSince all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric.\nTo understand the effect of the number of training examples, we evaluate the performance of all methods at 10, 25, 50, 100, and 200 training examples.\nWe also experiment with two different hypothesis bank sizes: 3 and 20 hypotheses to evaluate the impact of utilizing a larger number of hypotheses.\nThe detailed hyperparameters of our approach can be found in § B.3  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "To demonstrate the effectiveness of our hypothesis generation approach,\nwe present results via three evaluation methods.\nFirst, we show that in the standard supervised learning setup, our generated hypotheses enable more accurate predictions than baselines and even oracles when using a small set of examples.\nSecond, we evaluate the generated hypotheses by checking whether they can generalize across different inference LLMs and to out-of-distribution datasets.\nWe find surprisingly consistent performance even when using a different LLM to make inference from the generated hypotheses.\nFinally, we conduct a qualitative analysis to show that the generated hypotheses not only corroborate with existing theories but also provide novel insights about the tasks at hand.\nTable 3  ###reference_### presents an overview for the OOD deceptive review dataset.\nThis dataset differs from Deceptive reviews by including reviews from\nfour cities sourced from different websites (Li et al., 2013  ###reference_b14###).\nWe find that HypoGeniC outperforms few-shot learning by an average of 19.1%.\nDespite the distribution shift, HypoGeniC surprisingly increases accuracy from Deceptive reviews by an average of 3.3%, suggesting our hypotheses generalize well to this OOD dataset.\nClaude-2.1 remains the best performing model.\nIn comparison,\nthe performance of RoBERTa drops by 11%.\nAs a result, HypoGeniC with Claude-2.1 outperforms RoBERTa by 1.7%, demonstrating the robustness of hypothesis-based inference.\nRefer to § C.3  ###reference_### for more details."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance on Heldout Test Sets",
            "text": "As discussed in the introduction, a side product of our approach is an interpretable hypothesis-based classifier.\nWe compare its performance with standard supervised learning with fine-tuned RoBERTa and few-shot prompt learning (Table 1  ###reference_###).\nOur generated hypotheses improve inference over standard zero-shot and few-shot inference.\nAcross all LLMs, HypoGeniC outperforms the zero-shot learning by an average of 60% on Shoe sales, 22.7% on Deceptive reviews, 5.1% on  and 30.6% on Tweet popularity.\nSimilarly, we find that HypoGeniC shows an increase from few-shot learning by 31.7% on Shoe sales, 13.9% on Deceptive reviews, 3.3% on  and 24.9% on Tweet popularity.\nNote that these results are inflated on Tweet popularity as safety mode is triggered for Mixtral and Claude-2.1 for zero-shot and few-shot learning respectively.\nThe results show that hypothesis-based inference can increase the performance of LLMs significantly.\nOne exception is that our method performs slightly worse (by 1%) than the few-shot baseline in the Tweet popularity with GPT-3.5-turbo.\nOne possible reason is that the few-shot demonstrations are effective at eliciting the pretraining knowledge in GPT-3.5-turbo, possibly due to a large amount of tweets in pretraining data.\nFor more detailed results, refer to Appendix C  ###reference_###.\nWe also evaluate generated hypotheses with oracle inference, where the model retrospectively picks the best hypothesis for each prediction from the bank.\nWith oracle inference, HypoGeniC achieves on average 88.6% on Deceptive reviews, 84.1% on  and 88% on Tweet popularity across all LLMs, which are superior to results in Table 1  ###reference_###.\nThis result further suggests that hypotheses generated by HypoGeniC are of high quality and can lead to accurate predictions when the correct hypothesis is selected.\nHypoGeniC matches or even exceeds RoBERTa with the same number of training examples on most datasets.\nBoth HypoGeniC and RoBERTa yield 100% on the syntheic dataset. Moreover, HypoGeniC is 12.8% and 11.2% better than RoBERTa on nd Tweet popularity respectively with 200 training examples.\nSince RoBERTa learns by updating model weights to minimize the cross-entropy loss, it tends to benefit from more training examples, so we increase training examples to 1000 for RoBERTa.\nDespite the accuracy boost from more training examples, we find that HypoGeniC’s best result still outperforms RoBERTa by 3.7% on nd 0.7% on Tweet popularity.\nOne exception, however, is the Deceptive reviews dataset.\nWe suspect that as word-level features are very useful in this dataset (Ott et al., 2011  ###reference_b21###),\nthey could be tougher for LLMs to extract but easier for fine-tuned models to grasp.\nUpdating hypothesis bank leads to hypotheses of higher quality.\nComparing HypoGeniC with the “no updates” results, we find that updating hypotheses generally leads to better hypotheses, suggesting that our algorithm is effective at improving hypothesis quality.\nThe improvement is on average 0.7% on Shoe sales, 5.8% on Deceptive reviews, 8.1% on  and 7% on Tweet popularity.\nAnother advantage of HypoGeniC over “no updates” is that sometimes the training examples exceed the context window size of LLMs, which can lead to degraded performance (Figures 4  ###reference_### and 3  ###reference_###).\nEffect of inference strategy.\nFigure 2  ###reference_### shows HypoGeniC results with different inference strategies on Deceptive reviews.\nSingle-step adaptive inference\nis the most effective.\nGenerally, we find that hypotheses to be one-sided, focusing on either characteristics of truthful or deceptive reviews.\nWe thus need to consider more than one hypothesis to make a correct prediction, so best-accuracy hypothesis or two-step adaptive inference would not be ideal for this dataset.\nOn the other datasets, we find that the effect of inference strategy is much smaller.\nBest-accuracy hypothesis is sufficient for Shoe sales and  and filter and weighted vote works best for Tweet popularity.\nResults for all datasets are in § C.1  ###reference_###.\nWhichever inference strategy we use, the trend of HypoGeniC against few-shot learning and RoBERTa remains largely the same.\nGenerally, having more training examples and a larger hypothesis pool improves performance.\nWe show performance for different methods as number of training examples increase in Figures 4  ###reference_###, 3  ###reference_### and 5  ###reference_###.\nWe find HypoGeniC accuracy steadily increases as training size increases on Shoe sales, suggesting that an LLM is more likely to generate the best hypothesis given more examples.\nFor the real-world datasets, however, the performance sometimes peaks at training size at 25 or 100 before reaching to 200.\nWe suspect that the evaluation of the hypothesis bank would be less stable for the real-world datasets, since more than one correct hypotheses are needed for the task.\nWe also find that using a hypothesis pool of size 20 leads to better performance than using a pool of size 3.\n###figure_2###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Generalization of the Generated Hypotheses",
            "text": "Our primary interest lies in the quality of the hypotheses.\nA good hypothesis should enable accurate inference by any AI model or even human and also generalize to unseen out-of-distribution dataset.\nIn this subsection, we mix and match different LLMs for generation and inference.\nWe also evaluate the hypotheses in deceptive review prediction on a new out-of-distribution (OOD) dataset (Li et al., 2013  ###reference_b14###).\nWe find that the hypotheses generated by HypoGeniC generalize across models (Table 2  ###reference_###).\nGenerally, we find Claude-2.1 and Mixtral to be better at inference.\nThus, substituting the inference model with them lead to better performance for hypothesis generated with GPT-3.5-turbo.\nSubsituting Claude-2.1 and Mixtral as each other’s inference model lead to small changes in performance.\nOn Shoe sales, the performance remains high (>90%) regardless of the inference model used.\nPerformance even increases for Deceptive reviews and hen using Claude-2.1 as the inference model.\nFor the cases where performance drops from Claude-2.1 to Mixtral, the decrease is marginal: 2.3% on Deceptive reviews and 2.7% on Tweet popularity.\nThese results suggest that the hypotheses generated by HypoGeniC are generalizable across different LLMs, which somewhat contracts the claim in Qiu et al. (2024  ###reference_b24###) that LLMs cannot reliably interpret the hypotheses.\nWe suspect that the reason may be that our tasks only rely on natural language, while their tasks rely on notions of worlds and can be fed into symbolic interpreters.\nTable 3  ###reference_###  ###reference_### presents an overview for the OOD deceptive review dataset.\nThis dataset differs from Deceptive reviews by including reviews from\nfour cities sourced from different websites (Li et al., 2013  ###reference_b14###  ###reference_b14###).\nWe find that HypoGeniC outperforms few-shot learning by an average of 19.1%.\nDespite the distribution shift, HypoGeniC surprisingly increases accuracy from Deceptive reviews by an average of 3.3%, suggesting our hypotheses generalize well to this OOD dataset.\nClaude-2.1 remains the best performing model.\nIn comparison,\nthe performance of RoBERTa drops by 11%.\nAs a result, HypoGeniC with Claude-2.1 outperforms RoBERTa by 1.7%, demonstrating the robustness of hypothesis-based inference.\nRefer to § C.3  ###reference_###  ###reference_### for more details."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Qualitative Analysis",
            "text": "For the synthetic dataset, all models are able to find the true underlying hypothesis for Shoe sales: “customers tend to buy shoes that match the color of their shirt.”\nFor the real-world datasets, we compare our hypotheses with findings from the literature. We confirm the validity of some of our hypotheses and discover new insights about the tasks that previous studies did not touch upon.\nWe show a few examples in Table 4  ###reference_###, and the full list of hypotheses can be found in Appendix D  ###reference_###.\nOur hypotheses confirm useful features in existing literature.\nFor Deceptive reviews, we find that deceptive reviews are more likely to be emotional, use superlatives, or contain information that could not have been directly experienced. Similar findings are also found by previous studies on Deceptive reviews (Lai et al., 2020  ###reference_b13###; Anderson & Simester, 2014  ###reference_b1###; Ott et al., 2011  ###reference_b21###; Li et al., 2014  ###reference_b15###).\nFor Tweet popularity, we discover that tweets that are short and concise, with specific or relevant hashtags, or with emotional tones are more likely to be retweeted more, aligning with prior studies (Tan et al., 2014  ###reference_b29###; Gligorić et al., 2019  ###reference_b8###).\nFor  we find that\nrevealing something new or using vivid language and imagery can\ndrive engagement from readers to click on headlines. Previous studies also find these rules apply to online news headlines (Banerjee & Urminsky, 2021  ###reference_b4###; Sadoski et al., 2000  ###reference_b26###).\nWe also discover new insights with our generated hypotheses.\nFor the Deceptive reviews dataset, truthful reviews could mention the reviewer’s purpose for staying at the hotel (e.g., business trip, vacation), but deceptive ones tend not to have this information.\nFor  we find that headlines that frame the content in a personal or relatable way are clicked more.\nFor Tweet popularity, tweets that mention influential individuals or organizations are more likely to be retweeted.\nIntriguingly, one of our hypotheses contradicts a feature engineering result.\nOtt et al. (2011  ###reference_b21###) find that the token “future” is associated with deceptive reviews, while one of our hypotheses says that mentions of “past experiences or future travel plans” are indicative of truthfulness. This discrepancy is interesting, because the context for the token “future” is unclear.\nIt could be mentioned in the context of future plans but could also be mentioned as a complaint about “never going to stay at the hotel in the future.”\nFeature engineering is limited due to the contextual ambiguity, whereas our generated hypotheses and their interpretation by LLMs overcome such limitations.\nOur automatic evaluation of hypothesis quality also reflects negative findings.\nGiven mixed evidence from previous literature on the effect of “reading ease” on headline clicks,\nBanerjee & Urminsky (2021  ###reference_b4###)\nfinds that reading ease negatively impacts click-through rates in hrough careful feature engineering.\nConsistent with this result,\nwe found that the hypotheses that claim “straightforward” and “clear” writing to be indicative of higher click-through rates have relatively lower accuracies during training.\nDeceptive reviews contain more emotional terms.\nLi et al. (2014  ###reference_b15###)\nTruthful reviews would mention weddings or special occasions.\n\n###figure_3### Using vivid language and imagery helps.\nBanerjee & Urminsky (2021  ###reference_b4###)\nHeadlines that frame the content in a personal or relatable way are clicked more.\n\n###figure_4### Tweets with emotional tones are retweeted more.\nTan et al. (2014  ###reference_b29###)\nMentioning influential individuals or organizations leads to more retweets.\n\n###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Additional Related Work",
            "text": "Our work, in contrast, focuses on hypothesis generation between the input and the label for real-world challenging tasks and uses a Thompson sampling-style reward to propose novel algorithms."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose HypoGeniC, a novel method that leverages LLMs to generate hypotheses with the goal of discovering unknown knowledge. With HypoGeniC, we are not only are able to generate human-interpretable hypotheses but also achieve better predictive performance against competitive baselines and even oracles. Furthermore, our method can generalize well with different models and datasets, including open models. Notably, with our generated hypotheses, we uncover new insights in real-world tasks that are widely studied in social sciences.\nHypoGeniC can be directly applied to natural language related tasks in social sciences.\nWe encourage future work to explore hypothesis generation that requires additional modalities and/or leverages existing literature."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In this paper, we aim to provide a robust framework for hypothesis generation, as opposed to focusing on the optimization of results.\nThus, we did not perform an extensive hyperparameter search with the generation portion of HypoGeniC.\nWe did not adjust the value of , which determines  in Algorithm 1  ###reference_### to maintain efficiency.\nAdditionatlly, we only considered the effect of using a hypothesis bank size of  and  to only test using an extremely small hypothesis bank size and a large one.\nThe ideal hypothesis bank size may require further investigation.\nFinally, we only tested the size of our wrong example bank  as  to strike a balance between context window sizes and generation of good quality hypotheses.\nWe believe that a more thorough hyperparameter search could improve the performance of our methodology.\nAdditionally, HypoGeniC has high latency, specifically when using inference methods that require multiple prompts.\nFor example, the filter and weighted vote inference policy requires iterating through the top hypotheses to determine relevance and then performing inference if it is relevant.\nFor single-step adaptive inference and best accuracy hypothesis, however, HypoGeniC is efficient.\nWe also note that the main bottleneck with performing inference lies with performing API calls to the LLM, which is a limitation not directly related to HypoGeniC.\nGiven that we request reasoning for all inference prompts, the procedure can be time-consuming."
        }
    ]
}