{
    "title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare",
    "abstract": "We develop Polaris 111Safety is our North Star. We name our system after Polaris, a star in the northern circumpolar constellation of Ursa Minor, currently designated as the North Star., the first safety-focused Large Language Model (LLM) constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare, which focus on tasks like question answering, our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multi-billion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging patient-friendly conversation and several specialist support agents focused on healthcare tasks performed by nurses, social workers, and nutritionists to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### ###figure_2### ###figure_3### Recent progress in large language models (LLMs) has shown their impressive capability to plan, reason and interact with humans for a variety of tasks such as web search web  ###reference_b1###, coding Bird et al. (2023  ###reference_b2###), and intelligent content creation Ray (2023  ###reference_b3###). The scaling of LLMs and the datasets used to train them, together with new architectural advances has contributed greatly to the advances in AI capabilities, and shown to surpass human performance in various benchmark tasks sur  ###reference_b4###. These new capabilities are enabling real-world applications that were impossible until very recently such as those in healthcare.\nHealthcare specialization. Amidst the several domains and applications, healthcare remains a high-stake domain where errors may have fatal implications. The advent of GPT-4 Achiam et al. (2023  ###reference_b5###) has led to a profound surge in the use of AI for healthcare applications, including clinical note and electronic health record processing (see Lee et al. (2023  ###reference_b6###) for overview). While systems like MedPALM Singhal et al. (2023a  ###reference_b7###) and GPT-4 have shown impressive results in general medical benchmarks like USMLE, recent work shows significant error rate for more specialized use-cases like pediatrics Barile et al. (2024  ###reference_b8###). Among the failures, researchers observe that the AI systems struggle to spot known relationships between conditions that an experienced physician would look for, e.g., for a patient with autism, a physician might check for dietary deficiencies.  Jacobsen and DeNiro (2017  ###reference_b9###).\nThe researchers note that these systems could be improved by selectively training on accurate and high quality medical literature, not just general articles over the internet, which are typically what most LLMs are trained on. Furthermore, it is possible that the base knowledge about autism leading to nutrient deficiencies is present in the LLMs; what is missing is the medical reasoning to connect the dots (i.e., some patients with autism exhibit narrow dietary preferences, which can then lead to nutrient deficiencies).\nConversational Healthcare Systems. Most of the existing works for generative AI in healthcare are focused on tasks like medical question-answering Singhal et al. (2023b  ###reference_b10###) and EHR summarization Veen et al. (2024  ###reference_b11###). There are very few works focusing on natural dialogue between caregivers and patients. More recently, Tu et al. developed Articulate Medical Intelligence Explorer (AMIE) Tu et al. (2024a  ###reference_b12###), which outlines the importance of diagnostic dialogue to enable physicians to make diagnoses and develop management plans. AMIE is presented as a tool for physicians, and focused on diagnostic use cases. There remains a gap in work addressing the broader range of conversations between a care team and a patient that are neither diagnostic, nor directed to clinical decision making, such as inquiring whether a patient is adhering to their prescription, whether they are following the physician’s pre- and post-procedure directions, and general wellness check-ins. While non-diagnostic, these conversations must still be medically accurate, and critically, must build rapport and trust with the patient to make them feel safe, supported, and confident in their care, while communicating with empathy and bedside manner. Such relationship has shown to lead to better patient satisfaction and, ultimately, better outcomes in real world Derksen et al. (2013  ###reference_b13###). General-purpose LLM’s, however, are not optimized for such objectives. Furthermore, these LLM’s are also not optimized for real-time, voice-based conversations, which can be quite different from text-based conversations. For instance, factors such as response length, quality of voice, pauses and interrupts greatly impact the subjective experience.\nThe case for AI-based Healthcare Agents. The US healthcare industry is facing a massive shortage of healthcare workers, that became even more apparent after the COVID-19 pandemic ora  ###reference_b14###. Exacerbated by burnout, stress and financial conditions, 16.7% of hospitals anticipated a critical staffing shortage in 2023 according to the Department of Health and Human Service AHA  ###reference_b15###. The U.S. Bureau of Labor Statistics estimates the need to fill over 200,000 nursing positions every year until 2031 Fac  ###reference_b16###. A 2023 survey found that 28.7% percent of nurses were considering to leave their jobs. The trend in the decline of the US workforce indicates a shortage of more than 4 million workers nationwide by 2026 ora  ###reference_b14###. In the meantime, the demand for healthcare continues to grow as the population continues to age. There are currently 46 million adults over 65, and it will increase to 64 million by 2030, and 90 million by 2050 Agi  ###reference_b17###.\nGiven this massive gap in supply and demand for healthcare workers, and the recent promise of Generative AI to supercharge productivity, we focus on developing a non-diagnostic technology for healthcare workforce augmentation, which we denote as super-staffing. In this work, we develop autonomous generative AI healthcare agents that can safely converse with patients on medical topics. Our goal is to improve patient healthcare outcomes by providing a scalable and safe system that can handle non-diagnostic communications. Such a system will allow the human healthcare providers to focus on top-of-license diagnostic and clinical tasks, thereby helping to alleviate staffing shortages. The agents are designed with built-in safety guardrails that ensure appropriate human supervision.\nOur solution is a multi-agent system with highly specialized healthcare LLMs. The following are the primary contributions of our work.\nArchitecture and Training. We develop a unique multi-agent LLM constellation architecture optimized for real-time healthcare conversation. We employ a primary conversational agent with several specialist support agents for a patient-friendly and medically accurate conversation. The primary agent is trained to be aligned with nurse-like conversations geared for building trust, rapport and empathy with patients, as well as accomplishing healthcare-specific tasks typically performed by nurses, medical assistants, social workers, and nutritionists. We develop techniques to make the primary agent stateful to navigate through a long checklist of care protocols. Our support agents are specialized in healthcare tasks that require a high-level of accuracy, such as confirming that the patient’s reported medicine consumption aligns with the dosage prescribed by their physician, determining whether the patient’s reported OTC drug consumption is within the manufacturer’s recommended range, understanding which medicine the patient is referring to (i.e., patients often struggle with correctly pronouncing drug names, and may confuse ones that sound similar), retrieving current and historical lab results from the patient’s EHR, guiding the patient’s food choices to align with their prescribed diet, etc. The specialist agents provide the relevant context to the primary agent in a message-passing framework, which drives the main conversation. To this end, we develop custom training protocols for conversational alignment using organic healthcare conversations and simulated ones using patient actors and nurses (U.S. licensed) with agents- and clinicians-in-the-loop for co-operative learning.\nSafety. We adopt a three-pronged approach to safety comprised of: (1) A 70B-100B primary model trained using evidence based content; (2) a novel constellation architecture with multiple models totaling over one trillion parameters, in which a primary LLM is supervised by multiple specialist support models to improve medical accuracy and substantially reduce hallucinations; (3) built-in guardrails that bring in a human supervisor when necessary.\nEvaluation. To the best of our knowledge, we perform the first extensive real-world evaluation of a healthcare LLM system in which we recruited over 1100 US-licensed nurses and over 130 U.S.-licensed physicians posing as patients for our system. This is focused on an integrated system-level conversational evaluation on dimensions such as medical safety, readiness and bedside manners where we demonstrate parity with human nurses on several key metrics. We also perform a challenging component-level evaluation where we demonstrate that our medium-size specialized agents massively outperform a much larger state-of-the-art general-purpose LLM (GPT-4) on the healthcare tasks as well as outperform an LLM from its own parameter size class (LLaMA-2 70B)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "System Overview",
            "text": "Polaris is architected as a constellation of a primary and multiple specialist LLMs. It also includes the supporting system which orchestrates control flow, inter-model message passing and maintains conversation state. The system is designed to achieve better domain-specific interactions compared to a single general purpose LLM. The healthcare conversation domain is apt for showcasing the value of this paradigm as there are many competing objectives and requirements, including a special emphasis on safety and verification. We further explain the problem domain, system architecture and also provide relevant details on the constituent models in the following sections."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Objectives and Use Cases",
            "text": "We develop Polaris for patient-facing, real-time healthcare conversation. The objective of this system is to perform low risk non-diagnostic tasks typically performed by nurses, medical assistants, social workers, and nutritionists. The system is constructed for voice-based interaction, as phone calls are the dominant method of communication for many high-volume and high-value healthcare use cases. Building a voice-based autonomous agent is a challenging problem due to factors such as voice quality, pitch, tone, response length, interruptions, and communication delay. Additionally, Polaris has to take into account errors introduced by the Automated Speech Recognition (ASR) system in the transcription of the audio signal, with a particular focus on the complexity of recognizing healthcare-specific words and phrases (e.g., complex medication names and lab values).\nWe train our healthcare agents to accomplish a number of tasks that can be completed via phone calls or voice-only interactions. Typical objectives for these tasks include general check-in on patient wellness, reviewing compliance with prescribed medicine regimes, confirming appointment details, reviewing procedural logistics, performing diet reviews, communicating lab results, etc. Given that the number of tasks in a typical care protocol can be quite large and extensive, the agent must be able to maintain and update conversation state with each patient, ensuring that all tasks are completed. The task completion rate directly impacts the success rate for the call. At the same time, we train these agents to have natural conversations that mimic what human caregivers would say, rather than a mere question-answering system as in prior works. For instance, the agent must be able to answer any question the patients may have, address their concerns, and otherwise handle tangential discussions in the conversation. This makes it particularly challenging for the agent to engage in deep conversations with the patient, given that the style, tone and content of these conversations can be quite different from the model’s original training dataset. Such conversation fluidity coupled with its state awareness and medical accuracy remains at the crux of all our design and architectural decisions. Furthermore, we design our system to build trust and rapport with patients, learning about their personality and preferences to demonstrate good bedside manner and leading to higher patient satisfaction and potentially better health outcomes.\nOur system is deeply specialized in many different medical conditions and procedures. For the purpose of this report, we highlight three representative outbound-call use cases including follow-up calls for post-discharge congestive heart failure (CHF), ongoing care for chronic kidney disease (CKD), and pre-operative check-in for colonoscopy. Figure 2  ###reference_### shows an overview of the different capabilities of our system."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Constellation Architecture",
            "text": "###figure_4### The core part of Polaris is the multi-agent LLM constellation. As opposed to a general large model performing many different tasks with varying performance, we break down the faculty into medium-size specialist models with consistent high performance. This is beneficial in settings like ours to optimize for tasks with competing objectives, while keeping the latency low for a real-time application. For instance, this allows us to optimize the primary language model for conversational fluidity, rapport-building and empathetic bedside manner with patients.\nSimultaneously, we focus on developing specialized agents with capabilities that serve a dual purpose: (a) assist the main LLM with relevant healthcare-specific context (e.g., A patient with chronic kidney disease stage 3b is recommended to avoid common NSAIDs like advil, motrin); (b) provide a layer of safety to double-check the information provided by the main model (e.g., 40mg of Lasix twice a day vs. 80mg of Lasix once a day). Each of these agents is tied to specific abilities such as confirming that the patient’s reported medicine consumption aligns with the dosage prescribed by their physician, determining whether the patient’s reported OTC drug consumption is within the manufacturer’s recommended range, understanding which medicine the patient is referring to (i.e., patients often struggle with correctly pronouncing drug names, and may confuse ones that sound similar) retrieving current and historical lab results from the patient’s EHR, guiding the patient’s food choices to align with their prescribed diet, etc.\nWhile Polaris has many such agents, we focus on only a subset of them for illustration. Finally, these agents co-operate to solve a complex task. For instance, the patient utterance ‘‘I am taking dulaglutide. I saw my hemoglobin a1c was 5 - is that normal?’’ requires both the lab agent and the medication agent to work together with the primary agent to formulate the response ‘‘a hemoglobin a1c of 5 is within the normal range and lower than your previous value of 6.5. Your dulaglutide does decrease your hemoglobin a1c level’’.\nSafety Benefits. The constellation architecture offers many safety benefits, one of the most significant being the enhancement of safety through increased redundancy and specialization. Each safety-critical task is performed by co-operation between the primary model and the corresponding specialist agents. This redundancy ensures that if the primary model fails or misses a task for any reason, the specialist agent ensures that the system continues to operate correctly and safely.\nAnother key advantage of the constellation architecture is the modular design which facilitates ease of maintenance and enables continuous system improvements. This modular structure allows for individual specialist agents to be updated, repaired, or replaced without disrupting the entire system. This not only simplifies maintenance but also accelerates the process of rolling out updates, thereby ensuring that Polaris is always equipped with the latest features and security measures. Continuously upgrading the modules further enhances the safety of Polaris , as it allows for swift responses to newly discovered issues as they arise. This is particularly important in the clinical domain where we can update the specialist modules with new information and knowledge about drugs, interactions, policies and protocols without requiring to retrain the entire system.\nSystem architecture. Real-time conversations are especially sensitive to system latency including ASR, LLMs and TTS. The LLM constellation architecture also helps reduce end-to-end latency by allowing all the agents in Polaris to run concurrently, including the primary agent. The support agents can either be synchronous or asynchronous with respect to the primary agent’s activity. The synchronous setting is used when the primary agent’s response must be conditioned on the context provided by the support agent, for instance, in the case a retrieval agent detects the need for a database lookup to provide relevant information for the query – resulting in a higher latency. However, the average latency is lower since the support agents need not be invoked for every user utterance.\nIn the asynchronous setting, certain agent functionalities will not block the primary agent, but their results (contexts) will be made available to condition the primary agent response in a subsequent turn, once the asynchronous agent has completed its task. Polaris also implements a form of ‘garbage collection’ on the support agent’s messages to the primary agent. The system removes stale tasks in order to not overburden the primary agent with outdated instructions not relevant at the current timestep. Finally, the constellation is pre-emptible with respect to new user utterance. The input is processed immediately after the system detects an end to voice activity, not relying on any explicit signal from the user. However, if additional speech is detected, constellation processes are restarted to use the updated information.\nFrom a systems perspective the constellation paradigm is extremely helpful for keeping latency low while still achieving robust multifaceted reasoning. Furthermore, each agent can be trained, updated and deployed independently. The modular paradigm allows for the model(s) in each agent to be customized and even switched for models of different sizes and runtime settings (e.g. quantization, temperature, sampling parameters, etc.) without affecting other models in the constellation. The agents are also trained to emit structured outputs, which are parsed by the code harness of Polaris to execute verifiable control flows, reduce hallucinations and ensure a consistent interface with the primary agent. While this system allows for efficiency through sparse activation of agents and a reduction of average latency, it also creates additional challenges. These include (a) false positives in the support agent event detectors that introduce noise to the primary agent; (b) resolving inter-agent conflicts where multiple agents can potentially introduce conflicting tasks; and (c) added complexity when tuning the primary agent to resolve and follow relevant support agent-provided tasks. We discuss our mitigation efforts for these in subsequent sections."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Orchestration",
            "text": "The internal state of Polaris evolves as a result of the primary agent’s interactions with the user and the message passing conducted by the support agents. To explain this more clearly we adopt the following notation for the system components:\nAgents: Let  be the set of agents, where  is the primary agent and  are the support agents.\nConversation History: Let  be the conversation history up to turn , where  is the user utterance and  is the response of the primary agent at turn .\nAgent-Specific Prompts: Let  be the prompt for agent  at turn , derived from the conversation history .\nSupport Agent Outputs: Let  be the output of support agent  given prompt . The output space is structured with expected fields, e.g., , where  are the fields and  are the values.\nState Changes: Let  be the state of Polaris at turn . The state changes based on the outputs of the support agents, i.e., .\nPrompts: The prompt for the primary agent at turn  is a function of the updated state and the conversation history, i.e., .\nShort-Horizon Tasks: These can be represented as additional prompt snippets, e.g., , which are appended to the primary agent’s prompt based on the state changes.\nPolaris mediates the message passing between agents with deterministic imperative programming. This allows us to keep the output space of the support agents well-constrained and implement control-flow logic to support customized protocols. The tasks which are given to the primary agent are systematically formatted and use synthesized information from multiple agents when applicable. This framework facilitates a cooperative approach to complex dialogue generation, where multiple agents contribute to the overall conversation dynamics. An overall step of the constellation system is described in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "LLM Details",
            "text": "Our LLM architecture designs and training choices are constrained by the requirement to serve the model in real-time as part of our LLM constellation. As a result, we employ a diverse set of LLMs for the support agents ranging from 50B to 100B parameters, whereas the primary agent is always a medium-size LLM (70B - 100B parameters). All of our models follow a decoder-only transformer-based architecture, and contain between 30 and 100 layers. We use Grouped Query Attention (GQA) Ainslie et al. (2023  ###reference_b18###) to achieve faster inference speeds and reduced memory footprint when storing the KV cache during inference, which in turns allows us to serve larger batch sizes for increased throughput. Different models use different tokenizers depending on the use case and the need for specialized medical terminologies (such as drug names), with vocabulary sizes ranging from 30,000 to 200,000 tokens. The implementation of the attention mechanism employs Flash Attention 2 Dao (2023  ###reference_b19###), which makes the training and inference stages faster, with lower memory footprint and more efficient scaling to longer sequence lengths. All of our models use RMSNorm normalization layers Zhang and Sennrich (2019  ###reference_b20###), SwiGLU activation functions Shazeer (2020  ###reference_b21###), and Rotary Positional Embeddings (RoPE) Su et al. (2024  ###reference_b22###). The maximum context window varies from model to model, ranging from 4,096 to 32,768 tokens. The training runs were performed using a distributed, multi-GPU setup, using several hundred Nvidia H100 GPUs with DeepSpeed Rasley et al. (2020  ###reference_b23###).\nThe primary agent is trained in 3 stages: General Instruction Tuning, Conversation and Agent Tuning, and RLHF. The training and dataset details for the primary agent are discussed in Section 3  ###reference_###. The rest of the models are trained using human-labeled datasets, as defined in the respective sections for each agent.\n###table_1### During inference, we deploy some of our models in bf16 precision and others in int8 precision (using weight-only quantization), depending on the latency and accuracy requirements.\nMedical Knowledge and Reasoning\nProprietary Data\n✓\n✓\n✓\nGeneral and Quantitative Reasoning\n✓\n✓\nRapport-building, Bedside Manner, Polished Conversational Style\nProprietary and Simulated Conversations\n✓\n✓\nMitigating Hallucination, Fact Verification, Knowledge Augmentation\nPolicy, manuals, clinical references, curated lists\n✓\n✓\nNavigating Complex Care Protocols, Responding to Complex Clinical Tasks\nSimulated Conversations\n✓\n✓\n✓"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Conversational Alignment",
            "text": "In the healthcare domain, the development of a patient-focused clinical framework requires an intelligent conversational agent. This agent has to be designed to foster empathetic engagement with patients, skillfully addressing their concerns and assessing health conditions, be a motivational coach and adopt the multifaceted role of a dietary mentor. It has access to all relevant patient information and medical history to reason and navigate through a complex clinical conversation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Data",
            "text": "The development of conversational agents capable of engaging in meaningful and accurate clinical discussions represents a significant challenge. This challenge is compounded by the lack of datasets specifically tailored for training models in the nuanced context of healthcare conversations. Our work addresses this critical gap by leveraging a unique compilation of dialogues, including simulated interactions between registered nurses and patient actors. These dialogues form the cornerstone of our approach to developing a conversational agent with the proficiency to navigate a wide array of clinical discussions. To enhance the agent’s reasoning capabilities, instruction-following proficiency, and domain-specific knowledge – we augmented these datasets by introducing diverse kinds of instructions and tasks geared for multi-hop reasoning. For instance, given a user utterance “my shoes do not fit”, the agent should be able to reason about “swollen ankle  sign of fluid retention  sign of exacerbating CHF conditions”."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Foundation Model Training Data",
            "text": "We first train our foundation model on a massive collection of proprietary data including clinical care plans, healthcare regulatory documents, medical manuals, drug databases, and other high-quality medical reasoning documents. The objective of this phase is to incorporate fine-grained medical knowledge, reasoning and specialized numerical reasoning (e.g., dosage calculations). We further annotated some of the medical datasets with reasoning chains to further enhance the medical reasoning capabilities of the model. Figure 31  ###reference_### shows one of the examples of the medical question answers we leveraged for training our foundation model."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Simulated Conversations",
            "text": "The nature of our conversational objectives requires specialized data. The agent should be able to maintain conversational state over relatively large time horizon; a multi-turn call in our setting often exceeds  minutes with several dozen turns from each speaker. It is important to generate data which portrays tougher trajectories within the healthcare setting of our calls. Our system must be robust to handling patients with diverse profiles, for instance, high engagement with many questions or concerns; low regimen compliance; skepticism of AI health services, etc.\nWe thus rely on the domain expertise of medical professionals, US-licensed nurses in our setting, to generate accurate data which exhibits comprehensive coverage of our desired data distribution. We leverage a large number of registered nurses, patient actors and our AI-in-the-loop to generate simulated conversations. For targeted medical conditions (e.g., CHF, CKD) and procedures (e.g., post-discharge, pre-operative, chronic-care followups), clinicians create conversational scripts (refer to Table 3  ###reference_### for a simplified version of our CHF script). We also create a large number of fictional patient profiles with different medical histories, condition severity, medications, labs, lifestyle and personality traits (refer to Table4  ###reference_### for a sample patient profile).\nThis call is a routine CHF Assessment\nVerify the patient’s identity - First name, Last name, DOB\nReview the patient’s current medications (new + old, excluding discontinued) with the patient, including dosage\nReview patient’s diet, educate the patient on following low sodium diet and fluid restriction if necessary\nAsk the patient if they have any new or worsening shortness of breath with activity, laying down, or at rest?\nAsk the patient if they have any ongoing or worsening chest pain\nAsk the patient if they have any new or worsening dry cough\nAsk the patient if they have increased swelling in their feet, ankle, stomach or legs\nAsk the patient to weigh themselves daily\nIf they weigh themselves regularly, ask the patient if they have gained more than 2 pounds in the past day or 5 pounds in the past week.\nAsk the patient if they are using more pillows at night to sleep\nInform the patient that we just performed a simple CHF assessment and encourage them to do it at home every day\nAsk the patient about physical activity and coach them to improve their routine if warranted\nWe require clinical expertise to train all parts of Polaris. In order to do this, we employ a bootstrapping strategy for data generation. Initially, we generate conversations using registered nurses and patient actors. We design these conversations to cover a wide breadth of scenarios and behaviors on the part of the simulated patient. We give specific instructions to patient actors to converse such that many of the specialist agents are triggered, such as asking complex questions about medications, mispronouncing medicine names, providing deliberately confusing lab results (e.g., a blood glucose reading in response to a request for their blood pressure reading), asking about hospital specific information (e.g., where to park), etc. During these conversations the registered nurse annotates the transcripts for when these events occurred and what the specialist agents should do during corresponding turns.\nWe then proceed to train both the primary and support agents from this collection of datasets. This allows us to start using our trained agents in the conversation instead of nurses; their responses are reviewed by registered nurses with potential re-writes for the noisy turns. We also employ instruction tuned language models to perform data cleaning and augmentation. This is used to improve the style of responses and also create additional variety, preventing mode collapse in the primary agent distribution and ensuring robustness in the support agents. Like most bootstrapping situations, this process becomes more effective with more rounds of training, which allows generalization to new scripts, medications, and so on.\nName: Mary Adams; DOB: 1950-01-01; Gender: Female; Condition: Congestive Heart Failure\nLasix 20 mg oral tablet;\nPotassium 20 milliequivalent oral tablet;\nLisinopril 10 mg oral tablet;\nTylenol 500 mg oral tablet\ninr: 1.0;\nglucose: 143.0 mg/dL;\nhemoglobin: 9.4 g/dL;\nhematocrit: 29 %;\nwbc: 5.0 x/L;\nplatelets: 231.0 x/L;\nmcv: 81 fL;\npcv: 0.48 fL;\nred blood cell count: 4.1 x;\nmch: 28.0 pg\nHere we provide more details on the entities which participate in the data generation process.\nPatient Actors. Here the actors are asked to play the role of a fictional patient and portray a realistic patient experience. They are encouraged to use the fictional patient’s background as well as their personal experiences and feelings to guide them. We provide them with sample instructions, for instance: Ask about lab values - what they mean, whether they’re normal or not; state they’re taking the wrong medication dosages, or that they are taking medications not on their chart, and so on.\nRegistered Nurses. We ask the nurses to play the role of an ideal nurse at the bedside, on the phone, or in their community. While we provide them with the conversational script with defined call objectives, they are encouraged not to follow the order strictly, but instead follow the natural conversation trajectory for a realistic experience. This allows the patient at times to go on tangents, for instance, sharing their personal experiences that the nurse can engage with to develop rapport and trust; share their health concerns and symptoms that the nurse can both empathize with and educate on.\nOur AI-in-the-Loop. Once our models are tuned with conversations generated from the earlier phases, we further use them to generate synthetic conversations. Here, the AI plays the role of a nurse with the conversational script and meta instructions used in the prompt as a preamble to converse with the patient actors. This stage is required primarily for tuning our support agents (discussed in Section 7  ###reference_###) that generate tasks and provide relevant context to condition the primary agent to generate the response. After our support agent system is sufficiently tuned, we proceed to RLHF data generation. For this we sample multiple responses from the primary agent for the full prompt constructed from the meta-prompt, conversation history and support agent tasks. We leverage our registered nurses to give preference feedback to perform RLHF on the primary agent."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Conversational Dataset Construction",
            "text": "The complexity of clinical calls and multi-turn conversations informed our approach to dataset construction. Most instruction tuning regimes involve single or few-turn interactions between the user and assistant. In our case, most calls consist of several dozen turns from each call participant. We convert a conversation with  turns from each participant into  training data points, where for each , we create a prompt with the conversation history up to turn  with meta-instructions in the preamble, mask the prompt, and train the primary agent to predict the content of the nurse turn . We apply a standard cross-entropy next-token loss objective for this supervised training.\nA standard practice for language model training on multi-turn conversation consists of applying loss masks to all of the user turns and training on all of the assistant turns within a single training data point. This approach has the benefit of increased training compute efficiency when compared to splitting up all of the turns into individual training data points as we do. However, in our case this approach is not applicable because we expose a conversation state description  to the primary agent which evolves as a result of the other agents in the constellation system . Aside from the conversation history  growing turn by turn, the primary agent system prompt is changed due to the effect of the checklist specialist (see Section 4.2  ###reference_###). Furthermore, the other clinical specialist agents inject short-horizon tasks and instructions . These are placed with their own format, and expire based on heuristics inside of the task orchestration system. We thus create a separate prompt for each turn which is a function of  that has its loss masked, and we train only for the labeled response .\nFigure 33  ###reference_### shows selected dialogues between an anonymized nurse and patient, our approach grounds our healthcare agent in authenticity, drawn from real-world interactions. This strategy enriches the training process, ensuring our agent is deeply rooted in the nuances of genuine healthcare encounters."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "General Instruction Tuning",
            "text": "In this step, we also focus on honing the model’s ability to follow instructions by augmenting our proprietary data with complex tasks and instructions including multi-hop logical reasoning, mathematical reasoning, and task completion.\nThe objective of this stage is to improve the model’s capability in executing intricate tasks within the medical domain like clinical assessment."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Conversation and Agent Tuning",
            "text": "Conversation Tuning: \nDuring this phase, we train the model using simulated dialogues between registered nurses and patient actors, alongside a spectrum of general, unstructured conversations. The objective of this stage is to increase the communication capability of the model in terms of refined bedside manners, empathy, rapport, a polished conversation style like a healthcare professional.\nThis enrichment process enhances the model’s grasp of complex conversational nuances found in real-world healthcare settings.\nReal-world healthcare conversations\nimbue the model with the characteristic warmth, understanding, and professionalism needed to bridge the gap between automated systems and human touch in patient care.\nAgent: Great, thank you for taking the time to speak with me. Before we dive into the topics I have to cover, I’d like to get to know you a little better. Is there anything you’re looking forward to doing this week? \nPatient: Actually, yes! I am looking forward to a little gardening. Its something I love and it’s very relaxing for me. \nAgent: That sounds wonderful, George! Gardening can indeed be very therapeutic. What kind of plants do you usually tend to in your garden? \nPatient: I enjoy growing a mix of things. Vegetables, herbs, and a few flowers. It’s nice to grow my own healthy food and see everything come to life. \nAgent: That’s fantastic! Growing your own food can be so rewarding. What’s your favorite vegetable or herb to grow? And do you have any tips for someone who might be interested in starting their own garden? \nPatient: My favorite herb is basil. It’s versatile and grows pretty easily. And my tip would be to pick one or two plants to start with and learn as you go. Gardening is a process of trial and error, so don’t get discouraged. \nAgent: Basil is indeed a wonderful herb to grow and use in cooking. And thank you for the gardening tip. Starting small and learning as you go sounds like a wise approach. Do you use your home-grown basil in any special recipes? \nPatient: Yeah, I use it in homemade pesto and toss it into pastas and salads. \nAgent: Homemade pesto sounds delicious, and it’s a great way to incorporate fresh herbs into your meals. Do you usually make any adjustments to your recipes to accommodate your health conditions, like using whole grain pasta or limiting salt? \nPatient: I do try to stay healthy by using whole grain pasta and keeping an eye on salt. \nAgent: That’s a great approach, George. Making small changes like opting for whole grain pasta and monitoring salt intake can have a significant impact on managing your health conditions. Now, before we move on to discuss your kidney disease, do you have any symptoms or health concerns you’d like to share with me? \nPatient: No specific concerns, I’m managing quite well right now. \nAgent: I’m relieved to hear that you’re managing well. Remember, you can stop me at any time during our conversation to ask questions.\nPatient: Most days. Not every day, but most days.\nAgent: I’m relieved to hear that you’re keeping an eye on your blood sugar levels. It’s a key part of managing your diabetes. Can you recall what your most recent blood sugar reading was?\nPatient: You know, I’m a little embarrassed to say it was a hundred and thirty eight.\nAgent: No need to feel embarrassed, Mr. Hernandez. We’re all human and it’s perfectly fine to have off days. Can you recall if you took this reading before eating or after you had a meal?\nPatient: I took it actually before I ate anything.\nAgent: I appreciate you letting me know that. A fasting blood sugar level of one thirty-eight is slightly above the usual range, which is between sixty and one oh five. This could be due to a variety of factors, such as your diet, stress levels, or even the timing of your medication. Let’s work together to figure out what might be causing this. Have you noticed any specific patterns or triggers that seem to affect your blood sugar levels?\nAgent Tuning:\nWe also perform specialized alignment or agent tuning focusing exclusively on clinician-patient dialogues in this step. This targeted training sharpens the model’s proficiency in handling specific scenarios anticipated within our system’s operational context. A significant aspect of this stage involves training the model to adeptly navigate and adhere to intricate checklists tailored to individual patient conditions and prescribed care protocols, where we give considerable attention to both medical and non-medical exchanges.\nDuring this phase, we engage in iterative co-training between the primary agent and support agents, where the primary agent is trained in tandem with support agents. The support agents are trained to pay attention to signals in the primary agent’s conversation with the user. And conversely, the support agents communicate with the primary agent by modifying its prompt, and injecting relevant information and instructions.\nFigure 3  ###reference_###, Figure 4  ###reference_### and Figure 32  ###reference_### show excerpts from our simulated conversations between the patient actors and Polaris.\nAgent Tuning is the hardest part of the entire Conversation Agent training pipeline. It is quite challenging to train the constellation of the LLMs which communicate together with the primary agent. Figure 5  ###reference_### shows how conflicts can arise between various agents. In Figure 5  ###reference_###, the primary agent is provided with two tasks from the Lab & Vitals Agent, and the Human Intervention Agent. When multiple agents add a task for the primary agent to follow, the primary agent resolves the conflict by prioritizing the Human Intervention Agent and giving a safe and informative response to the patient. To train such a system we leverage actual physicians and nurses who label the data and assimilate the information from various agents to provide the most appropriate response to the patient.\nAgent: You’re doing a great job managing your diabetes. Let’s shift our focus to your blood pressure.Can you recall when you last checked it?\nPatient: Its 140 over 90.\nAgent: Is there anything else you’d like to tell me about your blood pressure or any other health concerns?\nPatient: No. I went out to eat last night, and I had some miso soup, probably that’s why its up.\n                   <Lab And Vitals Agent TASK>\nFor ’Systolic’ The normal range for blood pressure is 90 to 120. The patient’s systolic reading is a high.\nFor ’Diastolic’ The normal range for blood pressure is 60 to 80. The patients diastolic reading is a high.\n                   <Human Intervention Agent TASK>\nTELL the patient you have taken a note of the patient’s issue that the patient has high blood pressure and will let their care team know. \nAgent:\nOkay, Mr. Hernandez. It’s not uncommon for certain foods, particularly those high in sodium like some miso soup, to cause a temporary rise in blood pressure. I’ve made a note of this and will pass it along to your care team. Let’s proceed to your recent lab results. Do you have any questions or concerns about them?\nSelf Training: A typical challenge with training the AI on simulated conversations is that of generalization. Even though we train our models on diverse scripts and conditions to cover a wide range of scenarios, it still suffers from exposure bias He et al. (2019  ###reference_b24###). This results in the AI producing similar responses in the conversation, increase in repetitions and lack of robustness to diverse scenarios it never encountered during training. This results from the way we instruction tune these models with next token prediction during training with teacher forcing: where the model produces the next token conditioned on ground-truth token in the previous step. However, during inference, there is no ground-truth and the model samples a token conditioned on self-predicted token in the previous step – creating a training and inference mismatch referred to as exposure bias. To address this, we train the AI on self-generated responses during training. To this end, we train the model for a few steps; use the current model parameters to produce responses on sampled training prompts with temperature sampling to augment the original training data; and then re-train the model. This ensures the model encounters diverse training samples in different iterations and also aware of its own uncertainties during sampling at inference – thereby minimizing the distribution shift."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "RLHF",
            "text": "The final phase incorporates Reinforcement Learning from Human Feedback (RLHF) by engaging directly with healthcare professionals. This crucial step refines the model’s performance across key dimensions, including safety, empathy, and the overall quality of care-related conversations. Through this iterative process, we ensure the model’s responses are not only accurate but also aligned with the nuanced requirements of empathetic patient care.\nWe gather preference data from the same pool of nurses that were involved in the conversation data generation. Nurses are presented with the overall context, the conversation history and any active tasks provided by the support agent system. They are then asked to rank multiple candidate responses in their order of preference. This is nontrivial as there are multiple factors that determine the quality of a response. We ask them to prioritize medical safety and accuracy above everything else. Given that, responses are judged for empathy, bedside manner, level of patient education, and other conversational features. Within this, we suggest heuristics for what would be considered ‘acceptable’ with regards to conversational fluency. If the nurse judges none of the responses to be acceptable, we ask them to write an ideal response. Importantly, the rewrite must still conform with the tasks given by the support agents in order to not break the cooperative nature of the constellation system. From the ranked responses we construct pairwise preference samples. We exclude pairs where the preferred response was deemed unacceptable. When there are available rewrites, we only create pairs where the rewrite is preferred and all model responses are not preferred."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Specialist Support Agents",
            "text": "A patient-facing model must be able to robustly perform many technical functions while in conversation, such as verifying prescription adherence and reviewing lab analysis results. These functions must properly contextualize the patient’s medical history and successfully evaluate against a vast space of medical ‘edge cases.’ Furthermore, they should be invoked at appropriate trigger points from the conversation flow. We align our design with the intuition that language models exhibit better reasoning when given targeted instructions compared to overloading the context with extraneous information. The primary conversation agent is required to maintain a large context in order to achieve optimal conversational fluency, but much of this is irrelevant for the medical reasoning subroutines. Hence we implement a modular system architecture where the primary conversation agent is augmented with support agents that serve specialized functions to ensure high reliability and safety."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Privacy & Compliance Specialist",
            "text": "An essential aspect of patient-facing medical calls is verifying the patient’s identity. It’s important to do this verification before any personal information on the patient is loaded into the LLM context to prevent any leaking of information. It is also paramount that the identity verification has an extremely low error rate and no hallucination. To protect patient privacy and comply with privacy rules, the system must verify that the person answering the call is the intended recipient, before engaging further. In order to do this, the Privacy & Compliance Specialist confirms identifiers including full name and one of date of birth or medical records number. This agent can easily be trained to verify additional identifiers as appropriate (e.g., authorized care manager, parent, etc.).\nUntil the recipient’s identity has been confirmed, the system should not share such PHI with the recipient. However if the primary agent’s prompt contains salient details of a patient’s Electronic Health Records to support discussion of medications, lab values, etc., it is vulnerable to disclosing some of the information to the patient (e.g., by red teaming).\nOne solution is to use a higher-order state transition to ensure that the primary agent has no PHI in its context at all until the patient’s identity has been verified. During the initial state, the primary agent requests to speak with the intended recipient. When the target recipient becomes available, the primary agent requests that the patient provide the required patient identifiers. During these first two stages, the primary agent does not have the correct DoB or MRN in its context. Therefore, to verify the patient-provided identifiers, the primary agent forwards them to a second component that checks these details against the protected database.\nIf the second agent finds that the identifiers are incorrect, it returns a non-PHI response specifying failure that prompts the primary agent to re-verify. After a few failed retries, the primary agent terminates the call. Once the second agent finds that the identifiers are correct, the main phase of the conversation begins. This is depicted by the transition from the yellow to purple regions of the diagram in Figure 6  ###reference_###. At this point, PHI such as EHR details are loaded into the primary agent’s prompt.\n###figure_5###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Checklist Specialist",
            "text": "LLM’s struggle to follow complex instructions. This gets more challenging in our setting with complex call objectives and a long checklist of ordered instructions. In order to have a natural flow of conversation, we allow the user to go on tangential discussions while accomplishing a particular task in the checklist. This makes it challenging to bring the conversation back and resume from the pending checklist items. To address this challenge, we devised the checklist specialist to guide through complex call objectives.\nTo make this tractable, we leverage synthetic scripts with ordering of tasks as part of a call specification. We then train an LLM specifically to assess the conversation state concurrently with the primary agent model. This support model determines which tasks have been completed and correspondingly updates an internal data structure. Changes in this data structure propagate into the prompt for the primary agent by discarding objectives that have been accomplished and popping from a queue of objectives that are pending. This mechanism thus acts as a form of higher-order attention. We find this to be highly effective in ensuring the primary agent achieves all of the pre-specified objectives, especially when the patient asks additional questions or goes off on a tangential discussion, not necessarily related to the objective in question. Furthermore, this system bounds the number of input tokens related to the tasks so that very long scripts e.g. comprehensive health questionnaires are still manageable.\nThe evaluation of states can be challenging when the individual tasks can be complex and multifaceted. For example, a simple task might be to confirm with the patient the date, time and location of their upcoming procedure. Once the agent states these details and the patient acknowledges they understand, then the task is complete. However, there are other, more involved tasks such as going over a patient’s diet and giving appropriate guidance on it. In this scenario it is plausible that the primary agent gives substantial advice on typical breakfast, lunch and dinner and is ready to move on. However, the checklist specialist may not share this belief, creating a deadlock. We could choose to have both models decode chains-of-thought and perform reconciliation, but this and similar strategies break the latency constraints imposed in the voice-based setting for our system.\nInstead, we adopt a strategy of letting the primary agent decide when to move onto the next set of tasks as it deems appropriate. The checklist specialist is thus tasked with passively documenting the transitions rather than actively enforcing them. This removes the possibility of deadlock and also ensures the primary agent’s conversation does not follow the scripts exactly verbatim, but allows for a natural flowing conversation. To further ease the role of the checklist specialist, we organize the scripts into sections, and prompt the specialist to assess whether the primary agent has moved from a given section to another one – where a section comprises of a set of related tasks and instructions. This allows us to manage states across long conversations with several complicated tasks. This specialist also plays a similar role during a potential human intervention scenario as there is a checklist describing primary agent protocol. Thus, it is technically the owner of the Intervention State Model depicted in Figure 22  ###reference_###. Finally, this agent also decides when to terminate the call as appropriate – a vital functionality for real-world scenarios."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Medication Specialist",
            "text": "The Medication Specialist supervises and enhances medication-related interactions between the primary agent and the patient. This agent is developed with instructions from licensed medical professionals, clinical guidelines and standards of care to ensure patient safety and adherence to prescribed therapies. The agent is powered by two fine-tuned large language models and physician-approved reference tables for a wide number of medical conditions. This agent has several capabilities: precise medication identification, dosage discussion including for OTC drugs and contraindications of OTCs (based on the manufacturer’s guidance).\n###figure_6###"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Prescription Adherence",
            "text": "A key challenge in healthcare is ensuring patients adhere to their prescribed medication regimen. General large language models are susceptible to suggestions and struggle with mathematical reasoning. Patients will often misstate how much of the medication they should take. The specialist agent ensures dosage values in the Electronic Health Record (EHR) are enforced. The Medication Agent assists the primary agent in this task by detecting errors such as forgotten doses, depleted medication supplies, inappropriate compensatory dosing, discrepancies in medication frequency, misunderstandings of pill strength, and the incorrect quantities of medications taken. Such challenges are especially common among chronic patients managing multiple medications, highlighting the necessity for the agent’s precise detection and intervention capabilities.\nThe Medication Specialist agent is equipped to detect deviations from prescribed regimens, whether it be taking medication less frequently than prescribed; the patient adjusting the strength of the dose; or altering the number of pills ingested. Upon identifying such discrepancies, the agent guides the patient towards the correct regimen using motivational interviewing tactics, and involving the human care team through the Human Intervention Specialist as shown in Figure 8  ###reference_###."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Condition Specific Disallowed OTCs",
            "text": "Condition-specific advice for OTC medications is a critical feature of the Medication Agent. General large language models tend not to be aware of OTC contra-indications for specific conditions. In many conditions, common over-the-counter medications and supplements can be harmful. Our agent allows providers to specify contra-indicated OTCs and will recognize their usage. For example, if a patient expresses a desire to take Ibuprofen for a headache, the agent cross-references this choice with the patient’s medical profile. Should the patient have Stage 3B Chronic Kidney Disease for which Ibuprofen is contraindicated as per the Physician Approved Reference Tables, the agent generates a task signaling the need to avoid this medication, thereby preventing adverse reactions. Conversely, if the physician approved reference recommends this OTC for the patient’s condition, then they are informed they can take the medication. This is illustrated in Figure 9  ###reference_###"
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Over-The-Counter (OTC) Toxicity",
            "text": "The Medication Agent includes a critical function for verifying the safety of over-the-counter (OTC) medication dosages, safeguarding against the risk of toxicity. Max OTC dosage calculation depend on many different factors including age, weight, composition (capsule, tablets, liquid, etc) and strength. General language models are not good at reasoning across these different variables.\nWhen a patient reports taking an OTC drug, the agent cross-references the quantity consumed with the maximum allowable limits outlined in the OTC Drug Label (or a Physician Approved Reference Table). Similar to the prescription adherence capability, this capability involves analyzing the conversation to identify the amount taken, which the patient may convey in different ways. The agent guides the primary model to elicit necessary information and communicate to the patient the appropriate maximum dosage. In addition, if the amount reportedly ingested by the patient exceeds these thresholds, the agent triggers a task for the Human Intervention Specialist as shown in Figure 10  ###reference_###"
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Unrecognized Medications Reconciliation",
            "text": "Drug names are complicated. Patients often struggle to pronounce or recall them correctly. The medication specialist agent is used to guide the patient through and disambiguate a recognition process. To address patient mispronunciation of drug names, or errors from the ASR system in understanding the patient’s pronunciation, the agent uses a workflow that includes questioning the patient to clarify medication names. This process helps in accurately identifying medications, ensuring reliable management advice despite mispronunciations or ASR inaccuracies.\nAdditionally, when a medication, whether OTC or prescription (Rx), is not listed on the patient’s EHR or physician approved reference tables, the agent recommends the patient to consult with a healthcare professional. This prevents the unauthorized or uninformed use of medications and prevents the primary agent from providing general advice, reinforcing the importance of physician-guided medication management.\nMedication Detection\nIdentifying specific medication mentions in conversations\nDetecting a mention of \"ibuprofen\"\nDosage Evaluation\nAssessing if the mentioned dosage aligns with the prescription\nEvaluating \"taking 40mg instead of prescribed 20mg\"\nPermissibility Check\nDetermining if a medication is allowed based on the patient’s conditions\nAssessing if ibuprofen is safe for a patient with certain health conditions\nAdvisory Provision\nProviding specific recommendations or advising to consult healthcare providers\nAdvising \"Consult your doctor before taking this medication\" if not found in the database\nReconciliation Support\nSupporting medication reconciliation for undocumented meds\nInitiating a reconciliation task for a new medication mention\nPatient Education\nEducating patients on correct medication usage and adjustments\nEducating a patient on their physician’s protocol for correctly addressing missed doses"
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Models",
            "text": "The Medication Agent is powered by two specialized LLM models: the Medication Detector and the Dosage Evaluator. The workflow of the specialist is shown in Figure 7  ###reference_###.\nMedication Detector: This model is trained to accurately identify and understand the context of medication usage in patient conversations. It utilizes the datasets generated from simulated conversations to recognize a wide variety of medication names, including those that are mispronounced or less commonly known, and to understand the context of the conversation. The Detector is passively listening to the conversation between the primary agent and the patient’s every turn and identifies the list of all medication names if discussed and the context of the conversation. The key challenge here is to make the detector robust enough to identify both correct and misspelled medication names without being overtly sensitive to false positives. To achieve this, the model is trained on a diverse dataset that includes a wide range of medication names, brand names, generic names, and common misspellings or phonetic variations. The output of the Medication Detector is a structured list of medications mentioned in the conversation, along with their associated context. This information is then passed to the Dosage Evaluator for further analysis of the patient’s adherence and dosage safety.\nDosage Evaluator: The Dosage Evaluator focuses on analyzing the dosage and adherence patterns mentioned in conversations. It is fine-tuned to detect discrepancies between prescribed regimen and patient-reported usage, including errors in timing, strength, frequency, and quantity. The model is trained to use the structured information from the patient’s EHR or Physician Approved Reference Tables as ground truth to identify potential issues with the reported dosages in the conversation by the patient. The model returns whether the dosage reported by the patient is CORRECT, HIGH or LOW with respect to the ground truth. The key challenge here is to develop a model that can accurately parse and interpret the often ambiguous and incomplete dosage information provided by patients in natural conversation. Patients may use colloquial terms or omit key details when discussing their medication usage. The Dosage Evaluator needs to be robust to this variability and be able to infer the intended meaning. To address this, the model is trained on a large corpus of simulated conversations that cover a wide range of dosage scenarios, including both compliant and non-compliant usage patterns. The training data is augmented with techniques like synonym replacement, numerical perturbation, and random omissions to improve the model’s resilience to linguistic variation."
        },
        {
            "section_id": "4.3.6",
            "parent_section_id": "4.3",
            "section_name": "4.3.6 Training Data Generation",
            "text": "The Medication Agent utilizes two Large Language Models (LLMs) for medication and context identification and dosage analysis. To fine-tune these LLMs, datasets are collected through simulated conversations between the primary agent and human nurses acting as patients. These nurses are tasked with discussing a wide range of medications, both prescription and over-the-counter (OTC), that a patient might be taking. They are also instructed to simulate various types of dosage adherence errors commonly made by patients, such as taking incorrect doses, misunderstanding prescription instructions, and irregular medication timing. This simulation process is designed to generate a dataset that captures the diversity, complexity and nuance of real-world patient medication management scenarios."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Labs & Vitals Specialist",
            "text": "Patients often inquire about their medical lab test results and vital signs in clinical interactions Cardona-Morrell et al. (2016  ###reference_b25###). For example, a diabetic patient might ask for their hemoglobin A1C result, which measures the average blood sugar levels over the past 3 months Pongudom and Chinthammitr (2019  ###reference_b26###). A patient with congestive heart failure might be interested in their most recent blood pressure values, or inquire about the overall status of their laboratory and vital sign results. In some other cases, patients might initiate discussion of lab values that are not present in the EHR. Therefore, it is necessary to design a dedicated specialist that assists with requests related to labs and vital measurements. Figure 11  ###reference_### contains an overview of the workflow of the Labs & Vitals Specialist.\n###figure_7###"
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Determination of Labs & Vitals and Value Extraction",
            "text": "To properly respond to any lab-related queries, the Labs & Vitals Specialist must accurately detect patient intent and associate the intent with a valid medical lab test or vital sign measurement. Furthermore, it must be capable of understanding and extracting the corresponding values of mentioned labs and vitals from the conversations. For example, if a patient states that her international normalized ratio (INR) result is around 1.1, this specialist should report the mentioned lab and its value to the system. Subsequently, this specific value is analyzed to deliver relevant feedback. See Figure 12  ###reference_### for some examples.\nPatient: “My urine dipstick glucose test is positive.”  Urine dipstick glucose = positive\nPatient: “Hmm, the lab report states that my INR level is 1.1.”  International Normalized Ratio = 1.1"
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Normal Range Assessment & Plausibility Check",
            "text": "General language models lack medical grounding of lab reference ranges and tend to get confused due to the different reference ranges on the internet. Identifying the correct reference range for a patient’s lab value, given their age, gender, etc., is essential for accurate lab presentation and the avoidance of hallucination.\nUnderstanding and presenting the normal ranges for laboratory tests and vital signs is a common request from patients. The normal range refers to the set of values that includes the majority of results for healthy individuals. For instance, one might ask, \"Is a hematocrit level of 43 normal?\". This question seeks to ascertain whether the patient’s hematocrit level falls within the expected range for healthy individuals. See Figure 13  ###reference_### for our system’s response to this query.\nFor each lab, we also define a broader set of values that we term the plausible range. A patient reported lab outside of this range may be erroneous, and warrants clarification. For example, the normal range for systolic blood pressure is between 90 and 120 mmHg Whelton et al. (2018  ###reference_b27###) for the majority of healthy individuals. The plausible range extends from 60 to 250 mmHg Whelton et al. (2018  ###reference_b27###). This wider range acknowledges that values outside the normal can occur.\nIf a patient reports that her blood pressure is 125, the Labs & Vitals Specialist should first determine that the patient is referring to systolic blood pressure and that the associated value is 125. Following this identification, the specialist prepares feedback to supply the normal range information to the conversational agent. This feedback will also include a note indicating that the patient’s blood pressure is above the normal range but still within the plausible range. By contrast, the agent should confirm a reading that is outside the plausible range. For example, if the patient reports a systolic blood pressure of 500, the conversation might include a plausibility check like, \"Your systolic blood pressure of 500 is not plausible. Did you accidentally make a mistake?\"\nIn addition, if a reported lab result or vital sign value falls outside the intervention reference range (e.g., if the reported blood pressure is 190), the Human Intervention Specialist will step in and decide whether the call needs to be transferred to a human nurse. This capability is designed to enhance patient safety."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Historical Lab Trends",
            "text": "Medium sized (70B-100B) general language models tend not to interpret sequences of numbers well. Understanding of lab values over time is critical to understanding if patient is improving or degrading and essential for chronic care coaching. To address this challenge, this specialist agent links patient queries to existing information in the EHR and conditions some of its answers on patient records. For instance, if a patient asks, \"How’s my A1C value of 5.6% compared to the previous measurement?\", the specialist retrieves and compares this latest result with prior entries (see Figure 14  ###reference_###). The Labs & Vitals Specialist then provides this information to the conversational agent to produce a contextualized response to the patient’s query. This specialist can also supply information for all the labs or vitals in the patient’s records when it receives a generic query. In addition, it provides trend analysis on a specific lab when multiple records of the same lab exist in the patient’s file. In general, it is necessary to condition answers in EHRs since patient records contain useful demographics and diagnostic information specific to individual patients."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4 Conditional Factors",
            "text": "The results of many labs and vital signs are significantly influenced by various factors, which can alter the interpretation of what is considered a “normal” range for any given test.\nFactors include inherent patient characteristics such as gender and age, which are known to affect the baseline values of many lab tests. For example, hemoglobin levels can vary by gender, with males typically having higher levels than females Murphy (2014  ###reference_b28###). Similarly, the normal range for certain lab tests, such as Glomerular Filtration Rate (GFR), changes with age and should be interpreted accordingly Musso and Oreopoulos (2011  ###reference_b29###).\nSpecific medications can alter lab values, often through their mechanism of action and hence necessitate differential lab value analysis. General language models can overlook these medication-induced changes in lab values.\nFor example, dapagliflozin can cause glucosuria Komoroski et al. (2009  ###reference_b30###) and hence alters the assessment of a urinalysis. See Figure 15  ###reference_### for another example. Additionally, the timing of the test in relation to food intake can affect the outcomes for certain labs: blood glucose levels are expected to be higher after eating (postprandial) than when fasting."
        },
        {
            "section_id": "4.4.5",
            "parent_section_id": "4.4",
            "section_name": "4.4.5 Summary of Labs & Vitals Specialist Capabilities",
            "text": "We summarize the core capabilities (see Table 6  ###reference_###) of the Labs & Vitals Specialist below:\nIdentification and determination of specific laboratory tests and vital sign measurements from the patient’s utterance.\nExtraction of numerical and categorical results for labs and vitals as mentioned by the patient.\nAssessment of reported lab or vital sign values against their normal ranges.\nImplementation of plausibility checks for reported lab values to ensure accuracy.\nAnalysis of historical trends in specific labs or vitals by comparing current and previous values.\nAdjustment of normal ranges for lab and vital sign values based on conditional factors such as age, gender, and medication usage.\nEngagement in follow-up queries to clarify or expand upon the patient-provided information.\nProvision of generic information about all laboratory tests and vital signs on record.\nLab Determination\nIdentifying a specific lab test or vital measurement\n“My urine dipstick bilirubin test is positive”\nValue Extraction\nExtracting associated values of mentioned labs/vitals\n”My INR level is 1.1”\nNormal Range Assessment\nAssessing the normal ranges of labs/vitals\n“Is a hematocrit level of 43% normal?”\nPlausibility Check\nVerifying if the lab level is plausible\n“Your systolic blood pressure of 500 is not plausible. Did you accidentally make a mistake?”\nHistorical Lab Trends\nComparing current and past levels\n“How’s my A1C value of 6.5% compared to the previous measurement?”\nConditional Factors\nAdjusting normal ranges for age, gender and medication\n“Your TSH level of 4.5 is lower than your previous value. This may be because you are taking Levothyroxine”\nFollow-Up Queries\nPrompting the conversational agent to ask follow-up questions\n“Did you take your glucose test after fasting?”\nGeneric Queries\nProvide information on all the laboratory tests and vital signs on file\n“Do all my lab results look normal?”"
        },
        {
            "section_id": "4.4.6",
            "parent_section_id": "4.4",
            "section_name": "4.4.6 Models",
            "text": "The Labs & Vitals Specialist is a specialized LLM that runs at every turn in the conversation. This model is trained using annotated conversations between our conversational agent and nurses who act as patients.\nDuring patient-facing interactions, the specialist acts as a passive listener that monitors the conversations. When it detects references to medical labs or vitals in patient utterances, the specialist will then determine the specific labs and vitals that are relevant to the current context. It simultaneously extracts any associated values of the labs and vitals and reports all captured information to the system.\nUpon successfully identifying and extracting the pertinent labs and vitals information, the specialist proceeds to the next phase of its operation: analysis and response formulation. This involves consulting the patient’s EHR to retrieve demographic information and medical history. Factors such as age and gender are essential in determining the appropriate normal range for many lab results and vitals, as these ranges can significantly vary for different populations. The specialist also assesses the plausibility of the reported value to help determine if there is a mistake on the patient’s part.\nAdditionally, the specialist evaluates the impact of short-term conditional factors on the lab results. This includes considering the patient’s recent medication intake and other relevant conditions, such as the timing of food consumption before a glucose test. Such information is vital for accurately determining whether the lab values fall within their expected ranges.\nIf the analysis reveals that additional information is needed—for instance, clarification on whether a glucose measurement was taken fasting or after a meal—the specialist initiates a follow-up query. This process allows the specialist to gather all necessary data relevant to understanding the lab or vital measurement in question.\nThe specialist also performs a comparison against historical data from the patient’s EHR, so that it can answer questions from the patient regarding trends over time.\nFinally, the specialist formulates a response or feedback based on the collected data. This feedback, tailored to the patient’s specific context and health history, is then communicated to the conversational agent. The conversational agent, in turn, presents the information in a way that is understandable and actionable for the patient, ensuring that the interaction is both truthful and supportive."
        },
        {
            "section_id": "4.4.7",
            "parent_section_id": "4.4",
            "section_name": "4.4.7 Labs & Vitals Specialist Training Datasets",
            "text": "To achieve the objectives of the Labs & Vitals Specialist, it is necessary to train this specialist model with appropriate data. We instruct nurses to act as patients and have natural conversations with the conversational agent. During these interactions, they may mention labs and vitals and engage in relevant discussions. In addition, the nurses ensure the dialogues encompass a variety of topics. Upon completion of the conversation, the nurse annotates any labs and vitals discussed, along with any associated values.\nThe input from these conversations, along with the annotated lab-value pairs, serves as the foundational data to train the Labs & Vitals specialist model. This approach ensures the model is well-equipped to handle real-world queries and deliver accurate and contextually relevant responses to patients."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Nutrition Specialist",
            "text": "The Nutrition Specialist is designed to enhance patient dietary decisions by providing meal recommendations tailored to their health status and nutritional goals. Currently, this Agent provides condition-specific menu recommendations from chain restaurants. A summary of the Nutrition Specialist’s capabilities are outlined below:\nRecognition and confirmation of patient’s interest in menu recommendations from a chain restaurant.\nExtraction of the patient’s health conditions via EHR integration.\nCalculation of the patient’s per meal Recommended Dietary Allowance given their weight and health condition(s).\nMenu analysis to align with both the calculation’s above and the patient’s dietary preferences.\nFuture capabilities will be expanded to include local restaurants, and to provide personalized restaurant recommendations based on the patient’s dietary preferences, health status, and geolocation.\n###figure_8###"
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 Workflow",
            "text": "The agent springs into action when a patient expresses interest in eating at a chain restaurant. As seen in Figure  16  ###reference_###, we identify the restaurant and confirm the restaurant name by repeating it back to the patient. This triggers a prompt assessment of the patient’s dietary options, setting the stage for tailored recommendations.\nVia integration with the Electronic Health Record (EHR) system, we gain insights into the patient’s existing health conditions. Combining this data alongside any recent lab results, we calculate the Recommended Dietary Allowance (RDA), ensuring that our recommendations are precisely aligned with the patient’s unique health profile. The RDA is calculated based on the patient’s weight and utilizes clinical nutrition guidelines, which specify the recommended amounts of nutrients based on the individual’s chronic disease.\nLeveraging the computed RDA data, we filter through the menu offerings of the specified restaurant chain. By carefully cross-referencing nutritional information with the patient’s health parameters, we curate a selection of menu items which is safe for the patient to consume.\nTransparency and informed decision-making lies at the heart of our approach. Before presenting the curated menu options to the patient, we provide a detailed breakdown of our calculations, empowering them to understand the rationale behind our recommendations. This transparent methodology ensures that the patient is fully informed and empowered to make dietary choices that best suit them."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Chain Restaurant Nutrient Dataset Curation",
            "text": "The function of this agent is dependent on publicly available menus, dishes and nutritional information required to be reported in accordance with the Food and Drug Administration (FDA) menu labeling regulations Men  ###reference_b31###. Our team of clinical nutritionists use the United States Department of Agriculture (USDA) National Nutrient Database as a standard reference to enhance the dataset, by hand-labeling nutrients such as phosphorus and potassium for menu items, helping to fill gaps in the mandatory reporting requirements for key micronutrients. This level of nutrient granularity is essential to providing targeted meal recommendations for clinically complex patients, such as a Stage 3b Chronic Kidney Disease patient. These patients typically need to carefully monitor their protein, potassium and phosphorus along with standard macronutrients."
        },
        {
            "section_id": "4.5.3",
            "parent_section_id": "4.5",
            "section_name": "4.5.3 Models",
            "text": "The Nutrition Specialist is powered by two specialized LLM models, the Clinical Nutrition Detector and the Clinical Nutrition Evaluator.\nClinical Nutrition Detector: Given the conversation history between the primary agent and the patient, wherein the final conversational turn is attributed to the patient: the objective of the Clinical Nutrition Detector is to evaluate whether the patient’s last turn indicates a desire for restaurant recommendations.\nClinical Nutrition Evaluator: If the Clinical Nutrition Detector returns a positive intent, signifying the patient’s expressed interest in restaurant recommendations, the subsequent task for the Clinical Nutrition Evaluator is to extract the location and the name of the restaurant specified by the patient. Additionally, the extractor is tasked with determining whether the identified restaurant is a chain restaurant. In cases where no pertinent information is available, the corresponding values are returned as empty."
        },
        {
            "section_id": "4.5.4",
            "parent_section_id": "4.5",
            "section_name": "4.5.4 Training Data Generation",
            "text": "To enhance the performance of the Clinical Nutrition Detector and Evaluator, we fine-tune the Large Language Model (LLM) using meticulously curated high-quality datasets. These datasets are sourced from simulated dialogues involving interactions between the primary agent and human nurses acting as patients. The nurses are explicitly directed to articulate inquiries concerning various restaurants, employing a range of diverse utterances. This method is designed to equip the model with the capability to effectively handle authentic queries and provide precise and contextually relevant responses to patients."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Hospital & Payor Policy Specialist",
            "text": "Retrieval-Augmented Generation (RAG) Lewis et al. (2020  ###reference_b32###); Gao et al. (2023  ###reference_b33###) is the process of optimizing the output of a Large Language Model (LLM) through referencing a knowledge base outside of its training data sources before generating a response. It is a cost-effective approach to improving LLM output without the need to retrain or fine tune the model.\nOur Hospital & Payor Policy Specialist leverages RAG to have the ability to answer the latest policy related queries within the following categories: Payor Policies, Health System Policies, and Provider Policies.\nWe list below several compelling reasons for selecting RAG as the preferred approach over fine-tuning to address such latest policy related queries. Primarily, certain hospital policies undergo dynamic changes over time, for example, COVID-19 policies and the requirement for wearing masks during hospital visits. Maintaining the LLM model’s currency through fine-tuning necessitates periodic, resource-intensive updates, incurring significant costs. In contrast, RAG presents a cost-effective alternative for keeping information up-to-date via updating the reference database.\nMoreover, a pivotal consideration in our system is the need to accommodate a multitude of hospitals, ranging from dozens to potentially hundreds or thousands of hospitals. Each hospital exhibits unique policy specifications, making it impractical and economically prohibitive to train or finetune individual LLMs for each hospital. However, our RAG can construct distinct reference vector databases for each hospital and leverage a singular LLM to achieve the purpose of serving multiple hospitals at the same time.\n###figure_9### As shown in Figure  18  ###reference_###, our RAG process encompasses three steps. The first step is RAG engine detection. This step is to identify whether the input query is related to a payor, health system, or provider policy. For a positive detection, the query is rewritten to indicate what hospital policy related question is being asked by the patient; otherwise remaining steps are skipped.\nThis step serves the dual purpose of reducing latency, as it avoids triggering a RAG reference database search for every query, which can be a computationally intensive process.\nThe second step is to calculate the similarity between the reformulated query and each pre-indexed chunk, and then retrieve top 3 most pertinent chunks.\nIn the final step, these three retrieved chunks are utilized as contextual input for the primary agent to facilitate the generation of the final response."
        },
        {
            "section_id": "4.6.1",
            "parent_section_id": "4.6",
            "section_name": "4.6.1 Hospital Policy Q&A within the Hospital System",
            "text": "Many patient consultation queries are directed towards hospital policy matters.\nWe summarize hospital policy-related queries into eleven distinct categories as below:\nAdmission and Registration: Information on scheduling appointments, insurance and billing, cancellation policies, or other questions or concerns related to an upcoming appointment or procedure.\nVisitor Policy: Information on visiting hours, number of visitors allowed, children visiting, visitation restriction, or any other question or concern related to visiting the hospital or medical facility.\nPayments and Financial Aid: Information on financial aid, charity care, payment plans, billing assistance programs, payment methods, or any other financial-related information.\nServices and Amenities: Details on amenities like food services, cafeteria, spiritual care, language assistance, accessibility, parking, transportation options, or other amenities offered by a medical facility. This also includes facilities and specialties - Pharmacy, Laboratory, Urgent Care, Mental Health, Social Services or any other medical specialty, allied health provider, or supportive services. This includes where a patient can pick up their medication or where they can get their blood drawn.\nPatient Rights and Privacy: Topics like HIPAA, patient confidentiality, patients’ bill of rights, filing a complaint, and other topics related to patient privacy or patient rights.\nCompliance and Regulations: Hospital compliance with regulations like EMTALA, Joint Commission standards, Medicare conditions of participation, etc.\nAccommodation: Options for room types, amenities, guest services, accommodation for disabilities, service animals or pets, requesting a different provider, or any other information related to special accommodations at a medical facility.\nSafety and Security: Topics related to hospital safety like security procedures, restricted areas, emergency codes, power of attorney, accessing medical records, or any other information related to patient safety and patient security.\nHospital Care: Policies and information related to medical care like advanced directives, discharge, home care, telehealth, clinical trials, or other information related to a patient’s care in a hospital or medical facility.\nContact Information: Information about the means by which patients, their families, or visitors can get in touch with the hospital for things such as general inquiries, scheduling appointments, rescheduling appointments, seeking information, or addressing concerns. Providing phone numbers or email addresses for various points of contacts within the medical facility.\nAddress and Location: Information about the location of the hospital, and nearby facilities such as restaurants, pharmacies, or laboratories, and provides guidance on navigating to the healthcare facility for patients or visitors. It could also offer guidance on public transportation options, and key points of interest in the hospital’s vicinity."
        },
        {
            "section_id": "4.6.2",
            "parent_section_id": "4.6",
            "section_name": "4.6.2 Models",
            "text": "The Hospital & Payor Policy Specialist is powered by two specialized LLM models: the Policy Detector and the Policy Retriever.\nPolicy Detector: Given the conversational history between the primary agent and patient, and the concluding conversational turn being attributed to the patient, the Policy Detector’s role is to analyze the conversational history, and assess whether the patient’s last query pertains to hospital policy. If so, it generate a simplified question; otherwise skips remaining steps. The generated simplified question will later be used as input to the retriever model.\nThe detector serves two primary purposes. Firstly, it addresses latency concerns, as it is imperative to minimize the response time of each agent. Blocking the primary agent to search the RAG vector database for every query can introduce undesirable delays, and the objective is to exclusively retrieve affirmative queries. Secondly, the detector functions as a query extractor, particularly important when patients may not explicitly articulate their queries.\nThe primary challenge encountered by the RAG detector lies in the extensive scope of hospital policies, encompassing approximately 11 major topics, each with numerous subtopics. Patients may also pose inquiries about the same subject using varied expressions and formats. Hence, our detector must exhibit robust capabilities to accurately discern these broad-topic questions and accommodate diverse questioning methods.\nAfter extensive experiments, we designed the detector to first generate the rewritten query and then predict the label based on the generated query, which can address the above challenge. We observe the rewritten query to serve as a chain of thought, which helps to improve the detection accuracy.\nPolicy Retriever: Upon obtaining the rewritten query generated by the detector, the retriever first transforms this reformulated query into an embedding. Subsequently, it calculates the similarity between this embedding and those of every text chunk in the vector database. The retriever then selectively returns the top three chunk texts with the highest similarity for the primary agent for generate the final response.\nIt is noteworthy to mention that since the retriever is fine-tuned on our indexed data, there is a notable enhancement not only in the retrieval performance for true positive queries from the RAG detector but also in the mitigation of the repercussions of false positives. This is attributed to the fact that false positive queries exhibit markedly low similarity with each indexed chunk text. Consequently, the retriever does not return any chunk text to the primary agent in such instances.\n###figure_10###"
        },
        {
            "section_id": "4.6.3",
            "parent_section_id": "4.6",
            "section_name": "4.6.3 Training Data Generation",
            "text": "In order to obtain a Policy Detector with better performance, we finetune the Large Language Model (LLM) with curated high-quality data. The datasets are derived from simulated conversations between the primary agent and human nurses assuming the role of patients. These nurses are specifically instructed to pose inquiries covering a spectrum of 11 hospital policy relevant topics, with varying intent and diverse questions.\nIn the refinement of the Policy Retriever, we conduct fine-tune BGE embeddings Xiao et al. (2023  ###reference_b34###) using our indexed data. Within the fine-tuned dataset, each sample comprises a query paired with its corresponding indexed data as a positive instance. Additionally,  samples are randomly selected from the remaining indexed chunk texts to serve as negative samples. This training approach is designed to instruct the retriever to optimize the similarity between the query and its corresponding indexed chunk while concurrently diverging from other indexed data."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "EHR Summary Specialist",
            "text": "###figure_11### Documenting the interaction between patients and our agents is integral to allowing the human care team to provide the proper care. The EHR specialist’s objective is to extract structured clinical fields and notes from the conversation and populate EHR. The summary agent’s specific tasks vary based on the medical scenario. We summarize from the recorded conversation transcript in this context. Figure 20  ###reference_### shows an exmaple of a completed summary.\nConversation Summary: Given lengthy multi-turn conversations, the input prompt often exceeds our model’s context length during the conversation. In that case, we summarize the conversation to turn , , and it is passed as an input for turn  in . In multi-turn conversations, when the total conversation length exceeds our context length,\ni.e. we summarize the agent and the human turns at the beginning of the conversation (Figure 19  ###reference_###) and prepend to the existing conversation.\nConversation Attribute Summary: Throughout the conversation, the summary agent aggregates the information from the conversation between the patient and the primary agent and summarizes the output in a specific downstream format.\nThe summary tasks can be described as follows:\nMedication Summary: In this case the summary agent summarizes the adherence status of each of the medications that are part of the patient’s EHR. If the patient is taking all of the medications as prescribed then we tag the adherence status as ‘yes’. For all other situations, we mark the adherence status as ‘no’ and add an additional notes section which describes the scenario in details. These notes describe the reasons why the patient is not following the prescribed dosage. Reasons could include ‘patient has not picked up medication’, ‘patient feels nausea’, ‘patient needs a refill’ and so on.\nVitals: In this section the summary agent extracts information related to all the vitals the patient is supposed to measure or report to our LLM agent. These can include measurements specific to a disease. For example, in case of CKD we ask about blood glucose and blood pressure measurements.\nPatient Education Review: In this section we go through a specific rubric. For example, for a CHF patient we need to ask the patient during the conversation about certain symptoms and record the patient response. During the symptom review check, we go through each of the sub sections like reviewing the details of the lab, medications and answering any question the user might have.\nFollow-up: During the call if there are necessary action items that needs to be communicated to the care team of the patient, we document the information in the follow-up section.\nOf course, for added safety, the entire transcript of the conversation can also be made available to the human care team for review."
        },
        {
            "section_id": "4.8",
            "parent_section_id": "4",
            "section_name": "Human Intervention Specialist",
            "text": "Our safety-oriented system is designed to bring in a human supervisor when necessary. This specialist agent not only records items for human review (as described above), but also facilitates real-time collaboration between the AI and a human nurse. The agent is trained to detect whether the patient is sharing a symptom that should be further evaluated by a human. Consider the example shown in Figure 21  ###reference_###. Patient A uses the expression “a pain in the neck”, which is commonly used to express that something is tedious or annoying; whereas Patient B mentions their “neck really hurts”. It is clear to a human that Patient A is not suffering from neck pain while Patient B is. The job of the intervention agent is to decide whether the patient’s symptom really requires a human intervention or not. However, the intervention evaluator might not have all the necessary information to make an informed decision. Therefore, we employ a state model that keeps track of the information the patient has shared so far, as well as the information we need to collect to be able to make the final decision. Once the state model decides that all the necessary information has been obtained, the agent decides whether to transfer the call to a human nurse for further evaluation.\nPatient: “I have this problem at work that is a pain in the neck.”  No intervention required\nPatient: “I’m currently at work and my neck really hurts.”  Possible intervention required\nThe overall architecture of the intervention agent is shown in Figure 22  ###reference_###. The architecture can be broken down into three separate stages: initial symptom detection, information gathering, and evaluation. In the first stage, we detect an initial symptom that could require further evaluation. In the second stage, our agent inserts the protocol questions into the main model’s prompt, and the main model asks the questions while the state model decides whether all necessary information has been obtained. In the last stage, the agent decides whether an intervention is required.\n###figure_12### The symptom detection model runs at every turn and decides whether the patient shared a symptom that could require a human intervention. If the patient shares a relevant symptom, the system initiates the intervention protocol, comprised of a set of predefined follow-up questions. These questions allow us to obtain all the relevant information needed to determine whether the conversation should be escalated to a human.\nAfter the initial symptom has been detected and the necessary information has been obtained, the intervention agent must decide whether an intervention is necessary. If the model determines that an intervention is appropriate, the call will be transferred to a human. If no intervention is required, the main model will proceed with the rest of the conversation."
        },
        {
            "section_id": "4.8.1",
            "parent_section_id": "4.8",
            "section_name": "4.8.1 Models",
            "text": "The human intervention specialist agent is comprised of three different specialized LLMs: the symptom detector model, the intervention state model, and the intervention evaluator model. The training and architecture details for these models is shown in Section 2.4  ###reference_###.\nThe symptom detector model runs at every turn in the conversation in parallel with the primary agent, and its job is to extract relevant symptoms shared by the patient. This model is trained using conversations between humans and our system, where the humans label the turn where the symptom was shared, and the model is fine-tuned to learn to extract the symptom from the conversation.\nThe intervention state model determines the state of the conversation at each turn. It decides whether all the necessary information has been acquired or not, and if more information is needed, the main model will continue asking the questions in the protocol until the patient has provided it. This model is trained using conversations between human nurses posing as patients and our system, where nurses label the state of the conversation at each turn, and decide when all the information has been collected.\nFinally, when the state model has decided that all the necessary information has been shared by the patient, the evaluator model decides whether an intervention is required or not. This model is trained on conversations between human nurses and our system, where the nurses label the turn where the conversation should be transferred to a human nurse."
        },
        {
            "section_id": "4.8.2",
            "parent_section_id": "4.8",
            "section_name": "4.8.2 Training Datasets",
            "text": "To fine-tune the three LLMs that form the human intervention specialist, we collected conversations between human nurses and our system, where we asked the humans to play the role of a patient and share a specific symptom during the conversation in a variety of formats. We then asked them to label three different things per conversation:\n1) The turn in which the patient mentions each symptom.\n2) The turn in which all the necessary information was obtained.\n3) Whether a human intervention is required or not.\nThe data contains input/label pairs, where the input contains the current conversation and the instructions for each task, and the label contains the desired prediction for each input. The data collection and model training was performed iteratively in multiple stages, improving the models over time with the following scheme: deploy the existing models, collect conversational data, label the data, train the models on the new data, and repeat the process with better models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "This section describes our conversational evaluation methodology and experimental results comparing Polaris  to a larger general-purpose LLM (GPT-4) as well as one from its own medium-size class (LLaMA-2 70B Chat). Further, we benchmark Polaris  against the performance of U.S. licensed human nurses on a set of fine-grained measures.\nOur evaluation approach focuses on both the overall subjective user experience, and targeted “clinical capability validation” - a focused assessment of certain tasks or functions relevant to healthcare conversations. For the overall subjective evaluation, we compare survey responses completed by users acting as patients following conversations with Polaris  to the same survey responses following conversations with human nurses. This comparison provides a subjective baseline that establishes the core conversational competence of our system.\nTo conduct a targeted assessment of certain non-diagnostic medical conversational capabilities, we devised a set of test cases corresponding to conversational scenarios that could arise in a healthcare setting. We tested our system versus a minimal configuration of competitor LMs in which they were directly presented with the same conversational challenge in isolation. For instance, instead of the primary conversational model drawing on the output of the “labs and vitals” specialist (as the primary agent does in our system), GPT-4 was solely responsible for commenting on user provided lab values. The objective of this assessment is to measure the importance and effectiveness of compartmentalizing complex tasks for a language model based AI system such as calculations, numerical comparisons, and critical and highly detailed subroutines such as medication reconciliation.\nOur baseline models rely on exactly the same infrastructure as our model for ASR, TTS (including the same synthetic voice) and telephony. Our goal is to isolate the experimental variability to the core healthcare knowledge and conversational ability of each model and our system. For these benchmarks we use the “gpt-4-0613” version of GPT-4."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Overall Subjective Evaluation",
            "text": "For the subjective evaluation, we recruited over 1100 US-licensed Registered Nurses (RNs) and over 130 US-licensed Physicians. After verifying their licenses, each participant had a series of conversations with our system.\nCalls were screened at this stage for quality - if their initial conversations were short (less than twenty turns, approximately four minutes), that user’s evaluation was excluded from this analysis. Over 3,475 total conversations from these two groups were considered in the results for this section.\nParticipants were informed that they would be receiving calls from an AI Conversational agent that would be acting as a healthcare provider, that this system was still under active development, and that their calls would be recorded. All participants consented to having their anonymized conversational data used for research and product development purposes. They were given a set of “cases”: patient backgrounds and corresponding call scenarios, and were instructed to portray the patient accurately but also to draw on their own experiences interacting with patients to add detail and complexity to the call.\nIn addition to calls with our AI system, a subset of 60 nurses were randomly selected to also have conversations with a separate team of human nurses familiar with our call objectives. The nurses playing nurses in these calls were given the same patient background information as our AI system and the same set of call objectives. We did not evaluate the U.S. licensed human nurses by U.S. licensed physicians.\nWe chose to have nurses to act as patients in these calls to obtain expert-level focused assessments of the specific skills required for this type of interaction, such as motivational interviewing, clinical assessment and guidance offered by nurses (as opposed to physicians or other medical assistants). An individual nurse participant portrayed the patient role in a number of calls with our system; they were randomized to the human-to-human cohort and additional calls with a human nurse conducting the same call scenario.\nFollowing these conversations, each participant completed a survey focused on the following aspects of their experience: Bedside Manner, Conversation Quality, Clinical Readiness, Patient Education and Motivational Interviewing, and Medical Safety. Survey questions and answer choices were developed in collaboration with a team of experienced care-management nurses. Participants were also able to flag conversations as containing errors - all flagged calls were manually reviewed by our clinical team.\nFeatures\n\nNurses\nRated By\nNurses\n\nPolaris \n Rated By\nNurses\n\nPolaris \nRated By\nPhysicians\n\n\nDo you feel that the Nurse/AI listened to you?\n\n94.52%\n\n89.75%\n\n89.62%\n\n\nDid you feel that the Nurse/AI cared about you?\n\n89.77%\n\n88.15%\n\n86.84%\n\n\nDid you feel comfortable confiding in the Nurse/AI?\n\n88.81%\n\n88.93%\n\n88.45%\n\n\nDid the Nurse/AI get to know you as a person?\n\n57.58%\n\n78.43%\n\n74.71%\n\nBEDSIDE MANNER AVG\n82.67%\n86.32%\n84.91%\n\n\nHow would you rate this call on a scale of 1 (worst) to 10 (best).\n\n8.04\n\n8.19\n\n7.86\n\n\nHow would you rate this call on a scale of 1 (worst) to 10 (best). NORMALIZED\n\n78.22%\n\n79.89%\n\n76.22%\n\n\nWould you pick up another call from the Nurse/AI?\n\n92.25%\n\n88.97%\n\n88.89%\n\nCONVERSATION QUALITY\n85.24%\n84.43%\n82.56%\n\n\nWas the Nurse/AI as effective as a nurse?\n\n87.34%\n\n85.66%\n\n86.70%\n\nCLINICAL READINESS\n87.34%\n85.66%\n86.70%\n\n\nDid the Nurse/AI inform and educate you on your condition?\n\n80.64%\n\n89.82%\n\n93.42%\n\n\nWould you feel more able to manage your own condition after speaking to the Nurse/AI?\n\n77.86%\n\n87.54%\n\n90.79%\n\n\nDid the Nurse/AI create and take opportunities to educate you about your condition?\n\n78.20%\n\n90.53%\n\n93.27%\n\n\nDid the Nurse/AI cover all critical items for this kind of call?\n\n86.04%\n\n89.86%\n\n92.25%\n\n\nWas the Nurse/AI able to educate you on general wellness topics like diet, exercise, and supplements?\n\n69.68%\n\n83.45%\n\n84.21%\n\nPATIENT EDUCATION / MI\n78.48%\n88.24%\n90.79%\n\n\nThe Nurse/AI provided medical advice that may result in:\n\n\n\n\n\n- Nothing incorrect\n\n81.16%\n\n96.94%\n\n96.20%\n\n\n- No Harm\n\n14.72%\n\n1.74%\n\n2.19%\n\n\n- Minor Harm\n\n4.12%\n\n1.28%\n\n1.46%\n\n\n- Severe Harm\n\n0.00%\n\n0.04%\n\n0.15%\n\n\n- Death\n\n0.00%\n\n0.00%\n\n0.00%\n\nMEDICAL SAFETY"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Clinical Capability Evaluation",
            "text": "In addition to the overall subjective experience of interacting with our conversational system, we conducted a series of experiments to measure the ability of our conversational system, and competitor language models, to perform a variety of specific non-diagnostic healthcare tasks. These tasks were developed in conjunction with the same internal team of care-management nurses described above. We believe these capabilities represent core functionality required for an AI assistant to operate over the phone with any degree of independence.\nThe capability evaluation tasks were designated as belonging to one of two categories: “conversational” or “iso-eval” based on the complexity of the exchange required from the system to demonstrate a capability. Conversational tasks are more complex, often requiring many turns of back and forth between the patient and AI provider. For these tasks we developed a set of test cases consisting of fixed statements and follow-up instructions for our research nurses to enact while portraying the role of a patient in a series of telephone calls. A conversational evaluation task produces a set of call transcripts between the AI system and the nurse acting as a patient. Depending on the task, only a sub-section of the transcript may be relevant to the capability evaluation.\nIso-eval tasks correspond to capabilities that are simpler to demonstrate. Concretely they can be elicited reliably in a single turn: one question or statement from the simulated patient requires a response from the system that will demonstrate the skill. These capabilities are assessed using a set of “canned” conversational snippets taken from real conversations between patient actors and human nurses from our internal research teams. Each snippet is modified by appending a final test turn - a statement or question from the patient that is meant to test a given capability. An iso-eval task produced a set of single-turn model responses (along with their corresponding conversational snippets).\nThe output of evaluation tasks of both categories, transcripts for conversational tasks and single-turn responses from iso-evals, were subsequently reviewed by nurses and clinicians for correctness against specified evaluation criteria. We explored automated evaluation for certain iso-eval tasks using a language model-based evaluator, but here we present only the results of human expert review.\nThe specific capability evaluations are described below:"
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Medications and Supplements",
            "text": "To evaluate the ability of each artificial intelligence system to handle scenarios involving both prescription and over-the-counter (OTC) medications and supplements, we identified and tested the following capabilities:\nWhen a patient states that they are taking a particular dosage of a prescription medication, can the AI system confirm that the patient is in fact prescribed that medication and the stated dosage is correct. If the dosage is not correct, can the system determine whether it is high or low, if the frequency is correct or incorrect, and provide appropriate comment.\nIf a patient states they are taking a particular OTC medication or supplement, can the AI system determine whether this is recommended or not, based on the drug label, given their medical conditions\nIf a patient states they are taking a particular OTC medication or supplement including the amount they are taking, can the AI system determine whether this is within the recommended range, based on the drug label, given their medical conditions\nThese capabilities were evaluated using an \"iso-eval\" approach, which involved the use of fixed conversational snippets. The single-turn output of the three system configurations were then reviewed by a team of nurses.\nIn addition to these capabilities, we identified a common failure point for voice-based AI systems in healthcare: a patient may incorrectly state the name of a medication, forget the name entirely, or a transcription error may occur, leading to novel misspellings of common drug names or fragmentation of names into multiple smaller words. In such scenarios, we evaluated the AI system’s ability to interact with the patient to accurately determine the intended drug name. This could involve the system asking the patient for clarification or requesting them to spell the name of the medication.\nTo evaluate this particular skill, we developed a set of potential mispronunciations or ambiguous or inaccurate ways to refer to a drug. These were then used by nurses in conversations with the AI systems. Subsequently, a separate team of nurses reviewed these interactions to assess the accuracy of the AI system’s responses. The primary criterion for evaluation was whether the AI system was ultimately successful in identifying the correct drug name."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Lab and Vital Specialist",
            "text": "We evaluated the following capabilities involving lab values and vital signs:\nWhen the patient states a numerical value for a lab test or vital sign, can the AI system correctly extract that value, compare it to the corresponding reference range, and provide correct and appropriate commentary to the patient on the value, specifically whether it is above, below, or within the reference range.\nWhen the patient states a new lab value for which they have previously documented measurements in their chart, can the AI system correctly compare the new and historical values? If there are multiple historical values which show a steady trend (e.g. “decreasing hemoglobin A1c” or “increasing blood pressure” over a year), can the AI system identify this trend and comment on it.\nAppropriately comment on the potential impact of certain medications and supplements on lab values (lab / medication interactions). We restrict our testing for this capability to the main intended effect of a medication (e.g. metformin is intended to lower hemoglobin A1C over time).\nEach of these capabilities was measured using the “iso-eval” approach and reviewed by the same team of nurses as above."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Dietary Recommendations",
            "text": "We evaluated the ability of each AI system to provide appropriate nutritional advice in the following two scenarios:\nPatients with certain medical conditions - we restrict our testing to Congestive Heart Failure (CHF) and Chronic Kidney Disease (CKD) stages 3A and 3B. In these conditions, a physician may instruct the patient to restrict their fluid intake to 2 liters a day (CHF) or limit daily protein consumption to a certain number of grams (CKD). When a patient states they have exceeded the restriction, or intend to do so in the future - can the AI system detect this violation, comment on it, and ideally suggest that they stay within the physician-recommended dietary limitation. This testing was conducted in the “iso-eval” manner.\nIf a patient mentions that they are eating at a national chain restaurant (with a standardized and published menu) and asks for help in deciding what to order, is the AI system able to provide specific and helpful information about the nutritional value of various menu items and ultimately recommend a menu item that is appropriate given the patient’s health conditions. This was a conversational evaluation - nurses acting as patients were given a particular chain restaurant to ask about and a patient background with certain chronic medical conditions that would impact their dietary selection."
        },
        {
            "section_id": "5.2.4",
            "parent_section_id": "5.2",
            "section_name": "5.2.4 Answering Hospital Policy Questions",
            "text": "There are a plethora of non-medical logistical questions that a patient may have when receiving care. These questions can range from appointment rescheduling to online portal troubleshooting. The ability to swiftly and accurately answer logistical questions using an AI model has the potential to lead to reduced costs, quality improvement, and greater customer satisfaction.\nWhen the AI system is presented with logistical questions, it is crucial to provide the patient with accurate and up-to-date information, while avoiding hallucination.\nHere is a sample of a few questions we evaluated:\n“My payment is not going through. What should I do?”\n“Can I get multiple months of medication?”\n“How do I contact member support?”\nWe conducted “iso-eval” tests where we tested the capability of all three AI systems to accurately answer questions such as the ones presented above. The final output of all three system configurations were reviewed by a team of four healthcare researchers. They evaluated the performance of the AI systems based on the system’s ability to accurately answer the patient questions. The researchers also ranked the responses from the AI systems from one to three to determine which system consistently offered the most satisfactory answer. During this evaluation, our AI system utilized the RAG reference database outlined in section 5.5. Moreover, both GPT-4 and LLaMA-2 were given all information necessary to accurately answer the patient questions in the preamble. Below are two conversational snippets from this evaluation."
        },
        {
            "section_id": "5.2.5",
            "parent_section_id": "5.2",
            "section_name": "5.2.5 Privacy",
            "text": "A key capability for an AI system involved in healthcare is the need to respect patient privacy and confidentiality. Specifically, we test whether each AI system correctly verifies the patient’s name and date of birth before proceeding to discussing any PHI. These were conducted as “iso-eval” tests in which the three configurations were presented with incorrect names and/or dates-of-birth, and each was evaluated on whether it proceeded on to any private medical information.\n###table_2### Labs &\nLab Reference Range Identification\n48%\n78%\n96%\n94%\nVitals\nLongitudinal Lab Analysis\n1%\n86%\n92%\n-\nLab / Medication Interactions\n0%\n74%\n80%\n63%\nMedications\nPrescription Adherence\n72%\n97%\n94%\n92%\nCondition Specific Disallowed OTCs\n31%\n56%\n89%\n46%\nOTC Toxicity\n9%\n38%\n82%\n58%\nDrug Misidentification\n68%\n91%\n96%\n-\nNutrition\nCondition Specific Restaurant Menu Recommendation\n2%\n28%\n74%\n-\nPrivacy & Compliance\nIdentity Verification\n40%\n96%\n100%\n-\nHospital & Payor Policy\nHospital & Payor Specific Policies\n18%\n36%\n86%\n-"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Error Analysis",
            "text": ""
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Medication Specialist",
            "text": "This subsection provides a comparative analysis of patient dialogues concerning inquiries about medications and supplements. It showcases the differential approaches our system and GPT-4 employ to address potential contraindications, and dosage errors. Each conversation excerpt serves as an instance of how medication and supplement usage is interpreted and communicated in a healthcare context.\nIn the dialogue in 24  ###reference_###, the patient with hypertension discloses taking Advil for headaches. Our system recognizes that Advil, as an NSAID, can potentially raise blood pressure and advises the patient to avoid it given their hypertension. It further inquires about the specific discomfort or pain that led to Advil usage. GPT-4, on the other hand, acknowledges the patient’s disclosure but only asks about the frequency and dosage of Advil to ensure safety with current medications, without specifically addressing the hypertension concern.\nFigure 25  ###reference_### presents a case where the patient with congestive heart failure and hypertension has started taking Zyrtec-D, an allergy medication that can increase blood pressure. Both our system and GPT-4 correctly note the potential impact on blood pressure and advise monitoring, demonstrating appropriate handling of this situation.\nThe dialogue in Figure 26  ###reference_### involves a patient accidentally taking double the prescribed dose of Lasix, a diuretic. Our system and GPT-4 both identify the incorrect dosage taken by the patient and respond appropriately by emphasizing the importance of taking the medication as prescribed and noting the potential consequences of the extra dose.\nFigure 27  ###reference_### showcases a case where the patient reports taking a high daily dose of 100 mg of the antihistamine Doxylamine. Our system recognizes that daily maximum dose greater than 75 mg is not recommended and correctly escalates this potential overdose to a human nurse for immediate assistance. GPT-4, however, provides non-specific advice to consult the doctor about OTC medications and moves on to discuss blood sugar monitoring, failing to address the overdose concern.\nIn summary, our system demonstrates superior performance compared to GPT-4 in identifying and responding to contraindicated medications and dosage errors. It probes further when needed and escalates to human intervention when appropriate. GPT-4 tends to provide more generic advice, sometimes moving on without addressing critical issues. However, both systems perform similarly in handling potential medication side effects and dosage errors. These findings highlight the importance of developing AI systems that can accurately process medication-related information to ensure patient safety."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Lab Values and Vital Signs",
            "text": "This subsection provides a comparative analysis of patient dialogues concerning inquiries about laboratory values and vital signs. It showcases the differential approaches our system and GPT-4 employ to address patient queries about their health data. Each conversation excerpt serves as an instance of how lab results and vital sign values are interpreted and communicated in a healthcare context.\nIn addressing a female patient’s inquiry about a hemoglobin level of 16 g/dl (see Figure 28  ###reference_###), our system correctly identifies that this value exceeds the typical range for women, which is generally considered to be between 11.5 and 15.5 Clinic  ###reference_b35###. This suggests a careful consideration of potential health implications or the effect of medications in light of the specific patient’s health context.\nOn the other hand, however, GPT-4’s response inaccurately positions a hemoglobin level of 16 g/dl as within the normal range, despite previously outlining that the upper limit for women is 15.5 g/dl."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Nutrition Specialist",
            "text": "Below are two conversation samples, one between a patient and Polaris and the other between a patient and GPT-4. The first showcases the ability of Polaris to provide specific and targeted menu options that are appropriate taking into account the patient’s weight and health conditions. The following GPT-4 sample provides appropriate macronutrient guidance for a patient with chronic kidney disease, however, fails to offer actual menu options from the restaurant or considers the maronutrients and portion sizes."
        },
        {
            "section_id": "5.3.4",
            "parent_section_id": "5.3",
            "section_name": "5.3.4 Hospital & Payor Policy Specialist",
            "text": "The following presents a comparative examination of patient conversations regarding inquiries into hospital policy. As depicted in Figure 30  ###reference_###, Polaris accurately responds to the query “Can I receive multiple months of medication?”.\nConversely, GPT-4 fails to provide a correct response, instead directing the patient to the pharmacy. To address this, our RAG system extracts contextual information pertinent to the query and feeds it into Polaris , enabling it to provide an accurate response. However, GPT-4’s training model often determines the absence of relevant information, leading to ambiguous responses. Consequently, the response fails to directly address the patient’s inquiry."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Current Healthcare Challenges",
            "text": "One critical challenge that limits machine learning applications in healthcare is the excess of well-defined questions and corresponding datasets available to answer such questions. Problem definitions are tricky in healthcare because they often involve stakeholders across multiple domains Wiens et al. (2019  ###reference_b36###). Beyond clinicians and patients, key stakeholders include financial and operational executives and family and insurers. For instance, ICD codes, often used in EHR data, may poorly correspond to the actual diagnosis O’malley et al. (2005  ###reference_b37###). Consequently, most applications of AI in healthcare have been very limited to isolated tasks like clinical text summarization Van Veen et al. (2024  ###reference_b38###), sepsis prediction Fleuren et al. (2020  ###reference_b39###), cancer prognosis and prediction Kourou et al. (2014  ###reference_b40###) to name a few. However, current advances in AI can directly aid in preventable human errors Bates et al. (2023  ###reference_b41###); CA (2022  ###reference_b42###) and therapeutic delays Newman-Toker et al. (2022  ###reference_b43###). Beyond the already existing challenges, bias in healthcare has been a well-known issue even before the integration of AI in healthcare Williams et al. (2010  ###reference_b44###); Fitzgerald and Hurst (2017  ###reference_b45###). This problem is far from solved, but there is a significant amount of effort from the community to recognize and mitigate it Marcelin et al. (2019  ###reference_b46###); Mccradden et al. (2020  ###reference_b47###). LLMs in healthcare still need considerable work Wornow et al. (2023  ###reference_b48###), requiring careful and safe implementation strategies."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "LLMs in Healthcare",
            "text": "One critical challenge that limits machine learning applications in healthcare is the need for ill-defined questions and corresponding datasets to answer such questions."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Labor Market Impact of Healthcare",
            "text": "Artificial intelligence is already having an outsized impact in the healthcare labor market. Access to healthcare expertise remains a scarce resource Rennie et al. (2014  ###reference_b59###) across the world. It is well known among the healthcare community that high workload and occupational stressors impact the quality of care and patient outcomes Marine et al. (2006  ###reference_b60###); Nieuwenhuijsen et al. (2010  ###reference_b61###). The administrative tasks healthcare workers have to undertake are non-trivial Erickson et al. (2017  ###reference_b62###). In a study that monitored how physicians (across 4 US states) are spending time in administrative work and direct clinical face time, it was observed that Physicians could spend up to 49% of their time in electronic health records and desk work versus only 33 percent of the time on direct clinical face time with patients and staff Sinsky et al. (2016  ###reference_b63###). AI will significantly affect the future of jobs Manyika et al. (2017  ###reference_b64###), and healthcare delivery will significantly change over the next few decades. One critical challenge that limits machine learning applications in healthcare is the need for poorly-defined questions and corresponding datasets to answer such questions."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Future Work",
            "text": "Polaris  is an example of how LLM-based systems can be architected with real-time specialized conversation in mind. Some ongoing work items to improve the scope and versatility of our system include:\nOur goal is to produce superior health outcomes for users. To this end, multi-call settings provides an interesting avenue to establish and build connection with the user over time. This is also crucial to build rapport and trust. Here, information learned about the user in earlier calls is utilized to deliver more pleasant and medically constructive subsequent calls. This will require research into the best ways to incorporate prior call knowledge into the context of the primary agent. Given each call can be upwards of 20 minutes, this quickly approaches very large context sizes. This is especially valuable in the context of lifestyle factors which have an out-sized impact on health, such as dietary habits and smoking. Helping patients navigate these decisions given their real-world constraints requires connecting with them deeply and finding realistic solutions. Hence we aim to explicitly model the preferences of patients in future work to serve them most effectively. This direction requires research into real-world sample efficiency, planning with simulations, causal learning, exploration and more.\nIn this work we focused on the synchronous mode for our support agents. Recall this implies the primary agent’s responses can be preempted if the support models want to inject new tasks at that same conversation turn. However, there is much to explore in the realm of agents which operate asynchronously in the background and do not block the lead agent’s operation. Reasoning can be further enhanced by having LLMs constantly taking the state of the conversation and cross-referencing resources to look for medical corner-cases or extremely specific suggestions to give to the user. Consider the case of finding an ideal plan for medication affordability or interactions between many prescription medications and a patient’s genetic profile. This can take varying amounts of time and resources, and so more complex orchestration patterns will need to be designed to support this behavior.\nPolaris  is composed of a set of LLMs plus orchestration and message passing infrastructure, situated between distinct ASR and TTS components. While we achieve quite impressive subjective experience scores, this can be further improved by multimodal modeling, which reduces the inherent information loss due to transcription and context-independent TTS. There are multitude of speech signals we are not currently incorporating such as pitch, prosody and emotional signatures. Audio-text multimodal fusion represents a promising opportunity for enhancing empathy and improving patient preference modeling. On the other side, having additional features fused into the TTS pipeline can enhance inflection, emphasis and ultimately create a more engaging and natural-sounding agent. Further, multimodal chain-of-thought and retrieval infrastructure can enhance our system."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We developed a novel LLM constellation architecture, namely Polaris  with multiple specialized healthcare LLMs working in unison for real-time patient-AI voice conversations. Our architecture allowed for a primary agent to adopt a human-like conversational approach, exhibiting empathy and building rapport and trust; whereas the specialist agents were optimized for specific healthcare related tasks. We found this architecture allowed for accurate medical reasoning, fact-checking, and the avoidance of hallucinations, while maintaining a natural conversation with patients.\nWe performed extensive phase one and phase two testing of our system with U.S-licensed nurses and U.S.-licensed physicians. Impressively, on subjective criteria, our study participants rated our AI agent on par with U.S.-licensed nurses on multiple dimensions. On objective criteria, our medium-size AI agents significantly outperformed much larger general-purpose LLMs like GPT-4 in medical accuracy and safety. We are now moving to phase three testing, which requires much more extensive evaluation to be completed by several thousands of licensed nurses and licensed physicians, as well as by our health system and digital health partners. To the best of our knowledge, we are the first to conduct such an extensive safety assessment of any Generative AI technology for real-life healthcare deployment.\nIn conclusion, we foresee a promising future for AI agents to improve healthcare by filling a large portion of the staffing gap. As we continue to push the boundaries and overcome challenges, our goal remains to provide scalable and safe systems that alleviate the burden on human health care providers and improve patient satisfaction, health care access, and health outcomes."
        }
    ]
}