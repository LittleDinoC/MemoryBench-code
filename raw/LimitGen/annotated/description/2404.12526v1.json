{
    "title": "Adaptive Memory Replay for Continual Learning",
    "abstract": "Foundation Models (FMs) have become the hallmark of modern AI, however, these models are trained on massive data, leading to financially expensive training. Updating FMs as new data becomes available is important, however, can lead to ‘catastrophic forgetting’, where models underperform on tasks related to data sub-populations observed too long ago. ††*Work done during internship at MIT-IBM Watson AI Lab.\nThis continual learning (CL) phenomenon has been extensively studied, but primarily in a setting where only a small amount of past data can be stored. We advocate for the paradigm where memory is abundant, allowing us to keep all previous data, but computational resources are limited. In this setting, traditional replay-based CL approaches are outperformed by a simple baseline which replays past data selected uniformly at random [37], indicating that this setting necessitates a new approach. We address this by introducing a framework of adaptive memory replay for continual learning, where sampling of past data is phrased as a multi-armed bandit problem. We utilize Bolzmann sampling to derive a method which dynamically selects past data for training conditioned on the current task, assuming full data access and emphasizing training efficiency.\nThrough extensive evaluations on both vision and language pre-training tasks, we demonstrate the effectiveness of our approach, which maintains high performance while reducing forgetting by up to  at no training efficiency cost.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The concept of the Foundation Models (FMs) [2  ###reference_b2###] has recently gained popularity\nand became ubiquitous in many downstream applications, including language [51  ###reference_b51###, 4  ###reference_b4###, 33  ###reference_b33###], vision [38  ###reference_b38###, 41  ###reference_b41###, 52  ###reference_b52###], and other application domains - advocating towards the ‘train-once-and-use-everywhere’ paradigm shift in AI/ML. One of the most attractive features of FMs is their ability for Zero-Shot [4  ###reference_b4###] prompting, few-shot In-Context Learning (ICL) [4  ###reference_b4###, 55  ###reference_b55###], and great transferability to any task [33  ###reference_b33###, 52  ###reference_b52###]. This is due to their massive scale pre-training, often on billions [43  ###reference_b43###] or trillions [33  ###reference_b33###, 51  ###reference_b51###] of data points. However, such power comes with high training costs. The pre-training data size is so large that normally each sample is observed only a few times or, as in language training, once (single epoch). Moreover, a common requirement for FMs is to rather frequently undergo ‘Extended Pre-Training’ (EPT)\n- a process of updating the model with new (massive) additional data intended to improve the model’s temporal currency. During EPT the original pre-training data cannot be naively replayed as, given the massive size of both pre-training and EPT data, it would effectively double the EPT cost (naturally assuming a single epoch and 50% replay mix for EPT as is customary in such cases). This would be prohibitive both in terms of the high cost (millions of dollars) as well as non-negligible negative environmental impact (extra heat emission).\nHowever, neglecting past data during EPT is prone to the issue of catastrophic forgetting [21  ###reference_b21###], where models updated with new data tend to underperform on previously seen data. This leads to an important question: how can we adapt large-scale FM models to an ever-evolving world without compromising on performance or efficiency?\n###figure_1### The realm of continual learning offers some insights, but also limitations. While current benchmarks effectively highlight the challenge of catastrophic forgetting by training on non-overlapping data tasks sequentially, they are less applicable to (massive scale) EPT, as they either restrict themselves to limited memory storage (while in practical EPT all data, past and current, is usually available) and do not take into account the training cost of replay. For practical EPT, we argue the cost impact needs to be minimal in the sense that the ‘continual’ EPT needs to have similar cost as ‘naive’ EPT (disregarding old data and catastrophic forgetting issue). This is intuitive, as even the tiny overhead fraction due to replay will be applied as a factor to the training cost (measured in millions of dollars for the large-scale models [33  ###reference_b33###]).\nThis new setting of restricted computation and unlimited storage of past data has recently been explored for continual learning of image classification tasks [37  ###reference_b37###]. The results of that work suggest that a simple baseline which randomly selects from all past data outperforms other replay-based CL methods. This creates a demand for a new approach which can better utilize all of the past data.\nTaking inspiration from prior works [3  ###reference_b3###, 10  ###reference_b10###, 17  ###reference_b17###] that have tried to select memory data intelligently by selecting the most representative samples of each memory replay dataset, we push the boundaries by considering not which past samples are the most representative (which is typically pre-decided before training future tasks), but rather which samples most effectively prevent forgetting conditioned on the current task data (which is decided during the training of future tasks). This notion is based on the intuitive concept that the model retains full access to previously seen data and that the ‘optimal’ replay data may be contingent upon the new data a model encounters during EPT.\nWe specifically propose an approach that dynamically adjusts the proportion of replay samples from each past task based on its propensity to be forgotten given the new task data. In such an adaptive memory replay for continual learning (visualized in Figure 1  ###reference_###), our algorithm efficiently decides on the optimal allocation of memory replay samples among past tasks to minimize overall forgetting, under the vital consideration of how to do this without the requirement for drastic computation. We do this through a combination of bandit estimation and Boltzmann sampling from clusters of old datasets store in memory. We evaluate our replay strategy for both vision and language large-scale pre-training tasks.\nIn particular, we propose and evaluate a zero-cost protocol that includes intelligent selection of both data to replay and reduction in the new EPT data to compensate for the (relatively small) extra cost of the selection algorithm itself.\nIn summary, we make the following contributions:\nWe present an adaptive memory replay for continual learning, a novel scheme inspired by a bandit estimation formulation that assumes full memory access and dynamically adjusts replay samples based on the new data, ensuring reduced forgetting.\nExtensive evaluations demonstrate the efficacy of our method across both vision and language large-scale pre-training tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": "In the past decade, there has been significant progress in continual learning to alleviate catastrophic forgetting. Regularization-based methods [24  ###reference_b24###, 44  ###reference_b44###, 27  ###reference_b27###] modify the model parameters with additional regularization constraints to prevent catastrophic forgetting. They store no data but explore extra regularization terms in the loss function to consolidate previous knowledge. Rehearsal approaches [40  ###reference_b40###, 40  ###reference_b40###, 6  ###reference_b6###] memorize or generate a small fraction of data points for previous tasks and utilizes them to retain the task knowledge. Importantly, what data to retain is decided during the task itself, and subsequently used throughout future tasks. Expansion approaches expand a model’s architecture as new tasks are encountered; these are highly effective for applications where a model growing with tasks is practical  [14  ###reference_b14###, 25  ###reference_b25###, 28  ###reference_b28###, 30  ###reference_b30###, 42  ###reference_b42###]. Our work does not consider these methods because the model parameters grow with the number of tasks, but acknowledge that the contributions could be incorporated into these approaches.\nRecently, prompt-tuning methods such as [53  ###reference_b53###, 54  ###reference_b54###, 48  ###reference_b48###] outperformed rehearsal-based methods without using a replay buffer by learning a small number of insertable model instructions or prompts. Another line of research is the parameter isolation-based approaches [29  ###reference_b29###, 58  ###reference_b58###, 20  ###reference_b20###] which focus on freezing the task-specific parameters and growing new branches for new tasks. [20  ###reference_b20###] propose adapters which add a small number of parameters to the model for training on downstream tasks. Low-Rank Adaptation (LoRA) [22  ###reference_b22###] extends on the above by using low-rank matrix counterparts of the original weights during fine-tuning, and keeps the actual weights frozen to further reduce inference costs.\nContinual Learning in Transformers:\nThe recent Vision Transformer (ViT) [12  ###reference_b12###] has made a pure Transformer architecture scalable for large scale image classification and several works [26  ###reference_b26###, 13  ###reference_b13###, 15  ###reference_b15###] have successfully applied the Transformers architecture for continual learning. In [26  ###reference_b26###], for each new task, the model is copied and fixed to be used as the teacher model in the distillation phase. In [13  ###reference_b13###], a unified model is learned by building upon a new architecture which dynamically expands the tokens processed by the last layer to mitigate forgetting. For each task, they learn a new task specific token per head using task-attention based decoder blocks. Recently, [15  ###reference_b15###] proposes a method based on pre-trained Transformers while maintaining strict control of the memory usage and reaching state-of-the-art predictive performance. However the above methods either train a new transformer or need to fine-tune large pre-trained transformer models which requires significant compute in contrast to our objective of achieving optimal performance with limited compute.\nCoreset Replay for Continual Learning:\nRehearsal-based methods use a memory buffer to store selective samples of the previous tasks. These samples are then replayed with new task data to prevent catastrophic forgetting. A notable rehearsal-based method, Experience Replay (ER) from memory [40  ###reference_b40###] interleaves the previous task samples with the current task data for optimizing the network parameters. E2E [10  ###reference_b10###] deploys a herding algorithm to bolster coreset representativeness of the past task training data. ERT [7  ###reference_b7###] further extends ER by a balanced sampling strategy and bias control. Selective replay [17  ###reference_b17###] proposes task-based rehearsal strategies for sample selection based on class-margin boundary, minimum confidence etc. DER++ [6  ###reference_b6###] mixes rehearsal with a distillation loss for preventing catastrophic forgetting. HAL [11  ###reference_b11###] integrates ER with an additional objective of keeping the predictions on anchor points of past tasks intact. MIR [39  ###reference_b39###], GDumb [36  ###reference_b36###] and ASER [46  ###reference_b46###] store samples based on parameter updates, order of sample arrival and memory-based class boundaries respectively. Finally, ACE [3  ###reference_b3###] explores various alternative population strategies to select coreset replay data. While the above methods consider the representativeness of data stored in the memory, they fail to take into"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "In continual learning (CL)111In our setting, the model does not have access to the task id during inference., the objective during task , is to find parameters  which minimize the loss  over the current dataset  and all previously seen datasets:\nTypically, CL approaches assume that past data cannot all be stored. Instead, experience replay approaches store a subset of past data from all previous tasks in a memory buffer , where the size of  is much smaller compared to the combined number of data points from all past tasks. These approaches [40  ###reference_b40###] use  to approximate the true objective (Eq. 1  ###reference_###) and minimize:\nwhere  is a hyper-parameter. The memory buffer is updated after each task but the total number of stored items is constant.\nThe resulting method’s computational requirements scale well with the number of learned tasks, but the limited size of  means that it becomes less effective at representing past data, as the number of learned tasks increases.\nThe (stochastic) K-armed bandit problem [5  ###reference_b5###] considers a setting in which there are  available actions, referred to as arms. Performing one of the actions returns a stochastic reward drawn from an unknown distribution. The problem is selecting a number of actions in a way which minimizes the expected regret, defined as the expected difference between the rewards obtained by always choosing the optimal action and the rewards obtained by following our strategy. At each step, a bandit strategy approximates the parameters of the reward distribution of each of the  actions. Thereafter, the strategy needs to select an action to perform. One such strategy is Boltzmann Exploration [23  ###reference_b23###] which computes the mean of the observed rewards for each action, and then uses all means to define a categorical distribution, from which the choice of action is drawn. Finally, if the action’s distributions change between steps, the bandit problem is referred to as non-stationary [59  ###reference_b59###]."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Adaptive Memory Replay",
            "text": "###figure_2### In this section, we first modify the typical CL setting by challenging the restrictive assumption that past data cannot be accessed. Second, we modify the objective which we minimize, in order to better reflect the new CL setting. Third, we link the resulting problem to that of multi-armed bandit allocation [5  ###reference_b5###] and detail our approach."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Replay: A New Perspective",
            "text": "For FM extended pre-training, we challenge the common CL assumption that past data is unavailable and alter the common CL setting after making two observations. First, data storage is cheap, therefore it is possible to store and sample from any dataset we have seen before. Second, computation is expensive, meaning that we cannot re-train on all of the past data in memory. Following this insight, we modify the CL setting so that all of the past data can be stored, but a CL algorithm’s computational demands to access and use the past data need to be constant and compensated during training to lead to zero extra cost (compared to naive training only on new data).\nBeing able to access all of the past data naturally leads to a rehearsal-based approach where one can only replay a limited subset , due to computational constraints. In contrast to the typical memory replay solutions,  is not fixed while training on a new task, but is instead allowed to adapt to the new data and the changing model while maintaining training efficiency. Therefore, we refer to it as an adaptive memory buffer.\nWe begin developing our approach by modifying the objective function to better reflect our goal. First, as detailed in the Appendix, we express the main objective (Eq. 1  ###reference_###) in terms of its forgetting  on past data, compared to the performance of the optimal parameters for the previous task:\nwhere  is a constant.\nThis change reflects the fact that we fine-tune the previously optimal model on the new task and that our focus is on minimizing forgetting, rather than improving our performance on past data. Next, we define the optimal memory buffer  as the subset of past data for which our model has the maximum forgetting. We then following replay-based methods (Eq. 2  ###reference_###) and similarly optimize:\nMinimizing this objective leads to our rehearsal-based CL algorithm with adaptive memory  which replays the past data points  that have currently been forgotten the most. Importantly, to minimize the overhead which replaying imposes on the training process, we keep the number of computed gradients constant by discarding a portion of the data on the new task. To do this, we remove  randomly selected data points for each batch of inputs from the new task, thus keeping the total number of processed inputs the same.\nIn order to compute Eq. 3  ###reference_###, we need to be able to select  — the subset of all past data with the highest forgetting. This subset changes as we update the parameters , and it is computationally infeasible to evaluate the forgetting of each of the past data points. Instead, we seek to divide all past data into clusters  of items expected to have similar forgetting values.\nThis allows us to infer the forgetting values of the data points in a cluster, based on a small number of evaluations, and use this to select data points for our adaptive memory buffer which exhibits a high amount of forgetting. Currently, we place all of the data from the same previous task into the same cluster, i.e. , and assume that it would exhibit similar forgetting values. We leave more elaborate clustering techniques for future work."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Adaptive Memory as a Bandit Optimization",
            "text": "Formally, we divide all past data into  disjoint subsets , s.t. , where the forgetting of an input is distributed according to a subset-specific distribution: , for . For the parameters  at training iteration , we would like to select a subset  which exhibits close to the worst forgetting, minimizing the following quantity:\nWe frame this as a non-stationary K-armed bandit (KAB) problem [47  ###reference_b47###], where pulling an arm and receiving a reward corresponds to sampling a data point from a cluster and evaluating its forgetting. Then, at each training iteration, we have to choose which of the K arms to pull in order to select  data points with maximum forgetting. As we select different  over all training steps, we would like to reduce the expected regret: , which is the expected difference in forgetting values between  and  over all training steps. In this work, we implement the Boltzmann Exploration [23  ###reference_b23###] approach which, at training step , approximates the mean reward of each arm, denoted by , and then uses all arms’ means as parameters for a categorical distribution over the choice of arms to pull. This distribution is then used to sample  arms and in turn sample the adaptive memory buffer .\nTo approximate the mean forgetting values  of cluster  at training step , we first sample a small number of data points from the cluster  and evaluate the average of their forgetting values — . We would like to compute  based on the previously computed mean value  and the currently computed forgetting average . However, we note that the forgetting values depend on our model’s parameters , thus change between training iterations. We account for this by using a moving average, which is used for KAB when the underlying distributions are non-stationary [49  ###reference_b49###]:\n.\nOnce we have approximated the mean forgetting values for all clusters, we use them to create a categorical distribution over the choice of clusters, with the help of the tempered softmax function [19  ###reference_b19###]. We compute: , where  is the temperature hyperparameter and  is the normalization constant. Afterwards, we use this distribution to sample  cluster indices. Finally, we sample one input from each selected cluster, uniformly at random, and combine the samples to create the adaptive memory buffer for the current training step.\nOur full method is summarized in Algorithm 1  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###table_1### ###table_2### ###table_3### ###table_4### We evaluate the efficacy of our proposed adaptive memory replay for continual learning of FM pre-training (i.e., extended pre-training) in both the vision and language domains. We utilize two distinct pre-trained models as our backbones for these experiments: a Vision Masked Autoencoder (MAE) [18  ###reference_b18###] pre-trained on ImageNet-1K for vision-related tasks, and LLaMA [50  ###reference_b50###] with 7 billion parameters for language experiments. The evaluation metrics for our experiments are twofold: test data Final Loss and test data loss Forgetting. The Loss metrics are standard for FM training (especially in Language Modeling) as they are known to correlate with downstream use due to the massive pre-training volumes and de-facto seeing most of the samples only once. These metrics are normalized between 0% and 100%, where 0% represents an offline upper bound with all data trained independently and identically distributed (iid), and 100% corresponds to the performance of the pre-trained model without any fine-tuning.\nBecause our primary contribution lies in our novel perspective of full-memory replay, we compare our approach with full memory-access iid data replay as opposed to typical continual learning methods. We hypothesize that coreset selection replay methods are effectively upper-bounded by full iid replay, given their goal of identifying the most representative data for replay (rather than our perspective of identifying the most forgotten data for replay as a function of the current data). Consequently, we do not compare our method against sampled data replay in our main results tables as we store all data in memory.\nFurthermore, our experiments are designed to demonstrate the advantages of our adaptive memory replay approach over traditional iid replay, especially in terms of computational efficiency and reduced forgetting. We consider gains of our approach to be orthogonal to the realms of non-replay regularization-based continual learning methods, and thus these comparisons are not the main focus of our results. Besides, from the perspective of computational efficiency, recent work has found such approaches to be impractical for computationally bounded continual learning [16  ###reference_b16###]. However, we do discuss the interaction of our approach with different continual learning strategies like regularization methods and knowledge distillation in our Appendix.\nThe hyperparameters for our experiments were meticulously chosen based on a series of small task experiments. We update our model on  new data examples per task. In the interest of computational resources for the larger Llama model, we approximate the training of all the model parameters with LoRA finetuning [22  ###reference_b22###] in the language modeling experiments. In our experience, conclusions attained for LoRA finetuning reflect the same in full model training. We use a learning rate of  for full model fine-tuning and  for LoRA-based fine-tuning. For our proposed adaptive memory replay bandit scheme, we found that a temperature of  and forgetting mean update ratio of  performed best. We compose our replay batches for both iid replay and our adaptive memory replay with a 1:1 ratio of replay data to new task training data. We conducted evaluations on a hold-out test dataset comprising 500 samples per dataset. Additional training details can be found in our Appendix."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Results for Vision SSL",
            "text": "In Tables 1  ###reference_###,2  ###reference_###,3  ###reference_###, we benchmark our proposed approach on 3 different continual pre-training sequences composed of vision datasets. Our goal was to demonstrate the robustness of our findings with a variety of unique and practical dataset sequences. The first dataset is the DomainNet [34  ###reference_b34###] dataset (Table 1  ###reference_###), containing 6 different domains of common objects. The next is the Medical MNIST dataset [57  ###reference_b57###] (Table 2  ###reference_###), from which we sampled 5 standardized biomedical image datasets containing the highest number of samples. Finally, we use 4 attribute splits from the Synthetic Visual Concepts (SyViC) dataset [9  ###reference_b9###] (3  ###reference_###).\nOur results demonstrate the advantages of our adaptive memory replay method in the vision domain. Our approach consistently outperforms full memory iid replay (which serves as an upper bound for other replay-based continual learning methods that sample from a limited coreset), achieving lower final loss and forgetting rates. The slight increase in normalized training time is negligible compared to the performance gains, and furthermore, we show a 0-cost result where we reduce the number of training steps of our approach to align with the training time of naive fine-tuning, and show that even this result outperforms iid replay in all three benchmarks.\nWe note that our strongest performance gains come from the DomainNet results. The gains for the medical data sequence and synthetic data pre-training are much more modest, yet remain pronounced. The synthetic data sequence is interesting in that forward transfer (i.e., negative forgetting) appears in all results - however, our method still has more forward transfer compared to iid replay. In practical terms, our results imply that vision systems equipped with our continual learning strategy would exhibit improved robustness over time, adapting to new data without significant loss of prior knowledge or computational costs. We re-iterate that there is much more room for improvement from our full-memory continual learning perspective - advanced strategies can close the gap between our method and the upper bound with fixed time costs by exploring interesting questions such as how to better cluster the data and which new data is more or less favorable to discard."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results for Causal Language Modeling",
            "text": "Our approach is further affirmed through our language experiments using the Llama model. In Table 4  ###reference_###, we benchmark on a 5-dataset sequence using datasets from Huggingface [56  ###reference_b56###]. These datasets were chosen based on the significant variations in loss observed post fine-tuning, thus providing a rigorous test for our approach. The datasets encompass a broad range of language tasks, ensuring that our results are representative of diverse language modeling scenarios.\nFurther specifics about these datasets are available in our Appendix.\nThe performance of our adaptive memory replay in our language experiments mirrors the success observed in the vision tasks. We see a significant reduction in both forgetting and final loss compared to the iid full-memory replay. Furthermore, the ‘0-cost’ variant of our method is particularly noteworthy, as it manages to retain a high level of performance without additional computational expenditure compared to naive fine-tuning. This aspect is crucial for applications where computational resources are limited, especially fitting LLM extended pre-training where due to high data volumes and enormous model sizes even a tiny fraction of extra cost is intolerable."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "In Figure 4  ###reference_###, we present a comprehensive comparison of the final loss versus training time for our adaptive memory replay method against the Oracle, using the Synthetic Visual Concepts dataset sequence [9  ###reference_b9###]. This plot demonstrates how our method converges towards the Oracle’s performance as we increase the compute budget of our method (via using more replay samples and discarding fewer new task samples). With a limited budget, there is a notable difference in the final loss between our method and the Oracle. However, as training progresses, our method steadily approaches the Oracle’s level of performance, matching and even outperforming Oracle (which as a fixed compute budget itself, pre-defined by the number of training steps we use) with a lower compute cost. The horizontal dotted line marks the point at which our approach reaches the Oracle’s normalized loss, showcasing the efficiency of our method in terms of both loss minimization and computational time. This result is significant as it not only validates the effectiveness of our adaptive memory replay in reducing the final loss but also highlights its capability to achieve this with a substantially lower training time.\n###figure_3###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we underscored the importance of adapting machine learning methodologies to the ever-evolving demands of real-world large-scale continual learning. Our findings, rooted in extensive evaluations across both vision and language tasks, validate the advantages of our adaptive memory replay for continual learning. By dynamically selecting past training data samples, our method offers a nuanced balance, ensuring minimized forgetting without imposing costly computational requirements. Ultimately, we hope to inspire methodologies that are both computationally efficient and effective in real-world continual learning scenarios, where data access and computational resources are bound by practical constraints. This research trajectory serves as a stepping stone towards the development of continually learning systems that efficiently and intelligently utilize all available data, enhancing their learning and adaptability across a series of tasks throughout their life-cycle.\nFuture work should focus on refining the adaptive memory replay mechanism, particularly exploring more sophisticated bandit-based selection strategies to further enhance the balance between retaining old knowledge and accommodating new information. The decision of which data to discard during the replay phase also warrants deeper investigation to avoid potential loss of critical information (and furthermore potential unrealized gains in discarding repetitive or similar samples). There is also a great need to develop advancing clustering techniques to capture the subtleties of data evolution which can lead to more representative memory buffers. In addition, bringing greater realism into continual learning models by incorporating real-world constraints and scenarios will be crucial, such as blurred task boundaries and online clustering."
        }
    ]
}