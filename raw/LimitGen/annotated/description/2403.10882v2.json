{
    "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
    "abstract": "Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human evaluation and GPT4. Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.\n\n\n\nKeywords: Large Language Model, Less-resourced Languages, Instruction-tuning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "A large language model (LLM) comprehends linguistic information and knowledge via pretraining to predict the subsequent word based on the given context Zhao et al. (2023  ###reference_b38###).\nHowever, the growth of LLMs increases the computing resources required for training, posing a challenge for smaller research groups to utilize them realistically Hoffmann et al. (2022  ###reference_b9###). To meet the demands of this era, numerous big tech companies and research institutes have been competing to launch multilingual LLMs (MLLMs) Touvron et al. (2023a  ###reference_b31###, b  ###reference_b32###); Workshop et al. (2023  ###reference_b37###). However, less-resourced languages (LRLs) are being overlooked Gu et al. (2018  ###reference_b8###).\nThe recently launched Llama2 Touvron et al. (2023b  ###reference_b32###) is an MLLM trained in more than 28 languages; however, only 0.06% of the data was used for the Korean language. This leads to two significant syntactic and semantic challenges. First, during the MLLM training, LRLs use minimal vocabulary based on the scarce training data, which limits their expression owing to the inadequate lexicon. Second, greater semantic knowledge is required to employ LLMs for specific tasks, such as question-answering, thereby rendering the models inapplicable or prone to hallucinations Zheng et al. (2023  ###reference_b39###); Peng et al. (2023a  ###reference_b23###).\nNumerous methods have been proposed to enhance the LRL performance. These include expanding the vocabulary of word embeddings Wang et al. (2019  ###reference_b34###); Schuster et al. (2019  ###reference_b29###), aligning multilingual embeddings by combining them with other languages Artetxe et al. (2017  ###reference_b1###, 2018  ###reference_b2###), and reinforcing the utility of LLMs using minimal training data. The Less Is More for Alignment (LIMA) study proposed a method to maximize the utility of LLMs using 1,030 high-quality instruction dataZhou et al. (2023  ###reference_b40###).\nBased on the existing research, it remains to be investigated whether MLLMs can expand their vocabulary and enhance the semantic inferencing capability of a specific language using minimum additional data.\nThis study explores the aforementioned aspects by proposing a method to enhance the Korean language capabilities of a representative MLLM, i.e., Llama2. The specific language abilities of the MLLM are enhanced using the following strategies:\n(1) Vocabulary expansion: To enhance its fundamental vocabulary, the MLLM was augmented with the dictionary of a specific language.\n(2) Knowledge enrichment: The vocabulary and knowledge information of this language were enhanced in the MLLM via pretraining.\n(3) Usability enhancement: High-quality instruction data were generated in the Korean language to improve its LLM applicability. Korean is not an LRL as it comprises various language resources and evaluation data Park et al. (2021  ###reference_b22###). However, Korean is experimentally suitable as a relatively less-resourced language because the Llama2 model uses limited Korean data and vocabulary during training.\nTo enhance the vocabulary, knowledge reinforcement, and usability, 7,478 Korean vocabulary entries were added and pretraining was performed using a Korean–English corpus. The 1,030 English data, proposed by LIMA  Zhou et al. (2023  ###reference_b40###), were restructured by three Korean linguistics to ensure their practical similarity with the Korean language, thus enhancing their usability.\nThe effectiveness of the three enhancement methods were validated by addressing the following questions: (1) What are the advantages of expanding the Korean vocabulary? (2) Is it effective to connect the knowledge of high- and low-resource languages via pretraining? (3) Do the actual usability and accuracy improve using the proposed Korean LIMA data? We propose the Bllossom model that applies the three aforementioned methods. Quantitatively, this model demonstrated an average performance improvement from 1.8% to 8% across eight tasks compared to the model without vocabulary expansion. For the qualitative evaluation of each model, the answers to 300 queries were compared using human and preference evaluations based on GPT4. Consequently, the Bllossom model outperformed the other Korean models of the same size by 93%. The contributions of this study are as follows:\nA method for enhancing the vocabulary, knowledge, and usability of LRLs using MLLMs was proposed.\nA method for constructing instruction data based on language-specific features was presented and demonstrated by constructing a Korean LIMA dataset.\nFor easy utilization, the data, models, and services used to construct and evaluate the Korean LLM were made publicly accessible111github.com/Anonymous-introduce-paper/LR  ###reference_er/LR###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   MLLMs",
            "text": "Multilingual LLMs are advantageous in accumulating vast training data from a single language. Owing to the increasing parameters and LLM training data, models such as FLAN-T5 Wei et al. (2022a  ###reference_b35###), BLOOM Workshop et al. (2023  ###reference_b37###), Falcon, Llama Touvron et al. (2023a  ###reference_b31###), and Llama2 Touvron et al. (2023b  ###reference_b32###), which are smaller but comprehensively acquire the multilingual knowledge, have attracted scholarly attention. Llama2 is a multilingual language model trained using large-scale publicly available data (including CommonCrawl, Github, Wikipedia, and ArXiv) for more than 28 languages. Thus, it possesses cross-language understanding capabilities. However, languages with non-Latin character systems, such as Korean and Chinese, exhibited inferior performances Cui et al. (2023  ###reference_b6###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Open-source Korean LLMs",
            "text": "EleutherAI developed Polyglot-Ko, which is a monolingual Korean LLM pretrained on 1.2 TB of Korean data and contains models with sizes up to 12.8B Ko et al. (2023  ###reference_b13###). KoAlpaca222https://github.com/Beomi/KoAlpaca  ###reference_github.com/Beomi/KoAlpaca### is a model based on Polyglot-Ko that automatically translates the SFT data of the English Alpaca Taori et al. (2023  ###reference_b30###) into Korean and performs SFT with a total of 21K data. Similarly, Kullm Lab and research (2023  ###reference_b15###) was proposed based on Polyglot-Ko and tuned using additional instruction data. Kullm used 153K SFT training data by translating English SFT datasets, including the GPT-4-LLM Peng et al. (2023b  ###reference_b24###), Vicuna Chiang et al. (2023  ###reference_b4###), and Dolly from Databricks Conover et al. (2023  ###reference_b5###). Additionally, models that perform the Korean SFT based on multilingual models, such as Llama2, have been launched. Komt is an instruction-tuned model based on Llama2 using a total of 1,543K data processed from existing Korean natural language processing data. Ko-Platypus2 Lee et al. (2023a  ###reference_b16###) enhances the logic knowledge of LLMs using a translated dataset from English Open-Platypus into Korean. This model is tuned using Llama2 with 25K SFT data. The aforementioned Korean models were used in the experiment, and the access links and summary information for each model are listed in Table 4  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Enriching the MLLM vocabulary",
            "text": "This section introduces the following two approaches to the three language enhancement methods proposed in the Introduction: (1) vocabulary expansion and (2) knowledge enrichment. We propose a method to expand the Korean vocabulary in Llama2, which is a representative multilingual LLM, and reinforce the knowledge information between the Korean and English languages via CLM-based pretraining.\n###table_1### ‘_’, ‘<0xED>’, ‘<0x96>’, ‘<0x84>’, ‘<0xEB>’, ‘<0xB2>’, ‘<0x84>’, ‘<0xEA>’, ‘<0xB1>’, ‘<0xB0>’, ‘를’, ‘_’, ‘<0xEB>’, ‘<0xA8>’, ‘<0xB9>’, ‘는’, ‘_’, ‘공’, ‘<0xEB>’, ‘<0xA3>’, ‘<0xA1>’\n‘햄’, ‘버’, ‘거’, ‘를’, ‘_먹는’, ‘_’, ‘공’, ‘룡’"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Vocabulary expansion",
            "text": "The training data of Llama2 consisted of 89.7% English words, and the tokenizer dictionary () was composed of 90% English (or Latin) words. The majority of the remaining words were rare words, neologisms, and LRLs categorized as out-of-vocabulary (OOV). To address this issue, Llama2 employed the SentencePiece tokenizer Kudo and Richardson (2018  ###reference_b14###) that uses a UTF-8 byte fallback mechanism to handle the OOV words by decomposing them to the UTF-8 byte level. Therefore, the words not in  are represented without expanding the tokenizer vocabulary.\nThe Korean vocabulary was expanded despite having a method to represent the language. Table 1  ###reference_### compares the tokenizing results of Llama2 and the proposed model with an expanded Korean vocabulary for the sentence “햄버거를 먹는 공룡”. In the original Llama2, the Korean word “햄” was decomposed into the tokens “<0xED>”, “<0x96>”, and “<0x84>”, and “버” was decomposed into “<0xEB>”, “<0xB2>”, and “<0×84>” at the byte level. Contrastingly, the tokenizer with an expanded vocabulary tokenized “햄” and “버” in their original forms. The tokenizing results of the existing Llama2 model can lead to the following two problems, as indicated in the Chinese ALPACA Cui et al. (2023  ###reference_b6###):\nIncreased token length: The model cannot represent an OOV using a single token that requires three or four byte tokens. This reduces the possible input length of the model and increases the encoding and decoding times.\nDuplication of byte tokens: “햄” and “버” are unrelated tokens; however, they are represented using the same byte token “<0x84>”. Therefore, the model may experience confusion while learning two semantically unrelated words with partially identical representations.\nThese limitations were overcome by introducing the Korean vocabulary, as shown in Equation 1  ###reference_###. A new embedding was generated by combining the existing Llama2 vocabulary  and KoBERT 333https://github.com/SKTBrain/KoBERT  ###reference_### vocabulary .\nThe KoBERT vocabulary, designed by considering Korean morphemes, consists of  words, whereas . The union of the two vocabularies has a size of . Therefore, the size of the newly added dictionary was .\nHere,  used the word embeddings trained on the original Llama2, and the newly added word embeddings  were randomly initialized."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Enriching the knowledge information by MLLM pretraining",
            "text": "This section introduces methods to reinforce the word and knowledge information of MLLM via CLM-based pretraining. For queries in the Korean language, the publicly released Llama2 13b model responds in English or alternates between English and Korean (code-switching), indicating a limited Korean expression. However, the content is often accurate when Llama2 responds to a Korean query in English. When asked “이탈리아 수도에 대해 한국어로 소개해줘” (“Introduce me to the Italian capital in Korean”), the model replies, “로마 is the capital city of Italy and…” where the proper nouns “로마” (Rome) and “콜로세움” (Colosseum) are generated in Korean but the detailed explanations are provided in English. This is because the knowledge acquired through pretraining was predominantly in English.\nThis limitation can be overcome by aligning the knowledge of the Korean and English languages in the MLLM by further pretraining it on a small amount of data. The MLLM expanded with Korean vocabulary was trained on the English and Korean Wikipedia, thereby bridging the extensive English knowledge (accounting for 89.7%) and limited Korean knowledge (0.06%). This method aligns with that of the multilingual BERT approach, which was pretrained on the Wikipedia data from 104 languages Pires et al. (2019  ###reference_b25###).\nEquation 2  ###reference_### shows that the proposed model was pretrained using CLM. Given an input token sequence  the model predicts the next token , computes the loss by taking the negative log-likelihood of the predicted token probability, and minimizes this loss.\nHere,  represents the loss function of the language model over the pretraining dataset ,  signifies the model parameters,  is the target token for prediction, and  refers to the dictionary expanded using the Korean vocabulary. Table 3  ###reference_### lists the specific compositions and sources of . The loss function accounts for the prediction accuracy of each token within the pretraining dataset. The Korean and English bilingual corpora were adopted in the training method and the model was trained on 33 GB of pretraining data for one epoch with a batch size of 8.\n###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Instruction Tuning on LIMA",
            "text": "The Korean language capability was enhanced by pretraining and the existing knowledge between the English and Korean languages was bridged. However, models trained during pretraining have limited applicability because they are specialized for predicting only the subsequent token. Consequently, high-quality Korean SFT data are required to accurately understand the user intent and generate desired responses. This section describes the method for reconstructing Korean SFT data based on English LIMA and introduces an instruction-tuning approach using these data."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Building the Korean LIMA",
            "text": "The Korean LIMA dataset for SFT was constructed based on a version that underwent machine translation using the English LIMA dataset. Consequently, post-processing was required to address the following issues: (1) discrepancies within the authentic Korean linguistic styles owing to machine translation and (2) exclusion of the Korean cultural context stemming from the characteristics of raw sources in the English LIMA dataset. The Korean LIMA dataset used in this study underwent a human review of the initial machine-translated text and modifications to reflect the Korean cultural context, which involved replacing the named entities and changing the main topic. For the human review process, we recruited the reviewers with Korean as their native language, ensuring that they calibrated all the translated data to the most natural Korean linguistic style.\n\nThe raw sources for the English LIMA dataset were posts from the English-speaking community forums, such as Stack Exchange and WikiHow, which reflected the cultural context of English speakers. The cultural context refers to a broad spectrum encompassing everything from daily consciousness to political, economic, and social systems. For instance, a sample from the English LIMA dataset, “How to make banana muffins?” may be irrelevant to the Korean culture because “banana muffins” are neither a popular consumable nor a frequently baked item in Korea. To reflect the Korean cultural context in the dataset, we modified the instances in the English LIMA data that featured Western cultural contexts, particularly the American contexts. These modifications ranged from narrow changes, such as renaming the entities, to broader adjustments, such as entirely altering the dataset topic to fit the Korean context (see examples in Table 2  ###reference_###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   MLLM Training using the Korean LIMA",
            "text": "The Stanford Alpaca Taori et al. (2023  ###reference_b30###) is an instruction tuned model based on the Llama trained on 52k instruction data. The corresponding training code is open-source444github/tatsu-lab/stanford_alpaca  ###reference_atsu-lab/stanford_alpaca###. We adapted the training script to instruct our model using the Korean LIMA dataset. Instruction-tuning follows the SFT method, wherein prompts are provided as inputs to the model which is subsequently trained to produce the user-desired responses. While this process is similar to pretraining, it differs in that only the output of the prompt is used to compute the loss. This can be mathematically represented as follows:\nWhere  represents the model’s parameters,  denotes the SFT dataset, and  signifies the token sequence of the template containing the instruction and output.\nPretraining and instruction-tuning require substantial GPU resources. Recent research proposals have focused on training only specific portions of the model that require minimum GPU resources. LoRA Hu et al. (2022  ###reference_b10###) is a representative method that involves freezing a pretrained model and infusing each of its layers with trainable rank-decomposition matrices for further training. To apply LoRA, one must choose which parts of the entire model to train. This study trained only the linear layers of the transformer attention, including the query, key, and value, along with the expanded word embedding (as shown in Figure 1  ###reference_###).\nConsequently, 5.977% of the total Llama2 parameters were used for training, thereby facilitating the training of our model on a single A6000 GPU.\nFigure 1  ###reference_### shows the three proposed enhancement methods. The final model underwent the following sequence: (1) vocabulary expansion, (2) bilingual pretraining, and (3) instruction tuning (SFT). Within this context, “Trainable” (red) and “Frozen” (blue) refer to regions where the parameters were updated and not updated during training, respectively. SFT was concurrently performed using the constructed Korean and English LIMA datasets.\n###figure_1###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Quantitative evaluation",
            "text": "This section describes the following experiments to explore the three research objectives presented in the introduction: (1) comparison between the models with and without the expanded Korean vocabulary; (2) comparison between the model pretrained on the Korean–English bilingual training data and that trained only on the Korean data; and (3) variations in performance owing to instruction-tuning using the LIMA dataset."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Evaluation Environment",
            "text": "###table_3### To ensure a fair comparison of LLMs, it is essential to define the task selection for evaluation and specify the LLM model used in the evaluation. To quantitatively evaluate the problem-solving capability of LLMs from various perspectives, the tasks involve language comprehension and inference, sentiment analysis, etcetera Park et al. (2021  ###reference_b22###); Zhou et al. (2023  ###reference_b40###). The Korean LLMs were comprehensively evaluated based on eight datasets.\nThe benchmarks for the evaluation tasks were KLUE’s NLI, STS, and YNAT, and Naver AI’s Ko-SBI Lee et al. (2023b  ###reference_b17###), and KOBEST’s BoolQ, HellaSwag, SentiNeg, COPA Jang et al. (2022  ###reference_b11###), which are described as follows.\nNatural Language Inference (NLI): A classification dataset predicting the relationship between two sentences.\nSemantic Textual Similarity (STS): A classification dataset measuring the semantic equivalence between two sentences.\nYNAT: A classification dataset that infers the topic of a given sentence.\nSBI: A classification dataset aimed at identifying social stereotypes or biases.\nBoolQ: A question answering dataset for yes/no questions.\nHellaSwag: A commonsense NLI dataset.\nSentiNeg: A sentiment classification data.\nCOPA: A classification dataset determining the cause/effect based on a paragraph.\nThe experiments were performed using the Polyglot team’s public branch of EleutherAI’s lm-evaluation harness Gao et al. (2021  ###reference_b7###) to ensure reproducibility and compare the models. Each model was evaluated using the same data and prompt commands. Table 5  ###reference_### lists the STS evaluation prompts for which each system generated an answer.\nFor fair evaluation, the model to be evaluated must accurately represent the backbone of the training model and size of the used data, which are defined for the proposed model in Table 4  ###reference_###. The Model column is structured in the format “Model-Pretrain Language-Option”. The Pretrain Language value is ‘bi’ denotes a model that simultaneously uses the Korean and English languages for pretraining. The Option field denotes the application of SFT, where “biSFT” represents the implementation of the Korean and English LIMA data, whereas “koSFT” denotes the usage of only the Korean LIMA data. The Bllossom model refers to the model with vocabulary expansion applied to Llama2. For instance, Llama2-ko is a model pretrained on Llama2 in Korean and Bllossom-bi is a model pretrained in Korean and English after vocabulary expansion. The Bllossom-bi-koSFT is a Bllossom-bi model tuned using the Korean LIMA data. Polyglot-ko and KoAlpaca, are presented in Section 2  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Experiment Results",
            "text": "(Overall) Table 6  ###reference_### shows the performances of various models proposed in Table 4  ###reference_###. Compared to the monolingual models (such as Polyglot-Ko, KoAlpaca, and Kullm), the proposed multilingual Bllossom models (referred to as “ours”) exhibited an average performance with an increment of approximately 4.57 points. The MLLM performance was affected by the presence or absence of pretraining. The difference between the performances of Llama2-ko, which underwent only pretraining, and Llama2-koSFT, which underwent only SFT, was a substantial 6.2 points.\n(The influence of vocabulary expansion)\nIn Table 6  ###reference_###, Bllossom-ko outperformed Llama2-ko by approximately 1.8 points. For NLI and STS which infer the relationship between two sentences, the Bllossom-ko model with an expanded vocabulary outperformed by 9.15 points. Contrastingly, Llama2-ko, which did not undergo vocabulary expansion, performed better on SBI by 8.8 points. Thus, vocabulary expansion improves the overall comprehension, reasoning, cognition, and causal understanding of the Korean language.\n(The influence of bilingual pretraining)\nThe Bllossom-ko and Bllossom-bi models in Table 6  ###reference_### differ on the usage of English and Korean bilingual training data during pretraining. The models exhibited similar performances with scores of 58.9 and 58.6, respectively. However, the following observations were made: (1) In contrast to Bllosson-bi, Bllossom-ko exhibited a bias issue wherein the model responded in Korean even when queried in English. (2) For the SBI tasks, Bllossom-bi outperformed by 11.6 points than Bllossom-ko. And it underperformed 11.2 and 10.6 points on the STS and HellaSwag tasks, respectively. Quantitatively, the impact of bilingual pretraining was minimal; however, a significant performance difference was qualitatively observed owing to bilingual pretraining.\n(The influence of SFT on the Korean LIMA) This experiment evaluated the impact of 1K Korean LIMA data by comparing Llama2 and Llama2-koSFT, which performed SFT on Llama2.\nIn Table 6  ###reference_###, Llama2, the backbone, outperformed by an average of 2.2 points. Similarly, the performance of Komt, which underwent an extensive SFT on Llama2, was approximately reduced by six points. The models based on Polyglot-ko, such as KoAlpaca and Kullm, exhibited lower performance than that of the backbone. Therefore, SFT may not significantly influence the quantitative evaluations of classification tasks. However, Llama2-koSFT empirically produced better responses than Llama2 based on qualitative factors, such as the quality of the generated responses, vocabulary, and completeness. Therefore, the following section analyzes the effects of SFT based on qualitative evaluations performed by humans Lee et al. (2023c  ###reference_b18###) and GPT."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Qualitative evaluation",
            "text": "Based on LIMA Zhou et al. (2023  ###reference_b40###), the qualitative evaluation was performed by humans and GPT. The former involved posing the same question to LLMs A and B and the evaluators subsequently deciding among the responses based on the following three options: Model A is better, Model B is better, or neither is significantly better. Contrastingly, GPT4-based evaluation enabled the GPT to decide among these options. We translated 300 entries from the LIMA human evaluation test dataset into Korean and proceeded with evaluation. According to the LIMA study, the 1k LIMA training data and LIMA human evaluation test dataset were designed to have completely different topics, styles, and tasks. Therefore, tuning the model using LIMA training data negligibly improves the performance Zhou et al. (2023  ###reference_b40###).\n(Overall) Figure 2  ###reference_### and Figure 3  ###reference_### show the results of human and GPT4 evaluations, respectively. When comparing Bllossom to Koalpaca and Kullm models of the same size, Bllossom outperformed them in human and GPT4 evaluations and even outperformed the larger Llama2-70b-chat model. Another interesting point is the qualitative evaluation results for human and GPT4 were similar. This was also observed in LIMA."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Human-assisted preference evaluation",
            "text": "###figure_2### In Figure 2  ###reference_###, the number of times Bllossom won both Koalpaca and Llama2 in human evaluation was 124. The 124 tasks included answering real-world user requests for recommendations, answering questions requiring imagination or creativity, organizing travel itineraries and writing code. In contrast, there were 40 instances where Bllossom gave inferior answers compared to both models, mostly for factual QA. This suggests that Bllossom is not yet as knowledgeable as the larger models, which is likely due to the difference in data size from the pre-training phase."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Preference Evaluation using GPT4",
            "text": "Using the methodology proposed by LIMA, GPT4 was used to compare the performance of Bllossom with six other models. The evaluation was conducted on the Korean LIMA test data.\n(Comparing Bllossom with Korean models based on Llama2) Figure 3  ###reference_### shows that Komt and Ko-Platypus2 are models based on Llama2-13b that underwent SFT, similar to Bllossom. However, these two models were exclusively subjected to SFT without vocabulary expansion or pretraining. They were fine-tuned using extensive datasets that were either autogenerated or translated, and thus, of lower quality. Qualitatively, Bllossom exhibited superior performance with a margin exceeding 40%, indicating that pretraining significantly influences the Korean proficiency. During the human evaluation of history-related questions, Komt and Ko-Platypus2 either failed to provide answers or exhibited hallucinations more frequently compared to Bllossom. This can be attributed to Bllossom gaining additional knowledge during pretraining.\n###figure_3### (Comparing Bllossom with Polyglot-ko-based Korean models) We discuss whether the proposed Bllossom model exhibits a better qualitative evaluation than Korean monolingual LLMs. Polyglot-ko is a representative Korean monolingual model pretrained on vast Korean datasets and KoAlpaca and Kullm are models trained based on Polyglot-ko. Figure 3  ###reference_###, shows that the Bllossom model has a 9~56% higher probability of producing superior answers than the two monolingual models utilizing Polyglot-Ko as their backbone. During pretraining, Llama2 incorporated a relatively limited set of Korean data; however, its training dataset significantly expanded when English data was included, surpassing the dataset size of Polyglot-ko. This suggests that the bilingual pretraining, which was carried out to augment the deficient proficiency in Korean, has somewhat assisted in bridging the knowledge between Korean and English Cui et al. (2023  ###reference_b6###).\n(Comparing Bllossom with GPT4 and Llama) We discuss the Korean-language proficiency of the proposed Bllossom model. The Llama2-70b model, which has significantly more parameters, was evaluated. Based on the results in Figure 3  ###reference_###, the Bllossom model was selected for approximately 14% of the answers than Llama2-70B. Therefore, in case of an extreme difference in the number of parameters, the differences in performance can be fairly compensated via techniques such as word expansion and pretraining. The qualitative evaluation results for OpenAI’s much larger GPT4 model indicated its superiority in frequent answering.\n(The effect of bilingual dataset for SFT) In Figure 4  ###reference_###, the Bllossom-bi-koSFT model and the Bllossom-bi-biSFT model differ based on whether bilingual data was utilized for SFT. We conducted a comparative evaluation of the two models using both Korean and English. For the Korean and English LIMA test data, the win ratio for the Bllossom-bi-biSFT model was overwhelmingly high at 67% and 95%, respectively. This indicates that contrary to qualitative evaluation, the effect of bilingual SFT in quantitative evaluation is significant.\n###figure_4### (The effect of English language)\nWe investigated whether a model tuned to Korean based on Llama2 would perform poorly in English. As shown in Figure 5  ###reference_###, all the models appear to have significantly lost their English proficiency compared to the original Llama2 model. Nevertheless, Bllossom showed much better performance compared to Komt and Ko-Platypus2. From this, we can infer that while acquiring Korean proficiency, the Bllossom model has a lesser reduction in English capabilities compared to other Korean LLMs.\n###figure_5###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "This study proposed three methods for enhancing the MLLM capability of LRLs. First, to improve the Korean vocabulary capability of the existing Llama2 model, the vocabularies from KoBERT and Llama2 were merged to create a new embedding. Second, pretraining was performed using bilingual data to enhance knowledge information by aligning high- and low-resource languages. Third, instruction-tuning was performed using the English and refined Korean LIMA datasets to accurately understand the user intent and produce the desired response. Quantitative assessments were performed using eight benchmark datasets and qualitative assessments were conducted using humans and the GPT4 model to investigate the proposed model. The experimental results revealed that the proposed Bllossom model outperformed the pre-existing Korean monolingual models that require vast computing resources and supervised data."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethical Considerations",
            "text": "While we have no ethical concerns regarding the current work, our commitment to upholding the highest ethical standards in all our activities and human evaluations remains unwavering."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Limitations",
            "text": "In this paper, we proposed a method to enhance Korean language in MLLM. However, to apply the same method to other languages, the following efforts are required. (1) For building LIMA data, one needs to translate 1,030 data, (2) one also need to translate 300 training data for testing."
        }
    ]
}