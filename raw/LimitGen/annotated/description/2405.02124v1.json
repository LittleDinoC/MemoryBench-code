{
    "title": "1 Introduction",
    "abstract": "In this paper, we present a novel approach for text independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (wav2vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained thanks to forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Pronunciation is pivotal for language acquisition and effective communication. However, it often receives insufficient attention in second language (L2) education, leading to persistent challenges for learners in achieving clarity. Technology, particularly Computer-Assisted-Language-Learning (CALL) has emerged as a significant aid in supporting L2 pronunciation development across formal and informal learning environments Chapelle (2009  ###reference_b1###). Recent advancements in deep learning offer promising avenues for enhancing language learning by providing unlimited targeted and immediate feedback, addressing the class time and size limitations preventing educators from differentiating their pedagogy on such an individual skill.\nTo improve pronunciation, integrating speech processing technologies with traditional teaching methods can provide a comprehensive approach to language learning. One fundamental task in this domain is text independent phone-to-audio alignment, a process that involves aligning phonetic representations with corresponding audio signals without relying on pre-determined text. This task is essential for accurately mapping the phonemes to their acoustic representations, contributing to the development of precise and effective speech processing technologies in language learning Golonka et al. (2014  ###reference_b2###); Tits and Broisson (2023  ###reference_b3###).\nText independent phone-to-audio alignment faces a significant challenge due to the difficulty in obtaining extensive and well-annotated datasets. A possible solution to this challenge entails using established systems (like text dependent phone-to-audio alignment systems) to extract temporal information from publicly available speech datasets. This approach can be refined through the application of transfer learning and self-supervised learning methodologies in the development of a solution.\nTransfer learning Tan et al. (2018  ###reference_b4###), is a method within the field of deep learning. Its approach involves pre-training a model on a large dataset and subsequently fine-tuning it on a smaller dataset tailored to the specific task. This methodology has demonstrated considerable success across various domains, particularly in the context of self-supervised learning.\nIn Self-supervised learning Jaiswal et al. (2020  ###reference_b5###) a model is trained to autonomously learn representations of input data without the need for explicit supervision. This proves particularly advantageous when labeled data is either limited or entirely unavailable. Within the field of speech technology, the application of self-supervised learning through transfer learning has proven invaluable in addressing several complex scenarios. For example, in Automatic Speech Recognition (ASR) for low resource languages Zoph et al. (2016  ###reference_b6###), where annotated data may be scarce, transfer learning allows the utilization of pre-trained models on more data-abundant languages, adapting them effectively to the target language. Likewise, in emotional or expressive speech synthesis Tits et al. (2020  ###reference_b7###, 2019  ###reference_b8###, 2021  ###reference_b9###), speech emotion recognition Tits et al. (2018  ###reference_b10###), voice conversion Zhou et al. (2022  ###reference_b11###) and pronunciation assessment Hu et al. (2015  ###reference_b12###).\nIn this paper, we take full advantage of state-of-the-art methodologies in deep learning, self-supervised learning, and phonetic representation to present a novel approach to text independent phone-to-audio alignment.\nMost state-of-the-art self-supervised systems perform well on American English which means that other varieties of English get penalised and the model is biased towards American English. The rationale to develop the system proposed in the paper was the pedagogical needs to create a system that performs equally well on other variants of English as it does on American English. For our system, we use the self-supervised model Wav2Vec2, fine-tuned for phoneme recognition using CTC loss. We integrate this with a dimensional reduction model based on Principal Component Analysis (PCA) and a frame-level phoneme classifier. The resulting model pipeline produces a vector of probabilities for every audio frame. From the same model we also extract predicted phonemes and their boundaries and hence use this information for a text independent phone alignment system.\nThe major contributions of this paper are as follows: First,\nwe propose a text independent phoneme alignment system using self-supervised learning. This not only advances the state-of-the-art in this specific task but also opens avenues for broader applications in language learning and speech processing systems. Second, this system functions effectively with diverse English language variations (British) and is capable of accommodating various languages, making it language-independent."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Phone-to-Audio Alignment",
            "text": "Text independent phone-to-audio alignment involves predicting a sequence of phones and their temporal locations within speech signal with the aid of prior linguistic information, such as a known text or phone sequence."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Phoneme Recognition",
            "text": "Text independent phone-to-audio alignment involves predicting a sequence of phones and their temporal locations within a speech signal with prior linguistic information, such as a known text or phone sequence."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Systems Predicting Phones and Boundaries",
            "text": "Text independent phone-to-audio alignment involves predicting a sequence of syllables and their temporal locations within speech signal without prior linguistic information, such as a known text or phone sequence."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Our Proposed Method",
            "text": "Our proposed method is an innovative approach, using wav2vec2, Principal Component Analysis (PCA) for dimensional reduction and frame-level phoneme classification, offers a robust text independent phone-to-audio alignment. The system is explained in details in subsection 3.3 and the system’s architecture is depicted in Figure 1. The model’s robustness is demonstrated through evaluation using synthetic native data, using the TIMIT Garofolo (1993  ###reference_b21###) and SCRIBE dataset. Thanks to its versatility, we expect that our method will find applications in language learning and speech processing systems.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Pre-trained Self-Supervised Model",
            "text": "In this Section, we explain what open-source model we use as a basis222https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft  ###reference_lsr-53-espeak-cv-ft### in order to leverage learned cross-lingual representations of speech adapted towards the phonetic space.\nThe basis of this model is a self-supervised model (wav2vec2) trained to extract latent representations from the audio data. The wav2vec2 model is trained on a large dataset of unlabeled speech data, using a contrastive predictive coding loss function to learn a representation that is capable of predicting future audio frames and can then be utilized for downstream tasks. The pre-training loss is defined as Baevski et al. (2020  ###reference_b22###):\nWhere,  is contrastive loss,  is diversity loss and  is the hypertuned parameter\nIn the above equation,  represents the fixed temperature, and sim signifies the cosine similarity between context representations and quantized latent speech representations. The term  resembles the Softmax Function, but it employs cosine similarity instead of a score. For ease of optimization, the negative logarithm of the fraction is also applied.  is the context network output centered over masked time step  and  is the true quantized latent speech representation.\nIn this paper we use the version of the model pre-trained on cross-lingual speech data (53 different languages) Conneau et al. (2020  ###reference_b18###) that was then fine-tuned Xu et al. (2021  ###reference_b19###) for the task of phoneme sequence prediction using a CTC loss, and the result is an open-source 333https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft  ###reference_lsr-53-espeak-cv-ft###.\nThe model itself thus predict sequences of phoneme labels, but no phone boundaries. What we propose in the following sections is an approach for leveraging the learned cross-lingual speech representations of this model that was already oriented to a phonetic space thanks to the CTC fine-tuning. The goal of this approach is to require limited amount of data and to be less biased towards American English accent compared to existing models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Data Processing and Knowledge transfer",
            "text": "Based on the representations that we can extract from the model explained in the previous section, we used PCA to reduce dimensionality while retaining most of the variance in the data. We use the last hidden layer before classification heads as our speech representations.\nWe aimed to preserve a substantial amount of variance of the data. Experimenting without PCA and with varying levels of variance retention (99%, 95%, and 90%), we determined that retaining 95% offers the most favorable compromise in terms of classification results. We hypothesized that this choice may serve as a means of noise filtration and provided a more manageable space for classifiers to process effectively. To train this dimension reduction model and the subsequent frame classifier model we used the MAILABS Solak (2019  ###reference_b23###) dataset\nwhich is a large dataset of speech data, containing recordings from various languages and accents to train our reducer and classifier. We used the American and British English datasets from MAILABS.\nWe worked at the frame level for training the shallow dimension reduction and classifier models. As we observe from Figure 2, the frequency of phonemes in natural languages is not uniform. Hence, it is essential to perform data balancing on the data extracted from MAILABS for unbiased training. To perform data balancing, we randomly selected an equal number of latent frames in order to have a balanced set of labelled latent frames for every phoneme.\n###figure_2### Finally, we trained a frame-level phoneme classifier based on this reduced space. To train this model, we thus built a reduced latent frame dataset from information extracted from MAILABS dataset. For the classifier we use K-Nearest Neighbors (KNN) with 10 neighbours. The frame classifier produced a probability matrix, with each row representing a frame and its corresponding predicted probability for a phoneme. We selected the phoneme which has the maximum posterior probability for each frame. Subsequently, consecutive frames with identical phonemes were grouped together and the start and end timings of each group was computed using frame indices and timestamps. To filter noise, we applied a threshold of 0.5 over the probabilities to remove phonemes that have less than the threshold value. After this step, we merged the consecutive groups to obtain the final timings of the audio sequence.\nOur proposed approach, which integrates a reduction model with a classifier model can adeptly model intricate relationships stemming from the speech representation learning model. The task of mapping between these learned representations and phonetic units is efficiently managed by a compact and effective model. This ensures our ability to comprehend the complex relationships within the data while maintaining computational efficiency."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we describe the experiments conducted to evaluate the performance of our proposed phone-to-audio alignment model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text independent phone-to-audio alignment on TIMIT",
            "text": "In evaluating the performance of our text independent phone-to-audio alignment model, we conducted a comprehensive comparison with the state-of-the-art model, charsiu Zhu et al. (2022  ###reference_b20###).\nIn the charsiu model the authors compare two systems: text independent phone-to-audio alignment and text dependent phone-to-audio alignment using Wav2Vec2. The Wav2Vec2-FS is a semi-supervised model which learns alignment using contrastive learning and a forward sum loss. The second model, Wav2Vec2-FC, is a frame classification model trained on labels for forced alignment, capable of both forced alignment and text-independent segmentation. The evaluation of both the systems for charsiu has been performed on the TIMIT dataset. It has been used for this evaluation because of the availability of human annotations especially phoneme level. We use a similar approach to evaluate the same metrics for our model.\nThe assessment is based on statistical measures, namely precision, recall, F1 score, and r-value. In the referenced work, the authors present their best-performing model, W2V2-FC-32k-Libris, as can be observed in Table 1.\nIn our case, for the task of phoneme recognition, we compare our model with the text independent W2V2-FC-10ms charsiu model. The statistical metrics for our proposed model outperform that of the charsiu model. We compare the TIMIT test dataset to assess our model and we observe from Table 1 that the r-value for our model deteriorates in performance. Apart from r-value, all the other metrics: Precision, Recall and F1 value have shown to perform well for our model.\nThe tests for our model are also performed on the TIMIT dataset but has the possibility to extend to other datasets with real speech and also other languages.\nThe r-value, which is known as the Pearson correlation coefficient measures the similarity between the ground truth phonemes and the predicted phonemes. Some of the reasons for low r-value could be alignment errors, variability in the pronunciation of speakers or changes in speaking style within the dataset. However, we need more experiments to confirm this hypothesis."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Text-independent phone-to-audio alignment on SCRIBE",
            "text": "A second part of the experiments in our proposed model is to evaluate it on British English.\nThe SCRIBE444https://www.phon.ucl.ac.uk/resource/scribe/  ###reference_/### dataset is a small corpus of read speech and spontaneous speech specializing in British English. It consists of 200 ’phonetically rich’ sentences and 460 ’phonetically compact’ sentences. The ’phonetically rich’ sentences are phonetically balanced. The ’phonetically compact’ sentences are based on a British version of the MIT compact sentences (as in TIMIT). There are 45 files in the dataset of 30-50s containing 5-10 sentences.\nThe audio files and the phoneme annotations needed some processing before we could start using the dataset. The phonemes are in SAMPA form and needed to be converted to the English Arpabet. Even after the conversion from SAMPA to Arpabet, there were symbols that we could not retrieve so we filtered them out.\nWe evaluated the charsiu model and our proposed model using SCRIBE and achieved the metrics that are depicted in Table 2.\nUpon close examination of the results presented in Table 2, it becomes evident that the metrics associated with our proposed model exhibit a greater uniformity in values when compared to charsiu. Furthermore, these metrics generally surpass those of charsiu, albeit with the exception of precision. The uniformity observed in our model’s metrics can be attributed to the utilization of a more balanced reduced frame dataset during training, which serves to mitigate biases and yield more consistent outcomes. Moreover, the superior quality of audio files within the SCRIBE dataset, resembling professional studio recordings with minimal noise, likely contributes to the heightened metrics observed for both charsiu and our proposed model when contrasted with the TIMIT dataset.\nConsequently, our system demonstrates enhanced generalization across various accents, laying the foundation for potential expansion to other languages. The key lies in leveraging the underlying Wav2vec2 XLSR framework, and the methodology employed with MAILABS can seamlessly be replicated with different datasets, opening avenues for utilizing non-native English accents and broader linguistic applications."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this paper, we introduced an innovative text independent phone alignment system designed to be language-independent. Our method harnesses a self-supervised model (wav2vec2) fine-tuned for phoneme recognition through a CTC loss, alongside a dimensional reduction model (PCA) and a frame-level phoneme classifier. Through experiments using synthetic native data, we assessed our model’s performance on both American and British English, benchmarking it against the state-of-the-art charsiu model. Encouragingly, our results demonstrate robustness and applicability to diverse accents, such as British English.\nHowever, certain limitations are acknowledged. Firstly, the reducer and classifier have been trained on a restricted amount of data specific to native English, posing a constraint. Secondly, our model relies on forced-aligned datasets of native speech for effective learning, introducing another limitation. Thirdly, the SCRIBE dataset that we used is a smaller dataset of British English as compared to the TIMTI dataset. Lack of well-annotated datasets, especially on phoneme level is one of our biggest challenges.\nFuture research directions could explore the incorporation of datasets containing non-native English data. This involves re-training the shallow reducer and classifier models using non-native speech data. Furthermore, extending our approach to languages beyond English and evaluating its performance with real-world data from language learners presents an intriguing avenue for exploration. Our work lays the foundation for further experiments in the realm of text independent phone-to-audio alignment, especially in the context of non-native English.\nPart of this work was done during the project REDCALL that is partially funded by a FIRST Entreprise Docteur program from SPW Recherche555https://recherche.wallonie.be/.\nPart of this work was done during the project DEEPCALL that is partially funded by a Win4Doc program from SPW Recherche.\n-\\extralength0cm\nReferences"
        }
    ]
}