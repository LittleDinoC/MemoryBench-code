{
    "title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding",
    "abstract": "Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts.\nTo address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding.\nProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation.\nThis collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM’s representation space and the LM’s input space.\nUnlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language Models (LMs) have achieved impressive successes across diverse domains Brown et al. (2020  ###reference_b7###); Touvron et al. (2023  ###reference_b57###); Liu et al. (2023c  ###reference_b40###). Remarkably, the extensive biological literature in their training data has enabled LMs to excel in text-based protein understanding tasks, such as biological and medical question-answering (QA) Taylor et al. (2022  ###reference_b56###); OpenAI (2023  ###reference_b48###). These results show the potential of using LMs as the natural language interface for biomedical tasks. It further accentuates the importance of harnessing LMs to drive advancements in areas like drug discovery and protein property prediction Kim et al. (2021  ###reference_b28###); Fan et al. (2023  ###reference_b17###).\n###figure_1### ###figure_2### ###figure_3### Here we focus on LM’s capability in text-based protein understanding, which enables textual interpretations of proteins. It can be assessed through diverse downstream tasks, particularly protein-to-text generation Guo et al. (2023  ###reference_b22###) and retrieval Xu et al. (2023  ###reference_b60###). Specifically, the retrieval task is finding an existing text description best matching a particular protein, while the generation task is delineated into two problems, as illustrated in Figure 1  ###reference_###:\n1) protein captioning, where the LM generates a description of a specific protein’s functions, and 2) protein QA, where the LM answers questions about a protein.\nTable 1  ###reference_### overviews prior efforts Liu et al. (2023b  ###reference_b39###); Xu et al. (2023  ###reference_b60###); Taylor et al. (2022  ###reference_b56###); Guo et al. (2023  ###reference_b22###) in this field. Delving into these previous studies, we identify two research gaps:\nLack of Exploration for Protein-to-Text Generation.\nProtein-to-text generation is a conditional generation task Keskar et al. (2019  ###reference_b27###), requiring the LM to perceive proteins as the generation condition.\nPrevious studies Xu et al. (2023  ###reference_b60###); Liu et al. (2023b  ###reference_b39###) based on cross-modal contrastive learning Radford et al. (2021  ###reference_b50###) hardly interpret proteins as the direct inputs to the LM.\nWhile notably two studies, Galactica and ProteinChat, have paid certain explorations, they unfortunately appear to be quite constrained by key limitations.\nSpecifically, Galactica Taylor et al. (2022  ###reference_b56###) incorporates only a limited set of protein sequences in its pretraining data, thereby potentially restricting its capacity for comprehensive protein understanding;\nProteinChat Guo et al. (2023  ###reference_b22###) seeks to project protein representations to text space by training a linear projector, which might prove inadequate in capturing the intricate relations between proteins and texts.\nMissing Quantitative Evaluation. The progress in protein-text modeling is difficult to track without proper benchmarks. As Table 1  ###reference_### illustrates, the quantitative evaluations for these models are mostly missing, posing a challenge to further advancement in this field.\nTo bridge these research gaps, we propose ProtT3: Protein-to-Text Generation for Text-based Protein Understanding. As Figure 2(a)  ###reference_sf1### illustrates, ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a Protein Language Model (PLM) as its protein understanding module, thereby effectively conditioning the protein-to-text generation process. PLMs Madani et al. (2023  ###reference_b44###) are specialized LMs pretrained solely on protein sequences. They can generate powerful protein representations that are instrumental in presenting the proteins’ 3D structures and indicating their potential properties Chen et al. (2023  ###reference_b9###). To enable the LM to understand the PLM’s protein representations, ProtT3 integrates an expressive cross-modal projector – Q-Former Li et al. (2023a  ###reference_b29###) – to map protein representations into the text space of the LM. This design enables the LM to consume proteins as inputs. However, working with a large LM with billions of parameters raises a new challenge of maintaining the efficiency of downstream adaptation. Therefore, we incorporate a LoRA Hu et al. (2022  ###reference_b24###) adapter into the LM for efficient fine-tuning purposes.\nTo facilitate effective protein-text modeling, ProtT3 employs a two-stage training process to enhance protein-text modeling, as outlined in Figure 2(b)  ###reference_sf2###. The first stage involves protein-text retrieval training with three cross-modal tasks Li et al. (2022a  ###reference_b30###): protein-text contrasting, protein-text matching, and protein captioning. This stage not only empowers the cross-modal projector with the capability of protein-text retrieval, but also serves as a “warmup” before the second stage by encouraging the extraction of text-relevant protein features. In the second stage, we connect the cross-modal projector to the LM and conduct protein-to-text generation training.\nOur key contributions are summarized as:\nWe introduce ProtT3, a new framework aiming to bridge the modality gap between texts and proteins. Through a cross-modal projector, ProtT3 jointly uses a PLM for protein understanding and an LM for text processing, enabling effective protein-to-text generation.\nTo set benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein QA, and protein-text retrieval. The datasets, evaluation scripts, and our pretrained checkpoints will be made available online.\nProtT3 achieves state-of-the-art performances across various tasks. For protein captioning, ProtT3 surpasses the baseline by over 10 BLEU-2 scores in the Swiss-Prot Bairoch and Apweiler (2000  ###reference_b4###) and the ProteinKG25 Zhang et al. (2022  ###reference_b62###) datasets. For protein-text retrieval, ProtT3 outperforms baselines by over  in retrieval accuracy on the Swiss-Prot and ProteinKG25 datasets. Lastly, ProtT3 achieves 2.5% improvement of exact match performance for protein QA on the PDB-QA Guo et al. (2023  ###reference_b22###) dataset."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Protein Language Models (PLMs). PLMs are transformer-based LMs pretrained on small corpora of protein sequences for protein understanding and protein generation Lin et al. (2022  ###reference_b37###); Madani et al. (2023  ###reference_b44###); Nijkamp et al. (2022  ###reference_b47###); Chen et al. (2023  ###reference_b9###); Elnaggar et al. (2021  ###reference_b16###); Rives et al. (2021a  ###reference_b51###); Meier et al. (2021  ###reference_b46###). Similar to LMs for texts, PLMs are pretrained by the objective of masked language modeling Lin et al. (2022  ###reference_b37###) or auto-regressive modeling Madani et al. (2023  ###reference_b44###). PLMs have demonstrated promising performances on downstream tasks of 3D structure prediction and protein property prediction Lin et al. (2022  ###reference_b37###); Chen et al. (2023  ###reference_b9###). However, they cannot process texts due to the absence of text in their pretraining data."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Model Architecture",
            "text": "Here we introduce ProtT3’s three key components: a PLM for protein understanding, an LM for text processing, and a cross-modal projector to bridge the modality gap between the first two components.\nProtein Language Model (PLM).\nWe utilize ESM-2 Lin et al. (2022  ###reference_b37###) to encode protein sequences of amino acids. ESM-2 is an encoder-only transformer LM Vaswani et al. (2017  ###reference_b59###) pretrained on 60M protein sequences by masked language modeling Devlin et al. (2019  ###reference_b12###). PLMs have shown promising performances for protein folding Lin et al. (2022  ###reference_b37###), multiple sequence alignment Rives et al. (2021b  ###reference_b52###), and protein property prediction Xu et al. (2023  ###reference_b60###), demonstrating their effectiveness for capturing protein characteristics. For efficiency, we freeze ESM-2’s weights in our training process.\nLanguage Model (LM). We choose Galactica Taylor et al. (2022  ###reference_b56###) as the base LM. Galactica is a decoder-only transformer LM pretrained on a large collection of scientific papers, spanning disciplines like biology and medicine. Notably, Galactica has demonstrated a high-level understanding of protein concepts through its promising performances in biomedical QA benchmarks Jin et al. (2019  ###reference_b25###); Hendrycks et al. (2021  ###reference_b23###). Furthermore, Galactica includes a small set of protein sequences in its pretraining data, and shows a capability for understanding protein sequences through the task of protein keyword prediction Taylor et al. (2022  ###reference_b56###). Therefore, we also leverage Galactica as a baseline for protein-to-text generation to ablate the effectiveness of incorporating an additional PLM for protein understanding.\nCross-Modal Projector. We employ a cross-modal projector based on Q-Former Li et al. (2023a  ###reference_b29###) to bridge the modality gap between the PLM and the LM. Q-Former has demonstrated promising performances in vision-language tasks. As Figure 3  ###reference_###a illustrates, Q-Former consists of two transformers: one for protein encoding and another for text processing.\nSpecifically, the protein transformer maintains  learnable query tokens  as inputs. These query tokens can interact with the PLM through the cross-attention modules, in order to extract protein features. We denote the protein transformer’s output as , containing protein features. For text input, the text transformer adds a [CLS] token at the beginning, and uses the [CLS] token’s output as the text representation. Both transformers share the self-attention to enable interactions between proteins and texts. Details are in Section 4.1  ###reference_###.\nQ-Former’s weights are initialized from PubMedBERT-Abstract Gu et al. (2022  ###reference_b21###), a BERT LM pretrained on paper abstracts from the PubMed database111https://pubmed.ncbi.nlm.nih.gov/  ###reference_pubmed.ncbi.nlm.nih.gov/###. It has shown promising performances in understanding biomedical concepts under the BLURB benchmark Gu et al. (2022  ###reference_b21###). The cross-attention module is added into the Q-Former every two layers and is randomly initialized."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Training Method",
            "text": "In this section, we introduce ProtT3’s two training stages: protein-text retrieval and protein-to-text generation. The training process leverages a dataset of protein-text pairs , where  is a protein sequence and  is the corresponding text sequence."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Stage 1: Protein-Text Retrieval Training",
            "text": "Inspired by BLIP Li et al. (2022a  ###reference_b30###, 2023a  ###reference_b29###), we jointly employ three objectives for protein-text retrieval training: protein-text contrasting, protein-text matching, and protein captioning. These objectives are tailored for Q-Former’s architecture, training it to extract protein features that are relevant to the text descriptions. This stage empowers the cross-modal projector with retrieval ability, and also serves as a “warmup” before the next stage.\nProtein-Text Contrasting (PTC). We employ cross-modal contrastive learning Radford et al. (2021  ###reference_b50###) to align the protein representation and text representation from the Q-Former. As illustrated in Figure 3  ###reference_###b, Q-Former’s self-attention module separately processes the query tokens and text tokens without any interaction. This enforces the query tokens to extract protein features from the PLM, in order to generate protein representations that align with the corresponding text representations in contrastive learning.\nFormally, let  be a batch of protein-text pairs. We denote the protein representations as , where  is the reprentation of the protein ’s -th query token; and denote text ’s representation as , which is the [CLS] token’s output.\nProtein-text similarity is measured by the maximum similarity between  and each row of Z. The contrastive learning loss  can then be written as:\nwhere  is the cosine similarity; Temperature  is empirically set to .\nProtein-Text Matching (PTM). PTM is a binary classification objective aiming to discriminate whether a protein-text pair matches or not. Unlike PTC which computes protein-text similarity by applying cosine similarity on their output representations, PTM can obtain more fine-grained protein-text similarity. As illustrated in Figure 3  ###reference_###b, PTM feeds the query tokens and text tokens into the same self-attention module, allowing them to interact at the Q-Former’s early layers. In this way, the query tokens can capture information on both proteins and texts. The mean pooling of query tokens’ representations is then fed into a linear classifier for PTM prediction. During training, for each positive protein-text pair , we randomly sample two negative pairs:  and . Let  be the Q-Former’s predicted probability that  is a matched pair, the PTM loss function  can be written as:\nIn experiments, we find that PTM surpasses PTC for protein-text retrieval. However, PTM incurs a higher computational cost for encoding every protein-text pair. To balance performance and speed, we use PTC to obtain the top  ranked candidates, and then use PTM for re-ranking.\nProtein Captioning (PCap). Protein captioning trains the Q-Former to generate text descriptions of given proteins. As shown in Figure 3  ###reference_###b, we apply a special masking strategy for this objective: 1) Bi-directional attention mask is applied to query tokens, enabling them to interact with each other but not text tokens; 2) Causal attention mask is used for text tokens, allowing them to attend query tokens and the preceding text tokens, but not the following text tokens. This design ensures the text tokens extract protein features exclusively from the query tokens, because they cannot directly interact with the proteins. Meanwhile, the query tokens are enforced to extract protein features through cross-attentions, satisfying the informational needs of protein captioning. Let  be the probability that Q-Former generate text  given protein . The protein captioning loss  can be written as:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Stage 2: Protein-to-Text Generation Training",
            "text": "In this stage, we train ProtT3 for protein-to-text generation. As Figure 2(a)  ###reference_sf1### illustrates, we connect the cross-modal projector to the LM, feeding the protein representations Z into the LM, so as to condition the text generation process by protein information. Note that, we use a linear layer to project Z to the same dimension of the LM’s input.\nWe train ProtT3 for each generation dataset separately, and append different text prompts after the protein representations to further control the generation process. For example, we use the text prompt of “Describe this protein’s function” for protein captioning, and “Question: Does this protein contain polymer entities? Answer:” for protein QA. For training, we use the same loss as the protein captioning task in the previous section.\nFor training efficiency, we incorporate Low-Rank Adaptation (LoRA) Hu et al. (2022  ###reference_b24###) adapters into the LM. LoRA adds pairs of trainable rank decomposition matrices into the selected weights of the LM. For example, LoRA modifies a pretrained weight  by adding a pair of matrices  and :\nwhere  is kept frozen and only the newly added BA is tuned. By using a small rank , LoRA can adapt an LM to a new task while requiring little memory for storing gradients. This method has shown comparable performances to full-parameter fine-tuning Hu et al. (2022  ###reference_b24###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We begin by introducing the datasets of protein-text pairs, followed by the experimental results. Details on experimental settings, such as hyperparameters and baseline implementations, are provided in Appendix C  ###reference_###. In the experiments, ProtT3 utilizes  as the base LM and  as the PLM."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Protein-Text Dataset Collection",
            "text": "In this section, we detail the protein-text pair datasets and the data processing procedure employed in our study. For all the used datasets, we discard protein sequences longer than  tokens, and carefully split the datasets to ensure no overlap between train/valid/test sets. Dataset statistics are available in Table 2  ###reference_###, and details are in Appendix A  ###reference_###.\nSwiss-Prot Bairoch and Apweiler (2000  ###reference_b4###) is a protein sequence database with text annotations. We process the dataset following Xu et al. (2023  ###reference_b60###), but excluding protein names from the text annotations to prevent information leakage. The resulting text descriptions concatenate the annotations of protein functions, locations, and families.\nProteinKG25 Zhang et al. (2022  ###reference_b62###) is a knowledge graph derived from the Gene Ontology Aleksander et al. (2023  ###reference_b3###) database. We transform its triples into free texts by first aggregating the triples of the same protein, and then filling the protein information into a pre-defined text template.\nPDB-QA Guo et al. (2023  ###reference_b22###) is a protein single-turn QA dataset derived from RCSB PDB222https://www.rcsb.org  ###reference_www.rcsb.org###. It includes 30 question templates about proteins’ structures, properties, and supplementary information. As shown in Table 3  ###reference_###, to enable a fine-grained evaluation, we categorize the questions into four types, based on the answer’s format (string or number) and content focus (structure/property or supplementary information).\nIt is worth noting that supplementary information is hard to predict given the protein sequence alone.\nTraining Pipeline. In training stage 1, we train ProtT3 on the combination of the Swiss-Prot and ProteinKG25 datasets for protein-text retrieval. In training stage 2, we load the checkpoint from stage 1 and conduct separate fine-tuning on the three datasets for protein-to-text generation tasks."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Protein Captioning",
            "text": "We evaluate protein captioning performances on the Swiss-Prot and ProteinKG25 datasets. Following Edwards et al. (2022  ###reference_b15###), we use the evaluation metrics of BLEU Papineni et al. (2002  ###reference_b49###), ROUGE Lin (2004  ###reference_b36###), and METEOR Banerjee and Lavie (2005  ###reference_b5###). Additionally, we report the percentage that the prediction exactly matches the ground truth annotation. For comparison and ablation study purposes, we report the performances of LoRA fine-tuned , and ProtT3’s two variants: 1) ProtT3 w/ MLP Proj., which replaces ProtT3’s cross-modal projector by an MLP, following Liu et al. (2023a  ###reference_b38###); and 2) ProtT3 w/o stage 1, which skips ProtT3’s training stage 1. We do not compare with ProteinChat Guo et al. (2023  ###reference_b22###) because it requires 3D protein structures, which are unavailable for these two datasets.\nTable 4  ###reference_### presents the results. We observe the following: 1) ProtT3 and its variants substantially outperform the LoRA fine-tuned Galactica, with ProtT3 showing a 10-point improvement in BLEU-2 scores. This underscores the significance of incorporating a PLM for protein understanding and ProtT3’s effectiveness in undertanding protein inputs. 2) ProtT3 consistently surpasses its two variants on both datasets, highlighting the advantages of using a Q-Former projector and training stage 1.\n###figure_5### ###figure_6###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Protein-Text Retrieval",
            "text": "We evaluate protein-text retrieval performances on the Swiss-Prot and ProteinKG25 datasets. Following Su et al. (2022  ###reference_b55###), our evaluation includes protein-text retrieval in a batch of  random samples and retrieval in the entire test set. We use Accuracy and Recall@20 as the evaluation metrics. For retrieval, ProtT3 first retrieves the top  candidates by PTC, and then uses PTM for re-ranking. For comparison, we employ ProtST Xu et al. (2023  ###reference_b60###) and ProteinCLAP Liu et al. (2023b  ###reference_b39###) as baselines, and present ProtT3 w/o PTM and ProtT3 w/o PCap for ablation studies.\nTable 5  ###reference_### shows the results. We observe that: 1) ProtT3 outperforms baselines by over  accuracy for retrieval in the test set, highlighting its capability in aligning proteins with corresponding text descriptions. 2) PTM improves ProtT3’s retrieval accuracy in test set by  on both datasets. This is because PTM allows protein and text information to interact at the Q-Former’s early layers, achieving more fine-grained protein-text similarity measurements. 3) PCap improves ProtT3’s retrieval accuracy by . This is because\nPCap encourages the query tokens to extract protein information most\nrelevant to the text input, therefore aiding in protein-text alignment."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Protein Question-Answering",
            "text": "We evaluate protein QA performances on the PDB-QA dataset. Considering that the answers in PDB-QA typically consist of  words, we select exact match as the evaluation metric. For comparison, we employ ProteinChat Guo et al. (2023  ###reference_b22###) and LoRA fine-tuned  as baselines. We also assess a  that consumes only questions without proteins during training and prediction (i.e., LoRA ft, Q-only). This baseline measures the dataset’s uni-modal bias Cadène et al. (2019  ###reference_b8###): the proportion of questions that can be correctly answered without looking at proteins.\nThe results are shown in Table 6  ###reference_###. We observe that: 1) ProtT3 surpasses baselines by 2.5% overall, and consistently outperforms them in predicting protein structures and properties. This demonstrates ProtT3’s multi-modal understanding ability for the proteins and the textual questions.\n2) The baseline consuming question only (i.e., LoRA ft, Q-only) presents comparable performance, indicating that the dataset has a substantial uni-modal bias. This finding suggests an opportunity to explore debias techniques Mahabadi et al. (2020  ###reference_b45###) in future studies.\n3) ProteinChat underperforms other methods, possibly because it only trains a linear layer between the frozen PLM and LM. The linear layer is insufficient to map the protein representations into the text space of the LM, and adapt the LM to QA.\n4) Models perform worse on questions about numbers or supplementary information. This observation suggests the potential benefit of augmenting these models with external tools Schick et al. (2023  ###reference_b53###) like calculators and search engines."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Ablation Studies on Pretrained Models",
            "text": "Here we conduct ablation studies on the pretrained models used in our method. Specifically, we ablate the impact of different PLMs in training stage 1, and ablate different LMs in training stage 2.\nAblating PLMs. In training stage 1, we replace the ESM-2 protein encoder with its smaller variants, namely ESM-2 and ESM-2, to evaluate their performances for protein-text retrieval. As shown in Table 7  ###reference_###, we can observe that retrieval performance increases monotonically with model size, a trend consistent with previous observations in the LM domain.\nAblating LMs. In training stage 2, we replace Galactica to Phi-1.5 Li et al. (2023c  ###reference_b34###). Unlike Galactica, Phi-1.5 is pretrained on general domain data but not focusing on scientific literature. Table 8  ###reference_### shows protein captioning performance on the ProteinKG25 Zhang et al. (2022  ###reference_b62###) dataset. We can observe that Galactica significantly outperforms Phi-1.5 for protein captioning, although they have similar sizes. We attribute this performance gap to their pretraining corpus, with Galactica performing better for pretraining on more scientific literature."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Examples of Protein-to-Text Generation",
            "text": "Figure 4  ###reference_### shows three examples of protein captioning.\nIn the first example, ProtT3’s caption is more accurate by correctly identifying the DMRT family, while Galactica does not. In the second example, both models fail to identify the protein’s function. Nevertheless, ProtT3’s prediction regarding the protein family is more accurate. The third example shows that both models successfully predict the subcellular location and protein family. ProtT3 goes a step further by predicting the protein’s function, which is closer to the ground truth description.\nFigure 5  ###reference_### shows three examples of protein QA. We can observe that both ProtT3 and Galactica answer the first two questions about protein property/structure correctly, and fail on the third question, which requires a numerical answer. On the other hand, ProteinChat struggles with all three questions, failing to answer each of them."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Works",
            "text": "In this work, we propose ProtT3, a new protein-text modeling framework. ProtT3 aims to facilitate text-based protein understanding via protein-to-text generation and protein-text retrieval. To achieve this, ProtT3 integrates a PLM into an LM to enhance the LM’s protein understanding ability. A cross-modal projector enables this integration by bridging the modality gap between the two modules. To promote future research, we set benchmarks for protein-text modeling tasks, including protein captioning, protein QA, and protein-text retrieval, where ProtT3 significantly outperforms existing baselines.\nLooking ahead, we plan to explore enabling LMs to understand 3D protein structures Li et al. (2024  ###reference_b31###) and apply this understanding to more tasks of drug discovery, property prediction Liu et al. (2023d  ###reference_b41###); Li et al. (2022b  ###reference_b33###, 2023b  ###reference_b32###), molecule generation Luo et al. (2024  ###reference_b42###), and OOD generalization Fang et al. (2023  ###reference_b19###, 2024a  ###reference_b18###)."
        }
    ]
}