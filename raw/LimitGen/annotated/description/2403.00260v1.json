{
    "title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents",
    "abstract": "This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text.\nThe complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations.\nTo address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Research publications are the main source for the discovery of new materials in the field of materials science, providing a vast array of essential data. Creating structured databases from these publications enhances discovery efficiency, as evidenced by AI tools like GNoME (Merchant et al., 2023  ###reference_b20###). Yet, the unstructured format of journal data complicates its extraction and use for future discoveries (Horawalavithana et al., 2022  ###reference_b12###). Furthermore, the manual extraction of material details is inefficient and prone to errors, underlining the necessity for automated systems to transform this data into a structured format for better retrieval and analysis (Yang, 2022  ###reference_b37###).\n###figure_1### Scientific papers on polymer nanocomposites (PNCs) provide essential details on sample compositions, key to understanding their properties. PNCs, which blend polymer matrices with nanoscale fillers, are significant in materials science for their customizable mechanical, thermal, and electrical characteristics. The variety in PNCs comes from different matrix and filler combinations that modify the properties. However, extracting this data poses challenges due to its distribution across texts, figures, and tables, and the complexity of -ary relationships defining each sample. An example in Figure 1  ###reference_### illustrates how sample details can be spread over various paper sections.\nIn this paper, we construct PNCExtract, a benchmark designed for extracting PNC sample lists from scientific articles. PNCExtract focuses on the systematic extraction of -ary relations across different parts of full-length PNC articles, capturing the unique combination of matrix, filler, and composition in each sample. Many works have explored -ary relation extraction from materials science literature (Dunn et al., 2022  ###reference_b6###; Song et al., 2023a  ###reference_b25###, b  ###reference_b26###; Xie et al., 2023  ###reference_b36###; Cheung et al., 2023  ###reference_b1###) and other domains (Giorgi et al., 2022  ###reference_b7###). However, these studies primarily target abstracts and short texts, not addressing the challenge of extracting information from the entirety of full-length articles. PNCExtract addresses this by requiring models to process entire articles, identifying information across all sections, a challenge noted by Hira et al. (2023  ###reference_b10###).\nCompared to other document-level information extraction (IE) datasets like SciREX (Jain et al., 2020  ###reference_b15###), PubMed (Jia et al., 2019b  ###reference_b17###), and NLP-TDMS (Hou et al., 2019  ###reference_b13###) which also demand the analysis of entire documents for -ary relation extraction, our dataset marks the first initiative within the materials science domain. This distinction is important due to the unique challenges of IE in materials science, particularly with polymers. The field features a complex nomenclature with chemical compounds and materials having various identifiers such as systematic names, common names, trade names, and abbreviations, all with significant variability and numerous synonyms for single entities (Swain and Cole, 2016  ###reference_b27###). Furthermore, there is a scarcity of annotated datasets with detailed information, which complicates the creation of effective IE models in this area.\nIn light of these challenges, our dataset is designed for a generative task to navigate the complexities of fully annotating entire PNC papers, which involve annotating named entity spans, coreferences, and negative examples (entity pairs without a relation). The complexity of PNC papers, due to their various entities and samples, makes manual annotation both time-consuming and prone to errors. Consequently, encoder-only models, which require extensive annotations, fall short for our purposes. In Table 1  ###reference_###, we compare PNCExtract with previous IE approaches in the scientific domain.\nWe introduce a dual-metric evaluation system comprising a partial metric for detailed analysis of each attribute within an -ary extraction and a strict metric for assessing overall accuracy. This approach distinguishes itself from prior works in materials science, which either focused on the evaluation of binary relations (Dunn et al., 2022  ###reference_b6###; Xie et al., 2023  ###reference_b36###; Song et al., 2023a  ###reference_b25###; Wadhwa et al., 2023  ###reference_b33###) or used strict evaluation criteria (Cheung et al., 2023  ###reference_b1###) without recognizing partial matches.\nWe further explore different prompting strategies, including one that aligns with the principles of Named Entity Recognition (NER) and Relation Extraction (RE) which involves a two-stage pipeline, as well as an end-to-end method to directly generate the -ary object. We find that the E2E approach works better in terms of both accuracy and efficiency.\nMoreover, we present a simple extension to the self-consistency technique (Wang et al., 2023b  ###reference_b35###) for list-based predictions. Our findings demonstrate that this approach improves the accuracy of sample extraction. Since the extended length of articles often exceeds the context limits of some LLMs, we also explore condensing them through a dense retriever (Ni et al., 2022  ###reference_b21###) to extract segments most relevant to specific queries. Our findings indicate that condensing documents generally enhances accuracy. Since existing document-level IE models (Jain et al., 2020  ###reference_b15###; Zhong and Chen, 2021  ###reference_b40###) are not suited for our task, we employ GPT-4 with our E2E prompting on the SciREX dataset and benchmark it against the baseline model. Our analysis shows that GPT-4, even in a zero-shot setting, outperforms the baseline models that were trained with extensive supervision.\nLastly, we discuss three primary challenges encountered when using LLMs for PNC sample extraction. Code for reproducing all experiments is available at https://github.com/ghazalkhalighinejad/PNCExtract  ###reference_Extract###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "PNCExtract Benchmark",
            "text": "In this section, we first describe our dataset, including the problem definition, and the dataset preparation. Then we describe our evaluation method for the described task.\nWe curate our dataset using the NanoMine data repository (Zhao et al., 2018  ###reference_b39###). NanoMine is a PNC data repository structured around an XML-based schema designed for the representation and distribution of nanocomposite materials data. The NanoMine database is manually curated using Excel templates provided to materials researchers. NanoMine database currently contains a list of  full-length scholarly articles and their corresponding PNC sample lists. While NanoMine includes various subfields, our study focuses on the “Materials Composition” section. This section comprehensively details the characteristics of constituent materials in nanocomposites, including the polymer matrix, filler particles, and their compositions (expressed in volume or weight fractions). The reason for this focus is that determining which sample compositions are studied in a given paper is the essential first step toward identifying and understanding more complex properties of PNCs. Out of the  articles, we focus on  and disregard the rest due to having inconsistent format (see Appendix A  ###reference_###). These  articles contain a total of  samples. For each sample, we retain  out of the  total attributes in the Materials Composition of NanoMine (see Appendix B  ###reference_### for details).\nDocument-level information extraction requires understanding the entire document to accurately annotate entities, their relations, and saliency. These make the annotation of scientific articles time-consuming and prone to errors. We found that NanoMine also contains errors. Given the challenge of reviewing all  samples and reading through  articles, we adopted a semi-automatic approach to correct samples. Specifically, for an article, we consider both the predicted and ground truth sample list of a document. Using our partial metric (detailed in Section 2.3  ###reference_.SSS0.Px3###), we match predicted samples with their ground-truth counterparts and assign a similarity score to each pair. Matches are classified as exact, partial, or unmatched—either true samples or predictions. We then focus on re-annotating samples with the most significant differences between prediction and ground truth, especially those partial matches with lower similarity scores and unmatched samples. This method accelerates re-annotation by directing annotators towards specific attributes and samples based on GPT-4 predictions. Following this strategy, we made three types of adjustments to the dataset: deleting  samples, adding , and editing  entities.111This work includes contributions from polymer experts, under whose mentorship all authors received their training.(See Appendix G  ###reference_### for details).\nTo accurately calculate the partial and strict metrics, standardizing predictions is essential. The variability in polymer name expressions in scientific literature makes uniform evaluation challenging. For example, “silica” and “silicon dioxide” are different terms for the same filler. Our dataset uses a standardized format for chemical names. To align the predicted names with this standard, we use resources by Hu et al. (2021  ###reference_b14###), which list  matrix names with their standard names, abbreviations, synonyms, and trade names, as well as  filler names with their standard names. We standardize predicted chemical names by matching them to the closest names in these lists and converting them to their standard forms.\nFurthermore, our dataset exclusively uses numerical values to represent compositions (e.g., a composition of “” should be listed as “”). Predictions in percentage format (like “”) are thus converted to the numerical format to align with the dataset’s representation.\nOur evaluation incorporates an attribute aggregation method. For both the “Matrix” and “Filler” categories, a prediction is considered accurate if the model successfully identifies either the chemical name or the abbreviation. For the “Composition”, a correct prediction may be based on either the “Filler Composition Mass” or the “Filler Composition Volume”. This approach allows for a broader assessment, capturing any correct form of attribute identification without focusing on the finer details of each attribute.\nThis metric employs the F score in its calculation, which proceeds in two steps. Initially, an accuracy score is computed for each pair of predicted and ground truth samples where we compute the fraction of matches in the <Matrix, Filler, Composition> trio across the two samples.\nThis process results in  score combinations, where  and  represent the counts of predicted and ground truth samples. The next step involves translating these comparisons into an assignment problem within a bipartite graph. Here, one set of vertices symbolizes the ground truth samples, and the other represents the predicted samples, with edges denoting the F scores between pairs. The objective is to identify a matching that optimizes the total F score,\nwhich can be computed using\nthe Kuhn-Munkres algorithm (Kuhn, 1955  ###reference_b18###).\nin  time (where ).\nNote that if , a one-to-one match for each prediction may not be necessary. Once matching is done, we count all the correct, false positive, and false negative predicted attributes (the attributes of all the unmatched predicted samples and ground-truth samples are considered false positives and false negatives, respectively). Subsequently, we calculate the micro-average Precision, Recall, and F.\nFor a stricter assessment, a sample is labeled correct only if it precisely matches one in the ground truth. Predictions not in the ground truth are false positives, and missing ground truth samples are false negatives. This metric emphasizes exact match accuracy."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Definition",
            "text": "We define our dataset as , where each  is a peer-reviewed paper included in our study. Corresponding to each paper , there is an associated list of samples , comprising various PNC samples. Formally,  is defined as , where  represents the -th PNC sample in the sample list of the -th paper, and  denotes the total number of PNC samples in . Each sample  is a JSON object with six entries: Matrix Chemical Name, Matrix Chemical Abbreviation, Filler Chemical Name, Filler Chemical Abbreviation, Filler Composition Mass, and Filler Composition Volume. Table 2  ###reference_### presents the count of samples with each attribute marked as non-null. The primary task involves extracting a set of samples  from a given paper ."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Dataset Preparation",
            "text": "We curate our dataset using the NanoMine data repository (Zhao et al., 2018  ###reference_b39###  ###reference_b39###). NanoMine is a PNC data repository structured around an XML-based schema designed for the representation and distribution of nanocomposite materials data. The NanoMine database is manually curated using Excel templates provided to materials researchers. NanoMine database currently contains a list of  full-length scholarly articles and their corresponding PNC sample lists. While NanoMine includes various subfields, our study focuses on the “Materials Composition” section. This section comprehensively details the characteristics of constituent materials in nanocomposites, including the polymer matrix, filler particles, and their compositions (expressed in volume or weight fractions). The reason for this focus is that determining which sample compositions are studied in a given paper is the essential first step toward identifying and understanding more complex properties of PNCs. Out of the  articles, we focus on  and disregard the rest due to having inconsistent format (see Appendix A  ###reference_###  ###reference_###). These  articles contain a total of  samples. For each sample, we retain  out of the  total attributes in the Materials Composition of NanoMine (see Appendix B  ###reference_###  ###reference_### for details).\nDocument-level information extraction requires understanding the entire document to accurately annotate entities, their relations, and saliency. These make the annotation of scientific articles time-consuming and prone to errors. We found that NanoMine also contains errors. Given the challenge of reviewing all  samples and reading through  articles, we adopted a semi-automatic approach to correct samples. Specifically, for an article, we consider both the predicted and ground truth sample list of a document. Using our partial metric (detailed in Section 2.3  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###), we match predicted samples with their ground-truth counterparts and assign a similarity score to each pair. Matches are classified as exact, partial, or unmatched—either true samples or predictions. We then focus on re-annotating samples with the most significant differences between prediction and ground truth, especially those partial matches with lower similarity scores and unmatched samples. This method accelerates re-annotation by directing annotators towards specific attributes and samples based on GPT-4 predictions. Following this strategy, we made three types of adjustments to the dataset: deleting  samples, adding , and editing  entities.111This work includes contributions from polymer experts, under whose mentorship all authors received their training.(See Appendix G  ###reference_###  ###reference_### for details)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Metrics",
            "text": "Our task involves evaluating the performance of our model in predicting PNC sample lists. One natural approach, also utilized by Cheung et al. (2023  ###reference_b1###), is to verify if there is an exact match between the predicted and the ground-truth samples. This method, however, has a notable limitation, particularly due to the numerous attributes that define a PNC sample. Under such strict evaluation criteria, a predicted sample is considered entirely incorrect if even one attribute is predicted inaccurately, which can be too strict considering the complexity and attribute-rich nature of PNC samples.\nHence, we also propose a partial metric which rewards predicted samples\nfor partial matches to a ground truth sample.\nHowever, computing such a metric first requires identifying the optimal\nmatching between the predicted and ground truth sample lists,\nfor which we employ a maximum weight bipartite matching algorithm.\nThis approach acknowledges the accuracy of a prediction even if not all attributes are perfectly matched.\nAdditionally, we also apply a strict metric, similar to the approach of Cheung et al. (2023  ###reference_b1###), where a prediction is considered correct only if it perfectly matches with the ground truth across all attributes of a PNC sample.\nTo accurately calculate the partial and strict metrics, standardizing predictions is essential. The variability in polymer name expressions in scientific literature makes uniform evaluation challenging. For example, “silica” and “silicon dioxide” are different terms for the same filler. Our dataset uses a standardized format for chemical names. To align the predicted names with this standard, we use resources by Hu et al. (2021  ###reference_b14###  ###reference_b14###), which list  matrix names with their standard names, abbreviations, synonyms, and trade names, as well as  filler names with their standard names. We standardize predicted chemical names by matching them to the closest names in these lists and converting them to their standard forms.\nFurthermore, our dataset exclusively uses numerical values to represent compositions (e.g., a composition of “” should be listed as “”). Predictions in percentage format (like “”) are thus converted to the numerical format to align with the dataset’s representation.\nOur evaluation incorporates an attribute aggregation method. For both the “Matrix” and “Filler” categories, a prediction is considered accurate if the model successfully identifies either the chemical name or the abbreviation. For the “Composition”, a correct prediction may be based on either the “Filler Composition Mass” or the “Filler Composition Volume”. This approach allows for a broader assessment, capturing any correct form of attribute identification without focusing on the finer details of each attribute.\nThis metric employs the F score in its calculation, which proceeds in two steps. Initially, an accuracy score is computed for each pair of predicted and ground truth samples where we compute the fraction of matches in the <Matrix, Filler, Composition> trio across the two samples.\nThis process results in  score combinations, where  and  represent the counts of predicted and ground truth samples. The next step involves translating these comparisons into an assignment problem within a bipartite graph. Here, one set of vertices symbolizes the ground truth samples, and the other represents the predicted samples, with edges denoting the F scores between pairs. The objective is to identify a matching that optimizes the total F score,\nwhich can be computed using\nthe Kuhn-Munkres algorithm (Kuhn, 1955  ###reference_b18###  ###reference_b18###).\nin  time (where ).\nNote that if , a one-to-one match for each prediction may not be necessary. Once matching is done, we count all the correct, false positive, and false negative predicted attributes (the attributes of all the unmatched predicted samples and ground-truth samples are considered false positives and false negatives, respectively). Subsequently, we calculate the micro-average Precision, Recall, and F.\nFor a stricter assessment, a sample is labeled correct only if it precisely matches one in the ground truth. Predictions not in the ground truth are false positives, and missing ground truth samples are false negatives. This metric emphasizes exact match accuracy."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Modeling Sample List Extractions from Articles with LLMs",
            "text": "As mentioned in Section 1  ###reference_###, our dataset is designed for a generative task, making encoder-only models unsuitable for two main reasons. First, these models demand extensive annotations, such as named entity spans, coreferences, and negative examples, a process that is both time-consuming and error-prone. Second, encoder-only models struggle with processing long documents efficiently. While some studies have successfully used these models for long documents (Jain et al., 2020  ###reference_b15###), they had access to significantly larger datasets. Our dataset, however, contains detailed domain-specific information, making it challenging to obtain a similarly extensive dataset.\n###figure_2### Consequently, within a zero-shot context222The context length is prohibitive for attempting few-shot approaches., we explore two prompting methods: Named Entity Recognition plus Relation Extraction (NER+RE) and an End-to-End (E2E) approach."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "NER+RE Prompt",
            "text": "Building on previous research (Peng et al., 2017  ###reference_b23###; Jia et al., 2019a  ###reference_b16###; Viswanathan et al., 2021  ###reference_b32###), which treated -ary relation extraction as a binary classification task, our NER+RE method treats RE as a question-answering process, following the approach in Zhang et al. (2023  ###reference_b38###). This process is executed in two stages. Initially, the model identifies named entities within the text. Subsequently, it classifies -ary relations by transforming the task into a series of yes/no questions about these entities and their relations. For evaluation, we apply only the strict metric, as the partial metric is not suitable in this binary classification context.333While partial evaluation is theoretically possible by considering all potential samples identified in the NER step, such an approach would yield limited insights.\nThe NER+RE approach becomes computationally expensive during inference, especially as the number of entities increases. This leads to an exponential growth in potential combinations, expanding the candidate space for valid compositions and consequently extending the inference time."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "End-to-End Prompt",
            "text": "To address this challenge, we develop an End-to-End (E2E) prompting strategy that directly extracts JSON-formatted sample data from articles. This method is designed to efficiently handle the complexity and scale of extracting -ary relations from scientific texts, bypassing the limitations of binary classification frameworks in this context."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Self-Consistency",
            "text": "The self-consistency method (Wang et al., 2023b  ###reference_b35###), aims to enhance the reasoning abilities of LLMs. Originally, this method relied on taking a majority vote from several model outputs. For our purposes, since the output is a set of answers rather than a single one, we apply the majority vote principle to the elements within these sets.\nWe generate  predictions from the model, each at a controlled temperature of . Our objective is to identify which samples appear frequently across these multiple predictions as a sign of higher confidence from the model.\nDuring evaluation, each model run generates a list of predicted samples from a specific paper. We refer to each list as the -th prediction, denoted . For each predicted element , we determine its match score , by counting how frequently it appears across all predictions . This score can vary from , meaning it appeared in only one prediction, to , indicating it was present in all predictions.\nWe then apply a threshold  to filter the samples. Those with a  at or above  are retained, as they were consistently predicted by the model. Samples falling below this threshold suggest less confidence in the prediction and are removed."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Condensing Articles with Dense Retrieval",
            "text": "LLMs, such as LLaMA2 with its token limit of , face challenges in maintaining performance with longer input lengths. Recent advancements have extended these limits (Dacheng Li* and Zhang, 2023  ###reference_b3###; Tworkowski et al., 2023  ###reference_b31###); however, an increase in input length often leads to a decline in model performance. This raises the question of whether condensing articles could serve as an effective strategy to address such limitations. We, therefore, employ the Generalizable T5-based Dense Retrievers (GTR-large) (Ni et al., 2022  ###reference_b21###) to retrieve relevant parts of the documents.\nThis process involves dividing each document  into segments () and formulating four queries () to extract targeted information regarding an entity.444The queries are: “What chemical is used in the polymer matrix?”, “What chemical is used in the polymer filler?”, “What is the filler mass composition?”, and “What is the filler volume composition?”. On average, each segment consists of 60 tokens.\nWe then calculate the similarity between each pair of segments and queries (, ). For every query , we select the top  segments () based on their similarity scores. These top segments from all four queries are then combined to form a condensed version of the original document ()."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our experiments, we employ LLaMA-7b-Chat (Touvron et al., 2023  ###reference_b30###), LongChat-7B-16K (Dacheng Li* and Zhang, 2023  ###reference_b3###), Vicuna-7B-v1.5 and Vicuna-7B-v1.5-16K (Chiang et al., 2023  ###reference_b2###), and GPT-4 Turbo (OpenAI, 2023  ###reference_b22###). The LongChat-7B-16K and Vicuna-7B-16K models are fine-tuned for context lengths of 16K tokens, and GPT-4 Turbo for 128K tokens.\nWe divide our dataset into  validation articles and  test articles. We assess the performance using micro average Precision, Recall, and F1 scores, considering both strict and partial metrics at the sample and property levels. We also compare two different prompting strategies NER+RE and E2E. Moreover, we consider the self-consistency technique.\nTable 4  ###reference_### demonstrates that shortening documents proves beneficial in most cases. Additionally, Figure 3  ###reference_### shows the trend of partial F scores as document length increases. We observe that GPT-4’s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top  segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\nAdditionally, Table 5  ###reference_### provides bootstrap analysis from  resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents.\n###figure_3### The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is  sec/article, faster than  sec/article for GPT-4 Turbo (NER+RE).\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for  on the validation set (see Appendix D  ###reference_.SSS0.Px3###). Based on that, we employ  and  predictions on the test set. Table 4  ###reference_### shows that self-consistency enhances the strict and partial F.\nAdopting the partial metric has several advantages. First, it helps identify specific challenge areas. For example, in Table 6  ###reference_###, we show the model faces the most challenges in accurately predicting Composition. Furthermore, human annotations for PNC samples are often error-prone (Himanen et al., 2019  ###reference_b9###; McCusker et al., 2020  ###reference_b19###), hence one potential use of an LLM like GPT-4 would be to identify errors and send them back for re-annotation. The partial metric can help prioritize which samples to re-annotate.\nNanoMine aggregates samples from the literature, including those presented in tables and visual elements within research articles. As demonstrated in the first example of Figure 4  ###reference_###, a sample is derived from the inset of a graph. Our present approach relies solely on language models. Future research could focus on advancing models to extract information from both textual and visual data.\nThe composition of PNC includes a variety of components such as hardeners and surface treatment agents. A common issue in our model’s predictions is incorrectly identifying these auxiliary components as the main attributes. For example, the second row in Figure 4  ###reference_### shows the model predicting the filler material along with its surface treatments instead of recognizing the filler by itself.\nThe expression of chemical names is inherently complex, with multiple names often existing for the same material. In some cases, predicted chemical names are conceptually accurate yet challenging to standardize. This suggests the necessity for more sophisticated approaches that can handle the diverse and complex representations of chemical compounds. The third example in Figure 4  ###reference_### shows an example of this."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Benchmarking LLMs on PNCExtract",
            "text": "In our experiments, we employ LLaMA-7b-Chat (Touvron et al., 2023  ###reference_b30###  ###reference_b30###), LongChat-7B-16K (Dacheng Li* and Zhang, 2023  ###reference_b3###  ###reference_b3###), Vicuna-7B-v1.5 and Vicuna-7B-v1.5-16K (Chiang et al., 2023  ###reference_b2###  ###reference_b2###), and GPT-4 Turbo (OpenAI, 2023  ###reference_b22###  ###reference_b22###). The LongChat-7B-16K and Vicuna-7B-16K models are fine-tuned for context lengths of 16K tokens, and GPT-4 Turbo for 128K tokens.\nWe divide our dataset into  validation articles and  test articles. We assess the performance using micro average Precision, Recall, and F1 scores, considering both strict and partial metrics at the sample and property levels. We also compare two different prompting strategies NER+RE and E2E. Moreover, we consider the self-consistency technique.\nTable 4  ###reference_###  ###reference_### demonstrates that shortening documents proves beneficial in most cases. Additionally, Figure 3  ###reference_###  ###reference_### shows the trend of partial F scores as document length increases. We observe that GPT-4’s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top  segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\nAdditionally, Table 5  ###reference_###  ###reference_### provides bootstrap analysis from  resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents.\n###figure_4### The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is  sec/article, faster than  sec/article for GPT-4 Turbo (NER+RE).\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for  on the validation set (see Appendix D  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###). Based on that, we employ  and  predictions on the test set. Table 4  ###reference_###  ###reference_### shows that self-consistency enhances the strict and partial F.\nAdopting the partial metric has several advantages. First, it helps identify specific challenge areas. For example, in Table 6  ###reference_###  ###reference_###, we show the model faces the most challenges in accurately predicting Composition. Furthermore, human annotations for PNC samples are often error-prone (Himanen et al., 2019  ###reference_b9###  ###reference_b9###; McCusker et al., 2020  ###reference_b19###  ###reference_b19###), hence one potential use of an LLM like GPT-4 would be to identify errors and send them back for re-annotation. The partial metric can help prioritize which samples to re-annotate."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Results",
            "text": "In Table 4  ###reference_### we report the partial and strict metrics for multiple models and settings. We report the best results for each model in the condensed paper setting, selected across different , which correspond to average token counts per document of , , and , respectively. Further details on the results across various levels of document condensation are available in the Appendix E  ###reference_###. The results highlight several key observations:\nTable 4  ###reference_###  ###reference_###  ###reference_### demonstrates that shortening documents proves beneficial in most cases. Additionally, Figure 3  ###reference_###  ###reference_###  ###reference_### shows the trend of partial F scores as document length increases. We observe that GPT-4’s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top  segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\nAdditionally, Table 5  ###reference_###  ###reference_###  ###reference_### provides bootstrap analysis from  resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents.\n###figure_5### The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is  sec/article, faster than  sec/article for GPT-4 Turbo (NER+RE).\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for  on the validation set (see Appendix D  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###  ###reference_.SSS0.Px3###). Based on that, we employ  and  predictions on the test set. Table 4  ###reference_###  ###reference_###  ###reference_### shows that self-consistency enhances the strict and partial F.\nAdopting the partial metric has several advantages. First, it helps identify specific challenge areas. For example, in Table 6  ###reference_###  ###reference_###  ###reference_###, we show the model faces the most challenges in accurately predicting Composition. Furthermore, human annotations for PNC samples are often error-prone (Himanen et al., 2019  ###reference_b9###  ###reference_b9###  ###reference_b9###; McCusker et al., 2020  ###reference_b19###  ###reference_b19###  ###reference_b19###), hence one potential use of an LLM like GPT-4 would be to identify errors and send them back for re-annotation. The partial metric can help prioritize which samples to re-annotate."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparing with Baselines",
            "text": "Previous works on document-level -ary IE (Jain et al., 2020  ###reference_b15###; Jia et al., 2019b  ###reference_b17###; Hou et al., 2019  ###reference_b13###), have relied on encoder-only models, making them unsuitable for our specific task. For comparative purposes, we prompt GPT-4 on the SciREX dataset (Jain et al., 2020  ###reference_b15###), which comprises  annotated full-length machine learning papers. As shown in Table 7  ###reference_###, when prompted in a zero-shot, end-to-end manner, GPT-4 Turbo outperforms the baseline methods. Note that the baseline model, trained on  papers, received extensive supervision in the form of mention, coreference, binary relation, and salient mention identification.\nThis suggests that we would need to expend a large amount of annotation effort on PNCExtract to build a supervised pipeline comparable to\nthe zero-shot GPT-4 approach presented here.\n###figure_6###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis of Errors",
            "text": "Accurately extracting PNC samples is a complex task, and even state-of-the-art LLMs fail to capture all the samples. We find that out of  ground-truth samples,  were not identified in the model’s predictions. Furthermore,  of the  predictions were incorrect. This section discusses three categories of challenges faced by current models in sample extraction and proposes potential directions for future improvements.\nNanoMine aggregates samples from the literature, including those presented in tables and visual elements within research articles. As demonstrated in the first example of Figure 4  ###reference_###  ###reference_###, a sample is derived from the inset of a graph. Our present approach relies solely on language models. Future research could focus on advancing models to extract information from both textual and visual data.\nThe composition of PNC includes a variety of components such as hardeners and surface treatment agents. A common issue in our model’s predictions is incorrectly identifying these auxiliary components as the main attributes. For example, the second row in Figure 4  ###reference_###  ###reference_### shows the model predicting the filler material along with its surface treatments instead of recognizing the filler by itself.\nThe expression of chemical names is inherently complex, with multiple names often existing for the same material. In some cases, predicted chemical names are conceptually accurate yet challenging to standardize. This suggests the necessity for more sophisticated approaches that can handle the diverse and complex representations of chemical compounds. The third example in Figure 4  ###reference_###  ###reference_### shows an example of this."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works have focused on training models specifically for the tasks of NER and RE. Building on this, recently Wadhwa et al. (2023) and Wang et al. (2023a) suggest that LLMs struggle to effectively carry out these tasks through prompting. In the specific area of models trained on a materials science corpus, MatSciBERT (Gupta et al., 2022) employs a BERT (Devlin et al., 2018) model trained specifically on a materials science corpus. Song et al. (2023b) further developed HoneyBee, a fine-tuned Llama-based model for materials science. MatSciBERT was not applicable to our task, as detailed in Section 3, and HoneyBee’s model weights were not accessible during our research phase. Other contributions in this field include studies by Shetty et al. (2023), Hiroyuki Oka and Ishii (2021), and Tchoua et al., focusing on the extraction of polymer-related data from scientific articles. Similar to Dunn et al. (2022), Xie et al. (2023), Tang et al. (2023) and Cheung et al. (2023) our study also focuses on extracting -ary relations from materials science papers. However, our approach diverges in two significant aspects: we analyze full-length papers, not just selected sentences, and we extend our evaluation to partial assessment of -ary relations, rather than limiting it to binary assessments."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion and Future Works",
            "text": "We introduced PNCExtract, a benchmark focused on the extraction of PNC samples from full-length materials science articles. To the best of our knowledge, this is the first benchmark enabling detailed -ary IE from full-length materials science articles. We hope that this effort encourages further research into generative end-to-end methods for scientific information extraction from full-length documents. Future investigations should also consider more advanced techniques for condensing entire scientific papers. To overcome the challenges in PNC sample extraction discussed in Section 4.3  ###reference_###, future studies could investigate multimodal strategies that integrate text and visual data. Additionally, experimenting fine-tuning methods could lead to more precise chemical name generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Although our dataset comprises samples derived from figures within the papers, the current paper is confined to the assessment of language models exclusively. We acknowledge that incorporating multimodal models, which can process both text and visual information, has the potential to enhance the results reported in this paper.\nFurthermore, despite our efforts to correct NanoMine, another limitation of our study is the potential presence of inaccuracies within the dataset.\nAdditionally, our paper selectively examines a subset of attributes from PNC samples. Consequently, we do not account for every possible variable, such as “Filler Particle Surface Treatment.” This limited attribute selection means we do not distinguish between otherwise identical samples when this additional attribute could lead to differentiation. Acknowledging this, including a broader range of attributes in future work could lead to the identification of a more diverse array of samples."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We do not believe there are significant ethical issues associated with this research."
        }
    ]
}