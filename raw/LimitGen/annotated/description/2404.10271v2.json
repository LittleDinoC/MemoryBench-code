{
    "title": "Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
    "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans’ expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But\nhow do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about “collective” preferences or otherwise use it to make collective choices about model behavior?\nIn this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on\nSocial Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, reinforcement learning from human feedback (RLHF) has become the primary strategy that leading AI companies such as OpenAI (OpenAI, 2023  ###reference_b80###), Anthropic (Anthropic, 2023  ###reference_b1###), Meta (Meta, 2023  ###reference_b73###), and Google (Google, 2023  ###reference_b47###) use to make pretrained large language models (LLMs) more capable and controllable (Christiano et al., 2017  ###reference_b19###; Ziegler et al., 2019  ###reference_b110###) and to align them with human values. However, RLHF faces many limitations and challenges (Casper et al., 2023  ###reference_b15###; Lambert & Calandra, 2023  ###reference_b61###), including unrepresentative data (Prabhakaran et al., 2021  ###reference_b88###; Feffer et al., 2023  ###reference_b36###), unrealistic models of human decision-making (Hong et al., 2022  ###reference_b53###; Freedman et al., 2021  ###reference_b41###; Siththaranjan et al., 2023  ###reference_b99###; Lambert et al., 2023  ###reference_b62###), and insufficient modeling of human diversity (Kirk et al., 2023  ###reference_b57###; Freedman et al., 2023  ###reference_b42###) which may lead to political bias (Motoki et al., 2023  ###reference_b75###; Rozado, 2024  ###reference_b90###).\nWe propose that ideas from social choice theory (Arrow, 2012  ###reference_b3###; Fishburn, 1973  ###reference_b38###; Kelly, 1988  ###reference_b56###; Brandt et al., 2015  ###reference_b13###)—e.g., concerning whose preferences should be integrated into decisions and how this should be done—are needed to solve many of these open problems.\nWhile models that are solely pretrained on internet data may produce repetitive or harmful text, RLHF trains models to follow instructions (Ouyang et al., 2022  ###reference_b82###) and produce helpful and “harmless” outputs (Bai et al., 2022a  ###reference_b7###) based on human judgments.\nRLHF gathers example outputs from a pretrained LLM or examples written by humans.\nNext, humans are asked to select the outputs that best meet specified criteria (such as being “helpful” or “unbiased”).\nThese judgments, often called preferences, are then used to fine-tune the LLM.\nFrom a social choice perspective, this method raises several critical questions: Which humans are asked to judge outputs? What criteria do they use? How are their judgments combined? And how do their expressed judgments relate to their actual preferences?\nConstitutional AI (CAI), which involves reinforcement learning from AI feedback (RLAIF), is an alternative addressing some of these questions (Bai et al., 2022b  ###reference_b8###).\nHumans produce a “constitution” that specifies principles that the LLM is trained to align with.\nHowever, one must still decide who has input on the constitution and how it is constructed.\nBai et al. (2022b  ###reference_b8###) construct it “in a fairly ad-hoc way […] for research purposes”, but developing safe and ethical AI requires a more principled approach as in Ganguli et al. (2023  ###reference_b44###) or OpenAI (2024  ###reference_b81###). How then should one aggregate diverse preferences into a representative constitution?\nSocial choice theory has long studied similar questions.\nThis position paper argues that methods from social choice should be applied to address questions such as which humans should provide input, what type of feedback should be collected, and how it should be aggregated and used.\nBy taking into account the lessons from social choice theory, one can avoid naïve mistakes and reinventing the wheel, while leveraging feedback to address challenging design problems (Dobbe et al., 2021  ###reference_b29###).\nWe also highlight areas in which new work is required to extend social choice to new problems unique to training safe and ethical AI.\nThere are several advantages to addressing the above problems in a principled way. First, it will likely result in a fairer system, taking into account the input of a broader group of people. Second, it promises to give generally more accurate feedback about questions of truthfulness, as suggested by a significant body of literature on “epistemic democracy”—voting to settle questions about facts (Pivato, 2017  ###reference_b87###). Intuitively, having input from diverse people makes it less likely that something important is missed. Third, it will likely result in broader buy-in into the system.\nOne may have concerns about this approach; for example, will feedback from diverse people be inconsistent and result in inconsistent system behavior? Social choice theory provides examples where naïve aggregation of preferences or judgments leads to seemingly irrational collective choices, such as cyclical preferences (Schwartz, 2018  ###reference_b95###) or logically inconsistent conclusions (List & Pettit, 2002  ###reference_b69###). Then again, social choice theory also provides the tools for thinking about such issues and preventing them.\nSocial choice is not new to computer scientists; computational social choice (Brandt et al., 2015  ###reference_b13###) is a well-studied topic, with a biennial workshop since 2006. However, while many of those researchers affiliate with the AI community, there has not yet been much work connecting computational social choice to the alignment of modern AI systems.\nIn the following, we first give background on value alignment, RLHF, and social choice. Then we discuss several questions at their intersection. We think that significant further research is needed to answer these questions well and that good answers are needed in order to build AI systems in a responsible way based on potentially diverging feedback from multiple stakeholders. In contrast, ad-hoc approaches may result in systems that fail to represent their stakeholders well, that marginalize significant groups of stakeholders, and that create a basis for conflict between groups of people or the multiple AI systems that represent them."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Our proposed research agenda requires background from so far mostly disjoint communities. Readers familiar with some of the following can skip the corresponding subsections."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Value Alignment",
            "text": "As AI systems become more capable, it becomes critical that they act in alignment with human and societal values (Gabriel, 2020  ###reference_b43###). Many approaches to value alignment exist, such as, e.g., formal games in which AI agents must align with humans to solve them (Shah et al., 2020  ###reference_b97###), empirics on the relation between neural network activations and morally relevant output features (Zou et al., 2023  ###reference_b111###), and evaluations of the ethical behavior of LLMs (Pan et al., 2023  ###reference_b84###). RLHF is a particularly popular but so far limited approach."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Reinforcement Learning from Human Feedback",
            "text": "RLHF begins with generating and evaluating a dataset of model outputs .\nIn vanilla RLHF, humans are then shown paired completions  to prompts \nand asked which output  they prefer (Christiano et al., 2017  ###reference_b19###; Lee et al., 2021  ###reference_b67###).\nOther RLHF variants ask humans to rank or provide scores for groups of outputs (Ziegler et al., 2019  ###reference_b110###; Ouyang et al., 2022  ###reference_b82###), and many additional variations exist (Wu et al., 2023  ###reference_b106###).\nThe next step is to fit a parameterized reward model .\nFor LLMs,  is typically a neural network with weights .\nRLHF methods assume that there is a ground-truth reward function  that the human preferences reflect (up to noise).\nThe reward model\nis then optimized to match the likelihoods of the human preferences observed in the data. If the training data comes from diverse sources, this implicitly amounts to a rather intransparent or flawed form of preference aggregation (Siththaranjan et al., 2023  ###reference_b99###; Xu et al., 2023  ###reference_b107###; Chakraborty et al., 2024  ###reference_b17###; Ge et al., 2024  ###reference_b45###).\nThe final step is to use reinforcement learning to train a policy that maximizes rewards from the reward model.\nThis involves many design decisions—which RL algorithm to use, how to regularize the updates, and whether to gather further online feedback during training.\nSee Uc-Cetina et al. (2023  ###reference_b105###) for a survey on the use of RL to train LLMs."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Alternate Preference-Based Fine-Tuning Objectives",
            "text": "Since common RL methods like PPO (Schulman et al., 2017  ###reference_b94###) can be unstable, novel techniques for optimizing a model based on collected preference data have been proposed. Rafailov et al. (2023  ###reference_b89###) introduce Direct Preference Optimization (DPO), which recasts RLHF to converge to the best solution by directly optimizing a loss on the preference label dataset, rather than sampling online from the LLM policy or training an explicit reward model.\nAnother variant emerged to remove the dependency on pairwise data.\nEthayarajh et al. (2023  ###reference_b34###) propose a loss function termed Kahneman–Tversky Optimization (KTO) that enables learning a policy from unpaired preferences.\nThe authors further claim that the effectiveness of various losses for RLHF depends on the properties they share with proposed human utility functions (Kahneman & Tversky, 1979  ###reference_b55###)."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Constitutional AI",
            "text": "Bai et al. (2022b  ###reference_b8###) further explore the design space with Constitutional AI (CAI), which relies on RL from AI Feedback (RLAIF).\nRLAIF is a larger set of techniques for using AI to augment or generate feedback data, including pairwise preferences (Lee et al., 2023  ###reference_b66###; Sharma et al., 2024  ###reference_b98###; Castricato et al., 2024  ###reference_b16###).\nBy employing a human-written set of principles, which they term a constitution, Bai et al. (2022b  ###reference_b8###) use a separate LLM to generate artificial preference and instruction data used for fine-tuning.\nA constitution  is a set of written principles indicating specific aspects to focus on during a critique phase.\nThe instruction data is curated by repeatedly sampling a principle  and asking the model to revise its latest output  to the prompt  to align with .\nThis yields a series of instruction variants  from the principles  used for critique.\nThe final data point is the prompt  together with the final completion , for some .\nThe preference data is constructed in a similar, yet simpler way by using a subset of principles from  as context for a feedback model.\nThe feedback model is presented with a prompt , a set of principles , and two completions  and  labeled as answers (A) and (B) from a previous RLHF dataset.\nThe feedback models’ probability of outputting either (A) or (B) is recorded as a training sample for the reward model, as discussed in Section 2.2  ###reference_###."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Social Choice",
            "text": "Modern social choice theory began in the 1960s with Arrow’s Impossibility Theorem (Arrow, 1951  ###reference_b2###; McLean & Urken, 1995  ###reference_b71###). Arrow considered the problem of aggregating multiple individuals’ preferences—in his case rankings—into a social preference, subject to some desiderata. In particular, he required the aggregation function to be defined for any family of individual preferences; that the social preference relation be complete and transitive (it is then called a social welfare function); that the social preference between alternatives  and  should depend only on individual preferences between  and  (Independence of Irrelevant Alternatives, IIA); and that unanimous individual preference for  over  should imply social preference for  over . Arrow proved that the only aggregation functions satisfying these desiderata are dictatorships: there is one individual  such that no matter what others prefer, if  prefers  to , then the social preference ranks  above  as well.111Mishra (2023  ###reference_b74###) applies Arrow’s Theorem to RLHF. A similar theorem (see Taylor 2005  ###reference_b103###, § 1.3) holds for social choice functions where, instead of asking for a social ranking, we ask for just a single winner, or even just a set of choice-worthy alternatives. Arrow’s Theorem stimulated a huge literature exploring the consequences of weakening his desiderata (see, e.g., Campbell & Kelly 2002  ###reference_b14###, Holliday & Pacuit 2020  ###reference_b52###). The general takeaway is that for ordinal preference aggregation, in order to avoid dictatorships, oligarchies and vetoers, one must weaken IIA and allow the social preference between  and  to depend in part on preferences involving other alternatives. This allows for many alternative methods of aggregating individual preferences (see, e.g., Brams & Fishburn 2002  ###reference_b11###; Zwicker 2016  ###reference_b112###; Pacuit 2019  ###reference_b83### and the voting methods in the Preferential Voting Tools  ###reference_st/### library). Figure 1  ###reference_### gives an example in which three well-known methods disagree. The costs and benefits of these and other methods are systematically studied from various angles (axiomatic, computational, empirical, etc.) in social choice theory. Since Arrow, social choice theory has grown to study aggregation not only of individuals’ preferences, both ordinal and cardinal (d’Aspremont & Gevers, 2002  ###reference_b27###), but also of their approvals of alternatives (Laslier & Sanver, 2010  ###reference_b65###), grades given to alternatives (Balinski & Laraki, 2010  ###reference_b10###), judgments about propositions (Grossi & Pigozzi, 2022  ###reference_b49###), subjective probabilities for propositions (Dietrich & List, 2016  ###reference_b28###), and other types of objects (Rubinstein & Fishburn, 1986  ###reference_b91###). Recent work has also started to explore social choice methods for RLHF (Song et al., 2024  ###reference_b100###; Swamy et al., 2024  ###reference_b102###; Dai & Fleisig, 2024  ###reference_b26###; Ge et al., 2024  ###reference_b45###). In the following, we discuss some of the aggregation problems that might arise in the context of AI alignment."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "What Are the Collective Decision Problems and their Alternatives in this Context?",
            "text": "If we want to use social choice methods for aligning AI systems, we first need to specify what the concrete options/objects are, then collect preferences over them, and finally make actual or simulated collective choices between them. These options are called alternatives.\nIn some contexts, the set of alternatives is easy to comprehend and enumerate, e.g. candidates for a position. In other settings, there are exponentially many alternatives, but the set is still easy to comprehend, e.g., if each of  propositions must be either accepted or rejected (Lang, 2007  ###reference_b64###).\nFor AI alignment, it is harder to see how best to determine the set of alternatives for evaluation. It could be the set of all possible parameterizations of a given network architecture, but this would surely be conceptually intractable.\nFor an LLM, most RLHF approaches ask the evaluator to choose between a small, explicit set of alternative responses to some prompt, handwritten or sampled from the pretrained LLM. Alternately, we could consider all possible responses as alternatives. Evaluators could then indicate their preference by providing the preferred response themselves. Such exemplars are often used for fine-tuning and can be used to learn evaluators’ preferences and generate responses that well-represent them (Fish et al., 2023  ###reference_b37###).\nWhile this does not address questions about how to generalize beyond a single prompt, it is a useful way of conceptualizing the alternatives.\nOne might conceive of the alternatives as probability distributions over responses; LLMs are anyway configured to respond stochastically. This might be desirable not only for creativity but also to promote fairness and representativeness of responses. In response to a controversial question, fairness might militate against always giving the same answer, as any one answer will inevitably omit some relevant considerations on one side of a debate. There is a large literature on social choice rules that output probability distributions. Their input could be the evaluators’ stated preferences between distributions (Fishburn 1973  ###reference_b38###, Ch. 18), or stated preferences between plain alternatives (Brandt, 2017  ###reference_b12###), since evaluators might have difficulties comparing probability distributions. Indeed, the type of objects chosen by a social choice rule (e.g., distributions over responses) need not match the type of objects about which individuals state preferences (e.g., responses).\nMulti-winner rules (Faliszewski et al., 2017  ###reference_b35###; Elkind et al., 2017  ###reference_b32###) form a middle ground between deterministic single-winner rules and probabilistic rules. They pick a small, predetermined number of answers that best reflect what the voters want, which could be combined into a single response that lists these answers as bullet points to provide the user with a representative overview of possible answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Who Provides the Human Feedback?",
            "text": "Let us assume a stakeholder population of people who will be affected by an AI system and whose preferences would therefore ideally be considered in aligning the AI.222Some stakeholders, such as small children and non-human animals, whose preferences we cannot easily elicit might need to be represented by dedicated others. When it is infeasible to elicit feedback from all stakeholders, we must select a smaller group to query. One can try to select a suitably representative subset such that the alignment obtained using their feedback sufficiently approximates the alignment that would be obtained from all stakeholders’ feedback.\nHere one could draw on ongoing work in social choice theory on how to select citizens’ assemblies that are representative of a full population (e.g., Flanigan et al. 2021  ###reference_b39###; Landemore & Fourniau 2022  ###reference_b63###), as well as work in statistics on efficient stratified sampling (e.g., Meng 2013  ###reference_b72###).\nAlternatively, one could let stakeholders vote on their representatives, e.g., with a voting procedure designed to elect proportionally representative assemblies (see, e.g., Ch. 4 of Lackner & Skowron 2023  ###reference_b59###). Stakeholders might also delegate their feedback rights to others (who may in turn delegate, etc.), as in liquid democracy (see Paulin 2020  ###reference_b85###).\nWork up to now has used evaluator recruitment methods such as Mechanical Turk (Freedman et al., 2020  ###reference_b40###; Bai et al., 2022a  ###reference_b7###); Upwork, Scale AI, or Lionbridge (Stiennon et al., 2020  ###reference_b101###; Ziegler et al., 2019  ###reference_b110###); and purpose-built platforms (Noothigattu et al., 2018  ###reference_b78###). We believe this component of the RLHF pipeline deserves a more in-depth discussion, including one informed by social choice theory."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "What Is the Format of Human Feedback?",
            "text": "As discussed, human feedback for AI systems can have various forms; which of these are most natural and useful?\nHere, we can draw on a significant literature on\npreference elicitation (see, e.g., Sandholm & Boutilier 2006  ###reference_b92###), studying how best to query agents for their preferences in several domains.333Incidentally, Li et al. (2023  ###reference_b68###) propose to use LLMs for this.\nThis literature is closely tied to that of communication complexity (e.g., Kushilevitz & Nisan 1997  ###reference_b58###), which is concerned with minimizing the number of bits needed to communicate something. Both of these topics have also been studied in voting settings (Conitzer & Sandholm, 2002  ###reference_b21###, 2005  ###reference_b22###; Service & Adams, 2012  ###reference_b96###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Multiple Format Options",
            "text": "In general, we want the type of input or feedback that we ask of humans to be\n(1) natural to give,\n(2) informative about their preferences and values,\nand\n(3) of a type that can be used to align AI systems.\nFor example, having humans comment on an AI output in an open-ended text box may satisfy 1 and 2, but not 3 (at least, with current methods). Having them sort responses alphabetically may satisfy 1 and 3, but not 2. Having them directly rank neural networks based on inspecting their weights may satisfy 3 but not 1 or 2.\nNeedless to say, different choices for the type of input may lead to differently aligned systems and have different behavioral effects on humans (cf. Section 8  ###reference_###).\nPerhaps one should let individuals choose the format in which they give input or feedback. In traditional social choice this is uncommon—though there may be some flexibility in how preferences are expressed (e.g., allowing voters to only rank a few alternatives rather than all (Halpern et al., 2023  ###reference_b50###), or to give numerical ratings instead of rankings) and some variety in the interaction mechanism (e.g., one can vote for candidates individually, or pull a lever to vote for all candidates of a single party at once). Going forward, however, it is easy to imagine giving evaluators choices from a range of different ways to give their input. For example, the input could be individual responses, whole dialogue sessions, longterm interactions with the same user, or published guiding principles. The feedback could be expressed as approval/disapproval votes, pairwise comparisons of alternatives, full or partial rankings of the alternatives, giving precise or imprecise ratings of the form “I rate A between 7 and 9”, or even in free-form verbal feedback that an LLM can interpret as some formal preference data such as a partial ordering. Moreover, the evaluation could be on various aspects of the system’s behavioral patterns, as done in fine-grained RLHF (Wu et al., 2023  ###reference_b106###), or in RLHF that optimizes for multiple attributes (Dong et al., 2023  ###reference_b30###) such as helpfulness, humor, toxicity, etc.\nSection 5.2  ###reference_### discusses how we may process all this heterogeneous data."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Processing Diverse and Informal Feedback",
            "text": "Recall that in RLHF, human feedback is typically used to train a reward (or “preference”) model, which maps any possible AI system response to a numerical rating.\nThe concept of reward models could also be used to convert an evaluator’s diverse input into a common form, in order to then aggregate it\nwith other evaluators’ input.\nFirst, an individual evaluation interpretation model  could be trained to map a tuple of inputs of the form  to a numerical evaluation .\nAs before,  represents a prompt to the AI system,  the set of possible AI responses, and  a particular response.\nVector  represents the relevant features of a certain evaluator , and  is a language representation of ’s feedback on possible responses  to ,\ncontaining preference- and evaluation-related statements of various types (see Section 5.1  ###reference_###).\nIn practice,  would likely be based on an LLM pretrained to understand the texts , , , and , that is then fine-tuned to the interpretation task described above. Then the output  of  is a numerical rating of  given by evaluator  that is trained to be\n(approximately) consistent with the verbal evaluation  of that evaluator. We note that this task can be seen as a form of meta-learning.\nOne could then use the trained evaluation interpretation model  to train another model—an individual preference model —that skips verbal evaluations and directly maps inputs  to ratings . Namely, any tuple  can be converted into supervised training data  for , containing simulated ratings .\nThe hope is that the individual preference model  would be able to simulate the rating of any evaluator (represented by their features ),\nas long as the evaluator, prompt, and response set come from the same distribution as the one  was trained on.\nSimilar to the preference models used in current RLHF,  could finally be used to fine-tune the actual AI system or steer its behavior in real time.\nIn fact, if the evaluators’ features  are omitted in the training process sketched above,  reduces to the standard preference model as is already used in RLHF. This is vulnerable to, for example, evaluators that strategically misreport (Siththaranjan et al., 2023  ###reference_b99###), or to issues that arise from a disproportionate representation in the set of evaluators.\nWe add evaluators’ features  to  so that preferences can later be aggregated in a transparent and deliberate manner via an additional social choice step, as we discuss next."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "How Do We Incorporate Diverse Individual Feedback?",
            "text": "Here we sketch several variants of two approaches for including diverse input or feedback into AI systems in a consistent way using methods from social choice theory.\nThe first proposes adding an additional preference aggregation step somewhere during training, thereby turning RLHF into RLCHF: Reinforcement Learning from Collective Human Feedback.\nThe second approach instead proposes adding an additional simulated collective decision step somewhere in the training or the system’s real-time decision procedure, similar to Bakker et al. (2022  ###reference_b9###) and Jarrett et al. (2023  ###reference_b54###).\n###figure_1###"
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Proposal: Reinforcement Learning from Collective Human Feedback (RLCHF)",
            "text": "Preference aggregation could be incorporated into RLHF in several ways, early on or rather late.\nFor simplicity, assume a base version of rankings-based RLHF that (1) takes a database\nof prompts  together with corresponding sets of possible responses , (2) asks one associated evaluator  to provide a ranking  of the elements of , (3) turns this ranking into  many data points for training a common preference model  that produces numerical ratings , and (4) uses these ratings as rewards in fine-tuning the actual\nLLM via reinforcement learning.\nThe earliest point one may introduce preference aggregation is between steps (2) and (3). Instead of a single evaluator, we may ask the members of a jury  to provide individual rankings . Using some ordinal social welfare function , those rankings can then be aggregated into a collective ranking  to be used in step (3). We call this “RLCHF using aggregated rankings” (Fig. 2  ###reference_###).\n###figure_2### Alternatively, one could use cardinal preference aggregation between steps (3) and (4). For this, change step (3) so that a model of individual preferences is trained, mapping pair  and evaluator  with features  to predicted ratings . Also, generate a large sample of feature vectors  that is representative of the stakeholder population. Then a cardinal social welfare function  can be used to aggregate simulated individual ratings into a social rating  which can be used in step (4). We call this “RLCHF using evaluator features and aggregated ratings” (Fig. 3  ###reference_###)."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Proposal: Simulated Collective Decisions",
            "text": "RLCHF, as described above, keeps the reinforcement learning step that requires numerical rewards, and it uses ordinal or cardinal preference aggregation to produce those rewards for all possible responses . A different approach would replace reinforcement learning by something else and introduce social choice methods in the form of simulated collective decisions rather than preference aggregation.\nFor one thing, one could modify “RLCHF using evaluator features and aggregated ratings” into “Supervised Learning from Simulated Collective Decisions”, as shown in Fig. 4  ###reference_###. For this, in step (3) from above, use the individual preference model  and feature vectors  not to produce an aggregated rating but to simulate a collective choice that picks a single winning response .\nHere,  is now a single-winner social choice function. Then in step (4), use data point  to train the actual AI system via supervised (rather than reinforcement) learning.\nInstead of picking a single winner , we could also use a multi-winner social choice function  that outputs, say, a set of three responses . These can then be (creatively) combined into a single response, e.g.,\nby merging them into a bullet-point list and adding “The following are (three) typical answers to your question: …” at the beginning.\nA more radical modification would drop the fine-tuning-via-learning step altogether (leaving the LLM only pretrained) and instead simulate the collective choice at inference time. Whenever the live system is prompted with some , generate  many candidate responses  and  many evaluator feature vectors  representative of the stakeholder population for the\nproblem , and then directly return the winner \nof the simulated collective choice. Here, too,  could be a multi-winner or probabilistic social choice rule."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Which Traditional Social-Choice-Theoretic Concepts Are Most Relevant?",
            "text": "Social choice studies a wide variety of concepts, the relevance of which depends on the specific application. For example, consider the concept of false-name-proofness (Yokoo et al., 2001  ###reference_b108###, 2004  ###reference_b109###; Conitzer & Yokoo, 2010  ###reference_b23###), meaning that no-one can benefit from participating multiple times under multiple accounts. This is relevant when voting over the internet, but irrelevant for in-person faculty meetings where faculty vote publicly by raising their hands.\nSo, rather than studying every single social-choice-theoretic concept in the context of aligning AI systems,\nwe should be careful to evaluate which traditional concepts are most relevant. In the following, we give just a few examples."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Independence of Clones",
            "text": "###figure_3### In social choice problems, sometimes multiple alternatives, say  and , compare very similarly against every other alternative , according to the preferences of individuals. Such alternatives are referred to as clones, a notion that can be formalized in several ways. According to a strict notion of clones (Tideman, 1987  ###reference_b104###),  and  are clones if, for every individual, if that individual prefers  to some other alternative , then they also prefer  to , and if they instead prefer  to , then they also prefer  to . According to a more liberal notion (Laffond et al., 1996  ###reference_b60###),  and  are clones if, whenever a majority of individuals prefer  to some other alternative , then a majority prefers  to  as well, and whenever a majority prefers some  to , then a majority prefers  to  as well.\nSometimes the introduction of a clone can affect the outcome of an election. Suppose a group of people are voting over where to go for dinner, and the only two alternatives are a Chinese restaurant and an Indian restaurant. 52% of the voters prefer the Chinese restaurant. But then, someone points out that the Chinese restaurant has two floors and argues that the two floors should be considered separate options.\nSo now the alternatives are , , and . Nobody really cares all that much about the floor, but suppose that 26% of the voters prefer , and 26% of the voters prefer  (adding up to the original 52%). Further suppose that the voting rule used is Plurality, in which the alternative that appears at the very top of voters’ rankings the most often wins. Then the Indian restaurant ends up winning now with 48% of the vote. This seems like an undesirable property for a voting rule to have; it would be better for the introduction of a clone never to make a difference.444More precisely, introducing a clone should not affect whether a non-clone (e.g., the Indian restaurant in our example) is selected or which non-clone is selected. But it may affect which clone, if any, is selected. For instance, a clone-independent rule could select  over  in our example, if among the 48% of people who prefer , a strict majority of them prefer  to . This latter desirable property is called independence of clones. Perhaps when choosing restaurants, this is not that relevant, as restaurants will rarely be clones (unless the floors of restaurants are treated separately). On the other hand, when choosing responses for a chatbot, it may be quite common for two responses to be very close to each other, indicating its importance to this context. We note that Borda Count (see Figure 1  ###reference_###), which is implicitly used in some standard approaches to RLHF (Siththaranjan et al. 2023  ###reference_b99###), badly violates independence of clones."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Strategic Voting",
            "text": "Another concern is strategic voting (or strategic feedback).\nStrategic voting consists of casting a vote that does not reflect one’s true preferences, in order to obtain a better result for oneself. For example, consider an election with plurality voting, as described above. A voter might perceive that her top-ranked alternative has no chance of winning and therefore strategically vote for another alternative.\nStrategic voting poses a problem because we can no longer take votes (or feedback) at face value. Unfortunately, in general, every reasonable voting rule will sometimes introduce incentives to manipulate (Gibbard, 1973  ###reference_b46###; Satterthwaite, 1975  ###reference_b93###). These incentives to manipulate might be reduced if voters lack full information about the preferences of other voters (Conitzer et al., 2011  ###reference_b24###) or about the voting rule that will be used (Holliday & Pacuit, 2019  ###reference_b51###). But we often cannot guarantee such ignorance, just as we often cannot guarantee computer security through obscurity.\nWhat form might strategic voting in a context such as RLHF take? If rating responses range on a scale from (say) 0 to 10, a natural strategy is to overreport. E.g., if one evaluator does not really like a response (at the level of a 3), but suspects that others would like it (say, two other evaluators give it a 6), then this evaluator may strategically give a rating of 0 to “compensate” for the other reviewers. This manipulation would be successful if we eventually aggregate ratings by taking their average: the average will be pulled down to 4, instead of the 5 that would result from reporting truthfully, so that the average is closer to the 3 that the evaluator believes is ideal. If instead we use the median as the aggregate, then this manipulation is ineffective—the median would remain 6. Indeed, the median is strategy-proof in this context: misreporting one’s preferences never helps, as long as one’s only goal is to move the median rating closer to one’s “true” rating (cf. Moulin 1980  ###reference_b76###)."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Anonymity",
            "text": "In democratic contexts, a standard desideratum on voting rules is anonymity: if two voters swap their ballots before submitting them, the output of the voting rule will not change (the rules in Figure 1  ###reference_### all satisfy anonymity in this sense). This captures the idea that the voting rule should not favor some voters over others. Anonymity prohibits not only the extremes of dictatorship (recall Section 2.5  ###reference_###) but even any kind of weighted voting wherein some voters’ votes count for more than others. However, in the context of AI development, one might consider aggregating human feedback in a way that violates anonymity (cf. the weighted majority rule discussed in Nitzan & Paroush 1982  ###reference_b77###). Perhaps some evaluators are more experienced or more highly rated than others; perhaps some are influenced by others, so their input should not be considered completely independent inputs for aggregation; and so on. In general, whether the same democratic norms applied to voting also apply in an AI context is an important question for discussion."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Principles as Voters",
            "text": "While it is standard in social choice for the voters to be humans, this is not required by the social choice theory framework. In some applications of social choice to AI ethics and safety, possibly including Constitutional AI (recall Section 2.4  ###reference_###), we might regard different ethical principles as the “voters” who can rank or otherwise evaluate the outputs of an AI system (cf. Greene et al. 2016  ###reference_b48###). This is analogous to applications of social choice theory in the philosophy of science, where the “voters” are theoretical virtues that may rank scientific theories differently (Okasha, 2011  ###reference_b79###), or to multi-criteria decision-making, where the “voters” are relevant factors that may rank the options differently (Arrow & Raynaud, 1986  ###reference_b4###). Of course, such ethical principles could themselves be outputs of some prior social choice procedure in which the voters are humans (cf. Collective Constitutional AI in Ganguli et al. 2023  ###reference_b44###).\nThis suggests a possible alternative architecture for applying social choice to AI—one sitting somewhere between the extremes of a spectrum that ranges from Constitutional AI at one end (in which principles are the whole show, while social choice does not appear) to the RLHF version of reinforcement learning as described in previous sections (where principles play no role at all). In this alternative model, each respondent would be required to justify her rankings of alternative AI responses in terms of their level of satisfaction of each of a number of principles taken from a fixed menu. The AI system would use the results to train for several independent tasks: for each principle, separately learn how to rate responses to queries based on that principle alone; and learn how to aggregate those separate ratings into an overall rating of the responses. These would be composed to form the final stage of a simulated collective decision—the stage in which the voters are the principles."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "How Should We Account for Behavioral Aspects and Human Cognitive Structures?",
            "text": "Preference elicitation often makes idealized assumptions, e.g., that each queried individual has well-defined and consistent preferences and will answer in a way that is consistent with them (up to some random noise). But in reality, myriad behavioral effects kick in. For example, McElfresh et al. (2021  ###reference_b70###) study how (in the context of kidney allocation) humans can become indecisive when asked to give input that will have moral implications.\nThis leads to a variety of questions about how to align AI systems in the context of behavioral effects. Should we correct for them? That would seem to require having a model where such a behavioral effect obscures humans’ “true” values.\nBut do these “true” values correspond to anything real in the world? Do we run the risk of the “correction” actually removing valuable information? Could the ability to make such “corrections” in fact be abused to intentionally remove inconvenient feedback?"
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "How Do We Navigate a Multiplicity of AIs?",
            "text": "Consider, again, a group of people voting on a restaurant for their dinner. If there is a significant disagreement, then rather than forcing a minority to go someplace they really do not like, it can make sense to split into multiple groups, each going to their favorite restaurant. Similarly, perhaps it makes sense to create multiple AI systems—for example, to recognize strong inter- and intra-cultural variations that have been identified in some non-homogenous populations (Awad et al., 2018  ###reference_b5###; Peters & Carman, 2024  ###reference_b86###).\nDepending on the situation, the people providing feedback might be split into groups ex ante (e.g., each country makes a system based on their own citizens’ feedback), but also ex post, where we first collect feedback and then form groups of people. The latter approach is closely related to the topic of representation in voting theory (Faliszewski et al., 2017  ###reference_b35###).\nThere is also the slightly different scenario where one AI system is in place, and some group of people believe that it is not serving them well. Hence, they might decide to pool their resources and create their own system. The literature on cooperative game theory\n(cf. Chalkiadakis et al. 2011  ###reference_b18###), sometimes referred to as coalitional game theory, touches on these considerations (and indeed also plays a role in questions of representation, as in Aziz et al. 2017  ###reference_b6###).\nFinally, let us highlight possible shortcomings to creating multiple AI systems.\nAs in the restaurant example, it may have the result of unnecessarily dividing people into separate groups. Moreover, splitting into groups may not be feasible if it does not dovetail with existing social structures. For example, the US Federal Government may want to adopt a single system that will impact all its citizens, and adopting two systems would be tantamount to splitting the country in two.\nFinally, unlike in the case of the restaurants, the multiple AI systems may have to interact with each other, creating the risk of conflict between AIs with different goals. The nascent literature on cooperative AI (Dafoe et al., 2021  ###reference_b25###; Conitzer & Oesterheld, 2023  ###reference_b20###) may help ensure these interactions do not result in adverse outcomes. Nonetheless, it might be best to see if we can completely avoid having multiple AIs with competing goals, or at least design them in a way that makes conflict between them less likely."
        }
    ]
}