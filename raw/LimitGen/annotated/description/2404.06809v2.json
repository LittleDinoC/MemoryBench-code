{
    "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
    "abstract": "The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes.\nIn this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG.\nAt its core, CAG aims to equip models with the ability to discern and process information based on its credibility.\nTo this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG.\nFurthermore, to accurately evaluate the models’ capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios.\nExperimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance.\nMoreover, our model supports customized credibility, offering a wide range of potential applications.\n111Our code, benchmark, and models are available at https://github.com/panruotong/CAG",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, Large Language Models (LLMs) (Brown et al., 2020  ###reference_b7###; OpenAI et al., 2023  ###reference_b28###; Touvron et al., 2023  ###reference_b39###; Anil et al., 2023  ###reference_b2###) have experienced significant growth and demonstrated excellent performance in multiple domains (Kojima et al., 2022  ###reference_b23###; Thirunavukarasu et al., 2023  ###reference_b38###; Ziems et al., 2023  ###reference_b54###; Min et al., 2023  ###reference_b26###).\nWith the ascendancy of LLMs, Retrieval-Augmented Generation (RAG) has attracted significant interest. RAG mitigates the knowledge bottleneck of LLMs by incorporating externally retrieved documents into their generation process. This inclusion helps diminish the occurrences of hallucinations and misinformation during generation, thereby substantially enhancing the quality of output from LLMs (Petroni et al., 2021  ###reference_b31###; Zhu et al., 2021  ###reference_b52###; Mallen et al., 2023  ###reference_b25###).\n###figure_1### However, RAG for large language models remains significantly impacted by flawed information.\nThis is mainly because the retrieval process often provides noisy, outdated, and incorrect contexts which adversely affects RAG, substantially reducing its effectiveness.\nSpecifically, previous research Shi et al. (2023a  ###reference_b33###); Chen et al. (2023a  ###reference_b10###) has found that LLMs are highly sensitive to noise, which impacts LLMs’ capacity to discern and trust accurate information, ultimately affecting the outcomes they generate.\nFurthermore, due to the temporal insensitivity of LLMs  (Su et al., 2022  ###reference_b37###; Zhao et al., 2024  ###reference_b51###), these models struggle to discern outdated information solely based on their internal knowledge. More critically, because LLMs are trained on extensive collections of historical text, there’s an inherent risk that outdated information will align with the models’ internal knowledge bases. This alignment can inadvertently encourage LLMs to favor and perpetuate outdated information.\nBesides, the prevalence of misinformation on the current web poses a significant challenge for large models, which struggle to identify misinformation using only their inherent knowledge (Xie et al., 2023  ###reference_b46###; Pan et al., 2023  ###reference_b29###). This difficulty makes them susceptible to misinformation, leading to the generation of incorrect answers.\nTherefore, flawed information, characterized by noisy, outdated, and incorrect information, has substantial negative effects on RAG.\nFrom a cognition perspective, a common approach humans adopt to combat flawed information is to assess the credibility of external information Burgoon et al. (2000  ###reference_b8###). Specifically, information that is current, evaluated, and sourced from highly credible origins is typically regarded as more timely, accurate, and reliable.\nMotivated by this, we introduce Credibility-aware Generation (CAG), a universally applicable framework designed to address flawed information encountered during RAG. At its core, CAG seeks to equip models with the ability to discern and process information based on credibility.\nBy attributing varying levels of credibility to information differing in relevance, temporal context, and source, and integrating these credibility indicators into the generative processes of LLMs, CAG significantly mitigates the issues arising from flawed information.\nUnfortunately, we have discovered that existing LLMs are not inherently sensitive to directly provided credibility information in the prompt. This deficiency restricts their capacity to optimally employ credibility for discerning and processing information.\nTo endow models with the capability of CAG, we propose a novel data transformation framework based on existing Question Answering (QA) and dialogue datasets.\nThis framework transforms the data into a format that incorporates credibility and can be utilized to guide model credibility-based generation, thereby training the model to utilize credibility in addressing flawed information.\nSpecifically, our data transformation process comprises two core steps:\n1)\nMulti-granularity credibility annotation, which assigns credibility to text units at both document and sentence levels by dividing retrieved documents into varying granularities.\n2) Credibility-guided explanation generation, which prompts LLMs to generate credibility-guided explanations given questions, retrieved documents with credibility annotation and golden answers.\nFinally, we utilize instruction fine-tuning to train the model, enabling it to generate responses based on credibility.\nTo rigorously assess the ability of the model’s credibility-aware generation in managing flawed information, we have developed a comprehensive benchmark encompassing various real-world scenarios, including open-domain QA, time-sensitive QA, and misinformation-polluted QA.\nIn these settings, several indicators, including retrieval relevance, temporal validity, and source authority, are considered as the given credibility measurement.\nThe main goal of this benchmark is to measure how well a model can answer questions when given the context documents with corresponding credibility annotations.\nExperimental results on multiple datasets across multiple scenarios demonstrate the efficacy of our approach in leveraging credibility information.\nOur model significantly outperforms various prevalent RAG approaches applied to both open and closed-source LLMs of diverse scales. Additionally, it exhibits robust resilience against noisy documents, maintaining high performance levels even as alternative strategies experience sharp deteriorations.\nAll these results verify the effectiveness of the proposed CAG framework and corresponding training algorithm.\n###figure_2### The main contributions of this study are summarized as follows:\nWe present Credibility-aware Generation, a universal framework to handle the flawed information challenge in RAG.\nWe propose a novel data transformation framework that leverages existing QA and dialogue datasets. This framework transforms existing datasets into data annotated with credibility and guides models to generate responses based on credibility, thereby equipping the model with Credibility-aware Generation capability.\nWe construct a comprehensive benchmark and evaluate model performance in credibility-aware generation, encompassing real-world scenarios of open-domain QA, time-sensitive QA, and misinformation polluted QA.\nExperimental evidences demonstrate that our model effectively understands and employs credibility to generate responses, significantly surpasses other RAG-based strategies, and maintain robustness despite the increasing noise in the context."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Credibility-aware Generation",
            "text": "Credibility-aware Generation is designed to enable models to discern and process information based on its credibility.\nSubsequently, we will provide formal definitions for both RAG and CAG, illustrating their divergence.\nIn the Retrieval-Augmented Generation process, user input  initiates the retrieval of a set of related documents  from a large corpus  based on how closely these documents match the input. Then, it combines the input  with these documents  to generate responses , formalized as , where  denotes the concatenation operation.\nCompared to RAG, the Credibility-aware Generation offers additional credibility for each document.\nInitially, through credibility assessment based on various scenarios, each retrieved document has been assigned a level of credibility.\nThen, these documents  with their credibility  are synthesized with the user input  as augmented input. LM generates responses  based on this augmented input,\nformally represented as .\nThis approach ensures that the generated responses not only incorporate the content of the documents but also consider the credibility of each document, thereby enhancing the reliability of responses."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Teaching Model to Credibility-aware Generation",
            "text": "In this section, we endow LLMs with the capability of CAG. A potential approach involves directly providing the credibility annotations of each document in the prompt. Unfortunately, as indicated in Table 2  ###reference_###, our experiments reveal that even advanced LLMs, such as ChatGPT, exhibit limited sensitivity to credibility.\nTo this end, we introduce a novel data transformation framework. Through multi-granularity credibility annotation and credibility-guided explanation generation, we transform existing QA datasets into data that includes credibility annotations which can guide the model to generate credibility-based responses. Then, through instruction fine-tuning, we train the model to generate responses grounded in credibility assessments."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Multi-granularity Credibility Annotation",
            "text": "To cater to the varied requirements for credibility across different scenarios and enhance the model’s comprehension of credibility, we collect training data encompasses Open-domain QA, Machine Reading Comprehension (MRC) datasets, and dialogue datasets and propose a multi-granularity credibility annotation method.\nFirst, we divide the retrieved documents to create a multi-granularity corpus, encompassing sentence and document levels. Then, the retriever assesses the match between each retrieval unit and the query, assigning a relevance score, and classifies documents into three levels: high, medium, and low, employing either equi-frequency or equi-distance segmentation.\nThis approach of using levels instead of scores aims to simplify representation, thereby improving the model’s understanding and providing a certain degree of fault tolerance.\nConsequently, we gather about 15k training datasets, within which the contexts of the QA data are annotated with different granularities of credibility. The detailed composition of the training data is shown in the Appendix A.1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Credibility-guided Explanation Generation",
            "text": "To facilitate the model’s comprehension and effective utilization of credibility, we employ LLM to generate explanations for the answers. These explanations stem from an analysis of both the content and credibility of the documents.\nGiven the limitations of current LLMs in comprehending credibility effectively, we utilize chain-of-thought prompts to guide LLM to generate credibility-guided explanations given questions, retrieved documents with credibility and golden answers. In this case, LLM is required to analyze document content and credibility, as well as the rationale for the derived answer after integrating all the information.\nConsidering the accessibility and advanced capabilities of GPT-3.5, we elect to employ GPT-3.5 for the generation of explanations.\nIn this way, we obtain high-quality, credibility-guided answer explanations.\nThen, we replace the original answers in the training data with credibility-guided explanations to form a novel QA dataset based on credibility.\nWithin this dataset, the inputs include questions and external documents annotated with credibility, while the outputs are credibility-guided explanations."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Instruction Fine-tuning",
            "text": "Through the two steps above, the training dataset obtained contains credibility, which can be used to facilitate the model in gaining the capacity for CAG.\nWe fine-tune the language model on this dataset to empower the model to discern and process information according to its credibility.\nAs defined by Iyer et al. (2023  ###reference_b18###), the loss function is as follows:"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Credibility-aware Generation Benchmark",
            "text": "To rigorously evaluate the ability of credibility-aware model generation to handle flawed information, we construct the Credibility-aware Generation Benchmark (CAGB). This benchmark encompasses the following three specific scenarios where the integration of credibility is essential:\nOpen-domain QA\naims to accurately answer questions on a wide variety of topics without being limited to any particular area. It encompasses a broad spectrum of real-world applications that urgently require the integration of external knowledge to enhance the language model’s ability to address queries. This scenario thus necessitates the ability to effectively identify and process noise information.\nTime-sensitive QA\naims to give answers that are both correct and up-to-date, using the most recent information available. It poses a challenge for LLMs due to the rapidly changing nature of internet information. The inevitable inclusion of outdated documents when incorporating external sources further complicates matters. Even with timestamps provided for each document, LLMs might still erroneously prioritize outdated documents. This situation underscores the critical need for credibility in time-sensitive QA scenarios.\nMisinformation polluted QA\naims to tackle the issue of ensuring accurate answers in an environment polluted with misinformation. It presents a substantial challenge to LLMs, attributed to the misuse of LLMs and the consequent proliferation of fake news and misinformation (Zhuo et al., 2023  ###reference_b53###; Pan et al., 2023  ###reference_b29###). Consequently, in this scenario, it is crucial to\ntake into account the quality and credibility of any introduced external information.\nIn the following, we will provide a detailed description of data construction for each scenario, and the statistics of CAGB are shown in the Table 1  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Open-domain QA",
            "text": "Our research utilizes data from several challenging QA datasets that have noise in the context they provide. HotpotQA (Yang et al., 2018b  ###reference_b49###) and 2WikiMHQA (Ho et al., 2020  ###reference_b17###) both require reasoning across multiple documents, and feature a high proportion of distracting documents (60%-80% and 80%, respectively). Importantly, the data we utilize from HotpotQA is extracted from the dev subset, whereas our training dataset is derived from the train subset.\nMusique (Trivedi et al., 2021  ###reference_b41###) questions are of higher complexity, with up to 90% of distracting passages. ASQA (Stelmakh et al., 2022  ###reference_b35###) is a long format QA dataset focused on ambiguous questions. RGB (Chen et al., 2023b  ###reference_b11###) is a specialized benchmark used for evaluating the capabilities of models in the RAG scenario, with noise robustness being one of its aspects.\nWe assign credibility to the documents provided in the dataset in terms of retrieval relevance.\nThe retriever is used to assign relevance scores based on the similarity of each document to the query. We then divide the documents into three categories based on the scores at equal intervals, including high, medium, and low.\nSubsequently, we divide the documents into three credibility levels, specifically high, medium, and low, based on relevance scores distributed at equal intervals."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Time-sensitive QA",
            "text": "In order to construct a diverse, high-quality, and up-to-date news dataset, we annotate 500 time-sensitive questions along with their corresponding dates.\nThese questions originate from real-world scenarios, including news question answering data from RealTime QA (Kasai et al., 2022  ###reference_b22###), TAQA (Zhao et al., 2024  ###reference_b51###), and questions adapted from news reports.\nTo simulate the simultaneous occurrence of varied information on the Internet, we utilize Google search API to gather three relevant documents and four distracting documents for each question, the latter being either irrelevant or outdated. This approach to document selection is crafted to emulate the intricate and heterogeneous nature of real-world information landscapes. Each news includes its publication date, thereby aiding in the evaluation of its timeliness.\nFor document credibility annotation, we assess credibility based on relevance and time difference between the document’s publication and the posed question.\nDocuments that are highly relevant to the given query and possess strong timeliness are considered to have high credibility, while documents that are irrelevant or outdated are regarded as having low credibility.\nWe ensure the accuracy of answers by manually annotating.\nThe obtained time-sensitive dataset with outdated document settings and credibility annotation is named EvolvingTempQA."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Misinformation Polluted QA",
            "text": "We create a up-to-date multiple-choice quiz dataset, comprising both real and fake news for each question.\nThe dataset construction bases on RealTime QA, utilizing weekly news quizzes from CNN and other news platforms. To maintain the dataset’s real-time relevance, we select news from July 1, 2023, onwards, comprising 480 questions with four options and one supporting news item each.\nTo simulate the generation of fake news, we first generated a claim utilizing the LLM, based on a question and a randomly selected incorrect option.\nThis process transforms the question and incorrect option into a deceptive statement. Subsequently, we choose GPT-3.5 and Qwen as the generators for fake news, guiding them to generate documents of varying styles based on the claim, including news style and Twitter style. The prompts used and examples of the generated content are detailed in the Appendix A.4  ###reference_###.\nThe fictitious news articles produced by LLMs, due to their authenticity being deliberately compromised, are classified as having low credibility. Conversely, news articles from reputable news websites are considered to possess high credibility. We set the ratio of fake news at 0.5, 0.67, and 0.75 to evaluate the robustness of model against misinformation under various levels of pollution.\nBy simulating the process of generating fake news and annotating credibility based on source, we obtain a misinformation polluted QA dataset in the news domain, named NewsPollutedQA."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To demonstrate the effectiveness of our framework in handling flawed information in real-world QA scenarios, we conduct comprehensive experiments under three scenarios within the CAGB.\nAll these results verify the effectiveness of the proposed CAG framework and the corresponding training algorithm. Additionally, our models maintain robustness even with the increasing noise in the context.\nIn the following sections, we will discuss our experiments and conclusions in detail.\nWe compare our method with the following three strategies incorporated with 6 LLMs across various scales:\nRetrieval-based concatenates documents from the dataset with questions as input.\nRetrieval and reranking employs an advanced reranking mechanism to reorder retrieved documents, giving priority to those with greater relevance (Xie et al., 2023  ###reference_b46###).\nRetrieval and credibility incorporates credibility as a prefix to the retrieved documents in the prompt, aiming to assess the model’s ability to understand and utilize credibility.\nWe evaluate advanced models, including ChatGPT\n(gpt-3.5-turbo-0613), LLaMA-2-7B, 13B, 70B, Vicuna-7B-v1.5 and Mistral-7B-Instruct (Jiang et al., 2023  ###reference_b20###). Additionally, we create a dataset mirroring the model training data but without credibility annotations and with initial answers, on which we fine-tune the same base model, and named the trained model vanilla IFT.\n###figure_3### We use LLaMA-2-7B, LLaMA-2-13B and Mistral-7B as our base models. To provide relevance scores, we use SPLADE (Formal et al., 2021  ###reference_b15###) as our retriever. Our model training is based on the Fastchat framework and carry out on two A100-80G GPUs.\nFor all language models, we incorporate 3-shot QA examples within the prompt. We set the temperature parameter to 0.01 during inference. We employ Exact Match (EM) (Rajpurkar et al., 2016  ###reference_b32###) as the primary evaluation metric for all datasets. The prompts used for our evaluation are provided in the Appendix A.3  ###reference_###.\nIn scenarios including open-domain QA, time-sensitive QA, and misinformation pollutedQA, existing LLMs, including ChatGPT and LLaMA-2-70B, face challenges due to interference from flawed information. In the retrieval-based open-domain QA scenario, the average EM score for ChatGPT is only 41.5%, while the EM score for LLaMA-2-70B is 44.1%.\nAll models exhibit low performance on the Musique, NewsPollutedQA, which are characterized by high ratios of flawed information.\nThe method of reranking using externally provided relevance scores can assist the model to a certain extent, as the model is sensitive to the order of documents (Xie et al., 2023  ###reference_b46###; BehnamGhader et al., 2023  ###reference_b5###).\nOur models significantly surpass all baseline models across the 7 datasets under 3 scenarios, including ChatGPT and LLaMA-2-70B enhanced with retrieval and reranking. For instance, on the 2WikiMHQA dataset, our CAG-7B improves 26.6% of EM score over the LLaMA-2-7B model and 28.2% of EM score over the Vicuna-7B model under retrieval-based.\nThe models, developed through training on LLaMA 7B, 13B, and Mistral 7B with CAG, not only exhibit improved reliability in its outputs but also excel in new, challenging situations, including time-sensitive QA and misinformation polluted QA. This performance, achieved within an open-domain QA framework lacking temporal or source integration, underlines the model’s robust capability for CAG, effectively managing diverse flawed information and affirming the universality of CAG."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Setup",
            "text": "We compare our method with the following three strategies incorporated with 6 LLMs across various scales:\nRetrieval-based concatenates documents from the dataset with questions as input.\nRetrieval and reranking employs an advanced reranking mechanism to reorder retrieved documents, giving priority to those with greater relevance (Xie et al., 2023  ###reference_b46###  ###reference_b46###).\nRetrieval and credibility incorporates credibility as a prefix to the retrieved documents in the prompt, aiming to assess the model’s ability to understand and utilize credibility.\nWe evaluate advanced models, including ChatGPT\n(gpt-3.5-turbo-0613), LLaMA-2-7B, 13B, 70B, Vicuna-7B-v1.5 and Mistral-7B-Instruct (Jiang et al., 2023  ###reference_b20###  ###reference_b20###). Additionally, we create a dataset mirroring the model training data but without credibility annotations and with initial answers, on which we fine-tune the same base model, and named the trained model vanilla IFT.\n###figure_4### We use LLaMA-2-7B, LLaMA-2-13B and Mistral-7B as our base models. To provide relevance scores, we use SPLADE (Formal et al., 2021  ###reference_b15###  ###reference_b15###) as our retriever. Our model training is based on the Fastchat framework and carry out on two A100-80G GPUs.\nFor all language models, we incorporate 3-shot QA examples within the prompt. We set the temperature parameter to 0.01 during inference. We employ Exact Match (EM) (Rajpurkar et al., 2016  ###reference_b32###  ###reference_b32###) as the primary evaluation metric for all datasets. The prompts used for our evaluation are provided in the Appendix A.3  ###reference_###  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Overall Results",
            "text": "The main results of the three scenarios are presented in the Table 2  ###reference_###, we can clearly see that our model efficiently understands and utilizes credibility information to provide more accurate and credible responses. In the following, we analyze the experimental results in detail:\nIn scenarios including open-domain QA, time-sensitive QA, and misinformation pollutedQA, existing LLMs, including ChatGPT and LLaMA-2-70B, face challenges due to interference from flawed information. In the retrieval-based open-domain QA scenario, the average EM score for ChatGPT is only 41.5%, while the EM score for LLaMA-2-70B is 44.1%.\nAll models exhibit low performance on the Musique, NewsPollutedQA, which are characterized by high ratios of flawed information.\nThe method of reranking using externally provided relevance scores can assist the model to a certain extent, as the model is sensitive to the order of documents (Xie et al., 2023  ###reference_b46###  ###reference_b46###; BehnamGhader et al., 2023  ###reference_b5###  ###reference_b5###).\nOur models significantly surpass all baseline models across the 7 datasets under 3 scenarios, including ChatGPT and LLaMA-2-70B enhanced with retrieval and reranking. For instance, on the 2WikiMHQA dataset, our CAG-7B improves 26.6% of EM score over the LLaMA-2-7B model and 28.2% of EM score over the Vicuna-7B model under retrieval-based.\nThe models, developed through training on LLaMA 7B, 13B, and Mistral 7B with CAG, not only exhibit improved reliability in its outputs but also excel in new, challenging situations, including time-sensitive QA and misinformation polluted QA. This performance, achieved within an open-domain QA framework lacking temporal or source integration, underlines the model’s robust capability for CAG, effectively managing diverse flawed information and affirming the universality of CAG."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Analysis Study",
            "text": "In the following, we will present several analysis against the robustness and limitation of current Credibility-aware Generation."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Noise Robustness Analysis",
            "text": "Previous research has demonstrated that an increase in the proportion of noise within the context significantly degrades model performance (Xie et al., 2023  ###reference_b46###; Chen et al., 2023b  ###reference_b11###).\nTo assess the robustness of diverse methods against flawed information, we vary the ratio of noisy documents within the total document set across three distinct datasets: RGB, EvolvingTempQA and NewsPollutedQA, and observe the consistency in performance changes across different models as the ratio of noisy documents increased.\nWe present the results in Figure 3  ###reference_### and can see that:\nCredibility-aware Generation makes the model robust to flawed information, which enhances its ability to discern and prioritize accurate information.\nAs the proportion of noise in the context increases, most of the models exhibit performance degradation aligning with the observations made by Chen et al. (2023b  ###reference_b11###).\nHowever, our models show greater robustness compared to others, notably displaying performance improvements on EvolvingTempQA as the noise ratio rises from 0.4 to 0.6."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Analysis of Discarding Low Credibility Documents",
            "text": "Upon assigning credibility to the documents in context, an alternative intuitive strategy is to simply discard low credibility documents. However, considering that credibility assessments are not precise, this strategy may inadvertently filter out helpful information, thereby impairing the accuracy of the model’s responses.\nTo demonstrate this, we compare the performance of LLMs in this setting with that of CAG-7B in Open-domain QA.\nThe results are shown in Figure 4  ###reference_###, we can clearly see that:\nby preserving more document information and differentiating them based on explicit credibility in the prompt, our CAG framework mitigates the risk of losing valuable information. As a result, the accuracy and comprehensiveness of the responses are improved.\n###figure_5###"
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Effect of Credibility Annotation Accuracy",
            "text": "To investigate the impact of credibility annotation accuracy on the performance of CAG and to identify the upper limit of their potential,\nWe conduct a comparison between the use of golden credibility annotations and retriever-based credibility annotations within Open-domain QA using both the CAG-7B and CAG-13B models. Golden credibility annotations refer to labeling golden support evidence as high credibility and other text as low credibility.\nThe results of our experiments are presented in Table 3  ###reference_###.\nWe can find that: The precision of retrieval model annotation credibility is a primary factor limiting the current performance of CAG.\nThe results, as presented, clearly demonstrate that reliable credibility annotations are instrumental in unlocking the model’s potential. Compared with the use of SPLADE to label credibility, the use of golden credibility labels on the CAG-7B has resulted in an average improvement of 14.4% of EM across three datasets."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Customized Credibility Applications",
            "text": "In demonstrating the capability of customized credibility in CAG, this paper presents 3 examples that highlight its diverse application scenarios, including personalized response generation and the resolution of knowledge conflicts."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Personalized Response Generation",
            "text": "###figure_6### ###figure_7### LLMs tailored to individuals consider individual preferences and requirements, thereby enhancing service precision and user satisfaction.\nBaek et al. (2024  ###reference_b3###) maintain an entity-centric knowledge base from the user’s search history, enriching LLM to provide customized services. This knowledge base reflects users’ current and potential interests. Upon receiving a novel query, the system initially retrieves relevant content. If the obtained entities correspond to those present in the user’s knowledge base, the system deems this information relevant, attributing higher credibility to the associated documents. Consequently, the CAG module\ncan generate personalized responses based on documents with credibility annotations, as illustrated in Figure 5(a)  ###reference_sf1###. Moreover, by maintaining user profiles to record preference,in recommendation scenarios, the system retrieves numerous documents based on user input and assigns credibility to documents based on their alignment with the user’s profile, achieving personalized and controllable recommendations, as show in Figure 5(b)  ###reference_sf2###."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Knowledge Conflict Resolution",
            "text": "###figure_8### In real-world scenarios, controversial questions are often encountered, and the retrieved documents tend to contain contradictory evidence. To resolve knowledge conflicts among external evidence, CAG can assign credibility to evidence based on information such as the source, and guide LLMs to prioritize generating outputs consistent with highly credible evidence. Figure 6  ###reference_### illustrates a simple example, where the sample question comes from a dataset specifically focused on controversial issues in real-world scenarios Wan et al. (2024  ###reference_b43###). Therefore, CAG can be utilized to resolve conflicts between public databases and private data, as well as between general knowledge bases and proprietary knowledge bases, by assigning high credibility to private data and proprietary knowledge bases."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Retrieval-Augmented Generation (Lewis et al., 2020  ###reference_b24###) integrates a retriever with a generator to improve text generation quality by relying solely on internal knowledge (Izacard and Grave, 2021  ###reference_b19###; Borgeaud et al., 2022  ###reference_b6###; Shi et al., 2023b  ###reference_b34###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This paper proposes Credibility-aware Generation to address the challenge of flawed information. To equip the model with CAG capabilities, we introduce a data transformation framework aimed at generating credibility-based dataset, upon which we fine-tune the model.\nTo effectively verify the ability of model credibility-aware generation to handle flawed information, we construct a benchmark from different real-world scenarios. Experimental results show that our model can effectively understand credibility, exhibiting robustness in the face of flawed information and significantly outperforming other models with retrieval augmentation.\nMoreover, our framework is widely applicable to various real-world scenarios, offering customizable, reliable, and controllable outcomes.\nFor instance, by constructing a unique interest library and profile for each user, and assigning credibility to retrieved documents based on this profile, personalized responses can be generated accordingly.\nThis paper also sheds light on many future directions such resolving knowledge conflicts and designing more systems to incorporate external knowledge into LLMs."
        }
    ]
}