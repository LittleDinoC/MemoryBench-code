{
    "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
    "abstract": "Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications.\nConventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications.\nOur proposed metric, termed “Revision Distance,” utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score.\nOur results show that for the easy-writing task, “Revision Distance” is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts.\nMoreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "You can’t manage what you can’t measure well.—Cruz-Cázares et al. 2013  ###reference_b7###\nWith the continuous development of large language models (LLMs) such as ChatGPT111https://openai.com/blog/chatgpt  ###reference_openai.com/blog/chatgpt###, GPT-4(OpenAI,  ###reference_b15###), and Llama(Touvron et al., 2023  ###reference_b20###), a plethora of application research and development work based on LLMs has emerged.\nDuring the model training phase, the main focus is optimizing the model’s loss in an isolated environment. However, LLM-based applications should be human-centered, prioritizing user experience and utility. This raises a key question: How do we evaluate LLM-based applications from a human-centric perspective?\n###figure_1### Imagining the scenario where developers employ automatic evaluation metrics (Lin, 2004  ###reference_b13###; Papineni et al., 2002  ###reference_b17###; Zhang et al., 2020  ###reference_b23###; Zhao et al., 2019  ###reference_b24###) like ROUGE(Lin, 2004  ###reference_b13###) to evaluate LLM-generated text for writing assistance debugging. Whereas ROUGE only provides a high-level evaluation score to measure textual surface similarity.\nSince disregarding end-users, the evaluation result is inadequate and misaligns with user needs and preferences.\nTo address this gap, we explore alternative human-centered evaluation metrics, putting the user at the forefront of our evaluation.\nThis paper focuses on the prevalent application scenario for LLMs, specifically, the LLM-powered writing assistant in easy-writing scenarios and challenge-writing scenarios from email, and letter writing to academic writing 222We use the “Related Work” section Generation (RWG) (Liu et al., 2023  ###reference_b14###; Chen et al., 2021  ###reference_b4###) as the testbed for academic writing, which requires heavy knowledge reasoning work and complex concept understanding ability..\nDuring the AI-human collaborative writing process, AI-generated text often requires extended revisions.\nAdditionally, recent studies suggest that LLMs can produce human-like behavior, such as providing human preferences feedback (Bai et al., 2022  ###reference_b1###; Lee et al., 2023  ###reference_b11###), conducting text quality evaluation (Chiang and Lee, 2023  ###reference_b5###; Fu et al., 2023  ###reference_b8###). Therefore, we assume that the LLM can be a proxy user for generating revision edits, aligning with actual human editing behaviors.\nDrawing from these insights, our proposed metric, , incorporates the iterative process of user-driven text revision. It quantifies the number of edits a user must take to an LLM-generated text to achieve a predefined quality threshold.\nIn the reference-based evaluation setting, we compared our metric with ROUGE, BERT-Score, and GPT-Score across two writing tasks: the easy-writing task and the challenge-writing task. For each task, we sample texts from two models to form a comparison group. Then we apply text evaluation metrics to assess the text quality.\n(1) For the easy-writing task, we find that our metric consistently aligns with baseline metrics, supporting the intuition that a stronger model should produce texts with superior evaluation scores.\n(2) For more challenging tasks, our metrics can still provide stable and reliable evaluation results even if most of the baseline indicators encounter different issues.\nIn reference-free scenarios, the “Revision Distance” metric aligns closely with human judgment in approximately 76% of cases in the dataset from ultrafeedback dataset (Bartolome et al., 2023  ###reference_b2###). Furthermore, by categorizing the types of edits made, our metric provides a more fine-grained analysis than those metrics that only yield scores.\nThe contributions are listed as follows: 1) We highlight the significance of the end-user’s perspective in the text evaluation in the context of LLM-power writing assistant. 2) Aligning with real-world human editing behaviors, we propose a human-centered text evaluation metric, which provides a self-explain and fine-grained insight for developers and end-users. 3) Based on broad and various test tasks, we conduct an experiment to demonstrate the utility of our proposed human-centered metrics."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The text evaluation methods can be categorized into human evaluation and machine-generated approaches. Human evaluation is widely recognized as the most natural way to evaluate the quality of a given text, which often involves human annotators and qualitative analyses (Clark et al., 2021  ###reference_b6###; Belz et al., 2023  ###reference_b3###). This method is often expensive and time-consuming work and requires extensive domain expertise for domain-specific scenarios.\nOn the other hand, current automated evaluation methods tend to generate a comprehensive score that is facilitated in comparing new models with established state-of-the-art approaches. These include metrics such as ROUGE (Lin, 2004  ###reference_b13###), BLEU (Papineni et al., 2002  ###reference_b17###), BERTScore (Zhang et al., 2020  ###reference_b23###), MoverScore (Zhao et al., 2019  ###reference_b24###), BARTScore (Yuan et al., 2021  ###reference_b22###), and DiscoScore (Zhao et al., 2023a  ###reference_b25###) typically compute a similarity (or dissimilarity) score between a model-generated text and a reference text.\nLarge language models have been adeptly utilized for roles such as aiding in data annotation (Li et al., 2023  ###reference_b12###) and delivering feedback that mirrors human preferences (Bai et al., 2022  ###reference_b1###; Lee et al., 2023  ###reference_b11###; Pang et al., 2023  ###reference_b16###).\nFor the evaluation stage, Chiang and Lee (2023  ###reference_b5###) found that the LLM evaluation is consistent with the human evaluation results. The GPTScore (Fu et al., 2023  ###reference_b8###) has been proposed to score the model-generated text. Similarly, (Jain et al., 2023  ###reference_b9###) also studied the efficacy of LLMs as multi-dimensional evaluators.\nIn conclusion, current metrics tend to yield a comprehensive score that detaches the task context for model development and optimization. However, the ultimate application of LLMs is human-centered, prioritizing the user experience and utility.\nConsequently, a context-independent numerical score is insufficient in LLM application scenarios.\nOur proposed metric shifts the text evaluation to a human-centered perspective, which incorporates the iterative process of user-driven text revision."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Revision Distance",
            "text": "###figure_2### As depicted in Figure 2  ###reference_###, we frame the context of AI-powered writing assistance, with LLMs serving dual functions: as the proxy of the user () and as the generator (). The  acts as the pivotal component of the writing assistant application.\nGiven the user input content, the  generates a draft , such as emails, letters, articles, and “Related Work” sections. The  quantifies the number of edits from .\nFor the reference-based evaluation setting, we utilize the human-written text or ChatGPT output as the ground truth. The  is designed to produce structured revision edits, improving the consistency of the  to the ground-truth text . In scenarios where no ground truth text is available, we require the  to refine the given text towards an ideal form, as envisioned by the  itself333This ideal version is not explicitly generated but rather serves as an implicit standard within the revision edits generation prompt..\nThese revision edits are produced to improve  to closer align with the ideal version, which mimics the revision process of human writers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation for Reference-based Setting",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Task and Dataset",
            "text": "To validate the utility of our proposed metric, we have constructed two distinct datasets to address both the easy-writing task and the challenge-writing task. The challenge-writing task refers to the scenario that requires heavy knowledge reasoning and complex concept understanding. For the easy-writing task, we use the task of emails, letters, and articles generation as a testbed. For the challenge-writing task, we employ academic writing as the testbed. The test dataset details in this evaluation setting are described in Appendix A  ###reference_###."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Text Generation Models",
            "text": "To assess the discriminative capacity of our revision distance metric, we designed strong and weak writing applications. The terms “strong” and “weak” refer to the generation ability of utilized LLM, as detailed in Table 1  ###reference_###. (1) For the easy-writing task, we employ two Mistral-series models (Jiang et al., 2023  ###reference_b10###); (2) For the challenge-writing task, we employ GPT-4 and its variant 444The models employed in both tasks are detailed in Appendix B  ###reference_### and Appendix C  ###reference_###, respectively..\n###figure_3### ###figure_4###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Result Analysis",
            "text": "###figure_5### ###figure_6### ###figure_7### ###figure_8### As shown in Table 2  ###reference_###, our metric shows utility for easy and challenging writing tasks. Different from other metrics, smaller  indicate better text quality.\nTo assess the metric’s ability to differentiate between models, we calculate the relative change rate from the “Weak” model to the “Strong” model.\nNotably, existing metrics have reached saturation for the easy-writing tasks, exhibiting a limited relative change rate regarding the performance of distinct models. Conversely, our metric demonstrates better efficacy in discerning the nuanced capabilities of diverse models.\nIt’s observed that  yields a larger change rate, highlighting the enhanced discriminative capacity of our metric.\nAdditionally, for the complex academic writing task, we conducted a human evaluation555We selected 20 paragraphs from both methods for expert analysis. Five AI field specialists assessed the LLM-generated content, focusing on content quality, structural coherence, and argumentative strength.. Based on the evaluation results, we categorized texts as “Chosen” or “Rejected.” Our  metric aligns with human preferences, indicating superior text quality with fewer revisions for “Chosen Texts.” In contrast, the ROUGE metric often misaligns with human judgments, erroneously assigning higher scores to “Rejected Texts.”"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation for Reference-free Setting",
            "text": "To demonstrate the performance of our evaluation method in scenarios, where ground truth is unavailable, we extracted 41 cases related to writing tasks from the UltraFeedback dataset(Bartolome et al., 2023  ###reference_b2###). Each case contains a chosen response and a rejected response.\nWhen applied to the selected cases, our “Revision Distance” metric aligns with human judgments in 76% of instances, indicating that chosen responses typically necessitated fewer revisions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Qualitative Analysis",
            "text": "Based on the analysis of revision edit details, we classify the revision actions into three categories: (1) Reference Order Revision, (2) Reference Comparison Revision, and (3) Reference Description Revision. The description of three categories is shown in Appendix LABEL:cat_rev.\nFor complex writing tasks, the challenge often lies in knowledge reasoning of concepts. CoT prompting can dramatically improve the multi-step reasoning abilities of LLMs (Wang et al., 2023  ###reference_b21###). As shown in Table 3  ###reference_###, CoT-based GPT-4 can provide text with fewer revisions related to Order and Comparison issues in “Related work” writing tasks.\nThe improvements can be attributed to the enhanced knowledge reasoning capabilities of the CoT-based method. There also exists a slight decline in the reference description dimension.\nIn conclusion, the fine-grained analysis revision edits can provide insightful feedback for future model improvement."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "With the rapid advancement of LLM-based applications, the pivotal question arises:\n“how can we evaluate LLM-based applications from a human-centered perspective?”\nExisting evaluation metrics, typically used for model development, merely yield a context-independent numerical score, lacking user relevance.\nOur research shifts text evaluation from a predominantly model-centered perspective to a human-centered one.\nUsing the LLM-powered writing assistant as a test scenario, we take a comprehensive experiment on diverse writing tasks to validate the effectiveness and reliability of our “Revision Distance” metric.\nThis metric converts text evaluation into contextualized text revisions, clearly highlighting textual discrepancies and offering users a detailed, transparent rationale for the scores.\nOur findings demonstrate the metric’s applicability and dependability in both reference-based and reference-free contexts."
        }
    ]
}