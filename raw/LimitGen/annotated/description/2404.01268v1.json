{
    "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
    "abstract": "Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs.\nTo address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the release of ChatGPT in late 2022, anecdotal examples of both published papers (Okunytė, 2023  ###reference_b49###; Deguerin, 2024  ###reference_b15###) and peer reviews (Oransky & Marcus, 2024  ###reference_b51###) which appear to be ChatGPT-generated have inspired humor and concern.111Increased attention to ChatGPT-use by multilingual scholars has also brought to the fore important conversations about entrenched linguistic discrimination in academic publishing (Khanna et al., 2022  ###reference_b34###). While certain tells, such as “regenerate response” (Conroy, 2023b  ###reference_b13###; a  ###reference_b12###) and “as an AI language model” (Vincent, 2023  ###reference_b66###), found in published papers indicate modified content, less obvious cases are nearly impossible to detect at the individual level (Else, 2023  ###reference_b18###; Gao et al., 2022  ###reference_b22###). Liang et al. (2024  ###reference_b42###) present a method for detecting the percentage of LLM-modified text in a corpus beyond such obvious cases. Applied to scientific publishing, the importance of this at-scale approach is two-fold: first, rather than looking at LLM-use as a type of rule-breaking on an individual level, we can begin to uncover structural circumstances which might motivate its use. Second, by examining LLM-use in academic publishing at-scale, we can capture epistemic and linguistic shifts, miniscule at the individual level, which become apparent with a birdseye view.\nMeasuring the extent of LLM-use on scientific publishing has urgent applications. Concerns about accuracy, plagiarism, anonymity, and ownership have prompted some prominent scientific institutions to take a stance on the use of LLM-modified content in academic publications. The International Conference on Machine Learning (ICML) 2023, a major machine learning conference, has prohibited the inclusion of text generated by LLMs like ChatGPT in submitted papers, unless the generated text is used as part of the paper’s experimental analysis (ICML, 2023  ###reference_b30###). Similarly, the journal Science has announced an update to their editorial policies, specifying that text, figures, images, or graphics generated by ChatGPT or any other LLM tools cannot be used in published works (Thorp, 2023  ###reference_b58###). Taking steps to measure the extent of LLM-use can offer a first-step in identifying risks to the scientific publishing ecosystem.\nFurthermore, exploring the circumstances in which LLM-use is high can offer publishers and academic institutions useful insight into author behavior. Sites of high LLM-use can act as indicators for structural challenges faced by scholars. These range from pressures to “publish or perish” which encourage rapid production of papers to concerns about linguistic discrimination that might lead authors to use LLMs as prose editors.\nWe conduct the first systematic, large-scale analysis to quantify the prevalence of LLM-modified content across multiple academic platforms, extending a recently proposed, state-of-the-art distributional GPT quantification framework (Liang et al., 2024  ###reference_b42###) for estimating the fraction of AI-modified content in a corpus. Throughout this paper, we use the term “LLM-modified” to refer to text content substantially updated by ChatGPT beyond basic spelling and grammatical edits. Modifications we capture in our analysis could include, for example, summaries of existing writing or the generation of prose based on outlines.\nA key characteristic of this framework is that it operates on the population level, without the need to perform inference on any individual instance. As validated in the prior paper, the framework is orders of magnitude more computationally efficient and thus scalable, produces more accurate estimates, and generalizes better than its counterparts under significant temporal distribution shifts and other realistic distribution shifts.\nWe apply this framework to the abstracts and introductions (Figures 1  ###reference_### and 7  ###reference_###) of academic papers across multiple academic disciplines,including arXiv, bioRxiv, and 15 journals within the Nature portfolio, such as Nature,  Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications. Our study analyzes a total of 950,965 papers published between January 2020 and February 2024, comprising 773,147 papers from arXiv, 161,280 from bioRxiv, and 16,538 from the Nature portfolio journals. The papers from arXiv cover multiple academic fields, including Computer Science, Electrical Engineering and Systems Science, Mathematics, Physics, and Statistics. These datasets allow us to quantify the prevalence of LLM-modified academic writing over time and across a broad range of academic fields.\nOur results indicate that the largest and fastest growth was observed in Computer Science papers, with  reaching 17.5% for abstracts and 15.3% for introductions by February 2024. In contrast, Mathematics papers and the Nature portfolio showed the least increase, with  reaching 4.9% and 6.3% for abstracts and 3.5% and 6.4% for introductions, respectively.\nMoreover, our analysis reveals at an aggregate level that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently and papers with shorter lengths. Results also demonstrate a closer relationship between papers with LLM-modifications, which could indicate higher use in more crowded fields of study (as measured by the distance to the nearest neighboring paper in the embedding space), or that generated-text is flattening writing diversity."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Various methods have been proposed for detecting LLM-modified text, including zero-shot approaches that rely on psychological signatures characteristic of machine-generated content (Lavergne et al., 2008  ###reference_b38###; Badaskar et al., 2008  ###reference_b3###; Beresneva, 2016  ###reference_b6###; Solaiman et al., 2019  ###reference_b57###; Mitchell et al., 2023a  ###reference_b47###; Yang et al., 2023a  ###reference_b70###; Bao et al., 2023  ###reference_b5###; Tulchinskii et al., 2023  ###reference_b62###) and training-based methods that finetune language models for binary classification of human vs. LLM-modified text (Bhagat & Hovy, 2013  ###reference_b7###; Zellers et al., 2019  ###reference_b74###; Bakhtin et al., 2019  ###reference_b4###; Uchendu et al., 2020  ###reference_b63###; Chen et al., 2023  ###reference_b10###; Yu et al., 2023  ###reference_b73###; Li et al., 2023  ###reference_b39###; Liu et al., 2022  ###reference_b44###; Bhattacharjee et al., 2023  ###reference_b8###; Hu et al., 2023a  ###reference_b28###). However, these approaches face challenges such as the need for access to LLM internals, overfitting to training data and language models, vulnerability to adversarial attacks (Wolff, 2020  ###reference_b68###), and bias against non-dominant language varieties (Liang et al., 2023a  ###reference_b40###). The effectiveness and reliability of publicly available LLM-modified text detectors have also been questioned (OpenAI, 2019  ###reference_b50###; Jawahar et al., 2020  ###reference_b32###; Fagni et al., 2021  ###reference_b19###; Ippolito et al., 2019  ###reference_b31###; Mitchell et al., 2023b  ###reference_b48###; Gehrmann et al., 2019  ###reference_b24###; Heikkilä, 2022  ###reference_b27###; Crothers et al., 2022  ###reference_b14###; Solaiman et al., 2019  ###reference_b57###; Kirchner et al., 2023  ###reference_b36###; Kelly, 2023  ###reference_b33###), with the theoretical possibility of accurate instance-level detection being debated (Weber-Wulff et al., 2023  ###reference_b67###; Sadasivan et al., 2023  ###reference_b53###; Chakraborty et al., 2023  ###reference_b9###). In this study, we apply the recently proposed distributional GPT quantification framework (Liang et al., 2024  ###reference_b42###), which estimates the fraction of LLM-modified content in a text corpus at the population level, circumventing the need for classifying individual documents or sentences and improving upon the stability, accuracy, and computational efficiency of existing approaches. A more comprehensive discussion of related work can be found in Appendix G  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background: the distributional LLM quantification framework",
            "text": "We adapt the distributional LLM quantification framework from Liang et al. (2024  ###reference_b42###) to quantify the use of AI-modified academic writing. The framework consists of the following steps:\nProblem formulation: Let  and  be the probability distributions of human-written and LLM-modified documents, respectively. The mixture distribution is given by , where  is the fraction of AI-modified documents. The goal is to estimate  based on observed documents .\nParameterization: To make  identifiable, the framework models the distributions of token occurrences in human-written and LLM-modified documents, denoted as  and , respectively, for a chosen list of tokens . The occurrence probabilities of each token in human-written and LLM-modified documents,  and , are used to parameterize  and :\nEstimation: The occurrence probabilities  and  are estimated using collections of known human-written and LLM-modified documents,  and , respectively:\nInference: The fraction  is estimated by maximizing the log-likelihood of the observed documents under the mixture distribution :\nLiang et al. (2024  ###reference_b42###) demonstrate that the data points  can be constructed either as a document or as a sentence, and both work well. Following their method, we use sentences as the unit of data points for the estimates for the main results. In addition, we extend this framework for our application to academic papers with two key differences:\nWe use a two-stage approach to generate LLM-produced text, as simply prompting an LLM with paper titles or keywords would result in unrealistic scientific writing samples containing fabricated results, evidence, and ungrounded or hallucinated claims.\nSpecifically, given a paragraph from a paper known to not include LLM-modification, we first perform abstractive summarization using an LLM to extract key contents in the form of an outline. We then prompt the LLM to generate a full paragraph based the outline (see Appendix for full prompts).\nOur two-stage approach can be considered a counterfactual framework for generating LLM text: given a paragraph written entirely by a human, how would the text read if it conveyed almost the same content but was generated by an LLM? This additional abstractive summarization step can be seen as the control for the content.\nThis approach also simulates how scientists may be using LLMs in the writing process, where the scientists first write the outline themselves and then use LLMs to generate the full paragraph based on the outline.\nWe use the full vocabulary instead of only adjectives, as our validation shows that adjectives, adverbs, and verbs all perform well in our application (Figure 3  ###reference_###).\nUsing the full vocabulary minimizes design biases stemming from vocabulary selection. We also find that using the full vocabulary is more sample-efficient in producing stable estimates, as indicated by their smaller confidence intervals by bootstrap.\n###figure_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Implementation and Validations",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Collection and Sampling",
            "text": "We collect data from three sources: arXiv, bioRxiv, and 15 journals from the Nature portfolio. For each source, we randomly sample up to 2,000 papers per month from January 2020 to February 2024. The procedure for generating the LLM-generated corpus data is described in Section  3  ###reference_###. We focused on the introduction sections for the main texts, as the introduction was the most consistently and commonly occurring section across diverse categories of papers.\nSee Appendix C  ###reference_### for comprehensive implementation details."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Data Split, Model Fitting, and Evaluation",
            "text": "For model training, we count word frequencies for scientific papers written before the release of ChatGPT and the LLM-modified corpora described in Section 3  ###reference_###. We fit the model with data from 2020, and use data from January 2021 onwards for validation and inference. We fit separate models for abstracts and introductions for each major category.\nTo evaluate model accuracy and calibration under temporal distribution shift, we use 3,000 papers from January 1, 2022, to November 29, 2022, a time period prior to the release of ChatGPT, as the validation data. We construct validation sets with LLM-modified content proportions () ranging from 0% to 25%, in 5% increments, and compared the model’s estimated  with the ground truth  (Figure 3  ###reference_###). Full vocabulary, adjectives, adverbs, and verbs all performed well in our application, with a prediction error consistently less than 3.5% at the population level across various ground truth  values (Figure 3  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Main Results and Findings",
            "text": "We apply the model to estimate the fraction of LLM-modified content () for each paper category each month, for both abstracts and introductions. Each point in time was independently estimated, with no temporal smoothing or continuity assumptions applied.\nOur findings reveal a steady increase in the fraction of AI-modified content () in both the abstracts (Figure 1  ###reference_###) and the introductions (Figure 7  ###reference_###), with the largest and fastest growth observed in Computer Science papers. By February 2024, the estimated  for Computer Science had increased to 17.5% for abstracts and 15.5% for introductions. The second-fastest growth was observed in Electrical Engineering and Systems Science, with the estimated  reaching 14.4% for abstracts and 12.4% for introductions during the same period. In contrast, Mathematics papers and the Nature portfolio showed the least increase. By the end of the studied period, the estimated  for Mathematics had increased to 4.9% for abstracts and 3.9% for introductions, while the estimated  for the Nature portfolio had reached 6.3% for abstracts and 4.3% for introductions.\nThe November 2022 estimates serve as a pre-ChatGPT reference point for comparison, as ChatGPT was launched on November 30, 2022. The estimated  for Computer Science in November 2022 was 2.3%, while for Electrical Engineering and Systems Science, Mathematics, and the Nature portfolio, the estimates were 2.9%, 2.4%, and 3.1%, respectively. These values are consistent with the false positive rate reported in the earlier section ( 4.2  ###reference_###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Temporal Trends in AI-Modified Academic Writing",
            "text": "We apply the model to estimate the fraction of LLM-modified content () for each paper category each month, for both abstracts and introductions. Each point in time was independently estimated, with no temporal smoothing or continuity assumptions applied.\nOur findings reveal a steady increase in the fraction of AI-modified content () in both the abstracts (Figure 1  ###reference_###  ###reference_###) and the introductions (Figure 7  ###reference_###  ###reference_###), with the largest and fastest growth observed in Computer Science papers. By February 2024, the estimated  for Computer Science had increased to 17.5% for abstracts and 15.5% for introductions. The second-fastest growth was observed in Electrical Engineering and Systems Science, with the estimated  reaching 14.4% for abstracts and 12.4% for introductions during the same period. In contrast, Mathematics papers and the Nature portfolio showed the least increase. By the end of the studied period, the estimated  for Mathematics had increased to 4.9% for abstracts and 3.9% for introductions, while the estimated  for the Nature portfolio had reached 6.3% for abstracts and 4.3% for introductions.\nThe November 2022 estimates serve as a pre-ChatGPT reference point for comparison, as ChatGPT was launched on November 30, 2022. The estimated  for Computer Science in November 2022 was 2.3%, while for Electrical Engineering and Systems Science, Mathematics, and the Nature portfolio, the estimates were 2.9%, 2.4%, and 3.1%, respectively. These values are consistent with the false positive rate reported in the earlier section ( 4.2  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Relationship Between First-Author Preprint Posting Frequency and GPT Usage",
            "text": "We found a notable correlation between the number of preprints posted by the first author on arXiv and the estimated number of LLM-modified sentences in their academic writing. Papers were stratified into two groups based on the number of first-authored arXiv Computer Science preprints by the first author in the year: those with two or fewer () preprints and those with three or more () preprints (Figure 4  ###reference_###). We used the 2023 author grouping for the 2024.1-2 data, as we don’t have the complete 2024 author data yet.\nBy February 2024, abstracts of papers whose first authors had  preprints in 2023 showed an estimated 19.3% of sentences modified by AI, compared to 15.6% for papers whose first authors had  preprints (Figure 4  ###reference_###a). We observe a similar trend in the introduction sections, with first authors posting more preprints having an estimated 16.9% LLM-modified sentences, compared to 13.7% for first authors posting fewer preprints (Figure 4  ###reference_###b).\nSince the first-author preprint posting frequency may be confounded by research field, we conduct an additional robustness check for our findings. We find that the observed trend holds for each of the three arXiv Computer Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.LG (Machine Learning), and cs.CL (Computation and Language) (Supp Figure 12  ###reference_###).\nOur results suggest that researchers posting more preprints tend to utilize LLMs more extensively in their writing. One interpretation of this effect could be that the increasingly competitive and fast-paced nature of CS research communities incentivizes taking steps to accelerate the writing process. We do not evaluate whether these preprints were accepted for publication.\n###figure_2###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Relationship Between Paper Similarity and LLM Usage",
            "text": "We investigate the relationship between a paper’s similarity to its closest peer and the estimated LLM usage in the abstract. To measure similarity, we first embed each abstract from the arXiv Computer Science papers using OpenAI’s text-embedding-ada-002 model, creating a vector representation for each abstract. We then calculate the distance between each paper’s vector and its nearest neighbor within the arXiv Computer Science abstracts. Based on this similarity measure we divide papers into two groups: those more similar to their closest peer (below median distance) and those less similar (above median distance).\nThe temporal trends of LLM usage for these two groups are shown in Figure 5  ###reference_###. After the release of ChatGPT, papers most similar to their closest peer consistently showed higher LLM usage compared to those least similar. By February 2024, the abstracts of papers more similar to their closest peer had an estimated 22.2% of sentences modified by LLMs, compared to 14.7% for papers less similar to their closest peer.\nTo account for potential confounding effects of research fields, we conducted an additional robustness check by measuring the nearest neighbor distance within each of the three arXiv Computer Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.LG (Machine Learning), and cs.CL (Computation and Language), and found that the observed trend holds for each sub-category (Supp Figure 13  ###reference_###).\nThere are several ways to interpret these findings. First, LLM-use in writing could cause the similarity in writing or content. Community pressures may even motivate scholars to try to sound more similar – to assimilate to the “style” of text generated by an LLM.\nAlternatively, LLMs may be more commonly used in research areas where papers tend to be more similar to each other. This could be due to the competitive nature of these crowded subfields, which may pressure researchers to write faster and produce similar findings. Future interdisciplinary research should explore these hypotheses.\n###figure_3###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Relationship Between Paper Length and AI Usage",
            "text": "We also explored the association between paper length and LLM usage in arXiv Computer Science papers. Papers were stratified by their full text word count, including appendices, into two bins: below or above 5,000 words (the rounded median).\nFigure 6  ###reference_### shows the temporal trends of LLM usage for these two groups. After the release of ChatGPT, shorter papers consistently showed higher AI usage compared to longer papers. By February 2024, the abstracts of shorter papers had an estimated 17.7% of sentences modified by LLMs, compared to 13.6% for longer papers (Figure 6  ###reference_###a). A similar trend was observed in the introduction sections (Figure 6  ###reference_###b).\nTo account for potential confounding effects of research fields, we conducted an additional robustness check. The finding holds for both cs.CV (Computer Vision and Pattern Recognition) and cs.LG (Machine Learning) (Supp Figure 14  ###reference_###). However, for cs.CL (Computation and Language), we found no significant difference in LLM usage between shorter and longer papers, possibly due to the limited sample size, as we only parsed a subset of the PDFs and calculated their full length.\nAs Computer Science conference papers typically have a fixed page limit, longer papers likely have more substantial content in the appendix. The lower LLM usage in these papers may suggest that researchers with more comprehensive work rely less on LLM-assistance in their writing. However, further investigation is needed to determine the relationship between paper length, content comprehensiveness, and the quality of the research.\n###figure_4###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our findings show a sharp increase in the estimated fraction of LLM-modified content in academic writing beginning about five months after the release of ChatGPT, with the fastest growth observed in Computer Science papers. This trend may be partially explained by Computer Science researchers’ familiarity with and access to large language models. Additionally, the fast-paced nature of LLM research and the associated pressure to publish quickly may incentivize the use of LLM writing assistance (Foster et al., 2015  ###reference_b21###).\nWe expose several other factors associated with higher LLM usage in academic writing. First, authors who post preprints more frequently show a higher fraction of LLM-modified content in their writing. Second, papers in more crowded research areas, where papers tend to be more similar, showed higher LLM-modification compared to those in less crowded areas. Third, shorter papers consistently showed higher LLM-modification compared to longer papers, which may indicate that researchers working under time constraints are more likely to rely on AI for writing assistance. These results may be an indicator of the competitive nature of certain research areas and the pressure to publish quickly.\nIf the majority of modification comes from an LLM owned by a private company, there could be risks to the security and independence of scientific practice.\nWe hope our results inspire\nfurther studies of widespread LLM-modified text and conversations about how to promote transparent, epistemically diverse, accurate, and independent scientific publishing.\nWhile our study focused on ChatGPT, which accounts for more than three-quarters of worldwide internet traffic in the category (Van Rossum, 2024  ###reference_b64###), we acknowledge that there are other large language models used for assisting academic writing.\nFurthermore, while Liang et al. (2023a  ###reference_b40###) demonstrate that GPT-detection methods can falsely identify the writing of language learners as LLM-generated, our results showed that consistently low false positives estimates of  in 2022, which contains a significant fraction of texts written by multilingual scholars. We recognize that significant author population changes (MacroPolo, 2024  ###reference_b46###) or other language-use shifts could still impact the accuracy of our estimates. Finally, the associations that we observe between LLM usage and paper characteristics are correlations which could be affected by other factors such as research topics. More causal studies is an important direction for future work."
        }
    ]
}