{
    "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
    "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing their weights, which is a particularly promising capability for long-context LLMs that can potentially learn from many examples.\nRecently, Lin et al. (2024) proposed Urial, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance.\nIn this work, we show that, while effective, ICL alignment with Urial still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LMs.\nUnlike for tasks such as classification, translation, or summarization, adding more ICL demonstrations for long-context LLMs does not systematically improve instruction following performance.\nTo address this limitation, we derive a greedy selection approach for ICL examples that noticeably improves performance, yet without bridging the gap to instruction fine-tuning.\nFinally, we provide a series of ablation studies to better understand the reasons behind the remaining gap, and we show how some aspects of ICL depart from the existing knowledge and are specific to the instruction tuning setting.\nOverall, our work advances the understanding of ICL as an alignment technique.\nWe provide our code at https://github.com/tml-epfl/icl-alignment.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities.\nHowever, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models.\nTraditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Chung et al., 2022  ###reference_b4###) or preference data (DPO and KTO, Rafailov et al., 2023  ###reference_b14###; Ethayarajh et al., 2024  ###reference_b5###), and reinforcement learning (RLHF, Ouyang et al., 2022  ###reference_b13###).\nInspired by Brown et al. (2020  ###reference_b3###) who showed that LLMs can learn from demonstrations provided as part of the input—a concept known as in-context learning (ICL)—Lin et al. (2024  ###reference_b10###) have recently studied the feasibility of in-context alignment.\nThey found that including merely three carefully selected question-answer pairs in the prompt is sufficient to make base models follow instructions and interact with users at a similar level to instruction-finetuned models on their own benchmark.\nThe possibility of customizing base models via ICL without changing weights is potentially game-changing due to its simplicity and flexibility.\nIn this work, we evaluate the Urial prompt strategy proposed by Lin et al. (2024  ###reference_b10###) across several base models, including GPT-4-Base111We received access to the base GPT-4 model via the OpenAI Researcher Access Program., on established instruction following benchmarks like MT-Bench (Zheng et al., 2023  ###reference_b18###) and AlpacaEval 2.0 (Li et al., 2023  ###reference_b9###).\nAlthough Urial achieves reasonable performance, it still lags behind instruction-finetuned models.\nIn response, we test various strategies to improve Urial, particularly leveraging models with extensive context windows which allow for longer in-context prompts.\nFirst, we study the effect of many-shot in-context learning by adding high-quality demonstrations from existing instruction datasets.\nThis approach, however, leads only to limited improvements compared to Urial, without a clear improvement trend for a large number of in-context examples.\nThis is in contrast to many-shot ICL for tasks like summarization, translation, or classification, where providing many examples is clearly beneficial (Agarwal et al., 2024  ###reference_b1###; Bertsch et al., 2024  ###reference_b2###).\nThen we propose a simple greedy algorithm to select the in-context examples which optimize the MT-Bench score.\nThis selection scheme allows us to nearly close the gap between Mistral-7b-v0.2 and Mistral-7b-Instruct-v0.2 by adding two additional demonstrations to the Urial prompt.\nFinally, we revisit the findings of Min et al. (2022  ###reference_b11###) regarding what makes ICL effective: unlike for classification and multiple choice tasks, providing questions with correct answers is crucial for the effectiveness of ICL on instruction-following tasks. Furthermore, adding incorrect demonstrations further degrades the model performance.\nOverall, our work provides a more nuanced picture of ICL as an alignment technique compared to previous works."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Systematic evaluation of Urial",
            "text": "First, we compare the performance of several base models with the Urial in-context prompt to that of their instruction fine-tuned versions.\nTable 1  ###reference_### shows the results on MT-Bench (Zheng et al., 2023  ###reference_b18###) which is one of the most popular benchmarks for instruction following ability of LLMs.\nWe note, however, that Lin et al. (2024  ###reference_b10###) originally tested Urial on their own dataset, in which only  of the examples are obtained from MT-Bench.\nWe compare several LLMs of different sizes and capabilities, ranging from Llama-2-7B (Touvron et al., 2023  ###reference_b16###) to the base GPT-4 model (OpenAI, 2023  ###reference_b12###).\nFor simplicity, we consider the official instruction-tuned LLMs, i.e., provided from the same source of the base models, even if there exist several third-party fine-tuned versions which can achieve higher scores.\nWe observe that base models with Urial achieve competitive performance on the 1st-turn score, but cannot match the performance of their instruction fine-tuned counterparts in all cases except for Llama-2-70B and Mistral-7B-v0.1 (note that both were originally included in Lin et al. (2024  ###reference_b10###), unlike most others). Conversely, the 2nd-turn score with Urial is significantly worse than for instruction-tuned LLMs:\nwe hypothesize that this is because no multi-turn demonstrations are given in context. Indeed, Zhou et al. (2023  ###reference_b19###) show that, even in IFT, having a few multi-turn training examples significantly improve the performance of the model on multi-turn conversations.\nBecause of this, in the rest of the paper, we focus on the 1st-turn score and single-round conversations, and leave extending ICL for multi-round conversations to future work.\nWe detail the breakdown of the results from Table 1  ###reference_### over categories in Table 5  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Many-shot ICL for instruction following",
            "text": "In the following, we explore whether we can close the gap between ICL and fine-tuning on instruction following tasks.\nIn particular, we focus on Mistral-7B-v0.2 (Jiang et al., 2023  ###reference_b7###) since (a) both Urial and the aligned model achieve competitive performance in Table 1  ###reference_###, and (b) this base model has a 32k context window which allows us to use many ICL examples.\nMoreover, we measure performance with both the MT-Bench 1st-turn score and the AlpacaEval 2.0 win rate, and report the results in Table 2  ###reference_###. More details can be found in App. A  ###reference_###.\n###figure_1### Experimenting with different selection criteria.\nFirst, we test adding random demonstrations (after simple pre-filtering) to the three already included in Urial. For this, we score the quality of the examples in Evol-Instruct-70k (Xu et al., 2024  ###reference_b17###) with GPT-3.5-Turbo, and randomly sample demonstrations among the highest quality instructions (those with score at least 4.5 on a 0-5 scale, see Fig. 4  ###reference_###).\nTable 2  ###reference_### shows that adding 17 or 37 random examples (which makes 20 and 40 examples in total) provides only moderate improvements, around , and Fig. 1  ###reference_### indicates that such an increase quickly reaches a plateau (see the Evol-Instruct-Random curve), without clear benefits from scaling up to 50 demonstrations.\nWe also test a more refined strategy: we sample instructions from OpenHermes (Teknium, 2023  ###reference_b15###), which includes annotations on the topic of each example, to ensure diversity between categories. This approach is however worse than Urial, regardless of the number of ICL examples used (see the OpenHermes-Diversity curve in Fig. 1  ###reference_###).\nFinally, inspired by the unsupervised ICL of Agarwal et al. (2024  ###reference_b1###), we re-use the OpenHermes examples, this time without the answers (questions only) and adding instead the three Urial complete demonstrations (both questions and answers).\nThis strategy significantly improves the MT-Bench score (from 7.0 to 7.5, see the OpenHermes-Unsupervised curve) and is slightly better than using the random examples from Evol-Instruct.\n###figure_2### Adding examples based on greedy search.\nNext, we try to greedily maximize the MT-Bench score by testing 100 high-quality instructions from Evol-Instruct-70k as the 4th additional example to Urial.\nFor each resulting ICL prompt, we compute the MT-Bench score with GPT-4-Turbo instead of GPT-4 as the former is faster, cheaper, and mitigates potential overfitting to the exact benchmark score. In most cases, there is an improvement, although it is often small (see the score distribution in Fig. 2  ###reference_###).\nComputing the true MT-Bench (GPT-4 as judge) with the best performing point as the 4th ICL example yields a significant improvement over Urial, from 6.99 to 7.46 (Table 2  ###reference_###).\nWe then repeat this procedure sequentially to find a 5th and a 6th demonstration while restricting the search space at each round (by keeping only the instructions leading to a high enough MT-Bench score) to reduce the computational cost.\nAs shown in Table 2  ###reference_###, this strategy achieves 7.69 MT-Bench score, almost closing the gap with Mistral-7B-Instruct-v0.2 (8.06). In addition, it even improves AlpacaEval 2.0 win rate from 8.09 of Urial to 9.22 without directly optimizing for it. This improvement indicates that the found IC prompt does not completely overfit to the MT-Bench score.\nFinally, we use the scores obtained in the first round of the greedy search to scale-up the number of ICL examples: the Evol-Instruct-Greedy curve in Fig. 1  ###reference_### is obtained by progressively adding the instructions with the highest scores. However, adding demonstrations in this way beyond the 6 examples does not consistently improve the MT-Bench score.\nTakeaways. These results show that, unlike in the setting of Agarwal et al. (2024  ###reference_b1###); Bertsch et al. (2024  ###reference_b2###), simply scaling the number of ICL examples is not sufficient to consistently improve the instruction following performance.\nHowever, a greedy search approach can help to bridge the gap to the results of fine-tuned models. We highlight that our greedy search is performed with a limited computational cost, i.e., the set of candidate examples is small and the judge is slightly suboptimal (GPT-4-Turbo instead of GPT-4). We expect that more effective ICL prompts could be found with additional resources."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Understanding ICL for instruction following",
            "text": "In the following, we provide further analyses of the challenges associated with ICL in the context of alignment."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Transferability of in-context examples",
            "text": "In Table 4  ###reference_###, we report the performance of applying the in-context examples found by greedy search (added to Urial) on Mistral-7B-v0.2-32k (see Sec. 3  ###reference_###) to Llama-2-7B-80k (Fu et al., 2024  ###reference_b6###) and Mixtral-8x22B-v0.1-4bit (Jiang et al., 2024  ###reference_b8###).\nAdding the new examples does not provide a consistent improvement: while it can increase the MT-Bench score (+0.47 on Llama-2-7B-80k, +0.30 on Mixtral-8x22B-v0.1-4bit), it can also, in some cases, give worse results than the original Urial.\nSimilarly, it yields mixed results when measured by win rate on AlpacaEval 2.0, which was not optimized by the greedy search.\nThis analysis suggests that the most effective ICL demonstrations might vary across base LLMs, potentially because of differences in their pre-training.\nThis could further explain why Urial underperforms on multiple models in Table 1  ###reference_###, especially on those which have become available only recently and were not used for selecting the URIAL examples."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions and discussion",
            "text": "In this work, we have first illustrated that ICL via Urial provides a good baseline for instruction-following alignment, but presents a few limitations: it mostly performs slightly worse than fine-tuning on single-turn conversations and does not generalize well to multi-round ones.\nFocusing on single-turn evaluation, we have shown that simply adding more in-context demonstrations is not sufficient to reliably improve performance, unlike what happens for less open-ended tasks (Agarwal et al., 2024  ###reference_b1###; Bertsch et al., 2024  ###reference_b2###).\nEven with more sophisticated approaches, fully closing the gap between ICL and IFT does not appear easy.\nWe have provided several analyses of why this could be the case, showing that ICL for instruction following presents different behavior compared to the tasks discussed in Min et al. (2022  ###reference_b11###): in our case, the demonstrations need to be carefully chosen and of high quality, with correct answers to each question.\nWe conjecture that via ICL, the LLM can learn to infer the response style, but the overall learning signal is not sufficiently strong to benefit from a large amount of examples, despite the very long context windows which have become available only recently. Interestingly, a few incorrect ICL demonstrations are sufficient to severely degrade the performance.\nIn general, LLMs still require many different post-training steps beyond simple instruction following: domain-specific continued pre-training, long-context fine-tuning, fine-tuning for function calling, safety training against jailbreaks, prevention of prompt injections, etc.\nThe difficulty of achieving even the baseline instruction-following alignment through ICL raises questions about the feasibility of obtaining more refined capabilities without fine-tuning."
        }
    ]
}