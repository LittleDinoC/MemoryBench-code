{
    "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
    "abstract": "Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs  proficient in Chinese.\nHowever, the evaluation of these models remains underdeveloped due to a lack of benchmarks.\nTo address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels.\nWith CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese.\nWe also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.111The dataset, evaluation scripts, and model outputs are released in https://github.com/zexuanqiu/CLongEval. We will regularly update the test results for newly released long-context LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks, including machine translation Hendy et al. (2023  ###reference_b15###); Jiao et al. (2023  ###reference_b20###), fact checking Huang et al. (2023  ###reference_b17###), text style transfer Reif et al. (2022  ###reference_b39###); Li et al. (2020  ###reference_b26###) and other generation tasks Hu et al. (2023  ###reference_b16###); Li et al. (2022  ###reference_b25###).\nTo enable LLMs to support more intricate and diverse applications, an increasing number of studies focus on extending the context window these models can handle. Consequently, many long-context LLMs that support Chinese have emerged, including both commercial models OpenAI (2023  ###reference_b33###) and open-source ones Cui (2023  ###reference_b9###); Bai et al. (2023a  ###reference_b2###); ZhupuAI (2023  ###reference_b50###); InternLMTeam (2024  ###reference_b18###), of which the context lengths span from 32K to 200K. Despite these developments, the efficacy of models in long-context settings remains underexamined, primarily due to the lack of a robust evaluation benchmark.\n###figure_1### Recently, a few benchmarks have been proposed for the evaluation of English long-context LLMs An et al. (2023  ###reference_b1###); Bai et al. (2023b  ###reference_b3###). As for the Chinese domain, there exists only a bilingual benchmark Bai et al. (2023b  ###reference_b3###), wherein only 5 out of 21 test tasks are designed for Chinese. This benchmark offers only 1K instances in total, with an average token length capped at approximately 13K222The reported average length is 13,386 characters, yielding at most 13K tokens after tokenization., rendering it inadequate for a comprehensive evaluation. Therefore, there is an urgent need for a high-quality Chinese benchmark for long-context LLMs. Considering the considerable advancements in this field, the establishment of such a benchmark facilitates a thorough investigation of existing models, which might bring insights to the research community.\nIn this paper, we present CLongEval, a benchmark designed for evaluating Chinese long-context LLMs. Prior to the construction of datasets, we conduct a systematic analysis of the key capabilities requisite for handling long context,\nto ensure a thorough assessment of the model’s functionality (§ 2  ###reference_###). Analogous to the human problem-solving paradigm, the basic functionalities of long-context LLMs can be conceptualized as: (1) the capacity to precisely identify and acquire the key information framing in either partial or full context; and (2)\nthe competence to reason out the answer based on the given information in either an extractive or abstractive manner.\nThese key abilities establish the evaluation framework behind CLongEval, which is illustrated in Figure 1  ###reference_###.\nTo accommodate models with varying spans of context windows, we consider three subsets within CLongEval: small set (1K-16K), medium set (16K-50K), and large set (50K-100K) (§ 3.1  ###reference_###).\nIn dataset construction, we select test tasks that correspond to the capabilities outlined in the evaluation framework. Moreover, we ensure that primary test tasks are highly aligned with real-life user scenarios so that the benchmark can accurately reflect models’ capability in practical applications Xiong et al. (2023  ###reference_b46###) (§ 3.2  ###reference_###).\nOverall, we craft 7 distinct tasks in CLongEval: 2 tasks are human-annotated, 1 task is GPT-4-annotated and 4 tasks are re-constructed from public datasets.\nWith CLongEval, we evaluate 8 long-context LLMs proficient in Chinese, including two commercial models known for their powerful long-text processing capability: Moonshot-v1-128K and GPT-4-Turbo-128K (§ 4.3  ###reference_###). We highlight several key findings in the long-context setting: (1) The commercial models consistently outperform open-source models across tasks, and the performance gap is particularly evident in tasks that primarily involve straightforward information extraction. (2) Extraction with full context is the most challenging setting. GPT-4-Turbo displays a more significant decline in performance as context length increases, compared to other settings. (3) For tasks requiring an understanding of partial context, the answer’s position within a long context does not consistently lead to significant performance fluctuations. More analysis and observations are elaborated in § 4.4  ###reference_### and Appendix A  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Evaluation Framework in CLongEval",
            "text": "To offer a thorough and systematic evaluation, we analyze the key capabilities necessary for the efficacy of long-context LLMs.\nGenerally speaking, the capacity for a long-context LLM in interpreting human textual instructions largely depends on its ability of information acquisition. Moreover, an indispensable ability for these models extends beyond mere information collection to encompass reasoning based on the assimilated information.\nIt refers to the capability to recognize and parse relevant information framed in extensive and complex textual input. It bottlenecks LLMs’ effectiveness in synthesizing its contextualized knowledge to execute a wide array of tasks, from answering questions to carrying out complex instructions. Moreover, as the length of the input text increases, maintaining a coherent and precise grasp of the input information becomes increasingly challenging.\nIn this evaluation dimension, we conduct a two-fold classification based on the distribution of information requisite for task fulfillment: full-context and partial-context information acquisition. For each category, we introduce an array of test tasks tailored to assess the model’s proficiency, including tasks that demand an accurate comprehension of the entire input (i.e. full-context), and those that rely on a correct understanding of selective snippets of the input (i.e. partial-context), respectively. Figure 1  ###reference_### illustrates the categorization of tasks that require either partial- or full-context in information acquisition.\nIt refers to the ability to perform the inferential process of synthesizing a conclusion from presented lengthy statements Pi et al. (2022  ###reference_b36###).\nIn real-world applications, most tasks require not only a precise understanding of the input text but also the capacity of reasoning based on the provided information. LLMs equipped with proficient reasoning abilities can navigate the problem-solving and decision-making procedure, both of which are crucial cognitive functions necessary for handling complex tasks Pomerol (1997  ###reference_b37###).\nIn the reasoning process, outputs can be synthesized through two ways: content extraction and abstraction. Accordingly, the evaluation dimension of reasoning incorporates these two distinct types of test tasks.\nThe abstraction tasks involve generating new content not explicitly in the source material, demanding a deeper understanding and recombination of input. In contrast, extraction tasks assess the model’s ability to directly identify and extract information from the input without altering the original content. This framework enables a nuanced evaluation of LLMs’ reasoning capabilities, including the capacity for generating novel insights and accurately retrieving information.\nFigure 1  ###reference_### illustrates the tasks to test the reasoning ability in either an extractive or abstractive manner.\nMin\nMax\n#Data\nMin\nMax\n#Data\nMin\nMax\n#Data\n1,693\n16,224\n294\n14,437\n50,553\n398\n49,636\n99,038\n299\n768\n15,589\n358\n14,669\n43,225\n353\n41,481\n88,731\n356\n1,022\n13,716\n300\n12,958\n48,677\n400\n42,046\n87,092\n300\n738\n13,521\n303\n13,635\n44,100\n402\n43,875\n84,475\n300\n1,069\n16,385\n550\n15,956\n51,016\n300\n51,404\n98,803\n150\n1,249\n17,830\n400\n18,006\n56,086\n400\n55,367\n95,073\n300\n1,773\n24,597\n400\n23,894\n74,004\n400\n73,984\n125,529\n300"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The CLongEval Benchmark",
            "text": "The Long Story QA task involves LLMs answering questions based on a context snippet from a long story. To excel at this task, the model should identify the relevant snippet and abstractively reason out the answer. Unlike the normative and objective nature of MultiFieldQA (Bai et al., 2023b  ###reference_b3###), the stories we choose are narrative, creative, and inherently longer, offering a valuable addition to lengthy single-document QA evaluation. Inspired by NarrativeQA (Kočiskỳ et al., 2018  ###reference_b21###), this task involves annotated questions that refer to non-consecutive parts of the text. Annotators are encouraged to provide concise answers in their own words rather than copying directly from the snippet.\nWe curate 153 Chinese narrative novels from a website333https://www.wenshuoge.com  ###reference_www.wenshuoge.com###. that gathers public domain books. The collected novels cover genres including martial arts, social themes, and mysteries. 200 non-overlapping stories are extracted from the collection, and the number of questions per story is proportional to its length in tokens, resulting in more questions for longer stories. We then extract snippets from each story, evenly distributed throughout, with an average of 720 Chinese characters per snippet. The number of snippets for each story corresponds to the expected number of questions. Then 6 question-answer pairs for each snippet are generated by instructing GPT-4-Turbo following the aforementioned annotating principles. Annotators then select a specific question-answer pair that is most related to a given snippet from 6 options. In addition, it is ensured that the complexity of questions related to characters, events, and the reasons behind occurrences is maintained. The questions are manually revised to include chapter information and replace pronouns with character names, making the questions more specific. There are 995 question-answer pairs based on 200 stories (i.e., contexts), with an average question length of 18.5 Chinese characters and an average answer length of 11.0 Chinese characters. Note that snippets are used for annotation, whereas during testing, the model is still required to find answers from the entire story.\nThis task is designed to assess a model’s long-term memory capability. This task utilizes inputs from multi-day conversations between a user and a companion chatbot, where the model is required to accurately respond to questions about specific details from the conversation history of a particular day.\nIt determines the ability to maintain contextual understanding, ensure meaningful interactions, and interpret user behaviors over time Zhong et al. (2023  ###reference_b49###). Moreover, this capability becomes increasingly crucial as the length of the model’s input extends, presenting a greater challenge in retaining a precise memory of the input content.\nTo construct the test dataset, we utilize dialogue records from 140 days of interactions between 80 virtual users and companion chatbots, and manually annotate 1,067 QA pairs. We adopt the experimental setting in Zhong et al. (2023  ###reference_b49###) to construct the evaluation dataset.\nFor the user profiles, we manually craft profiles for 20 virtual users, including names, personalities, and topics of interest, and prompt GPT-4-Turbo to generate the rest virtual user profiles.\nLeveraging the user meta-information, we employ GPT-4-Turbo to simulate dialogues between different users and companion chatbots in 140 days. Due to the limitation of context windows, we apply the hierarchical event summary in Zhong et al. (2023  ###reference_b49###) to generate long dialogues.\nAll the generated conversation records are reviewed and deduplicated.\nGiven the dialogue records, we manually craft 1,067 probing questions and answers to evaluate the model’s ability to accurately retrieve relevant memories and generate appropriate responses.\nText summarization is to distill information from a source text and present it in a condensed form.\nAs a pivotal task in natural language processing, summarization requires a full-context understanding of input and complex reasoning.\nIn CLongEval, we introduce a long story summarization task that comprises long-context input based on the story, which is more practical needs and poses more challenges in the aggregation of long-context information.\nTo obtain high-qulity long-context Chinese corpus, we utilize Cloud Translation API444https://cloud.google.com/translate  ###reference_###. to translate the BOOKSUM dataset (Kryściński et al., 2022  ###reference_b22###) into Chinese, which covers books from various domains and includes highly abstractive, human written summaries on three levels: paragraph, chapter-, and book-level. Formally, each sample  in BOOKSUM comprises a textual input  and its corresponding summary , and  may be a paragraph, a chapter, or a whole book.\nWe choose continuous paragraphs or chapters  in expected length and concatenate them to construct long-context input  to ensure coherent semantics from the translated BOOKSUM dataset. Subsequently, we utilize GPT-4-Turbo to aggregate the corresponding summaries  of the chosen continuous paragraphs or chapters into an overall summary , which can be regarded as the appropriate and highly abstractive summary of the constructed long-context input.\nAll the generated summary  is passed to manual check and refinement to guarantee the quality.\nIn this task,  news articles are stacked in one single context, with each article containing a news index (ranging from 1 to N) and its content. The goal of this task is to assess whether the LLMs can comprehensively read all news articles in a long context and determine the category of each news from given possible category pools at once. Completing this task requires the model to carefully read and analyze all the information within the long context. This task is akin to SpaceDigest of ZeroScrolls (Shaham et al., 2023  ###reference_b40###) or PassageCount of LongBench (Bai et al., 2023b  ###reference_b3###), where LLMs analyze long contexts piece by piece. However, their requested outputs are aggregated numbers (e.g., the count of positive reviews), making it difficult to gauge the LLMs’ genuine understanding of each part. The proposed stacked news labeling task, in contrast, presents a more demanding challenge that tests the ability to comprehend lengthy contexts.\nTo construct this dataset, we begin by extracting a subset from THUnews (Sun et al., 2016  ###reference_b42###), an extensive collection of around 840K Chinese news articles.The subset include 9 categories: Sports, Entertainment, Home, Real Estate, Education, Politics, Gaming, Technology, and Finance. Each category contains an equal number of news articles, with the sampled articles having an average Chinese character count of 588.1. We randomly select news articles from different categories to fill the context until the desired context length was reached. Finally, we create 1005 contexts as test samples.\nTypo Detection is aimed at extracting misspelled Chinese characters from a given input. Unlike prior works (Tseng et al., 2015  ###reference_b44###; Lv et al., 2023  ###reference_b31###) that focus on sentence-level typo recognition, our tacked typo detection aims to identify all typos present in the lengthy input, which is of practical importance. This task requires LLMs to have full-context understanding capabilities as well as distinguished information extraction abilities.\nWe utilize the collected Chinese narrative corpus same as Long Story QA to generate 1000 contexts as test samples. Each context is divided into multiple paragraphs, identified by a paragraph ID that starts from 0 and increases incrementally. We randomly select some paragraphs and choose one Chinese character as a typo candidate from each selected paragraph. A corresponding homophone is then used to replace the chosen character, creating a homophonic typo. To maintain a balanced distribution of typos, the number of typos is determined based on the data length: 10 for the small set, 20 for the medium set, and 30 for the large set. Roughly half of the paragraphs in each context contain misspelled characters.\n###table_1### LStQA\nLCvMem\nLStSum\nStNLab\nStTDet\nKpRet\nTblQry\n29.34\n41.10\n10.29\n0.59\n0\n2.86\n7.50\n35.52\n29.34\n14.29\n4.97\n0.09\n6.39\n9.75\n31.94\n47.71\n11.20\n4.31\n0\n11.18\n6.64\n49.36\n53.40\n16.37\n0.46\n0.91\n33.67\n22.60\n49.55\n58.34\n17.29\n16.46\n2.27\n21.87\n20.75\n53.82\n57.41\n17.00\n11.16\n0.91\n34.97\n17.25\n60.21\n51.76\n21.56\n25.36\n66.50\n79.70\n84.24\n16.90\n26.30\n7.74\n0\n0\n1.21\nN/A\n18.41\n22.45\n8.56\n0\n0\n0.93\nN/A\n29.59\n32.07\n8.13\n0\n0\n1.45\n4.50\n25.13\n36.84\n13.99\n0\n0\n1.64\n6.25\n51.20\n38.29\n17.38\n37.40\n9.32\n22.40\n52.76\n19.03\n18.16\n2.36\n0\n0\n0.89\n2.67\n15.62\n28.39\n8.31\n0\n0\n0.51\n0.67\nIn this synthetic key-passage retrieval task, the context comprises a JSON object serialized as a string, containing multiple key-passage pairs. Each key is a unique string of 32 randomly generated characters including both letters and numbers, while the corresponding value is a continuous passage in Chinese. The objective of this task is to retrieve the corresponding passage directly based on the given key.\nUnlike LStQA and LCvMem, KpRet focuses on the model’s information extraction ability, rather than summarizing the answer from a located snippet. KpRet draws inspiration from the synthetic key-value retrieval task mentioned in (Liu et al., 2023a  ###reference_b29###), but differs in that we aim to provide semantically meaningful natural language text instead of randomly generated 128-bit UUID strings, aligning more closely with real-world scenarios of passage retrieval (Nguyen et al., 2016  ###reference_b32###). The main challenge lies in accurately retrieving and reproducing relatively long passages in their entirety.\nAll passages are sampled from three Chinese QA datasets, namely WebQA (Li et al., 2016  ###reference_b27###), Sogou QA555https://github.com/sherlcok314159/ChineseMRC-Data  ###reference_RC-Data###., and CMRC2018 (Cui et al., 2018  ###reference_b10###), ensuring no repetition among them. All the passages exhibit a relatively consistent length, with an average of 81.2 Chinese characters. To construct 200 contexts, we have generated a substantial number of key-passage pairs. For each context, we uniformly select 5 questioned keys according to the position, resulting in a total of 1000 test examples.\nIn the table querying task, a context consists of multiple tables formatted in Markdown. In table querying, the objective is to locate a specific table within the context and retrieve a value from that table based on querying conditions. Unlike the key-value data structure in KpRet, TblQry involves the model’s simultaneous utilization of both row and column indices to extract a specific value from the table. Our question format follows a conditional pattern: ”In Table A, when the value of Column B is C, what is the value of Column D?” In this question format, LLMs need to first identify Table A among multiple tables in the context, then locate the row based on the value of Column B and retrieve the value of Column D. Moreover, Unlike KpRet which returns long passages, TblQry typically returns shorter values like numbers or names, with an average token length of 5.0. Therefore, this task primarily assesses LLMs’ advanced contextual querying abilities rather than their proficiency in reproducing complex passages.\nAll the tables used in this task are sourced from WikTable (Zhong et al., 2017  ###reference_b48###), a collection of English tables. We filter out excessively long tables and retain only those with a token count not exceeding 2000 tokens. Due to resource constraints, we only translate the column headers and the conditioned column into Chinese using the Cloud Translation API for each table. This ensures that the questions are posed in Chinese, while the returned values from the tables remain in English or numerical format. In total, 180 contexts containing multiple tables are constructed. The number of questions for each context is proportional to the number of tables it possesses, and we evenly distribute the tables that need to be queried across different positions within each context. Finally, 1100 test samples are generated."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset Configuration",
            "text": "Anchored by the capabilities outlined in the evaluation framework, we create CLongEval, which contains 7,267 test samples across 7 tasks, including Long Story QA, Long Conversation Memory, Long Story Summarization, Stacked News Labeling, Stacked Typo Detection, Key-Passage Retrieval and Table Querying. Among them, Long Story QA and Long Conversation Memory are human-annotated, Long Story Summarization is GPT-4-annotated and the rest 4 tasks are re-constructed from public datasets.\nAn overview of all tasks in CLongEval and detailed statistics are provided in Table 1  ###reference_###. In this paper, we use InternLM2 InternLMTeam (2024  ###reference_b18###) tokenizer to tokenize the input and report the number of tokens as context length.\nWe notice the divergence of context lengths supported by existing long-context LLMs. To ensure a broad scope of applicability of CLongEval, we stratify the benchmark into three subsets: a small set, a medium set, and a large set.\nSpecifically, the small set primarily includes test data with lengths ranging from 1K to 16K tokens, the medium set mainly encompasses lengths from 16K to 50K tokens, and the large set primarily extends from 50K to 100K tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Construction",
            "text": "For a comprehensive evaluation, 7 tasks are collected in alignment with the predefined evaluation framework. The examples for each task are provided in Appendix B  ###reference_###.\nThe Long Story QA task involves LLMs answering questions based on a context snippet from a long story. To excel at this task, the model should identify the relevant snippet and abstractively reason out the answer. Unlike the normative and objective nature of MultiFieldQA (Bai et al., 2023b  ###reference_b3###  ###reference_b3###), the stories we choose are narrative, creative, and inherently longer, offering a valuable addition to lengthy single-document QA evaluation. Inspired by NarrativeQA (Kočiskỳ et al., 2018  ###reference_b21###  ###reference_b21###), this task involves annotated questions that refer to non-consecutive parts of the text. Annotators are encouraged to provide concise answers in their own words rather than copying directly from the snippet.\nWe curate 153 Chinese narrative novels from a website333https://www.wenshuoge.com  ###reference_www.wenshuoge.com###  ###reference_www.wenshuoge.com###. that gathers public domain books. The collected novels cover genres including martial arts, social themes, and mysteries. 200 non-overlapping stories are extracted from the collection, and the number of questions per story is proportional to its length in tokens, resulting in more questions for longer stories. We then extract snippets from each story, evenly distributed throughout, with an average of 720 Chinese characters per snippet. The number of snippets for each story corresponds to the expected number of questions. Then 6 question-answer pairs for each snippet are generated by instructing GPT-4-Turbo following the aforementioned annotating principles. Annotators then select a specific question-answer pair that is most related to a given snippet from 6 options. In addition, it is ensured that the complexity of questions related to characters, events, and the reasons behind occurrences is maintained. The questions are manually revised to include chapter information and replace pronouns with character names, making the questions more specific. There are 995 question-answer pairs based on 200 stories (i.e., contexts), with an average question length of 18.5 Chinese characters and an average answer length of 11.0 Chinese characters. Note that snippets are used for annotation, whereas during testing, the model is still required to find answers from the entire story.\nThis task is designed to assess a model’s long-term memory capability. This task utilizes inputs from multi-day conversations between a user and a companion chatbot, where the model is required to accurately respond to questions about specific details from the conversation history of a particular day.\nIt determines the ability to maintain contextual understanding, ensure meaningful interactions, and interpret user behaviors over time Zhong et al. (2023  ###reference_b49###  ###reference_b49###). Moreover, this capability becomes increasingly crucial as the length of the model’s input extends, presenting a greater challenge in retaining a precise memory of the input content.\nTo construct the test dataset, we utilize dialogue records from 140 days of interactions between 80 virtual users and companion chatbots, and manually annotate 1,067 QA pairs. We adopt the experimental setting in Zhong et al. (2023  ###reference_b49###  ###reference_b49###) to construct the evaluation dataset.\nFor the user profiles, we manually craft profiles for 20 virtual users, including names, personalities, and topics of interest, and prompt GPT-4-Turbo to generate the rest virtual user profiles.\nLeveraging the user meta-information, we employ GPT-4-Turbo to simulate dialogues between different users and companion chatbots in 140 days. Due to the limitation of context windows, we apply the hierarchical event summary in Zhong et al. (2023  ###reference_b49###  ###reference_b49###) to generate long dialogues.\nAll the generated conversation records are reviewed and deduplicated.\nGiven the dialogue records, we manually craft 1,067 probing questions and answers to evaluate the model’s ability to accurately retrieve relevant memories and generate appropriate responses.\nText summarization is to distill information from a source text and present it in a condensed form.\nAs a pivotal task in natural language processing, summarization requires a full-context understanding of input and complex reasoning.\nIn CLongEval, we introduce a long story summarization task that comprises long-context input based on the story, which is more practical needs and poses more challenges in the aggregation of long-context information.\nTo obtain high-qulity long-context Chinese corpus, we utilize Cloud Translation API444https://cloud.google.com/translate  ###reference_###  ###reference_###. to translate the BOOKSUM dataset (Kryściński et al., 2022  ###reference_b22###  ###reference_b22###) into Chinese, which covers books from various domains and includes highly abstractive, human written summaries on three levels: paragraph, chapter-, and book-level. Formally, each sample  in BOOKSUM comprises a textual input  and its corresponding summary , and  may be a paragraph, a chapter, or a whole book.\nWe choose continuous paragraphs or chapters  in expected length and concatenate them to construct long-context input  to ensure coherent semantics from the translated BOOKSUM dataset. Subsequently, we utilize GPT-4-Turbo to aggregate the corresponding summaries  of the chosen continuous paragraphs or chapters into an overall summary , which can be regarded as the appropriate and highly abstractive summary of the constructed long-context input.\nAll the generated summary  is passed to manual check and refinement to guarantee the quality.\nIn this task,  news articles are stacked in one single context, with each article containing a news index (ranging from 1 to N) and its content. The goal of this task is to assess whether the LLMs can comprehensively read all news articles in a long context and determine the category of each news from given possible category pools at once. Completing this task requires the model to carefully read and analyze all the information within the long context. This task is akin to SpaceDigest of ZeroScrolls (Shaham et al., 2023  ###reference_b40###  ###reference_b40###) or PassageCount of LongBench (Bai et al., 2023b  ###reference_b3###  ###reference_b3###), where LLMs analyze long contexts piece by piece. However, their requested outputs are aggregated numbers (e.g., the count of positive reviews), making it difficult to gauge the LLMs’ genuine understanding of each part. The proposed stacked news labeling task, in contrast, presents a more demanding challenge that tests the ability to comprehend lengthy contexts.\nTo construct this dataset, we begin by extracting a subset from THUnews (Sun et al., 2016  ###reference_b42###  ###reference_b42###), an extensive collection of around 840K Chinese news articles.The subset include 9 categories: Sports, Entertainment, Home, Real Estate, Education, Politics, Gaming, Technology, and Finance. Each category contains an equal number of news articles, with the sampled articles having an average Chinese character count of 588.1. We randomly select news articles from different categories to fill the context until the desired context length was reached. Finally, we create 1005 contexts as test samples.\nTypo Detection is aimed at extracting misspelled Chinese characters from a given input. Unlike prior works (Tseng et al., 2015  ###reference_b44###  ###reference_b44###; Lv et al., 2023  ###reference_b31###  ###reference_b31###) that focus on sentence-level typo recognition, our tacked typo detection aims to identify all typos present in the lengthy input, which is of practical importance. This task requires LLMs to have full-context understanding capabilities as well as distinguished information extraction abilities.\nWe utilize the collected Chinese narrative corpus same as Long Story QA to generate 1000 contexts as test samples. Each context is divided into multiple paragraphs, identified by a paragraph ID that starts from 0 and increases incrementally. We randomly select some paragraphs and choose one Chinese character as a typo candidate from each selected paragraph. A corresponding homophone is then used to replace the chosen character, creating a homophonic typo. To maintain a balanced distribution of typos, the number of typos is determined based on the data length: 10 for the small set, 20 for the medium set, and 30 for the large set. Roughly half of the paragraphs in each context contain misspelled characters.\n###table_2### LStQA\nLCvMem\nLStSum\nStNLab\nStTDet\nKpRet\nTblQry\n29.34\n41.10\n10.29\n0.59\n0\n2.86\n7.50\n35.52\n29.34\n14.29\n4.97\n0.09\n6.39\n9.75\n31.94\n47.71\n11.20\n4.31\n0\n11.18\n6.64\n49.36\n53.40\n16.37\n0.46\n0.91\n33.67\n22.60\n49.55\n58.34\n17.29\n16.46\n2.27\n21.87\n20.75\n53.82\n57.41\n17.00\n11.16\n0.91\n34.97\n17.25\n60.21\n51.76\n21.56\n25.36\n66.50\n79.70\n84.24\n16.90\n26.30\n7.74\n0\n0\n1.21\nN/A\n18.41\n22.45\n8.56\n0\n0\n0.93\nN/A\n29.59\n32.07\n8.13\n0\n0\n1.45\n4.50\n25.13\n36.84\n13.99\n0\n0\n1.64\n6.25\n51.20\n38.29\n17.38\n37.40\n9.32\n22.40\n52.76\n19.03\n18.16\n2.36\n0\n0\n0.89\n2.67\n15.62\n28.39\n8.31\n0\n0\n0.51\n0.67\nIn this synthetic key-passage retrieval task, the context comprises a JSON object serialized as a string, containing multiple key-passage pairs. Each key is a unique string of 32 randomly generated characters including both letters and numbers, while the corresponding value is a continuous passage in Chinese. The objective of this task is to retrieve the corresponding passage directly based on the given key.\nUnlike LStQA and LCvMem, KpRet focuses on the model’s information extraction ability, rather than summarizing the answer from a located snippet. KpRet draws inspiration from the synthetic key-value retrieval task mentioned in (Liu et al., 2023a  ###reference_b29###  ###reference_b29###), but differs in that we aim to provide semantically meaningful natural language text instead of randomly generated 128-bit UUID strings, aligning more closely with real-world scenarios of passage retrieval (Nguyen et al., 2016  ###reference_b32###  ###reference_b32###). The main challenge lies in accurately retrieving and reproducing relatively long passages in their entirety.\nAll passages are sampled from three Chinese QA datasets, namely WebQA (Li et al., 2016  ###reference_b27###  ###reference_b27###), Sogou QA555https://github.com/sherlcok314159/ChineseMRC-Data  ###reference_RC-Data###  ###reference_RC-Data###., and CMRC2018 (Cui et al., 2018  ###reference_b10###  ###reference_b10###), ensuring no repetition among them. All the passages exhibit a relatively consistent length, with an average of 81.2 Chinese characters. To construct 200 contexts, we have generated a substantial number of key-passage pairs. For each context, we uniformly select 5 questioned keys according to the position, resulting in a total of 1000 test examples.\nIn the table querying task, a context consists of multiple tables formatted in Markdown. In table querying, the objective is to locate a specific table within the context and retrieve a value from that table based on querying conditions. Unlike the key-value data structure in KpRet, TblQry involves the model’s simultaneous utilization of both row and column indices to extract a specific value from the table. Our question format follows a conditional pattern: ”In Table A, when the value of Column B is C, what is the value of Column D?” In this question format, LLMs need to first identify Table A among multiple tables in the context, then locate the row based on the value of Column B and retrieve the value of Column D. Moreover, Unlike KpRet which returns long passages, TblQry typically returns shorter values like numbers or names, with an average token length of 5.0. Therefore, this task primarily assesses LLMs’ advanced contextual querying abilities rather than their proficiency in reproducing complex passages.\nAll the tables used in this task are sourced from WikTable (Zhong et al., 2017  ###reference_b48###  ###reference_b48###), a collection of English tables. We filter out excessively long tables and retain only those with a token count not exceeding 2000 tokens. Due to resource constraints, we only translate the column headers and the conditioned column into Chinese using the Cloud Translation API for each table. This ensures that the questions are posed in Chinese, while the returned values from the tables remain in English or numerical format. In total, 180 contexts containing multiple tables are constructed. The number of questions for each context is proportional to the number of tables it possesses, and we evenly distribute the tables that need to be queried across different positions within each context. Finally, 1100 test samples are generated."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We study how the position of the referenced chunk in the context affects the model’s performance for four tasks that only need partial context. The results are shown in Figure 2  ###reference_###. The position of the referenced chunk in the context is discretized into six intervals, with larger numbers indicating a closer position to the end. It is observed that for LStQA and LCvMem, the evaluated models show a ”lost in the middle” phenomenon (Liu et al., 2023a  ###reference_b29###) where the models’ performance decreases when the referenced chunks are in the middle of the context. For KpRet, most open-source models only show some non-zero performance when the answer is located at the end of the context; GPT-4-Turbo shows a nearly linear decline in performance as the answer’s position in the context becomes deeper, while Moonshot-v1 does not exhibit significant degradation. Similarly, for TblQry, GPT-4-Turbo’s performance drops as the answer’s position goes deeper, eventually getting surpassed by Moonshot-v1. The performance across different positions on KpRet and TblQry does not exhibit a distinct pattern.\nWe are interested in evaluating LLMs’ performance for the tasks of StNLab and StTDet, without considering the stacked long-context scenario. For the news labeling task (NLab for short), we create a test set of 4,500 samples by sampling 500 news articles from each of 9 news categories on StNLab, and each time a news article is given as input for the LLM to determine its category. For the typo detection task (TDet for short), 3,000 paragraphs from StTDet are sampled as test samples with each containing a typo, and LLMs are asked to take each paragraph as a model input to identify typos.\nTable 4  ###reference_### reveals that open-source models achieve over 80% accuracy for news labeling and at least 18% accuracy for typo detection. However, when news articles or paragraphs containing typos are stacked to form longer texts, their accuracy drops dramatically, even reaching 0 in the medium set as shown in Table 2  ###reference_###. Also, detailed performance results on StNLab in Appendix A.1  ###reference_### illustrate that GPT-4-Turbo misclassifies a significant portion of the news articles as the context length increases. In contrast, Moonshot-v1 consistently achieves high accuracy scores as the context length increases."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "8 LLMs are selected for evaluation based on whether they feature long context capability and exceptional support for the Chinese.\nCommercial Models: (1) GPT-4-Turbo-128K, the GPT-4-1106-preview model (OpenAI, 2023  ###reference_b33###) with a 128K context window from OpenAI. (2) Moonshot-v1 666https://platform.moonshot.cn  ###reference_platform.moonshot.cn###. supporting up to 200K Chinese characters, developed by Moonshot AI. We call the 32K version to run the small set and call its 128K version for both medium and large sets. Open-source Models: (3) Chinese-LLAMA2-64K (Cui, 2023  ###reference_b9###), extending context length of Chinese-LLAMA2 to 64K via YaRN Peng et al. (2023b  ###reference_b35###) . (4) Chinese-Alpaca2-64K (Cui, 2023  ###reference_b9###), the 64K context version of Chinese-Alpaca2. (5) Qwen-7B-32K (Bai et al., 2023a  ###reference_b2###), extending Qwen-7B to 32K context length via NTK-aware scaled RoPE (bloc97, 2023  ###reference_b5###). (6) ChatGLM3-6B-32K (ZhupuAI, 2023  ###reference_b50###), the 32K context version of ChatGLM3-6B. (7) InternlM2-7B-200K (InternLMTeam, 2024  ###reference_b18###), effectively supporting ultra-long contexts of up to 200K tokens using dynamic NTK extrapolation (Liu et al., 2023b  ###reference_b30###). (8) InternLM2-20B-200K (InternLMTeam, 2024  ###reference_b18###), similar to InternLM2-7B-200K but is more robust and capable of handling intricate scenarios. For InternLM2-7B/20B with the small set, we rely on its native support for a 32K context window. The values of the maximum output token limit for each task under different subsets are listed in Table 5  ###reference_###. All the experiments are run on a server with 4 NVIDIA A100 (80GB) GPUs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "The evaluation is fully automatic. For LstQA and LCvMem, F1 is employed to measure the unigram overlap between the generated and reference answer after ignoring white spaces and punctuation. For LstSum, we use ROUGE-L (Lin, 2004  ###reference_b28###) to measure the n-gram overlap between the generated and reference summary. For both StNlab and StTDet, a metric called Average Accuracy is introduced. It measures the ratio of the number of segments correctly answered by the generated answer to the total number of segments in the gold reference. On StNLab, it indicates the percentage of news in the context that is correctly classified, while on StTDet, it denotes the accuracy of identifying misspelled words in the context. For KpRet, Edit Score based on Levenshtein distance is employed to measure the difference between the generated string and the gold reference string. For TblQry, Exact Match is utilized to measure whether the generated column value is identical to the gold reference.\nFor each of the 7 tasks, we first calculate the score per test sample using the aforementioned corresponding metrics and report the mean score across samples."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "###table_3### 23.18\n42.71\n7.88\n23.12\n20.85\n8.95\n30.72\n27.29\n11.06\n35.52\n44.20\n16.44\n45.09\n52.64\n16.79\n49.55\n58.64\n17.29\nTable 2  ###reference_### presents the performance on all datasets in CLongEval. We observe the following key findings from the experimental results: (i) For LstQA and LCvMem, GPT-4-Turbo does not show significant F1-score improvement compared to the top-performing open-source InternLM2-20B in the small set. However, it significantly outperforms InternLM2-20B (e.g., scoring 54.18 vs 36.84 on LCvMem). Also, Moonshot-v1 exhibits less noticeable score degradation on medium and large sets compared to open-source models on LStQA. (ii) For LStSum, both Moonshot-v1 and GPT-4-Turbo show consistent Rouge-L scores on small and medium sets. (iii) For StNlab and StTDet which require careful analysis of full-text chunks to output either labeling results or identify spelling errors, there is a substantial performance gap between open-source and closed ones, with scores of all evaluated open-source models in the medium set being zero. GPT-4-Turbo’s performance drops by 51.8% when moving from the small set to the medium one on StNLab. Meanwhile, Moonshot-v1 performs well on StNLab, with only an 11.83% decrease when expanding from the small set to the large one. (iv) For KpRet and TlbQry which involve information retrieval, all open-source models experience a sharp decline in performance as the input length increases, and Moonshot-v1 shows more robust handling of longer inputs compared to GPT-4-Turbo.\n###table_4### 4.31\n80.91\n0\n18.67\n4.97\n60.09\n0.09\n22.27\n0.46\n86.71\n0.91\n34.23\n16.46\n85.87\n2.27\n56.20\n11.16\n84.04\n0.91\n56.90\n86.71\n25.36\n62.06\n79.70\nTable 3  ###reference_### presents the performance of smaller context window models Baichuan2-7B (Yang et al., 2023  ###reference_b47###), Mistral-7B (Jiang et al., 2023  ###reference_b19###) and Yi-6B 777https://github.com/01-ai/Yi  ###reference_github.com/01-ai/Yi###. on LstQA, LCvMem, and LstSum. We also examine how the input length affects the performance of InternLM2-7B by truncating the context to 4K and 8K. Notably, shorter maximum context lengths result in lower scores, highlighting the need for effective long-context modeling in our benchmark."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "###figure_2### We study how the position of the referenced chunk in the context affects the model’s performance for four tasks that only need partial context. The results are shown in Figure 2  ###reference_###  ###reference_###. The position of the referenced chunk in the context is discretized into six intervals, with larger numbers indicating a closer position to the end. It is observed that for LStQA and LCvMem, the evaluated models show a ”lost in the middle” phenomenon (Liu et al., 2023a  ###reference_b29###  ###reference_b29###) where the models’ performance decreases when the referenced chunks are in the middle of the context. For KpRet, most open-source models only show some non-zero performance when the answer is located at the end of the context; GPT-4-Turbo shows a nearly linear decline in performance as the answer’s position in the context becomes deeper, while Moonshot-v1 does not exhibit significant degradation. Similarly, for TblQry, GPT-4-Turbo’s performance drops as the answer’s position goes deeper, eventually getting surpassed by Moonshot-v1. The performance across different positions on KpRet and TblQry does not exhibit a distinct pattern.\nWe are interested in evaluating LLMs’ performance for the tasks of StNLab and StTDet, without considering the stacked long-context scenario. For the news labeling task (NLab for short), we create a test set of 4,500 samples by sampling 500 news articles from each of 9 news categories on StNLab, and each time a news article is given as input for the LLM to determine its category. For the typo detection task (TDet for short), 3,000 paragraphs from StTDet are sampled as test samples with each containing a typo, and LLMs are asked to take each paragraph as a model input to identify typos.\nTable 4  ###reference_###  ###reference_### reveals that open-source models achieve over 80% accuracy for news labeling and at least 18% accuracy for typo detection. However, when news articles or paragraphs containing typos are stacked to form longer texts, their accuracy drops dramatically, even reaching 0 in the medium set as shown in Table 2  ###reference_###  ###reference_###. Also, detailed performance results on StNLab in Appendix A.1  ###reference_###  ###reference_### illustrate that GPT-4-Turbo misclassifies a significant portion of the news articles as the context length increases. In contrast, Moonshot-v1 consistently achieves high accuracy scores as the context length increases."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evaluation for Long-Context LLMs",
            "text": "Research work of long context modeling predominantly adopt perplexity as the evaluation metric Beltagy et al. (2020  ###reference_b4###); Sun et al. (2021  ###reference_b43###); Press et al. (2021  ###reference_b38###); Chen et al. (2023b  ###reference_b7###); Peng et al. (2023b  ###reference_b35###); bloc97 (2023  ###reference_b5###).\nSynthetics tasks, such as retrieval tasks, are used to assess and analyze the ability to model long input for LLMs Chen et al. (2023a  ###reference_b6###); Li et al. (2023  ###reference_b24###).\nHowever, as discussed in Sun et al. (2021  ###reference_b43###); Xiong et al. (2023  ###reference_b46###), the perplexity value and performance on synthetic tasks may not adequately reflect a language model’s capability in addressing tasks in real-world scenarios.\nRecently an English benchmark An et al. (2023  ###reference_b1###) are proposed for the evaluation of long-context LLMs. Bai et al. (2023b  ###reference_b3###) introduces a bilingual benchmark, but the quantity of test examples for Chinese is quite limited. Besides the targeted language, CLongEval differs from them in these aspects: (1) It includes novel tasks that closely simulate real-world LLM usage scenarios, and (2) The test samples possess a wider span of context lengths."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We presented CLongEval, a benchmark for Chinese long-context LLMs, which contains 7 tasks and 7,267 examples. To the best of our knowledge, CLongEval is the first benchmark in this setting. Based on two basic capabilities for long-context LLMs, i.e., information acquisition and reasoning, we collected corresponding tasks and datasets for a comprehensive evaluation.\nWe benchmarked 8 long-context LLMs  and provided an in-depth analysis regarding each fine-grained capability."
        }
    ]
}