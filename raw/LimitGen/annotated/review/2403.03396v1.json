{
    "title": "Japanese-English Sentence Translation Exercises Dataset for Automatic Grading",
    "abstract": "This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning.\nWe formalize the task as grading student responses for each rubric criterion pre-specified by the educators.\nWe then create a dataset for STE between Japanese and English including 21 questions, along with a total of  student responses ( on average).\nThe answer responses were collected from students and crowd workers.\nUsing this dataset, we demonstrate the performance of baselines including finetuned BERT and GPT models with few-shot in-context learning.\nExperimental results show that the baseline model with finetuned BERT was able to classify correct responses with approximately 90% in , but only less than 80% for incorrect responses.\nFurthermore, the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Sentence translation exercises (STEs) are often utilized as educational tools in the early stages of L2 language learning, particularly between language pairs that are linguistically distant from each other Cook (2010  ###reference_b5###); Butzkamm and Caldwell (2009  ###reference_b3###).\nFigure 1  ###reference_### shows an example of STE. Here, a learner translates a short sentence in their native language (L1) into the language they are learning (L2), and these translations are graded following analytic criteria within the grading rubric such as E3 and G4, which correspond to specific grammar items or expressions.\n###figure_1### This format facilitates the recognition of similarities and differences between the native language and the target language, which is especially effective in helping learners acquire basic grammar and expressions in the early stages of their language learning, thus enhancing their understanding of the desired modes of expression Cook (2010  ###reference_b5###).\nThe questions in these exercises are brief and repeatable tests that efficiently help learners practice specific grammatical items, basic vocabulary, and idioms at a certain proficiency level and learn the nuances of expression between L1 and L2.\nTeachers can also use these exercises as assessment tools to evaluate whether learners have mastered specific grammar items or a vocabulary level.\nHowever, because the responses to these exercises are descriptive, they pose a significant burden on educators in the form of manual grading and feedback.\nSuch a limitation restricts the frequency of these exercises despite the importance of repetitive training in language acquisition  Larsen-Freeman (2012  ###reference_b21###).\nTherefore, automating the correction and feedback for translation exercises has the potential to significantly transform the educational environment in language learning.\nTherefore, we aim to automate the grading of L1-to-L2 STEs.\nTasks that are closely associated with this challenge include Grammatical Error Correction (GEC), which evaluates the grammatical correctness of written sentences, and machine translation.\nSTEs, however, are substantially different from these tasks in that they are usually operationalized with explicit learning objectives and closely reflect educators’ intentions (§2.1  ###reference_###).\nSTEs not only clarify the learning objectives of a particular question but also allow for a more detailed learning analysis based on the performance of each evaluation item.\nThe motivation for incorporating educators’ intentions is also supported by studies that have found that the sole use of the GEC system does not elicit effective learner engagement Koltovskaia (2020  ###reference_b18###); Ranalli (2021  ###reference_b25###).\nTo achieve our goal, we perform three tasks: (1) question formulation, (2) dataset creation, and (3) evaluation of baseline systems for our task.\nTo the best of our knowledge, this is the first attempt at an automated STE grading for educational purposes.\nTherefore, we first formulate the question.\nAn important aspect of this formulation is to ensure that the established framework reflects the educators’ evaluation criteria.\nConsequently, we formulate our task as a classification of scores on each evaluation item according to the predefined rubrics.\nWe then develop the dataset for this task.\nThe questions and the rubric were created by English education experts, and answer scripts were collected from secondary education classrooms and through crowdsourcing.\nFinally, we evaluate the performance of the conventional automated scoring model typically used for short answer scoring (SAS), as well as the latest generative language models with few-shot learning.\nExperimental results showed that the baseline model using finetuned BERT successfully classified approximately 90% of correct responses in , but only less than 80% of incorrect responses.\nFurthermore, GPT models with few-shot learning showed poorer results than the BERT model, indicating that even with a state-of-the-art LLM, our proposed new task remains difficult and challenging.\nError analysis of the few-shot models revealed their lack of comprehension regarding the grading task.\nThe contributions of this study are the following:\nWe formulate the automated grading of sentence translation exercises as a new task, referencing the actual operation of STEs in educational settings.\nWe construct a dataset for the automated STE grading in accordance with this task design, which includes a total of 21 questions and  responses, and demonstrate the feasibility of the task.\nWe establish baseline performances for the task, showing potential for advancement."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Automatic scoring of sentence translation exercises",
            "text": "For a given STE, let  denote the set of analytic criteria.\nFor the input response text , the model outputs an analytic score  for a given analytic criterion , where 2, 1, and 0 represent “correct,” “partially correct,” and “incorrect,” respectively."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sentence translation exercises",
            "text": "Sentence translation exercises (STEs) are a language learning tool where a learner translates a sentence in L1 into a target L2.\nStudies have shown that the use of L1 in L2 education promots an understanding of differences and similarities between the two languages  Butzkamm and Caldwell (2009  ###reference_b3###); Cook (2010  ###reference_b5###), reduces incomprehension, and enhances learning focus Scott and De la Fuente (2008  ###reference_b27###).\nLanguage translation has also been effective in improving students’ four skills (speaking, writing, reading, listening) and promoting learning and communication skills Yasar Yuzlu and Dikilitas (2022  ###reference_b29###).\nBecause of these benefits, STEs are widely used in educational settings, particularly among beginners in language learning.\nFigure 1  ###reference_### shows an overview of an STE.\nA learner’s translated response is assessed using a grading rubric meticulously designed by educators to evaluate the learner’s L2 ability, such as vocabulary and grammatical understanding.\nSuch a rubric contains multiple analytic criteria aligned with the specific pedagogical objectives that an educator intends to assess in the question.\nThis aspect characterizes STE evaluation and distinguishes them from typical GEC tasks, which assess the overall correctness of the grammar.\nEvaluation based on the analytic scoring criteria highlights the degree to which the learning objectives are achieved. To this end, some degree of constraint is imposed on the question design and answer choices, limiting the freedom of translation.\nHowever, if translation variations are observed, all possible expressions are accounted for.\nThese restrictions in translation practice, as discussed in Cook (2010  ###reference_b5###), prevent learners from easily avoiding knowledge gaps and direct their attention to L2 aspects that they may find challenging. Therefore, these constraints can be useful in focusing students’ attention on specific language abilities.\nIn addition, the evaluation of translated sentences in educational settings is also different from that of general translations in that the former involves pedagogical objectives such as the acquisition of specific language knowledge."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Task formulation",
            "text": "The purpose of assessing the STE task is to determine how well students’ responses achieve the learning objectives defined by the instructors.\nTo effectively do so, instructors use a carefully constructed scoring rubric.\nEach STE question targets several learning objectives and evaluates other fundamental grammatical items (e.g., number, tense, etc.); therefore, a scoring rubric contains multiple independent analytical criteria to evaluate specific items.\nThese criteria serve as the basis for grading each student’s response, with a corresponding analytic score assigned to each grading item (see Table 1  ###reference_###).\nThe automatic scoring of analytic criteria was formulated by Mizumoto et al. (2019  ###reference_b22###) as an analytic score prediction task for reading comprehension questions.\nTherefore, this study also considers the analytic score prediction for the automatic scoring of STE.\nFor a given STE, let  denote the set of analytic criteria.\nFor the input response text , the model outputs an analytic score  for a given analytic criterion , where 2, 1, and 0 represent “correct,” “partially correct,” and “incorrect,” respectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Sentence translation exercise (STEs) dataset",
            "text": "To implement the automatic STE scoring, we introduce an STE dataset.\nThis dataset currently comprises 21 Japanese-to-English STE questions with detailed rubrics and annotated student responses.\nThese questions and the scoring rubrics were created by specialists in the design of English learning materials. The questions were constructed to cover all the major grammar topics in several well-known English textbooks used in Japanese high schools.\nTable 1  ###reference_### shows an example of a rubric, which contains 17 analytic criteria: three for grammar (labeled as “G”), seven for vocabulary and expression (labeled as “E”), and seven for word order (labeled as “O”).\nEach analytic criterion is evaluated on a three-point scale: 2 (correct), 1 (partially correct), and 0 (incorrect); the rubric lists the typical expressions for each scale.\nEssentially, STEs are designed such that they limit variations in correct responses from the outset.\nIn practical settings, however, educators may adjust the grading rubric by incorporating variations in correct responses, previously unidentified during the rubric’s initial creation, to accurately evaluate the student responses.\nTo replicate this process, we initially create the analytic criteria, followed by the collection of student responses as described in the following subsection.\nSubsequently, we refine the rubric by reviewing the collected responses, to preempt any challenges that may arise during the grading procedure.\nIn the following sections, we will discuss in detail the methods used to gather responses, as well as the annotation process, and statistically analyze the whole dataset.\nMizumoto et al. (2019  ###reference_b22###) also annotated specific substrings within responses that contribute to an analytic score. These substrings are called justification cues because they serve as the rationale for the analytic scores.\nWe also annotated justification cues in our dataset to enhance the interpretability of analytic scores.\nFor example, in Figure 1  ###reference_###, the phrase “before I saw” was annotated as a justification cue and was assigned an analytic score of “.”\nTo measure the quality of the annotations, we randomly selected 10 out of the 21 questions and asked a different annotator to annotate 20 responses for each question.\nWe then used Cohen’s Kappa coefficient  Cohen (1960  ###reference_b4###) to calculate inter-grader agreement for analytic scoring and the F-score to calculate agreement for justification cues.\nThe scores for all analytic criteria had an overall average Kappa coefficient of 0.74, indicating substantial agreement Landis and Koch (1977  ###reference_b20###).\nRegarding agreement for justification cues, the F-score was 0.92, signifying a high level of agreement among the annotators Mizumoto et al. (2019  ###reference_b22###); Sato et al. (2022  ###reference_b26###). This suggests that different annotators can consistently identify the same phrase as a justification cue for an analytic score.\nTable2  ###reference_### shows the dataset statistics.\nWe annotated a total of 3,498 responses for 21 questions, including 196 analytic criteria.\nFor the pilot question, ranging from Q1 to Q7, scoring included 1 (partially correct) whereas the other questions followed a binary scoring of 2 (correct) and 0 (incorrect).\nAdditionally, the number of instances with a grade of 0 was relatively fewer than those with a grade of 2.\nThis distribution was similar to the one observed in the pilot question and others.\nTherefore, we conclude that we have successfully gathered crowdsourcing workers whose English ability is equivalent to that of original high school students and that these workers have attempted to answer those questions correctly."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Collecting student responses",
            "text": "Ideally, student responses are compiled within classrooms and other practical learning environments. However, the number of responses that can be collected from actual classrooms is often limited, and the collecting process is time-consuming.\nTherefore, we constructed our dataset through a combined approach involving high school students and crowdsourcing workers to collect responses for response collection.\nIn this approach, we conducted a pilot data collection in which responses were obtained from high school students.\nThen, we analyzed these responses with English education experts and created the criteria for gathering crowdsourcing workers whose English abilities are equivalent to those of the high school students (see Appendix A  ###reference_### for details regarding the recruitment criteria).\nFinally, we hired workers who met the criteria and allowed them to answer the questions, thus collecting a sufficient amount of responses.\nTo maintain quality, we manually reviewed the collected responses and excluded those that significantly deviated from the expected responses.\nAs a result, we obtained an average of 167 responses per question.\nThe following section will present the statistics of the dataset."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Annotation:",
            "text": "As explained in Section 2.2  ###reference_###, the scoring task for STEs involves grading on a three-class classification for each analytic criterion.\nAnnotators are also asked to identify the specific phrase of the response that serves as a grading clue (referred to as justification cues).\nWe annotated both types of information in each response.\nWe hired professional graders to annotate those responses.\nAs demonstrated in Figure 1  ###reference_###,\nthe annotators assigned an analytic score to the responses based on each analytic criterion.\nMizumoto et al. (2019  ###reference_b22###  ###reference_b22###) also annotated specific substrings within responses that contribute to an analytic score. These substrings are called justification cues because they serve as the rationale for the analytic scores.\nWe also annotated justification cues in our dataset to enhance the interpretability of analytic scores.\nFor example, in Figure 1  ###reference_###  ###reference_###, the phrase “before I saw” was annotated as a justification cue and was assigned an analytic score of “.”\nTo measure the quality of the annotations, we randomly selected 10 out of the 21 questions and asked a different annotator to annotate 20 responses for each question.\nWe then used Cohen’s Kappa coefficient  Cohen (1960  ###reference_b4###  ###reference_b4###) to calculate inter-grader agreement for analytic scoring and the F-score to calculate agreement for justification cues.\nThe scores for all analytic criteria had an overall average Kappa coefficient of 0.74, indicating substantial agreement Landis and Koch (1977  ###reference_b20###  ###reference_b20###).\nRegarding agreement for justification cues, the F-score was 0.92, signifying a high level of agreement among the annotators Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###); Sato et al. (2022  ###reference_b26###  ###reference_b26###). This suggests that different annotators can consistently identify the same phrase as a justification cue for an analytic score.\nTable2  ###reference_###  ###reference_### shows the dataset statistics.\nWe annotated a total of 3,498 responses for 21 questions, including 196 analytic criteria.\nFor the pilot question, ranging from Q1 to Q7, scoring included 1 (partially correct) whereas the other questions followed a binary scoring of 2 (correct) and 0 (incorrect).\nAdditionally, the number of instances with a grade of 0 was relatively fewer than those with a grade of 2.\nThis distribution was similar to the one observed in the pilot question and others.\nTherefore, we conclude that we have successfully gathered crowdsourcing workers whose English ability is equivalent to that of original high school students and that these workers have attempted to answer those questions correctly."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We employ a BERT Devlin et al. (2019  ###reference_b7###)-based classification model and the GPT models OpenAI (2023  ###reference_b24###) with in-context learning as a baseline for our task formulation.\nThis section discusses these baseline models in detail.\nFirst, the response text sequence , with a prepended CLS token, is input into BERT, obtaining the intermediate representation  as follows:\nIn our task, a justification cue that indicates the rationale behind its score is provided for each response.\nBy utilizing this justification cue to train a model, we expect that the model will grade faithfully according to the rubric.\nTherefore, following  Mizumoto et al. (2019  ###reference_b22###), we use these justification cues as supervisory signals to train the model’s attention layer.\nHere, we perform pooling on the BERT-encoded representations using a Bi-LSTM and attention mechanism.\nThe sequence obtained from  by excluding  is input into the Bi-LSTM, yielding .\nThen we calculate the weighted sum as follows:\nwhere  is the weight of the -th word relative to the scoring rubric , calculated by the attention mechanism shown in Equation (4.1  ###reference_###).\nwhere  and  are learnable parameters. Finally, the evaluation value  for item  is obtained by the following formula:\nwhere  and  are the learnable parameters.\nThe analytic scoring model is trained to minimize the negative log-likelihood  for each analytic score.\nwhere  is the label (evaluation value) of the ground truth for scoring rubric .\nIn addition, as discussed in the Section 3.2  ###reference_###, the dataset contains the justification cues  for each analytic criterion for the response, where\n is the indicator of whether the -th token in the response is the justification cue for the score of the analytic criterion .\nWhen the gold justification cue includes  tokens, the sum of  is . Therefore, as a gold signal for , we use  divided by  during the training process.\nFollowing Mizumoto et al. (2019  ###reference_b22###), we use the MSE-based loss function to achieve supervised training of the attentions with justification cues.\nThus, the overall loss  is expressed as:"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Finetuned BERT model",
            "text": "We employ BERT, which is widely used in various NLP tasks, including SAS, as a baseline for this task.\nThis model is finetuened for each scoring item in the rubric using the training data.\nFirst, the response text sequence , with a prepended CLS token, is input into BERT, obtaining the intermediate representation  as follows:\nIn our task, a justification cue that indicates the rationale behind its score is provided for each response.\nBy utilizing this justification cue to train a model, we expect that the model will grade faithfully according to the rubric.\nTherefore, following  Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###), we use these justification cues as supervisory signals to train the model’s attention layer.\nHere, we perform pooling on the BERT-encoded representations using a Bi-LSTM and attention mechanism.\nThe sequence obtained from  by excluding  is input into the Bi-LSTM, yielding .\nThen we calculate the weighted sum as follows:\nwhere  is the weight of the -th word relative to the scoring rubric , calculated by the attention mechanism shown in Equation (4.1  ###reference_###  ###reference_###).\nwhere  and  are learnable parameters. Finally, the evaluation value  for item  is obtained by the following formula:\nwhere  and  are the learnable parameters.\nThe analytic scoring model is trained to minimize the negative log-likelihood  for each analytic score.\nwhere  is the label (evaluation value) of the ground truth for scoring rubric .\nIn addition, as discussed in the Section 3.2  ###reference_###  ###reference_###, the dataset contains the justification cues  for each analytic criterion for the response, where\n is the indicator of whether the -th token in the response is the justification cue for the score of the analytic criterion .\nWhen the gold justification cue includes  tokens, the sum of  is . Therefore, as a gold signal for , we use  divided by  during the training process.\nFollowing Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###), we use the MSE-based loss function to achieve supervised training of the attentions with justification cues.\nThus, the overall loss  is expressed as:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "GPT models with in-context learning",
            "text": "We evaluate the GPT-3.5 and GPT-4 models in the setting of few-shot in-context learning Brown et al. (2020  ###reference_b1###), which significantly minimizes the cost of building a scoring model specific to each grading item as well as the training examples required for finetuning.\nFurthermore, the GPT series demonstrates superior performance in tasks such as translation and summarization, among other tasks Gladkoff et al. (2023  ###reference_b13###); Helwan et al. (2023  ###reference_b15###).\nTherefore, we can expect the proficiency in grammatical knowledge required for automatic grading of STEs.\n###figure_2### Figure 2  ###reference_### shows the input template for the GPT models.\nThe input can be segmented into two parts.\nThe first part is a prompt that includes a task instruction, a description of the output format, an L1 sentence for translation, a focused single analytic criterion, and the scoring examples corresponding to that criterion.\nThe analytic criterion is a (literal) textual representation of a rubric item described in a single row in Table 1  ###reference_###.\nFor each score label, we provide a few-shot examples to illustrate the analytic criterion and its scoring (output examples) for in-context learning.\nThe second part is a student response.\nThe model leverages these two inputs to generate a score label for the specified criterion and identify the substring of the student response that justifies the evaluation.\nIn the GPT models, we treat the grading of each analytic criterion within a prompt as an independent grading task, thus the GPT models output a score for each analytic criterion independently.\nMore details of the input prompt can be found in Table 5  ###reference_### in the appendix."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In the experiment, we investigate the feasibility of our task formulation for STEs using the BERT model and the state-of-the-art large language models, GPT-4 and GPT-3.5.\nWe also investigate the impact of the number of in-context examples on the scoring performance.\nAs discussed in Section 5.2  ###reference_###, the models showed notably lower performance in grading incorrect responses than in grading correct responses.\nThis discrepancy may be due to the difference in the number of variations between correct and incorrect responses.\nAs shown in Figure 1  ###reference_###, the variation of acceptable correct responses is limited; meanwhile, the variation of incorrect responses shows considerable latitude, potentially encompassing any type of response besides the correct ones.\nConsequently, although the training data covered the majority of variations in correct responses, they cannot cover all potential incorrect responses.\nAdditionally, the GPT models significantly struggled in grading such incorrect responses, especially with fewer examples than the BERT models.\nTable 4  ###reference_### shows a grading error made by GPT-3.5, in which the model significantly failed to recognize an incorrect response.\nSuch grading errors constitute the majority of inaccurate predictions by GPT-3.5.\nWe hypothesized that these inaccuracies are due to the specialized prompt and response format of STEs, including scores, detailed rubrics, and justification cues.\nHence, during pretraining, GPTs are not exposed to such a task, despite the extensive corpora collected from the Web.\nUtilizing GPT-3.5 for few-shot in-context learning is expected to be more suitable for classroom applications than fine-tuning the model with a substantial amount of training data.\nHowever, our observations suggest that this application of GPT-3.5 is inadequate for grading STEs.\nTo investigate the appropriate number of in-context examples, we evaluate performance by varying the number of examples provided in the prompt.\nFigure 3  ###reference_### illustrates the -score of GPT-3.5 for each label as the number of in-context examples is varied between one, two, five, and 10.\nFrom the result, we can clearly see that the grading performance hardly changed even when the number of in-context examples was increased to more than two.\nAs a reason for this, in grading for correct responses, it is considered that our task design inherently results in a very limited number of patterns corresponding to correct expressions. Therefore, increasing the number of instruction samples may not significantly influence the accuracy for correct responses.\nIn the grading of incorrect responses, a considerable number of instances are labeled as incorrect due to the absence of expressions equivalent to the correct answers.\nIn such cases, justification cue string is not given in the instruction for GPTs and this makes it challenging to grasp scoring clues from the provided instruction examples, likely hindering the effective learning of appropriate grading and consequently impeding performance improvement.\n###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Settings",
            "text": "In our dataset, the label “partially correct” was infrequently used, which transformed the grading of certain criteria into a binary classification task.\nTherefore, we used the -score to evaluate the performance of the analytic score prediction as it applies to both three-class and binary classification.\nWe also performed a 5-fold cross-validation by dividing the dataset of each question into a training set, a development set, and an evaluation set following a 3:1:1 ratio.\nWe finetuned the BERT model (described in Section 4  ###reference_###) for 50 epochs on each training set.\nFor each epoch, we calculated -score for each analytic criterion and used the parameters that produced the best results on the development set for each analytic criterion, respectively.\nAppendix C  ###reference_### provides details regarding these hyperparameters.\nFor the GPT models, we randomly selected few-shot examples for each score from the training set.\nSome analytic criteria contained extremely few incorrect responses because typical high school students found them too easy. Therefore, to ensure a proper performance evaluation, we used only those criteria that contained 10% or more incorrect instances."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "Table 3  ###reference_### shows the performance of BERT, GPT-3.5, and GPT-4 on the test set in terms of  averages and standard deviations for each category (Expression, Word Order, Grammar).\nIn Section 4.2  ###reference_###, we hypothesized that the GPT models would demonstrate excellent performance because STEs evaluate the validity of English sentences within a highly limited grammar and vocabulary scope presented in an analytic criterion.\nSurprisingly, however, the BERT model outperformed the GPT models on our dataset.\nNevertheless, both models showed relatively high performance in grading correct responses.\nMeanwhile, the GPT models performed notably lower in grading incorrect responses.\nInterestingly, however, the GPT models outperformed BERT in grading partially correct responses.\nThis may be due to the limited data size for fine-tuning the BERT model for partially correct responses.\nWe also observed that the standard deviation exceeded 0.10 for nearly all results, indicating a substantial variance in grading performance across different analytic criteria, some of which showed poor results.\nThe result suggests that the grading of several analytic criteria is challenging for models.\nLLMs acquire sufficient knowledge about language, including grammar and vocabulary, through pretraining on massive corpora. However, these results showed that STEs grading remains a challenging task even for a cutting-edge LLM such as GPT-4, when provided with only few-shot examples.\nFurthermore, collecting and annotating enough responses to train the STE grading model poses a significant burden in actual educational settings, allowing room for improvement in deploying automatic grading models in actual classrooms."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Analysis",
            "text": "As discussed in Section 5.2  ###reference_###  ###reference_###, the models showed notably lower performance in grading incorrect responses than in grading correct responses.\nThis discrepancy may be due to the difference in the number of variations between correct and incorrect responses.\nAs shown in Figure 1  ###reference_###  ###reference_###, the variation of acceptable correct responses is limited; meanwhile, the variation of incorrect responses shows considerable latitude, potentially encompassing any type of response besides the correct ones.\nConsequently, although the training data covered the majority of variations in correct responses, they cannot cover all potential incorrect responses.\nAdditionally, the GPT models significantly struggled in grading such incorrect responses, especially with fewer examples than the BERT models.\nTable 4  ###reference_###  ###reference_### shows a grading error made by GPT-3.5, in which the model significantly failed to recognize an incorrect response.\nSuch grading errors constitute the majority of inaccurate predictions by GPT-3.5.\nWe hypothesized that these inaccuracies are due to the specialized prompt and response format of STEs, including scores, detailed rubrics, and justification cues.\nHence, during pretraining, GPTs are not exposed to such a task, despite the extensive corpora collected from the Web.\nUtilizing GPT-3.5 for few-shot in-context learning is expected to be more suitable for classroom applications than fine-tuning the model with a substantial amount of training data.\nHowever, our observations suggest that this application of GPT-3.5 is inadequate for grading STEs.\nTo investigate the appropriate number of in-context examples, we evaluate performance by varying the number of examples provided in the prompt.\nFigure 3  ###reference_###  ###reference_### illustrates the -score of GPT-3.5 for each label as the number of in-context examples is varied between one, two, five, and 10.\nFrom the result, we can clearly see that the grading performance hardly changed even when the number of in-context examples was increased to more than two.\nAs a reason for this, in grading for correct responses, it is considered that our task design inherently results in a very limited number of patterns corresponding to correct expressions. Therefore, increasing the number of instruction samples may not significantly influence the accuracy for correct responses.\nIn the grading of incorrect responses, a considerable number of instances are labeled as incorrect due to the absence of expressions equivalent to the correct answers.\nIn such cases, justification cue string is not given in the instruction for GPTs and this makes it challenging to grasp scoring clues from the provided instruction examples, likely hindering the effective learning of appropriate grading and consequently impeding performance improvement.\n###figure_4###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Grammar Error Correction (GEC) and Short Answer Scoring (SAS) are the two major research areas in the automatic evaluation of descriptive English responses.\nWe position this study between these two research domains.\nThe dataset we created for the STE task followed the format of the RIKEN SAS dataset, which contains questions on Japanese reading comprehension questions Mizumoto et al. (2019  ###reference_b22###); Funayama et al. (2023  ###reference_b11###).\nOther SAS datasets include BEETLE  Dzikovska et al. (2013  ###reference_b8###), ASAP-SAS,222https://www.kaggle.com/c/asap-sas/  ###reference_### Powergrading, and the SAF dataset Filighera et al. (2022  ###reference_b10###), which focus on science or reading comprehension.\nOur dataset is the first STE dataset to concentrate on grading grammar and vocabulary use."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Short Answer Scoring (SAS)",
            "text": "We have formally defined our STE grading task within the established framework of the automated SAS task.\nHowever, these two tasks fundamentally differ in terms of their intended objectives and the descriptive content to be evaluated.\nSeveral SAS studies have primarily examined closed-domain questions that require knowledge and understanding in specific areas, such as science or reading comprehension Mizumoto et al. (2019  ###reference_b22###); Burrows et al. (2015  ###reference_b2###); Galhardi and Brancher (2018  ###reference_b12###), and a typical SAS framework does not directly consider grammatical errors and word usage errors in responses.\nIn this study, we created detailed and stringent analytic criteria for measuring learners’ English proficiency, focusing on the grammatical aspects addressed in the questions.\nThe dataset we created for the STE task followed the format of the RIKEN SAS dataset, which contains questions on Japanese reading comprehension questions Mizumoto et al. (2019  ###reference_b22###  ###reference_b22###); Funayama et al. (2023  ###reference_b11###  ###reference_b11###).\nOther SAS datasets include BEETLE  Dzikovska et al. (2013  ###reference_b8###  ###reference_b8###), ASAP-SAS,222https://www.kaggle.com/c/asap-sas/  ###reference_###  ###reference_### Powergrading, and the SAF dataset Filighera et al. (2022  ###reference_b10###  ###reference_b10###), which focus on science or reading comprehension.\nOur dataset is the first STE dataset to concentrate on grading grammar and vocabulary use."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study introduced a novel task focusing on the automatic grading of Sentence Translation Exercises (STEs) for educational purposes.\nWe formalized STEs as a task of grading each analytic criterion predetermined by teachers’ intentions and constructed a dataset to implement the task.\nThis first-of-its-kind dataset emulates and reflects the practical form of L2 learning in the responses of learners. We also used finetuned BERT and GPTs with few-shot in-context learning to establish a baseline and demonstrate the feasibility of the formulated framework.\nIn our experiment, although the GPT models showed substantial performance in various NLP tasks, they remained inferior to the BERT model, suggesting that our newly defined task continues to be challenging even for the state-of-the-art LLMs, therefore necessitating further exploration.\nWith regard to future direction, we are contemplating the integration of technologies such as GEC and machine translation within our model. We aim to build cross-questions strategies to automatically identify expressions that diverge from a provided rubric while preserving the text’s fundamental meaning using a combination of these technologies.\nFor this purpose, our plan involves further subdividing the STE grading task and leveraging LLMs to address each minimized task such as correcting grammatical errors, assessing the consistency of meaning with L1, and identifying expressions aligned with the learning objectives in each exercise.\nThis approach also aims to investigate tasks where LLMs may not excel in STE scoring and enhance their overall performance.\nAdditionally, in an educational context, we also consider generating more comprehensive feedback comments on the scoring results, extending beyond the estimation of justification cues."
        }
    ]
}