{
    "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
    "abstract": "Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.\n\n\n\nKeywords: Code QA Dataset, Code Search, Contrastive Pretraining",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Code Question Answering (Code QA) represents a pivotal research area in software intelligence.\nOne popular task formulation is retrieval-based QA (Gu et al., 2018  ###reference_b9###), in which the primary objective is to effectively match user queries expressed in natural language to relevant code snippets from an existing corpus.\nThe prevailing approach for retrieval-based Code QA has been the utilization of dual-encoder-based representation models.\nThe core idea underlying this approach is to map natural language queries and code snippets into a shared representation space, where closely located vectors correspond to semantically similar meanings.\nTo learn a shared representation space for text and code, early research efforts adopted masked language modeling (MLM) objective on paired text-code dataset to align different modalities (Kanade et al., 2020  ###reference_b17###; Feng et al., 2020  ###reference_b8###), similar to the monolingual and cross-lingual pre-training approaches (Devlin et al., 2019  ###reference_b6###; Conneau and Lample, 2019  ###reference_b5###).\nSubsequent work discovered the potential of contrastive pre-training, and applied it to code representation learning by constructing large-scale paired datasets (Jain et al., 2021  ###reference_b16###; Li et al., 2022  ###reference_b20###).\nCurrent contrastive code representation learning methods such as CodeRetriever (Li et al., 2022  ###reference_b20###) typically rely on curated uni-modal (code-code pairs) or bi-modal data (text-code pairs).\nFew work even uses distinct encoder for text and code (Heyman and Cutsem, 2020  ###reference_b12###; Salza et al., 2023  ###reference_b29###).\nSuch pre-training design emphasizes the concept of modality distinction, diverging from the goal of establishing a unified representation space for different modalities.\nIn Figure 1  ###reference_###, we illustrate different data formats used for contrastive pre-training and analyse their chunk-level matching patterns.\nUni-modal data offers code-code matching patterns, whereas bi-modal data implies code-text matching patterns.\nWhile a combination of both data types during pre-training can enable models to learn both matching signals, a more data-efficient approach to capture all matching patterns is through mixed-modal data.\n###figure_1### Segmentation fault while copying a string to the memory allocated array\nFollowing is a program I am practicing;\nint main()\nint i = 0;\nchar **grid = (char **) malloc(5*sizeof(int));\nfor (i = 0 ; i < 5 ; i++)\ngrid[i] = (char *) malloc(6);\nstrcpy(grid[0], \"eabcd\");\nstrcpy(grid[1], \"fghij\");\nstrcpy(grid[2], \"olkmn\");\nstrcpy(grid[3], \"trpqs\");\nstrcpy(grid[4], \"xywuv\"); /*Segmentation Fault at this line*/\nreturn 0;\n\nI am getting a segmentation fault at the line strcpy(grid[4], \"xywuv\"); . What could be the reason? I have allocated the array to have 5 strings(rows) of 6 characters each(columns).\nYou are allocating the wrong type at line 3\nchar **grid = (char **) malloc(5*sizeof(int));\nShould be\nchar **grid = (char **) malloc(5*sizeof(char*));\nThis is because you are declaring string-array. Therefore, the malloc should be char* (string / character pointer)\nAlso the same if you were trying to declare 2-D integer array. It will be\nint **grid = (int **) malloc(5*sizeof(int*));\nBesides, the majority of code embedding models (Jain et al., 2021  ###reference_b16###; Wang et al., 2021a  ###reference_b30###; Li et al., 2022  ###reference_b20###) have primarily relied on CodeSearchNet (Husain et al., 2019  ###reference_b14###) as the main pre-training corpus. While CodeSearchNet is a valuable resource, its size and data distribution have inherent limitations that may impact the quality and diversity of learned code representations.\nRecent work has proposed to curate large-scale code datasets from GitHub (Allal et al., 2023  ###reference_b2###).\nYet their efforts mainly focus on training large-scale generative language models (LMs).\nIn parallel, some research endeavors have aimed to create code-related question-answering datasets from diverse sources, as evidenced by Huang et al. (2021  ###reference_b13###); Lee et al. (2022  ###reference_b18###).\nNevertheless, most of these datasets remain constrained by their scale, rendering them more suitable for stand-alone evaluation benchmarks rather than comprehensive pre-training corpus.\nTherefore in this research, we try to bridge these gaps by proposing ProCQA, a large-scale community-based programming question answering dataset mined from StackOverflow.\nProCQA encompasses an extensive collection of approximately 5 million QA pairs, spanning 11 different programming languages.\nThis dataset is distinguished by its comprehensive language coverage, the diversity of user queries, and its code-mixing data format111Please refer to Table 1  ###reference_### for an illustrative example.. It can be used as both an evaluation benchmark and a pre-training corpus.\nWe provide strict rule-based filtering and data decontamination procedure to ensure its quality and fairness.\nDifferent types of baseline models are trained and compared on this dataset to test its suitability as an evaluation benchmark.\nTo assess the efficacy of our proposed dataset as a pre-training corpus, we conducted large-scale modality-agnostic contrastive pretraining (MACP) on the code-mixing dataset, without making distinctions between text and code modalities.\nTo demonstrate whether MACP can learn a better aligned representation space, we evaluated it on extensive code retrieval benchmarks, covering supervised, zero-shot, and out-of-domain scenarios.\nExperiments reveal that compared to previous pre-trained code language models, MACP achieves substantial improvements on most tasks we considered, advancing the previous best code retrieval model CodeRetriever (Li et al., 2022  ###reference_b20###) by 110% points across different evaluation benchmarks.\nComprehensive ablation and analysis demonstrates the effectiveness of our proposed approach.\nThe contributions of this paper can be summarized as follows:\nWe create ProCQA, a large-scale dataset for programming question answering.\nProCQA is characterized by its practicality, diversity and mixed-modal data format.\nWe demonstrate its potential as an evaluation benchmark for comparing different code language models.\nBased on ProCQA, we present MACP, a code representation model pre-trained with modality-agnostic contrastive learning on the large-scale code-mixing dataset. MACP demonstrates remarkable performance gains over prior approaches across a wide range of code retrieval tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Code Language Models",
            "text": "Language models pre-trained on large-scale unlabeled corpora have demonstrated significant potential in code understanding and generation tasks.\nPrior works such as CodeBERT (Feng et al., 2020  ###reference_b8###) employed replaced language modeling on uni-modal and bi-modal data for pre-training.\nGraphCodeBERT (Guo et al., 2021  ###reference_b11###) advanced this approach by harnessing data flow encoded in the Abstract Syntax Tree (AST) of code to enrich code structural information during pre-training.\nUniXCoder (Guo et al., 2022  ###reference_b10###) unified three pre-training designs into one architecture and utilized AST structure and code comment to enhance the cross-modal alignment.\nThere are also some work on adapting generative language models for code, as exemplified by CodeT5 (Wang et al., 2021b  ###reference_b31###) and PLBART (Ahmad et al., 2021  ###reference_b1###). These models incorporate code structure information into the design of specific pre-training tasks.\nContrastive methods have also been introduced into code pre-training by several recent works with different approaches proposed for constructing positive and negative pairs (Jain et al., 2021  ###reference_b16###; Wang et al., 2021a  ###reference_b30###; Ding et al., 2022  ###reference_b7###; Bui et al., 2021  ###reference_b3###; Li et al., 2022  ###reference_b20###).\nIt is worth noting that current code language models’ pre-training corpus are primarily sourced from CodeSearchNet, consisting of 2 million code-text pairs. Limited efforts have been dedicated to mining large-scale datasets from GitHub (Allal et al., 2023  ###reference_b2###; Li et al., 2023  ###reference_b19###), but they mainly focus on training decoder language models rather than code representation models.\nAn exception is the work by OpenAI (Neelakantan et al., 2022  ###reference_b26###), but their models are only available via paid APIs and training data is not detailed."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   ProCQA",
            "text": "In this section, we outline the methodologies employed in the creation of the ProCQA dataset, along with the filtering strategies applied to ensure data quality and fairness. Additionally, we present an analysis of various dataset statistics and define two tasks utilizing this dataset to evaluate different baseline models. The source code is available at https://github.com/jordane95/procqa  ###reference_###.\nThis task is defined as finding the correct answer from a large-scale answer corpus.\nWe use answers from all splits of the dataset to form retrieval corpus.\nThe query is the concatenation of question and description.\nWe choose BM25 and some recent neural language models, such as BERT (Devlin et al., 2019  ###reference_b6###), CodeBERT (Feng et al., 2020  ###reference_b8###) and UniXCoder (Guo et al., 2022  ###reference_b10###).\nNeural LMs are fine-tuned with the contrastive learning objective (i.e., InfoNCE loss) on the question answer pairs from the training set.\nAll models are trained for 3 epochs with the batch size of 32 and the learning rate of 2e-5.\nBoth questions and answers are truncated to be maximum of 256 tokens.\nWe choose MRR@10, Recall@10 and Recall@100 as main evaluation metrics.\nResults are demonstrated in Table 4  ###reference_###.\nWe observe that text-only language models such as BERT are even inferior to unsupervised BM25, in terms of MRR@10.\nWith code-specific pre-training, CodeBERT can outperform the strong BM25 baseline.\nMore recent code language models such as UniXCoder performs best on this task.\nWe also consider a generative task formulation, in which the model is required to directly generate the answer to the question without additional reference.\nSimilarly, we benchmark several generative language models on this task.\nSelected baseline models include T5 (Raffel et al., 2020  ###reference_b28###), CodeT5 (Wang et al., 2021b  ###reference_b31###), PLBART (Ahmad et al., 2021  ###reference_b1###).\nModels are trained in a sequence-to-sequence manner by optimizing the cross-entropy loss of the answer sequence given question sequence with the same training hyperparameters as stated above.\nDuring inference, beam search decoding is used with a beam size of 5.\nWe use ROUGE Lin (2004  ###reference_b21###) as main evaluation metrics for this task and demonstrate results in Table 5  ###reference_###.\nIt is found that code language models is better than text-only models, indicating the effectiveness of code-specific pre-training.\nEven the best model struggles on this task because the answers are relatively long (mostly 100-200 words, see Figure 2  ###reference_###).\nThis indicates that ProCQA is a challenging dataset for long-form generative QA task.\nHow to improve the long-form question-answering performance of language models with limited parameters is also an interesting direction for future research."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Data Acquisition",
            "text": "To ensure the diversity and reflect real world user problems, we crawl our dataset from StackOverflow, a question answering community focusing on solving programming problems.\nUsers can post their problems on the website and wait for others’ answers.\nOne characteristic of this dataset is that both the question and answer are code-mixing, i.e., text and code are interleaved within these fields.\nSuch data format is very useful to indoctrinate and evaluate the model’s matching ability of different patterns.\nWe use the public dumps as of 2022/12 for raw data downloading222https://archive.org/details/stackexchange.\nWe extract the textual content consisting of code and texts from XML files.\nThree fields (title, question, answer) are kept.\nHTML tags are removed and only text content are kept using BeautifulSoup library."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Data Cleaning",
            "text": "A critical problem with these QA communities is that there are many unanswered questions and wrong answers.\nTo handle this issue, we apply some rule-based approaches to filter out low-quality questions and answers.\nMore specifically, we filter out questions/answers that are either too short (< 20 characters) or too long (> 4096 characters).\nWe only keep questions that have answers marked as accepted by the questioner since it is a natural annotation signal indicating the answer is helpful for the user."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Data Format",
            "text": "Data in ProCQA is formatted as triples illustrated in Table 1  ###reference_###.\nThe question is a concise user request.\nIt is coupled with a detailed description which explains the problem in more detail.\nThe answer is posted by other user and is the one accepted by the questioner.\nNote that in all data fields, code and text are interleaved, which provides a natural supervision signal for aligning the two modalities."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Data Statistics",
            "text": "We partition the dataset into different programming language subsets according to their tags contained in meta information.\nWe consider the following eleven languages based on their popularity: Python, Java, JavaScript, Ruby, C, C++, C#, Rust, PHP, Lisp and Go.\nDataset statistics are shown in Table 2  ###reference_###.\nWe split the dataset into train / valid / test set by a proportion of 80%:10%:10% following chronological order of posting date.\nIn addition, we analyse the question and answer length distribution of our ProCQA dataset in Figure 2  ###reference_###.\nMost of the QA pairs in ProCQA contain dozens or hundreds of words, which are much closer to real user questions.\n###figure_2###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Decontamination",
            "text": "Since ProCQA is crawled from StackOverflow, it many overlap with some evaluation sets constructed from the same source.\nTo avoid data contamination, we perform evaluation data deduplication for our ProCQA training set.\nSpecifically, we employ two methods for deduplication.\nThe first one is based on substring matching.\nTraining example in ProCQA dataset is dropped if it contains any substring that is part of the queries in the evaluation set.\nWe use three evaluation sets to perform deduplication (CoNaLa, SO-DS and StaQC).\nAfter this step, about 0.5% examples from the Python subset are dropped.\nOther subsets are influenced lightly.\nWe also apply fuzzy deduplication method based on MinHash but no additional duplicate is found."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "3.6.   Comparison to previous datasets",
            "text": "To better understand the difference with previous dataset, we summarize some key factors of our ProCQA and previous ones in Table 3  ###reference_###, including the number of supported programming languages (PLs), data format, size and data source.\nCodeNN (Iyer et al., 2016  ###reference_b15###) is also a dataset mined from StackOverflow for code summarization but contains much smaller amount of training examples and languages.\nCodeSearchNet (CSN) is on pair with ProCQA in terms of languages and size but drawn from a different data distribution (GitHub). Its queries are either documentation strings or comments rather than natural language questions, limiting its practicality in real scenarios.\nCoSQA and CS1QA contain some real user queries collected from Bing logs and classrooms but only cover Python and are limited in size.\nIn summary, ProCQA differs from previous work in the following main aspects:\n1. More diverse language distribution at a larger scale.\n2. Long-form questions and answers more aligned with real-world scenarios."
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "3.7.   Tasks",
            "text": "We define two tasks based on the collected dataset for pilot exploration, including answer retrieval and generation.\nWe choose C subset as a test bed for comparing multiple language models.\nThis task is defined as finding the correct answer from a large-scale answer corpus.\nWe use answers from all splits of the dataset to form retrieval corpus.\nThe query is the concatenation of question and description.\nWe choose BM25 and some recent neural language models, such as BERT (Devlin et al., 2019  ###reference_b6###  ###reference_b6###), CodeBERT (Feng et al., 2020  ###reference_b8###  ###reference_b8###) and UniXCoder (Guo et al., 2022  ###reference_b10###  ###reference_b10###).\nNeural LMs are fine-tuned with the contrastive learning objective (i.e., InfoNCE loss) on the question answer pairs from the training set.\nAll models are trained for 3 epochs with the batch size of 32 and the learning rate of 2e-5.\nBoth questions and answers are truncated to be maximum of 256 tokens.\nWe choose MRR@10, Recall@10 and Recall@100 as main evaluation metrics.\nResults are demonstrated in Table 4  ###reference_###  ###reference_###.\nWe observe that text-only language models such as BERT are even inferior to unsupervised BM25, in terms of MRR@10.\nWith code-specific pre-training, CodeBERT can outperform the strong BM25 baseline.\nMore recent code language models such as UniXCoder performs best on this task.\nWe also consider a generative task formulation, in which the model is required to directly generate the answer to the question without additional reference.\nSimilarly, we benchmark several generative language models on this task.\nSelected baseline models include T5 (Raffel et al., 2020  ###reference_b28###  ###reference_b28###), CodeT5 (Wang et al., 2021b  ###reference_b31###  ###reference_b31###), PLBART (Ahmad et al., 2021  ###reference_b1###  ###reference_b1###).\nModels are trained in a sequence-to-sequence manner by optimizing the cross-entropy loss of the answer sequence given question sequence with the same training hyperparameters as stated above.\nDuring inference, beam search decoding is used with a beam size of 5.\nWe use ROUGE Lin (2004  ###reference_b21###  ###reference_b21###) as main evaluation metrics for this task and demonstrate results in Table 5  ###reference_###  ###reference_###.\nIt is found that code language models is better than text-only models, indicating the effectiveness of code-specific pre-training.\nEven the best model struggles on this task because the answers are relatively long (mostly 100-200 words, see Figure 2  ###reference_###  ###reference_###).\nThis indicates that ProCQA is a challenging dataset for long-form generative QA task.\nHow to improve the long-form question-answering performance of language models with limited parameters is also an interesting direction for future research."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": "To assess the quality and utility of our proposed dataset, we evaluate its benefits to other code search benchmarks when acting as a pre-training corpus.\nWe also conduct ablation experiments to demonstrate the effectiveness of ProCQA over existing pre-training corpus CSN.\nWe first present evaluation results on six programming language subsets of CodeSearchNet in Table 7  ###reference_###.\nMACP trained with the newly proposed dataset outperforms previous best model CodeRetriever on all language subsets, by an average of 2.1 points.\nNext, we look at results on several challenging benchmarks, all collected from real-world user queries instead of docstrings.\nAs shown in Table 8  ###reference_###, our model significantly outperforms prior state-of-the-art models by up to 10 points on average.\nWe attribute the improvement to real-world user queries from ProCQA.\nAfter evaluating the cross-modal search ability of our embedding model, we zoom into the intra-modal retrieval performance by evaluating on a code clone detection benchmark, POJ-104.\nResults are illustrated in Table 9  ###reference_###.\nOur model outperforms previous best baseline CodeRetriever by +1.38 points.\nWe list the cross-lingual code retrieval performance of our model MACP and other baselines from  Guo et al. (2022  ###reference_b10###) in Table 10  ###reference_###.\nUniXCoder has significantly better zero-shot code retrieval performance, owing to its contrastive objective during pre-training.\nMACP consistently outperforms previous baselines by a large margin, setting new state-of-the-art performance on this task.\nIn Table 11  ###reference_###, we compare different models’ performance on Solidity and SQL, two languages unseen during pre-training.\nPrevious best model MAML (Chai et al., 2022  ###reference_b4###) applied model-agnostic meta learning on CodeBERT (Feng et al., 2020  ###reference_b8###) using Java and Python subsets from CSN for pre-training.\nIn addition, we also report the performance of GraphCodeBERT using our codebase as another baseline for comparison.\nOur model significantly improves the cross-domain code search performance on unseen languages.\nOne possible reason is that the diversity of language coverage in ProCQA equips the model with better language adaptation ability.\nWe first investigate the effect of different pre-training corpus by doing a series of controlled experiments where only the pre-training data distribution is changed.\nWe run two additional experiments by using the CSN and ProCQA dataset individually for pre-training.\nDue to space limitation, we report downstream fine-tuned retrieval performance on CodeSearchNet in Figure 3  ###reference_###.\nResults on other evaluation benchmarks follow the same trends.\nDespite CSN belongs to in-domain data for this evaluation benchmark, it still underperforms ProCQA when being used as a pre-training corpus.\nCombining two datasets gives better results.\nThis showcases the effectiveness of ProCQA dataset being used as a mixed-modal corpus for retrieval-oriented contrastive pre-training.\n###figure_3### We ablate on the choice of modality-agnostic contrastive learning by comparing to another setting which we explicitly distinguish text and code in data design.\nDue to the high difficulty of parsing incomplete code snippets in ProCQA, we conduct this ablation on the CSN pre-training corpus where code are well formed and can be parsed by existing tools. Bi-modal setting removes all comments in the code while mixed-modal setting keeps them.\nWe list results in Table 12  ###reference_###.\nThe evaluation set Adv Test only requires code-text matching, yet training with mixed-modal data formats still has benefits.\nTo avoid data contamination and ensure fairness, we performed de-duplication for the ProCQA dataset with respect to the relevant evaluation benchmarks from the same source, including CoNaLa, SO-DS and StaQC.\nIn Table 13  ###reference_###, we provide a quantitative analysis on the proportion of contaminated data for each evaluation set and the performance using raw and filtered version of the ProCQA dataset for pre-training.\nAlthough a large-proportion of the evaluation set is included in the raw pre-training data, removing them raises a limited degradation of model performance, as they only make up a small portion of the large-scale pre-training data."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Settings",
            "text": "Our model basically follows the two-tower architecture in which vector representations for code and text are produced by mean pooling over the last layer hidden representations of the language models. It is trained via the contrastive objective using the InfoNCE loss\nwhere  denotes the question,  denotes the corresponding answer,  is a set of negative samples,  is the temperature.\n is also enlarged with other examples from the same batch.\nThe main baselines we compare to are GraphCodeBERT (Guo et al., 2021  ###reference_b11###) and CodeRetriever (Li et al., 2022  ###reference_b20###). In addition to the text-code pairs in CSN used by GraphCodeBERT, CodeRetriever also employs sophisticated rules and learned models for mining high-quality code-code and code-text pairs from the raw CSN code corpus.\nInstead we use commonly available QA pairs from ProCQA mined by weak supervision.\nWe use the training split across all languages to construct different types of mixed-modal positive pairs.\nWe apply modality-agnostic contrastive pre-training on the ProCQA and CSN dataset and compare our model to previous code embedding models on various retrieval tasks.\nOur model is denoted as MACP."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Implementation Details",
            "text": "For fair comparison, our model is initialized with GraphCodeBERT, same as CodeRetriever.\nMACP is pre-trained with the contrastive objective in Equation 1  ###reference_### using cosine similarity and .\nTo balance low-resource languages, we sample each data batch from a multi-nominal distribution over different language subsets\nwith  equal to the size of subset  and smoothing parameter .\nWe run the contrastive pre-training for 10k steps with a global batch size of 6192.\nIn-batch negatives are used and shared across different GPUs.\nEach sequence is truncated at a maximum length of 128.\nThe learning rate is initially warmed up to 2e-4 for the first 10% steps, followed by a linear decay.\nWe utilize the same contrastive loss during fine-tuning on each downstream dataset.\nEach fine-tuning experiment only involves one dataset so we directly sample data after shuffling it.\nModels are trained using a peak learning rate of 2e-5 with the same scheduler as pre-training.\nThe maximum sequence length is 512.\nBatch size is 128 and each sample is accompanied with 7 randomly sampled negatives.\nTraining epochs is 3.\nOther hyperparameters are same as pre-training.\nWe only consider in-batch negatives for contrastive learning so we compare models under this setting.\nWe conduct all experiments on two NVIDIA A100 GPUs with 40G memory.\nWe use DeepSpeed, gradient checkpointing and mixed precision (FP16) encoding to reduce memory cost.\nThe pre-training process takes about 18 hours. Fine-tuning on all datasets is finished in one day."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Evaluation Benchmarks",
            "text": "To provide an extensive evaluation of the generalization ability of our pre-trained models, we select a large variety of code retrieval tasks from different domains under different settings.\nWe first evaluate on the CodeSearchNet benchmark (Husain et al., 2019  ###reference_b14###), which is widely used for evaluating the text-code search ability of code retrieval models.\nOne drawback of CodeSearchNet is the queries are not aligned to real user questions.\nSo, we also evaluate on some more challenging datasets, Adv Test (Lu et al., 2021  ###reference_b24###), CoSQA (Huang et al., 2021  ###reference_b13###), CoNaLa (Yin et al., 2018  ###reference_b34###), SO-DS (Heyman and Cutsem, 2020  ###reference_b12###), StaQC (Yao et al., 2018  ###reference_b33###).\nThe last three evaluation datasets follow the setting of  Heyman and Cutsem (2020  ###reference_b12###), where during inference both text description and code snippet are used for matching.\nThe main evaluation metric is MRR.\nThen, code-code search results on POJ-104 (Mou et al., 2016  ###reference_b25###) is also reported to evaluate the intra-modal retrieval ability.\nIn this dataset, Python program solutions of the same problem is regarded as positive pairs.\nThe objective is to retrieve relevant code snippets which answer the same problem.\nTo investigate the cross-lingual code retrieval ability, we use CodeNet (Puri et al., 2021  ###reference_b27###) as an evaluation benchmark, which is also a problem-solution dataset similar to POJ-104 but covers more languages.\nOn CodeNet, we consider the zero-shot retrieval of three programming languages (Ruby, Python and Java) following Guo et al. (2022  ###reference_b10###), where code is pre-processed by removing comments and replacing all separators with whitespace.\nPerformance is evaluated by MAP.\nFinally, to test whether our model can generalize to out-of-domain languages, we choose two text-code search datasets with languages unseen during pre-training. Smart Contracts (SC) (Yang et al., 2021  ###reference_b32###) contains Solidity programming language and Spider (Yu et al., 2018  ###reference_b35###) consists of SQL-query pairs.\nWe use the dataset split released by  Chai et al. (2022  ###reference_b4###).\nModels are evaluated by Recall@{1,5,10} and MRR@1000. The statistics of downstream evaluation benchmarks are illustrated in Table 6  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Results",
            "text": "In this section, we report and discuss the performance of MACP on the evaluation benchmarks introduced in the previous section, spanning both supervised and zero-shot settings.\nIn the supervised setting, MACP is directly fine-tuned on full training set and the last checkpoint is evaluated on the test set.\nIn the zero-shot setting, it is directly evaluated on the test set.\nWe first present evaluation results on six programming language subsets of CodeSearchNet in Table 7  ###reference_###  ###reference_###.\nMACP trained with the newly proposed dataset outperforms previous best model CodeRetriever on all language subsets, by an average of 2.1 points.\nNext, we look at results on several challenging benchmarks, all collected from real-world user queries instead of docstrings.\nAs shown in Table 8  ###reference_###  ###reference_###, our model significantly outperforms prior state-of-the-art models by up to 10 points on average.\nWe attribute the improvement to real-world user queries from ProCQA.\nAfter evaluating the cross-modal search ability of our embedding model, we zoom into the intra-modal retrieval performance by evaluating on a code clone detection benchmark, POJ-104.\nResults are illustrated in Table 9  ###reference_###  ###reference_###.\nOur model outperforms previous best baseline CodeRetriever by +1.38 points.\nWe list the cross-lingual code retrieval performance of our model MACP and other baselines from  Guo et al. (2022  ###reference_b10###  ###reference_b10###) in Table 10  ###reference_###  ###reference_###.\nUniXCoder has significantly better zero-shot code retrieval performance, owing to its contrastive objective during pre-training.\nMACP consistently outperforms previous baselines by a large margin, setting new state-of-the-art performance on this task.\nIn Table 11  ###reference_###  ###reference_###, we compare different models’ performance on Solidity and SQL, two languages unseen during pre-training.\nPrevious best model MAML (Chai et al., 2022  ###reference_b4###  ###reference_b4###) applied model-agnostic meta learning on CodeBERT (Feng et al., 2020  ###reference_b8###  ###reference_b8###) using Java and Python subsets from CSN for pre-training.\nIn addition, we also report the performance of GraphCodeBERT using our codebase as another baseline for comparison.\nOur model significantly improves the cross-domain code search performance on unseen languages.\nOne possible reason is that the diversity of language coverage in ProCQA equips the model with better language adaptation ability."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5.   Analysis",
            "text": "We first investigate the effect of different pre-training corpus by doing a series of controlled experiments where only the pre-training data distribution is changed.\nWe run two additional experiments by using the CSN and ProCQA dataset individually for pre-training.\nDue to space limitation, we report downstream fine-tuned retrieval performance on CodeSearchNet in Figure 3  ###reference_###  ###reference_###.\nResults on other evaluation benchmarks follow the same trends.\nDespite CSN belongs to in-domain data for this evaluation benchmark, it still underperforms ProCQA when being used as a pre-training corpus.\nCombining two datasets gives better results.\nThis showcases the effectiveness of ProCQA dataset being used as a mixed-modal corpus for retrieval-oriented contrastive pre-training.\n###figure_4### We ablate on the choice of modality-agnostic contrastive learning by comparing to another setting which we explicitly distinguish text and code in data design.\nDue to the high difficulty of parsing incomplete code snippets in ProCQA, we conduct this ablation on the CSN pre-training corpus where code are well formed and can be parsed by existing tools. Bi-modal setting removes all comments in the code while mixed-modal setting keeps them.\nWe list results in Table 12  ###reference_###  ###reference_###.\nThe evaluation set Adv Test only requires code-text matching, yet training with mixed-modal data formats still has benefits.\nTo avoid data contamination and ensure fairness, we performed de-duplication for the ProCQA dataset with respect to the relevant evaluation benchmarks from the same source, including CoNaLa, SO-DS and StaQC.\nIn Table 13  ###reference_###  ###reference_###, we provide a quantitative analysis on the proportion of contaminated data for each evaluation set and the performance using raw and filtered version of the ProCQA dataset for pre-training.\nAlthough a large-proportion of the evaluation set is included in the raw pre-training data, removing them raises a limited degradation of model performance, as they only make up a small portion of the large-scale pre-training data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this work we introduce ProCQA, a large-scale community-based programming question answering dataset mined from StackOverflow with strict filtering strategies for quality and fairness control.\nProCQA is featured by its practicality, diversity and code-mixing data format.\nFurthermore, through modality-agnostic contrastive pre-training on interleaved code and text data, our new dataset yields a language model that has a better aligned representation space between code and text, achieving state-of-the-art performance on a large spectrum of code retrieval tasks.\nIn future work, it would be interesting to explore the benefit of ProCQA to other generative code QA tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No.61977003) and the State Key Laboratory of Complex & Critical Software Environment (CCSE-2024ZX-16)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}