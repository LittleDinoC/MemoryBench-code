{
    "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
    "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "A standard approach for supervised text classification is fine-tuning language models such as BERT using an additional classifier head Radford et al. (2018  ###reference_b42###); Dong et al. (2019  ###reference_b12###); Devlin et al. (2018  ###reference_b11###); Yin et al. (2019  ###reference_b67###); Viswanathan et al. (2023  ###reference_b58###); Mosbach et al. (2023  ###reference_b34###). However, these approaches require large amounts of data to achieve state-of-the-art results Edwards et al. (2022  ###reference_b14###) which makes them unsuitable for classification tasks associated with class imbalances and data sparsity Giridhara et al. (2019  ###reference_b19###); Zhang and Wu (2015  ###reference_b71###); Türker et al. (2019  ###reference_b56###); Yin et al. (2019  ###reference_b67###). These problems often occur in real world applications where annotation of data can be performed only by scarce domain experts such as medical and legal domains or applications with highly imbalanced classes such as crime data and fraud detection Giridhara et al. (2019  ###reference_b19###); Zhang and Wu (2015  ###reference_b71###); Türker et al. (2019  ###reference_b56###).\nRecent advances in Natural Language Processing (NLP) lead to the emerge of an alternative approach based on using autoregressive text generation models Radford et al. (2019  ###reference_b43###) that have zero- and few- shot capabilities and perform unseen tasks through the use of\nprompting Schick and Schütze (2021a  ###reference_b47###); Radford et al. (2019  ###reference_b43###); Le Scao and Rush (2021  ###reference_b27###); Viswanathan et al. (2023  ###reference_b58###); Plaza-del Arco et al. (2023  ###reference_b41###). The ability of these models to understand natural language instructions let them generalise to different domains and tasks without the need of large training corpora Plaza-del Arco et al. (2023  ###reference_b41###). There have been even further improvements in the performance of these models in zero-shot settings by fine-tuning them on sets of instructions (task descriptions) Raffel et al. (2020  ###reference_b44###).\nThe promising results of these models against various benchmark datasets Wang et al. (2022b  ###reference_b63###); Liu et al. (2023  ###reference_b29###); Bang et al. (2023  ###reference_b2###) led to increased research into developing methods, mainly based on prompt engineering techniques Viswanathan et al. (2023  ###reference_b58###); Le Scao and Rush (2021  ###reference_b27###) for improving their generalisation capabilities. Further, there has been an increased attention into evaluating the suitability of these models for more specialised domains such as the legal, medical, and financial domain Sarkar et al. (2021  ###reference_b46###); Chalkidis et al. (2020  ###reference_b8###); Yin et al. (2019  ###reference_b67###); Labrak et al. (2023  ###reference_b25###). However, most of the proposed approaches are domain- and task-specific. There is lack of understanding of how these models perform in comparison to more established approaches for text classification. In general, analyses are performed for a small range of model types, domains, and tasks.\nOur work is the first attempt to systematically compare how text generation models using zero-shot and one-shot learning compare to more established but data-consuming approaches for classification based on fine-tuning language models. Our goal is to identify how well current large language models (LLMs) can adapt to different text classification tasks and domains given limited information, and outline the potential strengths and weaknesses of these models. For these purposes, we evaluate five heterogeneous models of different sizes, including traditional masked language models and more recent autoregressive LLMs. Our analyses span over 16 datasets from 7 domains representing binary, multiclass, and multilabel classification.\nOur main contributions are as follows. First, we explore an important but understudied problem of how suitable the newly developed text generation models such as LLaMA, Flan-T5, T5, and ChatGPT are for text classification in few-shot settings compared to lighter models that require training data such as RoBERTa or FastText. In addition to the performance, our analysis helps identify specific strengths and weaknesses of each type of model. Second, in contrast to the majority of existing research focusing on optimisation techniques for prompt creation, we analyse trends in the model’s performance that are non-prompt sensitive as well as look at how the amount of specificity provided in the prompt regarding the task and the domain affect the performance of the models. Third, we evaluate generalisation abilities of models for 7 domains, including real-world specialised domains, such as legal, medical, and crime data. We also analyse how the models’ behaviour changes for datasets used in the pre-training stage versus when testing on unseen datasets."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "We first introduce the different types of methods and models used for text classification along with their strengths and weaknesses (see Section 2.1  ###reference_###). Then, we discuss relevant work on comparing prompting and fine-tuning approaches for text classification as well as outline challenges and research gaps within existing work (see Section 2.2  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Prompting versus Fine-Tuning",
            "text": "Prompting in zero- and few- shot settings, also known as in-context learning (ICL), is the process of providing natural language instructions that describe a task as an input to a language model, including the expected output Labrak et al. (2023  ###reference_b25###). In few-shot prompting, the model is presented with some training examples along with the task instructions. In contrast to fine-tuning techniques, prompting does not involve changing the weights of the model which makes the approach less resource consuming. Additionally, previous research has suggested that prompting can lead to comparable or even better performance than standard fine-tuning techniques Gao et al. (2021  ###reference_b16###); Mosbach et al. (2023  ###reference_b34###). A drawback of this approach is the models’ sensitivity to the prompts where slight changes of the instruction can lead to big differences in the performance Schick and Schütze (2021b  ###reference_b48###); Le Scao and Rush (2021  ###reference_b27###); Sun et al. (2023  ###reference_b53###). Thus, much of the work on text generation models is focused on prompt optimisation techniques based on automatic generation for prompts Wang et al. (2022a  ###reference_b61###); Shin et al. (2020  ###reference_b50###), quantifying the benefits of prompting Schick and Schütze (2021b  ###reference_b48###); Le Scao and Rush (2021  ###reference_b27###), and improving the generalisation abilities of prompts Zhang et al. (2022  ###reference_b72###); Schönfeld et al. (2019  ###reference_b49###); Song et al. (2021  ###reference_b51###); Wang et al. (2022a  ###reference_b61###); Oniani et al. (2023  ###reference_b36###); Sun et al. (2023  ###reference_b53###)\nThere has been an increased research into evaluating and improving the performance of text generation models for zero and few shot classification in more specialised domains such as the legal, medical, and financial domains Ge et al. (2022  ###reference_b17###); Sarkar et al. (2021  ###reference_b46###); Chalkidis et al. (2020  ###reference_b8###). Labrak et al. (2023  ###reference_b25###) evaluate four state-of-the-art instruction-tuned large language models (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on a set of 13 real-world clinical and biomedical natural NLP tasks, including text classification. The results show that instruction-tuned models tend to be outperformed by a specialised model trained for the medical field such as PubMedBERT Gu et al. (2021  ###reference_b20###). This rises questions into the suitability of text generation models and prompting techniques for more specialised domains which require domain experts for annotation. Another research by Mosbach et al. (2023  ###reference_b34###) conducts a comparison between fine-tuning and prompting techniques for two text classification datasets showing that both approaches have similar performance, although with a large variation in results depending on properties such as model size and number of examples. These works show that adapting these models to tasks, especially text classification for more specialised domains, remains a challenge.\nThe variance in performance between tasks and models depending on the prompt design makes the generalisation of text generation models a challenging problem. The small scale on which analyses are performed does not give enough knowledge on how well prompting techniques compare to the more established models for classification across different text classification types and more challenging unfamiliar domains. In this paper, we address these challenges by performing a large-scale comparison between different model types across a wider range of classification tasks and domains."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setting",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "For our experiments we selected a suite of datasets representing all three classification types, i.e., binary, multiclass, and multilabel.\nThe datasets span across 7 domains and 13 classification tasks. Specifically, we selected the Twitter datasets from the SemEval 18 on emoji prediction Barbieri et al. (2018  ###reference_b3###), SemEval 18 on irony Detection Van Hee et al. (2018  ###reference_b57###), SemEval 19 on hate detection Basile et al. (2019  ###reference_b4###), SemEval 19 on offense detection Zampieri et al. (2019  ###reference_b68###), and SemEval 19 on sentiment analysis Nakov et al. (2019  ###reference_b35###). Further, we include datasets for topic categorisation such as BBC news111http://mlg.ucd.ie/datasets/bbc.html, AG News (Zhang et al., 2015  ###reference_b70###), Reuters (Lewis et al., 2004  ###reference_b28###), and 20 Newsgroups (Lang, 1995  ###reference_b26###) , as well as IMDB reviews dataset for polarity detection (Maas et al., 2011  ###reference_b32###), PCL dataset for patronising language detection (Perez Almendros et al., 2020  ###reference_b39###), and Toxic comments (Hosseini et al., 2017  ###reference_b22###). Additionally, we evaluate models for more specialised domains representing real world applications such as EU legislation documents (Chalkidis et al., 2019  ###reference_b7###) for legal legislation concepts detection, Hallmarks of cancer (Baker et al., 2015  ###reference_b1###) for detecting cancer hallmarks, Ohsumed (Joachims, 1998  ###reference_b23###) for cardiovascular diseases detection, and Safeguarding reports (Edwards et al., 2022  ###reference_b14###) for theme detection. Additionally, we perform prediction for the top classes as well as the sub-classes of the 20 Newsgroups and Safeguarding datasets. In this way, we can analyse how the models performance is affected by the number of classes. The main features and statistics of each dataset are summarized in Table 1  ###reference_###. For the EU legislation documents we have performed experiments with the 10 most frequent labels, similarly to Chalkidis et al. (2019  ###reference_b7###). For the Ohsumed dataset, we have selected the top 23 most frequent classes, similar to prior work Pilehvar et al. (2017  ###reference_b40###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Comparison Models",
            "text": "We compare three main types of models: generative language models, masked language models, and linear models, all described below.\nGenerative Language Models.\nWe include LLaMA 1 Touvron et al. (2023a  ###reference_b54###) and 2 Touvron et al. (2023b  ###reference_b55###) into the analysis as representatives of large auto-regressive generation models, both with 7 billion parameters. As a representative of smaller but instruction-tuned model, we use Flan-T5 Chung et al. (2022  ###reference_b9###). The model is fine-tuned using the Flan instruction tuning tasks collection (Chung et al., 2022  ###reference_b9###). We use the large Flan-T5 model with 780M parameters. We have also included T5 model Raffel et al. (2020  ###reference_b44###) into our analysis which we fine-tune, similarly to RoBERTa. In particular, we use T5 base model. We have downloaded the models from Hugging Face Wolf et al. (2019  ###reference_b65###).\nAs a representative of the GPT family of autoregressive models Brown et al. (2020  ###reference_b6###), we use OpenAI GPT 3.5-Turbo for our analysis. We added this model for completeness. However, given budget constraints and its closed nature for which few conclusions can be drawn, we only provide results for a sample of all datasets.\nMasked Language Models. As a representative of masked language model, we use RoBERTa Liu et al. (2019  ###reference_b30###), pre-trained on English language. It is known to achieve state-of-the-art results for many text classification tasks. We perform experiments with RobERTa base (125 million parameters) and RoBERTa large (354 million parameters) models to allow analysis into the effect of model size over the classification performance. We have downloaded the models from Hugging Face Wolf et al. (2019  ###reference_b65###).\nLinear Models. Finally, we use FastText Joulin et al. (2017  ###reference_b24###) (see Section 2.1.1  ###reference_.SSS1###) as a representative of a linear text classification model. Despite its simplicity the model provides a strong baseline for many text classification tasks and it is known to give comparable results to state-of-the-art methods, including language models such as BERT for some classification problems Zhou (2020  ###reference_b73###); Edwards et al. (2020  ###reference_b13###)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Prompting, Training and Evaluation",
            "text": "As mentioned in Section 1  ###reference_###, our aim is to estimate how well the text generation models perform for text classification when\ncompared to the more data consuming models such as RoBERTa and FastText. Therefore, we perform experiments for Flan-T5 and LLaMA in zero- and one- shot ICL settings. For zero shot, we provide information about the task to the model through prompting. For one shot, we randomly select a single training instance per label and we provide these examples along with the instruction to the model. To ensure robustness, the random selection of training samples is performed for three iterations and the results are averaged. For generating labels for the test sequences, we use default model settings. We judge the outputs as expected class labels or not by simply checking whether the output of the model matches one of the labels for the given classification task. We experiment with three different prompts which we describe further in Section 3.4  ###reference_###.\nAs for RoBERTa, we fine-tune it for the classification task on the training data of each dataset using a sequence classifier, a learning rate of 2e-5 and 4 epochs. In particular, we made use of RoBERTa’s Hugging Face default transformers implementation for classifying sentences Wolf et al. (2019  ###reference_b65###). As for T5, we fine-tune it using conditional generation, 2 epochs, and learning rate of 5e-5. Finally, we use FastText classifier with 25 epochs and softmax as the loss function.\nFinally, we report results based on standard micro and macro averaged F1 (Yang, 1999  ###reference_b66###)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Prompt Design",
            "text": "Our paper does not focus on identifying and describing most efficient prompt engineering practices (as majority of work described in Section 2  ###reference_###) but instead we focus on highlighting prompt-independent trends in the models performance in order to help outline advantages and disadvantages of out-of-the-box approaches for few shot text classification. We selected instructions that led to satisfactory results in previous research or have been used in the training set for the instruction-tuned models Flan-T5 Sun et al. (2023  ###reference_b53###); Wei et al. (2021  ###reference_b64###). These prompts vary in the detail they provide about the given task and domain. We want to analyse trends across models behaviour that are non-prompt sensitive as well as look at how the amount of specificity provided in the prompt affect the performance of the models. For these purposes, we use the following three prompts: (1) generic: a prompt which does not give information about the task or domain, used in Sun et al. (2023  ###reference_b53###); (2) task: describes the given task, i.e., classification; (3) domain: a prompt which gives more information about the domain, for instance, it specifies the type of test data, such as an article or tweet. We have created the domain-based prompts following examples provided in Wei et al. (2021  ###reference_b64###). Table 2  ###reference_### presents examples of the prompts per classification type222A list of all prompts is given in the Appendix.."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "The aim of our analysis is (1) identify if and how the use of prompts affect the performance of text generation models (see Section 4.1  ###reference_###); (2) compare performance of prompting and fine-tuning techniques in order to identify strengths and weaknesses of the different models – we focus on a comparison between the three types of classification, i.e., binary, multiclass, and multilabel (see Section 4.2  ###reference_###); and (3) perform a fine-grained analysis comparing models’ performance at the domain and dataset level (see Section  4.3  ###reference_###). In addition to this general comparison, we analyse separately the performance of closed-source GPT3.5 and models for the ‘IMDB reviews’ and ‘AG News’ datasets as they are used in the fine-tuning of the Flan-T5 model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Model and Prompt Analysis",
            "text": "A comparison between the two LLaMA models shows an advantage of LLaMA 2 over LLaMA 1 for both zero- and one-shot settings across all prompt types (see Figure 1  ###reference_### and Table 3  ###reference_###). The two models have similar performance in the zero-shot setting in terms of F1 score. However, the number of wrong labels for LLaMA 1 is much larger with 0.470 wrong labels compared to the 0.100 wrong labels from LLaMA2. Results in Figure 1  ###reference_### also show a clear advantage of Flan-T5 over the other models for all three prompts in terms of micro- and macro- F1 for both zero- and one- shot settings. The Flan-T5 model also leads to smaller number of wrong labels in zero-shot prompting. This suggests that smaller but instruction-tuned models can be more beneficial in zero- and few- shot classification in comparison to larger text generation models. Specifically, Flan-T5 has on average 0.110 improvement in micro- and macro-F1 for both zero- and one- shot settings over LLaMA 2.\n###figure_1### Further analysis into the prompts reveal that prompt choice does not lead to significant changes in the models behaviour where the deviation for the three prompts across all models is relatively small. For instance, for LLaMA 1 and LLaMA 2 is less than 0.02 difference in micro-F1 for both zero- and one- shot settings while for Flan-T5 it gradually decreases from 0.07 in zero-shot to 0.01 for one-shot. This suggests that smaller models such as Flan-T5 are more sensitive to the prompt in zero settings versus few shot learning. The benefits from one-shot prompting are evident across all three models where the F1 measure tends to increase and the number of wrong labels decreases. Flan-T5 improves its performance on a higher rate compared to to the other two models with around 0.047 increase in the micro-F1 score versus 0.027 increase for LLaMA 2. This illustrates the strong abilities of these models to learn tasks with minimal amount of training data."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Prompting versus Fine-tuning",
            "text": "Results in Figure 2  ###reference_### show the same trends for prompting methods where Flan-T5 outperforms LLaMA 1 and LLaMA 2 for all text classification types in terms of micro- and macro-F1. All three models improve their performance for one-shot prompting regarding the number of wrong labels. In one shot setting, Flan-T5 and LLaMA 2 tend to have close to 0 wrong labels with LLaMA 2 returning slightly lower number of irrelevant results, while Flan-T5 has a better F1 score (see Figures 2  ###reference_### and  3  ###reference_###333Macro-F1 results are available in the Appendix.). The advantage of LLaMA 2 over LLaMA 1 is clearly shown for all classification tasks, especially binary and multilabel where LLaMA 2 has a smaller number of irrelevant results and higher F1 score (see Figure 2  ###reference_###).\n###figure_2### ###figure_3### Regarding fine-tuning approaches, results in Figure 2  ###reference_### show a clear dominance of RoBERTa-large in one-shot setting for all classification types.\nWhen fine-tuning is performed using the entire dataset, T5 outperforms the rest of the models for binary classification with micro-F1 = 0.672 versus RoBERTa-large with micro-F1 = 0.607. However, for multiclass and multilabeling tasks, the performance of T5 decreases and the model is outperformed by both RoBERTa-base and RoBERTa-large. For instance, for multiclass problems RoBERTa-large achieves micro-F1 of 0.726 versus micro-F1 for T5 = 0.700. For multilabeling problems the performance gap between the models increases and RoBERTa-large has a micro-F1 = 0.788 versus T5 with micro-F1 = 0.718. These results suggest that fine-tuned masked language models are more suitable for complex classification tasks such as multiclass and multilabeling problems when the number of labels is higher versus fine-tuning text-to-text models such as T5.\nA comparison between prompting and fine-tuning techniques for low resource settings suggests a better performance of prompting for binary and multiclass problems (see Figure 2  ###reference_###) where Flan-T5 and LLaMA 2 outperform fine-tuning models by a significant margin. For instance, Flan-T5 has micro-F1 = 0.553 versus micro-F1 for RoBERTa-large with micro-F1 = 0.485 for binary classification in one shot settings. The advantage of prompting in one shot settings becomes even more evident for multiclass problems where Flan-T5 achieves micro-F1 = 0.489 versus RoBERTa-large with micro-F1 = 0.162. However, for multilabeling problems, fine-tuning approaches outperform prompting methods with a difference in micro-F1 of 0.082 between the best fine-tuned model, RoBERTa-large, and the best prompting model, i.e. Flan-T5. It is worth noting that during one-shot training, all models have been provided the same training examples. However, further analyses are needed to identify most efficient ways for representing multi-labeling problems as part of prompting techniques.\nDespite the better overall performance of prompting techniques in zero- and one- shot settings, these approaches lead to unsatisfactory performance when compared to fine-tuned masked language models on a larger training set. Further, the difference in the performance between the two techniques grows larger for more complex text classification tasks such as multiclass and multilabeling problems. For instance, for binary classification, the difference in performance in terms of micro-F1 between best performing prompting and fine-tuning technique is 0.119 while for multiclass the difference in performance is 0.240. This shows that large autoregressive text generation models coupled with few shot learning techniques still have room for improvement when it comes to text classification. Fine-tuned masked language models, despite being smaller, lead to better performance for text classification versus LLMs in ICL settings."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Trends across datasets and models",
            "text": "Results presented in Table 4  ###reference_### confirm findings from Section 4.2  ###reference_### showing a clear dominance of Flan-T5 over LLaMA for zero- and one-shot prompting for the majority of datasets. Exceptions are the ’irony’, ’sentiment’, and ’PCL’ datasets where LLaMA performs better for either zero or one shot setting, or both. For some datasets such as ‘hate’, prompting models give better performance in zero- shot than one-shot setting. However, models still improve performance for these datasets in terms of number of wrong labels. Further, the choice of one shot training instances can influence the performance of models in few-shot learning. For the purposes of this analysis we have selected the one shot examples randomly. Analysing the impact of the training examples in few-shot learning can be a future research direction which we leave for future work.\nIn contrast to the prompting approaches, results for the fine-tuned models do not show a clear dominance of either RoBERTa or T5. T5 shows a better performance for the majority of the binary classification tasks (those associated with Twitter datasets) as well as the datasets ‘AG news’, ‘20 News’ (top 6 classes)’, and the ‘legal’ domain. The two models attain a similar macro-F1 for the emoji prediction and safeguarding reports datasets.\nImpact of the number of labels.\nAnalysis into the effect of the number of classification labels in the performance shows an interesting trend with the fine-tuned models (RoBERTa and T5) performing slightly better for classification tasks with 6 to 9 labels than classification with less labels (see Figure 4  ###reference_###). For RoBERTa this trend occurs for both micro-F1 and macro-F1 while for T5 it appears only for micro-F1. This can be attributed to the nature of the binary classification tasks (’irony’, ’offense’, ’hate’) which express human emotions and represent the Twitter domain. This suggests that the models find it more challenging to categorise such texts versus more categorical-based datasets such as news and articles which are part of the datasets with 6 to 9 labels. In contrast, the performance of both prompting approaches decreases as the number of labels for the classification task increases.\n###figure_4### Datasets used for pre-training. As mentioned earlier in the section, we analyse the performance of models for the ‘IMDB reviews’ and ‘AG News’ datasets separately as they are used in the fine-tuning of the Flan-T5 model. For these datasets (see Table 5  ###reference_###) Flan-T5 performance significantly improves achieving micro- and macro-F1 results comparable to fine-tuning models on the entire dataset. For instance, for the IMDB dataset, the difference in macro-F1 between Flan-T5 and RoBERTa is 0.007 while for the AG news the difference in macro-F1 is 0.027. In contrast, the performance gap for the rest of the datasets between Flan-T5 and the best performing fine-tuning model is on average around 0.250 in micro-F1. This shows the significant impact that data contamination may have in the final results. However, a careful data contamination analysis becomes harder on large models for which training data is not available, and especially for closed models.\nGPT Analysis. Table 4  ###reference_### presents zero-shot prompting results for the GPT 3.5-Turbo model for the following datasets: ‘irony’, ‘offense’, ‘bbc’, ‘reuters’, ‘pcl’, and ‘safeguard’. We have used the class-based prompt for prompting with GPT 3.5 because it has shown to lead to the higher overall performance for Flan-T5 and LLaMA. Results show a clear advantage of the GPT-based model over Flan-T5 and LLaMA achieving on average 0.350 higher micro- and macro-F1 across the majority of the datasets, except for the ‘PCL’ dataset. Additionally, results achieved with zero-shot learning with GPT 3.5-Turbo outperform fine-tuned models on the entire dataset for the ‘irony’ dataset. However, for the rest of the datasets the model is still outperformed by fine-tuning approaches confirming the lack of generalisation abilities of few-shot learning techniques and text generation models for text classification."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This paper presents a large-scale study on how prompt-based LLMs in zero- and one- shot settings compare to smaller but fine-tuned language models for text classification. The evaluation spans across 16 datasets covering binary, multiclass, and multilabel problems. In particular, we compared three different types of models, i.e., linear models such as FastText, masked language models (RoBERTa), and text generation models tested in ICL settings (T5, Flan-T5, and LLaMA, as well as GPT 3.5-Turbo). Analyses on prompting techniques showed a clear advantage of the Flan-T5 model over LLaMA 1 and LLaMA 2 regardless of the prompt used for both zero- and one-shot settings. This shows that smaller but instruction-tuned models have better generalisation abilities for text classification than larger text generation models. Further, our analysis showed that results from zero- and few-shot learning LLMs are considerably lower in comparison to smaller models fine-tuned on the entire training set. This highlights the need for training data, even in the age of LLMs, and that fine-tuning smaller and more efficient language models can still outperform in-context learning methods of larger text generation models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "Aleksandra Edwards and Jose Camacho-Collados are supported by a UKRI Future Leaders Fellowship.\nThe safeguarding documents used for performing analysis in the paper have been collected in collaboration with the Wales Safeguarding Repository (WSR) project, funded by the National Independent Safeguarding Board (NISB), the Crime and Security Research Institute at Cardiff University (CSRI), and the School of Social Sciences at Cardiff University (SOCSI). We would like to thank the WSR team for their support."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "The main limitation of this research is the lack of experiments on fine-tuning Flan-T5 and LLaMA models as well as the lack of further analysis with larger text generation models such as LLaMA with 13 and 17 billion parameters. Moreover, the paper presents a study for zero- and one-shot prompting. As future work, we plan to extend analysis to understand how the number of training instances affect the performance of in-context learning approaches. Further, considering the sensitivity of in-context learning approaches to the given instructions, it would be beneficial to perform further analysis on a larger more diverse set of prompts. Finally, the paper presents results for a single high resource language (English). Experiments for other languages (especially low-resource) could show a different tendency."
        }
    ]
}