{
    "title": "AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval",
    "abstract": "In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage. Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage. Our method achieves 10 MB memory usage in query search even with billion-scale datasets with minor performance degradation. AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG). This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "In tasks of retrieving specific data that users are interested in from large-scale datasets such as images, music, and documents, Approximate Nearest Neighbor Search (ANNS), which gets close vectors to query vectors using pre-built indices constructed from the datasets, is commonly utilized. Usually, the index data is stored in fast memories (RAM) to shorten the search latency. However, the DRAM costs too much to store large datasets, especially those that reach a billion-scale, and slower but cheaper storages such as SSDs also need to be employed.\nIn the recent trend of Large Language Models (LLM), possible implementations of Retrieval-Augmented Generation (RAG)(lewis2020retrieval,  ###reference_b7###) use vector datasets from different domains as their external knowledge sources to generate more accurate answers, which need ANNS methods to retrieve information from those sources. For further improvement of LLMs with RAG, ANNS methods need to switch between multiple indices of different domains corresponding to users’ or applications’ requests. This requires the index data of all sources to be retained in RAM during the service time or loaded from storage every time of a request.\nDiskANN(jayaram2019diskann,  ###reference_b5###) is the de-facto standard of graph-based ANNS methods exploiting storages. This is adopted for the baseline of Big-ANN competion track of NeurIPS(bigannbench2023,  ###reference_b10###) and is employed in vector databese services such as Weaviate(weaviate-github,  ###reference_b3###) and Zilliz (milvus2021wang,  ###reference_b13###). DiskANN claims to acheive query search of billion-scale datasets with high recall and small memory usage. It reduces memory usage by compressing node vectors using Product Quantization(PQ) (jegou2010pq,  ###reference_b6###) and keeps high search recall by re-ranking all nodes in the search path by their full-precison vectors loaded from the storage. However, since DiskANN retains PQ-compressed vectors of all nodes in RAM, the memory usage is proportional to the scale of the datasets.\nAdditionally, increasing compression ratio reduces both of memory usage and recall, which leads to the trade-off between them.\nThis means DiskANN is not truly scalable to the scale of datasets and is not suitable for switching between multiple indices because loading all PQ vectors onto DRAM can take too long time.\nIn this paper, we propose AiSAQ: All-in-Storage ANNS with Product Quantization, a novel method of index data placement. Our implementation achieved only 10 MB of DRAM usage regardless of the scale of datasets. Our optimization for data placement in the storage index brings minor latency degradation compared to DiskANN, despite larger portion of the index are sitting in the storage. Moreover, index switch between multiple billion-scale datasets can be performed in millisecond-order.\nThe main contributions of this paper are follows:\nAiSAQ query search requires only 10 MB of RAM for any size of dataset including SIFT1B billion-scale dataset, without changing graph topology and high recall of original DiskANN.\nThanks to data placement optimization on SSD, AiSAQ achieves millisecond-order latency for 95% 1-recall@1 with tiny memory usage.\nAiSAQ achieved negligible index load time before query search. Applications can flexibly switch between multiple billion-scale corpus corresponding to users’ requests.\nIndex switch between different datasets in the same vector space can further reduce the switching time to sub-millisecond order, due to sharing their PQ centroid vectors.\nAs a concurrent work, LM-DiskANN(pan2023lm,  ###reference_b8###) was published just before this paper’s submission. However, our work proves the effectiveness on billion-scale datasets and proposes a new metric called index switching time, both of which are not stated in their paper."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Graph-Based ANNS algorithm",
            "text": "In a dataset  containing  points of -dimensional vector  and a given query vector , nearest neighbor search (NNS) is a task to compute the nearest neighbor vector  in a distance metric . Being a brute-force computation, NNS takes  time complexity and is not optimal for large-scale datasets.\nTo reduce the query search time, methods to approximately compute nearest neighbor vectors are proposed, which are called approximate nearest neighbor search (ANNS).\nAmong various approaches of ANNS, a graph-based method uses directed graph as an index. In the index construction phase, the algorithm regards the dataset vectors  as a node  and makes some edges between nodes based on their distances. In the query search phase, the algorithm starts from the entry point and move along the edges, which navigates to the candidate point of the nearest neighbor  of the query . These graph-based ANNS methods empirically acheive  time complexity(arya1993approximate,  ###reference_b2###). Some ANNS algorithms uses compressed vectors like PQ ones instead of full-precision vectors to reduce memory usage and computational cost by compromising precise distance calculation."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. DiskANN",
            "text": "DiskANN(jayaram2019diskann,  ###reference_b5###), one of graph-based ANNS methods, constructs a flat directed graph index by an algorithm called Vamana. In the query search phase, we traverse the graph according to a search algorithm called beam search derived from greedy search.\nBeam search algorithm of DiskANN updates top- candidates in every transition and sorts candidates by full-precision distances after graph transition.\nAlgorithm 1  ###reference_### shows detailed procedure of beam search used in DiskANN. The PQ compressed vector is stored on DRAM and is used for search path determination. On the other hand, the full-precision vector and IDs of outneighbors  of  is written in the continuous LBA (Logical Block Address) space on the storage like SSD (Figure 1  ###reference_###). In this paper, we call this information chunk of a specific node  “node chunk” of . In each graph hop, DiskANN reads the node chunk of  and determines the next search path comparing PQ distance between query and each of , looking up PQ vectors retained on the DRAM. After the graph hops finished, DiskANN sorts the candidate nodes in the search path by their full-precision distances, which is called re-ranking.\n###figure_1### With  bytes of each full-precision vector, max outdegree ,\nand  bytes (usually 4 bytes) to express a node ID or the outdegree, the size of a single node chunk is . Operation systems dispatches I/O requests to storage devices in “blocks”, whose size is  in most OS settings. In most cases,  stands and a block contains a single or multiple node chunk(s) in this case (Figure 1  ###reference_### (a)). If , the chunk uses muitiple blocks (Figure 1  ###reference_### (b)). In both cases, node chunk which doesn’t fit in the rest space after previous node chunk in a block is aligned to the start of the next block.\nThe query search of DiskANN dispatches I/O requests to read  block(s) used by a single node chunk. For example, assuming thar block size is 4 KB, the I/O size is 4 KB in Figure 1  ###reference_### (a) and 8 KB in Figure 1  ###reference_### (b)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Drawbacks of DiskANN",
            "text": "Memory Usage.\nDiskANN loads PQ vectors of all nodes in its dataset. This means memory usage of DiskANN is roughly proportional to the scale of the dataset . For instance, an index constructed from SIFT1B dataset with 32 byte per PQ vector, which corresponds to  compression of the original vector, requires 32 GB of RAM.\nIncreasing the PQ compression ratio brings recall degradation even with re-ranking, and the memory usage is still proportional to .\nSpatial Efficiency.\nIn the DiskANN beam search for a query, the number of nodes used for distance computation is much smaller than the dataset scale . Even for multiple queries, few nodes are accessed frequently, while most nodes are unused and just occupy a large amount of RAM.\nIndex Switch Time.\nConventional graph-based ANNS methods including DiskANN cost time to load vector data to DRAM before the query search starts, which also increases together with . To realize index switch, using muitiple large-scale datasets and switching the corpus corresponding to applications’ requests, conventional methods need to keep all vector data of multiple datasets in DRAM to avoid the loading time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Proposed Method",
            "text": "To address the above problems, we propose a novel method named AiSAQ: All-in-Storage ANNS with Product Quantization which offloads PQ vector data from DRAM to storage and aims almost zero memory usage with negligible latency degradation.\n###figure_2### AiSAQ obtains PQ vectors of outneighbors from its node chunk, while DiskANN looks up the memory."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Methodology",
            "text": "When the DiskANN beam search is located in a node , the PQ vectors of  correspond to their IDs stored in the node chunk of . This means these PQ vectors themselves can also be stored in the node chunk, which is the main idea of AiSAQ (Figure 2  ###reference_###).\nAiSAQ also needs to adjust the node chunks which has larger size than DiskANN to fill blocks effectively and to minimize the I/O latency increment. With  bytes of each PQ vector, the node chunk size of AiSAQ is . It is recommended to adjust the maximum degree  so that\n or \nstands.\nIn the query search phase, PQ vectors of  are obtained from the node chunk of  in the storage instead of memory and used to compute PQ distances to the query.\nUtilizing unused area of blocks in DiskANN by filling with PQ vectors,\nthis needs no additional I/O requests for getting PQ vectors and enables distance computations of each hop within the node chunk without DRAM.\nThe PQ vectors can be discarded after distance computations in each hop.\nSince the PQ vector of a specific node is obtained in the previous hop, the above algorithm can’t get PQ vectors of the entrypoints. Therefore, we keep only PQ vectors of  (1 in most cases) entrypoints in DRAM. In this method, we have to keep at most  PQ vectors in DRAM at once, which is independent of and much smaller than , and can aim near-zero memory usage."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Implementation",
            "text": "We implemented AiSAQ index creation algorithm based on the existing DiskANN programs (diskann-github,  ###reference_b11###) and its query search algorithm from scratch, only reusing some utilities from DiskANN. From Vamana graph and PQ vectors generated by DiskANN, a single AiSAQ index file is generated. Our implementation prioritized to lower the memory footprint, which does not support additional features of DiskANN such as filtering(gollapudi2023filtered,  ###reference_b4###) and dynamic indexing(singh2021freshdiskann,  ###reference_b12###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Evaluation",
            "text": "In this section, we conducted some query search experiments using AiSAQ index files and compared with DiskANN results in the same construct and search conditions."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Memory Usage",
            "text": "We measured peak memory usage of DiskANN and AiSAQ query search by /usr/bin/time command. Since both of the algorithms load the entire query files onto DRAM and that portion is not essential for algorithm comparison itself, measurement of memory usage was executed only for 10 queries without groundtruth data in each dataset. Table 2  ###reference_### shows the memory usage of DiskANN and AiSAQ given the same index construction and search parameters for each dataset. While DiskANN required up to 32 GB of RAM in SIFT1B dataset, AiSAQ used at most 14 MB of RAM even for billion-scale query search.\nThe breakdown of AiSAQ’s memory usage inspected with some commands like size and htop revealed that 300 KB of memory out of the entire 11 MB is used for the search program itself and statically allocated variables, and 5.8 MB for shared libraries. Since we reuse some of DiskANN’s utilities for current AiSAQ implementation, better implementation of AiSAQ beam search may further reduce memory usage."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Query Search Time",
            "text": "###figure_3### ###figure_4### ###figure_5### AiSAQ’s query search latencies outperforms DiskANN, while recall@1 is identical.\n###figure_6### Figure 4  ###reference_### shows recall-vs-latency plots of DiskANN and AiSAQ in each dataset. Since AiSAQ does not change the graph topology itself, recall@1 is identical to DiskANN in the same search condition. In some datasets like SIFT1M and KILT E5 22M, AiSAQ needs more blocks for a node chunk than DiskANN and the I/O request size for query search also becomes larger. Such datasets lead to I/O latency degradation compared with the same search conditions of DiskANN.\nThanks to the I/O queueing system of SSDs and our optimization with shorter time for distance computation in CPU, however, the latency degradation is not critical.\nOn the other hand, AiSAQ performs faster on SIFT1B because the I/O request sizes are the same .\nWe also conducted query search experiments for indices with various PQ vector sizes  to change the memory usage of DiskANN search. Figure 4  ###reference_### shows search latencies of DiskANN and AiSAQ search to achieve ¿ 95% recall@1 of each  by memory usage. DiskANN searches configured to consume less RAM require slower latencies with large search list size  to ensure high recall@1. In contrast to DiskANN’s memory-latency tradeoff, AiSAQ keeps the same small memory usage regardless of PQ vector size , which means AiSAQ can keep both of near-zero memory usage and millisecond-order search latencies in the high-recall area."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Index Switch",
            "text": "Table 3  ###reference_### shows the load time of DiskANN and AiSAQ indices before query search starts. While DiskANN’s load time increases with the dataset scale, AiSAQ keeps constantly short time since there is no need to load PQ vectors of the entire datasets. The load time of AiSAQ indices itself is the same order as its query search latencies. Higher-dimensional PQ centroids of KILT E5 22M brings longer time than the other datasets.\nIn applications of LLMs with RAG, it is possible that multiple datasets share the vector space (for example, images and documents are encoded to the same space by each encoder). In such situations, datasets are also able to share their PQ centroids. To obtain multiple datasets which can share the centroids, we devided KILT E5 22M dataset into 10 subsets, employing the PQ centroids generated by the whole 22M dataset. Then we built an index from each subset and searched across these indices with or without reloading the PQ centroids file. From table 4  ###reference_###, sharing the PQ centroids, which only needs to load 4 KB metadata for an index, significantly reduced index switch time even compared to AiSAQ with reloading. This method is especially effective for high-dimensional datasets like KILT E5."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusion and Future Works",
            "text": "In this paper, we proposed AiSAQ, a novel method of ANNS data offloading to storage. Our method achieved billion-scale ANNS with only 10 MB memory usage without significant degradation of performance. AiSAQ will be applicable to all graph-based ANNS algorithms,\nincluding future higher-spec algorithms.\nIn addition, reducing the index load time before query search enabled index switch between multiple large-scale dataset for various users’ requests. That will enable LLMs with RAG to employ more simple index addition or filter search algorithms."
        }
    ]
}