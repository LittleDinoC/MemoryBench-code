{
    "title": "Automating the Information Extraction from Semi-Structured Interview Transcripts",
    "abstract": "This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Qualitative methods such as interviews or focus groups with customers are an integral part of the research arsenal in a number of fields: marketing, social science, and medical studies (Avjyan, 2005  ###reference_b7###; Leeson et al., 2019  ###reference_b10###). This approach differs significantly from quantitative techniques in its ability to draw on individual experiences and delve deeper into the issue under study. However, unlike the results of quantitative surveys, in interviews, there is no ready-made information, no statistics, and no clear answers to the questions posed. The researcher unwittingly faces the problem of interpretational objectivity, and the question arises as to how to tackle it.\nThe general analysis of collected data in interviews mainly uses open coding technology (Fig 1  ###reference_###), which involves repeatedly reading the text to identify ”codes” that are in essence important thoughts, ideas, attitudes, and subjects. Further, the axial coding procedure is applied, where the relationships between the codes and their aggregation into higher-level categories are found (Saldana, 2016  ###reference_b15###). Several other coding methods are present and all of them involve independent work with the text, consisting of re-reading and finding the key thoughts of the informant in a large number of documents. This process often takes several weeks (Alshenqeeti, 2014  ###reference_b6###). Hence, it can be seen that this procedure requires a lot of human effort to process the text by oneself. So the research issue of implementing automatization of the whole process or pre-processing of the text corpus to facilitate the subsequent analysis appears.\n###figure_1### That’s why the main goal of current research is to automate the analysis of qualitative research results and elaborate the appropriate software that will help organizations and researchers dealing with large clients’ text corpora. To begin with, it is necessary to consider the existing solutions on the market and describe how the future service will differ."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Current coding practices",
            "text": "Each statement or significant segment of dialogue within an interview is assigned a ’code’ that summarizes its main idea. Codes are not just words, phrases, sentences, or thoughts but represent a unit of meaning that encapsulates key aspects of the data (Miles and Huberman, 1994  ###reference_b12###). Once coded, these segments are then organized into broader categories that reflect the underlying patterns and relationships within the dataset (Glaser and Strauss, 2017  ###reference_b9###). In practice categories and codes consist of one or two words to encapsulate the main meaning of a citation. However if the main thought of a sentence can be described only in a phrase, that is also allowed.\nDescribing the process in simpler terms, first, we summarize the main idea of each citation in the interview(Ryan and Bernard, 2003  ###reference_b14###). Then, we start grouping them into bigger categories. This means looking at all the little ideas we’ve found and seeing how they fit together into larger themes. We ask questions like, ”Do these codes share something in common?” or ”Are they talking about the same bigger idea?” This helps us organize our findings better.\nThese categories serve as the pillars for constructing a conceptual framework (Lochmiller, 2021  ###reference_b11###), which researchers often visualize in the form of a graph. Such a graph, akin to a mind map, interlinks individual responses, highlighting the associations and hierarchies amongst different thematic codes. This visualization assists in better understanding the collective narrative of the participants. Thus, the coding process is a critical interpretive phase in qualitative research, helping in developing conclusions and theoretical insights."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Overview of methods and tools",
            "text": "In general, researchers use software to facilitate the coding of interviews as follows: the text is conveniently placed on the screen with the possibility of highlighting parts of the sentence with a color marker and tagging them with codes, the number of which then counts itself (Atl, 2024  ###reference_b3###; MAX, 2024  ###reference_b5###). These programs also make space for drawing diagrams with codes and connecting them with arrows and written relationships. Thus, the software does not replace the analysis process as such but simply puts the researcher in a comfortable environment for the same workflow.\nWhen it comes to programs aimed at replacing some of the work some softwares replace part of the work by giving analytical tools such as word statistics and word cloud (Ded, 2024  ###reference_b4###). It is worth noting that all of these programs are paid, so not all researchers are inclined to use them. For example, in one of the works already described, coding was done in MS Word and Excel (Leeson et al., 2019  ###reference_b10###). Some programs focus on text processing in general, with clustering and collocation search capabilities (Ant, 2024  ###reference_b2###). However, the format of interview analysis is very specific, as it requires the building of models based on a set of answers to one question from several informants. Therefore, it is difficult to use such general-purpose software for the analysis of transcripts.\nThus, the purpose of this work is to develop a method for analyzing transcripts of qualitative research results, as well as to write user-friendly software that can be used by researchers who do not know the skills of programming."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Existing softwares",
            "text": "In general, researchers use software to facilitate the coding of interviews as follows: the text is conveniently placed on the screen with the possibility of highlighting parts of the sentence with a color marker and tagging them with codes, the number of which then counts itself (Atl, 2024  ###reference_b3###  ###reference_b3###; MAX, 2024  ###reference_b5###  ###reference_b5###). These programs also make space for drawing diagrams with codes and connecting them with arrows and written relationships. Thus, the software does not replace the analysis process as such but simply puts the researcher in a comfortable environment for the same workflow.\nWhen it comes to programs aimed at replacing some of the work some softwares replace part of the work by giving analytical tools such as word statistics and word cloud (Ded, 2024  ###reference_b4###  ###reference_b4###). It is worth noting that all of these programs are paid, so not all researchers are inclined to use them. For example, in one of the works already described, coding was done in MS Word and Excel (Leeson et al., 2019  ###reference_b10###  ###reference_b10###). Some programs focus on text processing in general, with clustering and collocation search capabilities (Ant, 2024  ###reference_b2###  ###reference_b2###). However, the format of interview analysis is very specific, as it requires the building of models based on a set of answers to one question from several informants. Therefore, it is difficult to use such general-purpose software for the analysis of transcripts.\nThus, the purpose of this work is to develop a method for analyzing transcripts of qualitative research results, as well as to write user-friendly software that can be used by researchers who do not know the skills of programming."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Research design",
            "text": "In this paper, several methods will be applied and compared with each other, and finally, the most appropriate method for analyzing qualitative research transcripts will be chosen. To achieve the goal, it is necessary to perform the following tasks:\nCompare different methods of topic modeling and choose the best one for semi-structured interview transcripts\nCreate a framework for visualizing selected keywords from transcripts\nWrite a frontend understandable to researchers\nThis paper will compare various methods to find the most suitable one for analyzing interview transcripts. Typically, topic modeling (Blei, 2012  ###reference_b8###) is used for either large texts with multiple topics or very short texts like tweets or reviews. Semi-structured interviews present a unique challenge as they include characteristics of both: they are short like tweets but can contain rich narratives similar to longer documents, making standard topic modeling approaches less effective.\nBefore constructing topic models, the necessary preprocessing of the natural text is done. First, the sentences were tokenized and lemmatized. Stop words, which are included by default were also removed. However, in addition to this, a complementary set of stop words was created, which was compiled independently after building the frequency tables of the tokens. The point is that many undesirable words occurring in interviews could be merged into one topic (this happened in one of the LDA models). Such words as ” probably, it turns out, in general, supposedly, like” were removed, as well as some verbs that refer to the process of reflection ”think, suppose”.\nFirst, it is possible to combine the answers to the questions in one large document and build topic models based on the combined answers to one question. Or it is possible to use all of the transcripts and build one large model. In this work, both methods were tested on one set of interviews and it was concluded that the topics obtained by answering one question gave unsatisfactory codes (denoted as W).\nWe can immediately notice in Table 1  ###reference_### that all the received topics are very similar. This could have been foreseen since more often than not one question is about one specific topic. Therefore, it was decided to abandon this method of compiling documents right away. Thus, future model development was conducted on the aggregate of all documents."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Experiments",
            "text": "The initial approach utilized the standard LDA package from Gensim to establish a baseline for topic modeling of interview transcripts. The best-performing LDA model achieved moderate quality metrics but faced challenges in clearly distinguishing differentiated topics.\nThis experiment combined LDA with BERT embeddings to enhance contextual understanding of topics. Initial language for interview texts is Russian, which is why the Russian language BERT model from Deep Pavlov and an autoencoder for dimensionality reduction were used. For the English version of interviews, the BERT base uncased model was used. Clustering was done using K-means, but the model struggled with topic clarity and overall quality.\nThe Top2Vec method was implemented using the same BERT model to get embeddings. UMAP was employed for dimensionality reduction, followed by clustering with HDBSCAN. This approach yielded more interpretable topics. Consequently, the BERT+UMAP+HDBSCAN algorithm was selected as the most suitable for semi-structured interviews, despite its longer processing time.\nAcross various sets of interviews, the BERT+HDBSCAN model consistently showed high topic diversity and better interpretability. However, for the prototype, model tuning was omitted in the frontend due to efficiency and time considerations. The model comparison for 10 topics and 2 sets of interviews is demonstrated in Table2  ###reference_### and Figure2  ###reference_###.\n###figure_2### Note: C_v measures topic coherence; Umass and UCI are coherence scores; NPMI is normalized pointwise mutual information; Topic diversity indicates the uniqueness of topics; Silhouette measures cluster separation; DBCV is density-based clustering validation."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. The prototype",
            "text": "The prototype was based on completed objectives. Firstly, the researcher uploads a\ndocument with all the interviews to the website. Then he has a choice to either save or dismiss the Interviewer’s phrases. This heavily depends on the researcher’s perspective and type of interview. Then he has to press the lemmatize button and preprocess. Shortly after that, the program will give him the most frequently used words in the text, which he can load into the file ”additional stop words” if he wishes and add to the website again111https://github.com/Likich/TM_graph  ###reference_###.\nNext, he can choose several suggested methods for analyzing the interview. LDA is the classic method and is fast, while BERT gives higher-quality results but takes more time. The user can also choose how many topics he would like to see in the result. Next, the researcher will be provided with an interactive graph of connections (Figure 3  ###reference_###). At the moment it is only available in English and Russian languages, but in the future, there will be support for more languages.\n###figure_3### Having the set of Topics: , the graph is constructed as follows: each vertex of the graph is a keyword that belongs to the topic : . For each keyword in the topic we have its weight that demonstrates the importance of a word in describing the topic : .\nThus, the graph is constructed as follows:\nIn the middle is the keyword with the highest weight. The question arises as to whether the word with the highest weight characterizes the entire topic. The answer is no, but in practice, the word with the most weight in the topic is less likely to occur in other topics and is less likely to affect the quality of visualization. Moreover, using for example three keywords will not promise that they play a key role in determining the semantics of the topic, and visualization will be even more difficult.\nIf a researcher is interested in knowing what citations mentioned this exact word, he can double-click the vertex of the network and citations will be shown. This phase is important to facilitate the interpretation of topics.\nAccording to this visualization, you can also see which topics are linked by co-occurrence and which are not. For example, in a series of interviews about social expectations, it was seen how the central keyword in the family topic, ”parent,” was related to ”education” as the central word in the education and work topic, and to ”shame,” the central word in the emotion topic. In this way, the researcher can understand which codes affect which, and which tops are completely disconnected and represent a separate topic. Based on these results, we can already hypothesize about the influence of indicators on others and prepare the methodology for the quantitative phase of the study."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Empirical perspective",
            "text": "Automating the coding process in qualitative research can be an advantage for various fields such as market research, customer feedback analysis, and clinical data analysis. These areas often deal with vast amounts of unstructured data like interviews, focus groups, and open-ended survey responses, where traditional manual coding is time-consuming and subject to human biases.\nIn market research automated coding can analyze customer interviews and group discussions faster, helping businesses find new trends and customer preferences more quickly. As for customer feedback analysis, automated coding can process customer feedback from various channels (social media, customer surveys, etc.) in real-time, enabling companies to respond promptly to customer needs and complaints. Analyzing qualitative feedback at scale allows for more personalized marketing strategies based on nuanced customer preferences and experiences. Creating a concept network from qualitative feedback can help in creating detailed customer journey maps, identifying pain points, and enhancing customer experience.\nIn healthcare, automated coding of patient interviews and feedback can provide insights into patient experiences, leading to improved care and treatment strategies reducing the time for data analysis, and accelerating research outcomes. Analysis of patient narratives and feedback can reveal insights into the efficacy of treatments and patient feedback, aiding in the improvement of therapeutic approaches."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8. Conclusion",
            "text": "The purpose of this study was to automate the analysis of semi-structured interviews, which is currently very time-consuming for individual researchers. It was also intended to write an application that would allow qualitative researchers with no knowledge of programming skills to use automatic text-processing methods. To achieve the goal this paper considered methods of coding texts, and considered the method of topic modeling, which identifies topics from the text with their keywords. Several methods of topic modeling were compared according to objective metrics and the most suitable one for semi-structured interview texts was selected.\nThe results showed that the best method is to convert the text into embeddings using BERT, then reduce the dimensionality of the resulting vectors using UMAP and clustering with HDBSCAN, followed by an algorithm to reduce the number of topics. For a basic assessment of model quality, it was decided to use the Topic Diversity metric, which will be important in constructing a pleasing and distinguishable visualization. After examining the front end, it became clear that with a high score on this metric, the graph is not cluttered with a large number of connections.\nThe theoretical significance of this work consists of testing more advanced and costly methods of interview analysis, whereas previous works mainly took the basic LDA, which proved to be the worst compared to methods using transformer architecture. The main practical contribution is that researchers in qualitative studies now will have access to automatic analysis of their work, or a convenient basis for subsequent analysis. It cannot be argued that such work completely replaces the individual researcher, who, firstly, is more familiar with the topic, and secondly, can analyze the truthfulness of the answers. However, automated analysis frees the researcher from his or her subjectivity and can help avoid judgmental attitudes."
        }
    ]
}