{
    "title": "SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations",
    "abstract": "There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used\ndata augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning ( 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively. SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed speech compared to SPIN.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Self-supervised learning (SSL) based pre-trained speech models such as HuBERT [1  ###reference_b1###], WavLM [2  ###reference_b2###] are becoming popular for their state-of-the-art performance on almost all speech applications. These models extract latent features that capture underlying factors of speech, such as acoustic-phonetic information, speaker information, semantic information, and more [3  ###reference_b3###]. These pre-trained representations are then fine-tuned for downstream application with labelled data. However, pre-trained SSL speech models may not be ideal for downstream tasks that do not align with the pre-trained objective (for example, handling overlapping speech [2  ###reference_b2###]). One way to overcome this issue is to introduce a pre-training objective that relates to the downstream task, such as training with overlapping speech in WavLM [2  ###reference_b2###]. However, this approach requires substantial amount of compute cost as the model is pre-trained from scratch. Another alternative falls within the realm of unsupervised or self-supervised fine-tuning (SSFT)[4  ###reference_b4###]. SSFT is applied on top of pre-trained models to learn task-specific representations. Then the SSL models are fine-tuned with labelled data on the downstream tasks for robust performance. For example, ContentVec [5  ###reference_b5###] employs content preserving strategies (by\ndisentangling speakers) on top of pre-trained HuBERT model to learn content-specific representations. However, ContentVec is not very cost-effective as it requires 19 hrs on 36 GPUs on top of the pre-trained HuBERT [1  ###reference_b1###] model. Another recent SSFT approach for content-related downstream task is speaker-invariant clustering (SPIN) [4  ###reference_b4###], which requires a compute cost less than 1% of ContentVec. SPIN employs speaker invariant clustering to improve content representations. The term SSFT was proposed in [4  ###reference_b4###] to distinguish fine-tuning methods using only audio [5  ###reference_b5###, 6  ###reference_b6###] from supervised fine-tuning using labelled data [7  ###reference_b7###].\n###figure_1### In this work, a simple and cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning is proposed to preserve content. Correspondence training [8  ###reference_b8###] is the task of learning similar representations from two different instances of the same spoken content. This technique has been successfully applied to extract high quality acoustic word embeddings (AWEs), where an auto-encoder takes input as a spoken word and the target output as the same word spoken by a different speaker [8  ###reference_b8###, 9  ###reference_b9###]. This technique ensures that the encoder learns only content and forget other unnecessary information such as speaker, duration, prosody, etc. Taking inspiration from this, for SCORE fine-tuning, a perturbed speech is generated from the original speech in such a way that the spoken content is preserved. Perturbed speech utterances are generated through the application of commonly employed data augmentation techniques in automatic speech recognition, such as speed perturbation [10  ###reference_b10###] and pitch shifting. After obtaining perturbed speech, the objective is to learn similar speech representations from both the original speech and perturbed speech, making the representations pitch and duration invariant. Shifting pitch (fundamental frequency) alters the speaker information while keeping the content same. To match the representations from perturbed and original speech, soft-DTW [11  ###reference_b11###, 12  ###reference_b12###] is used as a loss function. Soft-DTW is a popular loss function for time-series data, and it has also been successfully used for the multi-pitch estimation task in music information retrieval [13  ###reference_b13###].\nThe proposed method is tested on three content-related downstream tasks on SUPERB benchmark [14  ###reference_b14###]: automatic speech recognition (ASR), phoneme recognition (PR), and query-by-example spoken term discovery (QbE). The results are compared against the performance of vanilla models (HuBERT and WavLM) and recently proposed content-preserving SSFT methods such as ContentVec and SPIN for SUPERB benchmark .\nThe main contributions of this work are as follows:\nA novel cost-effective self-supervised fine-tuning method named SCORE is proposed to improve the content representations.\nWith just less than 5 hours of SCORE fine-tuning on a single V100 GPU, SCORE fine-tuned models outperform vanilla HuBERT and WavLM on the SUPERB benchmark for content-related tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Fig. 1  ###reference_### demonstrates the proposed SCORE fine-tuning method. SCORE involves two instances of the pre-trained model, one with frozen parameters () and the other with learnable top layers (), both having same initial model weights. Top layers are chosen for fine-tuning, as they encode phonetic content for most of the SSL models [15  ###reference_b15###, 16  ###reference_b16###]. More implementation details are described in Sec. 3  ###reference_###. The input to the SCORE is a pair of perturbed speech and original speech, randomly fed to either  or , as shown in Fig. 1  ###reference_###. Randomizing the input ensures that the model  does not exclusively focus on the characteristics of perturbed speech, and found to be crucial to observe the benefits of the proposed method. To obtain the perturbed speech, data augmentations used in ASR [10  ###reference_b10###] are employed, such as speech perturbation and pitch shift. Torchaudio [17  ###reference_b17###] is used for these perturbations, with SpeedPerturbation and PitchShift functions under torchaudio.transforms111https://pytorch.org/audio/stable/transforms.html  ###reference_s.html###.\nThe obtained representations from both the models  and  are projected to a lower dimension with linear feedforward layers and L2-normalized. Obtained sequences from both models ( and ) are different in lengths due to the perturbations. Therefore, a dynamic time warping based differentiable loss function soft-Dynamic Time Warping (soft-DTW) 222https://github.com/Maghoumi/pytorch-softdtw-cuda  ###reference_w-cuda###[11  ###reference_b11###, 18  ###reference_b18###, 19  ###reference_b19###] is used to match sequences of unequal lengths (Eq. 1  ###reference_###). This learning framework ensures that the model learns the speed and pitch invariant representations for the same spoken content.\nThe soft-DTW replaces the “min” operation in the DTW with “soft-min” operation. Soft-DTW computes the\nsoft-minimum of all alignment cost. The soft-DTW for two sequences  and\n is defined [19  ###reference_b19###] as follows:\nwhere  is the set of all possible paths. The  is the soft-min operator with a smoothing\nfactor  and  is the distance function.\nThe soft-min operator  is defined as:\nIn this work,  is used as a loss function for SCORE fine-tuning as described in Eq. 3  ###reference_###.\nIn all the experiments, we use a smoothing factor  of 0.1. However, to address potential negative values in  loss, a normalized version described in Eq. 4  ###reference_### is employed. This normalization guarantees a minimum loss value of zero for identical sequences, i.e., , and ensures  for any pair of sequences. This approach guarantees a consistently positive loss [20  ###reference_b20###, 21  ###reference_b21###]. Further, the loss from Eq. 4  ###reference_### is normalized by dividing it with the total sequence length . Algorithm 1  ###reference_### describes the entire SCORE fine-tuning method.\nThe reported numbers are from their respective papers and SUPERB benchmark leaderboard [14  ###reference_b14###] as of 13/09/2023 (https://superbbenchmark.org/leaderboard  ###reference_###).\nOur results when we run the SUPERB [14  ###reference_b14###] baseline scripts for HuBERT and WavLM for fair comparison."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Experiments are conducted on two SSL speech models: HuBERT and WavLM (BASE versions). These SSL models are fine-tuned with the SCORE method. After the SCORE fine-tuning, obtained models are used for supervised training for the content-related downstream tasks on the SUPERB benchmark.\nSimilar to SPIN [4  ###reference_b4###], the top 2 layers (11th and 12th) of the SSL models are fine-tuned as it is cost-effective (M trainable parameters) and most of the SSL models encode phonetic content in top layers [15  ###reference_b15###, 16  ###reference_b16###]. In this study, Wav2vec2 [7  ###reference_b7###] is omitted due to the fact that the linguistic content is less well represented\nin the final few layers [16  ###reference_b16###], which is crucial for content-related tasks. Fine-tuning the entire model, from bottom layers to top layers, would result in increased computational expenses, contradicting the study’s intended objectives. Furthermore, there is a concern that when the entire model is fine-tuned, the fine-tuning objective could potentially lead to a collapse of the original representations [22  ###reference_b22###] learned during pre-training.\nThe details about the data, SCORE fine-tuning, and evaluation on the SUPERB benchmark are described as following:\nData: In line with prior research [4  ###reference_b4###] and to ensure a fair comparison, experiments are performed on LibriSpeech’s [23  ###reference_b23###] train-clean-100 hours of data for SCORE fine-tuning. Consistent with earlier discoveries [4  ###reference_b4###], training more layers or additional data does not enhance results.\nSCORE Fine-tuning Details:\nThe representations obtained from the final Transformer layer (12th) of the models  and  are sequences of 768-dimensional vectors. These vectors are projected into 256-dimensional vectors with linear projection layers and then L2-normalized. The SCORE fine-tuning method is trained for 3.6k updates ( 1 epoch with effective batch size of 8). The model converged in just one epoch, and additional training did not yield any improvements. AdamW [24  ###reference_b24###] optimizer is used with a learning rate of  with 1k warm-up updates. One epoch roughly takes  5 hours on V100 GPU. More details are available at GitHub333https://github.com/Trikaldarshi/SCORE_Finetuning  ###reference_tuning###.\nSUPERB Benchmark: S3PRL toolkit 444https://github.com/s3prl/s3prl  ###reference_github.com/s3prl/s3prl### is used for all the SUPERB benchmark tasks. For ASR and PR, features from all the layers are aggregated with learnable weights. These aggregated features are then fed to the prediction head for each downstream task and fine-tuned with labelled data. For ASR, the prediction head consists of 2-layer 1024-unit Bi-LSTM network with CTC loss on characters [14  ###reference_b14###]. The ASR model is evaluated without any external language model. For PR, the prediction head is a frame-wise linear\ntransformation with CTC loss. More details can be found at SUPERB benchmark [14  ###reference_b14###]. Adam optimizer is used for both ASR and PR with learning rate of  and , respectively. We conducted experiments for each ASR and PR model five times and have provided the results, including the means and standard deviations, for both the vanilla models (HuBERT and WavLM) and their SCORE fine-tuned versions. For QbE, conventional supervised phoneme posteriorgram are replaced with SSL representations [14  ###reference_b14###]. For QbE, no training is required, and the evaluation is performed by running DTW on all layers separately and obtain a score for each query-document pair. For the evaluation on test set, the best layer is selected based on performance on dev set from QUESST 2014 [25  ###reference_b25###] data. In our case, we found that 12th layer provides best results for QbE for both HuBERT + SCORE and WavLM + SCORE."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussions",
            "text": "Table 1  ###reference_### shows the processed speech during training in “pre-training” stage and in “SSFT stage”. Processed speech is defined as “training steps × effective batch duration” to quantify machine-independent training costs [4  ###reference_b4###]. HuBERT + SCORE improves the HuBERT model on all three tasks with relative improvement of 1.09%, 3.58%, and 12.65% for ASR, PR, and QbE, respectively. WavLM + SCORE improves the WavLM model on ASR, PR and QbE with relative improvement of 0.32%, 2.68% and 0.76%, respectively. The results are also compared with a stronger baseline ContentVec [5  ###reference_b5###], which uses 76K hours of processed speech compared to the SCORE which uses only 100 hrs in SSFT stage. ContentVec provides better results in ASR and PR when compared with SPIN and SCORE at the compute cost of 76K hrs.\nHowever, both HuBERT + SCORE and WavLM + SCORE outperform ContentVec on QbE task.\nLike SPIN, the goal of this work is to strike a balance between improving the downstream task and the additional training (i.e. SSFT) required. SCORE only needs  0.5 % of processed speech when compared with ContentVec in SSFT stage. SCORE provides competitive results with the SPIN models. WavLM + SCORE outperforms WavLM + SPIN in QbE task. Performance of HuBERT + SCORE is close to HuBERT + SPIN on ASR.\nAmong all the SSFT method, SCORE uses the least amount of processed speech ( 100 hrs) in SSFT stage."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Works",
            "text": "A simple and cost-effective SSFT method named SCORE is proposed to improve content representations of the pre-trained SSL speech models. For both the HuBERT and WavLM models, their respective SCORE fine-tuned models outperformed the original models on the SUPERB benchmark for ASR, PR, and QbE. Compared to other existing approaches of SSFT, SCORE requires the least amount of processed speech (less than 0.5% of processed speech compared to ContentVec). SCORE provides competitive results with SPIN using 1/3 of the processed speech used by SPIN. While we observed relatively fewer improvements in ASR compared to PR and QbE, we speculate that a stronger data augmentation technique directly applicable on speech waveforms could provide better gains. We consider this research direction for our future work."
        }
    ]
}