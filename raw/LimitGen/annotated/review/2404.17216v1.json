{
    "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
    "abstract": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans–English and Yoruba–English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans–English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.\n\n\n\nKeywords: code-switch, LLM, few-shot, prompting",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Multilingual communities, exemplified well by various African countries, often engage in code-switching, where two or more languages are used within a single discourse Poplack (2001a  ###reference_b23###). This language practice highlights the need to develop more advanced natural language processing (NLP) technologies that can smoothly process and produce code-switched sentences. This will move the needle towards equitable representation of the world’s under-resourced languages, ensuring that everyone has equal access to these technologies (Solorio, 2021  ###reference_b29###).\nThere are numerous challenges in code-switching research. The main three are highlighted by Doğruöz et al. (2021  ###reference_b8###) as follows: i) data, which is related to quantity, quality and availability; ii) evaluation, which refers to benchmarks and metrics; and iii) challenges related to end-to-end applications, particularly the ability to process and produce code-switched data.\nThe focus of this paper is on the first challenge regarding data. While code-switching frequently occurs in written forms, due to the ubiquitous use of social media platforms, leveraging this data in NLP applications for code-switching presents many challenges. These platforms, with their extensive and diverse linguistic expressions, can be invaluable in gathering code-switched data. Yet, the practical utility of such data is hindered by various factors, including the informal, inconsistent nature of online language (Çetinoğlu et al., 2016  ###reference_b6###). It is common to use acronyms, emojis and make spelling mistakes which affect quality and usability of such data (Srivastava et al., 2019  ###reference_b31###). Furthermore the diversity of such data is limited to a specific type of language use (Winata et al., 2022  ###reference_b36###).\nTo address the shortage of available data, efforts have been made to create synthetic code-switched data using different methods: from using parallel corpora with linguistic constraints on where a switch can occur (Pratapa et al., 2018  ###reference_b25###; Rizvi et al., 2021  ###reference_b27###) to employing transformer-based models to generate diverse sentences that adhere to lexical and syntactic rules (Riktika et al., 2022  ###reference_b26###). A more recent study evaluated prompting of large language models (LLMs) to generate code-switched data for South East Asian languages (Yong et al., 2023  ###reference_b38###). They explored a few prompting templates with a limited number of topics in a zero-shot manner and cautioned against the use of synthetically generated data without involving native speakers of the language.\nIn this paper, we build on the work of (Yong et al., 2023  ###reference_b38###) to address the question about GPT’s ability to generate code-switched data. Our work overlaps in that we also use an LLM, OpenAI’s GPT, and various topics in the prompts. We increase the number of topics and provide topic-related keywords in an effort to increase diversity and reduce the model’s propensity to default to certain words. Our goal is not to evaluate various prompting templates, however, we add linguistic guidelines in the prompts to further increase diversity. We propose this as an approach towards language agnostic prompting. We also test the performance of GPT 3.5 with few-shot in-context examples. We specifically consider whether GPT can support the generation of larger code-switched datasets and to what extent.\nOur contributions are as follows: (i) we provide a framework to increase the diversity of synthetically generated code-switched data by prompting OpenAI’s GPT; and (ii) we position GPT as a pivot to address code-switched data scarcity in low-resource languages while emphasising the need for native speakers in the loop.\nIncreasing data availability is at the center of developing language models that serve multilingual communities. Our work is a step towards closing the gap in low-resourced and under-represented languages."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Code-Switched Text Generation via GPT-3.5 Prompting",
            "text": "Our prompt-based approach to code-switched (CS) text generation is heavily inspired by the work of Yong et al. (2023  ###reference_b38###), who collected synthetic CS data by prompting LLMs with requests along languages and topics. Their focus was on code-switching English with South-East Asian languages. In our case, we focus on two under-explored and under-resourced code-switching scenarios: Afrikaans–English and Yoruba–English. Although Afrikaans and English are typologically dissimilar (van Dulm, 2007  ###reference_b33###), they are both West Germanic languages and generating CS text should be easier. Yoruba is a tonal language and even more dissimilar to English which could provide challenges when creating synthetic CS data. We extend the limited topics covered in Yong et al. (2023  ###reference_b38###) and present GPT-3.5 not as an autonomous solution to CS data scarcity, but as a potential tool for supporting CS data curation efforts for under-resourced African languages. We specifically use GPT-3.5, firstly as a baseline to compare with the findings from Yong et al. (2023  ###reference_b38###) and secondly, due to the unavailability of the GPT-4 API at the time of our experiments111The API for GPT 4 was made available after we finished the majority of the experiments, \nhttps://openai.com/blog/gpt-4-api-general-availability  ###reference_availability###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Prompting for Afrikaans–English CS Sentences",
            "text": "Building on the prompt template from Yong et al. (2023  ###reference_b38###), which uses topics as guidelines, our approach extends this by (i) incorporating specific code-switching words related to each topic within the prompt and (ii) evaluating the effect of prompt complexity from basic (Section 3.1.1  ###reference_.SSS1###) to more comprehensive prompts (Section 3.1.2  ###reference_.SSS2###).\nWe curate a non-exhaustive list of common conversation topics and associate typical English words from native speakers of Afrikaans and from available online platforms. We cover 22 topics with a total of 355 keywords. For this paper we generate one sentence per keyword for the various prompts. We also develop a general list of words used in code-switching that is not directly linked to a specific topic consisting of 138 words. 90% of the keywords are nouns, verbs and adjectives which is in line with the notion that switching is more likely to occur on these open word classes as opposed to close word classes (such as pronouns and conjunctions) (Kodali et al., 2022  ###reference_b13###)."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   Topic-Keyword Basic Prompting",
            "text": "In the six different prompting templates of Yong et al. (2023  ###reference_b38###), one prompt specifically requests a native speaker to give a mixed sentence. This is an indirect way to impose a matrix language (ML). We explicitly include the use of a matrix language in our prompts (Jake et al., 2002  ###reference_b11###). This is to ensure that we adequately represent the low-resourced language. However, we recognise that grammatical constraints on CS is an open research question with varying definitions of acceptability that evolves over time (Bhat et al., 2016  ###reference_b3###).\nThe following shows the basic prompt we used (Prompt 1.1) and a few examples to highlight the behaviour of GPT-3.5 (English translation in Italics).\nTopic: education and training; keyword: skills\n\n\n\n\nExample 1: Ek moet my skills verbeter om ’n beter werksgeleentheid te kry.\nI must improve my skills to get a better job opportunity.\nTopic: general conversation; keyword: try\n\n\n\n\nExample 2: Ek sal probeer to finish my assignment op tyd.\nI will try to finish my assignment on time.\nThe matrix language is Afrikaans in Example 1 and English in Example 2. We see from these examples that GPT 3.5 does not necessarily follow the prompt with regards to the matrix language.\nWe do not evaluate word-level language identification therefore we do not explicitly measure adherence to the matrix language prompt in this paper.\nThe results of the generated sentences therefore indicate that GPT 3.5 is capable of generating some coherent sentences and can be corrected where the grammatical structure follows English. Section 4.3  ###reference_### gives a more detailed analysis of code-switch acceptability.\nA key observation from using this basic prompt for generating Afrikaans–English sentences is that sentences are one-dimensional with 80% of sentences starting with a singular personal pronoun: ‘Ek’ (English: ‘I’) (Section 4.2.1  ###reference_.SSS1###). This creates the opportunity to explore ways of adding diversity to the type of sentence through the use of basic linguistic guidelines (such as specifying pronouns) which is discussed in the following section."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Linguistic-Based Prompting",
            "text": "Since the word lists contain nouns, verbs and adjectives related to specific topics, content diversity in the sentences is addressed. These are also words that are most typically code-switched (Kodali et al., 2022  ###reference_b13###). To add further diversity in the type of sentence, we add basic linguistic guidelines in the form of varying pronouns (personal, impersonal, interrogative etc.), tenses (past, present and future that alters the verb) and using negative particles. The inclusion of negative particles is randomly initialised and not in each prompt. We also impose a rule that conjunctions must be in the matrix language since conjunctions are part of closed word classes and should less likely be switched.\nPrompt 2.1 is an example of a prompt using linguistic guidelines following with an example of the generated sentence (English translation in Italics). In Example 3 the prompts are adhered to, however, the conjunctions ‘but’ and ‘and’ are in English therefore note adhering to the guideline.\nOur preliminary observation is that the prompting approach can support the generation of CS sentences that are diverse. The effect of varying pronouns on sentence diversity is further evaluated in Section 4.1  ###reference_###. Word order structure mimics that of natural speech and can be corrected where needed. We give additional examples and an evaluation of the quality of the sentences in Section 4.3  ###reference_###.\nTopic: physical health and fitness; keyword: race; Pronoun: impersonal; Tense: past; Use a negative particle: No\n\n\n\n\nExample 3: Dit was super lekker om die race te hardloop, but ek ignore die consequences and het te veel geëet afterwards.\nIt was super nice to run the race, but I ignore the consequences and ate too much afterwards."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3.   Few-Shot Prompting",
            "text": "In the work from Yong et al. (2023  ###reference_b38###) they did not evaluate the effect of few-shot examples. We therefore evaluate two additional prompts: Prompt 1.2 and Prompt 2.2 where we add five examples of code-switched sentences to Prompts 1.1 and 2.1 respectively. These are general examples and not in the context of the topic."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Prompting for Yoruba–English CS Sentences",
            "text": "In this section we apply the same methodology (Section 3.1  ###reference_###) used to generate Afrikaans–English CS sentences to generate Yoruba–English CS sentences and provide brief observations. We develop similar topic keyword lists for Yoruba with most words overlapping with those developed for Afrikaans–English. In future work we will focus on developing common lists that cover a more diverse set of languages. The following are a few examples of the generated Yoruba–English sentences:\nTopic: information technology; keyword: spreadsheet; Pronoun: indefinite; Tense: future; Use negative particle: Yes\n\n\n\n\nExample 1: Mo ni ko relax, infact mo gba surprise pe spreadsheet jẹ Yoruba word.\nI said you should relax, infact I accept the surprise that spreadsheet is a Yoruba word.\nTopic: social media; keyword: cope; Pronoun: indefinite; Tense: present; Use negative particle: Yes\n\n\n\n\nExample 2: Kò sí èèyàn tó yàn ònà ní wáhálà, view yìí ni awọn ẹ̀dá tí wọ̀n ṣe làti cope.\nThere is no person that chooses problems as a path, this view is what the creatures XXX did to cope\nExamples 1 and 2 both follow the prompt guidelines with respect to the matrix language and tense. Example 1, however, uses a personal pronoun instead of an indefinite pronoun with Example 2 using the correct pronoun. XXX in Example 2 indicates a phrase that cannot be translated.\nWe observe that the prompting approach can also support the generation of Yoruba–English sentences that are diverse.\nWe provide observations on the coherence and naturalness of synthetic sentences in Section 4.4  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Evaluation of Generated Data",
            "text": "In this section, we evaluate our work in three parts: (i) we evaluate the diversity of the generated sentences, (ii) we comment on GPT 3.5’s adherence to the prompts provided, and (iii) we evaluate the quality of the sentences generated through a combination of statistical analysis and human evaluation of the sentences. We use the four prompt guidelines as discussed in Section 3  ###reference_###. For this paper we Romanised the Yoruba–English sentences for easier evaluation, however, we will include this in future work."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Data Diversity",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1.   Content Diversity",
            "text": "In Figure 1(a)  ###reference_sf1### (from Prompt 1.1) we see a large amount of general words being used compared with the number of sentences. We also note that the top three keywords (amazing, acknowledge, anyway) is the same as the top three keywords in the alphabetised list. In Prompt 2.1 we provide a randomised general word list to GPT 3.5 and in Figure 1(b)  ###reference_sf2### we observe a more even distribution of general words as a result. This indicates GPT 3.5’s sensitivity to prompts and the context provided.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2.   Linguistic Diversity",
            "text": "Since Prompts 2.1 and 2.2 asked “start the sentence with…”, all sentences were evaluated accordingly. We used a list of common Afrikaans and Yoruba pronouns to evaluate this prompt.\nFrom Figure 3  ###reference_### we observe an increase in diversity of the types of sentences with regards to the distribution of pronouns (Prompts 2.1/2.2). For Afrikaans–English, more than 90% of the sentences start with one of the specified pronouns. We also see an increase in the diversity of Yoruba–English sentences, however, there are still 35% of sentences starting with words other than the requested pronouns. It is not well understood why GPT 3.5 ignored these prompts. In the absence of linguistic guidelines in the prompt, we note that by adding few-shot examples, we lack diversity (Prompts 1.2 and 2.2).\n###figure_3### Similarly to pronouns, we use Afrikaans and Yoruba keywords that indicate past and future tense, negation (negative sentiment) and conjunctions to evaluate the effect of adding these guidelines to the prompts. In Table 1  ###reference_### we highlight the impact of these factors on distribution in sentences using Prompts 1.1 and 2.1 (prompts without example sentences).\nWe see in Table 1  ###reference_### that for Afrikaans–English, both the distribution of tenses (equal distribution between past and future) and the presence of negation improved. However, it is only negation that improved for Yoruba–English. We further elaborate on this observation in Section 4.2.1  ###reference_.SSS1###. The ratio of Afrikaans:English conjunctions decreased showing the guideline is not efficient. For Yoruba:English conjunctions we observe a slight improvement.\nThe above statistical evaluation of diversity shows that adding various linguistic guidelines to the prompts improves diversity. However, this does not consider whether a prompt is adhered to. In the next section, we evaluate GPT 3.5’s ability to execute prompts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Prompt Adherence",
            "text": "In Section 3.1  ###reference_### we already observed that GPT 3.5 does not always adhere to using the specified matrix language and since we do not consider word-level language identification in this paper, we exclude this when determining adherence.\nWe apply a simple approach to calculate prompt adherence. We express the number of prompts adhered to as a percentage of the total prompts given. In Prompt 1.1, the only prompt given is the topic keyword hence a total of one prompt (the same for Prompt 1.2). In Prompt 2.1, there are five prompts given: topic keyword, pronoun, tense, negative particle and conjunction. The average prompt adherence across the sentences is then used to represent overall prompt adherence."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1.   Statistical Evaluation of Prompt Adherence",
            "text": "In this section we present the prompt adherence for the four prompt guidelines. Keywords for pronouns, tenses, negative particles and conjunctions as per Section 4.2.1  ###reference_.SSS1###. Table 2  ###reference_### shows the overall prompt adherence.\nFrom Table 2  ###reference_### we see that the adherence to prompts for Yoruba–English is much lower than for Afrikaans–English in the linguistically guided prompts (Prompts 2.1 and 2.2).\nIn Afrikaans there are a few specific keywords such as ‘nie’, ‘nooit’, ‘nee’ (English: not, never, no) that indicate negation. Similarly for tenses, words like ‘was’, ‘gister’, ‘wil’, ‘more’ (English: was, yesterday, will, tomorrow) can be used for past and future tense. However, the Yoruba language is more complex and keywords like the above-mentioned are not adequate to identify negation and tenses, hence the lower prompt adherence.\nIn the next section (Section 4.3  ###reference_###) we use manual annotation of sentences for tenses and negation to re-evaluate prompt adherence."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2.   Manual Evaluation of Prompt Adherence",
            "text": "For manual evaluation of generated sentences, we sample 100 sentences each from the four prompt methods.\nWe manually annotate the sentences of Prompts 2.1 and 2.2 with tense (past or future) and negation (whether the sentence expresses some negative sentiment). In future work, external annotators will also be used.\nIn Table 3  ###reference_### we show the impact on the calculated prompt adherence (using Prompt 2.1) for the statistical (1) and manual (2) evaluation of the 100 sentences. The prompt adherence for Yoruba–English increased to 66% from 59% with a significant increase in the adherences to tenses.Afrikaans–English prompt adherence remains constant. The adherence to negation reduced slightly for both languages. This confirms the earlier comment that it is statistically more difficult to calculate prompt adherence for Yoruba–English without a human in the loop.\nWe conclude that there is potential in using GPT 3.5 as a supporting tool to generate diverse sentences with linguistically guided prompts. In the following sections we provide an overview of the quality of generated sentences to further determine the role that GPT 3.5 can play in addressing code-switched data availability."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Code-Switch Acceptability",
            "text": "The final part of our analysis looks at the quality of generated sentences. As mentioned in Section 4.3  ###reference_###, we sampled 100 sentences from each of the four prompt methods. For this part of the analysis, we rated the acceptability of a code-switch sentence according to: i) Yes, ii) Yes, with minimal changes or iii) No. We adopt the constraint-free approach of MacSwan (2000  ###reference_b16###).\nThe results of the manual annotation are shown in Figure 4  ###reference_###. We observe that the acceptability of Afrikaans–English sentences far outweighs that of Yoruba–English. We also see that adding few-shot examples increases acceptability (Prompts 1.2 and 2.2). Although we observe an increase in diversity through linguistic guidelines, the quality of sentences are sub-optimal. Subsequent work will focus on how correctable sentences can be used for improved prompting and/or fine tuning of language models. However, with further analysis and improvement, there is potential to use GPT 3.5 to support synthetic data generation.\n###figure_4###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Language Specific Observations",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1.   Afrikaans–English",
            "text": "In order to quantify the acceptability observed from internal evaluation, we randomly select 5 Afrikaans-English sentences from the dataset used for manual evaluation (Section 4.3  ###reference_###). Table 4  ###reference_### gives the sentences with translations and comments.\nIn our general overview we find that the typical mistakes made are as a result of following English grammar structure. However, for many sentences this does not affect the meaning and can be corrected.\nThe results from the various experiments therefore indicate that using GPT 3.5 (and it’s followers) can be considered as a method to generate large-scale data in Afrikaans-English code-switching."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2.   Yoruba–English",
            "text": "Similarly to quantifying the Afrikaans-English sentences, we give 5 randomly selected Yoruba-English sentences in Table 5  ###reference_### from the dataset used for manual evaluation (Section 4.3  ###reference_###).\nIt is hypothesised that the exposure of GPT 3.5 to the Yoruba language is to a much lesser extent than Afrikaans yielding a substantial amount of unacceptable sentences. Furthermore, as was postulated by Yong et al. (2023  ###reference_b38###), languages using the English alphabet and Latin script perform better on LLMs. Further analysis is required to improve prompting and quality of sentences."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusions and Future Work",
            "text": "In this paper we extended on the of Yong et al. (2023  ###reference_b38###) where they used prompting of LLMs (including GPT 3.5) to generate code-switch sentences. Our approach evaluates three dimensions: (i) diversity, through a wider range of topics, keywords, linguistic guidelines and few-shot examples; (ii) prompt adherence, to understand the ability of GPT 3.5 to follow these prompts; and (iii) quality, to determine the use of GPT 3.5 as a supporting tool to address code-switched data scarcity.\nWe evaluated two typologically diverse language pairs: Afrikaans–English and Yoruba–English.\nOur main findings are: (i) using topics, keywords and general context words increases coverage; (ii) linguistic-based guidelines increases diversity in the types of sentences, (iii) few-shot prompting increases the quality of sentences but is limited in diversity of the types of sentences;(iv) quality of sentences are much lower for languages that use non-Latin script (such as Yoruba); and (v) evaluating quality of data requires a human-in-the-loop.\nWe provide a framework for linguistically-guided prompting and we conclude that OpenAI’s GPT exhibits the ability to support synthetic code-switched data generation and can be invaluable to address the issue of data availability.\nIn future work we will address the following: i) include external annotation to cross-validate the quality of generated sentences; (ii) improve on the prompting guidelines to increase quality; (iii) use correctable sentences to improve the performance of the latest generation of OpenAI’s GPT to support large-scale generation; and (iv) expand to more African languages in an effort to develop a language agnostic approach to synthetically generate data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Ethical Considerations",
            "text": "Research in code-switching is not only focused on the grammatical aspects of this phenomenon but also the socio-pragmatic characteristics in discourse (Nel, 2012  ###reference_b18###). Large language models such as OpenAI’s GPT are influenced by social views and inherit encoded biases (Bender et al., 2021  ###reference_b2###). Our work propose the use of GPT to support efforts in synthetically generated code-switched data to increase the prevalence of under-resourced languages. We therefore carefully considered the method in which GPT was prompted to eliminate the introduction of bias. We use general topics and keywords with the goal to generate a diverse range of acceptable sentences.\nThe generated sentences were internally evaluated by native speakers of Afrikaans and Yoruba. We ensure the data is respectful to culture and social norms. We will continue to include humans-in-the-loop to ensure faithful data generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgements",
            "text": "We thank JP Morgan and ABSA for their financial support, and OpenAI for providing API credits."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ]
}