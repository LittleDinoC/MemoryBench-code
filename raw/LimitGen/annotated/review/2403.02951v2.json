{
    "title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation",
    "abstract": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task,\nsignificantly outperforming traditional methods.\nNevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks.\nAdditionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process,\nwhich hinders the assessment of LLMs’ cognitive capabilities and the optimization of LLM-based solutions.\nTo address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs.\nThen we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.\nOur study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task.\nThese findings offer valuable insights for facilitating the development of LLM-based Text-to-SQL systems.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Text-to-SQL, which involves the automatic transformation of natural language (NL) questions into structured SQL statements, is a pivotal component in facilitating seamless user interaction with databases Qin et al. (2022  ###reference_b35###).\nPrevious approaches to this task primarily focus on pattern matching between natural language and SQL statements, utilizing machine learning models to acquire the mapping between the two Zhong et al. (2017  ###reference_b60###); Li et al. (2023a  ###reference_b21###).\nHowever, the introduction and rapid advancement of Large Language Models (LLMs) have brought about a substantial transformation in this field Dong et al. (2023  ###reference_b9###); Pourreza and Rafiei (2024  ###reference_b33###).\nLLMs have emerged as powerful tools Ruan et al. (2023  ###reference_b38###); Kong et al. (2023  ###reference_b18###); Sui et al. (2023  ###reference_b43###), showcasing tremendous potential in comprehending complex NL questions and generating accurate SQL statements.\nBy combining advanced reasoning techniques and in-context learning capabilities, LLMs have significantly pushed the boundaries of the state-of-the-art in this domain, outperforming traditional methods by a considerable margin.\nDespite the continuous improvement of LLM-based methods in various benchmarks such as Spider Yu et al. (2018  ###reference_b56###) and BIRD Li et al. (2024  ###reference_b24###), there remains a critical gap in the systematic benchmarking of these solutions Katsogiannis-Meimarakis and Koutrika (2023  ###reference_b17###); Qin et al. (2022  ###reference_b35###); Kumar et al. (2022  ###reference_b19###).\nThe absence of a comprehensive framework for evaluating LLMs in Text-to-SQL complicates the design and assessment of effective systems.\nThe risk of overfitting, particularly for LLMs trained on coding tasks and open-source datasets, poses a significant challenge to the reliability of benchmark evaluations Roziere et al. (2023  ###reference_b37###).\nAdditionally, the optimal prompt engineering strategies that play a crucial role in guiding LLMs to generate accurate SQL queries are yet to be determined.\nAlthough various design schemes are explored in different methods Gao et al. (2023  ###reference_b13###), there is no consensus on the most effective prompt template.\nFurthermore, the current benchmarks, while comprehensive in their assessment of end-to-end Text-to-SQL tasks, have not yet provided a detailed exploration of the models’ performance across the various sub-tasks and components of the Text-to-SQL process Pourreza and Rafiei (2023  ###reference_b32###); Liu et al. (2023  ###reference_b27###).\nA detailed exploration of these sub-tasks is crucial for a thorough evaluation of LLMs’ cognitive capabilities and their role in facilitating the Text-to-SQL process.\nTherefore, it is necessary to develop a more granular benchmarking approach that can accurately reflect the multifaceted nature of Text-to-SQL and inform the creation of more effective LLM-based solutions.\nTo address the aforementioned challenges and fill the gap in the systematic benchmarking of LLMs in Text-to-SQL, we construct a comprehensive testing benchmark that provides a holistic assessment of LLM capabilities in this domain.\nOur approach begins with the construction of a Text-to-SQL dataset, designed to mitigate the risk of overfitting by considering question complexities, database sizes, and prerequisite knowledge.\nFormally, we devise five distinct tasks—Text-to-SQL, SQL Debugging, SQL Optimization, Schema Linking and SQL-to-Text—to comprehensively evaluate the capabilities of LLMs across the full spectrum of the Text-to-SQL process (see Figure 1  ###reference_###).\nSubsequently, we perform an extensive analysis of various techniques that are essential for improving the in-context learning abilities of LLMs and their precision in generating SQL queries. Specifically, our evaluations are summarized as follows:\nTo determine the optimal prompt template, we partition the prompt text into distinct components and perform thorough testing of LLMs’ performance on end-to-end Text-to-SQL tasks across all possible combination patterns.\nOur benchmarking approach encompasses a range of LLMs, including both general-purpose and coding-specific models with varying parameter sizes. We determine the performance boundaries of these models and identify their performance disparities (see Figure 2  ###reference_###).\nFor each task, we systematically assess the impact of information granularity on model performance and identify the optimal context learning strategies, such as zero-shot and few-shot, to maximize the performance of the models.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM-based Text-to-SQL Methods",
            "text": "With the advancements in LLMs, researchers are increasingly interest in creating a natural language interface for relational databases through the powerful linguistic and coding capabilities of LLMs, forge a new trend of LLM-based Text-to-SQL.\nGiven the powerful zero-shot reasoning and domain generalization abilities of LLMs, these methods successively refresh the record on the cross-domain Spider leaderboard.\nC3 Dong et al. (2023  ###reference_b9###), a zero-shot Text-to-SQL method built upon ChatGPT, provides treatment in terms of model input, bias and output, reaching an 82.3% execution accuracy on Spider leaderboard. DIN-SQL Pourreza and Rafiei (2024  ###reference_b33###) proposes decomposing the Text-to-SQL task into smaller sub-tasks effectively\nand achieves an accuracy of 85.3% on Spider. DAIL-SQL Gao et al. (2023  ###reference_b13###)\nrefreshes the accuracy of Spider with 86.6% through both supervised fine-tuning and a systematic study of in-context learning. It explores how to select the most helpful example and organize them properly in prompts in a few-shot scenario. Similarly, other studies investigate the selection of few-shot demonstrations by synthesising in-domain examples Chang and Fosler-Lussier (2023  ###reference_b3###) and retrieving question skeletons Guo et al. (2023  ###reference_b14###).\nMAC-SQL Wang et al. (2023a  ###reference_b47###) utilizes multi-agent collaboration for Text-to-SQL tasks and reach an accuracy of 59.6% on more challenging BIRD.\nSeveral other studies focus on special yet non-trivial scenarios, aiming to expand the scope of the Text-to-SQL task.\nDBCopilot Wang et al. (2023b  ###reference_b48###) considers a more realistic problem, wherein it deals with large-scale schemas, characterized by massive databases and a large number of tables. It proposes the use of a lightweight seq2seq copilot model for schema-routing, increasing scalability in comparison to traditional schema-linking.\nACT-SQL Zhang et al. (2023b  ###reference_b58###) designs a chain-of-thought (CoT) Wei et al. (2022  ###reference_b49###) prompt to improve the reasoning ability when generating SQLs, and extended to the multi-turn Text-to-SQL task Li et al. (2023b  ###reference_b22###).\nIn summary, these methods primarily focus on improving the performance of the overall Text-to-SQL task through several aspects of the sub-tasks of interest in this paper. However, these methods 1) do not individually and systematically study the performance of sub-tasks, and 2) typically rely on experiments using only OpenAI LLMs (ChatGPT, GPT4, etc.), without verifying the robustness of their approaches when apply to other open-source LLMs.\n###figure_2###"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Datasets and Evaluation Metrics",
            "text": "WikiSQL Zhong et al. (2017  ###reference_b60###) is considered the first large-scale dataset enabling the training and evaluation of learning-based Text-to-SQL methods, as well as offering a standardized benchmark for straightforward comparison across various methods.\nIt is also known as a cross-domain dataset, featuring over 25,000 tables and 80,000 question-SQL pairs comprising various domains derived from Wikipedia.\nHowever, the SQL queries in WikiSQL exhibit low complexity.\nSubsequently, most of the recent Text-to-SQL works are done on Spider Yu et al. (2018  ###reference_b56###) because it is widely acknowledged as the most challenging cross-domain benchmark. It comprises 10,181 queries covering 138 different domains, involves multi-table queries (embodied by JOIN), complex SQL clauses (ORDER BY, GROUP BY, and HAVING, etc.) and nested SQLs.\nSeveral variants of Spider have been designed to evaluate the adaptability of Text-to-SQL methods Shaw et al. (2020  ###reference_b40###); Gan et al. (2022  ###reference_b12###).\nSpider-Realistic Deng et al. (2020  ###reference_b6###) removes the explicit mention of column names in the NL questions while keeping the SQL queries unchanged, which is more aligned with the use-case of Text-to-SQL in practice.\nSpider-Syn Gan et al. (2021a  ###reference_b10###) replaces some schema words in the NL question with their synonyms that reflect real-world question paraphrases. Eliminating such explicit correspondence between NL questions and table schemas poses a greater challenge of Text-to-SQL methods.\nSpider-DK Gan et al. (2021b  ###reference_b11###) integrates artificial domain knowledge, aiming to investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge.\nGiven that Spider is specifically designed for benchmarking Text-to-SQL methods while diverging from practical scenarios, KaggleDBQA Lee et al. (2021  ###reference_b20###) constructs a small-scale cross-domain dataset from Kaggle, highlighting the diversity of actual web-sourced databases.\nIt comprises knowledge such as documentation and metadata of databases, raising an question of how this extra information could be used to improve the performance.\nThe newest seminal benchmark is BIRD Li et al. (2024  ###reference_b24###), involving 12,751 Text-to-SQL pairs and 95 databases with a size of 33.4 GB. It incorporates the advantages of previous datasets, such as Spider (which is cross-domain with complex SQLs) and KaggleDBQA (which requires the ability to use external knowledge evidence).\nIt is the first to be curated for evaluating SQL execution efficiency in large-scale databases, thereby further bridging the gap between academic setups and real-world applications.\nIn this paper, we construct a novel dataset built upon BIRD and use it in the evaluation (see Section 3.3  ###reference_###).\nTwo primary evaluation metrics for assessing the accuracy of SQLs on Spider are Exact Matching(EM) and Execution Accuracy(EX). EM measures whether the predicted query as a whole is equivalent to the gold query. It is possible to encounter false negative evaluations since a question might be solvable by multiple syntactically different but semantically identical SQL statements. EX is a more widely used metric, measures whether the result of executing the predicted query matches the gold value. We use EX to evaluate the accuracy of SQLs in this paper.\nAdditionally, BIRD further proposes Valid Efficiency Score (VES), an integrated metric assessing both accuracy of execution results (i.e., EX) and the execution efficiency of SQL queries. To increase the VES, methods are required to enhance both execution accuracy and efficiency of SQL queries. We use this metric to assess the SQL optimization in Section 4.3  ###reference_###. Refer to Appendix C.1  ###reference_### for detailed definitions of evaluation metrics."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Settings",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Formulation",
            "text": "In an LLM-based Text-to-SQL system, LLMs are employed to facilitate the transformation of natural language questions into executable SQL queries. Specifically, Let  be a natural language question and  be the database schema.  is defined by a tuple , where  represents multiple tables,  represents columns, and  represents foreign key relationships. The goal is to produce a SQL query  such that  is executable and accurately represents the intent of .\nGiven the prompt template , the generation process of the SQL query  by an LLM  can be formally defined as a conditional probability distribution:\nHere, LLM autoregressively generates each token,  denotes the -th token of the SQL query , and  denotes the length of the query ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Models",
            "text": "Our benchmarking study evaluates the performance of two distinct categories of LLMs with varying parameter sizes: general-purpose and coding-specific. General-purpose LLMs are designed for versatile text generation and comprehension across diverse domains, trained on extensive internet text datasets. Specifically, ChatGPT (gpt-35-turbo-16k) 111https://chat.openai.com, LLaMa2-Chat-70B Touvron et al. (2023  ###reference_b44###), InternLM-70B 222https://internlm.intern-ai.org.cnand InternLM2-20B are selected as the main baseline models. Coding-specific LLMs are fine-tuned and optimized for programming scenarios, excelling in code generation and technical language understanding. In this paper, the performance analysis of Codellama-34B Roziere et al. (2023  ###reference_b37###) and SQLCoder-34B 333https://github.com/defog-ai/sqlcoder are provided."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Dataset Construction",
            "text": "We conduct a preliminary assessment of the performance of various LLMs on multiple open-source datasets. As depicted in Table 1  ###reference_###, the performance of LLMs varies inconsistently across different datasets. Specifically, on the Spider dataset, Codellama-34B outperforms InternLM-70B and SQLCoder-34B, while on the Bird dataset, SQLCoder-34B surpasses InternLM-70B and Codellama-34B.\nOn the one hand, there may be differences in the problem types that different LLMs excel at handling. On the other hand, considering that LLMs learn and train from large corpora, these findings suggest that the performance discrepancies observed could be attributed to the potential utilization of open source datasets during the fine-tuning process for coding-specific LLMs. This poses challenges in ensuring the reliability of evaluation results obtained on these datasets.\nTo address the potential overfitting of LLMs, particularly those specialized in coding tasks, and to ensure a reliable and accurate assessment of their capabilities, we construct a novel dataset, termed “BigTable-0.2k”. This dataset is an extension and augmentation of the BIRD dataset, which is a recently released and widely acknowledged benchmark for evaluating Text-to-SQL parsing.\n###table_1### Specifically, our construction process involves a systematic analysis of the original BIRD dataset, identifying queries of varying difficulty levels and involving different numbers of tables (1, 2, 3, and more than 3). We modify and expand these queries by altering table and column names, as well as filtering conditions, to create a more diverse set of challenges. In cases where the original dataset lacks sufficient examples with four or more tables (there are only 20 instances in BIRD-Dev dataset), queries that involved three tables are expanded to four. As shown in Table 2  ###reference_###, this process generates 50 new instances for each category, resulting in the “BigTable-0.2k” dataset. Moreover, each item in the dataset underwent mutual verification by at least two individuals to ensure the accuracy.\nAlthough the prevalence of queries involving more than three tables may not align with common real-world applications,\nthis approach allows for a more nuanced evaluation of LLMs’ cognitive abilities across a spectrum of sub-tasks.\nAdditionally, the “BigTable-0.2k” dataset retains the original BIRD dataset’s attributes, such as its large-scale and complexity, diversity of data sources, cross-domain applicability, and the requirement for external knowledge reasoning. Moreover, we standardize the dataset format to align with other benchmarks, by combining user NL questions  with external knowledge  to form new, contextually rich questions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "In this section, we formally evaluate the different sub-tasks within the Text-to-SQL process to determine the performance differences among various LLMs and provide recommendations for addressing specific task requirements."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text2SQL",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Zero-shot Prompting Optimization.",
            "text": "Unlike previous learning-based studies, the primary challenge in LLM-based Text-to-SQL is the design of an effective prompt template  (as introduced in Section 3.1  ###reference_###) for LLMs to generate accurate SQL queries, known as prompt engineering. Researchers have evaluated a variety of prompt templates Gao et al. (2023  ###reference_b13###). However, these representations lack uniformity in their structure, making it difficult to find out how a specific feature within a prompt template impact performance.\nTo address this issue, we investigate a more unified series of prompt templates. As shown in Listing LABEL:lst:DDL/SimpleDDL_prefix- LABEL:lst:Complete/Chat_postfix, these templates differ across three features:\nDDL/SimpleDDL prefix affects the representation of the database schema .\n“DDL” (Data Definition Language) encompasses the standardized language that includes commands for defining the structure and properties of a database, providing detailed information necessary for database creation, including column types and primary/foreign keys.\n“SimpleDDL” is simplified by only supplying table and column names.\nMD/HTML/Coding infix wraps the entire prompt template with Markdown syntax, HTML snippets and code comment blocks.\nComplete/Chat postfix indicates the task of either completing SQL statements based on the \"SELECT\" clause or directly answering questions.\nThese features are combined to form a complete prompt template , and more details of these representations can be found in Appendix A.1  ###reference_###.\nWe test these templates on Spider dev set. As shown in Table 3  ###reference_###, “SimpleDDL-MD-Chat” (see Listing LABEL:lst:SimpleDDL-MD-Chat) consistently outperforms all other prompts when applied to all 5 backbone LLMs.\nTo this end, we consistently utilize the prompt template “SimpleDDL-MD-Chat” throughout the subsequent evaluations in this paper.\nThe prompt template “SimpleDDL-MD-Chat” achieves optimal performance in the Text-to-SQL task."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 End-to-End Text2SQL evaluation.",
            "text": "This research conducts an end-to-end evaluation of the Text-to-SQL capabilities of various LLMs using the \"SimpleDDL-MD-Chat\" prompt template on the \"BigTable-0.2k\" dataset, with results depicted in Table 4  ###reference_###.\nComparison of Performance Across Different Models. The results demonstrate a clear performance hierarchy among the models, with SQLCoder, CodeLlama, InternLM, and InternLM2 consistently outperforming Llama2-Chat. This finding highlights the effectiveness of coding-specific models, such as SQLCoder and CodeLlama, in the Text-to-SQL domain. Additionally, certain general-purpose models, like InternLM and InternLM2, can achieve performance levels comparable to specialized models, even without fine-tuning for coding tasks.\nDifficulty Comparison Across Different Numbers of GT Tables.\nWe examine the query difficulty based on the number of GT tables involved. The results reveal a decrease in EX as the number of GT tables increases. Notably, the EX for models on queries with two GT tables is unexpectedly lower compared to those with three or more GT tables. This observation can be attributed to the fact that queries involving two GT tables have the highest average number of columns (see Table 2  ###reference_###).\nAs the number of tables and columns involved in user queries increases, the Text-to-SQL challenge for LLMs significantly escalates.\n###figure_3###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "SQL Debugging",
            "text": "In recent studies, researchers have demonstrated that LLMs possess self-reflection and self-correction capabilities similar to those of humans Yao et al. (2022  ###reference_b53###); Shinn et al. (2023  ###reference_b41###); Zhang et al. (2023a  ###reference_b57###).\nAdditionally, previous studies also investigate the potential of LLMs to debug the code they generate Chen et al. (2023  ###reference_b4###); Pourreza and Rafiei (2024  ###reference_b33###). In this section, we provide a comprehensive analysis of the performance of numerous SQL debugging methods across LLMs."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Debugging Dataset",
            "text": "We collect incorrect SQL queries generated by various LLMs in Section 4.1.2  ###reference_.SSS2### and visualize the distribution of their error information, as shown in Figure 3  ###reference_###.\nThe error information can be divided into two categories:\nSystem Error refers to syntax errors in the SQL statement, and the detailed system error information is generated by the Database Management System (DBMS), e.g., “syntax error” and “no such column”.\nResult Error indicates that the syntax of the SQL statement is correct, but the execution result does not match the ground truth.\nThe word cloud distribution reveals that “no such column” and Result Error are the primary areas of error concentration for all models. Additionally, more advanced models exhibit a greater proportion of the Result Error. This aligns with expectations, as powerful models are less prone to low-level System Errors.\nHowever, the concise nature of the error information in the Result Error category significantly hampers the debugging performance. Therefore, we propose a further detailed classification method.\nSpecifically, in addressing the Result Error category, we categorize these errors based on the logical construction of SQL statements. This classification is prioritized according to the logical structure within the SQL query. It is delineated in order into the following five subcategories:\nTable Query Error pertains to issues related to the selection of tables in the SQL query. It is further subdivided into three types: Excessive/Missing/Incorrect Tables, which respectively address scenarios where unnecessary tables are included, required tables are omitted, or the wrong tables are referenced.\nColumn Selection Error focuses on the appropriateness of column selection. Similar to the Table Query Error, it is broken down into Excessive/Missing/Incorrect Columns.\nJoin Columns Error examines the errors associated with JOIN operations.\nCondition Filter Error encompasses errors that occur in the conditions used to filter the data, including incorrect comparisons or misapplied filters.\nData Processing Error pertains to errors in the data processing stage, which includes aggregations, calculations, etc. applied to the data within the SQL query.\nDuring the categorization process, we employ rules to determine the first three error types and utilize LLMs to perform binary classification for the last two error types (see Appendix A.2  ###reference_###). The distribution of different subcategories within Result Error is shown in the bottom section of Figure 3  ###reference_###.\n###figure_4### ###figure_5###"
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Debug Evaluation",
            "text": "To assess the impact of different levels of information granularity on performance, we propose 5 distinct strategies for self-debugging, progressively incorporating more detailed information:\nRegenerate. Simply regenerate the SQL query with the same prompt in Section 4.1  ###reference_###. This setting acts as a baseline to eliminate the impact of model randomness.\nw/ Wrong SQL. Let LLMs generate a new SQL query based on the wrong SQL statement.\nw/ Wrong SQL + System_error_info. Provide the wrong SQL statement, the corresponding System Error information and the rough Result Error information.\nw/ Wrong SQL + All_error_info. Add detailed Result Error information for those SQL queries that are syntactically correct but semantically wrong.\nw/ Wrong SQL + All_error_info + Comment. Add manual annotations for all error information. See Appendix A.2  ###reference_### for a detailed prompt template.\nWhat is the most powerful information organization of self debug?\nAs shown in Figure 4  ###reference_###, it is evident that the self-debugging performance of LLMs exhibits an upward trend with the introduction of more granular error information.\nIn the absence of additional information, LLM does not possess the capability to regenerate correct answers. However, all models are able to comprehend fine-grained error information, whether it includes comments or not, and rectify their own mistakes.\nDetailed error information and corresponding annotations greatly enhance the capabilities of LLMs, enabling them to effectively correct errors.\nCan LLMs benefit from multi-round self debug? As shown in Figure 5  ###reference_###, substantial improvements in EX are achieved in the initial rounds of debugging, yet the performance gain become marginal later. This indicates that conducting 1-2 debugging rounds might strikes a favorable balance between performance improvement and economic efficiency. In addition, we analyze the distribution of detailed error types during the multi-round self-debugging process. As the debugging rounds advance, we observe a reduction in System Error and a slight rise in Result Error. This suggests that although syntax errors are fixed, the regenerated SQL statements may still have semantic errors. With each round of debugging, the generated statements tend to transition from System Error to Result Error before eventually converging towards correct SQL statements.\nMulti-round self-debugging aids in error correction for LLMs, but there exists a performance boundary, with 1-2 rounds of debugging being the optimal choice.\nCan an LLM debug the error incurred by other LLMs (general debugging)?\nAs depicted in Table 5  ###reference_###, we select two representative LLMs, SQLCoder and InternLM, to assess their general debugging capabilities.\nWhen debugging erroneous SQL statements generated by other LLMs, additional error information can potentially impair the performance.\nConversely, the most naive approach of simply regenerating the SQL statements often yields the best results. This highlights the differences between different LLMs.\nThe debugger LLMs may not encounter errors caused by other LLMs, and these error type informations might confuse them. Consequently, it is unlikely to achieve performance improvement through general debugging. However, the integration of results generated by different LLMs holds promise as a future research direction.\nThe performance of cross-LLM SQL debugging is inferior to the direct regeneration. A multi-agent approach that integrates outputs from different models shows great potential."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "SQL Optimization",
            "text": "Execution efficiency of SQL queries is a critical aspect, particularly in real-time systems that utilize large-scale databases.\nIn this section, we further explore whether LLMs are able to enhance the execution efficiency of correct SQL queries.\nFormally, the SQL optimization process Pirahesh et al. (1992  ###reference_b31###) involves transforming the initial SQL queries  into an optimized form, denoted as , with the goal of improving efficiency while maintaining identical results:\nwhere  and  are the\ncorresponding prompt template and the additional information used for SQL optimization,  represents the mapping function of the LLM .\nVES is commonly employed to evaluate the efficiency of SQL query execution. However, in practice, LLMs can sometimes rewrite a correct SQL query into an incorrect one, making it challenging to figure out if the main reason for the decline in VES is due to these incorrect rewrites or a decrease in the SQL execution efficiency.\nTo this end, we suggest adopting a complementary metric C-VES (Correct-VES):\nHere,  represent the set of accurate SQLs (see Appendix C.1  ###reference_### for detailed notations). C-VES is designed exclusively to validate the capability of LLMs to generate more efficient SQL queries, regardless of the potential drawback of rewriting correct SQLs into erroneous ones.\nDo LLMs have the capability for SQL self-optimization?\nTo the best of our knowledge, we are the first to consider utilizing LLMs for SQL optimization.\nSpecifically, we devise an extensive suite of prompts  curated to SQL optimization:\nwith : In this basic form, only original SQL statements are provided.\nw/ : Further incorporates the database schema  and the user question .\nw/ demo: Introduce few-shot demonstrations without explanations. Demonstrations are intuitively designed, incorporating common optimization rules, such as substituting “COUNT(*)” with “COUNT(<column_name>)”.\nw/ demo + comments: Add an explanation for the few-shot demonstrations. See Appendix A.3  ###reference_### for a detailed prompt template.\nSimpleDDL-MD-Chat-Efficiency: To avoid the accumulation of errors caused by multiple generations, this prompt template require LLMs to directly generate the most efficient SQL query statement based on user query.\nThe effectiveness of these SQL optimization methods are demonstrated in Table 6  ###reference_###.\nAlmost all two-stage methods experience a significant decrease in VES.\nIt can be attributed to the possibility of LLMs optimizing the correct SQL statements into incorrect ones, thereby resulting in a further decrease in accuracy. Even when considering only the correct results, the performance improvement in terms of execution efficiency brought by the optimized SQL statements is almost negligible.\nFurthermore, it is intriguing to note that directly instructing the LLM to generate efficient SQL statements appears to achieve improved accuracy. This suggests that placing higher demands on the LLM could yield surprisingly positive outcomes.\nIn-context learning methods present challenges in achieving effective SQL optimization with LLMs."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "SQL-to-Text",
            "text": "The goal of SQL-to-Text is to transform the SQL query back into its original natural language question Xu et al. (2018  ###reference_b51###); Ma et al. (2021  ###reference_b30###); Shu et al. (2021  ###reference_b42###).\nWhile it seems that SQL-to-Text cannot serve as a sub-task within the Text-to-SQL pipeline to improve the performance of End-to-End Text-to-SQL systems, employing this conversion as a supplementary step within the pipeline can indeed provide valuable insights. By converting the generated SQL statements back into text and juxtaposing these with the semantics of the original user questions, we can assess the accuracy of the SQL statements produced.\nIn addition, it can assist researchers in evaluating the semantic comprehension capabilities of different LLMs, thus facilitating the development of more effective Text-to-SQL methodologies.\nTo this end, we assess the performance of SQL-to-Text across different LLMs (See Appendix A.4  ###reference_### for prompt templates).\nThe selected metrics for evaluation encompass the F1 values of Rouge-1/2/L and BertScore, along with the application of LLM to assess the semantic coherence between the two texts.\nThe evaluation results are depicted in Table 7  ###reference_###. ChatGPT and InternLM2 demonstrate the highest performance, followed by InternLM, while Codellama and SQLCoder exhibit comparatively lower performance. This highlights that even in regards to semantic description of code, general-purpose models exhibit significantly stronger descriptive capabilities compared to coding-specific models.\nUtilizing a general-purpose model for semantic description of SQL statements is a better choice."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Schema Linking",
            "text": "Schema linking is recognized as a crucial prerequisite of generating correct SQL queries.\nIt involves aligning entity references in the question with the corresponding schema tables or columns, requiring the model to understand both structure and value of the database, as well as the the semantics of user questions.\nIn LLM-based Text-to-SQL, prior studies Dong et al. (2023  ###reference_b9###); Pourreza and Rafiei (2024  ###reference_b33###) design prompt instructions with in-context learning examples to enable LLMs to retrieve linked tables and columns, which are then used for the downstream Text-to-SQL task.\nHowever, none of these methods individually evaluate the performance of schema linking, and explicit evaluation metrics have yet to be established.\nMoreover, despite considerable advancements in semantic comprehension and generalization brought by LLMs, the performance of schema linking is still far from promising.\nIn this section, we aim to bridge these gaps by: (1) introducing a elaborately designed metrics to assess schema linking methods, (2) presenting a novel schema linking method “PreSQL”, which demonstrates superior performance, (3) conducting a comprehensive evaluation of a range of schema linking methods across various LLMs."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 Evaluation Metric for Schema Linking: RES",
            "text": "What schema linking method is considered good? A straightforward goal of schema linking is that the GT tables should be retrieved as much as possible.\nHowever, due to the ambiguity inherent in natural language and the potential semantic similarities between candidate tables, retrieving more GT tables typically comes at the cost of a higher redundancy, known as the precision-recall trade-off.\nAs discussed in Pourreza and Rafiei (2024  ###reference_b33###), excessive table retrieval can introduce redundant joins between tables, potentially impairing the EX of Text-to-SQL generation.\nTherefore, the objective of schema linking is to retrieve all GT tables while avoiding the retrieval of excessive tables (with minimal redundancy).\nTo evaluate this, we design a comprehensive metric called Retrieval Efficiency Score (RES), defined as:\nwhere  and  denote the set of GT tables and retrieved tables for the -th instance, respectively,  refers to the scale of a set.\nWe emphasize that the RES serves as a more appropriate metric for evaluating schema linking than the F1-score. This is because it aligns the principle that recalling all GT tables is more important than increasing the precision of retrieval, as the former constitutes a prerequisite for generating correct SQL queries."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Schema Linking Evaluation",
            "text": "The methods we evaluated are as follows (see Appendix A.5  ###reference_### for the full prompt of these methods):\nZero Shot: A schema linking prompt proposed in C3 Dong et al. (2023  ###reference_b9###), which instructs the LLM to rank all the tables from relevant to irrelevant in a zero-shot manner.\nFew Shot: Instructing the LLM to retrieve only the most important tables with few-shot demonstration, presented in DIN-SQL Pourreza and Rafiei (2024  ###reference_b33###).\nPreSQL: First, we employ the zero-shot Text-to-SQL described in Section 4.1  ###reference_### to generate a preliminary SQL query. From this preliminary SQL, table and column entities are parsed to serve as the retrieval results for schema linking.\nFew Shot + PreSQL: This approach takes the union of the retrieval results from both the Few Shot and PreSQL methods, aiming to leverage the strengths of each.\nNote that we organize the information of the database schema in a way similar to \"SimpleDDL-\" in the prompt of all the methods mentioned above, which ignores the information about foreign keys.\nHowever, as argued in Wang et al. (2019  ###reference_b46###), foreign keys embody features of known schema relations and provide importance clues for understanding the database structure.\nTo this end, we conduct experiments under both settings, w/ and w/o foreign keys, to investigate how incorporating foreign keys in the prompt influences the performance of schema linking. Results are demonstrated in Table 8  ###reference_### (refer to Appendix B  ###reference_### for results on detailed metrics like Exact Match & Subset Match).\nWhich method achieved the best performance in schema linking?\nIt can be seen that code-specific models excel in performance when utilizing the PreSQL approach, whereas general-purpose models yield optimal results through the Few-Shot + PreSQL method. This aligns with our expectations, as these two types of models excel in coding tasks (see Section 4.1  ###reference_###) and semantic understanding tasks (see Section 4.4  ###reference_###), respectively.\nCan foreign key information facilitate schema linking?\nThe introduction of foreign key information yield improved performance across all methods and all LLMs. This is evident since a valid JOIN operation in SQL queries is typically based on foreign keys. The foreign key information helps the model retrieve more ground truth tables by indicating all potential table pairs involved in a JOIN operation.\nForeign key information is capable of advance the performance of schema linking. PreSQL yields the highest performance on coding-specific models, and integrating the results from Few Shot can further enhance performance on general-purpose models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we conduct a systematic benchmarking of the various sub-tasks within the Text-to-SQL pipeline, encompassing Text-to-SQL, SQL-Debugging, SQL-Optimization, SQL-to-Text, and Schema-Linking.\nOur comprehensive evaluation involves six distinct LLMs, spanning both general-purpose and coding-specific models.\nWe focus on determining the optimal prompt templates for each task, assessing performance variations among different approaches, and identifying the distinct capabilities and limitations of each LLM.\nThe results of the study demonstrate notable performance variations across the LLMs, underscoring the significance of careful model selection and prompt engineering in attaining optimal outcomes in text-to-SQL tasks.\nOur benchmarking provides a meticulous perspective on the pipeline, equipping the research community with strategies to improve the semantic understanding and computational performance of LLMs. This advancement contributes to the development of more reliable Text-to-SQL systems."
        }
    ]
}