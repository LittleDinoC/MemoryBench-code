{
    "title": "Assessing Adversarial Robustness of Large Language Models: An Empirical Study",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, but their robustness against adversarial attacks remains a critical concern. We presents a novel white-box style attack approach that exposes vulnerabilities in leading open-source LLMs, including Llama, OPT, and T5. We assess the impact of model size, structure, and fine-tuning strategies on their resistance to adversarial perturbations. Our comprehensive evaluation across five diverse text classification tasks establishes a new benchmark for LLM robustness. The findings of this study have far-reaching implications for the reliable deployment of LLMs in real-world applications and contribute to the advancement of trustworthy AI systems.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, the field of artificial intelligence has witnessed a remarkable surge in the development and application of Large Language Models (LLMs). These models, such as ChatGPT (OpenAI, 2023a  ###reference_b22###), GPT-4 (OpenAI, 2023b  ###reference_b23###), and Llama-2 (Touvron et al., 2023  ###reference_b29###), have demonstrated exceptional performance in various natural language understanding and generation tasks (Zhao et al., 2023a  ###reference_b42###). The success of LLMs can be attributed to the innovative training techniques employed, including instruction tuning, prompt tuning, Low-Rank Adaptor (LoRA) (Hu et al., 2021  ###reference_b11###; Dettmers et al., 2022  ###reference_b6###). These advances have made it possible to fine-tune and infer models like Llama-2-7B on consumer-level devices, thereby increasing their accessibility and potential for integration into daily life.\nHowever, despite their impressive capabilities, LLMs are not without limitations. One significant challenge is their susceptibility to variations in input types, which can lead to inconsistencies in output and potentially undermine their reliability in real-world applications. For example, when faced with ambiguous or provocative prompts, LLMs may generate inconsistent or inappropriate responses. To address this issue, several studies have been conducted to assess the robustness of LLM models (Zhu et al., 2023  ###reference_b44###; Wang et al., 2023d  ###reference_b35###). However, these efforts often overlook the importance of re-fine-tuning the models and conducting comprehensive studies of adversarial attacks with known adversarial sample generation mechanisms when full access to the model weights, architecture, and training pipeline is available (Guo et al., 2021  ###reference_b10###; Wallace et al., 2019  ###reference_b30###).\nIn this paper, we present an extensive study of three leading open-source LLMs: Llama, OPT, and T5. We evaluate the robustness of various sizes of these models across five distinct NLP classification datasets. To assess their vulnerability to input perturbations, we employ the adversarial geometry attack technique and measure the impact on model accuracy. Furthermore, we investigate the effectiveness of commonly used methods in LLM training, such as LoRA, different precision levels, and variations in model architecture and tuning approaches.\nOur work makes several notable contributions to the field of LLM evaluation and robustness:\nWe introduce a novel white-box style attack approach that leverages output logits and gradients to expose potential vulnerabilities and assess the robustness of LLMs.\nWe establish a benchmark for evaluating the robustness of LLMs by focusing on their training strategies, setting the stage for future research in this domain.\nOur comprehensive evaluation spans five text classification tasks, providing a broad perspective on the capabilities and limitations of the models across diverse applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robustness in NLP",
            "text": "With the rapid advancement in NLP research, its applications have become increasingly prevalent. This ubiquity underscores the growing need for reliable NLP systems that can effectively counteract malign content and misinformation. A seminal work (Jia & Liang, 2017  ###reference_b12###)\nhighlighted the vulnerabilities of NLP systems to adversarial attacks.\nThere are some works about various input perturbations, which could be categorized into three groups: character level, word level, and sentence level (Wei & Zou, 2019  ###reference_b36###; Karimi et al., 2021  ###reference_b13###; Ma, 2019  ###reference_b16###). At the character level, adversarial attacks focus on altering individual characters within a given text.  (Ebrahimi et al., 2018  ###reference_b9###) introduced HotFlip, utilizing gradient information to manipulate characters within text. (Li et al., 2019  ###reference_b14###) took a different approach by identifying words and modifying their characters. At the word level, adversarial strategies revolve around replacing specific words within the content. For instance, (Alzantot et al., 2018  ###reference_b1###) employs evolutionary algorithms to swap out words with their synonyms.  (Zhang et al., 2019  ###reference_b38###) utilized probabilistic sampling to generate adversarial examples. Furthermore, some researchers have explored adversarial tactics at the sentence level. (Jia & Liang, 2017  ###reference_b12###) suggested a method that introduces an extraneous sentence to the primary content, aiming to mislead reading models. On the other hand, (Peters et al., 2018  ###reference_b24###) adopted a new approach, where they employed an encoder-decoder framework to rephrase entire sentences. Recently,  (Wang et al., 2023a  ###reference_b32###) evaluated the robustness and trustworthiness of GPT-3.5 and GPT-4 models, revealing vulnerabilities such as the ease of generating toxic and biased outputs and leaking private information. Despite GPT-4â€™s improved performance on standard benchmarks, it is more susceptible to adversarial prompts, highlighting the need for rigorous trustworthiness guarantees and robust safeguards against new adaptive attacks.\nHowever, the landscape of NLP research is ever-evolving. With the introduction of more sophisticated models boasting novel architectures and training methodologies, there is an growing need to assess the robustness of these newer models. This is especially true for LLMs, which present unique challenges and opportunities in the realm of robustness research."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "LoRA innovates in fine-tuning pretrained language models for specific tasks, addressing the inefficiency of full fine-tuning in increasingly large models. By inserting trainable rank decomposition matrices into each layer and freezing the original model weights, LoRA significantly reduces the number of parameters requiring training.\nQuantization in large language models (LLMs) reduces the model size by lowering weight precision, with 8-bit precision presenting challenges due to errors from quantizing large-value vectors. These errors are pronounced in transformer architectures, requiring mixed-precision decomposition. This involves identifying outliers using a threshold, processing them in fp16, and quantizing the rest of the matrix at 8-bit precision. The two parts are then combined. The approach, exemplified by LLM.int8(), aims to make large models more accessible, trading off some performance for significant size reduction.\nQuantized Low-Rank Adapters (QLoRA) (Dettmers et al., 2023  ###reference_b7###) introduce an efficient technique to fine-tune large language models by significantly lowering memory requirements. QLoRA combines 4-bit quantization with Low-Rank Adapters, freezing the parameters of a compressed pretrained language model."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Open-source Large Language Models",
            "text": "We evaluate the following open-source large language models used in our experiments, as shown in Table 1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Fine-tuning Techniques",
            "text": "We apply the following fine-tuning techniques in our study.\nLoRA innovates in fine-tuning pretrained language models for specific tasks, addressing the inefficiency of full fine-tuning in increasingly large models. By inserting trainable rank decomposition matrices into each layer and freezing the original model weights, LoRA significantly reduces the number of parameters requiring training.\nQuantization in large language models (LLMs) reduces the model size by lowering weight precision, with 8-bit precision presenting challenges due to errors from quantizing large-value vectors. These errors are pronounced in transformer architectures, requiring mixed-precision decomposition. This involves identifying outliers using a threshold, processing them in fp16, and quantizing the rest of the matrix at 8-bit precision. The two parts are then combined. The approach, exemplified by LLM.int8(), aims to make large models more accessible, trading off some performance for significant size reduction.\nQuantized Low-Rank Adapters (QLoRA) (Dettmers et al., 2023  ###reference_b7###  ###reference_b7###) introduce an efficient technique to fine-tune large language models by significantly lowering memory requirements. QLoRA combines 4-bit quantization with Low-Rank Adapters, freezing the parameters of a compressed pretrained language model."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "###figure_1###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Adversarial Attack",
            "text": "In this study, our primary concern is text classification. Following the work by  (Meng et al., 2022  ###reference_b19###), we consider a sample sentence  containing  words, and its corresponding category label is . Our textual classification system is built upon  LLMs, represented as , coupled with a prompt indicating the categorization task, denoted as . In a formal sense:\n, where  stands for the given answer. The prediction is accurate when  equals .\nAn adversarial attack based on word replacement processes the original sample  to produce an adversarial version  by replacing the -th word  in  with an alternative word . To ensure that the original sample  and its adversarial counterpart  maintain semantic similarity, prevalent methodologies typically employ synonymous terms for replacements."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Geometry Attack Methodology",
            "text": "In our research, we extend the basic principles of adversarial attacks in the context of LLMs. Our focus is on exploiting geometric attacks (Meng & Wattenhofer, 2020  ###reference_b18###; Meng et al., 2022  ###reference_b19###) to assess the vulnerability of LLMs to adversarial perturbations. We propose a systematic methodology grounded in geometric attack insights. The following sections detail the steps of our approach:\n1) Gradient Computation for Influence Analysis: We commence by calculating the gradients of the generation loss  with respect to the embeddings  of input sentence . The cross entropy loss  measures the dissimilarity between the prediction and label examples in the output space. This computation is essential for all words, including those segmented into sub-tokens. For such words, gradients are computed for each sub-token and subsequently averaged. This initial step is crucial for identifying the words that exert significant influence on . We determine the gradient of  with respect to the embedding vector . This step determines the direction in which  should be adjusted to maximize the increase in the loss . The resulting gradient vector is denoted as .\n2) Selection of Candidate Words: Suppose we select a target word  from step 1. Utilizing the DeepFool algorithm (Moosavi-Dezfooli et al., 2016  ###reference_b20###), we identify potential replacement words, forming a candidate set . Candidates are filtered based on their cosine similarity to , with those below a defined threshold  being excluded. This process ensures that only semantically similar and relevant candidates are considered.\n3) Optimal Word Replacement and Projection Analysis: After replacing  with each candidate word, we compute the new text vectors . For each vector, we define the delta vector  as . The projection of  onto  is calculated as . The optimal replacement candidate  is selected based on criterion . This ensures that the chosen word  induces the largest possible projection  onto the gradient vector .\n4) Iterative Process for Enhanced Adversarial Strength: The selected word  replaces  in , updating  to . This iterative procedure is repeated for  cycles, where  is an adjustable parameter in our methodology. Throughout these iterations, an increase in  should be observed, indicating a continuous enhancement in the adversarial effectiveness of the altered input.\nThrough this methodically structured process, our research aims to uncover and analyze potential vulnerabilities in LLMs. We refined our methodology to enable prompt fine-tuning for attack generation tasks, expanding its application beyond the previously limited scope of classification tasks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment Settings",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiment Pipeline",
            "text": "This section introduces our methodology for evaluating the robustness of pre-trained LLMs against adversarial attacks. The procedure comprises three principal stages:\n1) Model Fine-Tuning: We fine-tune a pre-trained language model on target dataset with different fine-tuning techniques as described in Sec. 3.2  ###reference_###, evaluating its accuracy on the corresponding validation set to establish a performance baseline.\n2) Adversarial Attack Assessment: The fine-tuned model undergoes adversarial attacks described in Sec. 4.2  ###reference_###, and its performance is assessed on a test dataset altered with adversarial examples.\n3) Robustness Evaluation: We compare the modelâ€™s accuracy before and after the adversarial attacks to assess its robustness and vulnerability to such manipulations."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Datasets",
            "text": "To evaluate the modelâ€™s performance under various tasks and its resilience to attacks, we employed five classification datasets, categorized into binary and multi-class classifications. For binary classification, the datasets include IMDB (Maas et al., 2011  ###reference_b17###), MRPC (Dolan & Brockett, 2005  ###reference_b8###), and SST-2 (Socher et al., 2013  ###reference_b27###), and for multiclass classification, AGNews (Zhang et al., 2015  ###reference_b41###) and DBpedia (Auer et al., 2007  ###reference_b2###) are used. We will provide a more detailed introduction to these tasks/datasets in Appendix A.2  ###reference_###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Evaluation Metrics",
            "text": "We assess our modelâ€™s robustness and efficacy using four principal metrics, as described in Table 2  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "In this section, we conduct extensive experiments to evaluate the robustness of LLMs across five different datasets. These investigations are guided by three key research questions (RQ):\nRQ1: How does the robustness of variously sized models differ under adversarial attacks across distinct tasks?\nRQ2: Do contemporary training techniques for LLMs influence their performance and robustness?\nRQ3: How does the model architecture (e.g., fine-tuning with a classification head vs. prompt tuning), affect the robustness of the model?"
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Model Size (RQ1)",
            "text": "In this section, we analyze the performance metrics of various models across multiple tasks. The datasets under examination include IMDB, SST-2, MRPC, AGNews, and DBPedia. We measure the performance and robustness of LLMs with the metrics Acc, Acc/attack, ASR, and Replacement Rate described in Table 2  ###reference_###.\nThe results from the IMDB dataset in Table 3  ###reference_### reveal distinct performance variations among different model architectures. In the T5 Series, accuracy generally improves with increasing model size, from 60m to 11b parameters, but the relationship is nonlinear. This suggests that while larger models tend to be more accurate, the accuracy does not increase uniformly with model size. Furthermore, the resilience of these models to adversarial attacks does not follow a simple inverse relationship with model size. The larger T5-11b model shows a more noticeable decrease in accuracy under attack conditions.\nFor the OPT models, a similar upward trend in accuracy is observed with increasing model size, but the Attack Success Rate (ASR) is lower, suggesting better resistance to attacks. In comparison, the Llama models demonstrate superior performance in both accuracy and robustness against attacks.\nFrom Table 3  ###reference_### on the SST-2 dataset, there are distinct performance trends. The T5-11b, achieves the highest accuracy of 0.9656. However, its persistence to adversarial attacks is not highest. Notably, the highest ASR within the T5 series is recorded for the T5-770m model, indicating a trade-off as model size increases. In the case of the OPT series, the OPT-6.7b model stands out. However, similar to the IMDB dataset, this model also shows a significant decline in accuracy but more robust than T5 models. One more observation in the OPT series is the overall decrease in ASR with increasing model size, but this trend is disrupted at the 13b parameter mark, where an anomalous increase in ASR is observed. The Llama models, demonstrate consistently high accuracy. It also presents lower ASR compared to T5 models but similar performance to OPT models. For SST-2, the ASR of T5 models exhibit a trend entirely contrary to that observed for MRPC. It reaches its minimum at the T5-770m model. For the OPT models, although their ASR is much lower compared to the T5 series, there is a consistent decrease in ASR as the size of the OPT models increases. Regarding the Llama models, the 7b model slightly outperforms the 13b in terms of accuracy and ASR.\nIn Tables 4  ###reference_###, analyzing results from multi-class classification tasks, a distinct pattern emerges. These datasets reveal enhanced stability against synonym substitution attacks. For T5 models, the data shows a lower ASR on these tasks compared to binary datasets, suggesting a better resistance to attacks in complex classification scenarios. In contrast, OPT and Llama models exhibit a higher ASR on the AGNews and DBpedia14 datasets. Another result is that for both T5 and OPT series, there is a marked decline in ASR around the 770 million or 1 billion parameter threshold. This indicates an increased robustness and better handling of adversarial attacks with the scale-up of model size."
        },
        {
            "section_id": "6.1.1",
            "parent_section_id": "6.1",
            "section_name": "6.1.1 Analysis",
            "text": "When examining the accuracy of the model, we observed a trend where the accuracy gradually increases with the growth in model size. However, after reaching a certain size threshold, the accuracy tends to saturate, stabilizing around specific values. This phenomenon is particularly pronounced when tested on datasets like DBpedia. When comparing different models operating at the same parameter scale, their performances were found to be quite similar, without any significant disparities.\nHowever, the experiments related to robustness revealed more distinct differences. From Figure  2(a)  ###reference_sf1###, Figure 2(b)  ###reference_sf2### and Figure 2(c)  ###reference_sf3###, we have more intuitive results. Observing the performance of a uniform model across various datasets, we made the following observations:\nT5 Model: As the size of the T5 model increases, its ASR gradually decreases. This suggests that larger models, with more parameters, tend to have a deeper understanding of language. As a result, they can maintain stronger stability in the face of various disturbances. However, on datasets like MRPC and SST-2, there were noticeable fluctuations in performance. One possible explanation for this is that as the model size grows, the words selected based on the modelâ€™s gradient become more precise and have a more significant impact on the results. This introduces a trade-off related to model size.\nOPT Model: For the OPT model, a similar trend was observed across most datasets. As the model size increased, its robustness generally improved, aligning with the observations made for the T5 model.\nLlama Model: For the Llama model, the differences in performance between the two sizes were minimal. This suggests that the size variation did not significantly influence the modelâ€™s robustness.\nHowever, when comparing different models, the disparities become even more pronounced. It is obvious that the T5 modelâ€™s ASR and replacement rate are significantly higher than those of OPT and Llama. This indicated that Decoder-only Causal LMs have higher robustness against encoder-decoder architectures under synonym substitution adversarial attacks.\n###figure_2### ###figure_3### ###figure_4###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "LLMs Fine-tuning Techniques (RQ2)",
            "text": ""
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Instruction Tuning",
            "text": "To study the impact of instruction tuning on model robustness, we compared the performance of Flan-T5 with the standard T5. The Flan-T5 is an advanced variant of T5 that has undergone instruction tuning across over a thousand downstream tasks. In contrast, the traditional T5 was not trained with such an extensive procedure.\nBased on our experimental results, as shown in the table, there is a significant decline in accuracy for both T5 and Flan-T5 under adversarial attacks. This observation indicates that models, irrespective of whether they have undergone instruction tuning, remain susceptible to adversarial manipulations. Furthermore, consistent with our previous findings, we noticed that as the model size increases, the attack success rate tends to decline.\nInterestingly, as shown in Fig 3  ###reference_###, our results indicated that Flan-T5 exhibits a higher ASR than the standard T5. This suggests that models subjected to instruction tuning, like Flan-T5, can be more easily compromised. We hypothesize the primary reason for this observation:\nThe instruction tuning process for Flan-T5 encompassed datasets similar to IMDB. This might have rendered the model with a deeper understanding of tasks related to this data. As a result, attackers could more easily pinpoint words in the input that were influential and susceptible to replacement.\n###figure_5### ###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Precisions",
            "text": "In machine learning, balancing model size with precision is crucial. Model size indicates capacity, and precision affects information granularity. Larger models typically perform better but require more computational resources. Techniques like quantization and precision adjustments help deploy these models more efficiently. We studied the impact of precision settings on the robustness of T5-770m and OPT-1.3b models by comparing their performance under various precisions.\nFor the T5-770m and OPT-1.3b models, itâ€™s clear that as precision changes from fp16 to int4, there isnâ€™t a significant drop in their inherent accuracy. This indicates that models can handle reduced precision without compromising their general performance drastically. Whatâ€™s more, across different precision settings, the attack success rate for the T5-770m models remains fairly higher, which shows the same conclusion as in 6.1  ###reference_###. However, the precision settings do not show a consistent pattern of influence on the ASR and replacement rate.\nIn essence, while different models exhibit different robustness against adversarial attacks, the precision settings do not play a significant role in this robustness."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 LoRA",
            "text": "As mention in Sec 3.2  ###reference_.SSS0.Px1###, LoRA has been a groundbreaking approach, bringing about significant reductions in memory requirements during model training. In this case, the potential trade-off in question is model robustness.\nFor our investigation, we selected the T5-770m, OPT-1.3b, and OPT-2.7b models. Experiments were conducted under two conditions for each model: with and without the application of LoRA. The IMDB dataset served as our benchmark for this analysis.\nThe experiments show that adversarial attacks significantly reduce accuracy across all models, regardless of LoRAâ€™s use. However, crucially, both the attack success rate and replacement rate, key measures of resilience against adversarial tactics, were unaffected by LoRA. This indicates that while LoRA enhances optimization, it doesnâ€™t negatively impact the modelâ€™s defense against adversarial attacks, providing optimization benefits without sacrificing robustness."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Model Architectures (RQ3)",
            "text": "The architecture of a modelâ€™s output space significantly influences its performance and resilience against adversarial attacks. For models with a classification head, the output is simplified to a binary decision, contrasting with OPT models without such a head, which must identify â€™negativeâ€™ or â€™positiveâ€™ labels from a vast vocabulary. This distinction impacts the modelâ€™s accuracy.\nOur data shows that smaller models with a classification head are more accurate than headless ones due to their simplified output space, which aids decision-making, especially in models with limited processing power. Moreover, models with a head reach their peak performance faster, achieving accuracy saturation more quickly.\nHowever, an intriguing observation is the heightened attack success rate for models with a classification head. On the surface, this suggests that launching adversarial attacks against these models is a more straightforward task.One main factor contributes to this vulnerability is DeepFoolâ€™s efficacy with last layer FFN: In such models, DeepFool can more readily discern the optimal direction for launching its attack, amplifying the ASR. This marked efficiency underscores a reduced robustness in these models against adversarial intrusions.\n###figure_9###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper utilized a novel geometric adversarial attack method to assess the robustness of leading LLMs, utilizing advanced fine-tuning techniques for task-specific model adaptation. Our groundbreaking approach revealed that these models exhibit variable sensitivity to adversarial attacks, influenced by their size and architectural differences. This indicates inherent vulnerabilities in LLMs, yet suggests potential resilience in certain configurations. Contrary to expectations, LLM-specific techniques did not markedly reduce robustness. Future research could explore models like RLHF and model parallelism approaches within this framework. Additionally, the evolution of more complex adversarial attacks promises deeper insights into LLM strengths and weaknesses."
        }
    ]
}