{
    "title": "Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval",
    "abstract": "In today’s digital world, seeking answers to health questions on the Internet is a common practice. However, existing question answering (QA) systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel questions. Our study focuses on the open-domain QA setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed as a trustworthy collection of medical research documents, we answer health questions from three diverse datasets. We modify different retrieval settings to observe their influence on the QA pipeline’s performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the digital era, using the Internet to search for health information has become a prevalent behavior Jia et al. (2021  ###reference_b21###). Users turn to seek health advice online due to its ease of access, wide coverage of information, convenience of searching, interactivity, and anonymity Neely et al. (2021  ###reference_b32###). Health information sought online includes anything regarding the symptoms and treatments of different diseases. In general, health information seeking can lead to enhanced patient involvement in medical decision-making, improved communication with care providers, and improved quality of life Rutten et al. (2019  ###reference_b39###). Nevertheless, finding trustworthy and relevant evidence in abundant online content remains an open challenge Battineni et al. (2020  ###reference_b7###). Especially in the medical field, clinical recommendations can change with time, so finding the latest evidence is essential for reliable answers.\n###figure_1### Interacting with online search engines and conversational systems is done with question answering (QA). Technical solutions based on Machine Learning (ML) and Natural Language Processing (NLP) aim to automate this task and provide users with reliable answers to their inquiries. The purpose of QA systems is multi-fold: it helps scientists verify their research hypothesis by finding related studies, it allows lay users to find answers to their everyday health concerns, and enables factuality assessment of generative language models by fact-checking their responses over trusted evidence Jin et al. (2021  ###reference_b22###); Vladika and Matthes (2023a  ###reference_b47###).\nWhile QA can work with answering questions over a provided document, we are focusing on the more realistic and challenging problem of open-domain question answering, where extensive collections of documents with diverse topics have to be quried to find the relevant evidence Chen and Yih (2020a  ###reference_b11###). The open-domain QA systems usually consist of two main components, a retriever and a reader Zhu et al. (2021b  ###reference_b56###). The retriever’s task is finding the relevant documents that will serve as the main source of evidence for answering the question. The reader (QA module) performs the reasoning process between the question and found evidence, and produces the final answer. While both components are essential for the system, we posit that the retrieval is a more challenging part, considering that the QA module receives the input from it and the quality of the final answer depends on the retrieved documents Sauchuk et al. (2022  ###reference_b40###).\nRetrieving credible evidence documents relevant to the query ensures the final output’s quality. This is true for both the retrieval-powered text classification tasks and recently popular retrieval-augmented generation (RAG) approaches Cuconasu et al. (2024  ###reference_b15###). While much progress has been made in open-domain QA, addressing the challenges in retrieval settings still needs to be explored. These include assessing the quantity of documents needed to be retrieved for a reliable answer, the amount of evidence passages extracted from them, and the quality of the documents themselves, such as their recency and strength of findings. Figure 1  ###reference_### shows an example of a health question answered with two different retrieved documents – the more recent one has more up-to-date knowledge and findings.\nTo bridge this research gap, in this study, we perform an array of experiments to test the predictive performance of an open-domain QA system with different evidence retrieval strategies. We use three diverse datasets of biomedical and health questions, which contain discreet labels like yes and no as their final answers, and use their gold labels as ground truth. We use the large collection of 20 million biomedical research abstracts from PubMed as the knowledge base for evidence retrieval. By keeping the reader (answering module) fixed, we only vary the different retrieval aspects and measure the change in the QA performance as measured by classification metrics precision, recall, and F1. The settings we explore include the number of documents retrieved and sentences extracted from them, the articles’ publication year, and their number of citations. Our findings demonstrate that the QA performance is improved by accounting for the amount and quality of the retrieved documents.\nTo summarize, our research contributions are:\nWe evaluate the performance of an open-domain QA pipeline for health questions, using biomedical research papers as evidence source, concerning the different number of documents retrieved and sentences extracted from them.\nAdditionally, we evaluate the influence of the evidence quality parameters like year of publication and number of citations on the final predictive performance of the QA system, showing that time-aware evidence retrieval leads to improved performance.\nFinally, we take a deeper look into the results and provide insights with a qualitative analysis. We report on the problem of evidence disagreement and provide future directions on developing better health question answering systems to be deployed in the future.\nWe make our code and datasets publicly available in a GitHub repository.111https://github.com/jvladika/Improving-Health-QA  ###reference_th-QA###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "In this section, we outline the work related to our study."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Open-Domain Fact Verification and QA",
            "text": "Considering that the datasets we use only contain questions with discrete (yes/no) answers, our work is related to the task of automated fact verification (fact-checking). This task aims to verify the veracity of a factual claim based on credible evidence that supports it, refutes it, or does not provide enough information Guo et al. (2022  ###reference_b19###). Recent years have seen a rise in fact-checking datasets focusing on scientific knowledge, in particular health and medicine Vladika and Matthes (2023a  ###reference_b47###).\nWhile fact verification literature often works in a closed-domain setting with evidence documents provided, some recent work also explores the open-domain setting. Wadden et al. (2022  ###reference_b52###) observe significant performance drops in F1 scores of final verdict predictions when increasing the evidence corpus from a few thousand to half a million documents. Pugachev et al. (2023  ###reference_b37###) analyze how well consumer-health questions can be answered with built-in search engines of PubMed and Wikipedia. Expanding the scope even more, Vladika and Matthes (2024  ###reference_b49###) compare the performance of semantic search and BM25 over PubMed and Wikipedia, as well as Google search, for verifying biomedical and health questions.\nSome studies have analyzed the influence of time and quantity in evidence retrieval on downstream tasks. Allein et al. (2021  ###reference_b2###) trained time-aware evidence ranking models for time-sensitive news claims and show performance improvement. Likewise, Schlichtkrull et al. (2024  ###reference_b41###) constructed a dataset where evidence for given claims only appeared after the claim itself, thus eliminating temporal leaks. Regarding the document quantity, Oh and Thorne (2023  ###reference_b34###) analyze the influence of the number of retrieved evidence passages on the QA performance over two general QA datasets, showing that the performance often actually drops with the increasing number of retrieved snippets.\nTo the best of our knowledge, our paper uses the largest document collection so far for open-domain health QA by indexing the entire PubMed corpus of more than 20 million articles. Likewise, it is also the first study to test the influence of the number of documents retrieved on the final QA performance instead of fixing it to a certain number, like the commonly found 5 Thorne et al. (2018  ###reference_b45###) or 6 Wadden et al. (2022  ###reference_b52###); as well as testing the influence of the different number of sentences retrieved. While there has been existing research on time-aware evidence ranking in fact verification for news claims, our work is first to explore the temporal aspect for biomedical questions, as well as other evidence quality aspects like the number of citations of retrieved publications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Foundations",
            "text": "In this section, we explain the foundations of the study, including the used datasets, the evidence corpus, and the structure of the used QA system."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "We chose three datasets of biomedical and health claims in English, built for different purposes. We only use the questions and final labels (answers) from the datasets in our experiments. While the datasets provide the gold evidence passages used to derive the answers, we do not utilize them since the idea of our open-domain QA setting is that the retriever component has to discover the relevant evidence in a large document corpus.\nHealthFC Vladika et al. (2023  ###reference_b50###) is a question-answering and fact-checking dataset focusing on consumer health questions and common topics users search health advice online for. It includes diverse topics like dietary supplements, heart and lungs, reproductive health, cancer, and mental health. Medical practitioners manually answered and verified all the questions using the evidence from systematic reviews and clinical trials. There are 750 questions in total, out of which 205 are supported, 122 are refuted, and for 433 questions, there is not enough information (NEI) to answer. We use two variants of the dataset: HealthFC-3, which has all 750 claims and all three classes; and HealthFC-2, which only has 327 supported and refuted claims with two classes.\nBioASQ-7b Nentidis et al. (2020  ###reference_b33###) is a biomedical question-answering dataset constructed by biomedical researchers and designed to reflect the real information needs of biomedical experts. It is part of the ongoing challenge of the same name, focusing on biomedical semantic indexing and question answering. The evidence for answers comes from biomedical research publications, i.e., the same corpus of PubMed used in our study. Other than only exact answers, the BioASQ dataset also includes ideal answer summaries. The 7b version of the dataset we use has 745 claims, of which 614 are supported (\"yes\"), and 131 are refuted (\"no\").\nTREC-Health (Pugachev et al., 2023  ###reference_b37###) is a dataset of 117 popular health questions originating from two TREC shared challenges. TREC is an ongoing series of workshops centering on challenges in accurate information retrieval Voorhees et al. (2005  ###reference_b51###). The questions stem from two shared tasks: the TREC 2019 Decision Track Abualsaud et al. (2020  ###reference_b1###) and the TREC 2021 Health Misinformation Track Clarke et al. (2021  ###reference_b13###), both focusing on challenges with incorrect search engine results for health (mis)information. Questions cover common consumer health concerns, similar to HealthFC, but the two datasets do not overlap. The dataset consists of 113 questions, of which 61 are supported (\"yes\") and 52 refuted (\"no\").\nTable 1  ###reference_### gives an overview of the four datasets. With HealthFC and TREC-Health, we aim to target common health questions users would pose to a QA system, while BioASQ is selected in order to explore how do the QA results change for more complex, expert-geared biomedical questions.\neveryday health\neveryday health\nbiomedical research\nconsumer health"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evidence Corpus",
            "text": "We approach the QA task in the open-domain setting, meaning that evidence is unknown when the question is posed and must first be discovered in a vast evidence collection. Given that we work with medical and health-related questions, we chose a collection of biomedical research publications as the source of evidence.\nOur evidence corpus originates from PubMed, a large and trustworthy knowledge base of biomedical research publications Canese and Weis (2013  ###reference_b10###). Considering that the full text of most of these publications is not freely accessible, we use only the abstracts, which are always available. This does not hinder the performance since medical abstracts often already include a verdict on their main research hypothesis. The US National Library of Medicine provides every year MEDLINE, a snapshot of currently available abstracts in PubMed that is updated once a year. We used the 2022 version found on the official website.222https://www.nlm.nih.gov/databases/download/pubmed_medline.html  ###reference_/pubmed_medline.html### While this yields 33.4M abstracts, we pre-processed the data by removing any non-English papers, papers with no abstracts, and papers with unfinished abstracts, which yields 20.6 million abstracts."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "QA System",
            "text": "The question-answering system used for our experiments is in the form of a pipeline, based on the pipeline system from Vladika and Matthes (2023b  ###reference_b48###). This pipeline consists of two main parts: a retriever and a reader. The process of question answering is done sequentially, by first retrieving the evidence, performing reasoning over it, and finally producing a final answer. Our experiments focus on changing the different aspects of the retriever while keeping the reader completely fixed. This ensures that the experimental setup is consistent and that only one parameter is tested at a time.\nIn the retriever, given a question  and a corpus of  documents , the goal is to select the top  most relevant documents  for the given query. The selection is done with a function , which compares the similarity of the question (query) and each document in the corpus. The documents in our corpus are abstracts of medical publications. While abstracts are shorter versions of full documents, they can still contain irrelevant sentences for producing the final verdict.\nIn our first experiment, we use full documents  and the question  to predict the final answer. In the second experiment, we select only the top  most relevant sentences from the abstracts.\nFrom  candidate sentences  comprising the selected documents, top  sentences are selected as evidence sentences  with a function . These sentences are the most similar to the question .\nFinally, the answer is predicted from the given question  and evidence , where the evidence is either the complete documents (the first round of experiments) or a set of sentences (the second round of experiments). The reader model produces the final verdict and is one of three classes , , . While QA can be generative and elicit long answers, all datasets we use contain only the short yes/no/unknown answers. This makes using the standard classification metrics precision, recall, and F1 possible. We model answer prediction as the related task of recognizing entailment or natural language inference (NLI) and use an NLI model for the prediction.\nIn all experiments, majority voting is used to determine the final verdict. For the dataset HealthFC-3 with three classes, this can be one of the three classes (0, 1, 2). For other datasets, the majority is taken only from predictions of 0 and 1 (in case of a tie, 0 is predicted). Majority voting is chosen for convenience, but it is not optimal as the information on prediction disagreement. Future work should explore how to model the disagreement better.\nFor the  function that selects top k most relevant documents, we use BM25 as it was proven to be a strong baseline for retrieving documents for automated claim verification Stammbach et al. (2023  ###reference_b44###). It also ensures higher precision at the cost of coverage, which aligns with our use case – we want the retrieved documents to be relevant before being passed to the reader. We use a sentence embedding model and semantic search to select the sentences most similar to our query from abstracts. For , we select the model Spiced Wright et al. (2022  ###reference_b54###), which is a recent sentence similarity model optimized for paraphrases of scientific claims. For the final answer prediction model (reader) , we choose the DeBERTa-v3 model He et al. (2021  ###reference_b20###) since it was shown to be a highly potent model for natural language understanding and reasoning tasks. We use the variant of the model optimized for NLI prediction Laurer et al. (2024  ###reference_b27###). We do not fine-tune the models on the datasets in our experiments because we want to simulate a realistic QA system that has to answer unseen questions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct three groups of experiments to test the influence of different retrieval parameters on the performance of our QA system.\nThe first group of experiments consisted of keeping the QA pipeline consistent but increasing the number of retrieved documents (top k) that are forwarded to the final QA module. The motivation behind this was to find the fine balance between covering enough different studies but not saturating the module with noise and irrelevant articles. Considering the increasing popularity of retrieval-enhanced systems such as retrieval-augmented generation (RAG) pipelines Lewis et al. (2020  ###reference_b30###), retrieving only the relevant amount of documents or chunks is a significant challenge. We use BM25 as the retrieval technique because of its efficiency and its focus on enhancing precision instead of recall.\nInstead of taking the whole documents and sending them with the question to the reader, the second group of experiments selected only the top j most relevant sentences within all abstracts and used those as evidence. In this setup, we first retrieve the top 20 most similar abstracts with BM25. Afterward, all abstracts are split into sentences, which are embedded with the sentence-embedding model SPICED. After that, the top j most similar sentences to the question q, according to cosine similarity, are chosen from the pool of sentences (so multiple sentences can come from the same abstract). The QA module calculates the entailment probability between the question and each selected sentence, and finally, majority voting is performed.\nThe third and fourth group of experiments focused less on the technical parameters of the retrieval but more on the quality of the discovered evidence. So far, not many studies have leveraged the metadata of retrieved evidence documents for enhancing medical and health-related question answering or fact verification. We use two metadata parameters that should intuitively have an influence on the quality of the performance – year of publication of the retrieved research publication and the number of citations it has. The year was provided among the metadata that comes with PubMed, but getting the number of citations was more challenging, considering it is not foundin the MEDLINE dump. Therefore, we utilized the Semantic Scholar API Ammar et al. (2018  ###reference_b3###) by querying it with the PubMed ID (PMID) of the retrieved article and then calling the API to get the number of citations. Once we had both numbers, experiments consisted of filtering the pool of possible evidence documents by posing a restriction on the minimum year of publication and the minimum number of citations. Out of the top k documents we retrieved, only those published after a certain year, or those with at least a certain number of citations, were selected as the final evidence documents. The rest of the workflow is the same as in the first experiment: the documents are passed to the QA module and the answer is predicted.\nGiven that we work with discrete answers and labels, the evaluation metrics we used are macro-averaged versions of classification metrics precision, recall, and F1 score. Macro averaging implies that the arithmetic mean of the metric for each individual class is taken (e.g., ). For HealthFC-3, this is an average of three classes, while for the other datasets, it is an average of two classes. The motivation behind using the macro version is that it considers all classes equally important. We posit that in a deployed health QA system, users would be interested not only in detecting positive answers, but the system effectively discerning between both negative and positive answers to questions. This also follows the literature on automated fact-checking, which commonly uses macro-averaged scores Bekoulis et al. (2021  ###reference_b8###).\nAll the experiments were run on a single Nvidia V100 GPU with 16 GB of VRAM. The process of retrieving the top 100 most relevant documents for each dataset used one computation hour. The process of predicting the final answer with the top 100 most relevant documents also used one computation hour."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we present and describe the results of the conducted experiments. Table 2  ###reference_### shows the final classification scores when changing the number of documents retrieved. Similarly, Table 3  ###reference_### shows the final performance for different numbers of top sentences extracted. Tables 4  ###reference_### and 5  ###reference_### show the influence of filtering evidence documents based on their year of publication and number of citations."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Retrieved Documents and Sentences",
            "text": "An interesting trend is observed when looking at Table 2  ###reference_###. For all four datasets, the worst performance was when retrieving the highest amount of documents (50 and 100) and slowly increased towards the lower values. In the case of HealthFC-2 and TREC-Health, taking just the top document gave the best value of F1 (although the best macro precision and recall were with 5 and 10 documents). For BioASQ, taking into account just the top document is considerably worse than all other settings (because of poor recall), but the overall trend also holds for this dataset. As expected, the most challenging is the ternary version of HealthFC, but even there, the F1 performance increase of +2.5% is observed from 100 to 1 document. For binary HealthFC and TREC-Health, jumps from 100 to 1 of +18% and +10% are seen.\nWhen looking at Table 3  ###reference_###, the results are less consistent than in the previous case. In fact, for datasets HealthFC-3 and BioASQ, the effect is rather opposite to the one in the previous experiment. The performance kept dropping as the number of selected sentences was lowered. The increased amount of knowledge in the bigger corpus of sentences helped the performance. For the binary HealthFC and TREC-Health, the numbers generally increased towards the lower number of sentences retrieved, but there wasn’t a consistent pattern. Overall, the experiment showed that, in general, adding sentences that are relevant and semantically similar to the question increases the performance as opposed to adding more full documents to the QA system.\nIs intense physical activity associated with longevity? (Supported)\nEvidence, mainly from cross-sectional studies, suggests that physical activity is a potentially important modifiable factor associated with physical performance and strength in older age. It is unclear whether the benefits of physical activity accumulate across life or whether there are sensitive periods when physical activity is more influential. Cooper et al. (2011  ###reference_b14###) (Not Enough Info)\nPhysical activity plays an important role for achieving healthy aging by promoting independence and increasing the quality of life. (…) Indeed, there is evidence to suggest that increasing exercise intensity in older adults may be associated with greater reductions in the risk of cardiovascular disease and mortality. El Hajj Boutros et al. (2019  ###reference_b17###) (Supported)\nExercise training above the public health recommendations provides additional benefits regarding disease protection and longevity. Endurance exercise, including high-intensity training to improve cardiorespiratory fitness promotes longevity and slows down aging. Strength training should be added to slow down loss of muscle mass, associated with aging and disease. Pedersen (2019  ###reference_b35###) (Supported)\nIs dexamethasone recommended for treatment of intracerebral hemorrhage? (Refuted)\nDexamethasone contributed to many serious adverse events. Conclusions: Given the small sample size, these preliminary results have not shown a clear beneficial effect of dexamethasone against placebo in our patients. Prud’homme et al. (2016  ###reference_b36###) (Refuted)\nOverall, there is no evidence of a beneficial or adverse effect of corticosteroids in patients with either SAH or PICH. Confidence intervals are wide and include clinically significant effects in both directions. Feigin et al. (2005  ###reference_b18###) (Not Enough Info)\nDexamethasone has been used to treat cerebral edema associated with brain abscess. (…) Conclusions: In patients with a brain abscess treated with antibiotics, the use of dexamethasone was not associated with increased mortality. Simjian et al. (2018  ###reference_b42###) (Supported)"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evidence Quality",
            "text": "Table 4  ###reference_### tested the influence of the recency of the paper (year of publication) on the predictive performance. Once again, an interesting trend can be noted. The more recent the selected documents were, the more accurate the answers to health questions in our system. A similar phenomenon can be observed in Table 5  ###reference_### for the number of citations of the papers. When limiting the selection to more and more cited papers, the final score kept increasing. Nevertheless, the highest scores in Tables 3 and 4 are still lower than the top-1-document performance from Table 4  ###reference_###.\nIt intuitively makes sense that the more recent papers will provide the latest knowledge and insights into a research hypothesis, which then also better aligns with the gold labels of our datasets. This is also slightly biased by the recent creation date of our datasets, where annotators had access to the most recent knowledge. Likewise, the better the reputation of a paper (more citations), it could be assumed it will be a more reliable indicator of a correct answer. This wasn’t the case for all of the questions, and there were many examples where older or less cited papers aligned better with the gold labels. Still, Table 6  ###reference_### provides some statistics that show that the trend generally holds. On average, those documents that produced a correct verdict were around three years more recent (in both the mean and median) while having an average of 10 citations more (median four citations). On the other hand, all categories have considerable standard deviations, showing many outliers and exceptions to this rule.\nAnother challenging factor is that there could seemingly be an inverse correlation between the age of a paper and its number of citations – older publications have had more time to gather a bigger number of citations. After deeper inspection, we observed that being cited a lot over time is only true for high-quality studies. Overall, striking a balance by finding both those studies that are both recent and already highly cited is an optimal strategy.\nCan stress promote dementia? (Supported)\nTo test the hypothesis that high job stress during working life might lead to an increased risk of dementia and Alzheimer’s disease (AD) in late life. (…) Lifelong work-related psychosocial stress, characterized by low job control and high job strain, was associated with increased risk of dementia and AD in late life, independent of other known risk factors. Wang et al. (2020  ###reference_b53###) (Supported) [121 citations]\nPatients with Alzheimer’s disease (AD) or dementia are increasing in numbers as the population worldwide ages. Mid-life psychological stress, psychosocial stress and post-traumatic stress disorder have been shown to cause cognitive dysfunction. The mechanisms behind stress-induced AD or dementia are not known. Zhu et al. (2021a  ###reference_b55###) (Refuted) [3 citations]\nCan ginkgo biloba relieve the symptoms of tinnitus? (Refuted)\nGinkgo biloba is a plant extract used to alleviate symptoms associated with cognitive deficits, e.g., decreased memory performance, lack of concentration, decreased alertness, tinnitus, and dizziness. Pharmacologic studies have shown that the therapeutic effect of ginkgo(…) Søholm (1998  ###reference_b43###) (Supported) [Year: 1998]\nWe identified three systematic reviews including four primary studies, all corresponding to randomized trials. We concluded the use of Ginkgo biloba probably does not decrease the severity of tinnitus. In addition, it does not reduce the intensity of tinnitus or improve the quality of life of patients. Kramer and Ortigoza (2018  ###reference_b25###) (Refuted) [Year: 2018]"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this section, we discuss and provide deeper insights uncovered in the results."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Qualitative Analysis",
            "text": "Specific effects of retrieving multiple documents for open-domain health question answering are shown in Table 7  ###reference_###. For the first question, just retrieving the best document would have led to a study that does not provide a definitive conclusion to the question. However, the second and third most similar documents that were retrieved support the given research hypothesis. This held true even for further documents that cannot be shown in the table because of space constraints. On the other hand, the second question would have been more appropriately assessed by just looking at the best document instead of the top 3 documents. In fact, the second and third documents do not explicitly talk about the given question but rather a variation.\nExamples of positive influences of taking into account the qualitative properties of the evidence into account, namely the year of publication and number of citations, are shown in Table 8  ###reference_###. For the first question, a publication with above 100 citations directly answers the question and matches the gold annotation for this claim from the dataset. On the other hand, the second most similar retrieved document was a study with only three citations, which seems to be inconclusive about or even refuting the research hypothesis. Similarly, the answer to the second question on the effects of ginkgo is skewed by the top document from the 1990s that talks about the presumed positive effects of ginkgo. Two decades later, another meta-review analysis of systematic reviews on ginkgo biloba showed that it was not proven to help with tinnitus. This nicely demonstrates the changing nature of scientific knowledge and scientific consensus throughout time."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Future Directions",
            "text": "Based on our findings and discussion, we see that future work could focus on these directions:\nStrength of evidence. Taking into account the year of publication and number of citations has proven to be an effective strategy for enhancing the health question-answering performance. Similarly, further metadata could be taken into account to augment the process. In medical research, the strength of evidence is an important factor, and systems like GRADE are used to assess it Balshem et al. (2011  ###reference_b5###). Different types of studies, such as a single study, a randomized clinical trial, and a systematic review, all have different strengths. Including this could be an important factor in improving the reliability of answers.\nEvidence disagreement and variation. We observed how different studies and sources can come to differing conclusions regarding a claim. In this paper, we chose the majority vote among the top k documents as the final decision, but this diminishes the information about the prediction uncertainty. This is part of the broader ML problem of learning with disagreements Leonardelli et al. (2023  ###reference_b29###) and modeling human label variation Baan et al. (2024  ###reference_b4###). While usually applied to uncertainty in data annotation, it could also be applied in the future to uncertainty in answering questions with diverse evidence documents.\nInterpretability and user-centric results. Other than just predicting the final answer, the end users posing biomedical and health questions would appreciate making the results more interpretable. This includes aspects such as displaying the different evidence documents, highlighting important sections, and showing prediction probabilities. Modern large language models (LLMs) could be used to enhance the reasoning process and to generate user-friendly explanations of model predictions and decisions Lamm et al. (2021  ###reference_b26###)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we conducted a number of experiments assessing the performance of a health question-answering system in an open-domain setting. Moving away from the standard setup of working with a small evidence corpus, we expand the knowledge sources to a large corpus of more than 20 million biomedical research abstracts. We measured the answer prediction performance over three diverse datasets of health questions while varying four aspects: number of retrieved documents, number of extracted sentences, and evidence quality in the form of year of publication and number of citations. Our results show that a lower number of documents retrieved leads to better performance, with the ideal spot in the 1–5 range. We also show that the performance is improved and made more reliable by using a time-aware evidence retrieval process, i.e., retrieving only the highly cited and more recent papers. Our research leaves room for exploration of disagreeing and conflicting evidence, generating explanations for end users, and including further metadata. We hope our research will encourage more exploration of the open-domain health question answering setting and addressing real-world user needs."
        }
    ]
}