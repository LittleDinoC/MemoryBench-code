{
    "title": "Did Translation Models Get More Robust Without Anyone Even Noticing?",
    "abstract": "Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to “noisy” inputs, such as spelling errors, abbreviations, and other formatting issues.\nIn this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data.\nThis is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness.\nNext, we show that similar trends hold for social media translation experiments – LLMs are more robust to social media text.\nWe include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise.\nAltogether, we show that robustness to many types of noise has increased.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "For years, the conventional wisdom has been that neural machine translation (MT) models are highly sensitive to source-side artificial and natural noise at inference time (Belinkov and Bisk, 2018  ###reference_b7###).\nThis insight has motivated many works that seek to make MT models more robust to noise through either specialized training (Ebrahimi et al., 2018  ###reference_b9###; Karpukhin et al., 2019  ###reference_b15###; Park et al., 2020  ###reference_b24###; Vaibhav et al., 2019  ###reference_b44###) or bespoke architectures (Rust et al., 2022  ###reference_b34###; Salesky et al., 2021  ###reference_b35###).\nHowever, MT is increasingly being performed in a different paradigm than when these analyses and architectures were created.\nPreviously, models were mostly trained from scratch on task-specific data, often for a single language pair.\nNowadays, strong results often depend on large pretrained encoder-decoder models (NLLB Team et al., 2022  ###reference_b21###), instruction-tuned large language models (LLMs) like TowerLLM (Alves et al., 2024  ###reference_b3###), or opaque proprietary systems like ChatGPT.111https://chat.openai.com/  ###reference_chat.openai.com/###\nThese huge models may make existing robustness techniques more expensive (specialized training takes longer with larger models) or impossible (specialized architectures cannot be grafted onto an existing pretrained system).\nSo the question is, how necessary are these robustness techniques for LLMs?\nAre they still needed to mitigate the brittleness of subword-level transformers, or have larger models and training sets made today’s models sufficiently robust on their own?\n###figure_1### In this work, we investigate these questions through experiments on social media text and synthetically noised corpora.\nThese experiments have complementary roles: social media text contains diverse noise phenomena, but isolating their effect is not straightforward because the errors are not labeled.\nOn the other hand, synthetic errors may differ in major ways from “naturally occurring” noise, but they are interpretable and controllable, offering a way to measure noise in vitro.\nBy evaluating on a broad spectrum of error types, we can paint a more vivid picture of what kinds of noise, and at what quantities, cause problems for MT systems.\nWe make the following contributions:222Our code is available at https://github.com/deep-spin/robust-mt  ###reference_###.\nWe show (§3  ###reference_###) that large pretrained models are much more robust to synthetic source-side errors than conventional single-pair NMT models (see Figure 1  ###reference_###), even when their performance is similar on clean data. These results hold across several language pairs and varieties of noise, even though the large models lack architectural features that obviously encourage robustness to character noise.\nWe introduce (§3.1  ###reference_###) a novel technique for measuring the robustness of MT models by learning a regression to predict the quality decline as a function of how noisy the source is.\nWe show (§4.1  ###reference_###) that models that are robust to synthetic errors perform better at translating social media text.\nWe investigate the relationship between synthetic robustness and performance on “real-world” noise.\nWe conduct (§4.2  ###reference_###) reference-free MT experiments on MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization dataset that appears to have never before been used for MT.\nWe show that LLMs are more robust than conventional models to this type of noise.\nWe show (§5  ###reference_###) that source correction pipelines can be an effective approach to mitigate the impact of synthetic noise without substantially worsening performance on clean data, although they are significantly less effective with stronger models, suggesting that the benefits of source correction and model robustness are not complementary. Source correction is less effective on social media data, likely because there are not enough errors to outweigh the risk of error propagation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "In recent years, mainstream MT techniques have been based on the transformer (Vaswani et al., 2017  ###reference_b45###), which uses multi-headed self-attention to mix information across time steps.\nIn the original work, transformers used an encoder-decoder paradigm similar to recurrent MT models (Bahdanau et al., 2014  ###reference_b5###).\nThese models pair an encoder over the source with a decoder, an autoregressive language model that predicts target tokens one at a time.\nThese tokens usually come from a learned subword vocabulary (Kudo, 2018  ###reference_b17###; Sennrich et al., 2016  ###reference_b37###).\nInitially, transformer MT models were trained from scratch for a single language pair on parallel data from sources such as the OPUS parallel corpus collection (Tiedemann, 2012  ###reference_b40###).\nAlthough single language pair models often perform well, they struggle in the absence of large quantities of data, making it difficult to achieve good results on low resource language pairs.\nThis problem can be mitigated somewhat through multilingual training with systems like M2M-100 (Fan et al., 2021  ###reference_b10###) and NLLB-200 (NLLB Team et al., 2022  ###reference_b21###).\nLow resource language pairs often benefit from training data in other languages.\nOne challenge is language imbalance – the subword vocabulary and training procedure need to be designed to allow strong performance across covered language pairs in spite of this imbalance.\nIn parallel to these MT-centric developments, transformers have increasingly been used in a transfer learning set-up in which a large model is pretrained on some generic objective for which massive data is available.\nSuch a model can then be finetuned on a particular downstream task or set of tasks.\nWhen the pretraining objective is language modeling (Radford et al., 2018  ###reference_b27###), this makes it straightforward to use the model for text generation tasks such as MT.\nRecent generation-oriented models are often decoder-only: to use such a model for MT, the source is simply treated as part of the decoder’s context.\nSuch models have shown some success for MT (Hendy et al., 2023  ###reference_b12###).\nIn recent times, the paradigm has shifted from traditional finetuning to instruction tuning (Sanh et al., 2022  ###reference_b36###; Wei et al., 2022  ###reference_b46###), in which the finetuning data is accompanied by a prompt containing an instruction.\nThis has been shown to give models the ability to generalize to related tasks and has proven to be effective for MT (Alves et al., 2023  ###reference_b2###, 2024  ###reference_b3###).\nA common technique to increase robustness is to train MT models on examples with added source errors.\nGiven that high-quality corpora containing authentic errors are rare,\nthe added noise is generally synthetic (Karpukhin et al., 2019  ###reference_b15###), although it can be tuned to resemble natural errors (Martucci et al., 2021  ###reference_b18###; Vaibhav et al., 2019  ###reference_b44###).\nWhether training on synthetic noise is actually helpful for becoming robust to natural errors is an open question, with various works coming to contradictory conclusions (Belinkov and Bisk, 2018  ###reference_b7###; Vaibhav et al., 2019  ###reference_b44###).\nAnother hazard is that training on noise can reduce performance on clean data (Khayrallah and Koehn, 2018  ###reference_b16###), so the quantity of noise needs to be chosen carefully.\nAs an alternative to specialized training techniques, robustness can be achieved with architectures other than the ubiquitous subword-level transformer.\nModeling at the character or byte level (Sutskever et al., 2011  ###reference_b39###; Xue et al., 2022  ###reference_b47###) means that perturbations make only small changes to the sequence of tokens that the model is exposed to, whereas these same perturbations can cause a subword-level model to be exposed to completely different subword types.\nThis may make character- and byte-level models more robust, although the evidence is mixed (Mielke et al., 2021  ###reference_b20###).\nThese models are also much slower than subword-level models because of longer sequence lengths.\nAs an alternative, MT models can be trained on representations that are invariant to character shuffles (Belinkov and Bisk, 2018  ###reference_b7###) or on visual representations of text (Salesky et al., 2021  ###reference_b35###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Architectures for MT",
            "text": "In recent years, mainstream MT techniques have been based on the transformer (Vaswani et al., 2017  ###reference_b45###  ###reference_b45###), which uses multi-headed self-attention to mix information across time steps.\nIn the original work, transformers used an encoder-decoder paradigm similar to recurrent MT models (Bahdanau et al., 2014  ###reference_b5###  ###reference_b5###).\nThese models pair an encoder over the source with a decoder, an autoregressive language model that predicts target tokens one at a time.\nThese tokens usually come from a learned subword vocabulary (Kudo, 2018  ###reference_b17###  ###reference_b17###; Sennrich et al., 2016  ###reference_b37###  ###reference_b37###).\nInitially, transformer MT models were trained from scratch for a single language pair on parallel data from sources such as the OPUS parallel corpus collection (Tiedemann, 2012  ###reference_b40###  ###reference_b40###).\nAlthough single language pair models often perform well, they struggle in the absence of large quantities of data, making it difficult to achieve good results on low resource language pairs.\nThis problem can be mitigated somewhat through multilingual training with systems like M2M-100 (Fan et al., 2021  ###reference_b10###  ###reference_b10###) and NLLB-200 (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nLow resource language pairs often benefit from training data in other languages.\nOne challenge is language imbalance – the subword vocabulary and training procedure need to be designed to allow strong performance across covered language pairs in spite of this imbalance.\nIn parallel to these MT-centric developments, transformers have increasingly been used in a transfer learning set-up in which a large model is pretrained on some generic objective for which massive data is available.\nSuch a model can then be finetuned on a particular downstream task or set of tasks.\nWhen the pretraining objective is language modeling (Radford et al., 2018  ###reference_b27###  ###reference_b27###), this makes it straightforward to use the model for text generation tasks such as MT.\nRecent generation-oriented models are often decoder-only: to use such a model for MT, the source is simply treated as part of the decoder’s context.\nSuch models have shown some success for MT (Hendy et al., 2023  ###reference_b12###  ###reference_b12###).\nIn recent times, the paradigm has shifted from traditional finetuning to instruction tuning (Sanh et al., 2022  ###reference_b36###  ###reference_b36###; Wei et al., 2022  ###reference_b46###  ###reference_b46###), in which the finetuning data is accompanied by a prompt containing an instruction.\nThis has been shown to give models the ability to generalize to related tasks and has proven to be effective for MT (Alves et al., 2023  ###reference_b2###  ###reference_b2###, 2024  ###reference_b3###  ###reference_b3###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robustness to Character Noise",
            "text": "Character perturbations have been shown to have a large negative impact on the performance of MT models (Belinkov and Bisk, 2018  ###reference_b7###).\nConsequently, a number of techniques have been proposed to mitigate their impact.\nA common technique to increase robustness is to train MT models on examples with added source errors.\nGiven that high-quality corpora containing authentic errors are rare,\nthe added noise is generally synthetic (Karpukhin et al., 2019  ###reference_b15###  ###reference_b15###), although it can be tuned to resemble natural errors (Martucci et al., 2021  ###reference_b18###  ###reference_b18###; Vaibhav et al., 2019  ###reference_b44###  ###reference_b44###).\nWhether training on synthetic noise is actually helpful for becoming robust to natural errors is an open question, with various works coming to contradictory conclusions (Belinkov and Bisk, 2018  ###reference_b7###  ###reference_b7###; Vaibhav et al., 2019  ###reference_b44###  ###reference_b44###).\nAnother hazard is that training on noise can reduce performance on clean data (Khayrallah and Koehn, 2018  ###reference_b16###  ###reference_b16###), so the quantity of noise needs to be chosen carefully.\nAs an alternative to specialized training techniques, robustness can be achieved with architectures other than the ubiquitous subword-level transformer.\nModeling at the character or byte level (Sutskever et al., 2011  ###reference_b39###  ###reference_b39###; Xue et al., 2022  ###reference_b47###  ###reference_b47###) means that perturbations make only small changes to the sequence of tokens that the model is exposed to, whereas these same perturbations can cause a subword-level model to be exposed to completely different subword types.\nThis may make character- and byte-level models more robust, although the evidence is mixed (Mielke et al., 2021  ###reference_b20###  ###reference_b20###).\nThese models are also much slower than subword-level models because of longer sequence lengths.\nAs an alternative, MT models can be trained on representations that are invariant to character shuffles (Belinkov and Bisk, 2018  ###reference_b7###  ###reference_b7###) or on visual representations of text (Salesky et al., 2021  ###reference_b35###  ###reference_b35###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Robustness to Synthetic Noise",
            "text": "In our first experiments, we evaluate how various models perform in the presence of token-level synthetic errors.\nAlthough synthetic errors differ from “naturally occurring” noise, they are adjustable and function as a stress test for MT systems.\nWe use four types of synthetic perturbations, each of which is a plausible error based on the mechanics of typing.\nFor each noise type, we corrupt 10% to 100% of whitespace-delimited tokens in the FLORES-200 devtest data (NLLB Team et al., 2022  ###reference_b21###).\nWe use the following noise types:\nswap: flip two adjacent characters.\nchardupe: duplicate a character.\nchardrop: delete a character.\nkey: replace a character with an adjacent one on a keyboard. We use the QWERTZ layout for German, AZERTY for French, QWERTY for English and Portuguese, and South Korean Dubeolsik for Korean.\nFor Korean, we used hangul-jamo333https://github.com/jonghwanhyeon/hangul-jamo  ###reference_mo### to decompose hangul characters into jamo, which represent individual keystrokes, before applying perturbations.\nWe use models that differ in their scope (bi- or multilingual), architecture (encoder-decoder or decoder-only), and size (74M-7B parameters).\nOPUS: We use transformer encoder-decoder models trained from scratch on a single language pair and released as part of OPUS-MT (Tiedemann and Thottingal, 2020  ###reference_b42###). Model and vocabulary sizes are listed in Table 2  ###reference_###.\nNLLB (NLLB Team et al., 2022  ###reference_b21###), like OPUS, is an encoder-decoder transformer trained on parallel text.\nHowever, NLLB is a many-to-many system trained on data in 202 languages.\nWe use the 3.3 billion parameter version.\nTower: We use the 7 billion parameter version of TowerInstruct444https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1  ###reference_ct-7B-v0.1### (Alves et al., 2024  ###reference_b3###), a recently-released instruction-tuned LLM derived from Llama 2 (Touvron et al., 2023  ###reference_b43###).\nIt covers translation between 10 languages.\nGPT-3.5:555Specifically, we use gpt-3.5-turbo-1106. the architecture and training data of GPT-3.5 are unknown, making it difficult to draw scientific conclusions from its results. We include it because it has shown success at MT (Hendy et al., 2023  ###reference_b12###) and the related GPT-4 has been shown to be able to correct some character perturbations (Cao et al., 2023  ###reference_b8###).\nFor NLLB, Tower, and the models with a listed HF Path in Table 2  ###reference_###, we use public checkpoints from the Hugging Face transformers library.666https://github.com/huggingface/transformers  ###reference_s###\nFor the pten OPUS model, we use a script777https://github.com/huggingface/transformers/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py### to convert its Marian checkpoint888https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/por-eng  ###reference_allenge/tree/master/models/por-eng### to transformers format.\nWe directly use checkpoints from the Tatoeba Challenge (Tiedemann, 2020  ###reference_b41###) and perform inference with Marian (Junczys-Dowmunt et al., 2018  ###reference_b14###) for enko999https://object.pouta.csc.fi/Tatoeba-MT-models/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### and koen101010https://object.pouta.csc.fi/Tatoeba-MT-models/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### because the conversion script failed for them.\nWe generate translations using beam search (Reddy et al., 1977  ###reference_b29###) with a beam size of 5 for all models except GPT-3.5.\nFor GPT-3.5, we sample with temperature 0.\nWe use prompts for Tower111111‘‘Translate the following text from [source language] to [target language].nSource:[source text]n[target language]:’’ and GPT-3.5.121212‘‘Translate this sentence from [source language] to [target language].nSource:[source text]nTarget:’’\nOur base metric for scoring the translation performance on a corpus is COMET (Rei et al., 2020  ###reference_b31###).131313Specifically, we use COMET-22 (Rei et al., 2022a  ###reference_b30###).\nCOMET computes a normalized score for a hypothesis , conditioned on the source  and a reference .\nWhen we compute scores for translations from noisy data, we provide the COMET model the clean source, not the noisy version that was actually used to generate hypotheses.\nWe measure the trajectory of performance as the amount of noise is increased, as depicted in Figure 1  ###reference_###.\nTo represent this trajectory as a single number, for each configuration we fit a linear regression to predict how much COMET declines relative to the clean performance141414There is no need to learn an intercept term because the decline is relative to the model’s clean performance. as a function of the proportion of noised tokens.\nWe report the learned slope, which we call COMET-slope.\nThe higher (closer to zero) the COMET-slope is, the more robust the model is.\nThis metric can also be interpreted as the number of COMET points that would be lost if every token were corrupted.\nswap\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-73.61\n-65.80\n-71.02\n-66.00\n-74.02\n-50.02\n-73.35\n-64.01\n\nNLLB\n-22.45\n-18.20\n-21.81\n-20.75\n-19.65\n-22.89\n-21.03\n-20.34\n\nTower\n-19.42\n-28.54\n-18.70\n-27.64\n-18.48\n-26.16\n-17.39\n-28.79\n\nGPT-3.5\n-3.89\n-4.36\n-4.46\n-5.85\n-4.79\n-20.89\n-3.76\n-6.78\nchardrop\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-69.40\n-54.17\n-64.33\n-50.72\n-71.37\n-44.14\n-70.13\n-53.15\n\nNLLB\n-22.13\n-16.51\n-21.56\n-17.10\n-18.33\n-20.81\n-20.89\n-18.52\n\nTower\n-18.42\n-19.34\n-18.64\n-17.93\n-15.19\n-24.62\n-17.89\n-20.80\n\nGPT-3.5\n-6.59\n-6.55\n-7.32\n-5.68\n-6.72\n-17.81\n-6.63\n-7.09\nchardupe\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-56.35\n-35.17\n-47.42\n-33.00\n-65.89\n-31.11\n-58.98\n-35.65\n\nNLLB\n-3.73\n-4.02\n-3.60\n-5.36\n-2.82\n-4.19\n-3.76\n-4.27\n\nTower\n-3.21\n-4.13\n-2.88\n-4.39\n-3.70\n-7.66\n-2.79\n-3.76\n\nGPT-3.5\n-1.14\n-1.36\n-1.32\n-1.42\n-1.42\n-5.64\n-0.98\n-1.44\nkey\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-74.13\n-64.46\n-73.35\n-66.05\n-76.68\n-38.50\n-76.20\n-66.02\n\nNLLB\n-26.20\n-20.17\n-25.07\n-21.80\n-23.69\n-26.41\n-24.25\n-23.54\n\nTower\n-28.64\n-29.49\n-27.13\n-31.89\n-25.89\n-23.80\n-26.33\n-36.55\n\nGPT-3.5\n-8.19\n-9.17\n-8.17\n-8.63\n-8.91\n-16.31\n-7.78\n-10.27\nThe results in Table 1  ###reference_### show that on clean data, Tower and GPT-3.5 are the two strongest systems, with at least one of them recording the highest COMET score for all eight language pairs.\nThe gap between the strongest system and the much smaller OPUS models is at most  COMET points for all language pairs except ende.\nHowever, Table 3  ###reference_### shows that the differences become much larger on noisy data.\nFor all language pairs and noise types, OPUS suffers far more from perturbations than any of the other models do.\nOn the other end of the spectrum, GPT-3.5 is almost always more robust than other models, sometimes strikingly so.\nNLLB and Tower are between these two extremes – they are far more robust than OPUS, but only come close to GPT-3.5 for chardupe noise.\nFor swap and chardrop noise, NLLB is more robust than Tower when translating to English, while the reverse is true when translating from English.\nThis trend is less consistent for chardupe noise.\nFor key noise, NLLB is more robust than Tower for every pair except koen.\nAll models decline as the noise level is increased, but they do not decline in the same way.\nIn Figure 2  ###reference_### we show how the fluency of the model’s predictions, measured by the perplexity of GPT-2 (Radford et al., 2019  ###reference_b28###), is related to the quality of those predictions in terms of COMET.\nFor fren swaps, it is clear that NLLB and GPT-3.5 continue producing fluent English text even as the noise level increases.\n###figure_2### From these experiments, it is tempting to conclude that robustness depends largely on model size (OPUS is  times smaller than any other system) or on multilinguality (all systems except OPUS are multilingual).\nHowever, Figure 3  ###reference_### tells a different story.\nWe reran swap noise experiments with three extra models: the 600M and 1.3 billion parameter versions of NLLB (the former of which is distilled from the 54B version of NLLB) and the 1.2 billion parameter version of M2M (Fan et al., 2021  ###reference_b10###).\nDespite NLLB-1.3B and M2M-1.2B being multilingual models of similar sizes, they do not respond the same to noise: NLLB-1.3B follows a similar curve to NLLB-3.3B, while M2M-1.2B suffers nearly as much as OPUS.\nNLLB-600M is somewhere between these extremes.\nFurther work is needed to determine what factors influence this kind of robustness.\nIntroducing perturbations affects not only translation quality but also runtime.\nPerturbations create character sequences that are less similar to the data that tokenizers are trained on, which leads to more pieces being used to encode the sentence.\nThis is true even for chardrop noise, which increases the length of the tokenized sequence even as it shortens the detokenized sequence.\nIn Table 4  ###reference_###, we compare tokenizers by their fertility — the average number of subword pieces per whitespace word — on clean and key data.\nWhile OPUS tokenizers generally have very low fertility on clean data, it increases more than the other tokenizers, suggesting the tokenizer itself is less robust to character perturbations.\nIt is also notable that Tower and GPT-3.5 have high fertility even on clean Korean text.\nWhile this is a symptom of tokenizer unfairness in large models (Petrov et al., 2023  ###reference_b25###), it can also be a sign of tokenizer robustness: the higher the fertility, the closer the model is to byte-level tokenization.\nThis results in noisy token sequences that are much closer to the clean sequences for Tower and GPT-3.5, as can be seen in terms of F1 in Table 5  ###reference_###.\nThe same trend does not hold for the other languages."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experiments",
            "text": "In all of our synthetic experiments, we adopt a simple set-up: for each translation corpus, we introduce a particular type of perturbation into some percentage of the source-side tokens.\nWe then compare performance translating this perturbed corpus to the performance on clean data.\nA model’s robustness can be characterized by the steepness of its decline as the noise level is increased: a flatter slope indicates that the model handles noise better.\nWe use four types of synthetic perturbations, each of which is a plausible error based on the mechanics of typing.\nFor each noise type, we corrupt 10% to 100% of whitespace-delimited tokens in the FLORES-200 devtest data (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nWe use the following noise types:\nswap: flip two adjacent characters.\nchardupe: duplicate a character.\nchardrop: delete a character.\nkey: replace a character with an adjacent one on a keyboard. We use the QWERTZ layout for German, AZERTY for French, QWERTY for English and Portuguese, and South Korean Dubeolsik for Korean.\nFor Korean, we used hangul-jamo333https://github.com/jonghwanhyeon/hangul-jamo  ###reference_mo###  ###reference_mo### to decompose hangul characters into jamo, which represent individual keystrokes, before applying perturbations.\nWe use models that differ in their scope (bi- or multilingual), architecture (encoder-decoder or decoder-only), and size (74M-7B parameters).\nOPUS: We use transformer encoder-decoder models trained from scratch on a single language pair and released as part of OPUS-MT (Tiedemann and Thottingal, 2020  ###reference_b42###  ###reference_b42###). Model and vocabulary sizes are listed in Table 2  ###reference_###  ###reference_###.\nNLLB (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###), like OPUS, is an encoder-decoder transformer trained on parallel text.\nHowever, NLLB is a many-to-many system trained on data in 202 languages.\nWe use the 3.3 billion parameter version.\nTower: We use the 7 billion parameter version of TowerInstruct444https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1  ###reference_ct-7B-v0.1###  ###reference_ct-7B-v0.1### (Alves et al., 2024  ###reference_b3###  ###reference_b3###), a recently-released instruction-tuned LLM derived from Llama 2 (Touvron et al., 2023  ###reference_b43###  ###reference_b43###).\nIt covers translation between 10 languages.\nGPT-3.5:555Specifically, we use gpt-3.5-turbo-1106. the architecture and training data of GPT-3.5 are unknown, making it difficult to draw scientific conclusions from its results. We include it because it has shown success at MT (Hendy et al., 2023  ###reference_b12###  ###reference_b12###) and the related GPT-4 has been shown to be able to correct some character perturbations (Cao et al., 2023  ###reference_b8###  ###reference_b8###).\nFor NLLB, Tower, and the models with a listed HF Path in Table 2  ###reference_###  ###reference_###, we use public checkpoints from the Hugging Face transformers library.666https://github.com/huggingface/transformers  ###reference_s###  ###reference_s###\nFor the pten OPUS model, we use a script777https://github.com/huggingface/transformers/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py###  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py### to convert its Marian checkpoint888https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/por-eng  ###reference_allenge/tree/master/models/por-eng###  ###reference_allenge/tree/master/models/por-eng### to transformers format.\nWe directly use checkpoints from the Tatoeba Challenge (Tiedemann, 2020  ###reference_b41###  ###reference_b41###) and perform inference with Marian (Junczys-Dowmunt et al., 2018  ###reference_b14###  ###reference_b14###) for enko999https://object.pouta.csc.fi/Tatoeba-MT-models/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip###  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### and koen101010https://object.pouta.csc.fi/Tatoeba-MT-models/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip###  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### because the conversion script failed for them.\nWe generate translations using beam search (Reddy et al., 1977  ###reference_b29###  ###reference_b29###) with a beam size of 5 for all models except GPT-3.5.\nFor GPT-3.5, we sample with temperature 0.\nWe use prompts for Tower111111‘‘Translate the following text from [source language] to [target language].nSource:[source text]n[target language]:’’ and GPT-3.5.121212‘‘Translate this sentence from [source language] to [target language].nSource:[source text]nTarget:’’\nOur base metric for scoring the translation performance on a corpus is COMET (Rei et al., 2020  ###reference_b31###  ###reference_b31###).131313Specifically, we use COMET-22 (Rei et al., 2022a  ###reference_b30###  ###reference_b30###).\nCOMET computes a normalized score for a hypothesis , conditioned on the source  and a reference .\nWhen we compute scores for translations from noisy data, we provide the COMET model the clean source, not the noisy version that was actually used to generate hypotheses.\nWe measure the trajectory of performance as the amount of noise is increased, as depicted in Figure 1  ###reference_###  ###reference_###.\nTo represent this trajectory as a single number, for each configuration we fit a linear regression to predict how much COMET declines relative to the clean performance141414There is no need to learn an intercept term because the decline is relative to the model’s clean performance. as a function of the proportion of noised tokens.\nWe report the learned slope, which we call COMET-slope.\nThe higher (closer to zero) the COMET-slope is, the more robust the model is.\nThis metric can also be interpreted as the number of COMET points that would be lost if every token were corrupted.\nswap\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-73.61\n-65.80\n-71.02\n-66.00\n-74.02\n-50.02\n-73.35\n-64.01\n\nNLLB\n-22.45\n-18.20\n-21.81\n-20.75\n-19.65\n-22.89\n-21.03\n-20.34\n\nTower\n-19.42\n-28.54\n-18.70\n-27.64\n-18.48\n-26.16\n-17.39\n-28.79\n\nGPT-3.5\n-3.89\n-4.36\n-4.46\n-5.85\n-4.79\n-20.89\n-3.76\n-6.78\nchardrop\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-69.40\n-54.17\n-64.33\n-50.72\n-71.37\n-44.14\n-70.13\n-53.15\n\nNLLB\n-22.13\n-16.51\n-21.56\n-17.10\n-18.33\n-20.81\n-20.89\n-18.52\n\nTower\n-18.42\n-19.34\n-18.64\n-17.93\n-15.19\n-24.62\n-17.89\n-20.80\n\nGPT-3.5\n-6.59\n-6.55\n-7.32\n-5.68\n-6.72\n-17.81\n-6.63\n-7.09\nchardupe\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-56.35\n-35.17\n-47.42\n-33.00\n-65.89\n-31.11\n-58.98\n-35.65\n\nNLLB\n-3.73\n-4.02\n-3.60\n-5.36\n-2.82\n-4.19\n-3.76\n-4.27\n\nTower\n-3.21\n-4.13\n-2.88\n-4.39\n-3.70\n-7.66\n-2.79\n-3.76\n\nGPT-3.5\n-1.14\n-1.36\n-1.32\n-1.42\n-1.42\n-5.64\n-0.98\n-1.44\nkey\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-74.13\n-64.46\n-73.35\n-66.05\n-76.68\n-38.50\n-76.20\n-66.02\n\nNLLB\n-26.20\n-20.17\n-25.07\n-21.80\n-23.69\n-26.41\n-24.25\n-23.54\n\nTower\n-28.64\n-29.49\n-27.13\n-31.89\n-25.89\n-23.80\n-26.33\n-36.55\n\nGPT-3.5\n-8.19\n-9.17\n-8.17\n-8.63\n-8.91\n-16.31\n-7.78\n-10.27\nThe results in Table 1  ###reference_###  ###reference_### show that on clean data, Tower and GPT-3.5 are the two strongest systems, with at least one of them recording the highest COMET score for all eight language pairs.\nThe gap between the strongest system and the much smaller OPUS models is at most  COMET points for all language pairs except ende.\nHowever, Table 3  ###reference_###  ###reference_### shows that the differences become much larger on noisy data.\nFor all language pairs and noise types, OPUS suffers far more from perturbations than any of the other models do.\nOn the other end of the spectrum, GPT-3.5 is almost always more robust than other models, sometimes strikingly so.\nNLLB and Tower are between these two extremes – they are far more robust than OPUS, but only come close to GPT-3.5 for chardupe noise.\nFor swap and chardrop noise, NLLB is more robust than Tower when translating to English, while the reverse is true when translating from English.\nThis trend is less consistent for chardupe noise.\nFor key noise, NLLB is more robust than Tower for every pair except koen."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Analysis",
            "text": "###figure_3### All models decline as the noise level is increased, but they do not decline in the same way.\nIn Figure 2  ###reference_###  ###reference_### we show how the fluency of the model’s predictions, measured by the perplexity of GPT-2 (Radford et al., 2019  ###reference_b28###  ###reference_b28###), is related to the quality of those predictions in terms of COMET.\nFor fren swaps, it is clear that NLLB and GPT-3.5 continue producing fluent English text even as the noise level increases.\n###figure_4### From these experiments, it is tempting to conclude that robustness depends largely on model size (OPUS is  times smaller than any other system) or on multilinguality (all systems except OPUS are multilingual).\nHowever, Figure 3  ###reference_###  ###reference_### tells a different story.\nWe reran swap noise experiments with three extra models: the 600M and 1.3 billion parameter versions of NLLB (the former of which is distilled from the 54B version of NLLB) and the 1.2 billion parameter version of M2M (Fan et al., 2021  ###reference_b10###  ###reference_b10###).\nDespite NLLB-1.3B and M2M-1.2B being multilingual models of similar sizes, they do not respond the same to noise: NLLB-1.3B follows a similar curve to NLLB-3.3B, while M2M-1.2B suffers nearly as much as OPUS.\nNLLB-600M is somewhere between these extremes.\nFurther work is needed to determine what factors influence this kind of robustness.\nIntroducing perturbations affects not only translation quality but also runtime.\nPerturbations create character sequences that are less similar to the data that tokenizers are trained on, which leads to more pieces being used to encode the sentence.\nThis is true even for chardrop noise, which increases the length of the tokenized sequence even as it shortens the detokenized sequence.\nIn Table 4  ###reference_###  ###reference_###, we compare tokenizers by their fertility — the average number of subword pieces per whitespace word — on clean and key data.\nWhile OPUS tokenizers generally have very low fertility on clean data, it increases more than the other tokenizers, suggesting the tokenizer itself is less robust to character perturbations.\nIt is also notable that Tower and GPT-3.5 have high fertility even on clean Korean text.\nWhile this is a symptom of tokenizer unfairness in large models (Petrov et al., 2023  ###reference_b25###  ###reference_b25###), it can also be a sign of tokenizer robustness: the higher the fertility, the closer the model is to byte-level tokenization.\nThis results in noisy token sequences that are much closer to the clean sequences for Tower and GPT-3.5, as can be seen in terms of F1 in Table 5  ###reference_###  ###reference_###.\nThe same trend does not hold for the other languages."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Robustness to Social Media Text",
            "text": "The previous experiments show that large translation models and LLMs are more robust to synthetic character perturbations than conventional MT models.\nBut is this result applicable to “authentically noisy” domains such as social media text?\nThe nature of “noise” here is different than in the synthetic task: social media text does not necessarily contain many errors (Rello and Baeza-Yates, 2012  ###reference_b33###), but the domain is very different from FLORES.\nThese factors make it difficult to isolate the effect of noise from the general domain adaptation problem.\nIn an ideal world, we would have a translation corpus in which each example is a triple consisting of an original noisy source sequence, a manually annotated cleaned source sequence, and a gold standard translation.\nThis would allow translations of clean and noisy versions of the same source to be compared on some reference-based metric, isolating the effect of the errors.\nUnfortunately, to our knowledge no such corpus exists, so instead we perform two complementary investigations.\nFirst, we evaluate our models on MTNT (Michel and Neubig, 2018  ###reference_b19###), a noisy social media translation corpus.\nAlthough this is a useful test of our models’ capabilities, the noise in the corpus is not labeled and there is no clean version of the same data to compare to.\nThis motivates our second experiment, in which we translate data from MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization benchmark.\nTogether, these two experiments allow us to see both which models succeed and how badly they fail.\nMichel and Neubig (2018  ###reference_b19###) estimate that  percent of English tokens in MTNT enfr are misspelled.\nThis is much less noise than we used in our synthetic experiments, but is higher than in formal corpora.\nWe finetuned OPUS and NLLB on the MTNT train sets for enfr,\nusing early stopping with patience 3 and validating by loss every 100 steps.\nOther hyperparameters are in Table 7  ###reference_###.\nResults are shown in Table 8  ###reference_###.\nGPT-3.5 performs best.\nAmong other models, Tower is generally strongest, although NLLB roughly equals it after finetuning.\nOPUS performs worst.\nThe MultiLexNorm dataset covers 13 languages, but many of the corpora are tiny or are in languages that Tower does not cover, so we only use English, German, and Spanish151515For experiments involving Spanish, we use Helsinki-NLP/opus-mt-tc-big-en-es (234.8M parameters, 55k vocabulary) and Helsinki-NLP/opus-mt-es-en (77.9M parameters, 65k vocabulary) from transformers. as sources.\nIn experiments with English sources, we translate to German and Spanish; otherwise, we translate to English.\nStatistics are presented in Table 9  ###reference_###.\nAs MultiLexNorm lacks reference translations, we use three reference-free evaluation techniques.\nFirst, we use faux-BLEU (Anastasopoulos, 2019  ###reference_b4###), which computes  (Papineni et al., 2002  ###reference_b23###), where  is the hypothesis computed from the noisy source,  is the hypothesis computed from the clean source, and  is treated as a pseudoreference.161616Specifically, we use spBLEU (NLLB Team et al., 2022  ###reference_b21###).\nBy analogy we also compute faux-COMET.\nThese faux-metrics measure the similarity between  and , with faux-BLEU being a lexical metric that captures surface-level features, while faux-COMET is more semantic.\nIn addition, we use a new metric that we dub QE.\nGiven , , noisy and clean source sequences  and , and a reference-free quality estimation metric QE, .\nWe use COMETKiwi (Rei et al., 2022b  ###reference_b32###) to compute QE.\nA QE close to zero means that a model produces similar-quality outputs for both inputs, indicating robustness, whereas a large positive value indicates that translation quality suffers on noisy data.\nTable 6  ###reference_### shows the performance of all models with both noisy and gold-standard cleaned versions of the corpora.\nIn terms of QE, GPT-3.5 performs best for all language pairs.\nIt also has the best faux-COMET for all except ende (where Tower passes it).\nBy faux-BLEU it is outperformed by Tower for ende and NLLB for esen.\nThere is a contrast between the performance of NLLB and Tower.\nWhile Tower has the better QE for all language pairs and the better faux-COMET for all except esen, NLLB outperforms it by faux-BLEU for all except ende.\nThis suggests that NLLB preserves lexical structure, while Tower preserves “deeper” features.\n###figure_5###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "MultiLexNorm Experiments",
            "text": "While MTNT is an established benchmark and useful sanity check, it is not controllable like our synthetic experiments; we cannot isolate the effect of noise because there is no non-noisy version of the corpus.\nTherefore we pivot to evaluate models on translating MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization dataset that pairs social media text primarily from Twitter with manually cleaned versions of the same.\nSwitching from MTNT to MultiLexNorm comes with a trade-off: in order to gain clean sources, we lose references.\nThe MultiLexNorm dataset covers 13 languages, but many of the corpora are tiny or are in languages that Tower does not cover, so we only use English, German, and Spanish151515For experiments involving Spanish, we use Helsinki-NLP/opus-mt-tc-big-en-es (234.8M parameters, 55k vocabulary) and Helsinki-NLP/opus-mt-es-en (77.9M parameters, 65k vocabulary) from transformers. as sources.\nIn experiments with English sources, we translate to German and Spanish; otherwise, we translate to English.\nStatistics are presented in Table 9  ###reference_###  ###reference_###.\nAs MultiLexNorm lacks reference translations, we use three reference-free evaluation techniques.\nFirst, we use faux-BLEU (Anastasopoulos, 2019  ###reference_b4###  ###reference_b4###), which computes  (Papineni et al., 2002  ###reference_b23###  ###reference_b23###), where  is the hypothesis computed from the noisy source,  is the hypothesis computed from the clean source, and  is treated as a pseudoreference.161616Specifically, we use spBLEU (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nBy analogy we also compute faux-COMET.\nThese faux-metrics measure the similarity between  and , with faux-BLEU being a lexical metric that captures surface-level features, while faux-COMET is more semantic.\nIn addition, we use a new metric that we dub QE.\nGiven , , noisy and clean source sequences  and , and a reference-free quality estimation metric QE, .\nWe use COMETKiwi (Rei et al., 2022b  ###reference_b32###  ###reference_b32###) to compute QE.\nA QE close to zero means that a model produces similar-quality outputs for both inputs, indicating robustness, whereas a large positive value indicates that translation quality suffers on noisy data.\nTable 6  ###reference_###  ###reference_### shows the performance of all models with both noisy and gold-standard cleaned versions of the corpora.\nIn terms of QE, GPT-3.5 performs best for all language pairs.\nIt also has the best faux-COMET for all except ende (where Tower passes it).\nBy faux-BLEU it is outperformed by Tower for ende and NLLB for esen.\nThere is a contrast between the performance of NLLB and Tower.\nWhile Tower has the better QE for all language pairs and the better faux-COMET for all except esen, NLLB outperforms it by faux-BLEU for all except ende.\nThis suggests that NLLB preserves lexical structure, while Tower preserves “deeper” features.\n###figure_6###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Source Correction",
            "text": "So far we have shown that OPUS is less robust to synthetic noise than larger models and performs worse on social media text.\nNext we handle noise with source correction pipelines.\nPipelines cannot be learned end-to-end and introduce a risk of error propagation, but they are modular and interpretable, and can be used with closed-source models.\nThe effect of correcting synthetic enfr errors is shown in Figure 4  ###reference_###.\nFor OPUS models, source correction is often beneficial, with gains of more than 40 COMET points for mT5 over the baseline when at least 50% of tokens are noised.\nSource correction brings smaller performance gains for NLLB and Tower.\nSource correction has a slight negative effect on GPT-3.5, indicating that extremely robust models have little to gain from source correction.\nHowever, Table 12  ###reference_### shows that correcting clean inputs is nearly harmless.\nmT5 never loses more than  COMET points.\nEven the weaker JamSpell never causes a degradation of more than half a COMET point, suggesting that it could be useful because of its fast runtime.\nSocial media data is a challenge for both of our source correctors because they were trained on other domains.\nUsing either corrected source leads to a decline of at least one COMET point compared to the raw source.\nAlthough an oracle delivers some gains, it still cannot equal the performance of finetuning when it is available.\nThese results seem to confirm that MTNT is not particularly noisy in terms of spelling errors (Karpukhin et al., 2019  ###reference_b15###; Michel and Neubig, 2018  ###reference_b19###).\nBased on the results in Table 13  ###reference_###, it would be easy to conclude that source correction does not work for MTNT.\nHowever, Figure 5  ###reference_### shows that some sentences do benefit from correction, and others are not harmed.\nIn these terms, mT5 appears to be a higher-risk corrector than JamSpell, improving translation quality for  of examples (versus  for JamSpell) while making quality worse for  of examples (versus  for JamSpell).\nThis, in addition to the oracle performance, suggests that further research could make source correction a useful tool in this domain."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Correction Models",
            "text": "We use two architectures for source correction:\nJamSpell (Ozinov, 2019  ###reference_b22###) is a non-neural spell-checker with trigram context. We use the default English model from the package.171717https://github.com/bakwc/JamSpell  ###reference_github.com/bakwc/JamSpell###\nWe finetune mT5-based (Xue et al., 2021  ###reference_b48###) correctors on the Prob+Word dataset from NeuSpell181818https://github.com/neuspell/neuspell  ###reference_### (Jayanthi et al., 2020  ###reference_b13###). We use the mT5-Large (1.2B parameter) version of the architecture with the hyperparameter ranges in Table 10  ###reference_###.\nWe select the checkpoint with the best chrF (Popović, 2015  ###reference_b26###) when decoding a noised version of the FLORES English dev set.\nWe decode with a beam size of 5.\nThese two models are minimalistic and maximalistic approaches to correction.\nJamSpell is fast (more than 10k tokens per second on a laptop CPU) but uses only trigram context.\nOn the other hand, mT5 is a large encoder-decoder that leverages a long context.\nIts subword vocabulary allows it to make open-vocabulary corrections, but it is much slower than JamSpell and is prone to hallucination."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Correcting Synthetic Errors",
            "text": "The effect of correcting synthetic enfr errors is shown in Figure 4  ###reference_###  ###reference_###.\nFor OPUS models, source correction is often beneficial, with gains of more than 40 COMET points for mT5 over the baseline when at least 50% of tokens are noised.\nSource correction brings smaller performance gains for NLLB and Tower.\nSource correction has a slight negative effect on GPT-3.5, indicating that extremely robust models have little to gain from source correction.\nHowever, Table 12  ###reference_###  ###reference_### shows that correcting clean inputs is nearly harmless.\nmT5 never loses more than  COMET points.\nEven the weaker JamSpell never causes a degradation of more than half a COMET point, suggesting that it could be useful because of its fast runtime."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Correcting MTNT",
            "text": "###figure_7### ###figure_8### Social media data is a challenge for both of our source correctors because they were trained on other domains.\nUsing either corrected source leads to a decline of at least one COMET point compared to the raw source.\nAlthough an oracle delivers some gains, it still cannot equal the performance of finetuning when it is available.\nThese results seem to confirm that MTNT is not particularly noisy in terms of spelling errors (Karpukhin et al., 2019  ###reference_b15###  ###reference_b15###; Michel and Neubig, 2018  ###reference_b19###  ###reference_b19###).\nBased on the results in Table 13  ###reference_###  ###reference_###, it would be easy to conclude that source correction does not work for MTNT.\nHowever, Figure 5  ###reference_###  ###reference_### shows that some sentences do benefit from correction, and others are not harmed.\nIn these terms, mT5 appears to be a higher-risk corrector than JamSpell, improving translation quality for  of examples (versus  for JamSpell) while making quality worse for  of examples (versus  for JamSpell).\nThis, in addition to the oracle performance, suggests that further research could make source correction a useful tool in this domain."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We presented several experiments testing the robustness of MT systems to synthetic and natural noise.\nOn synthetic noise, we showed that large multilingual MT models and LLMs are far more robust than older techniques.\nThe experiments on social media translation showed that the same larger models also worked better on natural noise.\nWe added further support for this conclusion through reference-free translation experiments with a novel evaluation metric based on quality estimation.\nFinally, we exhibited circumstances in which pipeline-based source correction techniques can improve performance on noisy text, both synthetic and natural."
        }
    ]
}