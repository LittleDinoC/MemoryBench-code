{
    "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
    "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response.\nIn document-grounded response generation, for example, agent responses are expected to be relevant to a user’s query while also being grounded in a given document.\nIn this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response.\nProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time.\nWe apply ProMiSe to open source language models flan-t5-xxl and llama-2-13b-chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC,\ndemonstrating that self-refinement improves response quality.\nWe further show that fine-tuning llama-2-13b-chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The state-of-the-art large language models (LLMs) have demonstrated to be effective in generating new synthetic data, useful in improving zero-shot task generalization through fine-tuning without requiring vast amounts of human annotations. Various approaches have been proposed to show the ability of models to evaluate and critique responses Saunders et al. (2022  ###reference_b16###); Scheurer et al. (2023  ###reference_b17###); Shinn et al. (2023  ###reference_b18###); Ye et al. (2023  ###reference_b24###),\nas well as their potential to refine: given feedback, correct their outputs Welleck et al. (2022  ###reference_b22###); Peng et al. (2023  ###reference_b15###); Madaan et al. (2023  ###reference_b11###); Huang et al. (2023  ###reference_b9###); Wang et al. (2023b  ###reference_b21###). These explorations have studied various feedback mechanisms (human-in-the-loop, reward models to capture human preferences, model-generated feedback) and forms (pairwise comparisons, scalar scores, natural language descriptions), as well as refinement techniques (separate supervised refiners, domain-specific refinement).\nOf particular note are recent works exploring the self-refinement phenomenon Madaan et al. (2023  ###reference_b11###); Wang et al. (2023b  ###reference_b21###); Shinn et al. (2023  ###reference_b18###), leveraging the same LLM to perform critique and/or refinement on top of generating responses. The observations of these works unveil shortcomings: smaller instruction-tuned models fail to replicate the results of systems such as GPT-3.5 and GPT-4 in refinement, and in the absence of well-designed stopping mechanisms,\nself-refinement applied to high-quality responses can make the results worse Huang et al. (2023  ###reference_b9###). When humans correct themselves, they do it often with one or more objectives in mind, i.e. principles.\nSuch principles may include faithfulness, specificity, safety (i.e. non-toxic), relevance to a question posed, etc. and may vary across tasks — we seek to imbue these aspects into conversational agents, to ensure they are reflected in the agent’s responses.\n###figure_1### To this effect, we introduce an iterative, principle-guided approach to self-refinement in relatively smaller language models where refinement has previously proven unsuccessful. Our algorithm, termed Proxy Metric-based Self-Refinement (ProMiSe), combines proxy metric thresholding for different principles with independent principle-specific few-shot refinement and best-of-N rejection sampling. This allows for the deliberate selection of task-appropriate metrics with calibrated sufficiency thresholds, and specific prompts better designed to match the instruction-following capabilities of smaller models. In this manner, we perform multi-aspect self-refinement via iterative single-aspect improvement queries, as opposed to simultaneous refinement on many dimensions.\nWe apply this method to content-grounded question answering, demonstrating consistent improvements on a diverse set of evaluation metrics for single-turn response generation.\nWe then extend ProMiSe to multi-turn dialogue data generation to generate user queries in addition to agent responses. We fine-tune llama-2-13b-chat on the synthetic data, yielding significant improvement over the zero-shot baseline and supervised models solely fined-tuned on human annotations.\nCrucially, this approach is built on open-source models and does not rely on propietary models with black-box API access; we note, however, that the proposed algorithm can be directly applied to closed-source models as well. Furthermore, it can be extended to other tasks, provided that proxy metrics can be defined and a few in-context exemplars can be created for the relevant principles.\nOur key contributions are:\nWe introduce a novel domain-agnostic algorithm, ProMiSe, to perform multi-aspect self-refinement on desirable principles for a response through in-context learning, using proxy metrics as external quality feedback.\nProMiSe is applied to both content-grounded single-turn question answering and multi-turn dialogue generation. Extensive evaluations on MultiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics (RougeL, Bert-Recall, Bert-K-Precision, Recall, K-Precision) as well as LLM-as-judge with GPT-4, demonstrate its effectivenss both in few-shot and fine-tuning settings. We will release both the software and the synthetic dialogue data.\nWe analyze the relationship between the change in proxy metric scores and the downstream evaluation metrics, revealing an unsupervised correlation and reinforcing the efficacy of our method."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Algorithm",
            "text": "Given an input, e.g. a document and conversation history, ProMiSe executes three main steps: (i) Generate an initial response, (ii) Obtain external feedback via proxy metrics, and (iii) Refine the response with respect to each principle, if the response is deemed inadequate by the feedback mechanism. The last two steps are run iteratively until the response meets a quality threshold. We present a detailed description of these steps below.\nWe perform rejection sampling, this time on the set of refinement candidates, scoring with each metric in  and selecting the response, , with the highest scores on the majority of metrics. The scores of , the best refinement candidate, are then compared against the threshold . If the scores of  exceed the threshold on all  metrics, then we stop refinement and accept it as the final response. Otherwise, we now compare against the scores of the previous best response, . The user assigns weights  for the respective metrics in ; these importances should likely be informed by the principles in  which each metric corresponds to, and the user’s design goals. Then, given the scores for  and , we compute:\nFor each metric in , the indicator takes on a value of 1 if the new response is an improvement on the previous best response, with respect to that metric, or 0 otherwise, and is weighted by the elements in . If this sum fails to exceed a user-defined threshold of , we\ndo not update the best refinement response for this iteration (i.e. set ); else, we proceed to the next refinement iteration, until termination."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Initial Response Generation",
            "text": "For an input instance, we perform Best-of-N sampling to yield a set of responses, , from Language Model , given the input and an initial generation prompt. The initial generation prompt consists of an instruction and optional in-context demonstrations.\nThe instruction explicitly suggests that a response be generated which reflects desirable principles, the set of which is contained in . We determine the quality of the sampled responses based on a set of proxy metrics determined a priori, designated as . We note that the selected metrics should be designed by the user to improve alignment by reflecting the principle set for the response, , with respect to the current task. As such, each metric  is also predicated on the inputs provided which may be used as means to assess candidate responses – a text passage or document, conversation history, etc. (thus lending itself to the content-grounded setting). Each responses in  is scored with each metric  in , and the response with the highest scores on the greatest number of metrics is chosen as the best initial response, .\nNext, we determine the global sufficiency of  as an acceptable response, by comparing the proxy scores element-wise against a threshold , consisting of scalar values .  is the minimum value such that a response is deemed sufficient, for each metric  in . If the scores of  exceeds their respective thresholds, for all  components, we return it as the final response. If not (i.e.  fails to clear the threshold on at least one metric), we proceed to the refinement module."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Response Refinement",
            "text": "Our approach to response refinement is predicated on in-context exemplars of principle-specific refinement for the given task. The refinement prompt also contains the previous best response, denoted  — in the first iteration, this is equal to .\nAligning responses with multiple principles (i.e. where ) induces a multi-objective problem; rather than explicitly optimizing across the set simultaneously, we propose deliberate refinement with respect to one principle at a time, selecting an optimal candidate at each iteration based on the proxy metrics. For each iteration in the self-refinement phase, we loop through the set of principles  and generate a set of new responses, with the goal of each resulting response reflecting improvement on its respective principle. In each such query to Language Model , we introduce a principle-specific refinement prompt, consisting of in-context demonstrations of refinement and an instruction to improve the current best response, both with respect to the current principle. Examples of such prompts are contained in Appendix C  ###reference_###.\nWe perform rejection sampling, this time on the set of refinement candidates, scoring with each metric in  and selecting the response, , with the highest scores on the majority of metrics. The scores of , the best refinement candidate, are then compared against the threshold . If the scores of  exceed the threshold on all  metrics, then we stop refinement and accept it as the final response. Otherwise, we now compare against the scores of the previous best response, . The user assigns weights  for the respective metrics in ; these importances should likely be informed by the principles in  which each metric corresponds to, and the user’s design goals. Then, given the scores for  and , we compute:\nFor each metric in , the indicator takes on a value of 1 if the new response is an improvement on the previous best response, with respect to that metric, or 0 otherwise, and is weighted by the elements in . If this sum fails to exceed a user-defined threshold of , we\ndo not update the best refinement response for this iteration (i.e. set ); else, we proceed to the next refinement iteration, until termination."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evidence: Question Answering",
            "text": "We apply ProMiSe to content-grounded question answering: given a document and a conversation history, which may consist of a single user utterance (a question posed to the conversational agent) or a multi-turn dialogue between the user and the agent, we seek for the LLM to produce a response to the most recent user query.\nFor the generation of an initial response consistent with the content-grounded QA setting, we extract 3 instances from the MultiDoc2Dial Feng et al. (2021  ###reference_b6###) training data as in-context exemplars; the prompt template is included in Appendix C  ###reference_###. This includes the document, conversation history, and the gold response provided by the annotators. The in-context exemplars for query generation work similarly, with 3 demonstrations consisting of different conversation lengths (in number of utterances), but where the last utterance is the final user query.\nTo perform in-context refinement on a particular principle, we similarly take 3 in-context demonstrations from the training dataset, but seek to contrast between a better and worse response, with respect to the principle.\nTo accomplish this, we manually annotate a worse response for each instance relative to the gold response. In the prompt, we model this as 3 separate utterances: the worse agent response, a user turn probing the agent to improve its response to update along the principle, and another agent utterance containing the better response (i.e. the gold response). To more explicitly suggest the presence of a response quality difference, we include the tags “not {principle}” and “more {principle}”, for the two agent turns, respectively, where {principle} is either ‘specific’, ‘relevant’, or ‘accurate’.\nWe select three ROUGE metrics intended to correspond to each of the three principles. ROUGE-1 recall between the response and the document mostly represents specificity as more specific answers contain more details from the document. Next, we use ROUGE-L between the response and the document — this primarily addresses faithfulness, as a greater score would suggest a more extractive answer, which is clearly preferable to hallucinated facts. Finally, we compute ROUGE-L between the response and the conversation history to capture consistency between the user query and the response and relevance of the response to the query history.\nGiven a candidate response and the grounding document, WeCheck Wu et al. (2022  ###reference_b23###) addresses the faithfulness principle.\nOur experiments evaluate each model in three thresholding settings: solely using the three aforementioned ROUGE metrics, solely using the WeCheck model, and using a combination of both. If we use only WeCheck, rejection sampling is performed to yield the highest scoring response according to WeCheck\nand we determine whether a refined response constitutes an improvement solely using the WeCheck scores. If using both ROUGE and WeCheck, a sufficient response must clear the threshold on all four metrics. During refinement, we yield a reward indicator with each category (Rouge and reward model, i.e. WeCheck) which is 1 if deemed to have improved during the present iteration and 0 otherwise, and compute a weighted sum using a user-defined weight vector . If this sum is greater than 0.5, we update the best response to be the new one, else retain the previous best."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Set of Principles",
            "text": "We first identify an appropriate set of principles for the task, which define key characteristics of a good agent response. They are as follows:\nSpecificity.\nIf an initial response is too vague, this would likely lead to more user interactions asking the agent to make its response more specific.\nFaithfulness. We suggest that accurate, factual responses are those grounded in the document, and thus should have high (semantic and lexical) overlap with the document.\nRelevance and Consistency. The conversational agent response should be relevant to the most recent user query, and by induction to the entire conversation history."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "In-Context Demonstration Selection",
            "text": "We explore our algorithm through the generation of both a single agent response and an entire multi-turn dialogue.\nWe include the algorithm for multi-turn dialog generation in Appendix A  ###reference_###.\nFor the generation of an initial response consistent with the content-grounded QA setting, we extract 3 instances from the MultiDoc2Dial Feng et al. (2021  ###reference_b6###  ###reference_b6###) training data as in-context exemplars; the prompt template is included in Appendix C  ###reference_###  ###reference_###. This includes the document, conversation history, and the gold response provided by the annotators. The in-context exemplars for query generation work similarly, with 3 demonstrations consisting of different conversation lengths (in number of utterances), but where the last utterance is the final user query.\nTo perform in-context refinement on a particular principle, we similarly take 3 in-context demonstrations from the training dataset, but seek to contrast between a better and worse response, with respect to the principle.\nTo accomplish this, we manually annotate a worse response for each instance relative to the gold response. In the prompt, we model this as 3 separate utterances: the worse agent response, a user turn probing the agent to improve its response to update along the principle, and another agent utterance containing the better response (i.e. the gold response). To more explicitly suggest the presence of a response quality difference, we include the tags “not {principle}” and “more {principle}”, for the two agent turns, respectively, where {principle} is either ‘specific’, ‘relevant’, or ‘accurate’."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "External Proxy Metrics",
            "text": "To capture the aforementioned principles, we define relevant proxy metrics.\nThe proxy metrics should be reflective of response quality improvement along our chosen dimensions and should not directly optimize the final evaluation metrics.\nWe select three ROUGE metrics intended to correspond to each of the three principles. ROUGE-1 recall between the response and the document mostly represents specificity as more specific answers contain more details from the document. Next, we use ROUGE-L between the response and the document — this primarily addresses faithfulness, as a greater score would suggest a more extractive answer, which is clearly preferable to hallucinated facts. Finally, we compute ROUGE-L between the response and the conversation history to capture consistency between the user query and the response and relevance of the response to the query history.\nGiven a candidate response and the grounding document, WeCheck Wu et al. (2022  ###reference_b23###  ###reference_b23###) addresses the faithfulness principle.\nOur experiments evaluate each model in three thresholding settings: solely using the three aforementioned ROUGE metrics, solely using the WeCheck model, and using a combination of both. If we use only WeCheck, rejection sampling is performed to yield the highest scoring response according to WeCheck\nand we determine whether a refined response constitutes an improvement solely using the WeCheck scores. If using both ROUGE and WeCheck, a sufficient response must clear the threshold on all four metrics. During refinement, we yield a reward indicator with each category (Rouge and reward model, i.e. WeCheck) which is 1 if deemed to have improved during the present iteration and 0 otherwise, and compute a weighted sum using a user-defined weight vector . If this sum is greater than 0.5, we update the best response to be the new one, else retain the previous best."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Results and Discussion",
            "text": "We use two widely-used open-source language models to evaluate our algorithm for content-grounded question answering, Flan-T5-XXL Chung et al. (2022  ###reference_b4###) and Llama-2-13B-chat Touvron et al. (2023  ###reference_b19###)\nWe evaluate the technique on the test dataset of MultiDoc2Dial Feng et al. (2021  ###reference_b6###) (https://doc2dial.github.io/multidoc2dial/), content-grounded dialogues, and the validation dataset of QuAC Choi et al. (2018  ###reference_b3###) (https://quac.ai/), short form question-answering. Both datasets feature conversations wherein answers to queries posed by the user are expected to come from a document.111We use a sub-document split on MultiDoc2Dial, to remove the information retrieval (IR) component such that we only have the most relevant document as opposed to the entire set of candidate documents. We use the validation dataset of QuAC as the test data since the testset is not publicly available.\nWe use five automatic evaluation metrics: ROUGE-L Lin (2004  ###reference_b10###), BERTScore\nRecall, BERTScore K-Precision (K-Prec. hereafter), Zhang et al. (2020  ###reference_b26###), Recall, and K-Precision. ROUGE-L, BERTScore Recall and Recall measure the agreement between the candidate response and the provided gold response. BERTScore K-Prec. and K-Prec. measure the agreement between the candidate response and the grounding document. We chose Recall and K-Prec. metrics due to their strong correlation with human assessments of instruction-following models in content-grounded QA tasks, Adlakah et al. (2023  ###reference_b1###).\nWe explore the relationship between the improvement in the final evaluation metrics and the direction of change on the proxy metrics in ProMiSe. That is, is improvement on the proxy sufficiency metrics during the execution of the algorithm correlated with the downstream evaluation metric improvement from initial to final response?\nThe relationship between the proxy metric scores and the Rouge-L and BERTScore-Recall evaluation metrics is shown in Table 4  ###reference_###.\nWe find that the chosen proxy metrics appear to serve as an unsupervised link to the final evaluation metrics. The number of samples that improve for each proxy metric change are roughly similar, a trend noticeable across settings. Furthermore, a greater degree of improvement on proxy metrics (e.g. improving on all three ROUGE metrics) generally corresponds to a larger average improvement (or less negative change) for Rouge-L and BERTScore-Recall with respect to the gold response. This highlights the value of our external metric feedback technique: by optimizing on a scoring scheme while simultaneously preserving the integrity of the downstream evaluation metrics, we can capture a similar notion of response quality and sufficiency."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Multi-Turn Synthetic Dialogues",
            "text": "We generate synthetic dialogues of varying lengths from Flan-T5-XXL, containing refinement instances: the initial response, a user query to improve the response along a principle, and the refined response.\nThe dialogues are generated from scratch, bootstrapping solely on the grounding documents in MultiDoc2Dial training data.\nThey alternate between user and agent utterances, and consist of 1-3 agent responses (thus containing total 2, 4, or 6 turns). We sampled 10k dialogues with 2 turns, 2k with 4 turns and another 2k with 6 turns. We QLoRA fine-tune Dettmers et al. (2023  ###reference_b5###) Llama-2-13B-Chat model on these synthetic data. See Section D  ###reference_### for details.\nThe results are shown in Table 2  ###reference_###.\nWe observe sizable improvements across all metrics when comparing the performance without refinement, denoted initial, as opposed to with refinement, denoted final. Notably, these improvements are present on both lexical and semantic similarity measures; +6-6.75% for both BERT-Recall and Recall, and +7.5-8% for BERT K-Precision and K-Precision. Furthermore, merging the synthetic data with 38k samples of human annotated data from the MultiDoc2Dial train set yields improvements over solely training on human annotated data.\nThese results suggest the value of response quality refinement in generating high-quality synthetic data and yielding downstream improvements on evaluation metrics.\n###figure_2###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "LLM-as-a-Judge Evaluation",
            "text": "We also perform automated evaluation with GPT-4 as a judge, Zheng et al. (2023  ###reference_b27###), which has been shown strongly correlate to human evaluation. Given the initial and final generations, we prompt the model to impartially assess which response is better. We largely adapt the prompts used for MT-bench evaluation in Zheng et al. (2023  ###reference_b27###), which we show in Appendix F  ###reference_###.\nFor MultiDoc2Dial, we randomly sample 2,551 of the indices of the test set responses (exactly one quarter), and only perform evaluation on samples for which the initial and final responses differ as a result of refinement. With the QuAC dataset, analyze all 1,000 test set instances, likewise evaluating where initial and final responses differ. The results are shown in Figure 2  ###reference_###, where the numbers in percentage are the win rate of each response.\nWe find that GPT-4 deems the final response to be better than the initial response on all conditions for MultiDoc2Dial. The relative outlier is the QuAC dataset with RM-only; this is likely because WeCheck measures entailment rather than agreement. Often the correct short response is less likely to be entailed than an incorrect longer response by the grounding document. However, a higher win rate with the ROUGE + RM combination validates the complementary nature of our proxy metrics. Furthermore, the strong correlation between the automatic evaluation metrics in Table 1  ###reference_### with the GPT-4 evaluation results in Figure 2  ###reference_### evidences the efficacy of our algorithm."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Other Related Work",
            "text": "Various work on self-refinement may be distinguished according to the source of feedback, Pan et al. (2023  ###reference_b13###); Huang et al. (2023  ###reference_b9###). Internal feedback relies on the model’s inherent knowledge and parameters to reassess its outputs. External feedback incorporates inputs from humans, other models. Our work is inspired by Madaan et al. (2023  ###reference_b11###). Unlike Madaan et al. (2023  ###reference_b11###), however, who rely on very large LLMs (GPT-3.5, ChatGPT, GPT-4) as the source of internal feedback, we introduce external feedback with proxy metrics and enable self-refinement technique to work with relatively small LLMs including Flan-T5-XXL and Llama-2-13B-Chat in content-grounded setups.\nRegarding internal feedback, Bai et al. (2022  ###reference_b2###) experiment with method for training a harmless AI assistant through self-improvement. Wang et al. (2023a  ###reference_b20###) propose Shepherd, a language model tuned to critique its own responses and suggest refinements.\nAs for external feedback, Paul et al. (2023  ###reference_b14###) propose refiner, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Gou et al. (2023  ###reference_b8###) propose critic that interacts with appropriate tools, e.g. calculator, search engine, wikipedia, etc., to evaluate certain aspects of the text and then revise the output based on the feedback obtained during the validation process. Olausson et al. (2024  ###reference_b12###) critically examines the LLM’s ability to perform self-repair on problems taken from HumanEval and APPS and concludes that self-repair still lags behind what can be achieved with human-level debugging. Gao et al. (2023  ###reference_b7###) propose RARR (Retrofit Attribution using Research and Revision) that revises a generated text on the basis of the relevant evidence retrieved by re-search."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present a novel algorithm, ProMiSe, for self-refinement of language models.\nProMiSe uses external multi-aspect feedback via proxy metrics capturing desirable principles for a high-quality response.\nProMiSe is applied to content-grounded single-turn question answering and multi-turn dialogue generation. Extensive evaluations on MultiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics as well as LLM-as-a-judge with GPT-4, demonstrate its effectiveness in both few-shot learning and supervised fine-tuning setups. This approach crucially enables relatively small LMs like Flan-T5-XXL and Llama-2-13B-Chat to successfully perform self-refinement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our work employs two open-source LMs: flan-t5-xxl and llama-2-13b-chat. Therefore, the generated data, including the synthetic multi-turn dialogues, can be susceptible to the limitations of such LMs, particularly the biases inherent in the training data which may be harmful with hate, abuse and social stereotypes. We have tested the algorithm ProMiSe on English only although it would have been more desirable to verify the value of the algorithm in multi-lingual setups. We have conducted extensive evaluations including 5 well-known automtic evaluation metrics and LLM-as-a-judge with GPT-4, which has been shown to correlate well with human evaluations. Nonetheless, inclusion of human evaluation would have strengthened our position further."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics and Impact",
            "text": "Our technique can be used to guide generations towards user-specified targets; however, this could be applied to generate toxic or malicious content, by way of an adversarial principle selection. Nonetheless, we note that ProMiSe does present meaningful implications in enabling alignment to human preferences (where preferences, in this setting, refer to the user-defined principles). We will release the software for the ProMiSe algorithm, enabling others in the community to consider other principles of interest, or applications to other tasks."
        }
    ]
}