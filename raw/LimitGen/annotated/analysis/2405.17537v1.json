{
    "title": "BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
    "abstract": "Measuring biodiversity is crucial for understanding ecosystem health.\nWhile prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, DNA barcodes, and textual data in a unified embedding space.\nThis allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse DNA and image data.\nOur method surpasses previous single-modality approaches in accuracy by over 11% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As environmental change and habitation loss accelerate, monitoring biodiversity is crucial to understand and maintain the health of ecosystems.\nTaxonomic classification of organisms at scale is especially important for understanding regional biodiversity and studying species interactions.\nRecently, computer vision techniques have been used to classify species observed in images [17  ###reference_b17###, 58  ###reference_b58###, 60  ###reference_b60###, 40  ###reference_b40###] including for ecological monitoring [11  ###reference_b11###].\nHowever, relying solely on images for identifying and classifying organisms fails to consider the rich evolutionary relationship between species and may miss fine-grained species differences.\nTo better capture these distinctions, researchers have used DNA sequences for tasks such as genome understanding and taxonomic classification [26  ###reference_b26###, 66  ###reference_b66###, 42  ###reference_b42###, 8  ###reference_b8###, 3  ###reference_b3###].\nIn particular, DNA barcodes [23  ###reference_b23###], small sections of DNA from specific genes such as the COI gene [38  ###reference_b38###] in mitochondrial DNA, are useful for species identification [3  ###reference_b3###].\nHowever, collecting DNA requires specialized equipment and is more expensive and less accessible than images.\nA desirable approach is to use approaches that leverage DNA information at training time but only need images of new specimens at inference time [5  ###reference_b5###].\nMachine learning advances have made it possible to combine information from different modalities.\nFor instance, CLIP [47  ###reference_b47###] used contrastive learning to encode text (e.g., “cat”) and images (e.g., a photo of a cat) into a common space for zero-shot classification.\nA recent model BioCLIP [52  ###reference_b52###] aligned images of organisms with common names and taxonomic descriptions to classify plants, animals, and fungi. Most prior work only uses a subset of modalities (image only [17  ###reference_b17###, 58  ###reference_b58###, 60  ###reference_b60###], DNA only [26  ###reference_b26###, 66  ###reference_b66###, 42  ###reference_b42###, 8  ###reference_b8###, 3  ###reference_b3###], text and image [52  ###reference_b52###]) rather than combining information from images, text, and DNA barcodes.\nThey also often require a text description or complete taxonomic annotations, which are expensive and time-consuming to obtain.\nIn this work, we propose BIOSCAN-CLIP which uses contrastive learning to map biological images, textual taxonomic labels, and DNA barcodes to the same latent space.\nWith this aligned space, we do not need comprehensive or noise-free taxonomic annotations.\nWe flexibly take either images or DNA barcodes as input to predict the taxonomy.\nOur embedding space also enables future research leveraging multiple modalities to examine commonalities and differences between species.\nAside from ecological benefits, building such a foundation model for biodiversity provides a case study of broader challenges in identifying fine-grained differences, both visually and textually.\nVisual differences between species are often subtle, and the DNA and taxonomic labels do not have much semantic overlap with everyday natural language.\nThus, tokens for DNA and text from taxonomic labels are different from tokens typically found in large language models trained on internet data.\nWe showcase the benefits of pretraining with all three modalities through improved taxonomic classification accuracy over prior works in both retrieval and zero-shot settings."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "We review work on pretraining multimodal models on images, DNA, and text, including foundation models and multimodal learning with fine-grained images and their application in biological problems."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Fine-grained multimodal learning",
            "text": "Recent work on vision and language addresses the challenge of distinguishing between highly similar categories [22  ###reference_b22###, 51  ###reference_b51###].\nRadford et al. [47  ###reference_b47###] showcased how natural language supervision can significantly improve visual models. Contrastive learning on over 400 million pairs of images and text enabled matching multi-modal data and zero-shot transfer across diverse tasks. Later work built on this architecture to improve the embedding space [9  ###reference_b9###, 16  ###reference_b16###, 34  ###reference_b34###, 20  ###reference_b20###, 27  ###reference_b27###, 62  ###reference_b62###] or implement support for generation tasks [31  ###reference_b31###, 32  ###reference_b32###, 33  ###reference_b33###].\nWei et al. [60  ###reference_b60###] provide a comprehensive survey on deep learning for fine-grained image analysis.\nRecent works train domain-specific foundation models on large corpora of image or natural language datasets, leveraging similar architectures and training strategies [10  ###reference_b10###, 21  ###reference_b21###, 52  ###reference_b52###]. Other approaches extend the multimodal architecture to more than two modalities, including audio, video, or 3D representations [49  ###reference_b49###, 1  ###reference_b1###, 2  ###reference_b2###, 45  ###reference_b45###, 63  ###reference_b63###, 39  ###reference_b39###, 19  ###reference_b19###].\nWe demonstrate the value of applying similar strategies for pretraining foundation models to the problem of biodiversity monitoring."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Models for DNA and biological images",
            "text": "There is much work on machine learning for DNA data, especially in genome understanding [35  ###reference_b35###, 29  ###reference_b29###, 4  ###reference_b4###, 30  ###reference_b30###].\nRecently, there is increasing interest in developing foundation models of DNA sequences [55  ###reference_b55###, 26  ###reference_b26###, 66  ###reference_b66###, 67  ###reference_b67###, 3  ###reference_b3###, 13  ###reference_b13###, 43  ###reference_b43###, 8  ###reference_b8###].\nMany of these works leverage transformer architectures and self-supervised learning techniques such as masked language modeling [14  ###reference_b14###] to pretrain models on large DNA datasets for downstream genome analysis tasks such as promoter prediction or metagenomics binning [26  ###reference_b26###, 66  ###reference_b66###, 67  ###reference_b67###, 3  ###reference_b3###].\nZhou et al. [67  ###reference_b67###] incorporate curriculum-based contrastive learning to learn a “species-aware” embedding space for DNA.\nOther work explored using DNA data for taxonomic classification [36  ###reference_b36###, 42  ###reference_b42###, 3  ###reference_b3###].\nBERTax [42  ###reference_b42###] pretrained a BERT [14  ###reference_b14###] model for hierarchical taxonomic classification. However, they focused on superkingdom, phylum, and genus classification, which are coarser-grained categories and thus easier than classifying species. BarcodeBERT [3  ###reference_b3###] showed that models pretrained on DNA barcodes, rather than general DNA datasets, can be particularly effective for taxonomic classification.\nOur work extends these models for encoding DNA by learning a shared embedding space with images and text as well, addressing issues with the cost of obtaining DNA data in practice by allowing for cross-modal image queries.\nThere is also work on taxonomic classification based on images of plant, bird, and insect species [7  ###reference_b7###, 58  ###reference_b58###, 46  ###reference_b46###]. Methods for coarse supervision help learn fine-grained taxonomic categories, given the number and rarity of species [56  ###reference_b56###, 48  ###reference_b48###, 54  ###reference_b54###]. Contrastive learning has been used to better differentiate between fine-grained species characteristics [12  ###reference_b12###, 61  ###reference_b61###]. However, species differences are not necessarily easily detectable by visual cues alone. To that end, we leverage DNA data in addition to images to classify species, while still preserving the benefits of the relative ease of acquiring visual data of new organisms."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Multimodal models for biology",
            "text": "Prior works explored building pretrained models based on only images or DNA, and largely relied on fine-tuning classifiers on a set of known species. This limits those approaches to a closed set of species, whereas we are concerned with being able to identify unseen species, i.e. those for which we have no examples in the modality of interest. Furthermore, these works are limited to single modalities.\nRecent works have begun building multimodal models for biological applications [25  ###reference_b25###, 37  ###reference_b37###, 64  ###reference_b64###], including several works in taxonomic classification [52  ###reference_b52###, 44  ###reference_b44###, 5  ###reference_b5###, 6  ###reference_b6###].\nNguyen et al. [44  ###reference_b44###] introduced Insect-1M, a large-scale dataset with images annotated with their taxonomic levels (from class to species) and dense text descriptions. Their method applied contrastive learning across text and image modalities with a patch-based attention to build an “insect foundation” model.\nBioCLIP [52  ###reference_b52###] applied multimodal contrastive pretraining on images and text descriptions to a larger scale, collecting multiple datasets into the TreeOfLife-10M dataset.\nWhile achieving impressive results, these models and datasets only consider images and text, thus being more limited with new species, in which taxonomic labels are not available for alignment. They do not take advantage of the rich taxonomic knowledge available in sources like the Barcode of Life Datasystem (BOLD), that at the time of writing is approaching 15 M validated DNA barcodes, most of which have associated expert-assigned taxonomic labels.\nBadirli et al. [5  ###reference_b5###] employed a Bayesian zero-shot learning approach using DNA to model priors for species classification by image.\nThey relate unseen species to nearby seen species in the DNA embedding space, given barcode data for both, in order to construct Gaussian priors by which to classify images. Badirli et al. [6  ###reference_b6###] employ similar Bayesian techniques and combine image and DNA embeddings in the same space to predicting the genus for unseen species, assuming that genera are largely already discovered. While this approach learns to project between embedding spaces of different modalities, the alignment is not end-to-end optimized, limiting the amount of alignment which can be obtained.\nWe show that taking this BZSL approach and using our aligned embedding space of image and DNA learned using a contrastive loss, we can have a more accurate model.\nBy incorporating text into our model during pretraining, we can also leverage taxonomic annotations more explicitly, when available, without relying on their presence.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "To align representations of images, DNA barcodes, and taxonomy labels as text, we start with pretrained encoders for each modality and then apply LoRA finetuning [24  ###reference_b24###] with a multimodal contrastive loss (see Figure 1  ###reference_###).\nDuring inference, we use our fine-tuned encoders to extract features for a query image and match against a database of image and DNA embeddings (keys) for which the taxonomic information is already known.\nTo classify the query image,\nwe\ntake\nthe taxonomic information associated with the most closely matched key.\nWhile we can also query against the taxonomies themselves, the labels may be incomplete or unknown. Thus, images and DNA barcodes comprise a more robust and defining set of records against which to query."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "Contrastive learning.\nWe base our approach on a contrastive learning scheme similar to CLIP [47  ###reference_b47###], which uses large-scale pretraining to learn joint embeddings of images and text.\nIn contrastive learning, embeddings for paired samples are pulled together while non-paired samples are pushed apart, thus aligning the semantic spaces for cross-modal retrieval.\nFollowing prior work [49  ###reference_b49###], we extend CLIP [47  ###reference_b47###] to three modalities by considering the modalities in pairs with the NT-Xent loss [50  ###reference_b50###] between two modalities to align their representations.\nLet matrices , , and  represent the batch of -normalized embeddings of the image, DNA, and text modalities.\nThe -th row of each corresponds to the same instance corresponding to a physical specimen, thus rows  and  are features from the same sample, forming a positive pair.\nFeatures in different rows  and , , come from different samples and are negative pairs. The contrastive loss for pair  is\nwhere  is a fixed temperature, based on the initial value used in [47  ###reference_b47###]. We apply the loss symmetrically to normalize over the possible paired embeddings for each modality [65  ###reference_b65###, 49  ###reference_b49###].\nWe repeat this for each pair of modalities and sum them to get the final loss, .\nPretrained encoders.\nFor each data-modality, we a pretrained model to initialize our encoders.\nImages: we use a ViT-B network111Loaded as vit_base_patch16_224 in the timm library. pretrained on ImageNet-21k and fine-tuned on ImageNet-1k [15  ###reference_b15###].\nDNA barcodes: we use BarcodeBERT [3  ###reference_b3###] with -mer tokenization, pretrained on about 893K DNA barcodes using masked language modelling. The training data for BarcodeBERT is highly similar to the DNA barcodes in the BIOSCAN-1M dataset making it ideal for our study.\nText: we use the pretrained BERT-small introduced by Turc et al. [57  ###reference_b57###] to encode taxonomic labels.\nLow-rank adaptation.\nTo efficiently fine-tune the pretrained transformer models, we apply Low-Rank Adaptation (LoRA) [24  ###reference_b24###], a method for fine-tuning large neural network models that greatly reduces the number of trainable parameters.\nFor our LoRA implementation, we add an additional low-rank residual layer to the query and key projectors of each attention module. The projectors thus take the form , where  represents the frozen parameters of the pretrained weights from the projector, whilst  and  represent the added low-rank weights. By choosing a rank  much smaller than the input and output dimensions  and , fewer parameters need to be updated with LoRA than with the original layer ().\nBy using LoRA, we are able to train with less parameters and less memory, allowing us to train with larger batch sizes on limited resources.\nThis is especially important for contrastive learning as\nincreasing the batch size allows for more positive and negative pairs when calculating the contrastive loss, and thereby improves learning effectiveness.\nExperimentally, LoRA reduces the number of trainable parameters from 203M to 1.9M. With batch size 400, the model requires just 71.5GB, allowing us to train on an A100 with 80GB, which would be infeasible without LoRA."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Inference",
            "text": "To use the model for predicting taxonomic labels, we calculate the cosine similarity between the embedded input image (query) and reference image or DNA embeddings (keys) sampled from available species. We take the taxonomic label (order, family, genus, species) associated with the closest key as the prediction.\nThis method allows us to evaluate our model in a zero-shot setting on species which were not seen by the model during the LoRA fine-tuning, as it is not constrained to predict within a fixed set of classes. The embedding space also gives us the flexibility to use the representation in other downstream models, such as a supervised classifier or a Bayesian model similar to [5  ###reference_b5###, 6  ###reference_b6###]."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Task and data",
            "text": "To evaluate our method, we perform taxonomic classification given an input image. The input is a biological image (query) along with a reference set of labelled DNA barcodes, other labelled biological images (key), or known taxonomic labels encoded as text (as demonstrated in Table 5  ###reference_###). We match the input feature with the closest neighbours in a database using aligned representations and assess accuracy across taxonomic levels by averaging over samples and taxon classes.\nThe predictions are evaluated at each taxonomic level by averaging accuracy over samples (micro) and taxon classes (macro).\nUnlike basic fine-tuning with a fully connected layer, our approach identifies unseen species using images or DNA without knowing all potential species upfront. Instead, we use reference features for taxonomic prediction or novel class identification. We split our data so that some species are “unseen” during training, and report prediction accuracy for both seen and unseen species to study model generalization.\n###figure_2### Dataset.\nWe use the BIOSCAN-1M dataset [18  ###reference_b18###], a curated collection of over one million insect data records. Each record in the dataset includes a high-quality insect image, expert-annotated taxonomic label, and a DNA barcode.\nHowever, the dataset has incomplete taxonomic labels, with fewer than 10% of records labelled at the species level. This poses a challenge for conventional supervised methods, which would require species-level annotations, but our method is able to flexibly leverage partial or missing taxonomic information during contrastive learning.\nThe dataset also possesses a long-tailed class imbalance, typical of real-world biological data, presenting a challenge for modelling.\nGiven the vast biodiversity of insects, with an estimated 80% undescribed [53  ###reference_b53###], and the necessity to discern subtle visual differences, this dataset offers a significant challenge and opporutnity for our model.\nData partitioning.\nWe split BIOSCAN-1M into train, validation, and test sets to evaluate zero-shot classification capabilities and model generalization to unseen species.\nRecords for well-represented species (at least 9 records) are partitioned at an 80/20 ratio into seen and unseen, with seen records allocated to each of the splits and unseen records allocated to validation and test.\nAll records without species labels are used in contrastive pretraining, and any species with 2–8 records are added evenly to the unseen splits in the validation and test sets.\nImportantly, we ensure that any unseen species are mutually exclusive between the validation and test sets and likewise do not overlap with seen species for labeled records.\nFinally, among each of the seen and unseen sub-splits within the validation and test sets, we allocate equal proportions of records as queries, to be used as inputs during evaluation, and keys, to be used as our reference database.\nSee Figure 2  ###reference_### for split statistics and Appendix A  ###reference_### for further details.\nData preprocessing.\nDuring inference, we resized images to  and applied a  center crop. For the DNA input, following Arias et al. [3  ###reference_b3###], we set a maximum length of 660 for each sequence and tokenized the input into non-overlapping 5-mers. Finally, similar to Stevens et al. [52  ###reference_b52###], we concatenated the taxonomic levels of the insects together as text input. As we did not have the common names of each record, we used the order, family, genus, and species, up to known labels. With this approach, we can still provide the model with knowledge of the higher-level taxonomy, even if some records do not have species-level annotations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###figure_3### We study the model’s ability to retrieve taxonomic labels using images in the BIOSCAN-1M dataset [18  ###reference_b18###] of species that were either seen or unseen during contrastive learning.\nWe also experiment on the INSECT dataset [5  ###reference_b5###] for Bayesian zero-shot learning (BZSL) species classification. We report the top-1 accuracy for the seen and unseen splits, as well as their harmonic mean (H.M.). In the main paper, we focus on evaluation on the validation set using image embeddings as queries as images are the most readily available modality.\nIn the appendices, we provide full results on both the validation and test set (Appendix B.1  ###reference_###), experiments that query with DNA features (Appendix B.2  ###reference_###), and visualization of the aligned embedding space (Appendix B.3  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Comparison with BioCLIP",
            "text": "Next we compare our aligned embedding space with that of BioCLIP [52  ###reference_b52###] by adapting their zero-shot learning demo script to perform species-level image classification. We use their pretrained model on the BIOSCAN-1M validation set, with image or text embeddings as keys.\nKeeping the experimental setups almost identical, we compared BioCLIP with BIOSCAN-CLIP in Table 5  ###reference_###.\nFor BioCLIP, we combined the four concatenated taxonomic levels with their provided openai_templates as text input, while for BIOSCAN-CLIP, we used the concatenated labels only.\nWhen using images as keys, BIOSCAN-CLIP consistently outperformed BioCLIP, even when BIOSCAN-CLIP was trained only on images and text. Since BioCLIP was trained on a much broader dataset, including but not limited to BIOSCAN-1M, it may have performed worse on insects as it was also trained on non-insect domains.\nBIOSCAN-CLIP can also leverage DNA features during inference, while BioCLIP is limited to image and text modalities.\nWhen using text as keys, BIOSCAN-CLIP performed better than BioCLIP on seen species but marginally worse on unseen species. This is expected, as BIOSCAN-CLIP was not trained on the unseen species names, which we also would not know prior in practice.\nThis illustrates that text typically does not perform well as keys and reinforces the benefit of using DNA in pretraining and inference."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Bayesian zero-shot learning",
            "text": "We also use our learned embeddings in Bayesian zero-shot learning (BZSL) [5  ###reference_b5###] to show the benefit of our learned embeddings for species classification\non the INSECT dataset [5  ###reference_b5###], which contains 21,212 pairs of insect images and DNA barcodes from 1,213 species.\nWe compare different combinations of image and DNA encoders. As baselines, we use a ResNet-101 image encoder, pretrained on ImageNet-1K (used in Badirli et al. [5  ###reference_b5###]), and the ViT-B [15  ###reference_b15###] image encoder, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k. For DNA encoders, we evaluate the baseline CNN from Badirli et al. [5  ###reference_b5###]; DNABERT-2 [66  ###reference_b66###], a BERT-based model trained on multi-species DNA data; and BarcodeBERT [3  ###reference_b3###], which was pretrained on arthropodic DNA barcode data.\nTable 6  ###reference_### shows that using the baseline image encoder with BIOSCAN-CLIP-D surpasses all baseline methods in harmonic mean even without fine-tuning on the INSECT dataset, performing particularly better on unseen species. Furthermore, using BIOSCAN-CLIP-I improves performance in all metrics over the baseline image encoder, with the highest performance after fine-tuning of 56.2% seen accuracy and 28.8% unseen accuracy. Thus, our model demonstrates the benefits of learning a shared embedding space relating image and DNA data, both in performance and the flexibility of applying to downstream tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce BIOSCAN-CLIP, an approach for integrating biological images with DNA barcodes and taxonomic labels to enhance taxonomic classification by using contrastive learning to align embeddings in a shared latent space.\nDue to their low-cost and ease of acquisition, images are the most practical modality for fostering inclusive participation in global biodiversity tracking efforts.\nWe show the BIOSCAN-CLIP embedding space can be applied to fine-grained retrieval tasks for seen and unseen species, and leveraged flexibly for downstream tasks such as zero-shot learning. Underrepresented and unseen species pose the greatest challenge for our model, presenting an opportunity for future work."
        }
    ]
}