{
    "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
    "abstract": "Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood (LL) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) LL-based performance is still inferior to human performance; (iii) instruction-tuned models have worse LL-based performance than base models. In Experiment 2, we show that LL scores across models are modulated by context in the expected way, showing high performance on three metrics of context-sensitive plausibility and providing a direct match to explicit human plausibility judgments. Overall, LL estimates remain a more reliable measure of plausibility in LLMs than direct prompting.111Our results and code are available at https://github.com/carina-kauf/llm-plaus-prob.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The impressive empirical successes of large language models (LLMs) on many diverse (language) tasks (e.g., Devlin et al., 2019  ###reference_b14###; Liu et al., 2019  ###reference_b37###; Brown et al., 2020  ###reference_b9###; Achiam et al., 2023  ###reference_b1###; Bubeck et al., 2023  ###reference_b10###; Guo et al., 2023  ###reference_b21###) has fueled an explosive increase in their popularity. As LLMs are becoming more and more integrated in people’s everyday lives, it is critical to provide reliable assessments of their capabilities.\nAn important domain to test is LLMs’ general world knowledge. Language training data contains vast amounts of information about the world, including both factual knowledge explicitly stated in the input and distributional knowledge, inferrable via text co-occurrence patterns (Elazar et al., 2022  ###reference_b16###; Kang and Choi, 2023  ###reference_b29###). Leveraging world knowledge is important both for specific NLP tasks (e.g., information retrieval) and for general success of a language model during interactions with a user (e.g., establishing common ground).\nWe focus on one particular way to assess general world knowledge: estimates of sentence plausibility. Plausible sentences conform with world knowledge whereas implausible sentences violate it; thus, the ability to distinguish plausible and implausible sentences is an indicator of underlying world knowledge capabilities.\nTraditionally, NLP researchers evaluated the knowledge that LLMs distill into their weights through a combination of log likelihood comparisons on minimal sentence pairs Futrell et al. (2019  ###reference_b18###); Warstadt et al. (2020  ###reference_b57###); Hu et al. (2020  ###reference_b24###); Aina and Linzen (2021  ###reference_b2###); Pedinotti et al. (2021  ###reference_b48###); Sinha et al. (2022  ###reference_b54###); Hu et al. (2024  ###reference_b26###); Michaelov et al. (2023  ###reference_b40###); Misra et al. (2024  ###reference_b41###), probing the model’s representations of a stimulus Hewitt and Manning (2019  ###reference_b22###); Kim et al. (2019  ###reference_b32###); Eisape et al. (2022  ###reference_b15###); Müller-Eberstein et al. (2022  ###reference_b45###); Kauf et al. (2023  ###reference_b31###), adversarial datasets McCoy et al. (2019  ###reference_b38###); Kassner and Schütze (2020  ###reference_b30###), or causal interventions Geiger et al. (2020  ###reference_b19###), among others. Given the closeness to the unsupervised pretraining regime, minimal sentence pair comparisons of likelihood measures, in particular, have been widely adopted.\nMore recently, however, the focus of NLP researchers has shifted towards LLMs that have been fine-tuned to follow instructions (Chung et al., 2022  ###reference_b13###; Touvron et al., 2023  ###reference_b55###; Almazrouei et al., 2023  ###reference_b3###; Jiang et al., 2023  ###reference_b27###), as instruction tuning improves the alignment of the models with user intent and leads to better generalization to unseen tasks (Ouyang et al., 2022  ###reference_b47###). Because instruction-tuned models are designed to interact directly with a user through LLM-directed queries/prompts, natural language prompting has emerged as a way to directly query LLMs for the knowledge they encode (e.g., Li et al., 2022  ###reference_b36###; Blevins et al., 2023  ###reference_b8###). Critically, as access to log probabilities for newer models becomes restricted, it is important to understand what knowledge can be accessed, and what knowledge is inaccessible to the experimenter if prompting is the only way to interact with LLMs.\nThe link between sentence plausibility and sentence probability is indirect: raw log probabilities have been shown to reflect a number of factors that might not be relevant for a given task, including low-level properties of the stimulus such as sentence length, word frequency Kauf et al. (2023  ###reference_b31###), and the number of surface forms that refer to the same concept Holtzman et al. (2021  ###reference_b23###). Thus, direct prompting approaches might provide a more direct estimate of plausibility by filtering out influences of those additional factors. However, initial direct comparisons of log likelihood and prompting measures on different linguistic/semantic knowledge datasets has revealed that prompting may systematically underestimate the model’s internal knowledge by requiring the models not only to solve the task, but also to correctly interpret the prompt and to translate their answer into the desired output format Hu and Levy (2023  ###reference_b25###); Hu et al. (2024  ###reference_b26###).\nIn this paper, we test LLMs’ knowledge of plausibility in single-sentence (Experiment 1) and contextualized scenarios (Experiment 2). Our findings include:\nLog likelihood (LL) scores, while imperfect, are a more dependable measure of plausibility than natural language prompting evaluations.\nInstruction-tuning often alters an LLM’s log-likelihood scores in such a way that they become less consistent with human plausibility judgments relative to base model versions.\nLL scores can effectively model the contextual plausibility of events and replicate key patterns of human plausibility-judgment behaviors. Nevertheless, the LLMs’ ability to detect an implausibility within a target sentence locally does not reliably affect their evaluation of the full sentence."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Evaluating single-sentence plausibility in LLMs.    In Experiment 1, we evaluate plausibility estimates for single sentences describing common events (Table 1  ###reference_###). Earlier work in NLP aimed at modeling event-based semantic plausibility via distributional models of thematic fit: verbs and arguments were often considered in isolation, and the goal for the models was to estimate a continuous score expressing to what extent an argument noun (e.g., ball) was fitting a given semantic role of a verb (e.g., the patient role of to throw) (Baroni and Lenci, 2010  ###reference_b5###; Sayeed et al., 2016  ###reference_b52###; Santus et al., 2017  ###reference_b51###). In a more natural evaluation setting, researchers used sentence pairs derived from psycholinguistic experiments that differed only for one argument and displayed different degrees of plausibility (e.g., The mechanic was checking the brakes vs. The journalist was checking the brakes, from Bicknell et al., 2010  ###reference_b7###): in this case, a distributional model had to dynamically “compose” the plausibility of the two argument roles and guess which of the two sentences was the most plausible one (binary judgement task) (Lenci, 2011  ###reference_b35###; Chersoni et al., 2019  ###reference_b11###).\nWith the advent of Transformer-based language models, the analysis of their semantic knowledge has often been framed as a probability comparison between sound and anomalous, or atypical sentences (Michaelov and Bergen, 2020  ###reference_b39###; Beyer et al., 2021  ###reference_b6###; Pedinotti et al., 2021  ###reference_b48###; Kauf et al., 2023  ###reference_b31###; Misra et al., 2023  ###reference_b43###). Similarly to the binary judgement setting, a model has to score a sentence pair where two sentences differ only for the presence of a semantic violation, and assign a higher score to the plausible one.\nPedinotti et al. (2021  ###reference_b48###) and Kauf et al. (2023  ###reference_b31###) specifically tested event plausibility knowledge in LLMs. Pedinotti et al. (2021  ###reference_b48###) showed that LLMs achieve correlation with human judgements on par or better than traditional distributional models. Kauf et al. (2023  ###reference_b31###) investigated event plausibility using minimal sentence pairs, in the task of binary judgements. They showed that Transformer-based models retain a considerable amount of event knowledge from textual corpora and vastly outperform the competitor models (i.e., classical distributional models and LSTM baselines). Nevertheless, both studies show LLMs’ generalization capabilities to novel experimental manipulations of the target sentences are limited and that log probabilities are affected by task-irrelevant information, such as the frequency of words within a target sentence.\nEvaluating context-dependent linguistic judgments in LLMs.    \nIn Experiment 2, we evaluate context sensitivity of LLM plausibility estimates (Table 4  ###reference_###). Initial work in this domain shows that (Dutch) LLMs can modulate their probability estimates to accommodate a previously unlikely target word (e.g., A peanut falls in love) following a short licensing context Michaelov et al. (2023  ###reference_b40###) - such scenarios, similarly, were shown to elicit a reduced N400 amplitude in humans, as a neural signature of a decrease of processing complexity of the event (Nieuwland and Van Berkum, 2006  ###reference_b46###; Rueschemeyer et al., 2015  ###reference_b50###). Nevertheless, probability-based judgements of LLMs can also be adversely influenced by context, for example in cases where the context contains information that is not related to the task (for syntax: e.g., Sinha et al., 2022  ###reference_b54###, for factual knowledge: e.g., Kassner and Schütze, 2020  ###reference_b30###).\nComparing log likelihood measurements and prompt-based methods.    The direct interaction with models through natural language prompts is exciting for many reasons, including that it facilitates knowledge exploration in a way that begins to mimic the experimental procedure used for humans (Lampinen, 2022  ###reference_b34###). Nevertheless, Hu and Levy (2023  ###reference_b25###); Hu et al. (2024  ###reference_b26###) showed that the use of metalinguistic prompts for model evaluation may underestimate their true capabilities. They compared LLMs’ syntactic/semantic knowledge across four minimal sentence pair datasets and showed that, on average, direct probability measures were a better indicator of these knowledge types than answers to prompts (they also used the DTFit dataset, but their prompts did not explicitly probe the notion of plausibility).\nEvaluating the alignment of instruction-tuned models with humans.     Even though instruction-tuning has been claimed to better align the representations of LLMs and those computed by the human brain Aw et al. (2023  ###reference_b4###), others show that it does not always help for the alignment at the behavioral level Kuribayashi et al. (2023  ###reference_b33###). However, the work in this domain is still sparse."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiment 1: Explicit vs. Implicit Event Plausibility Judgments",
            "text": "In this section, we compare explicit (prompt-based) and implicit (LL-based) plausibility judgments in base- and instruction-tuned LLMs across base and instruction-tuned models from 3 families."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "###table_1### ###figure_1### ###figure_2### ###figure_3### We use two sentence sets adapted from previous studies and compare model scores with human plausibility judgements. A schematic illustration of the items in each of the datasets can be seen in Table 1  ###reference_###.\nEventsAdapt.    The EventsAdapt dataset Fedorenko et al. (2020  ###reference_b17###) is composed of 391 items, each of which includes (i) a plausible active sentence that describes a transitive event in the past tense (The teacher bought the laptop), (ii) the implausible version of the same sentence, constructed by swapping the noun phrases (The laptop bought the teacher), as well as passive voice alternatives (The laptop was bought by the teacher and The teacher was bought by the laptop). The items fall into one of two categories: a) animate-inanimate items (AI; The teacher bought the laptop), where the swap of the noun phrases leads to impossible sentences; and b) animate-animate ones (AA; The nanny tutored the boy), where role-reversed sentences have milder plausibility violations. Given these differences, we model the two subsets independently.\nDTFit.    The DTFit dataset Vassallo et al. (2018  ###reference_b56###) contains 395 items, each of which includes (i) a plausible active sentence that describes a transitive event in the past tense, where an animate agent is interacting with an inanimate patient that is either typical for the agent (The actor won the award); (ii) or less plausible version of the same sentence, constructed by varying the inanimate patient (The actor won the battle). The different degrees of typicality depend on the interaction of the patient with both the agent and the verb (e.g. a battle may be a typical patient for a winning-event, it is just not typical given that the agent is an actor). Thus, word content and not word order is used to distinguish between plausible and implausible sentences."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Human Plausibility Judgments",
            "text": "For DTFit, participants answered questions of the form “How common is it for a {agent} to {predicate} a {patient}.” (e.g. “How common is it for an actor to win an award?” on a Likert scale from  (very atypical) to  (very typical) Vassallo et al. (2018  ###reference_b56###). For EventsAdapt, participants evaluated the extent to which each sentence was “plausible, i.e., likely to occur in the real world” on a Likert scale from  (completely implausible) to  (completely plausible) Kauf et al. (2023  ###reference_b31###). We averaged human judgments to obtain a single score for each sentence, and assigned a hit every time that the plausible version of the sentence was scored higher than the corresponding implausible one by the human participants pool.\nLog Likelihood Score\n{The nanny tutored the boy., The boy tutored the nanny.}\nSentence Choice I\nHere are two English sentences: 1) The nanny tutored the boy. 2) The boy tutored the nanny. Which sentence is more plausible? Respond with either 1 or 2 as your answer. Answer: {1, 2}\nSentence Choice II\nYou are evaluating the plausibility of sentences. A sentence is completely plausible if the situation it describes commonly occurs in the real world. A sentence is completely implausible if the situation it describes never occurs in the real world. Tell me if the following sentence is plausible. The nanny tutored the boy. Respond with either Yes or No as your answer. Answer: {Yes, No}\nLikert Scoring\nYou will be given a sentence. Your task is to read the sentence and rate how plausible it is. Here is the sentence: \"The nanny tutored the boy.\" How plausible is this sentence? Respond with a number on a scale from 1 to 7 as your answer, with 1 meaning \"is completely implausible\", and 7 meaning \"is completely plausible\". Answer:\n{ 7,\n6,\n5,\n4,\n3,\n2,\n1 }\nSentence Judgment\nHere is a sentence: The nanny tutored the boy. Is this sentence plausible? Respond with either Yes or No as your answer. Answer: {Yes, No}\n###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Model Plausibility Judgments",
            "text": "Models.    For our experiments, we used the Base and the Instruct version of three popular autoregressive LLMs: Mistral (Jiang et al., 2023  ###reference_b27###), Falcon (Almazrouei et al., 2023  ###reference_b3###), and MPT (MosaicML NLP Team, 2023  ###reference_b44###), all of them with 7B parameters. We include GPT2-XL (Radford et al., 2019  ###reference_b49###) (1.5B parameters) as a baseline model.\nMetrics.    We adopt the evaluation paradigm by Hu and Levy (2023  ###reference_b25###) and evaluate models using (i) LL scores, and (ii) several zero-shot prompting methods (Table 2  ###reference_###). The LL score is the sum of the log-probabilities of each token  in a sentence, conditioned on the preceding sentence tokens .\nOur prompts, Sentence Choice I/II, Likert Scoring and Sentence Judgment are designed to explicitly query the LLMs’ knowledge of plausibility, using either the same or similar instructions to the task that humans solved (see §3.2  ###reference_###). For all prompting methods except Likert Scoring, we compare the probabilities that models assign to ground-truth continuations (in green) over implausible continuations (in red). For Likert Scoring, we ask models to generate a number from a constrained set of answers, using the outlines python library222https://github.com/outlines-dev/outlines  ###reference_### and compare the generated scores for plausible vs. implausible sentences (the results remain consistent across free vs. constrained generation prompting, see SI §B  ###reference_###, Figure 5  ###reference_###).333Note that the DTFit dataset was included in the study by Hu and Levy (2023  ###reference_b25###) and was evaluated using different models and different prompts. Their prompts are not applicable to the EventsAdapt dataset and do not prompt models explicitly for plausibility judgments. We include an evaluation of our models on their best-performing prompt for DTFit as a supplementary analysis (SI §A  ###reference_###, Figure 4  ###reference_###). In our main experiment, all prompts are framed using the direct plausibility query “is plausible”. Supplementary analyses show that this pattern of results remains consistent for alternative queries, such as “makes sense” (SI §B  ###reference_###, Figure 6  ###reference_###) and “is likely” (SI §A  ###reference_###, Figure 4  ###reference_###).\nBinary accuracy.    For each dataset item, we compare the scores/generations of the minimally different plausible and the implausible sentence conditions, and assign a hit for every time a higher score was assigned to the plausible version, the same as for the human scores. The binary accuracy for all models is the ratio of dataset items in which plausible sentences received a higher probability score. The chance level is 50% for all benchmarks except Sentence Judgment, where, following Hu and Levy (2023  ###reference_b25###), we compare the models’ propensity to output the ground truth answer in both plausible and implausible settings, leading to a chance performance of 25%."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "Result 1: LL scores are a more reliable plausibility measure in LLMs than prompting.\nOur analysis reveals that across model architectures and plausibility datasets, LL scores are a more reliable indicator of plausibility knowledge than prompt-based approaches. This shows that there is a direct connection between plausibility and probability measures derived from the context prediction pretraining objective of LLMs (see also Hu and Levy, 2023  ###reference_b25###). Nevertheless, for the best-performing model (across all prompting metrics, except Sentence Choice II), Mistral Instruct, Sentence Choice I certain prompting setups consistently match (Figure 1  ###reference_###, panels (b), (c)) or even outperform (Figure 1  ###reference_###, panel (a)) log likelihood task performance. Despite this success, our comparison critically shows that there is not a single prompt that reliably taps into plausibility knowledge across model architectures, and none of our tested models are robust against slight variations in the way in which prompting is set up (see also Sclar et al., 2023  ###reference_b53###). In fact, many of the prompting methods lead to chance-level performance or below-chance performance for most models, even though their log probabilities evidence substantial knowledge about what events are plausible vs. implausible. This result is in line with Hu and Levy (2023  ###reference_b25###)’s finding of a competence-performance gap when probing models’ metalinguistic judgments.\nResult 2: LL scores encode substantial plausibility knowledge but fall short of human performance.\nThe LL results in Figure 1  ###reference_### show that LLMs acquire substantial event knowledge from distributional linguistic patterns; all of them performing well above chance on the task. Nevertheless, they consistently fall short of human performance and do not improve reliably over older LLMs (especially in cases where an event is comprised of two animate event participants) (Figure 1  ###reference_###, left panel): On EventsAdapt (AI, impossible), all models were successful in distinguishing plausible and implausible sentences, even though all but one model (Falcon Base) fell short of human performance (all Bonferroni-corrected  except for Falcon base: ). At the same time, none of the models significantly outperformed the GPT2-XL baseline model. On the more challenging EventsAdapt (AA, unlikely) subset, all models performed significantly worse than humans in distinguishing AA plausible from implausible events (all ), and only one model, Mistral Base, significantly improved over the smaller baseline model (; all other ). Lastly, the high task performance on DTFit (AI, unlikely), we observe that LLMs can distinguish plausible and implausible AI event descriptions even when low-level distributional cues (like selectional preference restrictions) cannot be used to distinguish the minimal pairs. Although all models still fall short of human performance for this dataset at , all but two of the tested LLMs significantly improved over the GPT2-XL baseline on this dataset (only Mistral Base and Falcon Instruct are not better, ).\nResult 3: Instruction-tuning worsens LL score alignment with human plausibility judgments.\n###figure_7### ###figure_8### ###figure_9### Next, we zoom in on the comparison of LL scores derived from Base vs. Instruct variants of the same model. Because instruction tuning constrains model behaviors to align with human-desired response characteristics (Zhang et al., 2023  ###reference_b58###; Chia et al., 2023  ###reference_b12###), it is reasonable to assume that the models’ learned probability distributions align better with human expectations of plausible sequences than the base variant, which might be more susceptible to the reporting bias in textual corpora Gordon and Van Durme (2013  ###reference_b20###).\n###figure_10### A comparative analysis of the Base and Instruct results across different model architectures reveals no beneficial effect of instruction-tuning for gauging event plausibility through LL measurements: In all but one instance do instruction-tuned models performed similar or even slightly worse than their corresponding base model (Table 3  ###reference_###). Interestingly, the gap is most noticable for the most challenging dataset, EventsAdapt (AA, unlikely). An investigation of this difference shows that certain low-level features of the input may disproportionately affect the LLs that instruction-tuned models assign to word sequences: much of the performance difference is due to the instruction-tuned models’ worse performance in discerning plausible and implausible active-voice sentences (see Figure 2  ###reference_###). This variance highlights the fact that even though direct measurements of model-derived string probabilities in many cases encode task-relevant information (e.g., modeling of grammaticality, Warstadt et al. (2020  ###reference_b57###), of N400 effects, Michaelov and Bergen (2020  ###reference_b39###), etc.), they are additionally influenced by low-level features of the input Pedinotti et al. (2021  ###reference_b48###); Kauf et al. (2023  ###reference_b31###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment 2: Context-Dependent Plausibility Judgments",
            "text": "Experiment 1 has shown that LLs are the most reliable, albeit imperfect, metric for probing the plausibility of isolated sentences in LLMs. However, most of the time, humans and LLMs to not process sentences in isolation, but rather as part of a larger context. On the other hand, language models have also been shown to be sensitive to priming effects from inter-sentential context (Misra et al., 2020  ###reference_b42###; Kassner and Schütze, 2020  ###reference_b30###). In Experiment 2, we investigate whether LLMs appropriately modulate their judgments of event plausibility when provided with different discourse contexts. Specifically, we test how and to what extent judgments of event plausibility from minimal pair accuracies change in English LLMs in the presence of (i) supporting or (ii) non-supporting but related single-sentence contexts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "We evaluate the models’ context-aware plausibility judgements on three critical metrics:\nGeneral Plausibility. This metric measures the propensity of models to assign a higher probability to plausible sentences than to minimally different implausible sentence variants when no influencing context is present (similar to §3  ###reference_###). For every dataset item, we assign a model a hit in case\nContext-Dependent Plausibility. This metric measures the ability of models to increase the probability they assign to an a priori implausible sentence in the presence of a licensing context. For every dataset item, we assign a model a hit in case\nContext Sensitivity. This metric measures the models’ ability to selectively update sentence probabilities. For every dataset item, we assign a model a hit in case\nFor each metric, we evaluate model performance the likelihood they assign (i) a critical word within the target sentence and (ii) the target sentence itself. If a critical word consists of multiple tokens, we use the sum of the log likelihood scores of the word tokens. Whereas critical/target word likelihoods measures the ability of models to detect a contextually unexpected linguistic event, target sentence likelihood measures investigate whether implausibility is reliably reflected in the probability the models assign to tokens after encountering a semantically anomalous item. This is because the token likelihood of plausible and implausible sentences are shared up until the first occurrence of a contextually unlicensed word."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "Result 1: Target word LLs are better modulated by context than target sentence LLs.\n###figure_11### When comparing target word vs. target sentence LLs, a clear trend emerges: all models perform extremely well (around %) across all metrics when comparing the probabilities of target words (Table 5  ###reference_###, Word columns); at the same time, when using the likelihoods they assign to sentences as an indicator of event plausibility knowledge, performance degrades for two of the three metrics (Table 5  ###reference_###, Sent. columns). In particular, even though almost all LLMs are able to distinguish plausible and implausible sentences (General Plausibility  ###reference_###, similar to §3  ###reference_###); and are able to modulate the probability they assign an unexpected sentence in the presence of licensing context, they fail to update the sentence probabilities selectively (this is evidenced by the substantial drop in performance for the Context Sensitivity  ###reference_### metric across LLMs (although they perform significantly better than the baseline GPT2-XL model). This pattern suggests that while a semantically licensing context assists the models in up-weighing the probability of an otherwise implausible target word/event description (see Context-Dependent Plausibility  ###reference_###; in line with Michaelov et al., 2023  ###reference_b40###), contextual implausibility is not reliably reflected in LLMs’ sentence likelihoods. In particular, once an unexpected target word has been encountered (which the LLMs are able to discern, see Context Sensitivity  ###reference_###, Word columns), the LLMs appear to quickly adjust the predictions in the post-target region, in some cases assigning even higher probabilities to post-target words than in the Critical condition, with the consequence that the scores for anomalous sentences and contextually-licensed ones differ less significantly at the sentence level.\nThis suggests that a semantically-licensing context helps a model in predicting an otherwise anomalous word, but the global probability of the target sentence benefits less from a the specific context. After meeting an unexpected target word, LLMs seem to be quickly able to adjust the predictions for the following ones, with the consequence that the scores for anomalous sentences and contextually-licensed ones differ less significantly at the sentence level.\nResult 2: Context-modulated LLs align with human contextual judgment patterns.\nFinally, we investigate how contextual plausibility judgments correspond to human behavior for the same stimuli. We focus on the sensibility-judgment task, in which participants were asked to decide if a target sentence made sense (i) to them within the provided context, or (ii) to another person who did not have access to the context sentence Jouravlev et al. (2019  ###reference_b28###). Here, we model this dataset in a ‘single-participant setting’, by exposing the LLMs to the full items and comparing the log probabilities assigned to the target words in the three experimental conditions, with or without licensing context. Across models, we see a remarkable match between human- and model-derived plausibility scores, both in the isolated sentence and the contextualized setup. For completeness, we report results for the exact replication of the human study in LLMs, using Sentence Judgment prompts in SI §D  ###reference_###. We note that, again, LLs provide a better fit to human data, even though the prompting results for Instruct models matched the human behavioral patterns qualitatively (see also SI §C  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Overall, we show through careful investigation that LL scores, reflecting co-occurrence patterns distilled by LLMs from the task of next-word prediction at scale during pre-training, remain a more reliable measure of sentence plausibility than both (i) direct prompting and (ii) log likelihood scores from models finetuned to follow instructions. This is true in scenarios that encompass both isolated and context-dependent sentence plausibility estimates. Even though instruction-tuning has been claimed to align LLMs and human brain representations (Aw et al., 2023  ###reference_b4###), other studies show that it does not always help for the alignment at the behavioral level (Kuribayashi et al., 2023  ###reference_b33###). The results presented in our work are consistent with the latter finding.\nConcerning LLMs’ sensitivity to sentence context, we observe that by using LL scores at the level of the target word, all the models perform around 90% with respect to the ground truth and are well aligned to human judgement patterns. On the other hand, when using sentence-level LL scores, we notice that the models have the tendency to \"re-balance\" the log likelihoods after processing an unexpected word, with the consequence that semantically anomalous sentences and contextually-licensed ones become harder to distinguish.\nAlthough it is possible that model- and task-specific prompts will outperform raw LL scores as a way to estimate sentence plausibility, our work highlights that LL scores are an easy, zero-shot way to assess LLMs’ implicit knowledge. Thus, getting a raw LL estimate of model performance can provide an initial estimate of whether or not custom prompt-based solutions can be successful or—in some cases—obviate the need for prompt tuning altogether."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "CK and this work were partially supported by the MIT Quest for Intelligence. EC was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 15612222). This research was also partly funded by PNRR—M4C2—Investimento 1.3, Partenariato Esteso PE00000013—“FAIR—Future Artificial Intelligence Research”—Spoke 1 “Human-centered AI,” funded by the European Commission under the NextGeneration EU programme."
        }
    ]
}