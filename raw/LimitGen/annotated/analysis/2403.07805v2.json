{
    "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
    "abstract": "Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.\nHowever, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive.\nIn this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly.\nThrough carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs.\nFurthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language models (LMs) have recently showcased outstanding abilities in NLP tasks with a large amount of memory stored in their parameters (Brown et al., 2020  ###reference_b7###; Ouyang et al., 2022  ###reference_b27###). Through pre-training on large text corpora, LMs memorize factual knowledge about the world (Zhou et al., 2023  ###reference_b54###). Consequently, they show great performance in knowledge-intensive tasks (Petroni et al., 2021  ###reference_b31###) such as open-domain question answering (Kamalloo et al., 2023  ###reference_b15###; Ziems et al., 2023  ###reference_b58###; Mallen et al., 2023  ###reference_b23###). There is a growing interest in considering LMs as knowledge bases (Wang et al., 2021  ###reference_b47###; Heinzerling and Inui, 2021  ###reference_b11###; Petroni et al., 2019  ###reference_b32###; Cao et al., 2021  ###reference_b8###; AlKhamissi et al., 2022  ###reference_b1###). Despite the recent advances in applying LMs to solve downstream tasks, the fundamentals of how LMs store knowledge and access memory in their parameters remains a subject of ongoing research and intrigue (Tirumala et al., 2022  ###reference_b43###; Zhu and Li, 2023  ###reference_b56###; Allen-Zhu and Li, 2023  ###reference_b2###; Berglund et al., 2023  ###reference_b4###).\n###figure_1### In this paper, we draw inspiration from memory-accessing patterns observed in computer systems to explore whether LMs can access their parametric memory in a sequential or random manner.\nWe extrapolate these concepts to investigate LMs and delineate two memory access patterns: sequential memory access means that the model starts from the beginning of a memorized sequence, progressing through the content in consecutive order. Conversely, random memory access denotes that the model can commence from any location within the memorized content, without needing to start from the beginning. For instance, reciting a memorized poem line by line is considered sequential access, while directly starting from the third line involves random access.\nWith these concepts, we design experiments with both synthetic and real data to evaluate the language model’s ability to perform sequential or random access to memorized content, as illustrated in Figure 1  ###reference_###.\nWe limit our study to\ndecoder-only language models due to their increasing popularity and capability (Radford et al., 2019  ###reference_b34###; Brown et al., 2020  ###reference_b7###; Touvron et al., 2023a  ###reference_b44###, b  ###reference_b45###; Jiang et al., 2023  ###reference_b13###). We first ask the model to memorize key–value pairs of various types and show that the model is able to sequentially read memorized content to a satisfying degree. Next, we test the model’s random access ability by training it to recite a sentence or find an answer to a question in a memorized passage. In such tasks, the model’s performance falls drastically when it is required to extract a span in the middle of a passage, revealing its incapability to randomly access its memory.\nGiven that language models struggle to perform random access to their memory, we pursue two means for mitigation: recitation at inference time, and permutation during training. Recitation enables the model to sequentially read its parametric memory first before performing a task. The model’s performance can thus be enhanced by utilizing the recited content in its context window. We also show that simply permuting sentences in a passage during training to memorize content also improves performance.\nWe finally verify the challenge of random access through a case study on open-domain question answering. We reduce the difficulty of the task by allowing the model to memorize passages with ground-truth answers, yet we find that the model benefits the most from such memorization when it is allowed to recite a relevant passage and then answer the question. Overall, we make several contributions to further understand the memory access mechanisms of decoder-only language models:\nWe show that language models can access their memory sequentially and can reproduce memorized content. In contrast, they encounter significant challenges in random memory access.\nWe find solutions to mitigate the challenge of random access by permuting memorized content or explicitly reciting the memory before performing tasks.\nWe demonstrate the effect of poor random memory access ability in open-domain question answering, showing that the challenge could have broader implications on the applications of language models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Large language models store a considerable amount of knowledge in their parameters (Petroni et al., 2019  ###reference_b32###; Heinzerling and Inui, 2021  ###reference_b11###). They memorize useful knowledge such as facts and commonsense (Zhao et al., 2023  ###reference_b53###), but also sensitive personal information such as emails or phone numbers (Carlini et al., 2020  ###reference_b9###; Huang et al., 2022  ###reference_b12###). Existing approaches to understanding memorization include fine-grained analysis to locate the neuron that is associated with the knowledge (Meng et al., 2022  ###reference_b25###; Liu et al., 2024  ###reference_b22###) or macro analysis to understand the overall dynamics of memorization (Tirumala et al., 2022  ###reference_b43###; Speicher et al., 2024  ###reference_b39###). In this study, we do not aim to analyze the mechanisms of writing to language model’s memory. Instead, we consider the language model as a black-box memory store and focus mainly on how the model accesses its memory.\nOur investigation requires writing new content to the model’s parametric memory. There are mainly two ways to perform such knowledge injection without changing the model architecture (Ovadia et al., 2024  ###reference_b28###; Balaguer et al., 2024  ###reference_b3###): fine-tuning or retrieval augmentation. Retrieval augmentation (Lewis et al., 2020  ###reference_b20###; Shi et al., 2023  ###reference_b38###) retrieves relevant information and puts it into the model’s context while fine-tuning directly updates the model parameters. As the goal of our study is to investigate how the model accesses its parametric memory after writing to the memory, we choose finetuning as the method for introducing new knowledge to the model.\nPrevious works have shown that using prompts can effectively retrieve knowledge stored in large language models (Bouraoui et al., 2019  ###reference_b6###; Jiang et al., 2021  ###reference_b14###; Wang et al., 2021  ###reference_b47###). We follow earlier work to use prompts to query the model to access and regenerate memorized content. However, a notable difference is that prior work focuses on finding optimised methods to elicit the model’s knowledge obtained during pretraining (Youssef et al., 2023  ###reference_b51###; Liu et al., 2023  ###reference_b21###), while we directly use unique keys for memorizing and retrieving content.\nWe consider the language model as a memory store for passages, which is related to the recent advances in adopting a language model as an index for document storage and retrieval (Metzler et al., 2021  ###reference_b26###; Tay et al., 2022  ###reference_b42###; Wang et al., 2023  ###reference_b48###; Zeng et al., 2023  ###reference_b52###). In such indexes, each document is associated with a document identifier (ID), which could be keywords (Ren et al., 2023  ###reference_b36###; Bevilacqua et al., 2022  ###reference_b5###; Lee et al., 2023b  ###reference_b19###, a  ###reference_b18###) or numbers (Tay et al., 2022  ###reference_b42###; Wang et al., 2023  ###reference_b48###; Zhuang et al., 2022  ###reference_b57###; Zhou et al., 2022  ###reference_b55###). We also follow the practice and assign an ID to each document for storing and retrieving the documents. However, we do not ask the model to retrieve a relevant ID to a question. Instead, we provide the ID in the input, and investigate the possibility of sequentially or randomly accessing the corresponding document content."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Investigating Sequential and Random Memory Access",
            "text": "In this section, we investigate the ability of a language model to sequentially or randomly access its memory stored in the parameters. First, we provide formulations of language models serving as a memory bank of passages (§3.1  ###reference_###). Within this framework, we define sequential memory access as the process of starting from the beginning of a memorized passage and progressively generating subsequent content. In contrast, we conceptualize random memory access as the model’s ability to initiate recall from any chosen location in a memorized passage and accurately regenerate the subsequent content. Based on these definitions, we first investigate the model’s sequential memory access ability by requiring it to recite full passages word by word (§3.2  ###reference_###). Next, we test the random memory access ability of the model by asking it to recite selected sentences from memorized passages (§3.3  ###reference_###). We further assess the model’s random access proficiency through a more challenging task involving question answering (§3.4  ###reference_###).\nTo investigate whether the model can handle identifiers and passage content of different types, we set  and  and consider the following variations. For the type of passage content , we examine two categories: (1) natural language (NL), comprising Wikipedia paragraphs from SQuAD (Rajpurkar et al., 2016  ###reference_b35###), and (2) random strings (Rand), where each NL passage is substituted with a space-separated alphanumeric string maintaining the same number of tokens. Regarding the type of  (i.e., passage IDs), we explore three forms: (1) numerical strings (Num), such as ‘#123’; (2) rare random tokens (Rare), adopting the approach of Ruiz et al. (2022  ###reference_b37###) by random sampling three infrequent tokens; (3) article title (Title) of the Wikipedia page to which the passage belongs.\nWe adopt the GPT2-large model (Radford et al., 2019  ###reference_b34###) with 774M parameters as the base model. For better string memorization ability (Stevens and Su, 2023  ###reference_b40###), we use a pretrained checkpoint222https://huggingface.co/gpt2  ###reference_huggingface.co/gpt2### instead of training the model from scratch. We fine-tune the model for 100 epochs to ensure that the model fully converges, with a learning rate of . We measure memorization using both the BLEU score (Papineni et al., 2002  ###reference_b30###) and the Exact Match (EM) score, indicating the similarity between the generated content and the ground-truth passage.\nTable 1  ###reference_### shows that the model is able to sequentially access memorized content, with high BLEU and EM on validation passages. The model’s sequential access capability is further demonstrated by its adaptability to varying types of IDs and passages. Specifically, using titles or numbers as keys for natural language passages achieves higher performance than using rare tokens. We suspect that models might have difficulty associating rare tokens with the natural language content. Remarkably, the model’s access ability extends to passages composed of random characters ().\n###figure_2### To further test the memory capacity of the model, we carry out an additional experiment where we set the passage type to  and identifier type to  and construct passages each with 25 random tokens. As illustrated in Figure 2  ###reference_###, we fix  as 1k and increase  gradually from 1k to 500k to examine the ability of sequential memory access.\nWe observe that even with a training passage count of 50k, the model could accurately reproduce over 70% of memorized validation passages.\nHowever, there is also a bottleneck in parametric memory: the performance drops to nearly zero when the passage count exceeds 100k.\nWe attribute this bottleneck to the difficulty in training, as the model fails to converge on memorizing all the passages.\nTherefore, in subsequent experiments, we carefully manage the corpus size to ensure that the model memorizes all passages.\nWe follow Mallick et al. (2023  ###reference_b24###) to place markers at the boundaries of each sentence, obtained by the NLTK sentence splitter333https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html  ###reference_t_tokenize.html###: a passage is formatted as “[0] sent0 [0] [1] sent1 [1], …,”. In this case, the model only needs to learn to copy the content between these markers. Our selective recitation task requires the model to recite the -th sentence of passage  based on the given passage ID . The reading function is now , such as “What is sentence [1] of Document #2033?”. For reference, we also test the model’s performance in a baseline where the passage content is provided in the context window.\nAs we are testing for exact memorization, we use BLEU and EM scores to evaluate the model. Similar to §3.2  ###reference_.SSS0.Px1###, we use  training and  validation passages, with 1994 sentences and 200 sentences respectively. We set the type of ID to be  and only include passages with more than 3 sentences. All other hyperparameters stay the same as §3.2  ###reference_.SSS0.Px1###.\n###figure_3### We find that providing the passage ID does not enable the model to selectively recite the requested sentences. It scores poorly with a low EM of 34.5 and a 47.1 BLEU score, in contrast to the much higher 97.0 EM and 97.3 BLEU when the passage content is included in the context. A detailed analysis in Figure 3  ###reference_### reveals that the correct predictions are largely reciting the first sentence (). This verifies that the model can sequentially access the content to reproduce the first sentence. However, as the marker index increases, the model is required to skip preceding sentences and directly access a sentence in the middle of a passage. The model’s performance sharply declines, indicating its inability to randomly access middle or later sentences in memorized passages.\nWe experiment with the well-known SQuAD-v1 (Rajpurkar et al., 2016  ###reference_b35###) dataset because many of its questions are closely dependent on the passage, such as “How did the war start?”. This design compels the model to depend on the memorized IDs and passages rather than pre-existing knowledge. We explore the grounded QA task with variants of providing (1) the ID of the golden passage with the answer, (2) a random non-golden ID and (3) no ID. For comparison, we also consider the setups that do not involve writing passages to the model’s parametric memory. These include (1) closed-book QA, where the model is fine-tuned solely on QA pairs, serving as a baseline to assess the model’s reliance on prior knowledge for answering questions, and (2) open-book QA, where the golden passage content is concatenated with the question, setting the upper limit of extractive QA performance.\nWe experiment with different types of passage IDs. To ensure the uniqueness of using titles as passage IDs, we select  passages and  passages from the full SQuAD dataset, with over 2,000 and 300 questions respectively. The model is evaluated on F1 and EM following the original SQuAD evaluation script. The other hyperparameters are the same as mentioned in §3.2  ###reference_.SSS0.Px1###.\nThe results are presented in Table 2  ###reference_### (the settings with “+Recitation” are discussed in later sections).\nAs expected, the model performs the best in the open-book setting, as it only needs to locate the answer in the golden passage. In contrast, the closed-book QA setup\nyields the worst performance, as the model has no access to passages and relies solely on its parametric knowledge stored during pretraining.\nInterestingly, the form of the provided passage ID has minimal impact on performance. We observe similar performance regardless of whether the golden ID is provided, except when the type of ID is Title. In this case, providing a random incorrect ID harms performance. We suspect that this is because the title is usually an entity related to the passage topic, therefore offering useful clues. In cases where the ID does not carry semantic meaning (i.e., Rare and Num), the correctness or presence of the ID does not significantly affect the performance, which remains substantially below the open-book setting, despite the model memorizing all passages. This further validates the model’s inability to effectively access random memory, as it struggles to extract the answer even when provided with a correct passage ID.\nIn summary, our findings validate the hypothesis that LMs can effectively function as a memory bank, enabling sequential access to its memory.\nHowever, there are significant limitations in the model’s ability to randomly access its memory. Across both the simple selective recitation and the complex grounded question-answering tasks, the model consistently fails to accomplish the tasks by leveraging its memory, despite being explicitly provided with the corresponding passage IDs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Formulation",
            "text": "We abstract the language model as a memory bank and investigate its sequential or random access ability. We adopt a simple definition of a memory bank as a key–value store , where  represents a unique identifier (ID) assigned to the content of the -th passage111We use the term “document” and “passage” interchangeably in this paper, referring to a chunk of text..\nThere are two core functions that a memory bank needs to support: reading and writing.\nGiven that our memory bank is embodied as a language model, it is not straightforward to write and read the model’s memory.\nFollowing previous work (Zhu and Li, 2023  ###reference_b56###; Wang et al., 2021  ###reference_b47###), for writing to the memory bank, we use fine-tuning to update the model’s parameters.\nFor reading, we use prompting to elicit the model’s memory.\nSpecifically, for each passage  with its corresponding identifier , we create two types of data instances: writing,  and reading, , where  and  denote the prompts detailed in Appendix A.1  ###reference_###.\nAs the primary goal of our study is to test whether the model can read (access) its stored content sequentially or randomly, we mainly vary the reading function across different experiments.\nGiven a corpus consisting of  passages, we split the corpus into two subsets:  training passages and  validation passages. We adopt a mixed training strategy as described by Zhu and Li (2023  ###reference_b56###): During the training stage, we include  and  instances of T training passages, as well as  instances of V validation passages. Our objective is for the model to learn to associate each identifier with its passage content by training on the reading and writing instances of the training passages. During evaluation, we prompt the model with the  instances of the V validation passages to test the model’s memory access pattern."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Sequential Access: Full Recitation",
            "text": "We test the sequential access ability of the language model by asking it to reproduce the full passage content. Specifically, given an ID, the model is prompted to start from the beginning of the corresponding memorized passage and generate tokens consecutively. We evaluate the model’s performance to reproduce the content on the  validation passages, which requires the model to both memorize the passage content and sequentially access the memory with the provided key.\nTo investigate whether the model can handle identifiers and passage content of different types, we set  and  and consider the following variations. For the type of passage content , we examine two categories: (1) natural language (NL), comprising Wikipedia paragraphs from SQuAD (Rajpurkar et al., 2016  ###reference_b35###  ###reference_b35###), and (2) random strings (Rand), where each NL passage is substituted with a space-separated alphanumeric string maintaining the same number of tokens. Regarding the type of  (i.e., passage IDs), we explore three forms: (1) numerical strings (Num), such as ‘#123’; (2) rare random tokens (Rare), adopting the approach of Ruiz et al. (2022  ###reference_b37###  ###reference_b37###) by random sampling three infrequent tokens; (3) article title (Title) of the Wikipedia page to which the passage belongs.\nWe adopt the GPT2-large model (Radford et al., 2019  ###reference_b34###  ###reference_b34###) with 774M parameters as the base model. For better string memorization ability (Stevens and Su, 2023  ###reference_b40###  ###reference_b40###), we use a pretrained checkpoint222https://huggingface.co/gpt2  ###reference_huggingface.co/gpt2###  ###reference_huggingface.co/gpt2### instead of training the model from scratch. We fine-tune the model for 100 epochs to ensure that the model fully converges, with a learning rate of . We measure memorization using both the BLEU score (Papineni et al., 2002  ###reference_b30###  ###reference_b30###) and the Exact Match (EM) score, indicating the similarity between the generated content and the ground-truth passage.\nTable 1  ###reference_###  ###reference_### shows that the model is able to sequentially access memorized content, with high BLEU and EM on validation passages. The model’s sequential access capability is further demonstrated by its adaptability to varying types of IDs and passages. Specifically, using titles or numbers as keys for natural language passages achieves higher performance than using rare tokens. We suspect that models might have difficulty associating rare tokens with the natural language content. Remarkably, the model’s access ability extends to passages composed of random characters ().\n###figure_4### To further test the memory capacity of the model, we carry out an additional experiment where we set the passage type to  and identifier type to  and construct passages each with 25 random tokens. As illustrated in Figure 2  ###reference_###  ###reference_###, we fix  as 1k and increase  gradually from 1k to 500k to examine the ability of sequential memory access.\nWe observe that even with a training passage count of 50k, the model could accurately reproduce over 70% of memorized validation passages.\nHowever, there is also a bottleneck in parametric memory: the performance drops to nearly zero when the passage count exceeds 100k.\nWe attribute this bottleneck to the difficulty in training, as the model fails to converge on memorizing all the passages.\nTherefore, in subsequent experiments, we carefully manage the corpus size to ensure that the model memorizes all passages."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Random Access: Selective Recitation",
            "text": "Selective recitation is a straightforward synthetic task: asking the language model to reproduce a specific sentence of a memorized passage. This task is designed for its simplicity, as it does not require the model’s understanding of passage content. The focus is solely on the model’s capacity to access segments in a memorized passage. Successful random access would be indicated by the model’s ability to reproduce any sentence from within memorized passages, regardless of position.\nWe follow Mallick et al. (2023  ###reference_b24###  ###reference_b24###) to place markers at the boundaries of each sentence, obtained by the NLTK sentence splitter333https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html  ###reference_t_tokenize.html###  ###reference_t_tokenize.html###: a passage is formatted as “[0] sent0 [0] [1] sent1 [1], …,”. In this case, the model only needs to learn to copy the content between these markers. Our selective recitation task requires the model to recite the -th sentence of passage  based on the given passage ID . The reading function is now , such as “What is sentence [1] of Document #2033?”. For reference, we also test the model’s performance in a baseline where the passage content is provided in the context window.\nAs we are testing for exact memorization, we use BLEU and EM scores to evaluate the model. Similar to §3.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###, we use  training and  validation passages, with 1994 sentences and 200 sentences respectively. We set the type of ID to be  and only include passages with more than 3 sentences. All other hyperparameters stay the same as §3.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\n###figure_5### We find that providing the passage ID does not enable the model to selectively recite the requested sentences. It scores poorly with a low EM of 34.5 and a 47.1 BLEU score, in contrast to the much higher 97.0 EM and 97.3 BLEU when the passage content is included in the context. A detailed analysis in Figure 3  ###reference_###  ###reference_### reveals that the correct predictions are largely reciting the first sentence (). This verifies that the model can sequentially access the content to reproduce the first sentence. However, as the marker index increases, the model is required to skip preceding sentences and directly access a sentence in the middle of a passage. The model’s performance sharply declines, indicating its inability to randomly access middle or later sentences in memorized passages."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Random Access: Grounded Question Answering",
            "text": "Building on our earlier finding §3.2  ###reference_### that the model can memorize many passages each linked to a unique ID, we embark on a more pragmatic task: question answering grounded in a specific passage ID.\nThis task aims to evaluate whether the model can provide answers to questions by extracting a span from its memory.\nFor instance, a question might be framed as “According to document #3022, in what year did Chopin become a French citizen?” and the answer is “1835” in the passage with ID #3022.\nWe hypothesize that if LMs are capable of random memory access, they should navigate to the corresponding passage using the provided ID and extract the relevant span to answer the questions.\nWe experiment with the well-known SQuAD-v1 (Rajpurkar et al., 2016  ###reference_b35###  ###reference_b35###) dataset because many of its questions are closely dependent on the passage, such as “How did the war start?”. This design compels the model to depend on the memorized IDs and passages rather than pre-existing knowledge. We explore the grounded QA task with variants of providing (1) the ID of the golden passage with the answer, (2) a random non-golden ID and (3) no ID. For comparison, we also consider the setups that do not involve writing passages to the model’s parametric memory. These include (1) closed-book QA, where the model is fine-tuned solely on QA pairs, serving as a baseline to assess the model’s reliance on prior knowledge for answering questions, and (2) open-book QA, where the golden passage content is concatenated with the question, setting the upper limit of extractive QA performance.\nWe experiment with different types of passage IDs. To ensure the uniqueness of using titles as passage IDs, we select  passages and  passages from the full SQuAD dataset, with over 2,000 and 300 questions respectively. The model is evaluated on F1 and EM following the original SQuAD evaluation script. The other hyperparameters are the same as mentioned in §3.2  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\nThe results are presented in Table 2  ###reference_###  ###reference_### (the settings with “+Recitation” are discussed in later sections).\nAs expected, the model performs the best in the open-book setting, as it only needs to locate the answer in the golden passage. In contrast, the closed-book QA setup\nyields the worst performance, as the model has no access to passages and relies solely on its parametric knowledge stored during pretraining.\nInterestingly, the form of the provided passage ID has minimal impact on performance. We observe similar performance regardless of whether the golden ID is provided, except when the type of ID is Title. In this case, providing a random incorrect ID harms performance. We suspect that this is because the title is usually an entity related to the passage topic, therefore offering useful clues. In cases where the ID does not carry semantic meaning (i.e., Rare and Num), the correctness or presence of the ID does not significantly affect the performance, which remains substantially below the open-book setting, despite the model memorizing all passages. This further validates the model’s inability to effectively access random memory, as it struggles to extract the answer even when provided with a correct passage ID.\nIn summary, our findings validate the hypothesis that LMs can effectively function as a memory bank, enabling sequential access to its memory.\nHowever, there are significant limitations in the model’s ability to randomly access its memory. Across both the simple selective recitation and the complex grounded question-answering tasks, the model consistently fails to accomplish the tasks by leveraging its memory, despite being explicitly provided with the corresponding passage IDs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Mitigating Random Access Challenge",
            "text": "The earlier experiments show that in general, language models perform well in sequentially accessing their parametric memory, but encounter challenges in random memory access. This naturally raises the question: How can we mitigate the shortcomings in random memory access?\nWe extend the earlier experiments\nby integrating recitation and permutation into the respective reading and writing stages.\nFirst, we add a setup to the selective sentence recitation task: Based on the given ID, the model is tasked to first recite the entire content of the corresponding passage and then the specific sentence, altering the reading operation to . Similarly, for the grounded QA task, we ask the model to recite the passage associated with the input passage ID before answering the question. In the setup without an ID, the model is still trained to recite the golden passage.\nTo explore the effect of permutation during the writing stage, we perform permutation among sentences in a passage to create diverse  instances. For a -sentence passage we tested:\n(1) first, moving each sentence to the passage’s beginning to create  unique instances;\n(2) random, randomly shuffling the sentences  times to create  instances, where  is set to 4 by default;\nReciting the passage content effectively boosts the performance of selective recitation, as evidenced in Table 3  ###reference_###. With recitation, the model first sequentially accesses the content from its memory using the provided passage ID and subsequently loads this passage in the context to allow for random access. Conditioned on the recited content in the context, the model can therefore easily identify the correct sentence.\nSimilarly, explicitly reciting the golden passages markedly enhances question-answering performance, as shown in Table 2  ###reference_###. This observation is consistent across all three types of passage IDs. Conversely,\nintentionally prompting the model to recite a random passage leads to a decline in performance. This is likely because random passages introduce irrelevant information and confuse the model. Surprisingly, the recitation of relevant passages benefits performance even without an ID, although the improvement is smaller than with the golden ID. This verifies the effectiveness of recitation in more general settings of question answering.\nAnother way of enhancing random access is to perform permutation of sentences, as presented in Table 3  ###reference_###. Simply bringing every sentence to the start of the passage once (first) or randomly permuting the sentences many times (random) helps to solve the challenge of accessing the middle content of a passage. We also find that permutation enhances grounded QA performance (Appendix C  ###reference_###) from 26.7 to 31.3 in terms of EM. However, it is noteworthy that permutation does not alter the inherent sequential access pattern of parametric memory. Rather, by permuting the sentences and disrupting their original order, we allow more sentences to be sequentially accessible via the ID."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Proposed Method",
            "text": "To address the challenge, we start from the two operations supported by LMs as a memory store: reading and writing. During the writing phase, we hypothesize that performing permutation on the passage content can naturally enhance the model’s random access ability: any part of the content can be the starting point of a memorized sequence. In this setup, we change the sequential order of passage content to achieve random access.\n###figure_6### On the other hand, during the reading phase, leveraging the model’s context window presents a viable strategy. The attention mechanism (Vaswani et al., 2017  ###reference_b46###) enables the model to access any token within the context window, thereby inherently supporting random access (Packer et al., 2023  ###reference_b29###; Ge et al., 2023  ###reference_b10###). For tasks with a given ID, we could ask the model to sequentially recite the passage first, place it within the context, and subsequently query the model to perform span extraction tasks utilizing this context, as illustrated in Figure 4  ###reference_###.\nOur subsequent experiments are designed to evaluate the effectiveness of these two methods. Through empirical evaluation, we validate that content permutation during writing or recitation during reading can largely mitigate the challenge of random memory access and enhance performance.\nWe extend the earlier experiments\nby integrating recitation and permutation into the respective reading and writing stages.\nFirst, we add a setup to the selective sentence recitation task: Based on the given ID, the model is tasked to first recite the entire content of the corresponding passage and then the specific sentence, altering the reading operation to . Similarly, for the grounded QA task, we ask the model to recite the passage associated with the input passage ID before answering the question. In the setup without an ID, the model is still trained to recite the golden passage.\nTo explore the effect of permutation during the writing stage, we perform permutation among sentences in a passage to create diverse  instances. For a -sentence passage we tested:\n(1) first, moving each sentence to the passage’s beginning to create  unique instances;\n(2) random, randomly shuffling the sentences  times to create  instances, where  is set to 4 by default;\nReciting the passage content effectively boosts the performance of selective recitation, as evidenced in Table 3  ###reference_###  ###reference_###. With recitation, the model first sequentially accesses the content from its memory using the provided passage ID and subsequently loads this passage in the context to allow for random access. Conditioned on the recited content in the context, the model can therefore easily identify the correct sentence.\nSimilarly, explicitly reciting the golden passages markedly enhances question-answering performance, as shown in Table 2  ###reference_###  ###reference_###. This observation is consistent across all three types of passage IDs. Conversely,\nintentionally prompting the model to recite a random passage leads to a decline in performance. This is likely because random passages introduce irrelevant information and confuse the model. Surprisingly, the recitation of relevant passages benefits performance even without an ID, although the improvement is smaller than with the golden ID. This verifies the effectiveness of recitation in more general settings of question answering.\nAnother way of enhancing random access is to perform permutation of sentences, as presented in Table 3  ###reference_###  ###reference_###. Simply bringing every sentence to the start of the passage once (first) or randomly permuting the sentences many times (random) helps to solve the challenge of accessing the middle content of a passage. We also find that permutation enhances grounded QA performance (Appendix C  ###reference_###  ###reference_###) from 26.7 to 31.3 in terms of EM. However, it is noteworthy that permutation does not alter the inherent sequential access pattern of parametric memory. Rather, by permuting the sentences and disrupting their original order, we allow more sentences to be sequentially accessible via the ID."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We empirically study how language models access their parametric memory. Our experiments on both synthetic and realistic data demonstrate that while language models can adequately reproduce memorized content in a sequential manner, they struggle with the random access of segments in the middle of memorized content. We identify two effective strategies of recitation and permutation to mitigate the limitation of random memory access. Furthermore, through a controlled case study on open-domain question answering, we illustrate that allowing the model to recite and randomly access its memory significantly improves performance. Overall, our study not only provides a deeper understanding of memory access patterns in language models, but also highlights the implications of limited random memory access ability in practical application of language models."
        }
    ]
}