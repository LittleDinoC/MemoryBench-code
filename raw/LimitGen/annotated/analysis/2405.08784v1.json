{
    "title": "Refinement of an Epilepsy Dictionary through Human Annotation of Health-related posts on Instagram",
    "abstract": "Objective —\nTo (1) identify health-related terms used on social media posts that do not precisely match the health-related meaning of terms in a biomedical dictionary, (2) decide which terms need to be removed in order to improve the quality of the dictionary in the scope of biomedical text mining tasks, (3) evaluate the effect of removing imprecise terms on such tasks, and (4) discuss how human-centered annotation complements automated annotation in social media mining for biomedical purposes.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social media data, such as text, hashtags, or images in posts, allow researchers to gain unprecedented access to study human cohorts. It is now possible to quantitatively measure very large populations for their individual or collective experiences, behaviors, perceptions, and emotions [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###].\nSocial media is more than a source of information or a valuable communication tool for its users [7  ###reference_b7###]—it is also a valuable resource of information about human health and well-being [1  ###reference_b1###].\nIt has been used to track and predict various public health issues [8  ###reference_b8###], such as mental health disorders [9  ###reference_b9###, 10  ###reference_b10###], adverse drug reaction (ADR) [11  ###reference_b11###, 12  ###reference_b12###], drug-drug interactions (DDI) [13  ###reference_b13###, 14  ###reference_b14###, 15  ###reference_b15###], and substance abuse [16  ###reference_b16###, 17  ###reference_b17###].\nInstagram is one of the most popular social network services in the world; in 2021, Instagram reached more than 1 billion users worldwide [18  ###reference_b18###].\nThrough an application for smartphones or tablets, users can share photos and videos, often accompanied by long captions.\nAlthough most research on social media has been focused on data from Twitter (now named X) or Facebook , Instagram has great potential for social media research given its increasing number of users.\nIt has already shown its potential for large-scale social media analysis and monitoring of public health issues, such as DDI and ADR, uncovering behavioral pathology and associations between drugs and symptoms in depression [15  ###reference_b15###].\nIt can even be used as a tool for communication and support between patients and healthcare providers [19  ###reference_b19###], and other health-related applications [1  ###reference_b1###].\nFor these reasons, the work described here is focused on data from Instagram, but our methodology and results are applicable to other social media from Twitter to Reddit [20  ###reference_b20###].\nDespite the benefits of social media analysis for public health, social media research has not focused much on people with epilepsy (PWE), a chronic noncommunicable brain disorder and one of the most common neurological diseases [21  ###reference_b21###].\nIn the U.S., more than three million adults have epilepsy, and about 470,000 children were diagnosed with active epilepsy in 2015 [21  ###reference_b21###, 22  ###reference_b22###, 23  ###reference_b23###].\nEpilepsy-relevant Facebook pages and Twitter accounts play an essential role in providing information about drugs or correcting misconceptions or epilepsy stigma on online platforms [24  ###reference_b24###, 25  ###reference_b25###].\nFurthermore, our team has shown that even small cohorts of epilepsy patients on Facebook can inform experts about relevant behaviors involved in rare outcomes, such as sudden death in epilepsy [6  ###reference_b6###].\nTherefore, more research on PWE and their caregivers’ online behaviors on social media, from epilepsy-specific online groups to general-purpose platforms, is needed—especially to better understand their complex symptoms and medication schedules, including DDI and ADR.\nTo support such a research agenda, it is essential to develop automated annotation pipelines to mine and detect biomedical signals from large-scale social media data in general [1  ###reference_b1###, 26  ###reference_b26###].\nAt the core of such pipelines is the construction of biomedical dictionaries to tag relevant terminology in social media posts.\nTypically these are produced from databases and named entity recognition tools that were developed for scientific discourse, such as papers with experimental evidence available on PubMed [15  ###reference_b15###, 1  ###reference_b1###, 27  ###reference_b27###, 28  ###reference_b28###].\nHowever, it is unclear whether biomedical dictionaries built from scientific discourse and evidence are fit for the informal discourse and particularities of discussions on Instagram and other social media that are relevant for epilepsy (or other conditions of medical interest).\nTo address that question, here we present a human-centered dictionary refinement methodology and analysis tailored to tag clinically relevant terminology for the study of epilepsy cohorts on Instagram .\nFor comparison, OpenAI’s GPT series models, as representatives of Large Language Model (LLMs), were also used as an alternative annotation process, though leading to worse results than human annotators in our analysis.\nIn addition to producing a focused biomedical dictionary for social media, our manual annotation effort demonstrates that false positive terms (clinically relevant terms used in a clinically irrelevant context) with high frequency exist in social media discourse.\nIn other words, dictionaries built from biomedical terminology appropriate for the scientific literature, contain terms that are used in social media with other meanings.\nMoreover, those terms bias the knowledge inferences that automated pipelines might produce.\nIndeed, we demonstrate that the removal of just a few high frequency false-positive dictionary terms improves the biomedical knowledge extracted from the epilepsy cohort on Instagram, thus highlighting the importance of human annotation to improve the quality of cohort-specific social media analysis."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Data and Methods",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dictionary Construction",
            "text": "Our dictionary includes terms related to drugs, allergens, medical terms, and natural products, including cannabis. We construct this dictionary following [15  ###reference_b15###, 29  ###reference_b29###]. We recall the deails of that construction below.\nWe obtained these terms from a variety of existing medical ontologies and data sources.\nDrug, allergen, and food terms are retrieved from Drugbank (v.5.1.0) [30  ###reference_b30###].\nMedical terms including, but not limited to, disease symptoms and drug side effects are obtained from MedDRA (v.15) [31  ###reference_b31###].\nNatural products are retrieved from MedlinePlus [32  ###reference_b32###] and TCMGeneDIT [33  ###reference_b33###].\nFor cannabis, we manually added commonly referred terms and slang, such as ‘Mary Jane’ and ‘420’, to our dictionary, as detailed in [15  ###reference_b15###].\nIn addition, epilepsy terms commonly used by patients on Internet forums were manually added using a C-value [34  ###reference_b34###] tokenizer on the Epilepsy.com discussion forums.\nThese epilepsy-related terms include mentions such as ‘VNS’ (i.e. Vagus Nerve Stimulator) and were validated by an epilepsy specialist and matched to MedDRA codes.\nWe distinguish dictionary terms into four categories: Allergens, Drugs, Medical Terms, and Natural Products.\nAllergens include food names, ingredients, and animals (e.g., Orange, Duck);\nDrugs include medicine and chemical compounds (e.g., Diazepam);\nMedical Terms include status and conditions of putative medical relevance, such as physical, psychological, or physiological features (e.g., Headache, Feeling hot);\nNatural Products consist of plants and their extracted elements (e.g., Rose).\nImportantly, synonyms are possible for each term. Therefore, all those are matched—as child terms—to a unique parent (preferred) term.\nFor instance, Weed, Mary jane, 420 and Cannabis are all synonyms of the parent term Cannabis.\nThe parent term is also included as a child term for completeness of synonym lists.\nDrug names are treated similarly, whereby we keep the chemical name as parent terms (e.g. Diazepam), and all known commercial names (as extracted from DrugBank [30  ###reference_b30###]) as child terms (e.g. Valium).\nSome data sources of our dictionary already have term hierarchy, and we used it as the base of our parent term mapping (for example, the “preferred term” in MedDRA is mapped to the parent term in our dictionary).\nTable 1  ###reference_### lists examples of the extracted posts, terms, and their parent term.\nDrug brand names sometimes have very common names in the English language, which may increase the number of false-positive hits (e.g., Nighttime is a common word and a synonym for the drug Benadryl).\nTo account for that, we matched terms in our dictionary to the expected occurrence of such terms in the Brown Corpus [35  ###reference_b35###].\nTerms very commonly used in the daily English language were then ranked and removed.\nAfter this collection and initial automatic curation procedure, our dictionary contains 176,278 terms, of which 105,345 are Drugs, 66,961 are Medical Terms, 2,797 are Allergens, and 1,175 are Natural Products."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Data Collection and Post Tagging",
            "text": "Since June 2016, the collection of Instagram data via the platform’s API has been limited due to a company policy change.\nHere we use publicly available data from Instagram posts ranging from 2011 to early 2016 when they were collected and securely stored according to the platform’s terms on our servers [15  ###reference_b15###].\nThe data is comprised of the entire timelines (all time-stamped public posts) of Instagram users who produced at least one post mentioning a hashtag (i.e., #) with a drug name (or synonym) known to treat epilepsy.\nThe (parent) drug names we used to retrieve timelines include carbamazepine, clobazam, diazepam, lacosamide, lamotrigine, levetiracetam, oxcarbazepine, as well as all their brand name (child term) synonyms (e.g., Valium).\nIn addition, we added all user timelines that mentioned the epilepsy-associated hashtag ‘#seizuremeds’ which is commonly used among PWE on discussion forums such as Epilepsy.com.\nThis resulted in the collection of the entire timelines of a cohort of 9,890 users, comprising 8,496,124 posts.\nDuplicate posts from regrams were removed.\nIn order to protect user privacy, we did not extract demographic information from the collected accounts.\nThe caption field of all collected posts was subsequently tagged with the dictionary terms according to an automatic multi-word lexical matching pipeline, resulting in a total of 979,683 dictionary term matches on the more than 8 million Instagram posts on the dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Human-centered Annotation",
            "text": "Automatic lexical matching with biomedical dictionaries built from scientific discourse and evidence is not necessarily contextually accurate when tagging social media posts.\nIndeed, dictionary terms used in social media discourse may refer to alternate meanings without any putative clinical relevance.\nFor instance, the term ‘hot’ has multiple meanings depending on context, ranging from ‘having a fever’ to ‘sexual appeal’.\nNaturally, we are only interested in the potential clinical relevance of dictionary term usage.\nTherefore, we need to refine the dictionary terms to improve the accuracy of term matches in biomedical social media analytics. That is, the goal is to reduce false positive term matches, such as in the example above.\nTo do this we designed and pursued a manual annotation workflow to identify the dictionary terms most likely to be false positives in the context of Instagram discourse related to epilepsy.\nBecause it was unfeasible for our team to manually annotate over 8 million posts, a sample was randomly selected to provide a reasonable amount of posts for human annotators. This resulted in a set of 1,771 posts containing at least one matched term, for a total of 2,947 matches, associated with 466 unique parent terms.\nThe number and proportion of matches per dictionary category is:  = 874 (29.7%; 108 parent terms),  = 204 (6.9%; 64 parent terms),   = 1,647 (55.9%; 268 parent terms), and   = 222 (7.5%; 26 parent terms).\nTable 1  ###reference_### shows examples of posts and respective matched child terms (in red), their parent terms and categories.\nEach human annotator was given our annotation guidelines to understand the goal of the study and criteria to determine whether a matched term is used in the expected or correct sense (‘true-positive’) or not (‘false-positive’).\nAlso, they were provided with instructions with examples to learn how to annotate through our annotation tool.\nThis sample was subsequently used in our annotation workflow which comprises two rounds (See Figure 1  ###reference_### & SI: Figure S1  ###reference_###):\n###figure_1### Initial annotation & guideline refinement. 292 posts with 499 dictionary matches were used to test the annotation workflow and to refine our annotation guidelines, establishing a standard for deciding whether a matched term was used as intended by the biomedical dictionary (true positive) or not (false positive) in the context of the post. For instance, in the context of medical terms, if a matched term “A” is expressed as signifying a medical term, health condition, drug, food, or (potential) allergen (‘A’), an annotator is instructed to label it as ‘True.’ Conversely, if the term is expressed in a manner other than the intended dictionary meaning, but rather represents a metaphor, proper noun, or anything else, the annotator marks it as ‘False.’ Each sampled post was assigned to two annotators, with disagreements being collectively discussed to reach a consensus that triggered a revision to the guidelines.\nFull annotation review. Using the refined annotation guidelines obtained from the first step of the workflow (See Table 2  ###reference_###), all 1,771 posts in the sample and their 2,947 matches were reviewed independently by two annotators.\nData scientists in training as well as epilepsy researchers participated in the annotation.\nThey decided whether terms were appropriately matched (true-positive), inappropriately matched (false-positive), or unclear to determine (e.g., a term with unclear meaning).\nAnnotations were compared, achieving a good inter-rater reliability rating (Cohen’s Kappa=0.634) [36  ###reference_b36###].\n###figure_2### An example of the interface used by annotators is provided in Figure S2  ###reference_### in SI. In this interface, each row displays a complete Instagram post with a dictionary term match highlighted in red. An annotator can review the context of each post and determine whether the meaning of the red-highlighted word is expressed with the intended use of the biomedical dictionary term, following the criteria provided in the guidelines.\nNotice that annotators are not shown the photos that accompany the post text on Instagram. They are only shown the text, which makes this a difficult task for human annotators and more so for LLMs used below."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Impact of Removing Ambiguous Terms",
            "text": "After dictionary refinement, we evaluate the impact of term removal on the eigenvector-centrality\nof networks whose edge weights are obtained by tallying dictionary co-mentions in posts (their construction, especially how we dealt with parent terms and child terms, is detailed in SI Section B.1  ###reference_###).\nThese co-mention networks can be seen as associative knowledge structures that characterize the discourse of social media cohorts [38  ###reference_b38###, 39  ###reference_b39###, 15  ###reference_b15###, 26  ###reference_b26###].\nThe co-mention networks is built on the level of parent terms in the sense that each node is a parent term covering several synonyms in the dictionary.\nIn such associative knowledge networks, removing terms from the dictionary potentially reduces the set of nodes that comprise them (each parent term is represented as a node and the term removal is done in the level of child term). More importantly, removing terms also affects the edge weights between nodes, as co-mention counts are altered. Since the structure of connections is altered, all inferences one can make from these networks—such as information retrieval, link prediction, community structure, shortest paths [39  ###reference_b39###]—can be affected.\nWe chose eigenvector centrality because it is a popular measurement of node importance that accounts for indirect influence between nodes on undirected graphs, thus allowing us to assess the network-level impact of term removal [40  ###reference_b40###].\nAnother popular node centrality is PageRank centrality. Although it is powerful on directed graphs, it is highly correlated with (or almost exactly the same as) node degree when applied to undirected networks [41  ###reference_b41###, 42  ###reference_b42###]. Therefore, since our co-mention networks are undirected, we choose eigenvector centrality rather than PageRank to assess the effect of removing terms.\nThe top 20 terms ranked by eigenvector centrality, before and after dictionary refinement, are shown in table 5  ###reference_###.\nTop eigenvector centrality terms before the refinement include terms not particularly relevant to epilepsy, such as Cocoa or Tattoo.\nHowever, after dictionary refinement, not onlydo parent terms like Feeling hot disappear from the top, since their main child terms were no longer present in the dictionary, but we see that all the top 10 terms are associated with epilepsy, as attested by our epilepsy specialists.\nFor instance, Depression, a clinical diagnosis often co-morbid with epilepsy, jumps from 9th (0.040) to the top ranked centrality score term (0.599), closely followed by Anxiety.\nThis suggests that our dictionary refinement improved the quality of the top eigenvector centrality terms, bringing epilepsy-related terms to a more central role in the knowledge network.\n###figure_4### To investigate whether the observed changes on the top eigenvector centrality terms are simply due the high frequency of the 8 removed terms, we computed a null model experiment in which 8 terms with similar frequencies but smaller false positive rates (below ) are randomly sampled 1,000 times and removed, with subsequent computation of eigenvector centrality of the resulting networks.\nBecause the top  lists before and after dictionary refinement do not have the same set of terms, common rank difference metrics, such as Spearman’s coefficient and Kendall’s  cannot be directly applied.\nIn Figure 3  ###reference_###, we compare the top  terms by eigenvector centrality (of the dictionary term co-mention networks), before and after removing 8 dictionary terms, by counting the common elements ratio (CER) between the top  lists for each case.\nIt is very clear that the impact of removing the 8 terms with false positive rate above  (via human-annotation) is much higher than when we remove 8 random terms with same or higher frequency, but lower false-positive rate.\nFor  (the top 10 with largest eigenvector centrality), only  of the terms remain after removal of the 8 terms with largest false positive rate per human annotation. Whereas, in the case of random removal of 8 terms with similar frequency, on average,  of the terms remain unaffected in the top 10.\nThe difference between the selected terms and the null model remains large for all values of , even if CER increases.\nWhile the impact of removing random terms almost saturates after , at 91% CER, the impact of removing the 8 terms with high false positive rate remains significant for all , with over 20% different terms even for .\nAdditionally, we also use Fagin’s generalized Kendall’s distance , a metric specially designed to compare top  ranked lists [43  ###reference_b43###]. We use it to quantify the impact on rank of removing the 8 ambiguous terms by computing the distance between the eigenvector centrality rankings obtained before and after removal.\n is comparable only to lists with the same sizes (the same ), with a higher value denoting a higher rank difference, with the penalty of missing elements considered.\nThe value of  obtained using our refined dictionary (denoted ) for various values of  is given in Table 6  ###reference_###, alongside the average distance obtained by removing 8 similarly frequent random terms (denoted ).\nTable 6  ###reference_### shows that the 8 ambiguous terms selected through human annotation have statistically significantly larger impact on eigenvector centrality rankings (higher ) beyond the top 20 nodes than removal of similar random sets of terms. Though removing random high frequency terms could have a similar impact on the top 10 terms list in relatively rare cases (about 14% of the time), we observed no case (out of 1,000 samples) in which this occurred for the top 20 terms or beyond.\nMoreover, the difference between  for the 8 ambiguous terms and the average  for the randomly removed terms increases dramatically as the list size  increases.\nThe null model comparison, with both CER and Fagin’s generalized Kendall’s distance for rank comparison, demonstrates that removal of terms revealed as ambiguous by human annotation, have much higher impact on the knowledge network of biomedical terms than removing random terms with the same frequency (but lower false positive rate).\nIn other words, the high false-positive rate terms, estimated per human annotation, are likely confusing the co-mention network with many spurious edges that are not coherent with the remaining biomedical knowledge captured by the co-mention networks. Thus, removing them has a high impact on the network because their co-mention associations are quite different from those of other terms in dictionary.\nIndeed, random removals of terms with similarly high frequency are not as impactful, because many of the associations the random terms induce are more coherent with the structure of other biomedical term associations.\nIn summary, the network global impact of removing human annotated high false-positive rate terms highlights the value of human context-specific annotation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Comparing Social Media with Medical and Scientific Discourse",
            "text": "At the onset, the hypothesis behind our study is that while there is much discourse on general-purpose social media platforms that is of biomedical relevance, it is expressed differently than scientific discourse and unfolds simultaneously with many other contexts that are not relevant to health. Thus, human-centered dictionary refinement is needed to remove ambiguous terms in such platforms.\nTo emphasize this need, and how different general-purpose social media discourse is, we go beyond Instagram and deploy both the original and the refined version of our dictionary in other data sources: epilepsy-related abstracts from PubMed, epilepsy-related clinical trials from clinicaltrials.gov  ###reference_clinicaltrials.gov###, Twitter (now named X) posts from users who have mentioned epilepsy related terms, and discussion forums from Epilepsy.com  ###reference_Epilepsy.com###. Detailed descriptions of data harvesting and network construction of additional data sources are available in SI Appendix B  ###reference_### under each data source section.\nThe first two additional data sources pertain to scientific discourse.\nBoth Instagram and Twitter data sources represent epilepsy related users’ speech on a general-purpose social media. The last data source is a form of social media that is focused on a single topic of medical relevance—epilepsy, in this case.\nIn Table 7  ###reference_### we compare the impact of removing the 8 high false positive terms using Fagin’s generalized Kendall’s distance as in Table 6  ###reference_###.\nTwitter, being a general purpose social media like Instagram, received an impact approximately a half of Instagram received after terms removal, as measured by Fagin’s generalized Kendall’s distance.\nOther data sources received much less impact. Networks built from clinicaltrials.gov  ###reference_clinicaltrials.gov### data and the Epilepsy.com  ###reference_Epilepsy.com### forums data set received zero impact on top 20 terms list and at most 6% of the impact as Instagram on other lists. PubMed, being a much larger text corpus, received only at most 22% of the impact as Instagram.\nThis is also supported by Table S2  ###reference_###, Table S4  ###reference_###, Table S9  ###reference_###, and Table S11  ###reference_### in SI, in which we show that aside from Twitter, for all other data sources, there are only inconsequential impacts on the top 20 terms ranked by eigenvector centrality—except for the disappearance of those 8 terms we removed.\nIn other words, unlike in Instagram and Twitter , those 8 terms are either not ambiguous at all or their ambiguous usages do not affect the network analysis greatly in scientific database (PubMed and Clinical trials) nor in epilepsy-specific social media (Epilepsy Foundation Forums) discourse.\nThis suggests that general-purpose social media platforms such as Instagram are much noisier for biomedical surveillance, and for epilepsy research in particular, since users tend to post about many distinct aspects of their lives.\nIn contrast, in the Epilepsy Foundation (EF) forums users center discourse around their condition, which naturally makes it a rich resource for epilepsy research.\nThe same can be said for the chosen PubMed articles or Clinical Trials, where the scientific context is focused on epilepsy.\nTherefore, the impact of removing the same 8 terms from the knowledge networks of both the EF forums and the PubMed abstracts is much less pronounced in comparison to Instagram ."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "The disagreement between GPT-4 and human annotators is significant",
            "text": "Complementary to our human annotation efforts, we tested the feasibility of using a large language model instead of human annotators in the labeling process of our proposed dictionary refinement workflow. We assigned the same labeling task to OpenAI’s latest and most advanced model, GPT-4 (version 1106 in 2023) and its precesssor GPT-3.5 via OpenAI’s API, using the same guidelines we provided to human annotators [44  ###reference_b44###, 45  ###reference_b45###].\nWe find significant disagreement between GPT-4 and the decisions of human annotators.\nWe use OpenAI’s API to ask OpenAI’s GPT-3.5 (version gpt-3.5-turbo-1106) and GPT-4 (version gpt-4-1106-preview) to do the same labeling task as our human annotators.\nWe converted the human annotation guidelines (detailed in section 3.1  ###reference_### and summarized in fig. S2  ###reference_###), initially designed for humans and presented to them in slides, into a text-based prompt for the AI, undergoing several rounds of refinement for optimization.\nFor human annotators, the term matches to be annotated are highlighted in color, while for the LLM’s task, these terms are marked with asterisks (*) on both sides.\nThis method serves as a text-based alternative to visual highlighting, and is commonly used in prompting LLMs.\nFor each term tagged on a post, we query the LLM using two prompts: one system prompt based on the annotation guidelines, and one user prompt containing the entire text of post with the highlighted term.\nThe LLM was instructed to provide a response in json format, with both the decision label and justification for the classification.\nThe model’s response was then collected and parsed.\nFor term matches that the LLM returned different label than human annotators, we used the justification text generated by the model to suggest possible causes for the disagreement.\nFor each round of our prompt iteration, we used 200 tagged terms to test the performance.\nDuring those tests, GPT-3.5 consistently showed inferior performance compared to GPT-4, so we did not run a full-scale evaluation for GPT-3.5, choosing to focus on GPT-4 only.\nFor the final full-scale test, we sent 1,500 term matches to GPT-4 (corresponding to 1,500 rows in the tables for human annotators), using our final version of the prompt.\nTreating the consensus of two human annotators as the ground truth and grouping all “Uncertain” or “Disagree” (if the two annotators gave different annotations) labels with the negative label, we find a Matthews correlation coefficient (MCC) of  for GPT-4’s classification results. This improves slightly to  if we discard all “Uncertain” or “Disagree” from both GPT-4 and human annotators’ annotation.\nIn particular, GPT-4 tended to label correctly matched terms (“True Positive” in the annotation guideline and “match” for short in the following tables) as incorrectly matched (“mismatch” for short in the tables). In other words, it is more strict in designating a term as being used in a putatively clincally relevant way.\nAs shown in Table 8  ###reference_###, for the same set of 1,500 term matches, there are only 29 cases where the master annotator determined it as “mismatch” while the annotator 2 thought it was a “match”, in comparison, there are 285 cases where GPT-4 thought it was “mismatch” and the annotator 2 thought it was a “match”."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this paper, we show the importance of manual curation in developing a biomedical dictionary to study epilepsy-related discussions on social media.\nDespite its cultural importance and global reach, Instagram , the social media we study has not been a focus of social media research.\nIn general, most biomedical research on social media focuses on platforms with freely and easily accessible data, such as Twitter (now named X).\nIn addition, such studies are either interested in a few drugs or medical-related terms [46  ###reference_b46###, 47  ###reference_b47###, 48  ###reference_b48###], or on conditions with less associated stigma.\nVery few studies so far have analyzed the social media discourse of people with epilepsy and their caregivers [6  ###reference_b6###] using biomedical dictionaries.\nThe use of dictionary-based methods to study biomedical discourse, however, is not new.\nIn general, they have been used for knowledge discovery and applied to large corpus of clinician-written notes over the course of clinical examination, and more recently extracted from electronic health records [49  ###reference_b49###, 50  ###reference_b50###, 51  ###reference_b51###, 52  ###reference_b52###].\nAs much as these dictionaries have been shown to work well in clinical settings, they have not been tailored to the informal and broad discourse of social media.\nTherefore, our goal here using human annotation is to identify and evaluate the impact of false-positive dictionary terms matched to social media data.\nWe were able to identify and remove terms with high false-positive rates and that were not removed in our initial automated curated process, thus highlighting the importance of a human-in-the-loop throughout the curation process.\nHuman annotation has been widely used to analyze and understand sentiments, behaviors, perceptions, languages, or relations of people in social media [53  ###reference_b53###, 54  ###reference_b54###].\nEven though automated annotation techniques are able to identify and match terms faster than human annotation [55  ###reference_b55###], our results support prior literature showing human annotation is necessary when extracting term meaning from social media posts [56  ###reference_b56###, 57  ###reference_b57###].\nTherefore, balancing between the speed of automated annotation and the reliability and validity of human annotation can be one of key approaches to optimal results [56  ###reference_b56###, 58  ###reference_b58###, 59  ###reference_b59###].\nWe identified what kinds of terms tend to mean differently than their biomedical meaning through human annotation and how often they have been used in social media posts.\nThe false positive matches were from the different meanings of words. Proper nouns were one of the most common cases.\nBesides, words were often used as metaphors, which cannot be learned from the English dictionary.\nThese results imply that future research on social media with biomedical dictionaries should be cautious about terms with multiple meanings to perform a more precise analysis.\nAnother noise source in term tagging on social media is lack of context information.\nThere were cases where our annotators found it difficult to determine the true meaning of the words due to lack of context.\nIt would be impossible for machine based automated methods to determine whether those term matches should be tagged or not.\nIf there are abundant cases among them that the term was actually not used with the biomedical relevant meaning, they would introduce the same kinds of noise as the ambiguous terms we identified in this study, to the text mining analysis.\nOur results of dictionary refinement via human annotation also demonstrate how a dictionary with ambiguous terms can have a great impact on downstream data analysis.\nWe showed that removing a few terms with high false-positive rates from the dictionary, guided by human annotation, could significantly change the results of network analysis.\nWe found that the impacts of the refined dictionary were not limited to the terms we removed.\nThere is no reason to believe that eigenvector centrality is the only analysis that will benefit from dictionary refinement.\nTrivially, network analysis based on node weight generated by node centrality based methods could be affected.\nIn addition, in future studies, we can also test the impact of dictionary refinement on other network analysis, including, but not limited to community detection and link prediction, all the way to impact on actual application like information recommendation system.\nThe great difference in the impact of the same dictionary refinement on different data sources also suggests potential network-based methods for identifying low-quality terms in the dictionary.\nThose low quality terms could be either ambiguous terms we discussed in this study or terms should not be in the dictionary in the first place.\nIf our assumption is correct, the majority of the knowledge structure and spurious connections generated by noise shall have distinctively different structure patterns.\nEither seperating the noisy pattern from the majority knowledge structure directly, or simply removing nodes and measuring the impact like we did for the dictionary refinement, could be potentially feasible approaches to identify the low-quality terms.\nWe also found that some data corpus benefit more from this dictionary refinement than other data corpus, suggesting the need for dictionary refinement should tailor to each individual research topic and data corpus.\nThe four additional data sources besides Instagram may have different sets of their own ambiguous terms distinct from the 8 identified for Instagram . Indeed, the manual annotation focused only on Instagram posts and it is possible that a similar approach could be useful for refining dictionaries specific to the EF forums, PubMed abstracts, Clinical Trials and even Twitter .\nDue to the associated cost of manual annotation, it is beyond the scope of this work to do so.\nThe variation in ambiguous terms across datasets may be attributable to the inherent characteristics and the qualitative distinctions of each data corpus.\nLanguage usage and richness levels could be different between social media, online health communities, and medical resources. For instance, polysemous words in online health communities and medical resources have much higher chances of being used with their medical-relevant and professional meanings than their casual meanings than in social media.\nIdentifying these differences may benefit future researchers in improving the quality of biomedical signal analysis on different types of datasets.\nEspecially, it would be essential to be cautious about the noisy dataset, such as data from social media. Using deep learning models, multi-corpus training, and normalization could be other ways to improve the performance of social media mining regarding the differences between those resources [60  ###reference_b60###].\nIn Section 3  ###reference_###, we have shown that GPT-4, as a representative of the latest LLM, could not replace human annotators in this task.\nThere are several factors that contribute to this performance difference.\nThe most significant factor we observed was that the GPT-4 often stayed within the narrow definition of some terms in the guidelines.\nFor example, in the guidelines, we mentioned that some terms in our dictionary were imported from medical dictionaries like MedDRA.\nHuman annotators could understand why “marriage” and “tattoo” are in MedDRA in the first place, and why “orange” as food can be an allergen, then labeled the term matches as “true positive” when they thought it was appropriate, while GPT-4 believed that the term was not used as a medical term and therefore should be labeled as “false positive”.\nA few counterexamples in the prompt can help, but cannot eliminate such kinds of bias.\nGPT-3.5 did much worse than GPT-4 in this kind of bias (see SI Section C.3  ###reference_###).\nFor both models, it was hard to enumerate all possible term types for which the model may fail.\nIn this paper, we discuss the results of using GPT series models as a direct replacement for human annotators.\nHowever, it is not the purpose of this paper to find the optimal workflow to use LLM on these types of tasks.\nStill, we gained some valuable experience about prompt engineering for the labeling task in this paper and we recommend people to spend considerable time on prompt engineering if they aim to use LLM for similar tasks.\nThe time spent on prompt engineering should be included into the time cost in project management.\nThe prompt engineering is not just about writing one good prompt from scratch, but is more about performing a comprehensive test to catch the “slips” or the bias of LLM in some types of terms, then trying to correct the bias in many iterations of the prompt.\nThis could require substantial amounts of human annotated data, depending on the goal of the research, with more accurate estimation on the term mislabeling need more data, and coarser estimation requires less.\nSome sampling methods could be used to reduce the size of data set needed, but in principle cannot improve the coverage of corner cases.\nIf sufficient human annotated data are readily available, fine-tuning the model could potentially be more effective than prompt engineering.\nTaking into account all those steps, the use of LLM for term labeling still requires substantial time investment if accuracy is a priority.\nAlthough current LLMs are not perfect replacements for human annotators, the research on LLM is a fast developing field with better models coming out every year.\nWe should not underestimate the potential of LLM in the future application of social media mining.\nBearing in mind the prerequisites of conducting comprehensive validations prior to the deployment of LLMs, several promising approaches are currently emerging that utilize LLMs to enhance term tagging in the immediate or foreseeable future.\nOne possible hybrid approach is discussed above: first uses human annotation to align LLM output, then LLM could be used to annotate larger data corpus. LLM’s annotation will then be used for dictionary refinement. In this way, we accumulate much more annotated data, thus providing better coverage for low-frequency terms and leading to potentially better dictionary refinement, with the same amount of human annotation data.\nIn the future, LLM could also be used to tag each occurrence of the term directly for the entire text corpus without dictionary refinement, so the “correct” usage of high false positive rate terms can be tagged and contribute to downstream analysis, in contrast to discarding all matches of high false-positive rate terms.\nHowever, even if current LLMs are sufficiently accurate, their high operational costs still prohibit large-scale applications. This limitation is likely to be alleviated in the future."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This research identified the terms in our dictionaries with higher frequencies and false-positive rates in an epilepsy cohort on Instagram by using the human annotation method.\nOn comparison, GPT series models cannot replace human annotation in the annotation process.\nWe also identified which terms should be removed from our dictionary to obtain more epilepsy-relevant results from network analysis. Additionally, we identify the fundamental cause of incorrect term matches—having different meanings that are often used—and specific cases. Finally, through validation of the impact of the refined dictionary on eigenvector centrality analysis of the co-mention networks, we demonstrated the impacts and complementary role of the human annotation method to the automated annotation technique.\nOur results demonstrate that human-centered dictionary refinement should be performed when conducting social media mining of biomedical relevance, and epilepsy in particular. It remains to be determined if this approach is also necessary for analyzing other types of scientific or patient-centered textual resources.\nOur study has utilized a large number of terms in medical dictionaries and focused on Instagram , which may have significant potentials for large-scale social media analysis. Other social media studies on medical corpus have more focused on Twitter and Facebook , and most of them have selected a few specific medical terms in their analyses.\nOur results and implications may help future research in our research communities, including the areas of bioinformatics and health informatics, by improving analysis of medical topics on social media speeches.\nUltimately, this research would contribute to developing knowledge graphs that more truthfully represent the underlying knowledge structure, which can be used for personalized information systems that can support PWE, PWEC. And the workflow could also be applied to other diseases and support other disease cohorts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "This work was partially funded by National Institutes of Health, National Library of Medicine Program, grant 01LM011945-01 (LMR, AM, XW, RBC, and WRM), a Fulbright Commission fellowship (LMR), the NSF-NRT grant 1735095 “Interdisciplinary Training in Complex Networks and Systems” (LMR, AM, XW and RBC), and Fundação para a Ciência e a Tecnologia, grant PTDC-MEC-AND-30221-2017 (RBC)\nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."
        }
    ]
}