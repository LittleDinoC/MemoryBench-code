{
    "title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?",
    "abstract": "Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these characteristics at a time.\nTo measure progress towards the combined goal, we introduce the task of pronoun fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later.\nWe present RUFF, a carefully-designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters).\nWhen an individual is introduced with a pronoun, models can mostly faithfully reuse this pronoun in the next sentence, but they are significantly worse with she/her/her, singular they and neopronouns.\nMoreover, models are easily distracted by non-adversarial sentences discussing other people;\neven one additional sentence with a distractor pronoun causes accuracy to drop on average by 34%.\nOur results show that pronoun fidelity is neither robust, nor due to reasoning, in a simple, naturalistic setting where humans achieve nearly 100% accuracy.\nWe encourage researchers to bridge the gaps we find and to carefully evaluate reasoning in settings where superficial repetition might inflate perceptions of model performance.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Third-person pronouns (he, she, they, etc.) are words that construct individuals’ identities in conversations (Silverstein, 1985  ###reference_b54###).\nIn English, these pronouns mark referential gender for the entity they are referring to,\nwhich can also index an individual’s social gender, e.g., man, woman, non-binary (Cao and Daumé III, 2020  ###reference_b9###).\nCorrectly using the pronouns an individual identifies with is important, as misgendering (including through incorrect pronoun use)\ncan in the best case be a social faux pas (Stryker, 2017  ###reference_b57###) and in the worst case, cause psychological distress, particularly to transgender individuals (McLemore, 2018  ###reference_b41###).\nAccordingly, it is important for large language models (LLMs) to use pronouns faithfully and without causing harm.\nTo this end, many studies have explored how LLMs handle pronouns, showing that they stereotypically associate pronouns and occupations (Kurita et al., 2019  ###reference_b30###), reason about co-referring pronouns and entities better when they conform to stereotypes (Tal et al., 2022  ###reference_b58###), fail when exposed to novel pronoun phenomena such as neopronouns (Lauscher et al., 2023  ###reference_b33###), and cannot consistently reuse neopronouns during generation (Ovalle et al., 2023a  ###reference_b45###).\nThese shortcomings create quality of service differences and cause representational harm,\namplifying discrimination against certain pronoun users (Blodgett et al., 2020  ###reference_b6###; Dev et al., 2021  ###reference_b13###).\nHowever, a question that has gone unexamined thus far is: How robust is model faithfulness to pronouns when discussing more than one person? To answer this question, we propose pronoun fidelity (§2  ###reference_###), a new task to investigate realistic model reasoning about pronouns, and we introduce RUFF (§3  ###reference_###), a novel, large-scale dataset of over 5 million instances, carefully designed to evaluate this task. With this dataset, we present an analysis of pronoun fidelity across 37 popular language models covering a range of architectures and scales (11M-70B), to investigate whether models are reasoning, repeating, or just biased.\nFirst, we collect model pronoun predictions for occupations in the absence of context, to establish a “bias baseline” (§5  ###reference_###).\nNext, we evaluate whether models can overcome their biased pronoun predictions when explicitly shown what pronoun to use in context (§6  ###reference_###).\nAll models are good at this task, but there are significant disparities across pronoun sets.\nWe then test the robustness of this result by inserting naturalistic distractor sentences using a different pronoun to talk about another person (§7  ###reference_###).\nEven one non-adversarial distractor sentence vastly deteriorates model performance as shown in Figure 1  ###reference_###.\nFinally, in a detailed error analysis (§8  ###reference_###), we disentangle whether model errors can be attributed to distraction or falling back to bias, finding that encoder-only and decoder-only models behave in fundamentally different ways.\n###figure_1### Overall, our results show that models struggle to reason about pronouns in a simple, naturalistic setting and highlight the need for careful task design to ensure that superficial repetition does not lead to inflated claims about model reasoning.\nWe release all code and data to encourage researchers to bridge the gaps we find:\nhttps://github.com/uds-lsv/robust-pronoun-fidelity  ###reference_fidelity###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Pronoun Fidelity Task",
            "text": "Discussing multiple individuals is natural, frequent and well-studied in discourse;\nwe use both definite references and pronouns in natural language to establish continuity and coherence (Grosz et al., 1995  ###reference_b21###).\nWe formalize a version of these phenomena in our task:\ngiven a context in which a co-referring entity and pronoun are introduced, the task is to reconstruct the pronoun later in a sentence about the entity, independent of a limited number of potential distractors.\nIntroduction: The accountant had just eaten a big meal so her stomach was full.\n\n(Optional)\nDistractor 1: The taxpayer needed coffee because their day had started very early.\n…\nDistractor N: Their sleep had been fitful.\n\nTask sentence: The accountant was asked about ___ charges for preparing tax returns.\nMore formally, an introduction sentence  establishes a coreference between an entity  and a pronoun . A distractor sentence  explicitly establishes or implicitly continues a previously-established coreference between a different entity  and a different pronoun , i.e.,  and . Let  be a set of distractor sentences such that . When combined, an introduction sentence and the set of distractor sentences form a context. A task sentence  contains an unambiguous coreference between the entity  from the introduction and a pronoun slot  which must be filled.\nThe task is to maximize\nthe probability  of reconstructing the correct pronoun  in the sentence , given the context."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "RUFF Dataset",
            "text": "RUFF, our evaluation dataset for Robust pronoUn Fidelity at scale, is constructed with a 3-step pipeline to create naturalistic instances of our task with unambiguous answers: template creation (§3.1  ###reference_###), assembly (§3.2  ###reference_###), and validation (§3.3  ###reference_###).\nOur data covers 60 occupations (see Appendix B  ###reference_###) and four third-person pronouns (he, she, they and xe) in three grammatical cases (nominative, accusative and possessive dependent). Our occupations (and corresponding participants for distractor sentences) are chosen from Winogender schemas (Rudinger et al., 2018  ###reference_b48###), as their bias characteristics are well-studied in NLP. In addition to the English masculine (he/him/his) and feminine (she/her/her) pronouns, we heed Lauscher et al.  ###reference_b32###’s (2022  ###reference_b32###) call for more inclusive NLP research by examining two more pronoun sets that are less well-studied in NLP: singular they (they/them/their), the pronoun of choice of over 75% of respondents to the Gender Census (Lodge, 2023  ###reference_b39###), and xe/xem/xyr, the most popular neopronoun according to the same census."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Template creation",
            "text": "Task templates. We create one task sentence template per occupation and grammatical case for a total of 180 templates, designed to create an unambiguous coreference with the occupation only. For instance, charges for preparing tax returns can only belong to an accountant, never a taxpayer, which is the corresponding participant.\nContext templates. We create explicit (definite reference + pronoun) and implicit (pronoun-only) versions of 10 context templates per grammatical case, for a total of 30 templates. Each explicit context template begins with an entity and introduces the pronoun in a clause, e.g., The taxpayer needed coffee because their day had started very early, while implicit templates are simple sentences like Their sleep had been fitful.\nAll templates contain generic themes, e.g., universal human emotions and sensations (hungry/full, tired/energetic, unhappy/happy, etc.).\nThese templates can thus be applied to all occupations and participants, and allow for a controlled but still coherent setting."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Template assembly",
            "text": "We then instantiate and combine the previously created templates to assemble our data instances. First, we select an occupation () and one of its task templates. We pick a pronoun () to use as ground truth and instantiate a random context template with the selected occupation and pronoun.\nThe simplest version of the pronoun fidelity task includes just this introduction sentence followed by the task sentence. Instantiating 10 templates with 4 different pronoun sets and pairing them with task templates for 60 occupations across 3 grammatical cases gives us a total of 7,200 unique instances for this version of the task.\nTo create more complex data instances, we insert a variable number of distractor sentences between the introduction and task sentences, discussing a participant  who uses a different pronoun .\nThese are also sampled from the set of context templates (see Appendix C  ###reference_### for details).\nInstantiating 4 templates with 3 unused pronouns gives 86,400 unique instances with one distractor.\nOur stackable dataset design allows us to generate a vast amount of data of varying lengths, giving us a controlled setting to evaluate context effects on model predictions.\nWe subsample the data with three random seeds for the rest of our evaluation, ensuring that all occupations, cases, pronoun declensions and distractor pronouns are equally represented in each subsampled set of 2,160 sentences.\nAll data statistics are shown in Table 1  ###reference_###.\n###table_1###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Data validation",
            "text": "We validate all task and context templates. To verify that the pronoun fidelity task is easy and unambiguous for humans, we also validate a subset of these instances with 0-5 distractors.\nAnnotator information is shown in Appendix D  ###reference_### and all annotator instructions are provided in Appendix E  ###reference_###.\nTemplates. Two authors with linguistic training iteratively created and validated sentence templates for grammaticality and correct coreferences until consensus was reached.\nAn additional annotator independently rated 100% of the sentences as grammatical and with the correct coreferences.\nPronoun fidelity task. We sampled 100 instances with each possible number of distractors (0-5), for a total of 600 instances. One author and one annotator had to fill in the pronoun and they each performed with 99.8% accuracy.111They disagreed on non-overlapping instances which appeared to be random slips."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We list our models, evaluation methods, and metrics. Further details are provided in Appendix F  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "We experiment with 37 transformer-based language models (see Table 2  ###reference_###), chosen to evaluate the effects of architecture and scaling.\nOur 9 encoder-only models are from the BERT (Devlin et al., 2019  ###reference_b14###), RoBERTa (Liu et al., 2019  ###reference_b38###), ALBERT-v2 (Lan et al., 2020  ###reference_b31###) and MosaicBERT (Portes et al., 2023  ###reference_b47###) model families, as the first three remain well-used in NLP, and the last is trained on much more data.\nAs for our 20 decoder-only models, we select the popular Llama-2 (Touvron et al., 2023  ###reference_b60###) model family, as well as OPT (Zhang et al., 2022  ###reference_b66###) and Pythia (Biderman et al., 2023  ###reference_b5###) for their large range of model sizes.\nIn Appendix I  ###reference_###, we also experiment with eight popular chat models that are further trained with instruction-tuning and reinforcement learning, to evaluate task performance with prompting; specifically, we use decoder-only Llama-2-chat models (Touvron et al., 2023  ###reference_b60###) and encoder-decoder FLAN-T5 models (Chung et al., 2022  ###reference_b11###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Obtaining predictions",
            "text": "Decoder-only models. For the majority of our experiments, we follow Hu and Levy (2023  ###reference_b25###) in taking direct measurements of probabilities as a proxy for models’ metalinguistic judgements. We verbalize four versions of each data instance, i.e., we fill in the blank with each of the four pronouns we consider, creating four options. We then compute instance-level model log likelihoods for these options, and select the option with the highest log likelihood as the model’s choice.\nEncoder-only models. For comparability with decoder-only models, we follow the same procedure for encoder-only models, but use pseudo log likelihoods (Salazar et al., 2020  ###reference_b49###; Kauf and Ivanova, 2023  ###reference_b28###). We do not use masked token prediction due to tokenization issues with neopronouns (Ovalle et al., 2023b  ###reference_b46###); briefly, we want xe to be tokenized “normally” (which is often as two tokens) rather than a single UNK token.\nChat models. Following common practice, we evaluate chat models (FLAN-T5 and Llama-2-chat) using vanilla and chain-of-thought prompting in Appendix I  ###reference_###. Following Sclar et al. (2024  ###reference_b51###), we show the range of expected performance with 10 different prompts (see Appendix G  ###reference_###), inspired by the prompts to elicit coreferences in the FLAN collection (Longpre et al., 2023  ###reference_b40###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "As every instance of the pronoun fidelity task has a unique correct answer, we report accuracy averaged over the three randomly sampled subsets of our dataset. We show the standard deviation with error bars or shading.\nWhere possible, we perform significance testing with a Welch’s t-test and a threshold of 0.05. We use human performance as our ceiling, and compare models to a baseline of randomly selecting 1 of the 4 pronouns (i.e., 25%).\n###figure_2###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Model Predictions with No Context",
            "text": "We begin by creating a “bias baseline,” i.e., obtaining pronoun predictions from models on our task sentences in the absence of any context.\nIn Section 6  ###reference_###, we will examine whether models can overcome this bias with reasoning when provided with context establishing a single correct answer.\nExample: The accountant was asked about ___ charges for preparing tax returns.\nNo single answer (among his, her, their, xyr)\n\\endMakeFramed\nAs we cannot evaluate accuracy on a task with no single correct answer, we show the counts of model predictions of different pronoun declensions in Figure 2  ###reference_###, averaged over all models. Model-specific counts are shown in Appendix H  ###reference_###.\nEven though our task sentences are designed such that any pronoun set can be used grammatically, all models tend to assign higher probability to he/him/his than other pronoun sets.\nObtaining pronoun predictions without context is a popular method to measure model bias, with numerous papers (Kurita et al., 2019  ###reference_b30###, inter alia) showing that associations between occupations and pronouns are based on social gender stereotypes, e.g., doctor-he and nurse-she.\nHowever, model pronoun predictions might reflect dataset artifacts such as the choice of occupations, or be a statistical accident of the chosen templates (Seshadri et al., 2022  ###reference_b52###).\nIn addition, intrinsic biases may not correlate with actual pronoun use with context (Goldfarb-Tarrant et al., 2021  ###reference_b19###).\nIn order to test for such extrinsic behaviours, the rest of this paper examines whether models can override their intrinsic statistical biases on these same templates when provided with the right pronoun to use.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Injecting an Introductory Context",
            "text": "When models are provided with an introductory sentence explicitly establishing the pronoun to use for an entity, can they use that pronoun to refer to the same entity in the immediate next sentence?\nExample: The accountant had just eaten a big meal so her stomach was full. The accountant was asked about ___ charges for preparing tax returns.\nCorrect answer: her\n\\endMakeFramed\nAs Figure 3  ###reference_### shows, all models perform better than chance at pronoun fidelity with a simple introduction (up to 0.95 with MosaicBERT), but not as well as humans, who achieve perfect performance. We also see improvements with increasing model scale, with the exception of ALBERT-v2, as in Tay et al. (2022  ###reference_b59###).\nWhich pronouns are harder? Even in the simplest case of the pronoun fidelity task, patterns emerge when split by pronoun, as shown in Figure 4  ###reference_###. Overall model accuracy on he/him/his is significantly higher than she/her/her, which in turn is significantly higher than both they/them/their and xe/xem/xyr, in line with previous findings that language technology has gaps when it comes to neopronouns (Lauscher et al., 2023  ###reference_b33###).\nModels show intriguing patterns with these last two pronoun sets. Most encoder-only models appear to handle the neopronoun better than singular they (e.g., BERT-large has an accuracy of 0.78 on xe/xem/xyr compared to 0.60 on they/them/their), which warrants further investigation.\nDecoder-only models smaller than 6.7B parameters struggle with the neopronoun, with every OPT and Pythia model smaller than 2.7B parameters performing below chance, and in some cases (e.g., Pythia-14M, Pythia-70M and Pythia-160M) even performing close to 0.0.\nBeyond this scale, however, models perform better on xe/xem/xyr than on singular they, with Llama-13B achieving 0.96 accuracy on the neopronoun. These differences are statistically significant. As the training data for individual model families is the same, this might suggest that decoder-only models generalize to novel pronouns starting at the scale of 6.7B parameters, but in light of Schaeffer et al. (2023  ###reference_b50###), this result could just as well be a mirage resulting from our use of accuracy, a discontinuous metric.\nIn either case, our observations could also explain the poor performance that some previous studies of neopronouns find, as the largest model that Hossain et al. (2023  ###reference_b23###) experiment with, for instance, is OPT-6.7B.\nThe lower performance of bigger models with singular they could also be a reflection of human processing difficulties with definite, specific (singular) they, as has been observed in linguistics (Conrod, 2019  ###reference_b12###).\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Adding Distractors",
            "text": "To further probe whether models actually “reason” when provided with context, we systematically inject sentences containing distractor pronouns between the introduction and the task, reflecting a natural usage scenario where multiple people are discussed with definite references and pronouns.\nExample: The accountant had just eaten a big meal so her stomach was full. The taxpayer needed coffee because their day had started very early. Their sleep had been fitful. The accountant was asked about ___ charges for preparing tax returns.\nCorrect answer: her\nFigure 5  ###reference_### shows that distractors degrade performance for all models.\nEncoder-only and decoder-only models show different performance curves as more distractors are added:\nall decoder-only models get steadily worse, whereas encoder-only models perform the worst with one distractor and then seem to slowly recover, never quite reaching their level of performance with no distractors. Scaling generally holds within model families, with larger models performing better with more distractors than smaller models of the same type. Figure 6  ###reference_### examines the interplay of scaling and architecture at a higher level, comparing results on the easiest case of pronoun fidelity (no distractors) with the hardest case (5 distractors). Surprisingly, with no distractors, encoder-only models are much better than decoder-only models of the same scale, and their performance is comparable to or better than decoder-only models that are orders of magnitude larger; RoBERTa-base (125M) is 0.86 accurate compared to OPT-125M’s 0.55, and exceeds OPT-66B’s 0.83 despite being more than 500 times smaller. In the hardest version of our task with five distractors, encoder-only models are far better than all decoder-only models, which show dramatically degraded performance; Llama-70B only achieves 0.37 accuracy, compared to MosaicBERT’s impressive 0.87.\nThe lack of robustness of decoder-only models to distractors is striking, given that most state-of-the-art models today are decoder-only models.\nWe hypothesize that architectural differences might explain the performance gaps; encoder-only models might use bidirectional attention to more closely relate the entity mentions in the introduction and task sentences.\nTraining on next token prediction might also make decoder-only models prone to recency bias.\nUsing vanilla and chain-of-thought prompting (Appendix I  ###reference_###) show the same patterns of degradation, reinforcing that model pronoun fidelity is not robust, and good performance with no distractors (§6  ###reference_###) is likely not due to “reasoning” at all.\n###figure_16### ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23###"
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Discussion and Future Work",
            "text": "Our results show that even the biggest models of today are not up to the task of pronoun fidelity once it includes a single sentence discussing another person.\nAll models are easily distracted, but encoder-only models and decoder-only models show very different patterns both in performance degradation with more distractors and their reasons for errors.\nPerformance on this type of reasoning task should be evaluated carefully, with attention to how the overall patterns break down by different pronouns and accounting for the possibility of repetition.\nBelow we expand on some questions raised by our findings.\nImproving robust pronoun fidelity. A natural direction of future work is to solve the problem of robust pronoun fidelity.\nWe urge researchers interested in this direction to treat RUFF as an evaluation dataset, as it was designed.\nDue to the presence of positional and associative heuristics that we expand on in Appendix A  ###reference_###, RUFF should not be seen as a source of data for fine-tuning or in-context learning, which is also why we do not run these experiments.\nInstead, a promising direction might be to encourage models to explicitly track associations between pronoun sets and individuals, just as people do.\nOn “reasoning.” Throughout the paper, we refer to “reasoning,” but this is inaccurate. What looks like “reasoning” when we inject an introductory context starts to look much more like repetition—or stochastic parroting (Bender et al., 2021  ###reference_b3###)—when we add a distractor and see the same models performing drastically worse.\nEven the higher performance of encoder-only models cannot accurately be attributed to “reasoning” in the same way that we use this word for humans, as these models are not grounded in meaning from the real world (Bender and Koller, 2020  ###reference_b4###).\nWe use the word reasoning in line with other work in the field, but note that as these are all language models, it is more accurate to say that the way that decoder-only models model language is prone to repetition of recent examples of the same word class, compared to encoder-only models.\nWhy exactly do we see the patterns we see? Our dataset design and error analysis shed light on model behaviour, allowing us to evaluate different architectures comparably and disentangle the effects of repetition, distraction and statistical bias.\nHowever, it is beyond the scope of this paper to investigate where in the model architecture, neurons or pre-training data this comes from and what we can do about it towards improving reasoning and mitigating bias. Tools from interpretability literature, e.g., attribution analysis, could help here, and are an important direction for future work.\nBeyond our dataset. Given the breadth of our task definition, future work could include examining pronoun fidelity for participants, for names by extending Hossain et al. (2023  ###reference_b23###), with differently ordered sentences, and in more natural settings such as stories (Mostafazadeh et al., 2016  ###reference_b43###) or with real-world data as in Webster et al. (2018  ###reference_b64###) and Levy et al. (2021  ###reference_b36###). Additionally, we evaluate on a version of this task that allows us to quantify repetition, i.e., the grammatical case of the elicited pronoun is the same as the case shown in the context. Examining model performance where a pronoun is shown in one grammatical case and then elicited in a different one would be interesting to probe syntactic generalization."
        }
    ]
}