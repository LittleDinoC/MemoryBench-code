{
    "title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
    "abstract": "Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages.\nWe address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages.\nWe overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents.\nUsing PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact.\nOur findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) are increasingly being deployed in global contexts (Pichai & Hassabis, 2023  ###reference_b54###; Forbes, 2024  ###reference_b23###). Naturally, this has led to rapid advances in the multilingual capabilities of LLMs (Scao et al., 2022  ###reference_b60###; Üstün et al., 2024  ###reference_b77###; Yuan et al., 2023  ###reference_b85###). However, current toxicity evaluation benchmarks and safety alignment methods (Christiano et al., 2017  ###reference_b10###; Lee et al., 2024  ###reference_b41###) overwhelmingly focus on the English language, leading to significantly less safe responses in non-English languages (Wang et al., 2023  ###reference_b78###; Kotha et al., 2024  ###reference_b38###; Yong et al., 2023  ###reference_b82###). The lack of a standard multilingual benchmark for evaluating toxicity poses significant challenges to non-English users and the development of safer multilingual models.\nWe introduce PolygloToxicityPrompts (PTP),111We provide our dataset, code, and a leaderboard: https://huggingface.co/spaces/ToxicityPrompts/PTP  ###reference_pts/PTP### the first large-scale multilingual benchmark for evaluating neural toxic degeneration, defined as the propensity of LLMs to generate toxic text given a prompt (Gehman et al., 2020  ###reference_b26###). We create PTP by scraping over 100M documents from web-text corpora to collect naturally occurring toxic prompts. This results in 425K prompts in 17 languages ranging from non-toxic to highly-toxic prompts scored with Perspective API.222https://perspectiveapi.com/  ###reference_perspectiveapi.com/###\nPolygloToxicityPrompts provides three key improvements for multilingual toxicity evaluation, surfacing more toxic generations from LLMs than existing toxicity benchmarks (Figure 1  ###reference_###).\nFirst, PTP covers 17 languages while existing toxic degeneration work predominantly focuses on English (Gehman et al., 2020  ###reference_b26###; Lin et al., 2023a  ###reference_b43###).\nSecond, existing multilingual toxicity evaluation testbeds such as Üstün et al. (2024  ###reference_b77###) and RTP-LX (de Wynter et al., 2024  ###reference_b14###) are translations of RealToxicityPrompts (RTP; Gehman et al., 2020  ###reference_b26###), which can lack cultural nuances of toxicity and introduce deviations in toxicity, leading to under-estimated toxic degeneration (Sharou & Specia, 2022  ###reference_b62###; Costa-jussà et al., 2023  ###reference_b11###).\nThird, PTP’s naturally occurring prompts are more representative of real-world inputs than recent works on jailbreaking (Deng et al., 2023  ###reference_b15###; Wei et al., 2024  ###reference_b79###) and adversarial prompt generation (Zou et al., 2023  ###reference_b91###; Huang et al., 2023  ###reference_b30###), which lead to unnatural and often gibberish prompts.\n###figure_1### We evaluate 62 LLMs on PolygloToxicityPrompts to study the impact of prompt language, model size, alignment methods, and input prompt toxicity on toxicity.\nWe find significant toxicity in multilingual models, especially as the availability of language resources decreases. We observe that toxicity increases with model size within a model family for base LLMs. Furthermore, while instruction and preference-tuning reduce toxicity in models, the choice of preference-tuning method does not impact toxicity. Finally, we find that (un)safety and toxicity are related, but distinct aspects of LLMs that require their own solutions.\nOverall, our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research, notably, the need for multilingual toxicity mitigation and further investigations into the impact of model hyperparameters on toxicity. Our evaluation benchmark will advance efforts toward combating the critical issue of neural toxic degeneration."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works on evaluation datasets for studying biases and toxicity in models were created using templates or scraping web-text corpora. Sheng et al. (2019  ###reference_b63###); Nangia et al. (2020  ###reference_b50###); Nadeem et al. (2021  ###reference_b49###) use templated prompts to study social biases in pretrained language models. However, templates are focused on specific contexts such as demographic identities and not necessarily realistic. Thus, Gehman et al. (2020  ###reference_b26###) create RealToxicityPrompts by crawling English web-text for naturally occurring input prompts to evaluate toxicity in a sentence completion setting.\nMore recently, there has been a shift towards examining toxicity in input-response settings. Si et al. (2022  ###reference_b65###); Baheti et al. (2021  ###reference_b4###) use generations from dialogue models like DialoGPT (Zhang et al., 2020  ###reference_b87###) to study toxic degenerations in chatbots. Furthermore, the advent of instruction-tuned LLMs has led to studies of toxicity in real-world user-AI conversations. Zheng et al. (2024  ###reference_b89###) and Lin et al. (2023a  ###reference_b43###) collect user-AI interactions with automatic and manual toxicity annotations respectively to tackle a different toxic data distribution—namely instructions. However, most of these approaches are limited to English.\nMultilingual dataset curation for evaluating toxicity has utilized both manual and automated translation techniques. Recent work on AI safety evaluation (Wang et al., 2023  ###reference_b78###; Yong et al., 2023  ###reference_b82###; Deng et al., 2023  ###reference_b15###) create multilingual safety benchmarks by translating monolingual benchmarks into other languages. They observe that LLMs are primarily safeguarded for English, leading to significantly unsafe generations in other languages, especially as availability of languages decreases. While these works are aimed towards the broader area of safety, the absence of a standard multilingual toxicity evaluation benchmark has also led researchers to translate prompts from RealToxicityPrompts into other languages, either automatically (Üstün et al., 2024  ###reference_b77###) or using human annotations (de Wynter et al., 2024  ###reference_b14###). However, manual translations are expensive, not scalable, and can introduce cultural biases, whereas automated translations can introduce deviations in toxicity due to incorrect translations and hallucinations (Specia et al., 2021  ###reference_b67###; Sharou & Specia, 2022  ###reference_b62###; Team et al., 2022  ###reference_b72###; Costa-jussà et al., 2023  ###reference_b11###).\nBesides human-generated or naturally occurring data, a wealth of recent work has explored using machine-generated approaches to curate datasets and methods for evaluating the toxicity and safety of LLMs. Hartvigsen et al. (2022  ###reference_b28###) and Kim et al. (2022  ###reference_b37###) generate adversarial prompts about minority groups using classifier-guided decoding and conversations with a toxic partner respectively. Extensive research has studied red teaming (Perez et al., 2022  ###reference_b53###; Chao et al., 2023  ###reference_b9###; Mazeika et al., 2024  ###reference_b47###) and jailbreaking (Liu et al., 2023  ###reference_b46###; Wei et al., 2024  ###reference_b79###; Yu et al., 2023  ###reference_b84###; Deng et al., 2023  ###reference_b15###) to identify safety failures in LLMs and elicit harmful outputs. Furthermore, adversarial attack methods have also been shown to be effective against models without requiring substantial prompt engineering (Shin et al., 2020  ###reference_b64###; Zou et al., 2023  ###reference_b91###; Huang et al., 2023  ###reference_b30###; Jones et al., 2023  ###reference_b35###). However, such methods involve extensive prompt engineering, often leading to unnatural and non-representative prompts or model-specific artifacts (Das et al., 2024  ###reference_b13###). Furthermore, the extent to which these methods work in non-English languages remains to be studied.\nWhile the literature on toxicity evaluation has grown rapidly, their predominant focus on English highlights the need for multilingual benchmarks on naturally occurring toxic input prompts. We address this gap with PolygloToxicityPrompts, a collection of 425K naturally occurring prompts across 17 languages for evaluating toxicity."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "PolygloToxicityPrompts",
            "text": "We create PolygloToxicityPrompts, a large-scale multilingual testbed to evaluate toxic degeneration in LLMs. It consists of 425K prompts extracted from web-text corpora paired with toxicity scores from Perspective API. All 17 languages supported by Perspective API are represented in our testbed, namely: Arabic (ar), Chinese (zh), Czech (cs), Dutch (nl), English (en), French (fr), German (de), Hindi (hi), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv).\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###; Dodge et al., 2021  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_2### Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###); Üstün et al. (2024  ###reference_b77###) to measure a model’s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model’s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model’s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure’s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###) for decoder-only models, and Huggingface’s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Operationalizing and Evaluating Toxicity",
            "text": "We define toxicity as “a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion” (Wulczyn et al., 2017  ###reference_b80###; Borkan et al., 2019  ###reference_b8###). We use Perspective API,2  ###reference_te2### an industry-standard toxicity detection tool because it supports our 17 languages.\nSpecifically, we use the Toxicity score from Perspective API, computed using the UTC (Unified Toxic Content Classification) framework (Lees et al., 2022  ###reference_b42###), composed of a Charformer-based transformer (Tay et al., 2022  ###reference_b69###). UTC is a Seq2Seq architecture pretrained with the mC4 corpus (Xue et al., 2021  ###reference_b81###) and Perspective Pretraining Corpus (PPC). Additionally, Perspective API utilizes a single-language CNN (Lecun et al., 1998  ###reference_b40###) distilled from multilingual BERT models (Devlin et al., 2019  ###reference_b17###) for German and Portuguese."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Creation",
            "text": "We construct our dataset by scraping over 100M documents from the mC4 (Xue et al., 2021  ###reference_b81###) and The Pile (Gao et al., 2020  ###reference_b25###) corpora as they contain multilingual texts from a variety of domains. We also leverage Pile Curse,333https://huggingface.co/datasets/tomekkorbak/pile-curse-full  ###reference_k/pile-curse-full### a subset of The Pile scored using the bad words 444https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words  ###reference_aughty-Obscene-and-Otherwise-Bad-Words### list for our English split. We then extract Toxicity scores with Perspective API for all scraped documents. To obtain a stratified range of prompt toxicity, we sample 6250 documents from 4 equal-width toxicity levels (). We then split collected documents in half to form prompts and continuations, both of which are scored for toxicity. We provide preprocessing details, dataset statistics, and metadata analysis in Appendix A  ###reference_###.\nThe final dataset includes 25K naturally occurring prompts for each language, for a total of 425K prompts across 17 languages. Figures 10(a)  ###reference_.sf1### and 10(b)  ###reference_.sf2### show the prompt toxicity and length distributions of our collected prompts for all languages. We create our prompts using documents instead of sentences (Gehman et al., 2020  ###reference_b26###). Thus, our prompts are much longer than RealToxicityPrompts, with an average length of approximately 400 GPT-4 tokens.\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4###  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###  ###reference_b90###; Dodge et al., 2021  ###reference_b19###  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_3###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Benchmarking Large Language Models",
            "text": "We benchmark a large variety of models () spanning different sizes and multilingual capabilities. We follow the taxonomy proposed by Albalak et al. (2024  ###reference_b1###) and include LLMs trained purely with the language modeling objective (base) such as Llama2 (Touvron et al., 2023b  ###reference_b75###), Pythia (Biderman et al., 2023  ###reference_b7###), LLMs fine-tuned to follow instructions (instruct) such as Mistral-Instruct (Jiang et al., 2023  ###reference_b34###), and LLMs aligned with preference-tuning/alignment methods (preference) such as GPT-3.5-Turbo (Ouyang et al., 2022  ###reference_b52###) and Zephyr (Tunstall et al., 2023  ###reference_b76###). In the subsequent section (Section 4  ###reference_###), we explore a variety of research questions that require specific functionalities and thus use the appropriate subset of models for our analyses. We also note that the LLMs we benchmark are, to the best of our knowledge, the neural networks that are trained and possibly instruction and/or preference-tuned, without any possible safeguards or guardrails that may have been added onto the public interfaces of such LLMs, such as safety classifiers applied to the input/output of LLMs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Benchmarking Setup",
            "text": "Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###  ###reference_b26###); Üstün et al. (2024  ###reference_b77###  ###reference_b77###) to measure a model’s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model’s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model’s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure’s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###  ###reference_b39###) for decoder-only models, and Huggingface’s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference###  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Research Questions",
            "text": "To investigate multilingual toxic degeneration in a large suite of models, we obtain and score continuations for the 5K prompts per language contained in  (due to computational resource limitations). We find similar trends across all evaluation metrics and thus report only Average Toxicity for brevity.\nTable 1  ###reference_### previews our findings for the models with the lowest and highest Average Toxicity. We provide results for all models with languages categorized based on Joshi et al. (2020  ###reference_b36###)777Since all considered languages belong to categories 3 and above, we compare relative resource availability, that is, categories 3, 4 and 5 are referred as low-, medium- and high-resource respectively. in Table LABEL:tab:secondary_results.\nNext, we explore specific patterns concerning prompt language, model size, alignment methods, and prompt toxicity below. Finally, we also compare toxicity and safety detectors using Perspective API and Llama Guard Inan et al. (2023  ###reference_b32###) respectively.\nWe investigate the distribution of continuation toxicity for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation toxicity for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model toxicity on size. For instance, Touvron et al. (2023a  ###reference_b74###; b  ###reference_b75###) find that toxicity increases with model size, whereas\nGehman et al. (2020  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###) find that larger models are not necessarily more toxic. We hypothesize that toxicity might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on toxicity using the English split of our dataset. Figure 4  ###reference_### shows an overall increase in toxicity with an increase in model size, which plateaus near  parameters (effect size of the difference between  and  is small, Cohen’s , ).\n###figure_4### ###figure_5### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###; b  ###reference_b75###). More specifically, we find that the toxicity levels in  Pythia models are comparatively higher than the smallest  model (Cohen’s , ).\nThis implies that toxicity is a long-tail phenomenon that large enough models ( parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###).\nTo investigate the impact of model size on toxicity for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2’s pretraining data) as shown in Figure 6  ###reference_###.\nWe observe different trends in both model families when scaling from  to  — for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces toxicity for Tulu 2 models as they are scaled to  parameters. However, such differences are small (Cohen’s  for all combinations with  models).\nThere seems to be no conclusive answer as to whether model size affects toxicity in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on toxic degeneration in safety-aligned models.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen’s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen’s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen’s ) (Figure 7  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_6### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs’ definition of toxic content, which likely aligns better with Perspective API’s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###).\nWe examine the extent to which different model categories mirror input toxicity.\nWe find that the continuation toxicity of base models is most strongly correlated with input toxicity (, ).\nSurprisingly, preference models have a higher correlation between input and continuation toxicity (, ), compared to instruct models (, ).\nWe find that this is due in large to low-toxicity prompts, for which preference models mimic the input (low) toxicity in continuations better () than for high-toxicity prompts ().\ninstruct models also show a stronger correlation between prompt and continuation toxicity for low-toxicity prompts () than for high-toxicity ones (). This indicates that preference models better match input toxicity than instruct models, but predominantly in low-toxicity inputs, suggesting that preference models are better safeguarded against high-toxicity inputs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "How does Model Size impact Average Toxicity?",
            "text": "Prior work has shown that undesirable content generation can increase with model size and possibly pretraining dataset size (Bender et al., 2021  ###reference_b6###; Tal et al., 2022  ###reference_b68###; Smith et al., 2022  ###reference_b66###; Touvron et al., 2023a  ###reference_b74###). We conduct a similar investigation on the impact of model size on toxicity. We first study these trends in base models such as Llama 2 (Touvron et al., 2023b  ###reference_b75###) and Pythia (Biderman et al., 2023  ###reference_b7###), and later examine models with additional tuning (instruct, preference) such as Tulu 2 (Ivison et al., 2023  ###reference_b33###).\nWe investigate the distribution of continuation toxicity for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation toxicity for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model toxicity on size. For instance, Touvron et al. (2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###) find that toxicity increases with model size, whereas\nGehman et al. (2020  ###reference_b26###  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###  ###reference_b29###) find that larger models are not necessarily more toxic. We hypothesize that toxicity might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on toxicity using the English split of our dataset. Figure 4  ###reference_###  ###reference_### shows an overall increase in toxicity with an increase in model size, which plateaus near  parameters (effect size of the difference between  and  is small, Cohen’s , ).\n###figure_9### ###figure_10### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###). More specifically, we find that the toxicity levels in  Pythia models are comparatively higher than the smallest  model (Cohen’s , ).\nThis implies that toxicity is a long-tail phenomenon that large enough models ( parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###  ###reference_b73###).\nTo investigate the impact of model size on toxicity for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2’s pretraining data) as shown in Figure 6  ###reference_###  ###reference_###.\nWe observe different trends in both model families when scaling from  to  — for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces toxicity for Tulu 2 models as they are scaled to  parameters. However, such differences are small (Cohen’s  for all combinations with  models).\nThere seems to be no conclusive answer as to whether model size affects toxicity in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on toxic degeneration in safety-aligned models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "How do Alignment Methods impact Average Toxicity?",
            "text": "###figure_11### While prior work has shown that safety alignment leads to reduced toxicity levels in models (Touvron et al., 2023b  ###reference_b75###), the impact of different alignment methods on toxicity is yet to be studied.\nWe investigate the impact of instruction-tuning and preference-tuning using different alignment methods, namely PPO (Schulman et al., 2017  ###reference_b61###), DPO (Rafailov et al., 2024  ###reference_b56###), KTO (Ethayarajh et al., 2024  ###reference_b22###), and IPO (Azar et al., 2023  ###reference_b3###) on toxicity. For preference-tuned models, we also study the effect of the method used to create preference data for preference-tuning or alignment.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen’s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen’s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430###  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46###  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen’s ) (Figure 7  ###reference_###  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_12### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1###  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs’ definition of toxic content, which likely aligns better with Perspective API’s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###  ###reference_b13###)."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparing Toxicity and Safety Detectors: Perspective API vs. Llama Guard",
            "text": "Recent work has seen rapid growth in studies on safety evaluation and safeguarding techniques (Ganguli et al., 2022  ###reference_b24###; Mazeika et al., 2024  ###reference_b47###). For instance, Inan et al. (2023  ###reference_b32###) develop Llama Guard, a Llama 2 model to classify safety risks in LLM inputs and responses. However, the extent to which toxicity and safety overlap is unclear. To fill this gap, we compare Perspective API, a toxicity detector, and Llama Guard, a safety detector.\nSince Llama Guard only supports English, we compute scores for all models on the English split of  following the instructions in its model card.111111https://huggingface.co/meta-llama/LlamaGuard-7b  ###reference_rd-7b### We find that Perspective API toxicity scores are generally well-aligned with Llama Guard scores ().\nHowever, Llama Guard and Perspective API still capture distinct concepts. To analyze the differences between both evaluation methods, we examine the prompts and generations where the metrics differ the most (Table 5  ###reference_### in Appendix E  ###reference_###). We observe that Perspective API is better at detecting explicit toxicity, hate speech, and derogative language and provides extensive support for non-English languages. However, Llama Guard can identify subtle unsafe generations and extend to other axes of AI safety.\nOur findings suggest that LLM safety detectors may not be equipped to capture the full spectrum of toxicity."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "How does Prompt Toxicity impact Continuation Toxicity?",
            "text": "We investigate the relationship between input prompt toxicity and continuation toxicity at greater granularity, that is, without aggregating as in Average Toxicity. Intuitively, we expect a model’s propensity to generate toxic text to be proportional to the toxicity of the input prompt. Empirically, we find a Pearson correlation of  () between prompt toxicity and continuation toxicity.\nWe also find that continuation toxicity spans the entire toxicity range, regardless of input toxicity score, indicating that non-toxic prompts can yield toxic continuations and vice-versa, corroborating Gehman et al. (2020  ###reference_b26###). Furthermore, we investigate the correlations between prompt and continuation toxicity across languages and model families in Appendix B  ###reference_###.\nWe examine the extent to which different model categories mirror input toxicity.\nWe find that the continuation toxicity of base models is most strongly correlated with input toxicity (, ).\nSurprisingly, preference models have a higher correlation between input and continuation toxicity (, ), compared to instruct models (, ).\nWe find that this is due in large to low-toxicity prompts, for which preference models mimic the input (low) toxicity in continuations better () than for high-toxicity prompts ().\ninstruct models also show a stronger correlation between prompt and continuation toxicity for low-toxicity prompts () than for high-toxicity ones (). This indicates that preference models better match input toxicity than instruct models, but predominantly in low-toxicity inputs, suggesting that preference models are better safeguarded against high-toxicity inputs."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "How do different Data Sources elicit Average Toxicity?",
            "text": "Finally, we study the ability of different data sources to elicit toxicity from LLMs. Specifically, we compare Average Toxicity when generating continuations for naturally occurring prompts from PTP, RTP-LX (de Wynter et al., 2024  ###reference_b14###), and an automatically translated sample of user-LLM interactions from WildChat (Zhao et al., 2024  ###reference_b88###).121212We provide details about RTP-LX and WildChat in Appendix C  ###reference_###.\n###figure_13### Figure 9  ###reference_### shows that PTP consistently draws out higher Average Toxicity. While RTP-LX is comprised of naturally occurring prompts in English and their culturally-aware translations to other languages, we find that PTP is still able to capture more toxicity, likely due to longer prompt lengths, corroborating Anil et al. (2024  ###reference_b2###). Furthermore, we hypothesize that preference-tuning makes models less vulnerable to what users input into LLMs as opposed to naturally occurring toxicity, leading to higher toxicity levels elicited by PTP compared to WildChat."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present PolygloToxicityPrompts, the first large-scale multilingual benchmark of 425K naturally occurring prompts across 17 languages for evaluating toxic degenerations in LLMs. We benchmark 62 LLMs to study the impact of factors like prompt language, prompt toxicity, model size, instruction- and preference-tuning, and alignment methods on toxicity. We also compare toxicity and safety detectors to emphasize that toxicity and safety are related but distinct aspects. Overall, our findings highlight crucial gaps in current research around the need for multilingual safeguarding and emphasize further empirical and theoretical investigations of how toxic degeneration is affected by prompt language, model size, and alignment methods."
        }
    ]
}