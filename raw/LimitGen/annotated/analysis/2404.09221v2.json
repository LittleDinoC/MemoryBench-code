{
    "title": "\\scalerel* Exploring and Improving Drafts in Blockwise Parallel Decoding",
    "abstract": "Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation.\nBlockwise parallel decoding (BPD) was proposed by Stern et al. [38] as a method to improve inference speed of language models by simultaneously predicting multiple future tokens, termed block drafts, which are subsequently verified and conditionally accepted by the autoregressive model.\nThis paper contributes to the understanding and improvement of block drafts in two ways. First, we analyze the token distributions produced by multiple prediction heads. Secondly, we leverage this analysis to develop algorithms to improve BPD inference speed by refining the block drafts using n-gram and neural language models.\nExperiments demonstrate that refined block drafts yield a +5-21% increase in block efficiency (i.e., the number of accepted tokens from the block draft) across diverse datasets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The landscape of natural language processing has been profoundly reshaped by recent advances in autoregressive language models [3  ###reference_b3###, 43  ###reference_b43###, 30  ###reference_b30###, 33  ###reference_b33###, 42  ###reference_b42###]. These models have shown remarkable proficiency across a range of text generation tasks, including applications like question answering [34  ###reference_b34###] and summarization [15  ###reference_b15###]. However, a significant obstacle to their wider application is high inference latency, particularly for extremely deep models with hundreds of billions of parameters [16  ###reference_b16###, 31  ###reference_b31###, 7  ###reference_b7###]. This latency, intrinsic to decoding with autoregressive language models (LMs), imposes considerable computational burdens and limits real-time deployment.\nIn response to these latency challenges, the field has seen a shift towards decoding methods aimed at reducing the inference latency in large language models (LLM). One promising development is the concept of blockwise parallel decoding (BPD) [38  ###reference_b38###, 27  ###reference_b27###, 4  ###reference_b4###]. Unlike autoregressive decoding, which generates one token at a time, blockwise parallel LMs are outfitted with a set of prediction heads that propose and verify a draft, a block of subsequent tokens, in parallel. While BPD offers one solution to accelerated text generation, it also poses a challenge in ensuring that the proposed drafts are fluent and natural.\n###figure_1### ###figure_2### BPD inference speed depends on both the time it takes to produce a block draft and the draft’s agreement with the base LM’s output (1(a)  ###reference_sf1###). Unlike standard autoregressive LMs that generate tokens sequentially—ensuring consistency with all preceding tokens (e.g., ‘Messi’ following ‘Lionel’)—BPD employs a parallel strategy. Here, blockwise parallel LMs simultaneously predict multiple token drafts (e.g., ‘Lionel’ and ‘Ronaldo’), each independently. The primary challenge in BPD is ensuring that these concurrently generated tokens maintain consistency. Effective block drafters should prioritize coherent sequences such as ‘Lionel Messi’ over less likely combinations like ‘Lionel Ronaldo’, which a robust LM would not decode. The focus of this research is improving the quality of block drafts without altering the underlying model parameters."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Our contributions",
            "text": "In this paper, we first investigate properties made by the prediction heads of blockwise parallel LMs across several tasks; given these observations, we propose rescoring algorithms to produce higher quality block drafts."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Observations on block drafts",
            "text": "Consecutive repetitions  All heads within a block make predictions independently in a blockwise parallel LM. Unsurprisingly, we observe that this leads to block drafts with significant token repetition across heads. Consecutive repetition is pervasive across tasks, ranging from 20% to 75% of all neighboring draft tokens, depending on the task (subsection 6.1  ###reference_###).\nConfidence of different heads  We analyze the distribution of probabilities within each softmax head. Our empirical analysis reveals a key property of BPD: the block drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens. We find that the confidence of block heads correlates strongly with the quality of the block drafter (subsection 6.2  ###reference_###).\nOracle top- block efficiency  In the standard BPD algorithm (Algorithm 1  ###reference_###), the most likely token at each head is generated as the draft. As highlighted before, this approach is prone to two issues: (1) there might be consecutive repetitions and (2) the model might not be confident about the prediction at some of the heads.\nWe use block efficiency, the average number of draft tokens accepted during decoding, to measure the quality of a given drafter [24  ###reference_b24###, 41  ###reference_b41###].\nWe ask if the block efficiency can be improved by considering the top- most likely tokens at each head. To measure the potential benefit of considering top- tokens, we measure the block efficiency of the oracle path through this top- lattice, oracle top- block efficiency, and show that there is significant headroom for improvement across tasks (subsection 6.3  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "New algorithms",
            "text": "Based on these observations, we propose two algorithms to leverage the top- predictions at each head and improve BPD latency (1(b)  ###reference_sf2###). Neither of these algorithms require changes to the underlying blockwise parallel LMs.\nLocal rescoring via neural LMs  Given the top- predictions at each head, we refine the block draft by using a small neural, autoregressive LM to greedily rescore these local predictions (subsection 7.1  ###reference_###).\nWhile the block prediction scores are produced independent of each other, neural rescoring should favor sequences that are fluent, encouraging coherence between the predictions at each head.\nGlobal rescoring via n-gram LMs with multi-drafts  \nIf the blockwise parallel LM has  heads and we consider the top- tokens from each head, then there are  candidate drafts of length  that can be formed. We propose to use an n-gram model to efficiently rescore all paths, via dynamic programming, and generate the  most probable rescored paths as a batch of draft candidates. These  drafts can then be verified in parallel by the blockwise parallel LM (subsection 7.2  ###reference_###).\n###figure_3### There are two critical distinctions between the proposed algorithms: the amount of context/expressive power available to each class of rescoring model, and fundamental limitations of decoding with each class. While neural rescoring models are potentially more expressive and can leverage unbounded context, n-gram  LMs can be used to efficiently find the globally most likely rescored drafts from the exponentially-sized set of possible draft candidates.\nFigure 2  ###reference_### shows that our proposed methods enhance block efficiency, with one approach increasing it by up to +21.30%. This same method also optimizes resource usage, reducing key-value (KV) cache I/O by -2.54% and additionally using FLOPs per token by +4.04%.\nDescription of each algorithm is given in subsection 7.1  ###reference_###."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Organization",
            "text": "The remainder of this paper organized as follows. In section 3  ###reference_###, we discuss previous literature in reducing LLM latency. In section 4  ###reference_###, we define foundational concepts and terminology. section 5  ###reference_### describes our experimental setup, datasets, on methods. section 6  ###reference_### describes our analysis of the block drafts. In subsection 7.1  ###reference_###, we present the proposed BPD rescoring algorithms and empirical results, followed by a final discussion in section 8  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Efficient transformer inference",
            "text": "Works on improving transformer efficiency encompass both optimization of an existing set of model weights, or a fundamental change to the model architecture. Examples of the former include techniques such as quantization [44  ###reference_b44###, 45  ###reference_b45###, 9  ###reference_b9###] and model pruning [40  ###reference_b40###, 26  ###reference_b26###]. In parallel, neural architecture search has played a crucial role in identifying network structures that balance performance with efficiency [22  ###reference_b22###, 46  ###reference_b46###]. Relatedly, Elbayad et al. [10  ###reference_b10###] propose early-exiting at intermediate layers for faster inference, while Schuster et al. [35  ###reference_b35###] explore confidence thresholding for balancing speed and accuracy. These methods offer insights into optimizing decoding under resource constraints.\nOne important line of work has focused on modifying the decoding method in LMs. The adoption of non-autoregressive (parallel) decoding strategies [38  ###reference_b38###, 14  ###reference_b14###] marks a pivotal shift in this domain, addressing inference latency by simultaneously generating multiple tokens. Subsequent innovations have sought to refine this approach by incorporating additional context [6  ###reference_b6###], iterative refinement [20  ###reference_b20###], and tree-based attention mechanism [4  ###reference_b4###]. However, these refinements often require complex training or additional inference data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Efficient and effective decoding",
            "text": "There are several recent works that improve the speed of LLM decoding, including pioneering works like BPD and speculative decoding. Speculative decoding leverages a smaller ‘draft’ model to anticipate the outputs of a larger target model, improving average decode latency without loss in generation quality [24  ###reference_b24###, 5  ###reference_b5###, 20  ###reference_b20###]. The draft model is typically trained on the same corpus as the LLM, thus autoregressively generates similar drafts as the target model with reduced latency. Speculative decoding is most successful when a long sequence of speculated tokens are accepted by the target LM during verification, avoiding multiple serial calls to the target LM to generate the same sequence.\nOn the surface, contrastive decoding algorithms share some similarities with our proposed draft rescoring approach, insofar as a weaker model is used to modify the predictions of the target LM [25  ###reference_b25###, 21  ###reference_b21###]. However, in this work, we refine block drafts solely to improve latency. Like speculative decoding, our proposals have no effect on the quality of the target LM’s generated text."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "This section introduces notation and concepts, including algorithms for standard autoregressive decoding and BPD.\nAutoregressive decoding  Let  be an autoregressive LM parameterized by . The objective is to generate an output sequence  conditioned on an input sequence .\n is a vector of logits, , where  is the vocabulary over tokens. These logits define a conditional probability distribution at each time step , which by the chain rule yields .\nSequences are generated autoregressively, either through ancestral sampling from some form of the conditional next token distribution [17  ###reference_b17###], or by a beam search through the space of possible sequences to return a probable sequence. Greedy decoding, a special case of beam search, generates each token as . In this work, we consider greedy decoding exclusively, as this is the setting that Stern et al. [38  ###reference_b38###] was designed to accelerate.\nBlockwise parallel decoding  Let  be a blockwise parallel LM with block size . This model employs  distinct feedforward neural (FFN) layer with a single hidden layer, atop the target LM’s final hidden layer. The output of each FFN is followed by a softmax layer over the vocabulary to predict each of the  subsequent tokens in the block. In our experiments, the parameters of the FFNs are learned jointly with the base LM during training, and the weights of all softmax layers are tied to the input embedding table. Algorithm 1  ###reference_### describes the BPD greedy decoding procedure:\nPredict:  is used to generate a draft of  token predictions , conditioned on the prompt, , and existing generated text, .  is identical to the target LM greedy decode.\nVerify: At this stage, the target LM greedily generates next-token logits  conditioned on the existing prefix and block draft . Verification amounts to checking which block draft tokens match the autoregressive greedy decode from the target LM: (. Verification of all positions can be performed in parallel under the assumption that the target LM is a decoder-only transformer.\nAccept: Finally, the length of the longest contiguous prefix of draft tokens that match the target LM greedy decode is identified: . The decoded sequence is extended by  tokens and we iterate.111The decoded sequence is extended by  tokens since during verification we generate the token from the target LM, , at the first position where the draft differs from the target LM greedy decode. Note that in general, not all  tokens are accepted, and many of the draft tokens in each block are discarded. Since the additional time required to generate a block of tokens is fast relative to the time it takes for the forward pass of the target LM, a modest gain in accepted prefix length justifies the cost of draft generation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental setup",
            "text": "In this paper, we use  billion (B) parameter decoder-only transformer LMs with up to 9 blockwise heads.222This study is based on the original BPD framework, with a modification: we use decoder-only models instead of the T5 encoder-decoder architecture. Other setups are consistent with the approach in Stern et al. [38  ###reference_b38###].\nThe 1.5B model and all other LMs were pretrained on (English) C4 [32  ###reference_b32###] with the causal next token prediction objective tokenized with the GPT3 subword vocabulary [3  ###reference_b3###]. For the 1.5B blockwise parallel LMs, all heads were trained jointly to predict the following  tokens at each iteration. During pretraining, we use batches of 2048 subword sequences, each 512 tokens in length, amounting to B input tokens in total on TPUv3/TPUv4 [18  ###reference_b18###] with Jax [2  ###reference_b2###].\nWe evaluate the potential latency improvement of block drafts by block efficiency [24  ###reference_b24###, 41  ###reference_b41###]. In this context, block efficiency represents the theoretical speedup compared to standard greedy decoding. It is defined as the average number of tokens decoded per serial call to the blockwise parallel LMs. The formula for block efficiency is given by .\nIn this definition, the total number of decoded tokens is the sum of the number of accepted tokens across decoding steps, not necessarily all  predicted tokens in each block. Only the tokens that pass the ‘Verify’ stage and align with the base model’s predictions are accepted and integrated into the final sequence. This ensures that generated text is identical to the target LM, while achieving speedup. The total number of serial calls to  is the number of times the model processes a block of tokens. A block efficiency of 1 means that one is achieving no speedup relative to standard decoding.\nIn addition to a standard language modeling dataset, LAMBADA [29  ###reference_b29###], we conduct experiments across several classes of downstream tasks. In the realm of text summarization, we evaluate models on the XSUM [28  ###reference_b28###], MultiNews [11  ###reference_b11###], SAMSum [12  ###reference_b12###], NewsRoom [13  ###reference_b13###] and CNN/DailyMail [15  ###reference_b15###] datasets. Each of these datasets is characterized by distinct summary lengths and styles. For extractive QA, the SQuAD V1 dataset [34  ###reference_b34###] serves as our testbed. For each task aside from language modeling, we finetune the blockwise parallel LM for that task.333Details are given in Appendix C  ###reference_###. Table 1  ###reference_###444The performance metric for LM is perplexity, for QA is exact match, and for the remaining summarization tasks, the metric is ROUGE-L. shows that block efficiency varies dramatically across tasks and as a function of the number of block draft heads. We use all 9 block draft heads for subsequent experiments as this acts as an upper bound on possible block efficiency.\nTable 2  ###reference_### sketches how BPD acts on three examples from each class of tasks.\nLM: BPD excels at generating common multi-word expressions in a single step. For example, (no) ‘thing more than’, and (take) ‘his word for the’ are each drafted and accepted in a single step.\nQA: BPD also attains high block efficiency in extractive QA, where it correctly drafts multi-token entities copied from the input sequence. In SQuAD V1, it accurately completes the answer ‘Grumman’ from ‘Gru’ by adding ‘mman’, highlighting its ability to process multiple tokens at once and quickly extend answers.\nSUM: BPD’s effectiveness in SUM tasks varies by dataset. For formulaic summaries like CNN/DailyMail, it performs well, reflecting its alignment with LM and QA tasks. However, in narrative-driven datasets like SAMSum and XSUM, where concise summaries are required, the block efficiency of BPD is little better than standard decoding."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Exploration of predictive dynamics in BPD",
            "text": ""
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Confidence across multiple heads",
            "text": "Intuitively, predicting the identity of the  future token becomes harder as  increases. To better understand this phenomenon, we measure the confidence of the predictions by the entropy of the probability distribution. In 3(a)  ###reference_sf1###, we plot the normalized histogram of entropy of each head on the LAMBADA task. From the normalized histogram, it is clear that the entropy increases as we move from first head to the last head, which agrees with our intuition that hardness of predictions increases as  increases.\nHowever, we observed that the entropy of heads does not increase monotonically for all tasks. Let  be the average entropy of head  on a particular corpus, and let , be the index of the largest head such that the average entropy of each head increases monotonically to that point. We observed a strong correlation between  and block efficiency (3(b)  ###reference_sf2###). Heads with lower entropy (indicating more confident predictions) intuitively contribute more to efficiency. Nonetheless, simply maximizing the number of low-entropy heads is not optimal, but rather incorporating progressively higher entropy heads, up to a certain point, can benefit decoding efficiency. A linear regression confirms this with an R-value of . This analysis suggests that BPD head entropy could be used as a proxy for block efficiency, and thus inference latency."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Oracle top-k block efficiency",
            "text": "###figure_6### Oracle efficiency  The concept of oracle block efficiency in BPD serves as a theoretical benchmark, illustrating the headroom available from improving the quality of the block draft. To compute oracle block efficiency, we consider the top- most probable tokens at each head, and form a “sausage” lattice from these. This data structure is a weighted directed graph, which succinctly represents all possible drafts (and their score under the BPD model) that could be formed from selecting one of  tokens from each of the  heads (Figure 4  ###reference_###). In the automatic speech recognition and machine translation communities, it is known as a “confusion network” [36  ###reference_b36###, 23  ###reference_b23###].\n###figure_7### Given the top- lattice at each decoding step, we identify an oracle path that represents the path through the lattice that maximizes the length of the accepted prefix. This exercise, as shown in Figure 5  ###reference_###, gives us insight into how much headroom exists in improving block drafts.\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### Potential headroom from oracle selection  Oracle drafting is not practical, but rather a reference point. Analyzing the gap between actual BPD performance and the oracle upper bound (Figure 6  ###reference_###) helps us to understand the limitations of the original block drafts and potential areas for improvement. Additionally, exploring oracle efficiency as a function of the  in the top- lattice, demonstrates how “close” the block draft was to producing a stronger draft."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Lattice rescoring for improved block efficiency",
            "text": "Having explored BPD’s prediction dynamics, we propose two drafting algorithms to improve block efficiency through rescoring of the top- lattice. This section presents techniques for rescoring the top- lattice along with empirical results.\nEach of these algorithms is a modification of the block drafted in Stage 1 in Algorithm 1  ###reference_###. Instead of using the most likely token at each head as the prediction, we construct the top- sausage lattice of likely drafts from each head, where the set of top- tokens is denoted as  for head . This approach allows any token within  to be chosen for position , yielding a total possible combinations of:\nIn this lattice, any path from the start to final state represents a viable draft. Two algorithms are proposed to select a small number of -length drafts from this lattice, which are then passed to the verification step. The first algorithm employs neural autoregressive transformers (subsection 7.1  ###reference_###), while the second utilizes n-gram language models (subsection 7.2  ###reference_###)."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Local rescoring via neural models",
            "text": "A simple approach uses a small neural rescorer, interpolating between the logits of the rescorer LM and vanilla block draft logits with an interpolation weight (algorithm 2  ###reference_###). The rescored prediction is given by:\nwhere  represents the logit of the block draft at head , and  is the corresponding logit predicted by the small neural rescoring model, which is conditioned on the sequence . The parameter  is the weight placed on the rescorer’s prediction. We experiment with decoder-only transformers having 32, 61, and 94 million (M) weight parameters (Appendix C  ###reference_###). We use greedy rescoring when generating the neural draft."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Global n-gram rescoring",
            "text": "We also evaluate the quality of drafts generated by rescoring with an n-gram LM.\nRecall that blockwise parallel LMs can be used to compute a lattice representing  possible sequences. We rescore all of these sequences with an n-gram  model, select the top  sequences and pass them to the verification stage. When , we refer to this as n-gram rescoring and when , we refer to this as -best n-gram BPD.\nWhile global rescoring typically yields better results compared to local rescoring, rescoring  sequences with a neural LM and selecting the most likely sequence would take time , which is computationally prohibitive in most cases.\nHence, we take advantage of n-gram models, which are unique in that we can select the most likely sequence in time  using dynamic programming.\nWe use the OpenFST library [1  ###reference_b1###] to represent each n-gram model as a weighted finite state automaton and apply finite state composition with the top- lattice followed by extraction of the  most likely draft sequences. Training details for the n-gram models are given in Appendix C.3  ###reference_###.\n###figure_14### ###figure_15### ###figure_16### ###figure_17### ###figure_18### ###figure_19###"
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Empirical evaluation",
            "text": "Block efficiency  Table 4  ###reference_### and Figure 7  ###reference_### demonstrate the impact of lattice rescoring on block efficiency across various tasks. Autoregressive neural, n-gram LM, and -best n-gram BPD rescoring all demonstrate improvements in block efficiency, although gains are task-dependent.\nHigh initial block efficiency (LAMBADA, CNN/Daily): Both rescoring methods show little to no improvement, suggesting that vanilla BPD already produces high quality drafts.\nLow initial block efficiency (SQuAD V1, SAMSUM, XSUM, NewsRoom): Both neural and n-gram rescoring lead to block efficiency gains, particularly with neural LMs achieving the best performance in some cases. This suggests that rescoring helps refine predictions and navigate the lattice more efficiently in these scenarios.\n###table_2### Repairing repetitions  In subsection 6.1  ###reference_###, we note that vanilla block drafts are prone to token-level repetition and that rescoring with a simple language model reduces the incidence of this. Although rescoring reduces repetition overall in drafts, is this driving improvements in block efficiency? To answer this, we compared the drafts generated by greedy rescoring with the 61M parameter neural rescorer against vanilla drafts. Time step instances were considered wins/ties/losses based on the accepted prefix length of the rescored draft vs. vanilla draft. Table 5  ###reference_### displays the win frequency across tasks along with the percentage of wins/losses attributed to introducing/eliminating repetition.\nNote that in the tasks where rescoring improves block efficiency the most, NewsRoom and MultiNews, a high percentage of those repaired instances are driven by fixing erroneously repeated tokens. In fact, for MultiNews, 66.23% of block drafts are improved through repetition repair. We also evaluated the performance of rescoring with in-domain trained rescoring LMs, but found that they tended to perform no better than C4-trained LMs (Appendix D  ###reference_###)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a comprehensive analysis of BPD, highlighting its predictive dynamics and proposing methods to refine the generation of block drafts. Our study offers insights into BPD’s behavior, particularly the tendency for drafts to contain consecutive repetitions and its heads to exhibit varying confidence levels in predictions. We introduce a novel measure, oracle top- block efficiency, to explore potential improvements in block efficiency. Two algorithms are proposed for generating higher quality drafts: one for local rescoring with small neural models (i.e., neural BPD) and another for global rescoring with an n-gram LM and generating multiple drafts (i.e. -best n-gram BPD). These algorithms leverage the strengths of both blockwise parallel LMs and small rescoring models to reduce average decoding latency, pushing the boundaries of efficient text generation with BPD. We believe that this paper lays the groundwork for future exploration in optimizing LM decoding speed."
        }
    ]
}