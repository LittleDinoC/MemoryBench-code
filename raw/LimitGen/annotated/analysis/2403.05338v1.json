{
    "title": "Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings",
    "abstract": "Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour.\nCurrently, prompt-based models\nare gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet.\nIn this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models.\nIn contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of plausibility and faithfulness scores.\n\n\n\n\nKeywords: explainability, attribution scores, low resource",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Recently, two paradigms of using pre-trained transformer models, such as BERT or GPT-2 (Devlin et al., 2019  ###reference_b6###; Brown et al., 2020  ###reference_b3###),\nhave gained popularity: fine-tuning which adapts the weights of the model using task-specific training data, and prompting which defines or learns so-called prompts to retrieve knowledge from the model, often leaving the model’s weights unchanged.\nWhen deploying pre-trained models in real-world downstream applications, two challenges arise: (i) the need for explaining the results as the models are very complex (Madsen et al., 2022  ###reference_b15###), and (ii) the need for adapting the models in low-resource scenarios as applications in special domains or languages typically do not provide many labeled training instances (Hedderich et al., 2021  ###reference_b10###).\nFor challenge (ii), previous work has shown that fine-tuning models in low-resource settings is hard (or even impossible for zero-resource settings) while prompting can yield good performance in those cases (Brown et al., 2020  ###reference_b3###; Schick and Schütze, 2021  ###reference_b17###; Liu et al., 2022  ###reference_b11###).\nIn terms of challenge (i), there is a research gap of carefully analyzing the difference of fine-tuned models (FTMs) and prompt-based models (PBMs).\nMost methods that have been proposed to enhance models’ explainability (Ribeiro et al., 2016  ###reference_b16###; Lundberg and Lee, 2017  ###reference_b14###) have so far only been studied in the context of FTMs (Atanasova et al., 2020  ###reference_b1###; DeYoung et al., 2020  ###reference_b7###; Ding and Koehn, 2021  ###reference_b8###), e.g., to answer the question which attribution method works best for different models and tasks.\nTo the best of our knowledge, no previous work has explored attribution scores from PBMs (neither encoder-based nor decoder-based models, i.a., large language models) nor compared their quality to signals extracted from FTMs.\nIn this paper, we thus address the following questions: (1) How plausible and faithful are explanatory signals extracted from PBMs in comparison to FTMs? While plausibility shows how plausible an explanation is according to human understanding, faithfulness measures to what extent the deemed important tokens are truly important for the predictions of the model.\nThus, we evaluate explanations both from the perspective of humans and models, making the analysis comprehensive.111Those two dimensions are also commonly studied in related work on models’ explainability (Atanasova et al., 2020  ###reference_b1###; Ding and Koehn, 2021  ###reference_b8###). \nIn addition, we introduce a new dimension into the analysis, namely the number of training samples in order to carefully investigate the behaviour of different methods in low-resource settings.\nIn our second research question, we investigate the effects of different attribution methods: (2) How well do different attribution methods perform in terms of plausibility and faithfulness? We answer this question by comparing different methods (namely attention, Integrated Gradients and Shapley Value Sampling) using extensive statistical significance tests. We focus on explanations in the form of attribution scores that highlight the importance of different input parts since they are more closely related to the model input and output than, e.g., generated free-text explanations.\nOur third question concerns the choice of the underlying model, taking into account the new trend of using large language models:\n(3) Do the results for PBMs also hold for decoder-based large language models? We show that we get comparable results when extracting attribution scores from a large language model.\nFor the first time, our paper shows that prompt-based models yield more plausible explanations than fine-tuned models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of both plausibility and faithfulness scores.\nThus, prompting pre-trained (either encoder-based or decoder-based) transformer models is better in low-resource settings than fine-tuning them, not only in terms of task performance but also when extracting attribution scores as explanations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Extraction of Attribution Scores",
            "text": "We analyze attribution scores from three different kinds of models: encoder-only models (e.g., BERT and similar models) following either the prompt-based paradigm (called “PBMs” in the following) or the fine-tuning paradigm (called “FTMs”), and decoder-only models (e.g., large language models) following the prompt-based paradigm (called “LLMs”). We do not investigate encoder-decoder models as we want to avoid mixing effects from cross-attention and self-attention. In the following paragraphs, we describe how we extract attribution scores from the different model types.\nWe illustrate our approach for extracting attribution scores from encoder-only PBMs in Figure 1  ###reference_###: The input  is composed of the actual task input (yellow boxes), trigger tokens (i.e., tokens providing task information, shown in blue boxes), and a prediction token (i.e., the token that the model needs to predict to solve the task, shown in the pink box). \nBased on the input, the model computes probabilities for the prediction token. Given the predicted label tokens,222We decided to use the tokens from the verbalizer instead of the true task labels as in Atanasova et al. (2020  ###reference_b1###) because it cannot be assumed to have access to the true labels in real-world scenarios. we then extract attribution scores for the actual task input. In particular, we use attention scores, Integrated Gradients and Shapley Value Sampling in our study. For attention, we extract attention scores from the last hidden layer of the [MASK] token, average them across different attention heads and normalize the attention scores over the actual task input. For Integrated Gradients and Shapley Value Sampling, we calculate attribution scores using the Captum package.333https://captum.ai\n###figure_1### For FTMs, the process is similar except that there are no prompts appended at the end of the sentences. Instead of using the language modeling head (the [MASK] token) for prediction, we use the default classification head (the [CLS] token) for FTMs and extract attribution scores for each token of the actual task input based on the predictions.\nExtracting attribution scores from generative models is more challenging as they typically generate a whole sequence of output tokens and the position of the prediction token is not clear. To tackle this issue, we explicitly prompt the model to output only the verbalized class label.444We chose the verbalizer such that the label name is part of the model’s vocabulary and is not split into several subtokens. Prompts can be found in Section A.2  ###reference_###. Then we detect if the generated output corresponds to one of the verbalized class labels or not. If yes, we treat the class label as the prediction token. If not, we treat the first token in the generated output sequence as the prediction token. Finally, we extract attribution scores for the actual task input based on the \nprediction token, as we did in the extraction from PBMs or FTMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Experimental Setup",
            "text": "We use a sentiment classification (Tweet Sentiment Extraction (TSE)555https://www.kaggle.com/c/tweet-sentiment-extraction) and a natural language inference dataset (e-SNLI (Camburu et al., 2018  ###reference_b4###)) to cover tasks of different semantic depth and use their annotations of token-level explanations. Statistics of the datasets can be found in Table 1  ###reference_###.666For TSE, we exclude data with the neutral label because their annotated explanations are mostly the whole sentence. To create low-resource settings, we subsample the training sets into six low-resource sets, ranging from eight instances to the whole set.\nFor our main analysis, we focus on state-of-the-art encoder-based transformer models since running large language models (LLMs) on all our evaluation setups would have been infeasible due to extensive computational costs. In particular, we use BERT-base (Devlin et al., 2019  ###reference_b6###), BERT-large, and RoBERTa-large (Liu et al., 2019  ###reference_b12###). Nevertheless, we also perform a small comparative study with LLMs afterwards, namely with the Vicuna model (Chiang et al., 2023  ###reference_b5###), a fine-tuned LLaMA version (Touvron et al., 2023  ###reference_b18###).\nIn our study, we focus on discrete prompts because they are more explainable than continuous prompts and also the standard input for LLMs.\nTo be able to factor out possible differences stemming from the choice of the prompting method, we study three different methods:\nManual uses a prompt from Schick and Schütze (2021  ###reference_b17###) and fine-tunes all parameters of the model. BitFit  uses the manual prompt but updates only the bias terms of the model during fine-tuning (Logan IV et al., 2022  ###reference_b13###), and BFF automatically searches for a prompt (Gao et al., 2021  ###reference_b9###) and fine-tunes all parameters with that prompt.\nPrompts and verbalizers are provided in Table 2  ###reference_###.777For the LLM, we use manual prompts only.\nWe use 4-fold cross-validation to tune both PBMs and FTMs. The hyperparameters can be found in Section A.1  ###reference_###.\nWe evaluate the plausibility and faithfulness of the explanatory signals. Those two dimensions allow to investigate explanations both from the perspective of humans and models. They are also commonly used in related work on explainability (Atanasova et al., 2020  ###reference_b1###; Ding and Koehn, 2021  ###reference_b8###).\nPlausibility indicates how plausible an explanation is according to human intuition. We quantify this with average precision (Atanasova et al., 2020  ###reference_b1###).888 sklearn.metrics.average_precision_score\nFaithfulness shows a model’s ability to accurately represent its reasoning process. In related work, an established way of quantifying this is measuring the performance decrease when masking the most salient words (DeYoung et al., 2020  ###reference_b7###; Atanasova et al., 2020  ###reference_b1###). We follow Atanasova et al. (2020  ###reference_b1###) and create several dataset perturbations by masking\n0, 10, 20, …, 100% of the tokens in the order of decreasing saliency. To calculate a single measure for faithfulness, the area under the threshold-performance curve (AUC) is used. However, this measure does not allow cross-model comparisons. Therefore, we normalize the AUC as the proportion of the area under the curve to the whole area (calculated as the highest possible performance multiplied by the number of thresholds). The lower the normalized score, the better the explanation is in faithfully showing the model’s reasoning.\nTo investigate the statistical significance of our results, we apply Kruskal-Wallis tests and Dunn’s Tests for pairwise differences."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Results",
            "text": "For smaller training sizes, PBMs outperform FTMs but the trend reverses as the training size increases. To investigate whether the differences between PBMs and FTMs in the low/rich-resource settings are significant, we set up two bins for each task: we treat the two training sets with fewest data as low-resource and the two training sets with most data as high-resource.\nWithin the low-resource part of the data, we find all comparisons to be statistically significant (TSE: (89)=73.86, <0.001, e-SNLI: (89)=29.24, <0.001).\nWithin the high-resource part of the data, the differences are not significant.\nWe also calculate the random baseline for plausibility scores (0.436 for TSE and 0.476 for e-SNLI) and find that explanations provided by PBMs achieve considerably higher plausibility scores than the random baselines in low-resource setting.\nMethod-wise, we find that for both tasks, the plausibility scores of the explanations extracted by ShapSample are significantly higher than those from attention and Integrated Gradients.\nWe sample 20 instances per dataset for each attribution method to conduct a small error analysis in terms of plausibility of explanations. We find that Integrated Gradients tend to assign negative values to functional words. We also find that attention seems to encode sentence information into a single token, so a specific token can get high attribution scores.\nFigures 1(c)  ###reference_sf3### and 1(d)  ###reference_sf4### show that faithfulness scores are influenced by the attribution methods. For instance, explanations extracted from FTMs with ShapSample are more faithful than explanations from PBMs independent of the number of resources. Explanations from PBMs with attention lead to the lowest faithfulness scores across all training sizes. For both datasets, we observe significant differences for all attribution method pairs except for ShapSample and gold. Thus, Shapley Value Sampling attribution scores are comparably faithful as gold annotations.\nShapSample consistently yields more plausible explanations than methods. We assume the reason for this lies in the calculation of Shapley Values: it takes in every permutation of features enabled to calculate a feature’s importance. For instance, if we have a feature set “good”, “day”, the attribution score of the feature “good” is calculated by every permutation that contains it, i.e., “good” and “good day”. Whereas for Integrated Gradients, this is not considered. We think taking each permutation to calculate feature importance is helpful in models like BERT, as context is of vital importance. Attention is the least plausible; this observation is in line with previous works, e.g., Bibal et al. (2022  ###reference_b2###).\nPBMs yield more plausible attribution scores than FTMs in low-resource settings. We think this might be because PBMs pick up task information quicker than FTMs in the low-resource settings, so the explanations given by PBMs are more plausible. Our study with LLMs shows that the trends of LLMs are comparable to the trends of PMBs, indicating the relevancy of our findings."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Studying LLMs",
            "text": "Given the increased relevance of large language models, we now investigate whether our findings hold for them as well and which plausibility and faithfulness scores we get for them compared to PBMs (i.e., encoder-based models used with prompting).\nDue to the large computational costs for obtaining attribution scores from LLMs, we limit the number of test instances to 100 for each data set and evaluate the 8-shot setting only. For the LLM, the 8 training samples are provided in each input prompt. The prompts can be found in Section A.2  ###reference_###. For the PBM (we chose RoBERTa-Large with BitFit prompts which was the best performing individual model in our previous analysis), the 8 training samples are used to tune the bias terms of the model.\nThe results in Table 3  ###reference_### show that Shapley Value Sampling again leads to more plausible and faithful explanations for Vicuna.\nWhen comparing Vicuna with RoBERTa, we note larger performance gaps among the attribution methods. We further note that the plausibility scores of attention are even lower for Vicuna than for RoBERTa.\nA reason could be that LLMs encode a larger input context\nand, thus,\ninformation of tokens that are irrelevant to the prediction might also be encoded."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Discussion",
            "text": "ShapSample consistently yields more plausible explanations than methods. We assume the reason for this lies in the calculation of Shapley Values: it takes in every permutation of features enabled to calculate a feature’s importance. For instance, if we have a feature set “good”, “day”, the attribution score of the feature “good” is calculated by every permutation that contains it, i.e., “good” and “good day”. Whereas for Integrated Gradients, this is not considered. We think taking each permutation to calculate feature importance is helpful in models like BERT, as context is of vital importance. Attention is the least plausible; this observation is in line with previous works, e.g., Bibal et al. (2022  ###reference_b2###  ###reference_b2###).\nPBMs yield more plausible attribution scores than FTMs in low-resource settings. We think this might be because PBMs pick up task information quicker than FTMs in the low-resource settings, so the explanations given by PBMs are more plausible. Our study with LLMs shows that the trends of LLMs are comparable to the trends of PMBs, indicating the relevancy of our findings."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this paper, we studied attribution scores extracted from prompt-based models in comparison to fine-tuned models, and compared different attribution methods w.r.t. plausibility and faithfulness scores. Our main findings were: (1) Prompt-based models generate more plausible explanations in low-resource settings. (2) Shapley Value Sampling outperforms other attribution methods, such as attention and Integrated Gradients across tasks and settings and is similarly faithful as gold annotations. (3) Our findings seem to be transferable to generative large language models.\nDirections for future work are the investigation of soft prompts as well as a more extensive study of explanatory signals from large language models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Bibliographical References",
            "text": ""
        }
    ]
}