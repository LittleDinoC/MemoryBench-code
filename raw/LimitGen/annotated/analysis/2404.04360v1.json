{
    "title": "Prompt Public Large Language Models to Synthesize Data for Private On-device Applications",
    "abstract": "Pre-training on public data is an effective method to improve the performance for federated learning (FL) with differential privacy (DP). This paper investigates how large language models (LLMs) trained on public data can improve the quality of pre-training data for the on-device language models trained with DP and FL. We carefully design LLM prompts to filter and transform existing public data, and generate new data to resemble the real user data distribution. The model pre-trained on our synthetic dataset achieves relative improvement of 19.0% and 22.8% in next word prediction accuracy compared to the baseline model pre-trained on a standard public dataset, when evaluated over the real user data in Gboard (Google Keyboard, a production mobile keyboard application). Furthermore, our method achieves evaluation accuracy better than or comparable to the baseline during the DP FL fine-tuning over millions of mobile devices, and our final model outperforms the baseline in production A/B testing. Our experiments demonstrate the strengths of LLMs in synthesizing data close to the private distribution even without accessing the private data, and also suggest future research directions to further reduce the distribution gap.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "While recent advances of machine learning models significantly benefit from scaling up both training data and model size [24  ###reference_b24###, 2  ###reference_b2###, 32  ###reference_b32###, 15  ###reference_b15###, 39  ###reference_b39###], smaller models have advantages in practical deployment due to inference latency, service cost, and privacy benefits when hosted on the local devices. User data are particularly effective to improve the performance of relatively small models targeting a specific task [18  ###reference_b18###, 47  ###reference_b47###, 7  ###reference_b7###]. Privacy-preserving methods are necessary for training these models using real user data [3  ###reference_b3###]. Differential privacy (DP) [11  ###reference_b11###, 12  ###reference_b12###], a mathematical guarantee applied to characterize the learning process, is a widely acknowledged method to prevent models from memorizing individual user’s information in the training data. Cross-device federated learning (FL) [29  ###reference_b29###, 23  ###reference_b23###], where devices collaboratively learn a model without transferring user data, is popular for limiting data access.\n###figure_1### The usage of large-scale public and private data is important to achieve both privacy and utility for privacy-preserving methods. Training DP models from scratch to achieve state of the art utility with meaningful guarantees is challenging [40  ###reference_b40###]. Recent work [27  ###reference_b27###, 49  ###reference_b49###] show promising results with much better privacy utility trade-off by combining pre-training language models (LMs) on public data and fine-tuning on private data. Public pre-training has become a standard technique for DP training, FL [31  ###reference_b31###], and the combination of DP and FL [47  ###reference_b47###]. In this paper, we focus on Gboard (Google Keyboard, a production mobile keyboard application), where small on-device LMs are trained and deployed [47  ###reference_b47###]. The training pipeline has two stages (see Figure 1  ###reference_###): pre-training using server-side public data, and private fine-tuning over private user data with DP FL. The trained LM is deployed on the users’ mobile devices to support features such as next word prediction, smart compose, smart completion and suggestion to improve the users’ typing experience.\nPre-training on the server-side public data is particularly helpful in reducing the number of training rounds (and hence, the communication and computation cost) needed by DP FL over the private user data. Intuitively, pre-training allows a model to learn knowledge shared by the public and private domain, so that the privacy budget can be efficiently utilized during the private fine-tuning phase to learn important features specific to the private domain. Therefore, the closer the distribution between the public pre-training data and the private user data, the more savings in the privacy budget used by the DP FL. In this work, we explore whether the powerful large LMs (LLMs) can be used to improve the quality of the server-side pre-training data for Gboard.\nLLMs with billions of parameters have achieved impressive performance in the general language generation and understanding tasks (see, e.g., [2  ###reference_b2###, 32  ###reference_b32###, 15  ###reference_b15###, 39  ###reference_b39###] and the references therein). As LLMs are a strong representation of their training data111Pre-trained LLMs are considered to be public because their training data do not contain the on-device user data in Gboard. The privacy concerns of LLMs and their training data is an important independent topic [5  ###reference_b5###, 41  ###reference_b41###]., Wang et al. [42  ###reference_b42###] asked Can Public Large Language Models Help Private Cross-device Federated Learning, and explored two approaches: 1) knowledge distillation (from the teacher LLM to student on-device LM) in pre-training, which significantly reduces the public data size and slightly improves the final performance after DP FL fine-tuning; and 2) distribution matching, which splits the privacy budget in two phases, and uses an LLM and a FL-trained LM from the first phase to filter the public data for the second phase. However, both approaches in [42  ###reference_b42###] require non-trivial changes of the current public pre-training and DP FL fine-tuning pipeline. Moreover, Wang et al. [42  ###reference_b42###] did not fully exploit LLMs’ emergent ability to generate long text sequences.\nIn this paper, we propose a simple yet effective method to improve the public data quality by exploiting the strong generative ability of LLMs. As shown in Figure 1  ###reference_###, we carefully design the prompts to guide LLMs to generate data closer to the target domain. In our case, the target domain of Gboard is the private user typing data on their mobile phones. We investigate three types of LLM prompts: 1) filter and 2) transform the public C4 data [36  ###reference_b36###], and 3) generate diverse chat data by chain-of-thought [45  ###reference_b45###] style prompting. The synthesized data can be directly used as the server-side pre-training data without extra changes to the DP FL phase, and hence, is simple to deploy in practice. Furthermore, the synthetic data can be potentially used as the proxy data for other tasks (e.g., server-side evaluation) or models, which is another advantage over the previous methods proposed in [42  ###reference_b42###].\nThe quality of our LLM generated data is assessed by running production FL experiments over the real user data from millions of mobile devices. Compared to the baseline C4 pre-training data [47  ###reference_b47###], the LM pre-trained on our data gives relative improvement of 19.0% and 22.8% in the next word prediction accuracy when evaluated on the real user data (see Table 3  ###reference_###). The LM also achieves superior performance in A/B testing after fine-tuning with DP FL (see Figure 4  ###reference_###). Finally, we show that distribution gap between the public and private data can be further reduced, if a privately trained LM is available to filter the data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Federated Learning (FL) and Differential Privacy (DP). In cross-device FL, clients such as mobile devices collaboratively learn a model using the decentralized data. The Federated Averaging (FedAvg) algorithm and its variants  [29  ###reference_b29###, 43  ###reference_b43###] are widely used in practice.\nIn each training round (see Figure 2  ###reference_###), the server first broadcasts a global model to a subset of clients; each client then updates their local model with local data, typically by an SGD optimizer, and sends back the model delta by subtracting the learned and initial local model weights; the model deltas are aggregated and used as pseudo gradient on the server to update the global model. After training typically thousands of rounds, the final model will be deployed on mobile devices for inference.\n###figure_2### DP provides a quantifiable measurement of the privacy risk of models memorizing the individual user’s information in the training data. Combining DP with FL gives an advanced privacy-preserving training method. DP is achieved by two operations [30  ###reference_b30###, 22  ###reference_b22###, 8  ###reference_b8###, 47  ###reference_b47###]: 1) clipping the  norm of each client’s model delta to control their contribution, and 2) adding noise to the aggregated deltas on the server.\nIn this paper, we use a production FL system similar to [4  ###reference_b4###] to run DP FL algorithm to train an on-device LM (see Section 4  ###reference_### for the setup).\nWe fix the privacy and optimization parameters for fine-tuning with DP FL and only study the effectiveness of different pre-training public (proxy) data. See Appendix B  ###reference_### for a mathematical description of the DP definition and details of the algorithms, and Appendix C  ###reference_### for hyperparameters and DP guarantees.\nSynthetic Data Generation. Using LLMs to generate synthetic data has shown promising results in many applications.\nTaori et al. [38  ###reference_b38###] used self-instruct [44  ###reference_b44###] to fine-tune LLaMA 7B [39  ###reference_b39###] with synthetic instructions and answers generated by the large text-davinci-003 model [33  ###reference_b33###] with few-shot prompting.\nEldan and Li [13  ###reference_b13###], Gunasekar et al. [17  ###reference_b17###], Li et al. [28  ###reference_b28###] used GPT-3.5 and GPT-4 models [34  ###reference_b34###, 1  ###reference_b1###] to generate data to train smaller models of fewer than 2 billion parameters for coherent storytelling, coding in Python, and common sense reasoning.\nYu et al. [52  ###reference_b52###] prompted LLMs with attributes to generate synthetic data similar to an existing attributed dataset.\nZhu et al. [55  ###reference_b55###], Shu et al. [37  ###reference_b37###] used LLMs to filter and transform given text for the rewriting task. In this work, we use LLMs to synthesize data for Gboard, where the target domain distribution is user typing data on mobiles that are different from the public data on the web.\nPrivate data can be used in various DP methods to guide LLMs to generate synthetic data close to the private distribution.\nThese methods assume direct access to the private data for either directly fine-tuning an LLM [26  ###reference_b26###, 53  ###reference_b53###, 51  ###reference_b51###, 50  ###reference_b50###] or measuring the distance between the generated data and private distribution [46  ###reference_b46###].\nIt is challenging to apply these methods in a cross-device FL system due to the on-device resource limitations and privacy concerns of mobile users.\nZhang et al. [54  ###reference_b54###] proposed to prompt LLMs with classification labels to generate synthetic pre-training data for FL. The method is designed for classification tasks and is studied for image and speech data, which cannot be directly applied to learn a language model."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Prompt LLMs to Synthesize Private-like Data",
            "text": "We design proper prompts to guide LLMs to process or generate data, so that the resulting data can be closer to the private target domain, compared to the baseline C4 dataset [36  ###reference_b36###] used by the current production [47  ###reference_b47###]. Our target domain distribution is formed by real-users’ mobile keyboard typing data stored on their mobile devices. For privacy protection, we cannot directly collect or access the on-device examples. Instead, we use common sense knowledge to design the LLM prompts. While we focus on English, our approach can be easily applied to other languages. An instruction-tuned PaLM 2-S [2  ###reference_b2###] is used as the LLM throughout the paper.\nThree types of data are synthesized by the LLM: filtered C4, generated chat, and converted C4. While prompting LLMs to synthesize data has been explored previously as discussed in Section 2  ###reference_###, to the best of our knowledge, we are the first to study this in a production FL application for private data, and validate its effectiveness by extensive experiments over millions of mobile phones."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Filter: Is the Public Example Likely Typed On Mobile Phones?",
            "text": "The public C4 dataset [36  ###reference_b36###] has over 360 million examples (782GB on disk), and is used as the pre-training data by the current production [47  ###reference_b47###].\nEach example contains a paragraph of text extracted from a webpage. For each example, the LLM is prompted to output a binary answer:\n“Determine whether the following topic is likely to be discussed by people on their mobile phones. Give a score of 0 or 1, where 1 means very likely, and 0 means unlikely.”.\nThe filtered dataset has only 136GB on disk (around 30B tokens), about  of the original English C4 dataset, and is named LLM-filter-C4-136G. Table 1  ###reference_### lists a few positive and negative examples, showing that the LLM can understand the given example, and follow the instruction to make reasonable decisions for filtering."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Prompt to Directly Generate Chat",
            "text": "We exploit the generative ability of LLMs to directly generate synthetic chat data.\nThe key challenge is to ensure that the generated data are diverse [17  ###reference_b17###, 13  ###reference_b13###, 52  ###reference_b52###]. To improve diversity, we design the following prompts with seven variables:\n“Imagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message [RECEIVER] on your mobile phone on the [TIME] of a [DAY]. You want to chat about the following topic: [TOPIC]. Generate the conversation between you and your message receiver. Do not include information other than the conversation.”.\nAmong the seven variables, five of them are sampled from a predefined set of categorical values, and two (RECEIVER and TOPIC) are self-generated by the LLM inspired by the chain-of-thought prompting [45  ###reference_b45###]. As shown in Figure 3  ###reference_###, for given values of (AGE, GENDER, TIME, DAY, CHAT-APP), we first use LLM to generate a list of message receivers. Then we set RECEIVER to each value in the generated list, and use LLM again to generate a list of message topics. We post-process the generated list of receivers and topics to remove any duplications. Finally, we loop over the TOPIC in the generated list, and use LLM to generate the conversations.\n###figure_3### We describe the predefined values of the five variables, and LLM prompts used to generate the other two variables (RECEIVER and TOPIC).\nAGE: Uniformly sampled from 15 to 55, and 3 age groups: “between 55 and 59”, “between 60 and 64”, and “over 65”.\nGENDER: “male”, and “female”.\nTIME: “morning”, “afternoon”, and “night”.\nDAY: 11 common holidays (e.g., “New Year’s Day”), a special “vacation day”, and 28 values in the format “[WEEKDAY] in the [SEASON]” where “WEEKDAY” can take 7 values from “Monday” to “Sunday”, and “SEASON” can take 4 values from “spring” to “winter”.\nCHAT-APP: “Android Messages”, “Facebook Messenger”, “Snapchat”, “Instagram”, “WhatsApp”, “Discord”, and “Telegram”. As a sanity check of LLM’s knowledge, we’ve asked the LLM to describe the differences between those popular chat apps, and verified that the answers are reasonable.\nRECEIVER: Given values for AGE, GENDER, TIME, DAY, and CHAT-APP, we ask the LLM to generate a list of message receivers: “Imagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message someone on your mobile phone on the [TIME] of a [DAY]. Generate a list of potential message receivers.”.\nTOPIC: Given values for AGE, GENDER, TIME, DAY, CHAT-APP, and a RECEIVER value generated by the LLM, we ask the LLM again to generate a list of topics: “Imagine you are a [GENDER] at age [AGE]. You are using the [CHAT-APP] APP to message [RECEIVER] on your mobile phone on the [TIME] of a [DAY]. Generate a list of potential message topics.”.\nWe use top-k sampling [14  ###reference_b14###] with  and temperature 0.2. A higher temperature usually gives a longer list of candidate receivers and topics. Because of the resource limitations, we choose a fixed temperature and leave the exploration of other sampling parameters or methods such as [20  ###reference_b20###] as future work. The generated chat data has 19GB on disk (around 4B tokens), containing about 30 million multi-turn conversations (see Table 5  ###reference_### in the Appendix for a few examples)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Transform: Convert Public Example into Chat on Mobile Phones",
            "text": "###table_1### Despite carefully designing the LLM prompts, the chat data generated by directly prompting the LLM (Section 3.2  ###reference_###) can be less diverse than the C4 dataset. In Table 2  ###reference_###, we compute the percentage of words in the vocabulary that have appeared in the dataset. The vocabulary contains 30K words and is used by the on-device LM over the en-US (United States) population. If a word does not appear in the pre-training data, then the corresponding word embedding will not be learned during the pre-training phase. Therefore, a higher vocabulary coverage is usually desired. The synthetic chat data given by directly prompting the LLM has a lower vocabulary coverage 79.3% than the raw and filtered C4 datasets (both have 99.6%).\nMotivated by this observation, we apply another approach to generate synthetic chat data: transform the LLM filtered C4 dataset (obtained in Section 3.1  ###reference_###) into conversations. For each filtered C4 example, we ask the LLM to:\n“Convert the following article to a conversation that you may message over your mobile phone. Generate the conversation. Include as many details as possible.”.\nDue to the resource constraints, only 20% of the filtered C4 examples are converted to conversations. The resulting dataset has about 10GB size (around 2B tokens) and 10 million multi-turn conversations (see Table 6  ###reference_### in the Appendix for a few examples). As shown in Table 6  ###reference_###, despite its small size, this dataset inherits a good vocabulary coverage 99.0% from the original C4."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Combine Filtered, Generated and Transformed Synthetic Data",
            "text": "We combine the 19GB data directly generated by LLM in Section 3.2  ###reference_### and the 10GB data transformed from C4 in Section 3.3  ###reference_###, and obtain a chat dataset of 29GB with about 40 million multi-turn conversations. We name it LLM-syn-chat-29G. It exploits the generative ability of LLMs to synthesize chat to resemble the private user data in Gboard. Intuitively, the generated chat may have a distribution closer to the target distribution than the C4 data (see some evidence in Section 5  ###reference_###). However, as we will show in Section 4.1  ###reference_###, the LM trained on the LLM-syn-chat-29G alone achieves slightly lower accuracy than LLM-filter-C4-136G when evaluated on the real user data, possibly due to the diversity issue. Therefore, we combine LLM-syn-chat-29G and LLM-filter-C4-136G into LLM-mix-166G.\nWe also tried different ratios when combining the two datasets (e.g., half from synthetic chat and half from filtered C4), and found that simply combining all the data works the best."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "As described in [18  ###reference_b18###, 47  ###reference_b47###], our on-device LM is a one-layer LSTM [19  ###reference_b19###, 16  ###reference_b16###] with 670 hidden units, embedding dimension 96, and a 30K-size word-level vocabulary. The LM has about 6M parameters. As shown in Figure 1  ###reference_###, we follow [47  ###reference_b47###] to train the LM in two steps: 1) server-side pre-training, and 2) fine-tuning with FL and DP. In DP FL fine-tuning, we run on a production FL system similar to [4  ###reference_b4###] with the real user data. Experiments are performed over two populations of the mobile devices: United States (i.e., the “en-US” population) and India (i.e., the “en-IN” population).\nTo participate in a training round in cross-device FL system, the mobile devices have to satisfy local criteria such as being charging and connecting to unmetered network [4  ###reference_b4###, 21  ###reference_b21###], and minimum separation time across rounds [47  ###reference_b47###]. The total number of available devices are estimated to be around 13M for en-US and 8M for en-IN. Note that the exact population size is unknown as devices are not tracked or logged in the system."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Fine-tuning with Differentially Private Federated Learning",
            "text": "###figure_4### After pre-training on the server in Section 4.1  ###reference_###, we follow the recommended practices in [47  ###reference_b47###] to fine-tune the LM using a production FL system [4  ###reference_b4###]. For more details about the federated training algorithm and DP accounting, see Appendix B  ###reference_###.\nAs the FL training proceeds, we run federated evaluation of the trained LMs over a different set of devices (i.e., the holdout set) in the same population, and report the NWP evaluation metrics in Figure 4  ###reference_###. We highlight the following observations:\nCompared to the baseline (C4 pre-trained LM), the LM pre-trained on LLM data has a higher accuracy at training round 0. This is consistent with the metrics reported in Table 3  ###reference_###, which are potentially aggregated from more devices across multiple federated evaluation rounds.\nDuring the FL training, the LM pre-trained on the LLM data maintains superior (over the en-US population) or comparable evaluation accuracy (over the en-IN population). Specifically, to reach the 0.17 accuracy on the en-US population, our method needs around 600 rounds while the the baseline needs around 1100 rounds (i.e., almost 2x more), giving a significant saving in the communication and computation cost and an improvement in the privacy guarantees.\nA/B testing. After fine-tuned with DP FL, we conduct live A/B testing to measure the LM performance in the production environment. Specifically, we measure two metrics: WMR (Word Modified Ratio, i.e., the ratio of words being modified during typing or after committed) and WPM (Word Per Minute, i.e., the number of committed words per minute). For fairness, the LMs in A/B testing have the same privacy guarantees, achieved by using the same noise multiplier, same number of clients per round, same minimum separation time, and same number of training rounds (see Section C.3  ###reference_### for more details). For en-US, the FL fine-tuned English LM pre-trained on LLM data improves over the baseline WMR by 0.64% and WPM by 0.11%. For en-IN, the FL fine-tuned English LM pre-trained on LLM data improves over the baseline WMR by 0.05% and WPM by 0.04%. These improvements, especially in en-US, are significant for improving the users’ mobile typing experience.\nDiscussion on en-US vs en-IN. Our LLM synthetic data is more effective in the en-US population than the en-IN population. This indicates that the synthetic data may be less similar to the real user typing data in India. An interesting direction of future work is to see if using target country/region in the LLM prompts can help synthesize better data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Improve Synthetic Data with Fine-tuned On-device LM",
            "text": "So far we only use the common sense knowledge to prompt the LLM to synthesize data closer to the private distribution of user typing data in Gboard. While the LLM-based synthetic data provide impressive gains compared to the C4 baseline in pre-training as shown in Table 3  ###reference_###, the on-device LM still benefits a lot from fine-tuning over the real user data as shown in Figure 4  ###reference_###. This indicates a gap between the distribution of the LLM-based synthetic data and that of the real user data. In this section, we present a preliminary study of a simple strategy to close the gap, without changing the privacy-preserving way of accessing private data. Specifically, we ask the following question: can we reduce the distribution gap by using a fine-tuned on-device LM from Section 4.2  ###reference_### to filter the data?\nNote that the goal of this study is to use the fined-tuned LMs to filter the synthetic dataset to get better proxy data for the private domain. We only pre-train on the new proxy data and measure the quality improvement following Section 4.1  ###reference_###, but do not further fine-tune the pre-trained model with DP FL as in Section 4.2  ###reference_### and in [42  ###reference_b42###]. Training two models with DP FL on the same population not only complicates the standard production pipeline, but also needs to carefully allocate privacy budgets because the two models are accessing the same set of users (see [42  ###reference_b42###] for more discussions). Although the new proxy data is not used for improving the on-device LMs for Gboard, it can be of independent interest for other tasks. For example, the proxy data can be used to facilitate research simulation in the datacenter, and improve server-side models that cannot be easily fine-tuned with DP FL.\nFor each example in the LLM-mix-166G dataset, we compute 3 values: 1) OOV rate: the percentage of tokens not in the LM vocabulary; 2) pre-trained LM score: the average log-likelihood computed by the LM trained on LLM-mix-166G as in Section 4.1  ###reference_###; 3) fine-tuned LM score: the average log-likelihood computed by the LM pre-trained on LLM-mix-166G and fine-tuned over real user data with DP FL as in Section 4.2  ###reference_###. Examples satisfying the following conditions are kept in the filtered LLM data (see Appendix C  ###reference_### for more details): OOV rate  0.6; fine-tuned LM score  5; fine-tuned LM score  pre-trained LM score.\nAfter filtering LLM-mix-166G, the data size is decreased to 32GB (around 7B tokens). We use LLM-prox-32G to represent this new proxy dataset. We follow the same step described in Section 4.1  ###reference_### to measure the quality of LLM-prox-32G: train an LM from scratch on the data, and then perform federated evaluation over the real user data. As shown in Table 4  ###reference_###, on the en-US population, LLM-prox-32G further improves the accuracy from 0.1452 to 0.1509 compared to LLM-mix-166G, and achieves much higher accuracy than the baseline C4. This indicates that the filtered data have higher quality, despite only having 19% of the original LLM-mix-166G data. Table 4  ###reference_### also shows that a larger fraction of LLM filtered C4 examples are filtered out compared to the synthetic chat data, which potentially indicates that the majority of the LLM filtered C4 are less similar to the private user data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Inspired by recent advances in generative LLMs, this work studies whether LLMs can help differentially private federated learning for training small on-device models. In particular, we focus on Gboard (Google Keyboard, a production mobile keyboard application) where the goal is to learn a small on-device LM using the private user typing data. Our work provides evidence that even with no access to the private data, common sense knowledge and careful prompt design can help guide LLMs to synthesize data similar to the target domain. Effectiveness of our method is verified by extensive FL experiments over the real-world user data from the millions of mobile devices.\nOur results suggest a few interesting directions of future work, including developing more privacy-preserving quality measures, investigating other LLM prompting strategies such as adding the country/region information, and different sampling methods, as already pointed out in Section 4.1  ###reference_###, Section 4.2  ###reference_###, and Section 3  ###reference_###. Moreover, given the promising results from the preliminary study in Section 5  ###reference_###, further investigation on improving the quality of proxy data with guaranteed privacy-preserving methods can be a rewarding direction."
        }
    ]
}