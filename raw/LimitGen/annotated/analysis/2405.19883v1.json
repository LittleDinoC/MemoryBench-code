{
    "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
    "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world.\nTo this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively.\nUnder this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting.\nUnder proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning.\nAdditionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret.\nAs a remedy, we introduce an -greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small.\nFinally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large language models (LLMs) such as GPT-4 (OpenAI,, 2023  ###reference_b54###) and Llama 2 (Touvron et al.,, 2023  ###reference_b69###) has marked a significant leap in artificial intelligence, thanks to their striking capabilities in understanding language and performing complex reasoning tasks. These capabilities of LLMs have led to the emergence of LLM-empowered agents (LLM Agents), where LLMs are used in conjunction with tools or actuators to solve decision-making problems in the physical world. LLM Agents have showcased promising empirical successes in a wide range of applications, including autonomous driving (Wang et al., 2023b,  ###reference_b72###; Fu et al.,, 2024  ###reference_b20###), robotics (Brohan et al.,, 2023  ###reference_b8###; Li et al., 2023a,  ###reference_b40###), and personal assistance (Liu et al.,, 2023  ###reference_b48###; Nottingham et al.,, 2023  ###reference_b53###). This progress signifies a crucial advancement in the creation of intelligent decision-making systems, distinguished by a high degree of autonomy and seamless human-AI collaboration.\nLLMs only take natural languages as input.\nTo bridge the language and physical domain, LLM-agents typically incorporate three critical components: an LLM Planner, a physical Actor, and a multimodal Reporter, functioning respectively as the brain, hands, and eyes of the LLM-agent, respectively.\nSpecifically, upon receiving a task described by a human user, the LLM Planner breaks down the overall task into a series of subgoals.\nSubsequently, the Actor implements each subgoal in the physical world through a sequence of actions. Meanwhile, the Reporter monitors changes in the physical world and conveys this information back to the LLM Planner in natural language form. This dynamic interaction among Planner, Actor, and Reporter empowers LLM Agents to understand the environment, formulate informed decisions, and execute actions effectively, thus seamlessly integrating high-level linguistic subgoals with low-level physical task execution.\nThe revolutionary approach of LLM Agents represents a paradigm shift away from traditional learning-based decision-making systems.\nUnlike these conventional systems, LLM Agents are not tailored to any specific task. Instead, they rely on the synergy of their three distinct components—each trained separately and often for different objectives.\nIn particular, the LLM Planner is trained to predict the next token in a sequence on vast document data.\nMoreover, when deployed to solve a task, the way to interact with the LLM Planner is via prompting with the LLM fixed.\nThe Actor, as language-conditioned policies, can be trained by RL or imitation learning.\nMoreover, the Reporter, as a multimodal model, is trained to translate the physical states (e.g., images) into natural language.\nThis unique configuration prompts critical research questions regarding the theoretical underpinnings of LLM Agents, particularly concerning their decision-making effectiveness.\n###figure_1### In this work, we make an initial step toward developing a theoretical framework for understanding the dynamics and effectiveness of LLM Agents. Specifically, we aim to answer the following questions:\n(a) What is a theoretical model for understanding the performance of LLM Agents?\n(b) How do pretrained LLMs solve decision-making problems in the physical world via prompting?\n(c) How does an LLM Agent address the exploration-exploitation tradeoff?\n(d) How do the statistical errors of the pretrained LLM and Reporter affect the overall performance of the LLM Agent?\nTo address Question (a), we propose analyzing LLM Agents within a hierarchical reinforcement learning framework (Barto and Mahadevan,, 2003  ###reference_b4###; Pateria et al.,, 2021  ###reference_b55###), positioning the LLM Planner and the Actor as policies operating within high-level POMDPs and low-level MDPs, respectively (§3.1  ###reference_###).\nBoth levels share the same state space—namely, the physical state—though the LLM Planner does not directly observe this state but instead receives a language-based description from the Reporter, effectively navigating a POMDP.\nThe action space of the high-level POMDP is the set of language subgoals.\nMeanwhile, the state transition kernel is determined by the pretrained Actor, and thus is associated with a variable  that summarizes its dependency on low-level Actor. Such variable is unknown to LLM Planner.\nAfter pretraining, without prior knowledge of the Actor’s quality or the physical environment, the LLM Planner attempts to solve the high-level POMDP by iteratively generating a sequence of subgoals based on feedback from the Reporter via prompting.\nUnder this framework, the overall performance of the LLM Agent can be captured by the regret in terms of finding the optimal policy of the hierarchical RL problem in the online setting (§3.2  ###reference_###).\nFurthermore, to answer Question (b), we prove that when the pretraining data includes a mixture of expert trajectories, during the prompting stage, the pretrained LLM Planner essentially performs Bayesian aggregated imitation learning (BAIL) through in-context learning (Theorem 4.2  ###reference_theorem2###).\nThis process involves constructing a posterior distribution over the hidden parameter\nof the transition kernel, followed by generating subgoals that emulate a randomly selected expert policy, weighted according to this posterior distribution.\nSuch a Bayesian learning mechanism is encoded by the LLM architecture and is achieved through prompting.\nHowever, since the LLM has no prior knowledge of the physical environment, it needs to guide the Actor to explore the physical environment. We prove that merely adhering to BAIL-derived subgoals can lead to the inadequate exploration, resulting in a linear regret (Proposition 4.3  ###reference_theorem3###).\nTo mitigate this, i.e., Question (c), we introduce an\n-greedy\nexploration strategy, which occasionally deviates from BAIL subgoals in favor of exploration, significantly enhancing learning efficacy by ensuring a sublinear regret (Theorem 4.6  ###reference_theorem6###).\nSpecifically, to address Question (d) we establish that the regret is bounded by a sum of two terms (Theorem 5.7  ###reference_theorem7###): a\n-regret related to the number of episodes the LLM Agent is deployed to the hierarchical RL problem, and an additional term representing the statistical error from pretraining the LLM Planner and Reporter via maximum likelihood estimation (MLE) and contrastive learning, respectively (Theorem 2  ###reference_te2###, 5.5  ###reference_theorem5###).\nFinally, we extend our analysis to scenarios where the Planner utilizes the LLM as world model for inferring the upper-level POMDP’s transition model via Bayesian model aggregation (Proposition B.1  ###reference_theorem1###, Corollary B.3  ###reference_theorem3###). Our theoretical framework also accommodates a multi-agent context, where the LLM Planner coordinates with a collaborative team of low-level actors (Corollary B.4  ###reference_theorem4###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries and Related Works",
            "text": "The Large Language Models (LLMs) such as ChatGPT (Brown et al.,, 2020  ###reference_b9###), GPT-4 (OpenAI,, 2023  ###reference_b54###), Llama (Touvron et al.,, 2023  ###reference_b69###), and Gemini (Team et al.,, 2023  ###reference_b67###), are pretrained on vast text corpora to predict in an autoregressive manner. Starting from an initial token , where  denotes the dimension of token vector and  denotes the language space, the LLM, with parameters , predicts the next token with , where  and . Each token  specifies a word or word’s position, and the token sequence  resides in the space of token sequences . Such an autoregressive generating process terminates when the stop sequence token is generated.\nLLMs haved exhibited robust reasoning capabilities and a crucial aspect of their reasoning prowess is the in-context learning (ICL) ability. This ability is further enhanced through additional training stages (Iyer et al.,, 2022  ###reference_b33###), careful selection and arrangement of informative demonstrations (Liu et al.,, 2021  ###reference_b45###; Kim et al.,, 2022  ###reference_b37###), explicit instruction (Honovich et al.,, 2022  ###reference_b29###), and use of prompts to stimulate chain of thoughts (Wei et al., 2022b,  ###reference_b74###). Unlike fine-tuned models customized for specific tasks, LLMs showcase comparable capabilities by learning from the informative prompts (Li et al.,, 2022  ###reference_b42###; Liu et al., 2022b,  ###reference_b47###). Assume that prompt, denoted by , is generated based on a latent variable  autoregressively. The token follows a generating distribution such that  and , where  denotes the space of hidden information or concepts. The latent structure is commonly employed in language models, including topic models like LDA (Blei et al.,, 2003  ###reference_b6###), BERT (Devlin et al.,, 2018  ###reference_b14###), generative models like VAE (Kusner et al.,, 2017  ###reference_b38###), T5 (Raffel et al.,, 2020  ###reference_b60###), and is also widely adopted in the theoretical analysis of ICL (Xie et al.,, 2021  ###reference_b76###; Zhang et al.,, 2023  ###reference_b84###).\nTheoretical understanding of ICL is an active area of research. Since real-world datasets used for LLM pretraining are difficult to model theoretically and are very large, ICL has also been studied in stylized setups (Xie et al.,, 2021  ###reference_b76###; Garg et al.,, 2022  ###reference_b21###; Chan et al.,, 2022  ###reference_b11###; Hahn and Goyal,, 2023  ###reference_b26###; Zhang et al.,, 2023  ###reference_b84###). In this paper, we build upon the framework attributing the ICL capability to Bayesian inference (Xie et al.,, 2021  ###reference_b76###; Jiang,, 2023  ###reference_b35###; Zhang et al.,, 2023  ###reference_b84###), which posits that the pretrained LLMs predict the next token with probability by aggregating the generating distribution concerning latent variable  over the posterior distribution. Moreover, a series of practical experiments, including Wang et al., 2023a  ###reference_b71###; Ahuja et al., (2023  ###reference_b2###), provide empirical support for this Bayesian statement.\nLLMs, as highlighted in OpenAI, (2023  ###reference_b54###), are powerful tools for the task planning (Wei et al., 2022a,  ###reference_b73###; Hu and Shu,, 2023  ###reference_b31###). The success of LLM agent marks a shift from task-specific policies to a pretrain-finetune-prompt paradigm. By breaking down the complex tasks into subgoals, LLM Agent facilitates the effective zero-shot resource allocation across environments. For instance, envision a scenario where a robotic arm is tasked with “move a teapot from the stove to a shelf”, a task for which the robotic arm may not be pretrained.\nHowever, leveraging LLMs allows the decomposition of the task into a sequence of executable subgoals: “grasp the teapot”, “lift the teapot”, “move the teapot to the shelf”, and “release the teapot”.\nIn the conventional task-planning and decision-making problems, symbolic planners have commonly been employed to transform them into search problems (Bonet and Geffner,, 2001  ###reference_b7###; Ghallab et al.,, 2004  ###reference_b23###) or to design distinct reinforcement learning or control policies for each specific scenario. Recent empirical studies have shifted towards leveraging LLMs as symbolic planners in various domains, including robotic control (Mandi et al.,, 2023  ###reference_b49###; Brohan et al.,, 2023  ###reference_b8###; Li et al., 2023a,  ###reference_b40###; Du et al.,, 2023  ###reference_b16###), autonomous driving (Wang et al., 2023b,  ###reference_b72###; Fu et al.,, 2024  ###reference_b20###) and personal decision assistance (Li et al.,, 2022  ###reference_b42###; Lin et al., 2023a,  ###reference_b43###; Hu et al.,, 2023  ###reference_b30###; Liu et al.,, 2023  ###reference_b48###; Nottingham et al.,, 2023  ###reference_b53###). Another recent line of research has been dedicated to devising diverse prompting schemes to enhance the reasoning capability of LLMs (Wei et al., 2022b,  ###reference_b74###; Yao et al., 2023a,  ###reference_b78###; Yao et al., 2023b,  ###reference_b79###; Hao et al.,, 2023  ###reference_b27###). Despite the considerable empirical success, there is a lack of comprehensive theoretical analysis on LLM Agent. In this paper, we formalize this approach into a hierarchical LLM-empowered planning framework and provide a theoretical analysis of its performance.\nTwo recent works by Liu et al., (2023  ###reference_b48###) and Lee et al., (2023  ###reference_b39###) also aim to establish provable algorithms for planning with LLMs or decision-pretrained Transformers (DPT). In comparison, we discuss both the plausibility of taking LLMs as a subgoal generator (Lee et al.,, 2023  ###reference_b39###) and simulated world model (Liu et al.,, 2023  ###reference_b48###). Furthermore, we provide a statistical guarantee for pretrained models and conduct a detailed examination of the algorithm’s performance in practical settings, bringing our analysis closer to real-world applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Theoretical Framework for LLM Agents",
            "text": "To formalize the architecture of LLM Agents, we propose a general theoretical framework—Planner-Actor-Reporter (PAR) system. Furthermore, the problem is modeled as a hierarchical RL problem (Pateria et al.,, 2021  ###reference_b55###). Specifically, the Planner, empowered by LLMs, conducts high-level task planning within the language space; the Actor, pretrained before deployment, undertakes low-level motion planning within the physical world; and the Reporter, equipped with a sensor to sense the physical environment, processes the information and feeds it back to the Planner, bridging the gap between language space and the physical world (see §3.1  ###reference_###). Additionally, we present the performance metric and pretraining methods of LLMs for the Planner and translators for the Reporter in §3.2  ###reference_###.\nLet  be the space of language subgoals,  and  respectively denote the space of physical states and actions. At high-level step , the low-level MDP is specified by a transition kernel  and the rewards that depends on subgoal . Following this, the Actor is modelled as a language-conditioned policy , where  and . Assume that the Actor stops at step , regardless of the subgoal achievement. Subsequently, the Planner receives the observation of the current state  from the Reporter, and sends a new subgoal to the Actor based on the historical feedback.\nSuppose that a low-level episode corresponds to a single high-level action of the Planner. Thus, the high-level POMDP reuses the physical state space  as the state space, but takes the subgoal space  as the action space instead. Following this, the high-level transition kernel is jointly determined by the low-level policy  and the physical transition kernel  such that\nwhere we write .\nSince the LLM-empowered Planner cannot directly process the physical states, it relies on some (partial) observations generated by the Reporter. Specifically, let  describe the physical state  in language through a translation distribution , where  denotes the space of observations. At each step , a reward  is obtained, which depends on both the observation and the task  assigned by human users. Here,  denotes the space of potential tasks in language.\nThe Planner aims to determine a sequence of subgoal  such that when the Actor is equipped with policy , these subgoals maximize the expected sum of rewards. During task planning, the Planner must infer both Actor’s intention, i.e., policy , and the environment, i.e., physical transition kernel , from the historical information. Thus,  constitutes all the latent information to the high-level Planner, and denote  as the space of all potential latent variables with .\n###figure_2### To summarize, the interactive protocol is as below: at the beginning of each episode , Planner receives a task . At step , each module follows:\nAfter collecting  from Reporter, the Planner leverages LLMs for recommendations on task decomposition, and the policy is denoted by , where  represents the space of the trajectory sequence with arbitrary length. LLM’s recommendation is obtained by invoking the ICL ability with the history-dependent prompt:\nwhere  denotes the historical context and  is the trajectory until -th step. In the PAR system, Planner retains autonomy and is not obligated to follow LLM’s recommendations. Let  be the Planner’s policy, which partially leverages the LLM’s recommendation . The Planner selects , and sends it to the Actor.\nUpon receiving  from Planner, the Actor plans to implement  in physical world with pretrained skill sets, denoted by a subgoal-conditioned policy . A sequence of actions  is executed, where the dynamics follows  and  starting from . The low-level episode concludes at .\nAfter the low-level episode concludes, the Reporter collects and reports the current state  via observation  generated from , where  denotes the distribution of the pretrained translator. Subsequently, the observation  of the current state is sent back to the Planner, reinforcing to the ongoing task planning.\nThe strength of the PAR system lies in its resemblance to RL (Sutton and Barto,, 2018  ###reference_b66###), allowing the Planner to iteratively adjust its planning strategy based on feedback from the Reporter. Moreover, the Reporter empowers the system to process the real-time information and the integration of multiple modalities of raw data like RGB, images, LiDAR, audio, and text (Li et al., 2023b,  ###reference_b41###; Xu et al.,, 2023  ###reference_b77###). The Actor’s skill sets can effectively be pretrained using the goal-conditioned RL (Chane-Sane et al.,, 2021  ###reference_b12###; Liu et al., 2022a,  ###reference_b46###), language-to-environment grounding (Brohan et al.,, 2023  ###reference_b8###; Huang et al.,, 2022  ###reference_b32###) or pre-programmed manually (Singh et al.,, 2023  ###reference_b65###).\nIn this paper, we focus on the performance of the high-level Planner, and regard the low-level Actor as an autonomous agent that can use the pretrained skill sets following a fixed policy. For any latent variable  and policy  with , the value function is defined as\nwhere the expectation is taken concerning the initial state , policy , ground-truth translation distribution , and transition kernel . For all , there exists an optimal policy , where .\nTo characterize the performance under practical setting, we denote  as the value function concerning the pretrained translator , and for all , let  be the optimal policy in practice. Then, the regret under practical setting is defined as\nwhere  represents the Planner’s policy empowered by a pretrained  and the expectation is taken with respect to the context  defined in (3.2  ###reference_###) generated by taking  sequentially. Here, we focus on the performance when the Planner collaborates with a pretrained PAR system in an environment characterized by  and pretrained Reporter. Our goal is to design a sample-efficient algorithm that achieves a sublinear regret, i.e., .\nThe pretraining dataset consists of  independent samples with  episodes such that , where . For each sample,  specifies a low-level MDP with language-conditioned policies and  specifies the sequence of high-level tasks. Here,  and  denote the prior distributions. We assume that the joint distribution of each data point  in the dataset, denoted by , follows that:\nwhere  is the behavior policy that features how the contextual information is collected, and additionally the label, i.e., optimal subgoal, is sampled from the optimal policy  by experts. Subsequently, the latent information  is hidden from the context.\nTo pretrain LLMs, we adopt a supervised learning approach concerning the transformer structure, aligning with the celebrated LLMs such as BERT and GPT (Devlin et al.,, 2018  ###reference_b14###; Brown et al.,, 2020  ###reference_b9###). Specifically, the pretraining data is constructed based on . For clarity, we extract the language data without expert knowledge and write the collected data into a sequence of ordered tokens, i.e., sentences or paragraphs. For the -th sample , we write\nwith a length of , which contains  episodes with one task,  observations and  subgoals each. Following this, LLM’s pretraining dataset is autoregressively constructed with the expert guidance, denoted by , where  and let\nIn other words, when pretraining to predict the next subgoal, we replace the one sampled from the behavior policy with the one from the optimal policy. In practice, sentences with expert knowledge can be collected from online knowledge platforms such as Wikipedia (Merity et al.,, 2016  ###reference_b50###; Reid et al.,, 2022  ###reference_b61###). Following the pretraining algorithm of BERT and GPT, the objective is to minimize the cross-entropy loss, which can be summarized as  with\nand  is the pretrained LLM by algorithm in (3.7  ###reference_###). More details are deferred to §5.1  ###reference_###.\nTo pretrain translators, we employ a self-supervised contrastive learning approach, which aligns with celebrated vision-language models such as CLIP (Radford et al.,, 2021  ###reference_b59###) and ALIGN (Jia et al.,, 2021  ###reference_b34###). Let  be the contrastive pretraining dataset for translators, which is also constructed upon the dataset . Following the framework adopted in Qiu et al., (2022  ###reference_b58###); Zhang et al., (2022  ###reference_b83###), for each observation-state pair , a positive or a negative data point, labelled as  and , is generated with equal probability, following that\nPositive Data: Collect  with label .\nNegative Data: Collect  with label , where  is sampled from negative sampling distribution  that has a full support over the domain of interest.\nDenote  as the joint distribution of data collected by the process above. The learning algorithm follows that , where the contrastive loss  is defined as\nConsider function class  with finite elements with  serving as a set of candidate functions that approximates the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2### for justification). Following this, the pretrained translator for the Reporter by the algorithm in (3.8  ###reference_###) is thus defined as . More details are deferred to §5.2  ###reference_###.\nIn (3.5  ###reference_###), we assume that all pretraining data is generated from a joint distribution , and then split for pretraining of LLM and Reporter. In practice, the pretraining dataset for the Reporter can consist of paired observation-state data collected from any arbitrary distribution, as long as (i) the LLM and Reporter “speak” the same language, i.e., shared , and (ii) the coverage assumption can hold (see Assumption 5.6  ###reference_theorem6###).\nAs an example, noise contrastive estimation (NCE, Gutmann and Hyvärinen,, 2010  ###reference_b25###) is one of the most widely adopted objectives in contrastive representation learning. From the theoretical lens, to estimate unnormalized model  with , additional noise data is sampled from a reference distribution  and then estimate by maximizing  with  and . With slight modifications, we use a function class  to approximate the ratio  rather than the relative probability  itself. In practice, the most commonly used contrastive training objectives are variations of NCE and originated from the NLP domain (Schiappa et al.,, 2023  ###reference_b64###) by sharing the same idea of minimizing the distance between the positive pair and maximizing the distance between the negative pairs."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Planner-Actor-Reporter System",
            "text": "In this section, we delve into details of the PAR system under  Hierarchical Markov Decision Process (HMDP).\nAt the high level, the Planner empowered by LLM handles task planning by decomposing tasks into subgoals to solve a language-conditioned  Partially Observable Markov Decision Process (POMDP) with a finite horizon .\nAt the low level, the Actor translates these subgoals into the actionable steps in the physical world to handle a language-conditioned  Markov Decision Process (MDP) with a finite horizon 111Throughout the paper, we use the notation  to distinguish low-level elements from their high-level counterparts.. Please refer to the right panel of Figure 1  ###reference_### for a detailed example of LLM Agent, and see Figure 2  ###reference_### for an overview of the hierarchical interactive process.\nLet  be the space of language subgoals,  and  respectively denote the space of physical states and actions. At high-level step , the low-level MDP is specified by a transition kernel  and the rewards that depends on subgoal . Following this, the Actor is modelled as a language-conditioned policy , where  and . Assume that the Actor stops at step , regardless of the subgoal achievement. Subsequently, the Planner receives the observation of the current state  from the Reporter, and sends a new subgoal to the Actor based on the historical feedback.\nSuppose that a low-level episode corresponds to a single high-level action of the Planner. Thus, the high-level POMDP reuses the physical state space  as the state space, but takes the subgoal space  as the action space instead. Following this, the high-level transition kernel is jointly determined by the low-level policy  and the physical transition kernel  such that\nwhere we write .\nSince the LLM-empowered Planner cannot directly process the physical states, it relies on some (partial) observations generated by the Reporter. Specifically, let  describe the physical state  in language through a translation distribution , where  denotes the space of observations. At each step , a reward  is obtained, which depends on both the observation and the task  assigned by human users. Here,  denotes the space of potential tasks in language.\nThe Planner aims to determine a sequence of subgoal  such that when the Actor is equipped with policy , these subgoals maximize the expected sum of rewards. During task planning, the Planner must infer both Actor’s intention, i.e., policy , and the environment, i.e., physical transition kernel , from the historical information. Thus,  constitutes all the latent information to the high-level Planner, and denote  as the space of all potential latent variables with .\n###figure_3### To summarize, the interactive protocol is as below: at the beginning of each episode , Planner receives a task . At step , each module follows:\nAfter collecting  from Reporter, the Planner leverages LLMs for recommendations on task decomposition, and the policy is denoted by , where  represents the space of the trajectory sequence with arbitrary length. LLM’s recommendation is obtained by invoking the ICL ability with the history-dependent prompt:\nwhere  denotes the historical context and  is the trajectory until -th step. In the PAR system, Planner retains autonomy and is not obligated to follow LLM’s recommendations. Let  be the Planner’s policy, which partially leverages the LLM’s recommendation . The Planner selects , and sends it to the Actor.\nUpon receiving  from Planner, the Actor plans to implement  in physical world with pretrained skill sets, denoted by a subgoal-conditioned policy . A sequence of actions  is executed, where the dynamics follows  and  starting from . The low-level episode concludes at .\nAfter the low-level episode concludes, the Reporter collects and reports the current state  via observation  generated from , where  denotes the distribution of the pretrained translator. Subsequently, the observation  of the current state is sent back to the Planner, reinforcing to the ongoing task planning.\nThe strength of the PAR system lies in its resemblance to RL (Sutton and Barto,, 2018  ###reference_b66###  ###reference_b66###), allowing the Planner to iteratively adjust its planning strategy based on feedback from the Reporter. Moreover, the Reporter empowers the system to process the real-time information and the integration of multiple modalities of raw data like RGB, images, LiDAR, audio, and text (Li et al., 2023b,  ###reference_b41###  ###reference_b41###; Xu et al.,, 2023  ###reference_b77###  ###reference_b77###). The Actor’s skill sets can effectively be pretrained using the goal-conditioned RL (Chane-Sane et al.,, 2021  ###reference_b12###  ###reference_b12###; Liu et al., 2022a,  ###reference_b46###  ###reference_b46###), language-to-environment grounding (Brohan et al.,, 2023  ###reference_b8###  ###reference_b8###; Huang et al.,, 2022  ###reference_b32###  ###reference_b32###) or pre-programmed manually (Singh et al.,, 2023  ###reference_b65###  ###reference_b65###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Performance Metric and Pretraining",
            "text": "In this paper, we focus on the performance of the high-level Planner, and regard the low-level Actor as an autonomous agent that can use the pretrained skill sets following a fixed policy. For any latent variable  and policy  with , the value function is defined as\nwhere the expectation is taken concerning the initial state , policy , ground-truth translation distribution , and transition kernel . For all , there exists an optimal policy , where .\nTo characterize the performance under practical setting, we denote  as the value function concerning the pretrained translator , and for all , let  be the optimal policy in practice. Then, the regret under practical setting is defined as\nwhere  represents the Planner’s policy empowered by a pretrained  and the expectation is taken with respect to the context  defined in (3.2  ###reference_###  ###reference_###) generated by taking  sequentially. Here, we focus on the performance when the Planner collaborates with a pretrained PAR system in an environment characterized by  and pretrained Reporter. Our goal is to design a sample-efficient algorithm that achieves a sublinear regret, i.e., .\nThe pretraining dataset consists of  independent samples with  episodes such that , where . For each sample,  specifies a low-level MDP with language-conditioned policies and  specifies the sequence of high-level tasks. Here,  and  denote the prior distributions. We assume that the joint distribution of each data point  in the dataset, denoted by , follows that:\nwhere  is the behavior policy that features how the contextual information is collected, and additionally the label, i.e., optimal subgoal, is sampled from the optimal policy  by experts. Subsequently, the latent information  is hidden from the context.\nTo pretrain LLMs, we adopt a supervised learning approach concerning the transformer structure, aligning with the celebrated LLMs such as BERT and GPT (Devlin et al.,, 2018  ###reference_b14###  ###reference_b14###; Brown et al.,, 2020  ###reference_b9###  ###reference_b9###). Specifically, the pretraining data is constructed based on . For clarity, we extract the language data without expert knowledge and write the collected data into a sequence of ordered tokens, i.e., sentences or paragraphs. For the -th sample , we write\nwith a length of , which contains  episodes with one task,  observations and  subgoals each. Following this, LLM’s pretraining dataset is autoregressively constructed with the expert guidance, denoted by , where  and let\nIn other words, when pretraining to predict the next subgoal, we replace the one sampled from the behavior policy with the one from the optimal policy. In practice, sentences with expert knowledge can be collected from online knowledge platforms such as Wikipedia (Merity et al.,, 2016  ###reference_b50###  ###reference_b50###; Reid et al.,, 2022  ###reference_b61###  ###reference_b61###). Following the pretraining algorithm of BERT and GPT, the objective is to minimize the cross-entropy loss, which can be summarized as  with\nand  is the pretrained LLM by algorithm in (3.7  ###reference_###  ###reference_###). More details are deferred to §5.1  ###reference_###  ###reference_###.\nTo pretrain translators, we employ a self-supervised contrastive learning approach, which aligns with celebrated vision-language models such as CLIP (Radford et al.,, 2021  ###reference_b59###  ###reference_b59###) and ALIGN (Jia et al.,, 2021  ###reference_b34###  ###reference_b34###). Let  be the contrastive pretraining dataset for translators, which is also constructed upon the dataset . Following the framework adopted in Qiu et al., (2022  ###reference_b58###  ###reference_b58###); Zhang et al., (2022  ###reference_b83###  ###reference_b83###), for each observation-state pair , a positive or a negative data point, labelled as  and , is generated with equal probability, following that\nPositive Data: Collect  with label .\nNegative Data: Collect  with label , where  is sampled from negative sampling distribution  that has a full support over the domain of interest.\nDenote  as the joint distribution of data collected by the process above. The learning algorithm follows that , where the contrastive loss  is defined as\nConsider function class  with finite elements with  serving as a set of candidate functions that approximates the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2###  ###reference_theorem2### for justification). Following this, the pretrained translator for the Reporter by the algorithm in (3.8  ###reference_###  ###reference_###) is thus defined as . More details are deferred to §5.2  ###reference_###  ###reference_###.\nIn (3.5  ###reference_###  ###reference_###), we assume that all pretraining data is generated from a joint distribution , and then split for pretraining of LLM and Reporter. In practice, the pretraining dataset for the Reporter can consist of paired observation-state data collected from any arbitrary distribution, as long as (i) the LLM and Reporter “speak” the same language, i.e., shared , and (ii) the coverage assumption can hold (see Assumption 5.6  ###reference_theorem6###  ###reference_theorem6###).\nAs an example, noise contrastive estimation (NCE, Gutmann and Hyvärinen,, 2010  ###reference_b25###  ###reference_b25###) is one of the most widely adopted objectives in contrastive representation learning. From the theoretical lens, to estimate unnormalized model  with , additional noise data is sampled from a reference distribution  and then estimate by maximizing  with  and . With slight modifications, we use a function class  to approximate the ratio  rather than the relative probability  itself. In practice, the most commonly used contrastive training objectives are variations of NCE and originated from the NLP domain (Schiappa et al.,, 2023  ###reference_b64###  ###reference_b64###) by sharing the same idea of minimizing the distance between the positive pair and maximizing the distance between the negative pairs."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "LLM Planning via Bayesian Aggregated Imitation Learning",
            "text": "In this section, we first demonstrate that LLMs can conduct high-level planning through  Bayesian aggregated imitation learning (BAIL) in §4.1  ###reference_###, leveraging the ICL ability of LLMs with the history-dependent prompts. However, depending solely on LLM’s recommendations proves insufficient for achieving sample efficiency under the worst case (see Proposition 4.3  ###reference_theorem3###). Following this, we propose a planning algorithm for Planner in §4.2  ###reference_###, leveraging LLMs for expert recommendations, in addition to an exploration strategy."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLM-Empowered Planning Algorithm",
            "text": "Following the arguments above, we propose a planning algorithm for the Planner within a perfect PAR system. From a high level, the process of task planning is an implementation of policies from imitation learning (Ross and Bagnell,, 2010  ###reference_b62###; Ross et al.,, 2011  ###reference_b63###) with two key distinctions: (i) Planner collaborates with LLM, a “nascent” expert that learns the hidden intricacies of the external world from updating prompts; (ii) different from behavior cloning or inverse RL, Planner does not aim to comprehend LLM’s behaviors. Instead, the imitation is accomplished during the offline pretraining, and Planner shall selectively adhere to LLM’s suggestions during online planning. Next, we show that task planning solely guided by LLMs fails to achieve sample efficiency in the worst case.\nSuppose that Definition 4.1  ###reference_theorem1### holds. Given any , there exists an HMDP and specific latent varibale  such that if Planner strictly follows LLM’s recommended policies in Proposition 4.2  ###reference_theorem2###, it holds that .\nPlease refer to §C.4  ###reference_### for a detailed proof.\n∎\nProposition 4.3  ###reference_theorem3### indicates that relying solely on LLMs for task planning can result in a suboptimal  regret in the worst case when . Thus, additional exploration is essential to discern the latent information about the external world, a parallel to the practical implementations in latent imitation learning (Edwards et al.,, 2019  ###reference_b18###; Kidambi et al.,, 2021  ###reference_b36###) and LLM-based reasoning (Hao et al.,, 2023  ###reference_b27###; Nottingham et al.,, 2023  ###reference_b53###). In practice, while the language model can guide achieving a goal, it’s important to note that this guidance is not grounded in real-world observations. Thus, as pointed out by Grigsby et al., (2023  ###reference_b24###), the information provided in narratives might be arbitrarily wrong, which highlights the need for exploration to navigate new environments effectively. Similar to -greedy algorithms (Tokic and Palm,, 2011  ###reference_b68###; Dann et al.,, 2022  ###reference_b13###), we provide a simple but efficient algorithm for LLM-empowered task planning. Algorithm 1  ###reference_### gives the pseudocode. In each episode, the Planner performs two main steps:\nPolicy Decision (): Randomly decide whether to execute the exploration policy  or follow the LLM’s recommendations within this episode with probability .\nPlanning with LLMs (): If Planner decides to follow the LLM’s recommendations, the subgoal is obtained by prompting LLMs with , equivalently sampling from . Otherwise, the Planner takes sub-goal from .\nIn conventional -greedy algorithms, explorations are taken uniformly over the action space , i.e., . Recent work has extended it to a collection of distributions (e.g., softmax, Gaussian noise) for function approximation (Dann et al.,, 2022  ###reference_b13###). Following this, we instead consider a broader class of exploration strategies that satisfy the -distinguishability property below.\nWe say an exploration policy  is -distinguishable if there exists an absolute constant  such that for all  with , it holds that .\nThe -distinguishability implies the existence of exploration policy  that could well-distinguish the models with an -gap in Hellinger distance concerning the distribution of whole trajectory, which also impose condition over the model seperation.\nNext, we introduce the assumption over priori.\nThere exists a constant  such that .\nThe assumption asserts a bounded ratio of priors, implying that each  has a non-negligible prior probability. The assumption is intuitive, as a negligible priori suggests such a scenario almost surely does not occur, rendering the planning in such scenarios unnecessary. Now, we are ready to present the main theorem of the Planner under perfect setting.\nSuppose that Definition 4.1  ###reference_theorem1### and Assumption 4.5  ###reference_theorem5### hold. Given an -distinguishable exploration policy  and , Algorithm 1  ###reference_### ensures\nfor any  and , if the Planner explores with probability .\nPlease refer to §C.2  ###reference_### for a detailed proof.\n∎\nTheorem 4.6  ###reference_theorem6### states that the Planner’s algorithm can attain a  regret for planning facilitated by LLMs. The multiplicative factor of the regret depends on the horizon of the interactive process , the reciprocal of coverage rate  in Definition 4.4  ###reference_theorem4###, and the logarithmic term  including both the cardinality of candidate models and the prior coverage in Assumption 4.5  ###reference_theorem5###, which jointly characterizes the complexity of the physical world.\nLee et al., (2023  ###reference_b39###) has demonstrated that a perfect decision-pretrained transformer, similar to the role of LLM in ours, can attain a  Bayesian regret, i.e., , via ICL. In comparison, we focus on a more challenging setting that aims to control the frequentist regret, which is closer to applications, and attain a comparable result with additional exploration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Performance under Practical Setting",
            "text": "We also present two extensions. In §B.1  ###reference_###, we discuss the design of Planner by taking LLMs as World Model (WM). Here, the Planner prompts the LLM to predict the next observation rather than subgoals, alleviating the reliance on expert knowledge. By leveraging model-based RL methods like Monte Carlo Tree Search (MCTS) and Real-Time Dynamic Programming (RTDP), the Planner utilizes the LLM-simulated environment to optimize its strategies based on the contextual information. As shown in Proposition B.1  ###reference_theorem1###, the simulated world model via ICL conforms to Bayesian Aggregated World Model (BAWM). Hence, the LLM Planner achieves a regret at rate of  under practical setting with regularity conditions (see Corollary B.3  ###reference_theorem3###). Besides, we extend the results in §4  ###reference_### to accommodate the scenario of multi-agent collaboration, i.e.,  Actors. In §B.2  ###reference_###, we formulate the probelm as a cooperative hierarchical Markov Game (HMG) and establish a theoretical guarantee of  under the perfect setting (see Corollary B.4  ###reference_theorem4###). These two extention correponds to recent works on LLM planning as world model (e.g., Hu and Shu,, 2023  ###reference_b31###) and muti-agent collaboration of LLM Agents (e.g., Mandi et al.,, 2023  ###reference_b49###)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Pretraining Large Language Model",
            "text": "In this subsection, we elaborate on the pretraining of LLMs using transformer architecture. We employ a supervised learning algorithm minimizing the cross-entropy loss, i.e., , as detailed in (3.8  ###reference_###). Following this, the population risk follows that\nwhere ,  is distributed as the pretraining distribution, and  is the Shannon entropy. As the minimum is achieved at , estimated  and  are expected to converge under the algorithm with a sufficiently large dataset. Specifically, our design adopts a transformer function class to stay consistent with the architectural choices of language models like BERT and GPT. Specifically, a transformer model comprises  sub-modules, with each sub-module incorporating a Multi-Head Attention (MHA) mechanism and a fully connected Feed-Forward (FF) layer. See §A.2  ###reference_### for further details, and we specify two widely adopted assumptions in the theoretical analysis of LLM pretraining (Wies et al.,, 2023  ###reference_b75###; Zhang et al.,, 2023  ###reference_b84###).\nFor all  and , there exists a constant  such that all  with  satisfies that  almost surely.\nThe boundedness assumption requires that the -norm of the magnitude of each token is upper bounded by , and such an assumption holds in most settings.\nFor all latent variable , there exists a constant  such that for all  and  with length , it holds .\nThe ambiguity assumption states that the generating distribution is lower bounded, and the assumption is grounded in reasoning as there may be multiple plausible choices for the subsequent words to convey the same meaning. Next, we present the performance of the pretrained LLMs.\nSuppose that Assumptions 5.1  ###reference_theorem1### and 5.2  ###reference_theorem2### hold. With probability at least , the pretrained model  by the algorithm in (3.7  ###reference_###) satisfies that\nwhere  and  features the tranformer’s architecture,  denotes the mixing time of Markov chain 222Note that  directly satisfies Markov property since  and thus  for all ., and  is the size of dataset . See §A.2  ###reference_### for detailed structure and definitions.\nPlease refer to Theorem 5.3 in Zhang et al., (2023  ###reference_b84###) for a detailed proof.\n∎\nTheorem 2  ###reference_te2### states that the total variation of the conditional distribution, with expectation taken over the average distribution of context  in  (see Table 1  ###reference_### for definition), converges at . Note that the first two terms represent the approximation error and deep neural networks act as a universal approximator (Yarotsky,, 2017  ###reference_b80###) such that the error would vanish with increasing volume of network (Proposition C.4, Zhang et al.,, 2023  ###reference_b84###). For notational simplicity, we denote the right-hand side of theorem as ."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Pretraining Observation-to-Language Translator",
            "text": "In this subsection, we focus on the pretraining of observation-to-language translators under a self-supervised learning architecture using the contrastive loss. Consider the function class\nwith finite elements, and the contrastive loss  in (3.8  ###reference_###) is then defined over . Note that the contrastive loss can be equivalently written as the negative log-likelihood loss of a binary discriminator, following that , where we define\nBased on (5.1  ###reference_###) and the algorithm , the population risk follows that\nAs the minimum is attained at , where  is the distribution of the label conditioned on the  pair in contrastive data collection, estimated  and  are expected to converge, and thus the learning target is the ground-truth likelihood ratio  (see Lemma D.2  ###reference_theorem2###). Below, we assume the learning target  is realizable in , which is standard in literature (Qiu et al.,, 2022  ###reference_b58###).\nGiven a designated negative sampling distribution , there exists  such that  for all .\nNext we present the performance of the pretrained translator.\nSuppose that Assumption 5.4  ###reference_theorem4### holds. With probability at least , the pretrained model  by the algorithm in (5.1  ###reference_###) satisfies that\nwhere let  and  denotes the cardinality of the function class .\nPlease refer to §D.1  ###reference_### for a detailed proof.\n∎\nTheorem 5.5  ###reference_theorem5### posits that the average expectation of the total variation of the translation distribution regarding  converges at . For notational simplicity, write the right-hand side of the theorem as . Furthermore, the algorithm also ensures a more stringent convergence guarantee concerning -divergence: ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Performance with Pretrained PAR System",
            "text": "In this subsection, we delve into the performance of task planning with pretrained PAR system. We first introduce the online coverage assumption, which pertains to the distribution of online planning trajectories under practical scenarios and trajectories in pretraining datasets.\nThere exists absolute constants  and  such that for all latent variable ,  and policy sequence  from the Planner, it holds that (i)  for all ordered sequence , where  for all , and (ii)  for all state .\nHere,  denotes the distribution of the dynamic system with the pretrained translator. The assumption asserts that (i) distribution of the ICL prompts induced by policy sequences  from the Planner under practical scenarios is covered by the pretraining data, where  denotes the number of episodes described in . (ii) all states  are covered by the average distribution of the Reporter’s pretraining dataset. Similar conditions are adopted in ICL analysis (Zhang et al.,, 2023  ###reference_b84###), decision pretrained transformer (Lee et al.,, 2023  ###reference_b39###; Lin et al., 2023b,  ###reference_b44###) and offline RL (Munos,, 2005  ###reference_b52###; Duan et al.,, 2020  ###reference_b17###). Intuitively, LLM and reporter cannot precisely plan or translate beyond the support of the pretraining dataset. These conditions are achievable if an explorative behavior strategy  is deployed with a sufficiently large  when collecting data. We then present the main theorem regarding the practical performance.\nSuppose that Assumptions 4.5  ###reference_theorem5###, 5.1  ###reference_theorem1###, 5.2  ###reference_theorem2###, 5.4  ###reference_theorem4### and 5.6  ###reference_theorem6###. Given an -distinguishable exploration policy  and , under the practical setting, the Planner’s algorithm in Algorithm 1  ###reference_### ensures that\nfor any  and . The cumulative pretraining error of PAR system follows that\nwhere  are defined in Definition 4.4  ###reference_theorem4### and Assumption 5.6  ###reference_theorem6###, and pretraining errors  and  are defined in Theorem 2  ###reference_te2### and Theorem 5.5  ###reference_theorem5###. Under the practical setting, Planner should explore with probability .\nPlease refer to §D.2  ###reference_### for a detailed proof.\n∎\nTheorem 5.7  ###reference_theorem7### reveals that, in comparison to perfect scenario, the Planner can achieve an approximate  regret, but incorporating an additional pretraining error term that could diminishe with an increase in the volume of pretraining data. Besides, it further underscores the necessity of exploration, where the Planner should explore with an additional  to handle the mismatch between the ground-truth and the pretrained environment.\nThe challenge of establishing a performance guarantee in a practical setting arises from the mismatch between the ground-truth environment and the pretrained one, leading to a distributional shift in posterior probability. Besides, BAIL is realized through a pretrained LLM, which introduces its pretraining error inaddition. In response, we propose a novel regret decomposition and provide the convergence rate of posterior probability with bounded pretraining errors, distinguishing ours from the previous results in Lee et al., (2023  ###reference_b39###); Liu et al., (2023  ###reference_b48###).\nWe also present two extensions. In §B.1  ###reference_###  ###reference_###, we discuss the design of Planner by taking LLMs as World Model (WM). Here, the Planner prompts the LLM to predict the next observation rather than subgoals, alleviating the reliance on expert knowledge. By leveraging model-based RL methods like Monte Carlo Tree Search (MCTS) and Real-Time Dynamic Programming (RTDP), the Planner utilizes the LLM-simulated environment to optimize its strategies based on the contextual information. As shown in Proposition B.1  ###reference_theorem1###  ###reference_theorem1###, the simulated world model via ICL conforms to Bayesian Aggregated World Model (BAWM). Hence, the LLM Planner achieves a regret at rate of  under practical setting with regularity conditions (see Corollary B.3  ###reference_theorem3###  ###reference_theorem3###). Besides, we extend the results in §4  ###reference_###  ###reference_### to accommodate the scenario of multi-agent collaboration, i.e.,  Actors. In §B.2  ###reference_###  ###reference_###, we formulate the probelm as a cooperative hierarchical Markov Game (HMG) and establish a theoretical guarantee of  under the perfect setting (see Corollary B.4  ###reference_theorem4###  ###reference_theorem4###). These two extention correponds to recent works on LLM planning as world model (e.g., Hu and Shu,, 2023  ###reference_b31###  ###reference_b31###) and muti-agent collaboration of LLM Agents (e.g., Mandi et al.,, 2023  ###reference_b49###  ###reference_b49###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we embedded the LLM-empowered decision-making problem into a hierarchical RL framework named PAR system where at the high level, the LLM Planner decomposes the user-specified task into subgoals, and at the low level, the Actor(s) translate the linguistic subgoals into physical realizations while also providing feedbacks for augmenting the planning process through a trained reporter.\nUnder the perfect setting, we characterize the BAIL nature of the LLM-aided planning pipeline and the nessecity of exploration even under expert guidance. We also shed light on how the training errors of both LLM and reporter enter the ICL error under practical scenarios."
        }
    ]
}