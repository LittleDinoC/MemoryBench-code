{
    "title": "Clarinet: Augmenting Language Models to Ask Clarification Questions for Retrieval",
    "abstract": "Users often make ambiguous requests that require clarification. We study the problem of asking clarification questions in an information retrieval setting, where systems often face ambiguous search queries and it is challenging to turn the uncertainty in the retrieval model into a natural language question.\nWe present Clarinet, a system that asks informative clarification questions by choosing questions whose answers would maximize certainty in the correct candidate.\nOur approach works by augmenting a large language model (LLM) to condition on a retrieval distribution, finetuning end-to-end to generate the question that would have maximized the rank of the true candidate at each turn.\nWhen evaluated on a real-world retrieval dataset of users searching for books, our system outperforms traditional heuristics such as information gain on retrieval success by 17% and vanilla prompted LLMs by 39% relative.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Natural language is a flexible interface for users to interact with systems, but language is inherently ambiguous and users themselves may not know what they want.\nAs a result, systems must handle underspecified queries.\nWe study this in an information retrieval setting, where search ambiguity is a well-studied challenge Keyvan and Huang (2022  ###reference_b6###).\nWhile modern large language models (LLMs) can ask coherent clarification questions, they do not always ask questions that elicit information specifically about what the model is uncertain about.\nThis is particularly challenging in the case of retrieval, where it is unclear how to integrate the conversational abilities of an LLM with the external database or the retrieval system that represents the uncertainty over search candidates.\nIn contrast, approaches that use principled information theoretic measures such as information gain Oaksford and Chater (1994  ###reference_b9###); Van Rooy (2004  ###reference_b14###); Nelson et al. (2010  ###reference_b8###); Rothe et al. (2017  ###reference_b13###) or KL utility Nelson et al. (2010  ###reference_b8###); Hawkins and Goodman (2017  ###reference_b3###) can use the retriever distribution to evaluate the right questions to ask and explicitly select questions that reduce uncertainty. However, these methods require expensive inference-time generation and evaluation of potential questions and have not yet been able to scale beyond toy settings.\nIn this work, we investigate whether we can learn to ask good questions simply by distilling the search over good questions into an end-to-end model.\nWe present Clarinet, a framework for learning to ask clarification questions for information retrieval. Our approach works by augmenting a language model to condition on the retriever distribution and then finetuning the system end-to-end to generate informative questions.\nWe select informative questions by simulating interactions with a prompted LLM that acts as a user proxy, training only on questions that would have significantly increased the confidence in the true item if answered.\nIn contrast to heuristic-based methods, Clarinet distills the expensive, explicit search over questions at inference time into the model.\nThen, given user responses to clarification questions, we summarize the interaction history into a language posterior, a single natural language query describing what the system knows about the user’s desired item. We use this query to re-rank candidate items from the database.\n###figure_1### We evaluate our approach on a real-world dataset of users asking for help on an online forum (Goodreads) to find books they vaguely recall from a database of thousands of items, e.g., “written by a former journalist, takes place in NYC, involves a necklace” (Bhargav et al., 2022  ###reference_b1###). We evaluate systems interactively against a simulated user, which we implement as a prompted LLM that can answer clarification questions with oracle access to the true item.\nCompared to a purely dialogue-based approach that generates clarification questions prompted only with the dialogue history, our system asks better questions to achieve a 39% relative gain in top-1 retrieval accuracy after ten interactions with the simulated user.\nOur model also outperforms heuristic-based approaches that select a clarification question from a candidate pool with information gain or KL utility by 17% relative on top-1 retrieval accuracy, while being much simpler and cheaper at inference time."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Interactive NLP systems have used clarification questions as way to gather more information from users in settings such as classifying Yu et al. (2019  ###reference_b16###), conversational question answering Rao and Daumé III (2018  ###reference_b11###), and visually grounded 20 questions games White et al. (2021  ###reference_b15###). To select informative questions, much of the work draws heuristics that maximize expected information gain from simulated answers.\nSome of the works shift the clarification question generation and selection from using rule-based and heuristic methods to reinforcement learning. Rao and Daumé III (2019  ###reference_b12###) extended their utility used in Rao and Daumé III (2018  ###reference_b11###) in a reinforcement setting to generate useful and relevant questions. Meanwhile, Pyatkin et al. (2022  ###reference_b10###) presented an interactive system that asks relevant and informative clarification questions to learn salient contexts of a social or moral situation. Their approach to question generation utilized reinforcement learning, aiming to optimize the generation of questions that elicit responses containing morally relevant contexts.\nIn a more practical scenario, Zamani et al. (2020  ###reference_b17###) suggests both supervised and reinforcement learning models to generate clarifying questions that aid users in formulating search queries in search engines. Additionally, they explore techniques for generating potential answers for each clarifying question, allowing users to conveniently choose from a predefined set of answers. However, if the users don’t know what they are looking for, the predefined answers may not be very helpful."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We train and evaluate our system in a “tip of the tongue” (TOT) search setting, where users are searching for items they vaguely remember but for which they cannot formulate precise queries. To train and evaluate clarification question generation, we use the dataset from Bhargav et al. (2022  ###reference_b1###), which contains multi-turn interactions from community forums where users post queries searching for books where they were unable to find using conventional search engines. We filter the data specifically about book queries, resulting in 784 interactions, which we split into 156 for training and 628 for evaluation. The retrieval database consists of a corpus of 1877 documents.\nIn general, Clarinet is agnostic to the choice of retrieval model. In our experiments, we use Dense Passage Retriever (DPR) Karpukhin et al. (2020  ###reference_b5###) as the retriever over the book database.\nWe finetune the retriever for this task with the larger scale WhatsThatBook dataset from Lin et al. (2023  ###reference_b7###), which contains 11,552 initial query-document pairs for TOT book retrieval.\nWe use dot-product similarity between the DPR representation of the book information (book title, author, metadata, synopsis) and the DPR representation of a query synthesized by our Clarinet model (as described in Section 3.1  ###reference_###) to rank candidate books.\nAt each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user’s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B  ###reference_### for the prompt.\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target book.\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target’s rank high or improves the rank the most.\nOnly the questions that help the system to rank the target book as top 10, or help increase the rank by 10 will be used in the training. Users’ initial queries, past interactions, and the information of top book candidates (book candidates with retrieval probabilities that add up to 50% [no more than 3 books]) will be served as the model input.\n###figure_2### To produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\nWe train Flan-T5-base Chung et al. (2022  ###reference_b2###) to generate the selected clarification questions.\nThe model takes in the user’s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\nFor the books dataset, the information for each retrieval candidate includes the book title, author, published dates, and description. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for a book.\nWe use a Fusion-in-Decoder architecture (FiD) Izacard and Grave (2020  ###reference_b4###), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently.\nThe encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of , batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation.\n###figure_3###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Asking Clarification Questions with Clarinet",
            "text": "The retrieval dataset itself consists of initial (underspecified) queries from real users, and the true item they were searching for.\nTo train a Clarinet model, we transform a retrieval dataset into an interaction dataset by synthesizing a series of dialogue interactions with a user simulator model. Each interaction is seeded with a real user query from the dataset; we then generate a series of clarification questions and answers from the user simulator model.\nAt each turn, we generate a pool of candidate questions and filter for the most informative questions for finding the user’s desired items—i.e., the questions whose answers would increase the rank of the true candidate the most.\nThe clarification dataset thus consists of (interaction history , ) for each informative question  out of the candidate pool.\nWe finetune a LLM to generate these questions  conditioned on the interaction history and retrieval distribution.\nFor the TOT dataset, we run five 10-turn games for each initial user query.\nAt inference-time, we directly sample from the model to generate questions to ask the user. We update the retrieval distribution after each user response by condensing the interaction history thus far into a language posterior, a single retrieval query that we use to re-retrieve results from the database.\nAt each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user’s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B  ###reference_###  ###reference_### for the prompt.\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target book.\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target’s rank high or improves the rank the most.\nOnly the questions that help the system to rank the target book as top 10, or help increase the rank by 10 will be used in the training. Users’ initial queries, past interactions, and the information of top book candidates (book candidates with retrieval probabilities that add up to 50% [no more than 3 books]) will be served as the model input.\n###figure_4### To produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\nWe train Flan-T5-base Chung et al. (2022  ###reference_b2###  ###reference_b2###) to generate the selected clarification questions.\nThe model takes in the user’s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\nFor the books dataset, the information for each retrieval candidate includes the book title, author, published dates, and description. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for a book.\nWe use a Fusion-in-Decoder architecture (FiD) Izacard and Grave (2020  ###reference_b4###  ###reference_b4###), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently.\nThe encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of , batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation.\n###figure_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our experiments, we aim to answer the following questions:\nHow does our method compare to well-studied methods for asking both quantitatively (how effectively do our system’s questions help us narrow down the user’s item) and qualitatively (how do our system’s questions differ)?\nHow important is conditioning on information from the retriever vs. purely dialogue- or prompting-based approaches that reason purely in text and generate questions conditioned only on the dialogue history?\nWhat types of questions are helpful in increasing confidence in the target and ultimately achieving successful retrieval?\nIn this section, we present the system’s empirical performance by evaluating it in a book retrieval setting. In one retrieval game, the system will be given an initial query that vaguely describes the book that the user is looking for. The system will then be allowed to ask 9 clarification questions to identify the book (a total of 10 turns including the initial query).\nWe run experiments with a simulated user, which we implement as a GPT-3 model prompted with oracle information about the target book, removing the title so that the user simulator cannot output the name of the target item outright. We prompt the user simulator to mimic a user is searching for the book with a vague memory about the content.\nWe measure top-1 retrieval accuracy since there is one correct retrieval item. We compare model performance on cumulative retrieval success, where an interaction up to turn  is counted as successful if the correct item is retrieved at any of turn  and non-cumulative retrieval success, where the interaction is successful only if the correct item is the top candidate at turn .\nWe additionally show trends for mean reciprocal rank (MRR), the average reciprocal rank of the correct item, to contextualize model performance beyond top-1."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare Clarinet to a purely dialogue-based approach that randomly generates a question conditioned only on the dialogue history, without access to the retriever distribution. This model simply randomly samples a question from the pool of candidate questions generated by GPT-3, prompted with the initial query and dialogue history.\nAdditionally, we implement two information theoretic approaches to clarification question generation frequently used in prior work Rao and Daumé III (2018  ###reference_b11###); Yu et al. (2019  ###reference_b16###); White et al. (2021  ###reference_b15###). These approaches typically generate a pool of candidate questions at each turn, using different notions of question usefulness to select the best question to ask.\nIn contrast to Clarinet, because these approaches explicitly evaluate candidate questions, they are much more expensive at inference time.\nWe implement question selection based on expected information gain and Kullback-Liebler (KL) divergence, which we describe in more detail below."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Expected Information Gain (EIG)",
            "text": "Expected information gain (EIG) selects clarification questions that are most likely to yield the most information, in expectation over potential user responses. Formally, at turn  we want to choose the question  that optimizes:\nwhere  is the model’s current posterior belief over the true item  with the information\nup to turn .\n is the information gain (or equivalently, entropy reduction) in the candidate distribution  from observing answer , given that we asked :\nThus, to find the most informative question, the optimization problem can be simplified to:\nwhere the answer distribution is obtained by marginalizing over the current belief distribution  (subscript  omitted):\nTo compute the answer distribution , we answer every question  for every candidate  with a pre-trained Flan-T5-base model. The answer probabilities  are estimated by softmaxing over the summation of logits of tokens across every generation step.\n###figure_6###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 KL Divergence",
            "text": "The previous heuristic function helps pick the question candidate with the highest expected information gain. However, when there are only a few books with high confidence, the question selector using EIG will only try to select a question that differentiates between the top candidates. As a result, the selected questions in the subsequent turns can become very similar. Therefore, we’d also like a heuristic function that selects the question that is likely to change the current belief distribution.\nTo estimate the posterior after observing an answer to a question , , we answer every question  for every candidate item with a pretrained Flan-T5-base model, like the EIG baseline.\nis estimated by computing the cosine similarity between the answers calculated from different (question, book) pair.\nwhere  is the language embedding of the referenced answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In Figure 2  ###reference_###, we show that Clarinet achieves higher retrieval success than the dialogue-only baseline that randomly selects questions given only the dialogue context, as well as outperforming the information theoretic approaches. The performance of random question selection plateaus after a certain number of turns."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_7### We evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes’ Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target’s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target’s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "Next, we investigate the effect of the language posterior, FiD architecture, and gain in rank for train-time question selection.\nWe evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes’ Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_###  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target’s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target’s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion & Conclusion",
            "text": "Traditional heuristic approaches like EIG are often shown useful in the context of closed-form question selections where there is only a fixed number of vocabulary/answers and a limited amount of questions.\nThe computation time increases dramatically if the answer becomes open-ended. Although the retrieval performance using KL as the question selection function is decent, it needs to estimate the resulting confidence distribution of every question in the candidate pool. It will be very costly and inefficient to run in a real-time interactive system.\nWe presented an interactive retrieval system that helps users retrieve books by asking open-ended clarification questions, finetuning a LLM to generate informative questions end-to-end.\nOur Clarinet model adopts an architecture that encodes the query, interactions, and passages separately so that the model could learn to ask questions that help identify the target more quickly with limited training data.\nWe show that this approach can effectively distill the search over questions into the model, resulting in much cheaper inference while outperforming methods like EIG and KL that explicitly evaluate the usefulness of clarification questions at inference time."
        }
    ]
}