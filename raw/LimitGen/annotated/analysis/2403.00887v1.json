{
    "title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech",
    "abstract": "The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments suggest that Multi-output models perform comparably to individual models, efficiently capturing the intricate relationships between variables and speech inputs, all while achieving improved runtime.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In our increasingly digital world, the ability to glean profound insights from the nuances of human voices has assumed paramount importance. This paper delves into the captivating domain of predicting age, gender, and emotion based on vocal cues, a multidisciplinary field teeming with far-reaching applications.\nVoice analysis technologies have rapidly advanced, bringing transformative breakthroughs to various fields [1  ###reference_b1###]. These advancements not only optimize customer interactions but also hold the potential to revolutionize healthcare diagnostics, fundamentally altering our comprehension and engagement with human communication. In mental healthcare, the ability to discern emotions offers an opportunity to improve emotional and behavioral disorders [2  ###reference_b2###, 3  ###reference_b3###]. Moreover, the application of this technology extends to the retail sector, where it enhances the consumer experience [4  ###reference_b4###]. Simultaneously, the ability to detect age and gender variables finds relevance in evaluating the mental health requirements of distinct demographic groups [5  ###reference_b5###]. It also plays a crucial role in e-services and policy formulation [6  ###reference_b6###]. Thus, the ability to discern age, gender, and emotion from voice data emerges as a multifaceted field with a plethora of discernible applications."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Prior art",
            "text": "This research endeavour embarks on a comprehensive exploration of advanced deep learning architectures tailored for the prediction of age, gender, and emotional states. Additionally, it undertakes an exhaustive comparative analysis, meticulously examining diverse methodologies. These methodologies include individual models, where a single model is used to predict a single variable; multi-output models, which employ a single model to predict all three variables simultaneously; and sequential models, which cascade individual models to create a sequence representing the three variables at distinct stages. This scrutiny aims to elucidate the effectiveness of these approaches in the vital task of detecting these three pivotal variables.\nBefore beginning the experiments, addressing the critical challenge of sourcing a dataset that encompasses all three target labels was necessary. Consequently, a rigorous review of popular, openly available speech datasets was undertaken to test the models. Datasets like RAVDESS and IEMOCAP [7  ###reference_b7###, 8  ###reference_b8###] were found to have emotion labels but lacked age and gender annotations. The TESS dataset [9  ###reference_b9###] featured only two speakers, rendering it unsuitable for our purposes despite including all three variables. Similarly, the DES dataset [10  ###reference_b10###] comprised just four speakers, limiting its utility. The Common Voice dataset [11  ###reference_b11###] had age and gender labels but omitted emotion labels. As a result, the CREMA-D and EMO-DB datasets [12  ###reference_b12###, 13  ###reference_b13###] were identified as the only suitable sources containing all three labels and adequate data. An aggregate dataset was created by amalgamating these two datasets to facilitate our experiments.\nPrior research has produced remarkable results in predicting individual variables using dedicated models. For instance, [14  ###reference_b14###, 15  ###reference_b15###] achieved impressive accuracy rates of 90.47% and 92.73% on CREMA-D for emotion prediction using 1D CNNs and other deep learning architectures like GRU and LSTMs. Meanwhile, [16  ###reference_b16###] attained an accuracy of 76.83% using their copy-pasta architecture. Furthermore, [17  ###reference_b17###] detailed a multilingual speech-based gender classification method employing time-frequency features and the SVM classifier, achieving 81% classification accuracy on the EMO-DB dataset for gender classification. Several studies [18  ###reference_b18###, 15  ###reference_b15###] have explored the prediction of age and gender from speech, employing diverse methodologies involving MLPs and CNNs. Notably, limited research has been conducted on predicting all three variables—namely, age, emotion, and gender—from speech data.\nWhile [19  ###reference_b19###] proposed a \"one-source-to-detect-all\" solution for predicting age, gender, and emotion from one model, we identified methodological flaws in their approach. Specifically, these studies utilized three separate datasets to train three distinct tasks, a strategy that may not be optimal for enabling a model to discern all three variables from a single speech source.\nThe motivation behind employing multi-output models lies in their ability to concurrently specify the relationships between multiple outcome variables (Y) and feature variables (X). As underscored by [20  ###reference_b20###], this approach acknowledges the potential interdependencies between these outcome variables, resulting in enhanced statistical power or improved predictive accuracy. Moreover, [20  ###reference_b20###] observed that variations in dependency structures within the covariates influenced the performance of predictive methods. Notably, this impact was consistent for both univariate and multivariate approaches, indicating that neither approach held a definitive advantage concerning the dependency structures within the covariates.\nGiven these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.\nThe rest of the article is structured as follows: Section 2 details the methodology of various models used in this experiment, discusses the database and explains the feature extraction along with our novel SEGAA model. Section 3 elaborates on the results based on the experimental study. Section 4 presents the conclusion of this article."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_1### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_2### The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_3### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for ’emotion’ and ’age’ predictions and binary softmax activation for ’gender’ predictions. The hyperparameters governing the model’s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: ’emotion,’ ’age,’ and ’gender,’ utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_4###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Description",
            "text": "Given these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.\nThe EMO-DB database, a publicly accessible German emotional database, was developed by the Institute of Communication Science at the Technical University in Berlin, Germany. The data collection involved ten proficient speakers, equally distributed between genders (five males and five females). The database comprises 535 speech utterances covering seven distinct emotions: anger, boredom, anxiety, happiness, sadness, disgust, and neutrality.\nAlong with the EMO-DB dataset, the CREMA-D dataset was also used. CREMA-D consists of 7,442 authentic speech clips from a diverse group of 91 actors. The actors include 48 males and 43 females, with ages ranging from 20 to 74. They represent diverse racial and ethnic backgrounds, including African American, Asian, Caucasian, Hispanic, and Unspecified. During the recording process, the actors delivered a set of 12 distinct sentences, each expressed with one of six specific emotions: Anger, Disgust, Fear, Happiness, Neutrality, and Sadness. The study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class.\nThe primary goal with respect to the data augmentation part of this study is to enhance the diversity and quality of the dataset to improve the performance of emotion recognition models. The age and gender of the speakers were extracted from the speaker information provided and appended to the corresponding data sample. Consequently, our comprehensive dataset comprised a fusion of the EMO-DB and CREMA-D datasets.\nThe study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class. Simultaneously, we omitted data samples with the label \"boredom\" as we narrowed our scope to include the following emotions: Anger, Disgust, Fear, Happiness,\nNeutrality, and Sadness.\n###figure_5###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Feature Extraction / Pre-processing",
            "text": "We implemented audio processing functions to augment the original data samples. These functions include noise addition, time stretching, pitch shifting, and signal shifting. The noise addition process introduces random noise to the audio data, while time stretching alters the duration of the samples. Pitch shifting, on the other hand, modifies the audio pitch, and signal shifting shifts the audio signal along the time axis.\nOur feature extraction pipeline involves calculating essential audio features such as the Zero-Crossing Rate (ZCR), Root Mean Square Energy (RMSE), and Mel-frequency cepstral coefficients (MFCC). These features are widely used in audio and speech processing to capture relevant characteristics of the audio signals.\nThe augmentation techniques were applied to the original audio samples to construct the final dataset. Audio features were extracted for each sample, including the original data and its augmented versions. This process enabled the generation of diverse features, encompassing pitch, duration, and signal properties variations. The resulting dataset showcases a significant increase in size and diversity.\nWe split the dataset into input features (X) and target labels (Y). Y was one-hot encoded to represent the categorical variables. The dataset was then divided into training, testing, and validation sets, using a stratified approach to ensure class distribution balance. Input features were standardized using the Standard Scaler to enhance model convergence and performance. To maintain consistency, the testing and validation sets were transformed using the scaler fitted to the training set. This is then used to train and test the MLP and the SEGAA models, which finally output the age, gender and emotion results as described in Figure 1. The results of the respective models will then be compared and inferred from.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_6### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_7### The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_8### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for ’emotion’ and ’age’ predictions and binary softmax activation for ’gender’ predictions. The hyperparameters governing the model’s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: ’emotion,’ ’age,’ and ’gender,’ utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_9###"
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Individual Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_10### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for ’emotion’ and ’age’ predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the ’gender’ prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_11###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Multi-output Models",
            "text": "The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_12### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for ’emotion’ and ’age’ predictions and binary softmax activation for ’gender’ predictions. The hyperparameters governing the model’s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: ’emotion,’ ’age,’ and ’gender,’ utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_13###"
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Sequential Models",
            "text": "In this experimental setup, the individual models described in Section 3.3.1 were cascaded sequentially to predict the first variable from the speech features, followed by predicting the second variable from the speech features and the previously predicted first variable, and finally predicting the third variable from the speech features and the previously predicted second variable as described in figure 5. This experiment was conducted in three different sequences: first for the variables emotion, gender, and age; then for gender, age, and emotion; and lastly for age, emotion, and gender.\n###figure_14###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "This research investigates the task of speech-based detection of emotion, gender, and age using a diverse range of machine-learning models. Our primary objective is to develop accurate and robust models for classifying these three attributes from audio data. The experimental framework includes individual and multi-output Multi-Layer Perceptron (MLP) as well as SEGAA architectures. Additionally, we investigate unique sequences for predicting these attributes with our SEGAA models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Individual Models",
            "text": ""
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Emotion Detection",
            "text": "For emotion detection, both individual SEGAA and MLP models exhibit robust performance. The SEGAA model attains an accuracy of 96%, while the MLP model achieves 94% accuracy. These models demonstrate well-balanced precision, recall, and F1 scores, ranging from 0.94 to 0.96."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Gender Detection",
            "text": "The individual models demonstrate exceptional accuracy for gender detection. The SEGAA model attains a flawless accuracy of 100%, while the MLP model achieves 98%. Precision, recall, and F1 scores consistently reach 1.00 for both models."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Age Detection",
            "text": "Age detection, too, benefits from the individual SEGAA and MLP models, with the SEGAA model achieving 95% accuracy and the MLP model reaching 92%. These models display high precision, recall, and F1 scores, ranging from 0.92 to 0.95."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multi-output Models",
            "text": "Our investigation extends to multi-output SEGAA and MLP models, which tries to simultaneously predict emotion, gender, and age in that order. These models yield competitive results with those of the individual models, with accuracy values spanning from 84% to 99% across generations. Although the multi-output SEGAA Gen-0 model maintains strong precision and F1 scores, there is a slight decrease in recall for certain attributes, particularly in emotion detection. In contrast, the proposed SEGAA model effectively addresses these limitations, demonstrating exceptional performance across all aspects, encompassing emotion, age, and gender detection, which is highlighted in Figure 7."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In our experimentation, we conducted a rigorous comparative assessment of univariate and multi-output models to predict gender, age, and emotion from speech data, a crucial task with applications spanning various domains. Our analysis unveiled a noteworthy phenomenon associated with sequentially chaining univariate models: this approach increased error propagation as the inaccuracies generated by preceding models were amplified in subsequent stages. Despite this, univariate models exhibited slightly superior accuracy and F1 scores performance compared to multi-output models.\nIt is essential to emphasize that our experimental findings suggest that SEGAA demonstrates a level of predictive capability comparable to univariate models. Notably, it excels in capturing the intricate interrelationships between the variables and speech inputs with commendable efficiency. Furthermore, SEGAA achieves this without compromising runtime efficiency, making them an attractive alternative for addressing the complexities of predicting gender, age, and emotion from speech data. These insights, we believe, hold valuable implications for both researchers and practitioners in the field."
        }
    ]
}