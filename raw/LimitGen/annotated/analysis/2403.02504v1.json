{
    "title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing1footnote 11footnote 1Our replication package can be accessed at https://doi.org/10.7910/DVN/QTT84C.",
    "abstract": "The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rise of pretrain-finetune paradigm has greatly reshaped the landscape of natural language processing (NLP) (E. Hu \\BOthers., \\APACyear2022  ###reference_b3###). This new paradigm, characterized by the application of large pretrained language models, most notably BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###), and high efficacy on finetuned tasks even in the face of relatively few training samples, is now being widely applied in social sciences (Zhang \\BOthers., \\APACyear2021  ###reference_b18###; Wang, \\APACyear2023\\APACexlab\\BCnt1  ###reference_b15###, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###). While earlier works have recommended a minimum of 3,000 samples for NLP tasks using the bag-of-words approach (Wang \\BOthers., \\APACyear2022  ###reference_b17###), recent research applying the pretrain-finetune paradigm has shown that finetuned large models with just a few hundred labeled samples could yield competitive performance (Wang, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###).\n###figure_1### In the current tutorial, we aim to provide (1) an overview of the pretrain-finetune paradigm and (2) illustrative applications of the pretrain-finetune paradigm to research questions in social sciences (Figure 1  ###reference_###). We first provide an introduction to the key concepts in pretraining and finetuning, such as tokenization and encoding. We then use some of the most recent applications of the pretrain-finetune paradigm in social sciences to illustrate how to use this paradigm to advance quantitative research in our discipline."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Pretrain",
            "text": "Pretraining is the process of training a model with unlabeled raw data with no particular downstream tasks. Some of the most widely used pretrained models include BERT and RoBERTa. These large language models usually contain hundreds of millions of parameters. Pretraining these models from scratch requires access to large amounts of raw data and specialized hardware like graphics processing units (GPUs). The pretraining process consists of the following steps: (1) tokenization, (2) encoding, and (3) pretraining tasks, such as masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence-order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "Unlike earlier methods such as bag of words (Wang \\BOthers., \\APACyear2022  ###reference_b17###) or word embeddings (Mikolov \\BOthers., \\APACyear2013  ###reference_b10###), large langauge models mostly use subwords as a token, such as wordpieces in BERT (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and Byte-Pair Encoding (BPE) in RoBERTa (Liu \\BOthers., \\APACyear2019  ###reference_b8###) and GPT-3 (Brown \\BOthers., \\APACyear2020  ###reference_b1###). To illustrate how words are broken into subwords, let’s use a text snippet from the Journal’s description and tokenize it using the BPE tokenizer used in GPT-3.5 and GPT-4.222The snippet is taken from https://journals.sagepub.com/description/AMP. The tokenizer is available at https://platform.openai.com/tokenizer and was accessed on January 6th 2024..\nThe word “AMPPS” is split into “AM”, “PP” and “S”. The word “methodological” is split into “method”, “ological”. And lastly the word “non-methodology” is split into “non”, “-method”, and “ology”. All other words, including punctuation marks, remain individual units without further splitting. These units are then turned into tokens represented with non-negative integers.\nThese units are then turned into tokens represented with integers."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Encoding",
            "text": "Once we have tokenized the input text into tokens, we then retrieve the corresponding embeddings for these tokens, where the token id, a non-negative integer, would serve the retrieval key. These token embeddings are vector representations. For the BERT-large model, each such vector contains 1024 float numbers. These sets of vectors are then fed into the encoder layers.333BERT models use encoders and are the focus of this tutorial. Note that GPT models use decoders.\n###figure_2### The key component of the encoders is self-attention (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###). Intuitively, it computes how much attention each token pays to the other tokens and generates the new representation for a token by calculating the weighted average of all the tokens where the weight is based on self-attention.444For a similar illustration, readers can refer to Kjell \\BOthers. (\\APACyear2023  ###reference_b6###). For a more in-depth illustration, please refer to the original paper (Vaswani \\BOthers., \\APACyear2017  ###reference_b13###) and the tutorial on transformers at http://jalammar.github.io/illustrated-transformer/. Such encoders are then put on top of each other in a sequential manner. For example, in BERT-large models there are 24 encoder layers. In BERT-base models there are 12 encoder layers."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Pretraining tasks",
            "text": "Now that we have a good understanding of tokenization and encoding, let’s proceed to discuss how these parameters in these layers are trained using pretraining tasks. The goal is to train these large language models from scratch and embue them with world knowledge that exists in the training corpus (Longpre \\BOthers., \\APACyear2020  ###reference_b9###). Commonly used pretraining tasks include masked language modeling, next sentence prediction (Devlin \\BOthers., \\APACyear2019  ###reference_b2###) and sentence order prediction (Lan \\BOthers., \\APACyear2020  ###reference_b7###).555Later research has shown that the next sentence prediction does not contribute much to the pretraining process (Liu \\BOthers., \\APACyear2019  ###reference_b8###).\n###figure_3### Let’s see an example of how masked language modeling works. Suppose we have the following input text: “I enjoy trying new activities”. During training, some of the tokens will be randomly masked and the language model is tasked with predicting these masked tokens. In Figure 3  ###reference_###, the token “activities” is masked. The language model then is tasked with predicting this masked token. All tokens in the vocabulary are eligible candidates. Suppose the model predicts “activities” with a probability of 0.2. If we use cross entropy loss, then the loss term for this prediction is -log(0.2), which is its negative log-likelihood.666For details, please see https://github.com/google-research/bert/blob/master/run_pretraining.py#L303. Note that the higher probability the model assigns to the corrected token, the lower the loss. The language model is trained on this task over the entire corpus for a few times until the loss in prediction stops decreasing."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Finetuning",
            "text": "Finetuning is the process of training an already-pretrained model on a downstream task. It is characterized by (1) adding a small set new parameters to accommodate the new tasks and (2) a relatively small learning rate given the fact that the vast majority of the parameters are already reasonably trained during the pretraining stage.777In terms of training procedures, it is the same as training a supervised model from scratch. Readers could refer to a recent tutorial on how to train supervised models by Pargent \\BOthers. (\\APACyear2023  ###reference_b12###). Downstream tasks at this stage can be broadly grouped into classification and regression. Examples of classification tasks include sentiment analysis and topic classification; examples of regression include fatality prediction. This is the step where quantitative psychologists need to apply a pretrained model to a particular task, such as depression detection and personality classification. For each specific task, researchers need to specify the model’s input, targeted output (including the number of labels), and whether this is a classification task or a regression task.\n###figure_4### In Figure 4  ###reference_###, we illustrate how a k-class classification works. From the encoder layers, we retrieve a 1 by N vector for each input sample, where N is 768 in the case of BERT-base and 1024 in the case of BERT-large. Then we project this vector to 1 by K using an N by K matrix. This layer is often referred to as the fully connected layer. What follows next depends on whether the task is classification or regression. In the case of classification, where K is equal to or greater than 2, we apply a softmax function such that each of the K elements represents the probability of that corresponding class being the predicted class:888For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html.\nIn the case of regression, where K is 1, we can apply the loss function directly to the 1 by K vector (now 1 by 1). Mean squared error is a commonly used loss function for regression tasks.999For implementation details, refer to https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Practical Exercises",
            "text": "In this section, we provide two practical exercises to illustrate how researchers can finetune a large language model with relative ease to achieve state-of-the-art results in classification and regression, respectively.101010Some of the original results on classification and regression have been published at Political Analysis. All our exercises are written in Python in the format of Jupyter notebooks so that readers can follow along in an interactive manner.111111https://jupyter.org. For easy reproducibility and access to computation resources, all our computation is done on Google Colab.121212https://colab.research.google.com."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Regression",
            "text": "In regression tasks, researchers use an input text to predict a numerical value. In the following exercise, we predict the natural logarithm of fatalities on a country month level using the CrisisWatch text (Häffner \\BOthers., \\APACyear2023  ###reference_b5###). For a visual representation of the input and output, please refer to Figure 6  ###reference_###.\n###figure_6### In Häffner \\BOthers. (\\APACyear2023  ###reference_b5###), for the task of predicting the natural logarithm of fatalities on a country month level using the CrisisWatch text, the authors use random forest and XGBoost.131313For an introduction to these tree-based models, readers can refer to Pargent \\BOthers. (\\APACyear2023  ###reference_b12###) and Wang (\\APACyear2019  ###reference_b14###). The training set covers the period from 2003 to first half of 2020 and has 21,924 samples, the validation set covers the second half of 2020 and has 648 samples, and the test set covers the year 2021 and has 1296 samples. These two models achieve an R of 0.64 and 0.63 respectively and an MSE of 1.59 and 1.60 respectively. We use the same training set, validation set and testing set as in Häffner \\BOthers. (\\APACyear2023  ###reference_b5###). The large language model we use is ConfliBERT (Y. Hu \\BOthers., \\APACyear2022  ###reference_b4###). ConfliBERT has the same architecture as BERT-base and is pretrained from scratch using a large corpus in the politics and conflicts domain.141414https://github.com/eventdata/ConfliBERT/. When we finetune ConfliBERT using CrisisWatch texts, we set the learning rate to 2e-5 and set the number of training epochs from 10 (see Listing 2  ###reference_###).\nWe report two groups of finetuning results: ConfliBERT Length-256, where we set the max number of tokens to 256 and ConfliBERT Length-512, where we set the maximum sequence length to 512 (Wang, \\APACyear2023\\APACexlab\\BCnt2  ###reference_b16###).151515We note that 3,386 out of 23,868 samples (14%) contain more than 256 tokens. By setting the max sequence length to 256, we are effectively truncating these long samples to 256 tokens, thus reducing the amount of information that we give to the model. Setting the max sequence length to 512, which is the longest input length possible for BERT models, helps alleviate this problem. In Table 2  ###reference_###, we compare the performance of finedtuned ConfliBERT models with that of two baseline models. OCoDi-Random Forest is the dictionary-based model that leverages random forest, where OCoDi stands for Objective Conflict Dictionary. OCoDi-XGBoost is the dictionary-based model that leverages XGBoost (Häffner \\BOthers., \\APACyear2023  ###reference_b5###).\nWe observe that by finetuning ConfliBERT (Column 3), we are able to achieve a much lower mean squared error and a substantially higher R than the dictionary-based approaches. Further, by increasing the maximum sequence length from 256 to 512 (Column 4), we observe that the mean squared error on the test set decreases from 0.99 to 0.82 and that the R on the test set increases from 0.77 to 0.81. Comparing OCoDi-XGBoost and ConfliBERT Max Length, we observe that a finetuned ConfliBERT is able to achieve an MSE that is 49% lower than OCoDi-XGBoost and an R that is 29% higher."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The pretrain-finetune paradigm has revolutionized the field of natural language processing. In this tutorial, we have provided an intuitive and thorough walk-through of the key concepts therein. We also introduced some of the most common use cases that the pretrain-finetune paradigm supports. To help facilitate the wider adoption of this new paradigm, we have included easy-to-follow examples and made the related datasets and scripts publicly available. Practitioners and scholars in quantitative psychology and psychology more broadly should find our tutorial useful."
        }
    ]
}