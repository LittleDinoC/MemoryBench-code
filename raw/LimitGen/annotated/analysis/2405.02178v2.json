{
    "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
    "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application’s functionality and end-user needs.\nWe introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### One of the long-lasting goals for intelligent agents Winograd (1972  ###reference_b58###) is for them to seamlessly interact with humans in natural language and help their end-users with their tasks, such as completing household tasks, math tutoring, and so on.\nThe rapid development of open-source libraries Wu et al. (2023  ###reference_b59###); Li et al. (2023a  ###reference_b29###) helps that goal by simplifying the development of LLM-powered agentic applications for various user-centered tasks Liang et al. (2023b  ###reference_b34###); Hong et al. (2023  ###reference_b18###); Talebirad and Nadiri (2023  ###reference_b49###). To ensure that the application’s behavior meets the requirements of the application developers, it is also crucial to assess its potential utility to end users Dibia et al. (2023  ###reference_b13###), as this can significantly impact its improvement journey. Taking into account a range of applications, it is unrealistic to assume benchmarking for every domain, including but not limited to code generation Liu et al. (2024  ###reference_b36###), health care Andrew (2024  ###reference_b2###), and many others whose development we witness every day Wu et al. (2023  ###reference_b59###).\nMoreover, directly evaluating agentic applications poses challenges, as current approaches predominantly rely on end-to-end success metrics i.e., whether the application accomplishes tasks Shridhar et al. (2020b  ###reference_b47###, 2019  ###reference_b45###); Myers et al. (2023  ###reference_b40###). However, understanding a user’s interactions with an application involves much more than success alone Kiseleva et al. (2022a  ###reference_b24###, b  ###reference_b25###); Zhang et al. (2023  ###reference_b61###). Consider math problem solving, although it is important that the application solves the problem correctly, its ability to present and explain solutions based on various criteria, such as completeness, conciseness, and clarity, is crucial.\nFurthermore, success is not always clearly defined for a task. Recognizing such criteria and being able to quantify them is essential to assess whether developer requirements are being satisfied and if the application brings utility to the end-users. Given the objective of assessing arbitrary applications, relying solely on end-to-end success metrics is untenable, due to the expansive range of tasks requiring automation. The question is how to design a flexible methodology to assess the task utility for diverse set of applications?\nTo bridge this gap, we introduce AgentEval, a framework to gauge the utility of LLM-powered applications. Its goal is to assess the utility by providing application developers with insights into how the current flow can be characterized. AgentEval builds on recent work showing that LLMs can be a scalable and cost-effective alternative to human evaluation for open-ended tasks Li et al. (2023b  ###reference_b30###).\nAgentEval as illustrated in Fig. 1  ###reference_###, consists of the three following agents, formally defined in Sec. 3  ###reference_###:\n(1) CriticAgent suggests the list of criteria based on the task description and a pair of solutions, where one is preferred over the other one (e.g., successful and failed examples). For instance, for math problems, the criteria could be be Efficiency and Clarity of the proposed solution;\n(2) QuantifierAgent quantifies how the solution performs for each criterion and returns the utility function, e.g. for math problems, if the ’ Clarity is ‘not clear’, ‘moderately clear’, or ‘very clear’;\n(3) VerifierAgent verifies the quality of the assessment of the suggested criteria to make sure the criteria are essential, robust, informative and have high discriminative power.\nIn summary, our main contributions are:\nIntroducing AgentEval, a novel framework that leverages LLM-powered agents as a scalable and cost-effective alternative to human evaluations, to produce task utility through the collaboration of three agents: CriticAgent, QuantifierAgent and VerifierAgent; and\nAn in-depth analysis of AgentEval robustness for two applications across different solutions, that can be replicated on an unseen domain."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Evaluation of LLMs",
            "text": "Prior work Guo et al. (2023  ###reference_b15###); Ziyu et al. (2023  ###reference_b62###); Chang et al. (2023  ###reference_b9###); Liang et al. (2023a  ###reference_b33###) has extensively studied the evaluation of LLMs on various fronts: how ethically sound they are Stahl and Eke (2024  ###reference_b48###), how they align to human preferences Hendrycks et al. (2021a  ###reference_b16###); Köpf et al. (2024  ###reference_b28###), their robustness Wang et al. (2023b  ###reference_b53###), and the knowledge, and reasoning capabilities they posses Bian et al. (2023  ###reference_b6###).\nRecent work evaluates LLMs on more specialized tasks, such as medical domain Jin et al. (2019  ###reference_b21###), multi-modal tasks Mialon et al. (2023  ###reference_b39###); Bang et al. (2023  ###reference_b4###), or as agents in interactive environments Liu et al. (2023  ###reference_b37###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "User satisfaction prediction",
            "text": "Studies suggest that users interacting with various systems operate with specific utility functions in mind Li et al. (2020  ###reference_b31###); Azzopardi et al. (2018  ###reference_b3###); Ahmadvand et al. (2022  ###reference_b1###). Traditionally, metrics defining user satisfaction were designed using large-scale collected behavioral signals Kiseleva et al. (2014  ###reference_b22###), and were tailored to specific applications, such as intelligent assistants Kiseleva et al. (2016a  ###reference_b26###, b  ###reference_b27###), web search engines Williams et al. (2016a  ###reference_b55###, b  ###reference_b56###); Williams and Zitouni (2017  ###reference_b57###), dialogue systems See et al. (2019  ###reference_b43###), multi-turn conversations Li et al. (2021  ###reference_b32###) and general-purpose personal assistants Kiseleva and de Rijke (2017  ###reference_b23###).\nIt was demonstrated that assessing users’ satisfaction requires goes beyond a single metric. As such, here, we propose a flexible framework to assess user and developer requirements, which can eventually be used to improve the application flow."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Using LLMs as evaluators",
            "text": "More recently, there has been a growing trend in utilizing LLMs as evaluators Chiang and Lee (2023  ###reference_b11###); Fu et al. (2023  ###reference_b14###), such as for qualitative research Bano et al. (2023  ###reference_b5###), or summarization.\nSpecifically, Jain et al. (2023  ###reference_b20###) studied the efficacy of few-shot prompted LLM evaluators in evaluating summaries that were written by other LLMs. Similarly, Wang et al. (2023a  ###reference_b52###) explore if ChatGPT itself can be used as an evaluator, by prompting it to score texts. Other works Tjuatja et al. (2023  ###reference_b50###); Liu and Sun (2023  ###reference_b35###); Chiang and Lee (2023  ###reference_b11###) look at how LLMs can be used as proxies for human behavior, or work with humans, such as CoEval Li et al. (2023b  ###reference_b30###), which showed how LLMs can make human evaluation easier. Pan et al. (2024  ###reference_b41###) also show how LLM evaluators can help build models that increase performance on downstream task. Building on the above, a different line of works identify weaknesses in single LLMs as direct evaluators Huang et al. (2023  ###reference_b19###), and propose to improve them, such as a multi-step calibration framework Wang et al. (2023c  ###reference_b54###).\nGiven these drawbacks, recent work has looked at how multiple LLM agents can be used as evaluators. Chan et al. (2023  ###reference_b8###), proposed ChatEval, a multi-agent team that discusses and evaluates responses from agents on generation tasks (debate-style), leading to text that aligns with better human preferences. Similarly, Chern et al. (2024  ###reference_b10###) proposed a multiple agent-debate-assisted meta-evaluation framework.\nBuilding on these works, we propose an automatic multi-agent assessment of utility for arbitrary LLM-powered applications, to provide deep insights for developers. Our framework can uncover current flaws in these applications, and may lead to improvements in them, particularly if the application flow changes after it is applied, and then it is re-used."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Task Utility",
            "text": "Fig. 2  ###reference_### outlines a taxonomy of target tasks for LLM-powered applications, in terms of success metrics. At a high level, these tasks can be categorized into:\n1) Success is not clearly defined — Users use the system in an assistive manner, seeking suggestions from it, rather than expecting it to solve the task. For example, a user can request the system to generate an email. The user usually uses the system’s response as a template, which can later be edited.\nDirectly evaluating assistive tasks like these is hard, particularly for online evaluation, or when dealing with less well-defined tasks. One potential approach is to directly ask users how useful the help was, but this is not well-calibrated Borisov et al. (2018  ###reference_b7###), hard to quantify Sepliarskaia et al. (2018  ###reference_b44###), and expensive.\n2) Success is clearly defined — It is clear whether the system solved the task or not, for example, assisting with household tasks, where success is clear and measurable. This category can be further divided into two subcategories:\nan optimal solution exists — only one successful outcome is possible. For example, when asking an assistant to turn on a light, success is clearly defined, as there is only one way to do it.\nmultiple solutions exist — Increasingly, we observe situations where multiple trajectories of agent behavior can lead to success. For example, when asking an agent to suggest a food recipe, success could be multiple cuisines tasting good, but perhaps the recipe should not be expensive.\n\n###figure_2### AgentEval is currently focused on tasks where success is clearly defined and multiple successful solutions may exist.\nPrevious research on assistive agents suggests human pairwise preferences as one of the most optimal assessments, i.e. when the annotator is presented with two agents side by side and asked for their preferences Kiseleva et al. (2022b  ###reference_b25###). In this setup of side-by-side pairwise comparison, humans tend to suggest a list criteria, explaining why they prefer one agent over the other. For instance,‘the first agent was faster’ or ‘the second agent converses more naturally’. This comparative setup can guide humans to come up with a list of criteria that helps to infer the utility of the task. With this in mind, we designed AgentEval (Fig. 1  ###reference_###),\nby employing LLMs to help us understand, verify, and assess task utility, namely:\nCriticAgent: The goal of this agent is to suggest a set of criteria that can be used to assess task utility. The CriticAgent is given a task description, as well as optionally several pairs of solutions, where preferably some are preferred over the other ones, for instance, successful and failed examples. CriticAgent would return a set of criteria , where each criterion  is accompanied by a set of accepted values  as . For example, for solving math problems, the CriticAgent generated accepted values and criteria such as clarity, efficiency, and more - see Tab. 1  ###reference_###.\nQuantifierAgent: The goal of QuantifierAgent is to quantify each of the suggested criterion, to access the task utility of the system , for the end user. We define the Utility for task  as: . where  represents the task sample and  is the quantifier output for sample  based on the criterion . For example, for math problem solving, given the generated criteria shown in Tab. 1  ###reference_###, the solution’s Accuracy could be quantified as “Incorrect”, “partially correct” or “correct”. Eligible quantified values for quantification process are shown in “Accepted values” column in Tab. 1  ###reference_###\nVerifierAgent: There might be cases where not all the criteria suggested by CriticAgent help assess utility. Some criteria might be redundant, while others may not aid in distinguishing performance. VerifierAgent validates the quality of the criteria in terms of robustness and their distinguishability of noisy samples. Essentially, it checks (1) if the criteria can be quantified robustly over repeated samples, and (2) if QuantifierAgent can identify the adversarial attacked targeted samples from the original ones. If the sanity checks do not pass, VerifierAgent will update the list of criteria, to end up with a set of robust, stable, informative and distinguishable criteria for assessment.\nFinally, we note that AgentEval allows for incorporating a human in the loop in the role of a domain expert. For instance, CriticAgent could be replaced by a human expert who either comes up with the relevant criteria or helps VerifierAgent verify the useful criteria and filter out the unessential ones."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Datasets and Solutions",
            "text": "This section provides an overview of the datasets utilized in our study i.e., Math problem solving and ALFWorld household task.\nThe math dataset is chosen for its widespread usage and complex problem-solving scenarios that are fundamental in evaluating the effectiveness.\nALFWorld dataset offers a scenario involving multi-turn interactions within a moderately approximated multi-modal environment.\nEach dataset plays a critical role in evaluating different aspects of AgentEval’s capabilities, from handling complex theoretical problems to navigating real-world scenarios.\nIn both tasks, although success is clearly defined, multiple solutions exist for accomplishing the objectives. An example of Math problem solving and ALFWorld task is shown in Appendix A.1  ###reference_###.\nDue to space, we report all experiments about Math problem solving in the main paper and we keep all the experiments related to ALFWorld dataset in the Appendix A.3  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "MATH Problem Solving",
            "text": "Dataset: The MATH dataset is a substantial collection of 12,500 challenging mathematics problems from high school competitions Hendrycks et al. (2021b  ###reference_b17###). Each problem comes with a step-by-step solution and is tagged by difficulty levels.\nSimilar to the math problem experimental setup in Wu et al. (2023  ###reference_b59###), we carry out evaluations on 120 problems from level-5 by three different solutions. Due to limited space, for more details about this dataset, we refer readers to Appendix A.2  ###reference_###\nSolutions: In establishing solutions for this task to assess, we draw inspiration from the experiments showcased in Wu et al. (2023  ###reference_b59###). We evaluate the proposed methodology by AutoGen Wu et al. (2023  ###reference_b59###), as well as Langchain ReAct Yao et al. (2022  ###reference_b60###) and a Vanilla solver that employs GPT-4 to tackle the task. These solutions have previously demonstrated promising and competitive performance Wu et al. (2023  ###reference_b59###).\nIn Sec. 5.2  ###reference_###, we explore how the measured performance with AgentEval correlates with the ground truths."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "ALFWorld Household Task",
            "text": "Dataset: ALFWorld presents a set of language-based interactive decision-making tasks within simulated household environments Shridhar et al. (2020b  ###reference_b47###).\nALFWorld is the first interactive parallel environment that aligns text descriptions and commands with physically embodied robotic simulation.\nFinally, the dataset’s inclusion of household chores to more intricate problem-solving scenarios, provides a comprehensive testbed for evaluating the adaptability of multi-agent systems. For more information about the dataset and examples of the test cases, we refer the readers to Appendix A.3.1  ###reference_.SSS1###.\nSolutions:\nAs for the solutions to assess for ALFWorld Household tasks, similar to Wu et al. (2023  ###reference_b59###), we consider ReAct Yao et al. (2022  ###reference_b60###) as well as AutoGen with two agents and AutoGen with three agents Wu et al. (2023  ###reference_b59###). In Appendix A.3.2  ###reference_.SSS2###, we discuss in more details the solutions under assessment.\nWe assess and compare the performance of these three solutions using AgentEval."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Implementation Details",
            "text": "For all experiments, we use GPT-4 version 0613, accessed through Azure OpenAI services, as the LLM model and the temperature of 0. AgentEval utilizes AutoGen Wu et al. (2023  ###reference_b59###)\nfor implementation, since it provides a versatile environment where agents can be finely tuned and customized based on specific application needs. This is crucial for maintaining the flexibility to handle a wide range of applications. We tried to avoid much prompt engineering and tried to keep each agent’s instructions as if we are instructing human annotators. Moreover, another advantages of using AutoGen for implementation of AgentEval is that it has the flexibility to involve human in the loop. Each agent could be replaced by a human annotator. We further provide all the prompts used in our experiments in our Git repository."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Robustness Analysis and Verification",
            "text": "In this section, we first analyze the robustness of AgentEval, then further investigate how VerifierAgent can increase the stability of our assessment."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Diversity of Criteria",
            "text": "Here, our main goal is to study the diversity of the suggested criteria.\nWe investigate the extent inputs to AgentEval (Fig. 1  ###reference_### such as ‘Task Description’ and ‘Successful/Failed Executions’) contribute to CriticAgent for creating a more diverse set of criteria. To do so, we use two distinct methods, with CriticAgent generating (1) “task-based” criteria solely from the task description, and (2) “solution-based” criteria, derived from both the task and execution examples. For example, a solution to a mathematical problem, might satisfy criteria such as ‘Accuracy’ and ‘Clarity’, independent of the solution. However, when additional tools such as coding are used to solve the problems, additional criteria like ‘Code Efficiency’ may be introduced to the set of criteria. This makes sense, since the application leveraged coding to solve math problems.\nFig. 4  ###reference_### displays the number of unique criteria extracted for mathematical problem solving in task-based mode, and three different solution-based approaches.\nTo keep the balance between computational costs and analyzing the robustness, we conducted 50 runs of the CriticAgent with different seeds. Subsequently, for  iterations, we randomly select  samples, as shown on the x-axis of Fig. 4  ###reference_###, and present the average number of unique extracted criteria, along with its 95% confidence interval after repeating this process 50 times. We note that because the total pool of criteria includes 50 iterations in total, the confidence intervals become smaller when  get closer to the\nmaximum number of samples i.e., 50\nTo gain deeper insights into diversity of criteria, we took a closer look at them to study if they are truly unique or to what extent they have similarities. This is important to determine if CriticAgent, when continually generating criteria, will always produce new criteria, or if it will eventually converge to a set.\nWe noted that some criteria are similar but worded differently. For example, ‘Problem Complexity’ vs. ‘Problem Difficulty’ or ‘Time Taken’ vs. ‘Time to Completion’. Tab. 3  ###reference_### in the Appendix lists such instances.\nTo consolidate the similar criteria and reduce noise in the number of unique criteria and redundancy, inspired from previous work Liu et al. (2022  ###reference_b38###); Vahtola et al. (2022  ###reference_b51###); Reimers and Gurevych (2019  ###reference_b42###), we employ a pre-trained language model fine-tuned for paraphrasing111https://bit.ly/3UgsYOp  ###reference_bit.ly/3UgsYOp###, to measure the semantic similarity of criteria descriptions. Using a threshold , we classify pairs with cosine similarity greater than  as semi-identical ones and select one of them as the representative of the pair.\nFig. 4  ###reference_### illustrates the impact of different  values (0.7, 0.85, 1) on the diversity of criteria. A threshold of 1 means no filtering occurs.\nThis analysis shows that the solution-based approach has potential to produce more diverse criteria than the task-based approach, although this varies by the creativity of the model. For example, while the AutoGen solution demonstrates the highest diversity, task-based methods yield more unique criteria than ReAct and Vanilla Solver. Another interesting observation is that repeating the CriticAgent will eventually lead to a convergence in the number of criteria. This suggests that the\nCriticAgent’s ability to create new criteria will diminish, converging to an almost finite list of criteria, which will reduce the cost as well."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Verification",
            "text": "As outlined in Sec. 3  ###reference_### and illustrated in Fig. 1  ###reference_###, the VerifierAgent’s primary role is to ensure the selected criteria are effective toward evaluating the utility for the end-user, while maintaining robustness and high discriminative power. To achieve this, the VerifierAgent undertakes two main actions:\n(1) Criteria Stability:\nThe criteria should be essential and robust, meaning they should not be redundant and we should be able to quantify them stably if we repeatedly quantify it for an individual solution, showing no divergence.\nAs such, VerifierAgent enhances the criteria by iterating over the generation and quantification phases. It then consolidates these criteria by identifying and eliminating redundancies, followed by evaluating the dispersion of the distribution of the quantified criteria. This step modifies the criteria, ensuring that only the most robust criteria are retained.\n(2) Discriminative Power: A reliable evaluation should detect and withstand noise. To test that, we propose to use adversarial examples and then assess the system’s ability to differentiate between these compromised examples and standard cases. Should the system fail to distinguish effectively, it indicates that the criteria are insufficient for reliable assessment under varied conditions.\nWe note that both steps involve a tunable threshold that can be adapted based on application needs, ensuring flexible criteria validation. The proposed methodology for VerifierAgent is summarized in Algorithm 1  ###reference_### in the Appendix."
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Criteria Stability",
            "text": "Our goal here is to explore the stability of criteria and robustness of the quantifier for having a more essential, robust and stable set of criteria.\nWe specifically evaluate the QuantifierAgent’s robustness using criteria for mathematical problems (Table 1  ###reference_###), conducting 50 repeats of runs with different seeds on 120 problems (Section 4.1  ###reference_###). Ideal expected outcomes include consistent performance across all criteria on all the repeats.\nFig. 5  ###reference_### illustrates the distribution of quantifier values for both failed (dark blue) and successful cases (light blue) across all criteria through box plots. The more robust a criterion, the narrower the range of quantified performance (narrower box plots). Also, the less overlap between the successful and failed boxes, the higher the distinguishability of the criteria.\nWe observe that all four criteria, except ‘error analysis’ allow for easy differentiation between successful and failed cases. Additionally, some criteria prove to be more robust compared to others. We believe that such an analysis of the quantifier agent’s performance will yield valuable insights for enhancing reliability, trustworthiness, and explainability in performance evaluation. A detailed examination of the stability of each criterion, especially how they differentiate between successful and failed cases, is provided in Appendix A.4.2  ###reference_.SSS2###.\n###figure_5### Further, to refine and expand the criteria set without redundancy, we operate the CriticAgent multiple times i.e., we execute CriticAgent 50 times with varied seeds. The criteria are then summarized into one list of useful criteria using the LLM. Additionally, as explained in Section 6.1  ###reference_###, we remove similar and redundant criteria using pre-trained language models, thus obtaining a comprehensive list of criteria.\nThe refined criteria after 50 repeats are detailed in Tab. 4  ###reference_### in the Appendix.\nNow, we aim to determine the stability of these criteria through repeated quantifications. Our goal is to identify criteria that maintain consistent results without significant divergence, even when quantified multiple times.\nUsing this consolidated list, we measure the dispersion of quantified results using the coefficient of variation, a standardized metric that facilitates comparison across various test cases when QuantifierAgent quantifies them.\nGiven the consolidated list of criteria, we use the QuantifierAgent to quantify various test cases and report the coefficient of variation as a measure of the dispersion of the QuantifierAgent’s outputs with respect to each criterion across different seeds and report the mean coefficient of variation across all samples.\nwe run QuantifierAgent with 50 seeds and plot the change () in the sum of mean coefficient of variation across all criteria against the number of seeds, in Figure 6  ###reference_###. For each criterion, we compute the absolute difference with the mean coefficient of variation calculated when using  seeds, summing up the absolute differences across all criteria. According to the plot, after approximately 18 seeds, the magnitude of mean coefficient of variation stabilizes and becomes rather trivial.\nIn almost all cases, the mean coefficient of variation is around or below 0.5, which is relatively small, suggesting that QuantifierAgent is quite robust.\n###figure_6###"
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Discriminative Power",
            "text": "It is crucial to ensure the quality of quantification of each criterion. Ideally, this validation would involve comparisons with known pairwise samples, where sample  is definitively superior to  for a given criterion. If the evaluator also confirms superiority of  w.r.t , it has robust quantification. However, due to rapid expansion of LLM-powered applications, obtaining annotated data for many tasks is often unfeasible. Therefore, we propose using synthetically altered versions of samples for verification.\nLet us assume we have an alternative disturbed version of sample , which is called .\nAssuming sample  is more likely to outperform its disturbed version , our assessment should confirm this assumption by assigning better quantified performance  in comparison to . In experiments with mathematical problems, we introduced random noise by removing portions of the solution sentences from AutoGen, VanillaSolver, and ReAct’s results respectively, expecting that criteria like ‘Completeness’ or ‘Clarity’ would show be higherin  than in .\nWe disturbed solutions by removing 25% of the sentences and assessed the QuantifierAgent’s performance. As shown in Fig. 7  ###reference_###, criteria measuring aspects like ‘Clarity’ and ‘Completeness’ were lower in disturbed solutions (lighter bars), confirming QuantifierAgent’s high discriminative power and effectiveness.\nWe have already filtered out the criteria that were unstable, i.e., those that had a high mean standard deviation and dispersion when being quantified in the previous section. We report the results of the QuantifierAgent quantifying differences between original and disturbed samples on the comprehensive set of criteria shown in Appendix, as shown in Fig. 13  ###reference_### for the math problem-solving. In most cases, the QuantifierAgent quantifies the disturbed output to be worse than the original task output. We believe analyzing the QuantifierAgent’s performance will enhance the reliability, trustworthiness, and explainability in evaluations.."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 VerifierAgent",
            "text": "After modifying the list of criteria (Sec. 6.2.1  ###reference_.SSS1###), we have developed a stable and robust list of criteria that the QuantifierAgent can reliably quantify. Further, we also proposed a method for assessing whether the criteria can distinguish between noise-adversarially attacked samples and the original ones.\nThese two tests will serve as input for the VerifierAgent (described in Algorithm 1  ###reference_###), which can also have its threshold tuned for different applications. For instance, one might prioritize the stability of the criteria, while another may value the discriminative power of the AgentEval for specific applications. As such, the VerifierAgent will modify and update the criteria based on to what extend they pass the two tests, i.e., if the mean coefficient of variation is below a specific threshold and the percentage of adversarial testing it has passed. The VerifierAgent will then update the criteria if necessary. We believe that having a VerifierAgent would help continuously updating the criteria as needed because, by improving the systems, we may require new criteria that were not previously necessary for utility assessment.\n###figure_7###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "We introduced the AgentEval framework, designed to swiftly gauge the utility of arbitrary LLM-powered agentic applications. Our framework leverages recent findings suggesting LLMs as a scalable and cost-effective alternative to human evaluations for open-ended tasks. AgentEval consists of three agents: CriticAgent suggests criteria based on task descriptions and executions of the applications, QuantifierAgent quantifies how well the application flow aligns with these criteria, and VerifierAgent modifies the list of criteria if needed. This framework is customizable, adaptable, and can operate in various modes, employing combinations of LLMs, human inputs, and tools.\nWe believe that suggested AgentEval’s utility extends beyond immediate performance. It can uncover new system capabilities over time and adapt to changes in user needs tracked by developers. AgentEval can also enable developers to assess the alignment between application behavior and suggested user requirements, providing them with insights into areas for improvement.\nIn summary, our contributions include introducing the AgentEval framework, and conducting a robust analysis of its performance across various datasets and baselines. AgentEval represents a significant step towards assessing LLM-powered applications."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations and Ethics",
            "text": ""
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Limitations",
            "text": "Here, we discuss some limitations of the AgentEval framework. Firstly, the performance of the AgentEval is highly dependent on the quality of the output logs of the applications. Flaws or limitations in these outputs can significantly impact the framework’s ability to accurately assess utility.\nSecondly, our experiments were conducted exclusively with closed-source LLMs, specifically with GPT-4. This may limit the generalizability of our findings. Plans to include a broader array of LLMs, including open-source models, are considered for future studies to validate and possibly enhance the robustness of our conclusions.\nAdditionally, the tests conducted were limited to specific scenarios within math problem solving and household tasks. Expanding the diversity of test scenarios could help in understanding the broader applicability of the framework.\nThirdly, while AgentEval employs a novel methodology leveraging LLMs to estimate utility, the absence of human evaluation in our validation process could be viewed as a drawback. Human evaluations provide unique insights, especially in subjective aspects of utility that automated systems might overlook. However, such evaluations are often cost-prohibitive and logistically challenging, restricting our ability to implement them within this study. Especially do developers of agentic LLM-powered applications who needs insights fast as they go with the deployments.\nLastly, as LLM technologies evolve, the criteria and metrics used for evaluation may need to be updated or revised. What works for assessing current LLMs may not hold as these models become more advanced. Continuous updates to the evaluation framework will be necessary to keep pace with technological advancements."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Ethics",
            "text": "To the best of our knowledge, we did not violate any code of ethics with the experiments done in this paper. We reported technical details and results, with details in the main paper, Appendix, and code release. Our experimental results are an outcome of a Machine Learning model.\nOur AgentEval system has a variety of uses in real world settings, such as improving applications for end users or helping developers. However, we caution that it must be used carefully, as the outputs are from a ML model and can have real world consequences, if used incorrectly.\nThese and many other related issues are important aspects to consider when deploying a system like AgentEval in the real world."
        }
    ]
}