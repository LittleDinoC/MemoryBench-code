{
    "title": "CHisIEC: An Information Extraction Corpus for Ancient Chinese History",
    "abstract": "Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the “Chinese Historical Information Extraction Corpus”(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 relations. To establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of Large Language Models (LLMs) in the context of tasks related to ancient Chinese history. The dataset and code are available at https://github.com/tangxuemei1995/CHisIEC. \n\n\nKeywords: Ancient Chinese History, Dataset Annotation, Named Entity Recognition, Relation Extraction",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Historical and cultural heritage preservation is an important branch of digital humanities, where the rich tapestry of the past meets the cutting-edge tools of the digital age. This field has been significantly enhanced by applying various technologies, including Natural Language Processing (NLP), Computer Vision (CV), and Knowledge Graphs (KG).\nIn recent works, many studies have endeavored to structure cultural heritage and historical documents. For example,  Kim et al. (2022  ###reference_b11###) annotated a mixed multilingual corpus of Korean cultural heritage related to entities.\nIn addition, some historical documents, such as newspapers and periodicals, have also received attention.  Neudecker (2016  ###reference_b20###) and  Ehrmann et al. (2020  ###reference_b5###) focused on the entity annotation and recognition of European historical newspapers.  Bekele et al. (2016  ###reference_b1###) extracted the spatial and temporal entities from the Brazilian historic expedition gazetteer. Moreover, Nundloll et al. (2022  ###reference_b22###) identified custom entities and domain entities from the annals of a historical Botany journal.\nThe cornerstone of advancing the automatic Information Extraction (IE) models in this field lies in the availability of labeled data. However, the domain of ancient Chinese historical documents presents a unique challenge due to the extensive time span they encompass and the linguistic heterogeneity they exhibit. Although  Li et al. (2021  ###reference_b12###) and  Ji et al. (2021  ###reference_b10###) attempted to build a corpus of information extraction based on ancient Chinese historical documents, there are only 1,600 and 4,000 pieces of data, respectively. Unfortunately, these corpora are still significantly undersized to serve as a solid foundation for developing deep learning models. This limitation greatly hinders the implementation of IE techniques in the ancient Chinese historical documents domain.\nTo tackle the challenges associated with information extraction from ancient Chinese historical documents, we introduce an ancient Chinese Historical Information Extraction Corpus (CHisIEC), which is a high-quality specialized dataset for ancient Chinese historical documents and can be used for NER and RE tasks. In constructing this dataset, considering the large time span of ancient Chinese history, we select 13 historical books from the representative Twenty-Four Histories as the raw data, spanning over 1830 years. Then, we further combine the content and linguistic characteristics of the historical documents, define specific entity types and relation types, and craft detailed annotation guidelines. Finally, we invite annotators to annotate according to these guidelines to create the annotated dataset.\nThe contributions of this paper are as follows.\nWe construct an information extraction dataset for ancient Chinese historical documents, which is the largest available and contains both NER and RE tasks. Our dataset includes more than 130K tokens, 14,194 entities, and 8,609 relations.\nThe data in the corpus come from 13 historical books spanning up to 1830 years, preserving the characteristics of ancient Chinese historical documents in terms of time span and text heterogeneity.\nWe validate the utility of our dataset by conducting comprehensive experiments using models of varying sizes and within different paradigms, including pre-trained language models (PLMs) and large language models (LLMs)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Historical Dataset",
            "text": "Texts in history and cultural heritage exhibit heterogeneity and noise due to their association with diverse time periods, domains, and the influence of OCR results. Several works have attempted to preserve these data features in annotated data.\nFor instance,  Kim et al. (2022  ###reference_b11###) introduced an entity-related Korean cultural heritage corpus KoCHET, which encompasses three sub-tasks: NER, RE, and entity typing (ET). The raw data for this corpus is sourced from e-museum digitized data in both Korean and Chinese; Nundloll et al. (2022  ###reference_b22###) annotated custom entities such as people, nationalities, buildings, organizations, countries, times, and events in the Journal of Historical Botany, as well as domain entities such as plant names, observers, locations, spatial relationships, topographic attributes, and abundance.\nNewspapers serve as typical historical sources and form the foundational material for creating historical datasets. For instance,\n Ehrmann et al. (2020  ###reference_b5###) released the entity dataset HIPE, designed for evaluating named entity processing in French, German, and English historical newspapers. The dataset includes tasks related to entity recognition, classification, and linking, with corpus texts originating from newspapers spanning from 1798 to 2018. Additionally,  Neudecker (2016  ###reference_b20###) produced a corpus of 400 Dutch/French/German newspaper pages that were manually filtered and annotated with named entities such as people, locations, and organizations. The corpus consists mainly of pre-1900 newspaper texts with historical spelling variations.\nIn the field of Chinese history and cultural heritage, there are publicly available datasets.  Zinin and Xu (2020  ###reference_b31###) created a historical lexicon and semantic corpus named CCDH, utilizing open-source Twenty-four Histories, which consists of Classical Chinese gender-specific terms. CCDH supports both synchronic and asynchronous studies of gender terms in ancient Chinese.\nIn the domains of NER and RE,  Li et al. (2021  ###reference_b12###) constructed a few-shot ancient Chinese relation dataset (TinyACD-RC) containing 1,600 instances and 32 relation types. Additionally,  Ji et al. (2021  ###reference_b10###) developed a RE corpus with 25 relation types and 4413 samples based on Twenty-Four Histories."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Information Extraction",
            "text": "Information extraction is the foundation of NLP systems and aims to extract structured information from unstructured or semi-structured data sources automatically. In recent years, deep learning methods have made achievements in information extraction tasks Wu and He (2019  ###reference_b27###); Nguyen et al. (2022  ###reference_b21###); Liang et al. (2022  ###reference_b14###), these methods are categorized into two types, one is to divide the information extraction into multiple sub-tasks, and model the multiple sub-tasks separately, such as named entity recognition, relation classification, event triggering detection, and event argument classification. Another category is modeling IE as a joint task, e.g. named entity recognition and relation classification are often modeled as a joint task. For the joint task of RE, different modeling paradigms have been proposed, such as machine reading comprehension-based approach Li et al. (2019  ###reference_b13###); Zhao et al. (2020  ###reference_b28###), sequence labeling-based approach Zheng et al. (2017  ###reference_b30###), span-based approach Eberts and Ulges (2021  ###reference_b4###); Ji et al. (2020  ###reference_b9###), and generation-based approach Huguet Cabot and Navigli (2021  ###reference_b8###); Nayak and Ng (2020  ###reference_b19###).\nRecently, the development of large language models, such as GPT-3 Brown et al. (2020a  ###reference_b2###), ChatGPT Ouyang et al. (2022  ###reference_b23###), and GPT-4 111https://openai.com/research/gpt-4, has significantly advanced the field of natural language understanding and generation. These LLMs have been trained on massive text corpora to generate coherent and contextually accurate text.\nInstruction tuning Lou et al. (2023  ###reference_b18###) is a novel paradigm for using natural language instructions to guide LLMs to complete downstream tasks, and it shows great promise for observing the generalization of task sets. Some works  Gui et al. (2023  ###reference_b6###); Wang et al. (2023  ###reference_b25###) tried to transfer the IE task samples to instruction-formatted instances, then fine-tune LLMs in a supervised learning way Zhao et al. (2023  ###reference_b29###).\nRecent studies on LLMs such as GPT-3 Brown et al. (2020a  ###reference_b2###) have shown that LLMs perform well in a variety of downstream tasks without any training or fine-tuning, only formulating the task description and demonstrations in the form of natural language text as instructions, which is known as in-context learning  Zhao et al. (2023  ###reference_b29###). Many studies have achieved information extraction by adapting the in-context learning strategies. For example,  Wei et al. (2023  ###reference_b26###) convert the IE task into a multi-turn question-answering task, and then get structured data by asking ChatGPT in two-stages;  Ling et al. (2023  ###reference_b15###) added an error correction mechanism to enhance the confidence of the generated relations.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Corpus Annotation",
            "text": "CHisIEC is a specialized corpus designed for the study of ancient Chinese history. The raw data for this corpus is sourced from the Twenty-four Histories, a compilation of the official histories of the Chinese twenty-four dynasties. The Twenty-four Histories, also known as the “Official History” chronicle over 4,000 years of Chinese history. It consists of 3,213 volumes containing approximately 40 million tokens.\nWe select 22 volumes from 13 books within the Twenty-four Histories as the texts to be labeled, including The Records of the Grand Historian, The Book of Han, The Book of Tang, The History of Song, The History of Ming, and so on. These texts span over 1830 years of Chinese history. Subsequently, we randomly divided the text into segments, each approximately 100 tokens in length, resulting in a total of 150K characters."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Analysis on CHisIEC",
            "text": ""
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Linguistic Analysis",
            "text": "###figure_3### In this section, we analyze the linguistic features of the annotated corpus.\nFirst, the corpus has a long time span. It is based on the official histories of 13 dynasties, with the earliest text from The Records of the Grand Historian dating back to about 91 B.C. and the latest from The History of Ming in 1739 A.D. This resulted in a remarkable period of 1830 years.\nSecond, the long time span leads to high heterogeneity within the corpus. The language used in the corpus is ancient Chinese, which differs significantly from modern Chinese in vocabulary and grammar. Ancient Chinese is categorized into three developmental stages: Early Ancient, Middle Ancient, and Near Ancient, each with distinct linguistic features. Our dataset primarily contains texts from the latter two periods, contributing to the corpus’s high heterogeneity.\nThird, the corpus exhibits exceptionally high linguistic information density. While modern Chinese is known for its information-dense nature, ancient Chinese surpasses it in this aspect. This heightened information density is evident in the annotated data. Based on the statistics presented in Table 1  ###reference_###, on average, each sample contains six entities and four pairs of triplets."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Annotation Process",
            "text": "In this section, we will provide an overview of the annotation process and the annotation schema.\nAnnotation mode. In the practice of large-scale data annotation, we adopt the mode of “multi-person annotation” and “professional review”, the annotation process is shown in Figure 2  ###reference_###. Initially, we recruit 18 undergraduate students as annotators, dividing them into six groups, with an equal distribution of data to each group. Within these groups, annotators independently annotate the same text based on the provided annotation guidelines.\nSubsequently, any inconsistencies in the annotations within each group are identified, and resolutions are determined through discussions involving both a task researcher and an expert with a historical background. After these discussions, the annotators perform a second round of proofreading on the initial annotations, incorporating the suggestions provided by the experts.\nAnnotation guidelines. We design the following annotation rules to ensure the annotation consistency among the annotators.\nEntity type annotation is context-dependent. Certain words can function as both personal names and official positions. For example, “繇王不能矫其众持正。 (Yao King is not able to correct his people.)”, where “繇王 (Yao King)” is an official position, but due to the context in the sentence, it is labeled as a person.\nWhen labeling entities, fine-grained spans take precedence over coarse-grained spans. For example, in the sentence “闽越王无诸及越东海王摇者。 (King of Minyue, Wuzhu, and the King of Yuedonghai, Yao)”, “闽越王 (King of Minyue)” and “无诸 (Wuzhu)” are co-referential and are annotated as separate entities.\nRelation annotation is determined by context. In certain cases, there is a semantic overlap between relations, such as Collegiality and Superior-subordinate. Consequently, if the context unambiguously suggests a Superior-subordinate relationship, it will be labeled as such; if only two people are mentioned as working together, the relationship is labeled as Collegiality."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Schema for Task Annotation",
            "text": "Before corpus annotation, we formulate annotation specifications for different types of named entities and relations in ancient Chinese historical texts. We constantly update and improve the specifications in the annotation practice to make them more suitable for annotating ancient Chinese historical texts of different periods."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1.   Named Entity Type",
            "text": "Establishing a named entity taxonomy is essential for annotating Chinese historical texts. Drawing from Chinese historiography, there are four pivotal facets for comprehending history: “Catalog Studies”, “Chronology Studies”, “Historical Geography”, and “Official Systems”. Catalog studies focus on book-related matters, chronology studies delve into the timing of events and materials, historical geography explores the locations of people’s activities and events, and official systems research addresses changes within the administrative framework.\nIn line with this framework, we identify four primary entity categories: Person, Location, Official, and Book.\nPerson. As creators and witnesses of history, people are central in historical records across eras and dynasties, including emperors, politicians, cultural figures, generals, and other influential societal members.\nLocation. Spatial context situates people and events, providing the stage and background for historical activities, decisions, and changes. Locations range from regions, capitals, counties, geographical features, and palaces.\nOfficial position. Political figures commonly hold formal posts, with the evolution of official systems intertwined with the rise and fall of dynasties. Tracking offices illustrate sociopolitical characteristics of different periods.\nBook. As vessels of thought and culture, books offer insights into academic advancements and biographical details of historical figures."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2.   Relation Type",
            "text": "Twenty-Four Histories are mainly concerned with politics, military, and culture. By combining expert knowledge with textual content analysis, we focus on five prominent types of relations: war, political, geopolitical, family, and personal attributes.\nAs shown in Table 2  ###reference_###, within each of these domains, we identify common relation types to capture connections between entities. We describe in detail the definition of each relation type as follows.\n###figure_4### Political Support.\nPolitical support is a significant and recurring theme in ancient Chinese historical texts. It can manifest in various forms, including support from subjects to rulers and interactions among political allies.\nThis type of relation occurs between individuals, with the direction going from the supporter to the supported.\nTitle/office Holding. Officials, posthumous titles, seals, and temple titles record the organizational structure of the ancient Chinese political and social system, the transmission of power, and the performance of official duties. The Title/Office Holding relation signifies that a person holds a specific position and title. This relation is directional, pointing from the person to the official position.\nIn ancient Chinese historical documents, the appearance of a character is usually accompanied by their position or title, making this type of relation particularly prevalent.\nCollegiality. Collegiality describes the relationship between individuals who work in the same organization or institution, hold similar positions, or share similar status. In ancient Chinese historical documents, this relationship often signifies two or more people working for the same ruler or collaborating on a common task. Collegiality is a non-directional relation.\nSuperior-subordinate. This relation pertains to two individuals who hold a superior-subordinate relationship. It involves texts that explicitly indicate someone as a superior or subordinate or contain actions implying such a relationship. The direction of this relation type is that the superior points to the subordinate.\nAttack. This type of relationship is closely linked to politics and warfare. Ancient Chinese historical documents contain a wealth of records about wars, encompassing conflicts between nations, tribes, political factions, and individuals. In our corpus, this relation primarily involves two individuals or a person and a location, with the direction going from the aggressor to the target.\nDefend. This war-related relation is highly prevalent in ancient Chinese historical texts. It typically involves individuals leading armies stationed at specific locations for defensive purposes, a common military tactic in historical conflicts. This relation represents an action carried out by a person at a particular location. The direction of this relation goes from the person to the location.\nManage. This type of relation, categorized as a geopolitical relation, indicates that individuals are responsible for the management of specific locations. It often involves a person serving in a particular place and overseeing the state or county affairs. The direction of this relation goes from the person to the location.\nArrive. This is a geopolitical relation indicating that an individual arrives at a specific location. The direction of this relation goes from the person to the location.\nBirthplace. In ancient Chinese historical texts, it was common for individuals to be introduced along with their place of birth. The direction of this relation goes from the person to the location.\nParents. In ancient China, there was a strong emphasis on blood ties, with the simplest and most direct blood relation being that of parents. The direction of this relation type goes from parents to children.\nBrother. Brotherhood is one of the fundamental blood relations outside of parental relationships. In certain periods of China’s social history, brother relations within the family could be intertwined with political relations.\nIn our corpus, the direction of the Brother relation points from the older brother to the younger brother\nAlias. In ancient China, people usually had aliases such as Zi (字) and Hao (号) in addition to their names. The direction of this relation goes from persons to their aliases.\nWe give an example of the annotation, as shown in Figure 3  ###reference_###, where the annotator first annotates all the entities in the sentence and then annotates the relationships between the entities.\n###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Experimental Setting",
            "text": "We model NER as a sequence labeling task and RE as a relation classification task.\nWe select baseline models from different paradigms to assess the challenges of information extraction in ancient Chinese historical documents. In recent studies, researchers Wang et al. (2023  ###reference_b25###); Gui et al. (2023  ###reference_b6###) have achieved success using LLMs for information extraction. Therefore, we choose both open-source and closed-source LLMs as our baseline models. Our experiments involve techniques such as In-Context Learning Brown et al. (2020b  ###reference_b3###), LoRA Hu et al. (2021  ###reference_b7###), P-tuning Liu et al. (2022  ###reference_b17###), and Fine-Tuning on language models of various sizes.\nOur baseline models are as follows.\nSikuBERT 222https://huggingface.co/SIKU-BERT/sikubert  ###reference_###: a BERT model that has been incrementally trained with an ancient Chinese corpus. In our approach, we utilize SikuBERT as an encoder for both the NER task, where we used CRF as the decoder, and the RE task, for which we employed MLP+softmax as the classifier.\nSikuRoBERTa 333https://huggingface.co/SIKU-BERT/sikuroberta  ###reference_ta###: a RoBERTa model that has been incrementally trained with an ancient Chinese corpus. All experiments are conducted in a manner similar to those using SikuBERT.\nChatGLM2-6B 444https://huggingface.co/THUDM/chatglm2-6b  ###reference_###: an open-source bilingual (Chinese-English) chat model. We fine-tune it using the P-Tuning v2 technique Liu et al. (2021  ###reference_b16###). Our training samples, as shown in Table 3  ###reference_###, include three components: task description, input, and output.\nAlpaca2-7B 555https://github.com/ymcui/Chinese-LLaMA-Alpaca-2  ###reference_aca-2###:\na model based on LLaMA2 Touvron et al. (2023  ###reference_b24###), and it has been further pre-trained on a Chinese corpus. We fine-tuned it using LoRA Hu et al. (2021  ###reference_b7###), following the same training samples as for ChatGLM2-6B.\nGPT3.5: a large language model with approximately 200B parameters. Due to its closed-source nature, fine-tuning was performed exclusively through the In-Context Learning method. For the NER task, we select 5 random examples from the training set as demonstrations. In the case of the RE task, we draw one sample from the training set for each relation type as demonstrations, i.e., 12-way 1-shot."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   NER Experimental Results and Analysis",
            "text": "The experimental results for named entity recognition are shown in Table 4  ###reference_###. We report micro F1, macro F1, and the model’s performance on each entity type.\nFrom the NER experimental results, it’s clear that PLMs outperform LLMs. This could be due to two possible factors. First, PLMs are incrementally trained in Ancient Chinese, giving them a superior understanding of this language. Whereas ChatGLM2 and Alpaca2 may have a small percentage of Ancient Chinese in the training corpus, therefore, the Ancient Chinese understanding ability of them is inferior to that of PLMs. Second, fine-tuning, which involves adjusting all model parameters, appears to be more effective than the partial modifications made by LoRA and P-tuning. Remarkably, GPT-3.5 shows promising results with just five examples, suggesting the potential of in-context learning in the NER task with this model.\nIn addition, we observe the performance of models on each entity class and find that the two classes of entities, Official and Book, are relatively ineffective, which is probably because, officials are era-specific, with different names for different dynasties, while books may be due to insufficient training data.\n###table_3### ###table_4###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   RE Experimental Results and Analysis",
            "text": "Table 5  ###reference_### lists the experimental results for the RE task, where we also report micro F1 and accuracy.\nBy analyzing the results, we find that the performance of ChatGLM2 and Alpaca2 is comparable to that of PLMs. Even Alpaca2 achieved the best performance. As for GPT-3.5, the limited number of samples provided seems to hinder its ability to confine relation types to the predefined set, resulting in the generation of relation types not within the set and, consequently, lower recall rates. It’s possible that clearer, more comprehensive, and richer prompts could yield improved experimental results.\nDue to space constraints, we don’t report the F1 scores for each relation type, but we plan to add them in the appendix in the future. It’s worth noting that all models exhibit relatively poor performance in one specific relationship type: Political support. The experimental results for each model in the “Political support” relation are presented in Table 6  ###reference_###. We speculate that this could be attributed to the semantic complexity of this relation type, which includes terms such as “support”, “recommend”, and “rescue”. That makes it challenging for the models to summarize the features associated with this specific relation.\n###table_5### In summary, pre-trained language models remain highly potent base models when ample training data is available. It’s important to note that large language models exhibit substantial performance variations across different tasks. For instance, ChatGLM2 and Alpaca2 demonstrate superior performance in the RE task as opposed to the NER task. This can be attributed to the NER task’s greater demand for polyglot features from the model, including entity position identification and entity type recognition. In contrast, the RE task shares similarities with sentence classification, making it a more manageable challenge for these large language models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this paper, we propose CHisIEC, an ancient Chinese history corpus for NER and RE tasks. Our dataset contains texts from 13 dynasties, epitomizing the extensive temporal scope and text heterogeneity of Chinese historical literature. We conduct experiments on both the pre-trained language models and the large language models to validate the applicability of the dataset, and also evaluate the capability of the LLMs in the domain tasks of ancient Chinese history."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Acknowledgments",
            "text": "This research is supported by the NSFC project “the Construction of the Knowledge Graph for the History of Chinese Confucianism” (Grant No. 72010107003)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Bibliographical References",
            "text": ""
        }
    ]
}