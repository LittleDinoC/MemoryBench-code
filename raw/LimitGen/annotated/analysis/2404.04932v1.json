{
    "title": "Towards Understanding the Influence of Reward Margin on Preference Model Performance",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely used framework for the training of language models. However, the process of using RLHF to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model. Our research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios. To bridge this gap, our study introduces a novel method to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models. This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The integration of conversational AI technologies, specifically ChatGPT OpenAI (2023  ###reference_b12###), into the field of artificial intelligence signifies a significant advancement. In the realm of artificial intelligence and language models, the concept of \"alignment\" is considered essential Askell et al. (2021  ###reference_b1###). This concept focuses on ensuring that AI systems operate in a manner that aligns with human intentions and expectations Christiano et al. (2017  ###reference_b6###). The RLHF approach is a prominent method in this context and consists of two main phases. Initially, it utilizes preference data collected from a diverse range of crowdsource workers to train a reward model Ouyang et al. (2022  ###reference_b13###). Subsequently, reinforcement learning (RL) techniques are employed to improve the performance of the language model, aiming to maximize the rewards obtained. The reward model plays a crucial role in the RLHF process, aiming to accurately reflect human preferences.\nNevertheless, the process of using RLHF to develop a language model that is well-aligned presents challenges, especially when it comes to optimizing the reward model Casper et al. (2023  ###reference_b5###). A phenomenon referred to as \"reward hacking\" or \"reward over-optimization\" arises Gao et al. (2023  ###reference_b9###); Skalse et al. (2022  ###reference_b19###), where the language model cleverly identifies and takes advantage of weaknesses in the reward model in order to maximize rewards. This exploitation occurs because the reward model is based on static human preference data, while the language model’s input distribution dynamically evolves during the alignment process.\nOur study highlights two significant challenges in reward modeling. Firstly, the model’s limited generalizability becomes apparent when applied to out-of-distribution examples, exacerbated by policy drift during the reinforcement learning process, leading to changes in the prediction distribution that challenge the model’s accuracy Zhuang and Hadfield-Menell (2020  ###reference_b24###). Secondly, the presence of incorrect and ambiguous preferences in the dataset, attributed to low inter-annotator agreement (72.6% for InstructGPT) Ouyang et al. (2022  ###reference_b13###), undermines the model’s performance. This variability in labeling accuracy introduces uncertainty and potential biases, complicating the model’s ability to discern correct responses based on human preferences Bowman et al. (2022  ###reference_b3###).\nOur research has found that existing reward models, when trained using the traditional ranking objective based on human preference data, often struggle to effectively distinguish between responses that are more or less favorable in real-world scenarios. We argue that this issue is the main reason why the reward model has limited success in accurately capturing genuine human preferences. The standard ranking objective, which aims to order responses based on human preferences, does not inherently guarantee a comprehensive understanding of the intricacies and complexities present in real-world situations. Although the model is trained to determine which response is more preferable than another, it lacks a quantitative framework to measure the degree of superiority of one response over another.\nTo address this shortcoming, we propose incorporating a margin score into the training process of the reward model. The margin score is a numerical value that quantifies the extent of differences between various generations, specifically in terms of their alignment with human preferences Touvron et al. (2023  ###reference_b21###). By integrating this margin score, we aim to explicitly teach the reward model to assign more discrepant scores to generations that diverge significantly from one another, thereby enhancing its ability to recognize and prioritize more preferable responses. The margin score helps in mitigating the effects of noisy data. By focusing on the discrepancies in responses, the model is better equipped to filter out inconsistencies arising from ambiguous or incorrect preferences in the training dataset. Furthermore, the margin score introduces a new dimension to the RLHF process. Instead of solely optimizing for the top-ranked response, the language model is encouraged to generate responses that not only align with human preferences but also demonstrate a clear distinction in quality and relevance when compared to lower-ranked alternatives. This approach promotes a more robust and reliable model, one that is less susceptible to being misled by closely ranked but potentially misleading responses.\nIn the real practice of human preference modeling, accurately determining the difference in preference between different responses is a difficult task that requires significant effort and time cost. To bridge this gap, our study introduces a novel method based on reward confidence to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. This approach capitalizes on the inherent knowledge embedded within the model, utilizing reward confidence level as a means to explore the subtle nuances of preference differences.\nIn section 4  ###reference_###, we perform a comprehensive analysis of the reward margin distribution in models of different sizes and accuracies to support our hypotheses. Additionally, we utilize GPT-4 as an automatic annotator to examine the impact of reward margin on downstream tasks. This involves annotating the actual margin values between selected and non-selected responses and training a reward model using these margin values. Our experimental results provide empirical evidence that incorporating margin values into the training process significantly improves the effectiveness of reward models. In Section 5  ###reference_###, we present a novel methodology for training reward models based on the concept of reward confidence. Subsequently, in section 6.2  ###reference_###, we perform a series of experiments on various datasets that reflect human preferences to validate our approach. We not only evaluate the reward accuracy but also compare the win rate of the enhanced reward model against a baseline model in different settings. This comparative analysis not only demonstrates the superiority of our approach in terms of reward prediction accuracy but also highlights its effectiveness in practical applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Reward Modeling",
            "text": "The integration of conversational AI, notably GPT-4 OpenAI (2023  ###reference_b12###), into artificial intelligence has been a significant advancement. These large language models (LLMs) excel in tasks across mathematics, programming, and tool use, supported by a three-step training process: pre-training on token prediction, supervised fine-tuning to follow instructions, and reinforcement learning to optimize for desired behaviors. However, developing appropriate rewards for RL, particularly within the RLHF framework, is challenging. The effectiveness of RL in improving LLMs hinges on the quality of reward models trained to mimic human evaluative patterns. Wang et al. (2024  ###reference_b22###) also discuss how data featuring varying strengths of preference differently influence the performance of reward models. However, their analysis primarily addresses data characterized by incorrect, ambiguous, and normal preferences, in contrast to our study which concentrates on the preference margin within the reward model itself, making it applicable across various data types."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Reward Hacking",
            "text": "In RLHF, reward hacking poses a significant challenge by causing misalignment between reward model optimization and actual human preferences. Initially, optimization may improve performance, but as training progresses, the model may exploit RM vulnerabilities to gain higher rewards without meeting intended criteria. This leads to outputs that are either linguistically poor or overly verbose and misaligned with human preferences. It also complicates checkpoint selection, echoing Goodhart’s Law that a measure loses its value as a target Gao et al. (2023  ###reference_b9###).\nMultiple approaches have been proposed to mitigate reward hacking in RLHF.\nShen et al. (2023  ###reference_b18###) proposed to use a smaller reward model to learn the biases in the reward and a larger reward model to learn the true reward.\nRewarded Soup (Rame et al., 2023  ###reference_b15###) interpolates weights of policies trained to optimize different reward objectives, which can approximate more costly multi-policy strategies.\nEisenstein et al. (2023  ###reference_b7###) found that reward model ensembles can mitigate reward hackings, but not eliminating them.\nDifferent from these approaches, we focus on the learning objective of reward model to ensure it accurately reflects human values and preferences."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "In this comprehensive review, we delve into the RLHF methodology, a pivotal framework extensively employed in many domains Ziegler et al. (2019  ###reference_b25###). This approach is primarily segmented into three distinct phases: supervised fine-Tuning, preference sampling coupled with reward model training, and reinforcement learning fine-tuning utilizing proximal policy optimization (PPO) Schulman et al. (2017  ###reference_b17###). The journey commences with a baseline pre-trained language model, which is subsequently subjected to supervised training on a meticulously curated dataset tailored for specific downstream applications. This phase culminates in the creation of a model, symbolized as . Our study predominantly concentrates on enhancing the latter two stages of this pipeline.\nThe RLHF process starts with a pre-trained LM, which is then fine-tuned using a high-quality dataset for downstream tasks like dialogue and summarization, resulting in the refined model .\nIn the subsequent stage, the SFT model is engaged to respond to a user’s query, represented by . This interaction generates two distinct responses, . Human evaluators are then tasked with selecting their preferred response, leading to a preference notation , where  and  symbolize the chosen and rejected responses, respectively. Employing the Bradley-Terry model (Bradley and Terry, 1952  ###reference_b4###), we establish a preference distribution, anchored by the reward function , as elucidated in the following equation:\nwhere  denotes the logistic function. This setup is analogous to a binary classification task, leading to the formulation of a negative log-likelihood loss function, as depicted in Equation:\nHere, the dataset  comprises a series of comparative pairings , , . Within the context of language models, the reward network  is typically initialized using the SFT model  and further refined by integrating an additional linear layer atop the final transformer layer. This enhancement aims to yield a singular scalar output, representing the reward value Ziegler et al. (2019  ###reference_b25###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Investigating the Reward Margin on Preference Modeling",
            "text": "This study focuses on the reward model’s role in assessing the quality of model-generated responses and their corresponding prompts. The reward model assigns scores to indicate the response quality, which are pivotal for fine-tuning the policy model during RLHF. This process is instrumental in aligning the model more closely with human preferences, thereby augmenting its utility and safety. For the training phase of the reward model, our approach converts the accumulated human preference data into binary labels, namely ’chosen’ and ’rejected’. We adhere to the principle that scores for preferred responses must exceed those for their counterparts, as detailed in Equation 2  ###reference_###. This approach enables us to quantify the preference discrepancy between chosen and rejected using the formula .\nIn Fig.LABEL:fig:margin, we examine the impact of various model configurations on the distribution of reward margins. The accuracy of each model, as detailed in this investigation, is presented in Table 1  ###reference_###. Our findings confirm that the average reward margin for these models consistently remains above zero. This aligns with the theoretical expectation that positive rewards should outweigh negative ones and suggests that the model effectively distinguishes between different responses. A higher mean value correlates with the model’s enhanced capability in distinguishing various responses. However, a notable number of instances exhibit reward margins below zero, potentially due to dataset noise or anomalies, as discussed in the preceding section.\nRegarding the skewness metric, all model distributions exhibit a value exceeding zero, indicating a rightward skew. This skewness, particularly pronounced on the right side, implies a more sophisticated performance level of the model, underlining its improved ability to differentiate and allocate varied rewards. Furthermore, models that demonstrate higher efficacy exhibit lower kurtosis, suggesting a more even and broad distribution. This observation implies that the reward distribution should not be overly centralized around a specific mean, such as zero. Rather, it should exhibit a range of variability across different data pairings, each with unique margin values. This aspect, along with the noted skewness, is critical for ensuring a refined and precise differentiation among the outcomes linked to diverse data pairs. It underscores the importance of the reward model in assigning markedly different scores to generations with varying characteristics."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Our Methodology",
            "text": "In the domain of human preference modeling, accurately determining the difference in preference between different responses is a difficult task that requires significant effort and time cost. Traditional methods, commonly used in popular datasets like HH Bai et al. (2022  ###reference_b2###) and SHP Ethayarajh et al. (2022  ###reference_b8###), have typically used a simple approach where human annotators are asked to choose the preferred option from a pair. While this binary labeling technique is efficient, it fails to capture the nuanced range of preference differences that could offer deeper insights into the modeling of human preferences.\nTo bridge this gap, our study introduces a novel method based on local approximation of reward confidence to estimate the preference differences without the need for detailed, exhaustive labels from human annotators. Therefore, this approach ultimately aims to align with the goal of Figure LABEL:fig:margin, which is to have a larger margin mean to ensure that the quality of preference can be identified. At the same time, the overall distribution shifts to the right to reduce the impact of noise and to differentiate preference with greater granularity.\nTo achieve the first point, we construct our approach based on the idea of Equation 3  ###reference_###. We have transformed Equation 3  ###reference_### into the following mathematical form.\nIt calculates the Loss of each batch  with its samples , and  is the mean margin of the batch:\nThese Equations that use batch margin have two advantages: (1) increasing the model’s prediction of differences between preferences; (2) ensuring computational efficiency and enabling real-time updates.\nTo achieve the second point of the right-skewed nature of the overall distribution, and further reduce the changes of the Equation 2  ###reference_### to maintain theoretical guarantees, we design a simple threshold filtering method so that only a subset of samples adopt Equation 4  ###reference_### for loss calculation, shown in Equation 6  ###reference_###.\nwhere\nAs a filtering function,  is used to verify the status of the margin of the samples . With Equation 6  ###reference_###, it increases the margin of samples whose current margin is smaller than the average margin of the batch by adding a constraint of the average margin to the loss function, thereby shifting the overall margin to the right. In addition, it maintains the original loss function for samples with a current margin larger than the average margin of the batch to preserve their original properties."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "In this section, we offer a thorough assessment of our novel method for training reward models using datasets that consist of a wide range of human preferences. We begin by providing a detailed explanation of our experimental framework in Section 6.1  ###reference_###. Then, in Section 6.2  ###reference_###, we discuss the substantial enhancements our approach brings compared to traditional RM training methods. After that, we delve into the examination of the reward distribution generated by the RMs, demonstrating the influence of employing a diverse preference dataset on reward modeling.\nIn the course of our investigation, we utilized diverse human preference datasets to evaluate the efficacy of our approach in training reward models. These datasets included the Anthropic’s Helpful and Harmless dataset (referred to as HH) Bai et al. (2022  ###reference_b2###), the OpenAssistant multi-lingual conversations dataset (OASST1) Köpf et al. (2023  ###reference_b10###), the WebGPT comparisons dataset (WebGPT) Nakano et al. (2021  ###reference_b11###), the stanford human preferences dataset (SHP) Ethayarajh et al. (2022  ###reference_b8###), openai summarization dataset (TLDR) Stiennon et al. (2020  ###reference_b20###) and chatbot arena conversations dataset (chatbot) Zheng et al. (2023  ###reference_b23###).\nFor the HH, SHP and TLDR datasets, we adhered to the standard division of data into training and testing sets. The OASST1 dataset was prepared according to the preprocessing framework provided by tasksource/oasst1_pairwise_rlhf_reward 111https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward  ###reference_/oasst1_pairwise_rlhf_reward###, resulting in a training set of 18k instances and a test set of 952 instances. The WebGPT and chatbot dataset was partitioned into 90% instances for training purposes and 10% instances for evaluation.\nTo accommodate various experimental scenarios, we deployed language models (LMs) of differing capacities for the training of reward models, including Pythia-410m, TinyLlama-1.1B-chat 222https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0  ###reference_-1.1B-Chat-v1.0###, Pythia-1.4B, Pythia-2.8B, and Llama-7B-chat. We employed the last token embedding from the output hidden states as the pooled hidden representation. Subsequently, a linear layer, referred to as the RM head, with a scale-value output, was integrated to forecast reward scores. we set the batch size for all RM training at 128. The maximum permissible input sequence length was set at 2048 tokens. All reward models underwent fine-tuning for a single epoch, utilizing the AdamW optimizer with a learning rate of 9e-6.\nThe foundation of our evaluation is the accuracy metric, detailed in section 6.1  ###reference_###, which directly reflects the models’ ability to predict rewards aligning with human judgments across datasets. This allows for a quantitative comparison of reward prediction precision among models of varying sizes.\nAs illustrated in Table 2  ###reference_###, which presents the performance scores of different models on various preference benchmarks, the application of our method consistently enhances the accuracy across all models and benchmarks. Notably, Pythia-410M with our method outperforms its baseline by a significant margin, demonstrating a 6.31% increase in the HH benchmark and a 2.48% rise in the OASST1 benchmark. Similarly, the Pythia-2.8B model with our method shows improved accuracy, with a notable increase of 1.93% in the OASST1 benchmark and 2.10% in OASST1 benchmark. The Llama-7B-chat model achieves a remarkable accuracy and further improves to 71.95% and 78.34% with the integration of our method. Such enhancement is consistent across all models and datasets upon the application of our method, suggesting its effectiveness in refining model predictions to be more human-aligned.\nParticularly, the TinyLlama-1.1B-chat model, even as a smaller-scale model, demonstrates substantial competence, especially when equipped with our method, pushing its accuracy to second highest across almost all datasets. This reinforces the notion that incorporating better ability of pre-trained model can significantly elevate the performance of reward model.\nThese results collectively suggest that our method not only improves accuracy in a consistent manner but also scales effectively with model size, validating the robustness and efficiency of our approach in aligning model predictions with human preferences.\nHowever, it is also crucial to consider the role of diminishing returns as model size increases. Although the Llama-7B-chat model achieves the highest accuracy, the relative improvement over its base model is less pronounced compared to the gains observed with the Pythia-410M model. This suggests that our method’s efficiency may plateau at higher model scales, which could warrant a re-examination of our method’s mechanisms to ensure they remain impactful as model sizes continue to grow."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Setups",
            "text": "In the course of our investigation, we utilized diverse human preference datasets to evaluate the efficacy of our approach in training reward models. These datasets included the Anthropic’s Helpful and Harmless dataset (referred to as HH) Bai et al. (2022  ###reference_b2###  ###reference_b2###), the OpenAssistant multi-lingual conversations dataset (OASST1) Köpf et al. (2023  ###reference_b10###  ###reference_b10###), the WebGPT comparisons dataset (WebGPT) Nakano et al. (2021  ###reference_b11###  ###reference_b11###), the stanford human preferences dataset (SHP) Ethayarajh et al. (2022  ###reference_b8###  ###reference_b8###), openai summarization dataset (TLDR) Stiennon et al. (2020  ###reference_b20###  ###reference_b20###) and chatbot arena conversations dataset (chatbot) Zheng et al. (2023  ###reference_b23###  ###reference_b23###).\nFor the HH, SHP and TLDR datasets, we adhered to the standard division of data into training and testing sets. The OASST1 dataset was prepared according to the preprocessing framework provided by tasksource/oasst1_pairwise_rlhf_reward 111https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward  ###reference_/oasst1_pairwise_rlhf_reward###  ###reference_/oasst1_pairwise_rlhf_reward###, resulting in a training set of 18k instances and a test set of 952 instances. The WebGPT and chatbot dataset was partitioned into 90% instances for training purposes and 10% instances for evaluation.\nTo accommodate various experimental scenarios, we deployed language models (LMs) of differing capacities for the training of reward models, including Pythia-410m, TinyLlama-1.1B-chat 222https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0  ###reference_-1.1B-Chat-v1.0###  ###reference_-1.1B-Chat-v1.0###, Pythia-1.4B, Pythia-2.8B, and Llama-7B-chat. We employed the last token embedding from the output hidden states as the pooled hidden representation. Subsequently, a linear layer, referred to as the RM head, with a scale-value output, was integrated to forecast reward scores. we set the batch size for all RM training at 128. The maximum permissible input sequence length was set at 2048 tokens. All reward models underwent fine-tuning for a single epoch, utilizing the AdamW optimizer with a learning rate of 9e-6."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Main performance",
            "text": "In this section, we introduce the primary performance measures utilized to assess the efficacy of our approach in training reward models on various human preference datasets. The evaluation is organized around two fundamental elements: accuracy and analysis of reward distribution.\nThe foundation of our evaluation is the accuracy metric, detailed in section 6.1  ###reference_###  ###reference_###, which directly reflects the models’ ability to predict rewards aligning with human judgments across datasets. This allows for a quantitative comparison of reward prediction precision among models of varying sizes.\nAs illustrated in Table 2  ###reference_###  ###reference_###, which presents the performance scores of different models on various preference benchmarks, the application of our method consistently enhances the accuracy across all models and benchmarks. Notably, Pythia-410M with our method outperforms its baseline by a significant margin, demonstrating a 6.31% increase in the HH benchmark and a 2.48% rise in the OASST1 benchmark. Similarly, the Pythia-2.8B model with our method shows improved accuracy, with a notable increase of 1.93% in the OASST1 benchmark and 2.10% in OASST1 benchmark. The Llama-7B-chat model achieves a remarkable accuracy and further improves to 71.95% and 78.34% with the integration of our method. Such enhancement is consistent across all models and datasets upon the application of our method, suggesting its effectiveness in refining model predictions to be more human-aligned.\nParticularly, the TinyLlama-1.1B-chat model, even as a smaller-scale model, demonstrates substantial competence, especially when equipped with our method, pushing its accuracy to second highest across almost all datasets. This reinforces the notion that incorporating better ability of pre-trained model can significantly elevate the performance of reward model.\nThese results collectively suggest that our method not only improves accuracy in a consistent manner but also scales effectively with model size, validating the robustness and efficiency of our approach in aligning model predictions with human preferences.\nHowever, it is also crucial to consider the role of diminishing returns as model size increases. Although the Llama-7B-chat model achieves the highest accuracy, the relative improvement over its base model is less pronounced compared to the gains observed with the Pythia-410M model. This suggests that our method’s efficiency may plateau at higher model scales, which could warrant a re-examination of our method’s mechanisms to ensure they remain impactful as model sizes continue to grow."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Experiment of Best of N",
            "text": "Apart from assessing accuracy, we also explore the possibility of incorporating reward models with a Best-of-N policy, which is a commonly used technique in RLHF. However, due to potential for training instability and slower optimization with the PPO algorithm Gao et al. (2023  ###reference_b9###), we choose the best-of-N method instead. And more importantly, the best-of-N method allows for the decoupling of the reward model’s quality from the optimization process inherent to PPO, as outlined in Rafailov et al. (2023  ###reference_b14###). To elaborate, the Best-of-N approach involves generating n samples from the SFT model for each input and selecting the sample with the highest predicted reward."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Training Setup and Evaluation",
            "text": "We examine two policy models of varying sizes: 1B, and 6.9B, that have undergone training on a dataset that focuses on promoting beneficial and innocuous content. Additionally, we consider four reward models with sizes of 410M, 1.1B, 1.4B, and 2.8B. Additionally, the value of N is defined as the set {2, 4, 8, 16, 32, 64, 128, 256}. In line with previous studiesGao et al. (2023  ###reference_b9###); Ramé et al. (2024  ###reference_b16###), we use win rate to quantify generalization of reward models. We employ GPT-4 and a larger reward model as an evaluator to gauge the quality of different outputs, specifically evaluating their helpfulness and harmlessness. As shown in table.2  ###reference_###, Llama-13B-chat achieves better performance than all smaller-sized reward models. To assess helpfulness, we utilize a HH-RLHF test dataset and randomly select 500 prompts for evaluation. The specific prompts used for GPT-4 evaluation is same as Rafailov et al. (2023  ###reference_b14###)."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Experimental results",
            "text": "Our experimental setup aimed to rigorously evaluate the efficacy of the Best-of-N policy across different policy model sizes and varying values of N. The results, detailed in FigureLABEL:fig:bestofn, highlight the impact of the size of the reward model and the choice of N on the quality of generated responses in terms of helpfulness and harmlessness.\nFrom the FigureLABEL:fig:bestofn, we can observe some interesting results: Initially, as  increases, all models show improved win rates. This is expected since a broader selection typically leads to a better choice. Each model has a peak performance at a certain  value: =64 for Pythia-410M and =8 for Pythia-2.8B, suggesting an optimal  range that varies by model. After reaching peak performance, the win rate either plateaus or declines slightly, indicating diminishing returns with additional choices. This is particularly notable for larger models, implying a limit to the benefits of more options. The size of the reward model does not linearly correlate with the optimal  value, hinting at complex interactions between model capacity and the ability to discern quality output. The TinyLlama-1.1B model demonstrates consistent improvement, showing strong selection capabilities across a range of  values. All models perform above a 50% win rate, confirming their effectiveness in selecting high-quality outputs over baselines. In summary, the data suggests that while having more samples generally improves performance, there is an optimal number of samples for each model, beyond which the benefit plateaus or decreases. This optimal point is not directly proportional to model size and should be a focus for future optimization efforts.\nAs depicted in Figure LABEL:fig:bestofn_6.9b, a distinct trend emerges across the models, with the notable exception of the Pythia-410M model, which exhibits a pronounced upward trend. The other models also generally show an increase in win rate with the number of games played (), but the trends are not as smooth or consistent. This observation suggests that when the policy model significantly exceeds the size of the reward model, larger policy models derive diminished benefits from optimization efforts against a reward model."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, our study reveals the significance of the reward margin in the context of reinforcement learning from human feedback. The margin score not only serves as a critical parameter in training the reward model but also plays a pivotal role in shaping the policy model’s performance. The optimal margin setting strikes a balance, ensuring that the model can effectively discern between high and low-quality responses without overly penalizing subtle variations in response quality. Compared with expensive human annotation, we explore an adaptive margin techniques that dynamically adjust the reward margin based on the reward confidence. The evaluation on reward accuracy and downstream tasks underscore the benefits of this method, contributing to a more refined and efficient model of human preference."
        }
    ]
}