{
    "title": "SyllabusQA: A Course Logistics Question Answering Dataset",
    "abstract": "Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with  real course syllabi covering  majors, containing  open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In educational applications, artificial intelligence (AI) approaches have shown significant promise in improving learning outcomes Aleven et al. (2016  ###reference_b3###); VanLehn (2011  ###reference_b27###), by automatically providing feedback to students or engaging in tutoring dialogues with them. The key idea is to use AI to create an on-demand virtual teaching assistant to interact with many students simultaneously; see, e.g., Khamigo from Khan Academy Academy (2022  ###reference_b1###). These approaches can scale up the effort of expert human teachers and tutors, and relieve them from doing repetitive tasks so that they can focus on providing personalized feedback or designing new learning content Adamson and Rosé (2012  ###reference_b2###). In higher education, one promising avenue for AI-powered teaching assistants to reduce human effort is course logistics-related question answering (QA): answering student questions on logistics whose answers can be directly found or inferred from the syllabus.\nThere exist many approaches for automated QA in online courses (both logistics-related and content-related), using tools from rule-based AI systems Raamadhurai et al. (2019  ###reference_b20###); Feng et al. (2006  ###reference_b6###) and expert systems with knowledge bases Goel and Joyner (2017  ###reference_b8###); Goel et al. (2021  ###reference_b7###) to end-to-end text generation Zylich et al. (2020  ###reference_b33###). Recently, large language model (LLM)-based approaches have shown great promise to improve the coverage and answer quality over traditional QA approaches Hicke et al. (2023  ###reference_b9###). See Section A  ###reference_### in the Supplementary Material for a detailed discussion on related work. Unfortunately, these approaches are mostly developed and evaluated on proprietary data due to student privacy concerns, which prevents more researchers from contributing to the development of automated QA systems for education.\nDataset\nEdu.\nPublic\nDomain Diversity\nLong Doc.\nComplex Format\nAdv. Q\nQ Type\nA Type\nGeneration\nData Source\nChaTa Hicke et al. (2023  ###reference_b9###)\n✓\n✗\n✗\n✗\n✗\n✗\nOpen-ended\nNatural\nStudent-written\nPiazza\nFairytaleQA Xu et al. (2022  ###reference_b30###)\n✓\n✓\n✓\n✗\n✗\n✗\nOpen-ended\nNatural\nExpert\nLiterature\nBook Test Hill et al. (2015  ###reference_b10###)\n✗\n✓\n✓\n✗\n✗\n✗\nCloze\nSpan\nAutomated\nLiterature\nNarrativeQA Kočiský et al. (2018  ###reference_b13###)\n✗\n✓\n✓\n✓\n✗\n✗\nOpen-ended\nNatural\nCrowd-sourced\nMovies/literature\nCliCR Šuster and Daelemans (2018  ###reference_b23###)\n✗\n✓\n✗\n✓\n✗\n✗\nCloze\nSpan\nAutomated\nMedical reports\nNewsQA Trischler et al. (2017  ###reference_b26###)\n✗\n✓\n✗\n✗\n✗\n✓\nOpen-ended\nSpan\nCrowd-sourced\nNews\nQuAC Choi et al. (2018  ###reference_b5###)\n✗\n✓\n✓\n✗\n✗\n✓\nOpen-ended\nSpan\nCrowd-sourced\nWikipedia\nOpenBookQA Mihaylov et al. (2018  ###reference_b17###)\n✗\n✓\n✓\n✓\n✓\n✗\nMCQ\nMulti\nCrowd-sourced\nScience books\nHotpotQA Yang et al. (2018  ###reference_b31###)\n✗\n✓\n✓\n✓\n✗\n✓\nOpen-ended\nSpan\nCrowd-sourced\nWikipedia\n✓\n✓\n✓\n✓\n✓\n✓\nOpen-ended\nNatural\nCrowd-sourced\nCourse syllabi\nFor evaluation, especially in logistics-related QA that often contains critical information, the factuality of predicted answers is more important than measuring surface textual features. Moreover, text similarity metrics may not be suitable for some open-ended natural language generation tasks Amidei et al. (2018  ###reference_b4###). As an example, the answer “The final exam will be on Dec 15”, has high surface-level textual similarity with the reference answer, “The final exam is on Dec 14”, but contains a critical factual error that may lead to significant negative consequences to students.\nMeanwhile, human instructors and teaching assistants often answer student questions in a concise way, without giving any unnecessary information. Therefore, it is important for LLM-based approaches to generate answers that are both concise and precise.\nIn this paper, we introduce the SyllabusQA dataset for course logistics-related QA. We will publicly release this dataset and hope that it can be a benchmark for future work on developing and evaluating automated QA approaches for teaching assistance. Our contributions are:\nFirst, we collect a diverse set of  real course syllabi covering  majors across  universities, and employ crowd annotators to write  logistics-related QA pairs with the goal of simulating what students would ask in a real-world course.\nSecond, we detail the diverse composition of syllabi and QA pairs in SyllabusQA, in terms of syllabi domain, question types, answer sources, and different language styles. We lay out metrics to evaluate different aspects of open-ended automated QA approaches, in terms of both surface textual similarity and more importantly, the factuality of predicted answers grounded in the syllabus.\nThird, we conduct extensive experiments to benchmark the QA performance of several strong baselines on SyllabusQA. Overall, LLM-based approaches perform similar to humans on surface-level textual similarity metrics but worse on factuality metrics. We found that fine-tuning on real QA pairs from SyllabusQA performs much better than LLM prompting approaches and that retrieval-augmented generation is especially important.\nTo the best of our knowledge, SyllabusQA is the first publicly available real-world course logistics-related QA dataset. SyllabusQA assesses automated QA models on various natural language understanding aspects including handling challenging question types, from reasoning-based ones to adversarial ones, understanding of long input documents sourced from diverse course majors, processing complex input formatting including tables and schedules, and answering open-ended questions in a similar way as human instructors and TAs. Table 1  ###reference_### highlights the difference between SyllabusQA and existing datasets; see also Section 3.1  ###reference_### for a detailed discussion."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "SyllabusQA Data Collection",
            "text": "We now detail how we construct SyllabusQA to address some of the limitations in existing datasets.\nSyllabusQA is designed to include diverse question types across common logistics-related questions that human instructors and teaching assistants often face. There are  question types defined below, with real examples of each type:\nYes/No: questions that have answers consisting of a single word yes/no answer, e.g., “Is there a separate lab section for this class?”\nSingle-factual: questions that have answers containing a single explicit fact from the syllabus, e.g., “When are office hours?”\nMulti-factual questions that have answers combining multiple explicit facts from the syllabus, e.g., “What software and applications are used in this class?”\nSingle-hop reasoning: questions that have implicit answers requiring a single reasoning step, e.g., “I have not yet taken Advanced Biology, can I still take this course?”\nMulti-hop reasoning: questions that have implicit answers requiring multiple reasoning steps, e.g., “Can I start the class six weeks in and get an A?”\nSummarization: questions that have answers that require summarizing information from multiple parts of the syllabus, e.g., “Could you tell me how class participation grades are broken down?”\nAdversarial: questions that have no answers due to insufficient information, e.g., “Can I contact the instructor over Zoom?”\nQuestions in SyllabusQA can also be categorized by the source of their answers as defined below:\nExplicit: questions that have answers directly from the syllabus, including question types Yes/No, Single-factual, and Multi-factual.\nImplicit: questions that have answers not explicitly in the syllabus and require inference, including question types Single-hop reasoning, Multi-hop reasoning, and Summarization.\nInsufficient Information questions that do not have answers due to insufficient supporting information in the syllabus, including questions of type Adversarial.\nThe combination of multiple question types and answer sources makes SyllabusQA challenging for automated QA approaches. The involvement of adversarial questions, in particular, requires models to not hallucinate information not present in the syllabus, which is a critical test to pass before any potential deployment to real students.\nFor simplicity, we use a straightforward annotation task: given a syllabus, we ask an annotator to simulate logistics-related QA pairs seen in real course classrooms. They write  QA pairs for each of the  question types for a total of  QA pairs, to ensure QA pairs are evenly distributed across question types. Before the task, we guide them through a tutorial to help them get familiar with the task and write diverse QA pairs. We give them a list of  diverse QA examples manually written by us, covering a wide spectrum of QA examples seen in classrooms, with  examples for each question type, on a single example syllabus. We encourage annotators to write open-ended questions with answers written in their own words, giving them the flexibility to come up with questions that are difficult to answer.\nSee Section K  ###reference_### in the Supplementary Material for the instructions we provide.\nFor explicit questions with answers directly found in the syllabus, including the Yes/No, Single-Factual, and Multi-Factual types, an annotator additionally provides one, one, and up to five answer spans respectively, i.e., snippets directly copied from the reference syllabus, supporting their answer.\nFor Single-hop and Multi-hop reasoning questions, an annotator additionally provides one and up to five reasoning steps respectively, written in their own words to detail their thought process before arriving at the final answer.\nFor Summarization questions, an annotator additionally provides up to five answer spans from the syllabus used to construct the summary. This additional information provided by annotators encourages their answers to be cohesive and faithful to the reference syllabus and can be further used to aid development of automated QA approaches. For Adversarial questions, annotators do not write answers and we use “No/insufficient information” as the answer.\nSyllabusQA is designed to include questions written in different styles. Therefore, we recruited a large pool of over  annotators across two popular platforms, Amazon Mechanical Turk (AMT) and Prolific, all with an undergraduate bachelor’s degree or above, located in the United States or Canada.\nTo ensure diversity in language style, we asked each annotator to write a maximum of  QA pairs per syllabus, and a maximum of  total QA pairs across various syllabi. We collected this data over  months.\nTo ensure high data quality, we asked each annotator on AMT to write  QA pairs on an assigned syllabus in a screening session. We only include data from those who pass the screening test in our dataset and invite them for additional annotation rounds. For annotators on Prolific, in addition to Prolific’s screening process, we manually checked a random subset of QA pairs written by each annotator and found that their data meets our quality standards. For further quality control, we also run heuristics-based checks to filter out unsuitable QA pairs.\nSince our source syllabi are anonymized, we discard questions asking for personally identifiable information about instructors and/or teaching assistants. We also post-process answers to Yes/No type questions to a single word since some contain additional explanations.\nTo aid development of automated QA approaches, we explicitly aimed at encouraging language diversity across QA pairs in SyllabusQA by employing a large pool of annotators to reflect the diversity in real questions asked by students in classrooms. Therefore, inter-annotator agreement may be low, especially if evaluated on traditional text similarity measures. To formally measure this agreement, we had an independent expert annotator with extensive teaching experience as a college instructor, write oracle answers for around  of randomly sampled questions from the test set. We provide this expert with the same instructions and tutorial as the annotators and ask them to first select the question type before writing the answer accordingly.\nFollowing related work on open-form QA datasets Xu et al. (2022  ###reference_b30###), we do not use traditional inter-annotator agreement metrics since we asked annotators to write QA pairs in their own words and style, which makes these metrics unsuitable Amidei et al. (2018  ###reference_b4###). Instead, we mainly evaluate agreement on factual information overlap, which measures the overlap in key facts contained in the annotator’s answer and an oracle answer. We use a GPT-4-based evaluation of precision and recall of factual information present between the answers written by the expert and annotators, which we detail later in Section 4.2  ###reference_###. This metric results in an inter-annotator precision of  and recall of , indicating high overlap in factual information. As a reference, agreement measured in traditional surface textual similarity metrics is  in ROUGE-L F1 Lin (2004  ###reference_b15###) and  in BERTScore Zhang et al. (2020  ###reference_b32###). This result is not surprising since we found that although annotator-written answers have varied surface language styles, they contain very similar key information.\nWe analyze annotator-written ground-truth answers where an independent expert annotator manually verified a random sample of QA pairs from  of the test set on: 1) precision and 2) recall. For precision, we test if the ground-truth answer is relevant to the question and supported by the syllabus. For recall, we test if the ground-truth answer contains all the relevant information required to answer the question. We found that  () of ground-truth answers have perfect precision (recall),  () of ground-truth answers have partial precision (recall), and  () of ground-truth answers have poor precision (recall). Through a qualitative error analysis, we find the following error types leading to less-than-perfect precision/recall in ground-truth answers: 1) imperfect annotator\nrecall/human error, 2) ambiguous syllabus information, and 3) ambiguous annotator-written answers. We provide details and examples in Section C  ###reference_### in the Supplementary Material."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Source Syllabi",
            "text": "The source texts we include in this dataset are anonymized course syllabi, based on which QA pairs are generated. We collected  unique syllabi from instructors across  unique universities worldwide, at both undergraduate and graduate levels.\nWe include a wide range of diverse course subjects, from science and engineering to humanities and business (see Figure 1  ###reference_### in the Supplementary Material). We also collected syllabi from courses with different formats: lab-based ones, project-based ones, lecture-based ones, etc.\nTo ensure anonymity, we manually replaced personally identifiable information on instructors and teaching assistants with placeholders, along with any private information like platform login codes. On average, there are  pages per syllabus, with a minimum of  and a maximum of . We store all syllabi in their raw PDF format. Apart from the considerable variation in length, the diverse information formats in the syllabi, such as lists, tables, boxes, schedules, forms, and even diagrams and images, make QA on SyllabusQA especially challenging: doing so requires techniques ranging from parsing, information retrieval, to long-document QA."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Design Considerations",
            "text": "SyllabusQA is designed to include diverse question types across common logistics-related questions that human instructors and teaching assistants often face. There are  question types defined below, with real examples of each type:\nYes/No: questions that have answers consisting of a single word yes/no answer, e.g., “Is there a separate lab section for this class?”\nSingle-factual: questions that have answers containing a single explicit fact from the syllabus, e.g., “When are office hours?”\nMulti-factual questions that have answers combining multiple explicit facts from the syllabus, e.g., “What software and applications are used in this class?”\nSingle-hop reasoning: questions that have implicit answers requiring a single reasoning step, e.g., “I have not yet taken Advanced Biology, can I still take this course?”\nMulti-hop reasoning: questions that have implicit answers requiring multiple reasoning steps, e.g., “Can I start the class six weeks in and get an A?”\nSummarization: questions that have answers that require summarizing information from multiple parts of the syllabus, e.g., “Could you tell me how class participation grades are broken down?”\nAdversarial: questions that have no answers due to insufficient information, e.g., “Can I contact the instructor over Zoom?”\nQuestions in SyllabusQA can also be categorized by the source of their answers as defined below:\nExplicit: questions that have answers directly from the syllabus, including question types Yes/No, Single-factual, and Multi-factual.\nImplicit: questions that have answers not explicitly in the syllabus and require inference, including question types Single-hop reasoning, Multi-hop reasoning, and Summarization.\nInsufficient Information questions that do not have answers due to insufficient supporting information in the syllabus, including questions of type Adversarial.\nThe combination of multiple question types and answer sources makes SyllabusQA challenging for automated QA approaches. The involvement of adversarial questions, in particular, requires models to not hallucinate information not present in the syllabus, which is a critical test to pass before any potential deployment to real students."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Annotation Procedure",
            "text": "For simplicity, we use a straightforward annotation task: given a syllabus, we ask an annotator to simulate logistics-related QA pairs seen in real course classrooms. They write  QA pairs for each of the  question types for a total of  QA pairs, to ensure QA pairs are evenly distributed across question types. Before the task, we guide them through a tutorial to help them get familiar with the task and write diverse QA pairs. We give them a list of  diverse QA examples manually written by us, covering a wide spectrum of QA examples seen in classrooms, with  examples for each question type, on a single example syllabus. We encourage annotators to write open-ended questions with answers written in their own words, giving them the flexibility to come up with questions that are difficult to answer.\nSee Section K  ###reference_###  ###reference_### in the Supplementary Material for the instructions we provide.\nFor explicit questions with answers directly found in the syllabus, including the Yes/No, Single-Factual, and Multi-Factual types, an annotator additionally provides one, one, and up to five answer spans respectively, i.e., snippets directly copied from the reference syllabus, supporting their answer.\nFor Single-hop and Multi-hop reasoning questions, an annotator additionally provides one and up to five reasoning steps respectively, written in their own words to detail their thought process before arriving at the final answer.\nFor Summarization questions, an annotator additionally provides up to five answer spans from the syllabus used to construct the summary. This additional information provided by annotators encourages their answers to be cohesive and faithful to the reference syllabus and can be further used to aid development of automated QA approaches. For Adversarial questions, annotators do not write answers and we use “No/insufficient information” as the answer.\nSyllabusQA is designed to include questions written in different styles. Therefore, we recruited a large pool of over  annotators across two popular platforms, Amazon Mechanical Turk (AMT) and Prolific, all with an undergraduate bachelor’s degree or above, located in the United States or Canada.\nTo ensure diversity in language style, we asked each annotator to write a maximum of  QA pairs per syllabus, and a maximum of  total QA pairs across various syllabi. We collected this data over  months.\nTo ensure high data quality, we asked each annotator on AMT to write  QA pairs on an assigned syllabus in a screening session. We only include data from those who pass the screening test in our dataset and invite them for additional annotation rounds. For annotators on Prolific, in addition to Prolific’s screening process, we manually checked a random subset of QA pairs written by each annotator and found that their data meets our quality standards. For further quality control, we also run heuristics-based checks to filter out unsuitable QA pairs.\nSince our source syllabi are anonymized, we discard questions asking for personally identifiable information about instructors and/or teaching assistants. We also post-process answers to Yes/No type questions to a single word since some contain additional explanations.\nTo aid development of automated QA approaches, we explicitly aimed at encouraging language diversity across QA pairs in SyllabusQA by employing a large pool of annotators to reflect the diversity in real questions asked by students in classrooms. Therefore, inter-annotator agreement may be low, especially if evaluated on traditional text similarity measures. To formally measure this agreement, we had an independent expert annotator with extensive teaching experience as a college instructor, write oracle answers for around  of randomly sampled questions from the test set. We provide this expert with the same instructions and tutorial as the annotators and ask them to first select the question type before writing the answer accordingly.\nFollowing related work on open-form QA datasets Xu et al. (2022  ###reference_b30###  ###reference_b30###), we do not use traditional inter-annotator agreement metrics since we asked annotators to write QA pairs in their own words and style, which makes these metrics unsuitable Amidei et al. (2018  ###reference_b4###  ###reference_b4###). Instead, we mainly evaluate agreement on factual information overlap, which measures the overlap in key facts contained in the annotator’s answer and an oracle answer. We use a GPT-4-based evaluation of precision and recall of factual information present between the answers written by the expert and annotators, which we detail later in Section 4.2  ###reference_###  ###reference_###. This metric results in an inter-annotator precision of  and recall of , indicating high overlap in factual information. As a reference, agreement measured in traditional surface textual similarity metrics is  in ROUGE-L F1 Lin (2004  ###reference_b15###  ###reference_b15###) and  in BERTScore Zhang et al. (2020  ###reference_b32###  ###reference_b32###). This result is not surprising since we found that although annotator-written answers have varied surface language styles, they contain very similar key information.\nWe analyze annotator-written ground-truth answers where an independent expert annotator manually verified a random sample of QA pairs from  of the test set on: 1) precision and 2) recall. For precision, we test if the ground-truth answer is relevant to the question and supported by the syllabus. For recall, we test if the ground-truth answer contains all the relevant information required to answer the question. We found that  () of ground-truth answers have perfect precision (recall),  () of ground-truth answers have partial precision (recall), and  () of ground-truth answers have poor precision (recall). Through a qualitative error analysis, we find the following error types leading to less-than-perfect precision/recall in ground-truth answers: 1) imperfect annotator\nrecall/human error, 2) ambiguous syllabus information, and 3) ambiguous annotator-written answers. We provide details and examples in Section C  ###reference_###  ###reference_### in the Supplementary Material."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "SyllabusQA Data Analysis",
            "text": "###table_1### # pages / syllabus\n# tokens / syllabus\n# tokens / question\n# tokens / answer\nStatistics\nWe randomly split the dataset into train-val-test splits with a proportion of roughly -- in terms of QA pairs. We ensure no overlap in syllabi across splits to test the generalization ability of QA approaches to answer questions based on unseen new syllabi. We show detailed statistics of SyllabusQA in Table 2  ###reference_###. Overall, there are  QA pairs, almost uniformly distributed across  question types. There are  Explicit QA pairs,  Implicit pairs, and  Insufficient Information pairs. We also show a breakdown by question type in Section B  ###reference_### in the Supplementary Material.\nWe investigate question diversity in SyllabusQA, especially across questions based on the same reference syllabus. Questions with similar semantics across different syllabi are acceptable since their answers are grounded in different surrounding contexts in different locations. We compute the average intra-syllabus question pair similarity, then average across all syllabi, resulting in a ROUGE-L F1 score Lin (2004  ###reference_b15###) of , which indicates low intra-syllabus question similarity. This observation validates our design to encourage annotators to write diverse QA pairs with multiple question types and answer sources."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Comparison to Existing Datasets",
            "text": "We compare SyllabusQA to existing datasets both in the education domain and for more general-purpose QA in Table 1  ###reference_###. To the best of our knowledge, SyllabusQA is the first real publicly available course logistics-related QA dataset in educational settings; the data in Labutov et al. (2018  ###reference_b14###) is synthetically generated. ChaTA Hicke et al. (2023  ###reference_b9###) uses a proprietary dataset (with a publicly available sample) based on QA pairs on Piazza from an introductory computer science course. In contrast, SyllabusQA is public and more diverse by design, covering  majors. FairytaleQA Xu et al. (2022  ###reference_b30###) is a public dataset on reading comprehension QA from short fairytale story snippets of around  tokens. In contrast, SyllabusQA covers logistics-related QA with much longer context since the syllabi average  pages and K tokens long. CliCR Šuster and Daelemans (2018  ###reference_b23###) tests QA on medical documents with automatically generated Cloze-type questions with document spans as answers. In contrast, SyllabusQA is crowdsourced with diverse types of open-ended QA pairs written simulating teaching assistants.\nCompared to general-purpose QA datasets, NewsQA Trischler et al. (2017  ###reference_b26###) contains QA pairs grounded on news articles while QuAC Choi et al. (2018  ###reference_b5###) has QA on multi-turn dialogues, both with answers as text spans. In contrast, SyllabusQA contains QA pairs grounded on long course syllabi, which require techniques from long document QA. It also contains questions with open-ended answers. NarrativeQA Kočiský et al. (2018  ###reference_b13###) is sourced from movie scripts and books in plain text format. In contrast, SyllabusQA is sourced from course syllabi, containing different source text formats including tables and diagrams. HotpotQA Yang et al. (2018  ###reference_b31###) focuses on multi-hop reasoning questions. In contrast, SyllabusQA has seven different question types including summarization, multi-factual, and adversarial. OpenBookQA Mihaylov et al. (2018  ###reference_b17###) contains multiple-choice questions grounded on a set of facts. In contrast, SyllabusQA has different types of questions with open-ended answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "QA Performance on SyllabusQA",
            "text": "We benchmark several strong baselines using state-of-the-art LLMs on SyllabusQA. For open-source LLMs, we use the popular LLaMA 2 Touvron et al. (2023  ###reference_b25###) model family, including the chat variants of LLaMA 2-7B, LLaMA 2-13B, and LLaMA 2-70B. Using open-source LLMs ensures the reproducibility of results and mitigates student privacy concerns in real-world deployment. For completeness, we also benchmark against GPT-4 OpenAI (2023  ###reference_b19###) enabled with the retrieval assistant, a highly capable but proprietary LLM. We explore different approaches in our experiments including zero-shot prompting and supervised fine-tuning (SFT), with and without help from retrieval-augmented generation (RAG) techniques. We show all prompts in Section J  ###reference_### and model training details in Section F  ###reference_### in the Supplementary Material.\nWe design a novel LLM-based (GPT-4) evaluation metric which we call Fact-QA, inspired by FActScore Min et al. (2023  ###reference_b18###), to evaluate the factuality of predicted answers. FActScore evaluates information precision by breaking the generated text into a series of atomic facts and then computing the percentage of atomic facts supported by a reference knowledge source, such as Wikipedia. The key modification we make in our adaptation is to use the annotator-written ground truth answer as the reference source. We then swap the predicted and ground truth answers to compute information recall.\nTo compute precision, we compute the proportion of facts in the predicted answer supported by the reference answer, denoted as Fact-QA Precision. To compute recall, we compute the proportion of facts in the reference answer supported by the predicted answer, denoted as Fact-QA Recall. Intuitively, if a predicted answer has high precision, it is less likely to contain incorrect or irrelevant facts, signifying limited LLM hallucination. If a predicted answer has high recall, it is more likely to have covered all relevant facts needed to answer the question. Both aspects are important since a desirable answer should cover all necessary facts but nothing irrelevant, which may distract students when they read the answer. We also aggregate these metrics and compute a Fact-QA F1 score. Please see Section D  ###reference_### in the Supplementary Material for implementation details and our prompts.\nWe note that ideally, we should use the syllabus as the knowledge source to evaluate the factuality of an answer by providing it to GPT-4 for retrieval. However, due to the high cost associated with the OpenAI API, we use facts from the reference answer instead, which we found to be a high-quality summary of facts in the syllabus that are relevant to a question. Specifically, we examined  of questions from the test set, where we compared the Fact-QA Precision and Recall scores obtained by providing the entire syllabus to GPT-4 vs. providing only the annotator-written answer. Results show a high correlation, with Pearson correlation coefficients of  and , respectively, which suggest that when computing the Fact-QA metric, using facts from the much shorter reference answer is a good proxy for using the full syllabus.\nWe (the authors) investigate the effectiveness of our Fact-QA metric by performing a qualitative analysis to judge whether GPT-4’s output is accurate on  of the test set. We find that GPT-4’s outputs are correct, acceptable, and poor about , , and  of the times, respectively. By examining GPT-4’s errors in extracting facts and verifying claims, we find four major error types: 1) failures in logical reasoning, 2) overly granular atomic fact extraction, 3) arithmetic errors, and 4) incorrectly handling unanswerable questions (see Section D.2  ###reference_### in the Supplementary Material).\nWe (the authors) perform a human evaluation to examine the factual similarity between the expert annotator-written answers and the annotator-written ground-truth answers, on  of the test set. We find moderate to high Pearson correlation coefficients of , , and  with Fact-QA Precision, Fact-QA Recall, and Fact-QA F1, respectively, indicating that Fact-QA is an effective metric (see Section D.3  ###reference_### in the Supplementary Material).\n###table_2### Fact-QA F1\nFact-QA Precision\nFact-QA Recall\nROUGE-L F1\nBERTScore F1\nLLaMA-2-7B\n\n\n\n\n\nLLaMA-2-13B\n\n\n\n\n\nLLaMA-2-70B\n\n\n\n\n\nSearch Baseline\n\n\n\n\n\nLLaMA-2-7B + RAG\n\n\n\n\n\nLLaMA-2-13B + RAG\n\n\n\n\n\nLLaMA-2-70B + RAG\n\n\n\n\n\nLLaMA-2-7B + SFT\n\n\n\n\n\nLLaMA-2-13B + SFT\n\n\n\n\n\nLLaMA-2-7B + SFT + RAG\n\n\n\n\n\nLLaMA-2-13B + SFT + RAG\n\n\n\n\n\nLLaMA-2-13B + SFT + RAG + CoT\n\n\n\n\n\nGPT-4 + Retrieval Assistant\n\n\n\n\n\nHuman"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Approaches",
            "text": "In the basic Zero-shot approach, we prompt pre-trained LLaMA 2 models using only the question and system instructions in the prompt. We also experiment with Zero-shot with RAG, where we additionally include the top- relevant syllabi chunks retrieved in a process detailed below. As a sanity check, we include a Search Baseline where the answer to a question is simply the top- retrieved syllabus chunk, which reflects what a student would get using a simple keyword search (see Section E  ###reference_### in the Supplementary Material).\nIn SFT, we fine-tune LLaMA-2-7B and LLaMA-2-13B on the SyllabusQA training set. In SFT with RAG, we additionally include the top- relevant syllabi chunks retrieved from the reference syllabus and construct the input prompt similar to zero-shot with RAG for a fair comparison.\nIn Retrieval-augmented Generation (RAG), we retrieve chunks in the course syllabi that are relevant to the question and include them in the input to LLMs. Although a shorter syllabus can be completely included in the prompt, we chose to retrieve only relevant chunks from the syllabus to allow for generalization to longer syllabi. Moreover, using long input contexts packed with irrelevant information can decrease model performance Liu et al. (2023  ###reference_b16###). In our RAG pipeline, we parse syllabi from raw PDFs to text files using Adobe Acrobat’s PDF-to-text parser, which accurately parses varied text formats including tables, schedules, and lists. To accommodate the limited context length of LLMs, we chunk each syllabus to a maximum size of  characters per chunk and an overlap of  characters between adjacent chunks to preserve context. We use BM25 Robertson et al. (2009  ###reference_b21###), a popular and effective ranking function, to retrieve the top- syllabus chunks to include in the input.\nWe experiment with Chain-of-Thought prompting Wei et al. (2022  ###reference_b28###), leveraging the rich meta information of SyllabusQA. Specifically, we use an SFT with RAG approach on LLaMA-2-13B to first predict the question type, then generate reasoning steps written by the annotator for Single-hop and Multi-hop reasoning type questions only, before finally predicting the answer.\nWe also benchmark two additional approaches to provide more comparisons to the approaches above. The first is GPT-4 with Retrieval Assistant in a zero-shot prompting setting: we use the gpt-4-1106-preview model with the retrieval assistant from external files enabled, where we upload the raw PDF syllabus to GPT-4 and then prompt it to answer a question, along with system instructions explaining the task. Our prompt is shown in Section J  ###reference_### in the Supplementary Material. This approach provides us with an upper bound on performance of the zero-shot approach. The second is Human Performance, an estimated upper bound of QA performance on SyllabusQA for any approach. We simply calculate the agreement between answers written by the human expert and annotators on about  of the test set. This measure underestimates human performance, though, since our annotators are not instructors who are familiar with the syllabi, making them susceptible to imperfect recall, especially on longer syllabi."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "We now detail the evaluation metrics we use for the QA task on SyllabusQA. We report surface textual similarity using traditional metrics like ROUGE-L F1 Lin (2004  ###reference_b15###) and recent ones like BERTScore F1 Zhang et al. (2020  ###reference_b32###). Since these metrics are more aligned with language style rather than the factuality of an answer, we also use several metrics to measure the factuality of answers, which are more robust to surface textual features in answers than standard metrics.\nWe design a novel LLM-based (GPT-4) evaluation metric which we call Fact-QA, inspired by FActScore Min et al. (2023  ###reference_b18###  ###reference_b18###), to evaluate the factuality of predicted answers. FActScore evaluates information precision by breaking the generated text into a series of atomic facts and then computing the percentage of atomic facts supported by a reference knowledge source, such as Wikipedia. The key modification we make in our adaptation is to use the annotator-written ground truth answer as the reference source. We then swap the predicted and ground truth answers to compute information recall.\nTo compute precision, we compute the proportion of facts in the predicted answer supported by the reference answer, denoted as Fact-QA Precision. To compute recall, we compute the proportion of facts in the reference answer supported by the predicted answer, denoted as Fact-QA Recall. Intuitively, if a predicted answer has high precision, it is less likely to contain incorrect or irrelevant facts, signifying limited LLM hallucination. If a predicted answer has high recall, it is more likely to have covered all relevant facts needed to answer the question. Both aspects are important since a desirable answer should cover all necessary facts but nothing irrelevant, which may distract students when they read the answer. We also aggregate these metrics and compute a Fact-QA F1 score. Please see Section D  ###reference_###  ###reference_### in the Supplementary Material for implementation details and our prompts.\nWe note that ideally, we should use the syllabus as the knowledge source to evaluate the factuality of an answer by providing it to GPT-4 for retrieval. However, due to the high cost associated with the OpenAI API, we use facts from the reference answer instead, which we found to be a high-quality summary of facts in the syllabus that are relevant to a question. Specifically, we examined  of questions from the test set, where we compared the Fact-QA Precision and Recall scores obtained by providing the entire syllabus to GPT-4 vs. providing only the annotator-written answer. Results show a high correlation, with Pearson correlation coefficients of  and , respectively, which suggest that when computing the Fact-QA metric, using facts from the much shorter reference answer is a good proxy for using the full syllabus.\nWe (the authors) investigate the effectiveness of our Fact-QA metric by performing a qualitative analysis to judge whether GPT-4’s output is accurate on  of the test set. We find that GPT-4’s outputs are correct, acceptable, and poor about , , and  of the times, respectively. By examining GPT-4’s errors in extracting facts and verifying claims, we find four major error types: 1) failures in logical reasoning, 2) overly granular atomic fact extraction, 3) arithmetic errors, and 4) incorrectly handling unanswerable questions (see Section D.2  ###reference_###  ###reference_### in the Supplementary Material).\nWe (the authors) perform a human evaluation to examine the factual similarity between the expert annotator-written answers and the annotator-written ground-truth answers, on  of the test set. We find moderate to high Pearson correlation coefficients of , , and  with Fact-QA Precision, Fact-QA Recall, and Fact-QA F1, respectively, indicating that Fact-QA is an effective metric (see Section D.3  ###reference_###  ###reference_### in the Supplementary Material).\n###table_3### Fact-QA F1\nFact-QA Precision\nFact-QA Recall\nROUGE-L F1\nBERTScore F1\nLLaMA-2-7B\n\n\n\n\n\nLLaMA-2-13B\n\n\n\n\n\nLLaMA-2-70B\n\n\n\n\n\nSearch Baseline\n\n\n\n\n\nLLaMA-2-7B + RAG\n\n\n\n\n\nLLaMA-2-13B + RAG\n\n\n\n\n\nLLaMA-2-70B + RAG\n\n\n\n\n\nLLaMA-2-7B + SFT\n\n\n\n\n\nLLaMA-2-13B + SFT\n\n\n\n\n\nLLaMA-2-7B + SFT + RAG\n\n\n\n\n\nLLaMA-2-13B + SFT + RAG\n\n\n\n\n\nLLaMA-2-13B + SFT + RAG + CoT\n\n\n\n\n\nGPT-4 + Retrieval Assistant\n\n\n\n\n\nHuman"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Work and Conclusions",
            "text": "In this work, we introduced SyllabusQA, a diverse dataset consisting of real-world course syllabi and corresponding course logistics-related QA pairs. We also benchmarked the performance of several strong, LLM-based automated QA baselines on this dataset. Results show that fine-tuning and retrieval-augmented generation are helpful but there remains a gap between the performance of LLM-based approaches and that of humans on answer factuality. We will make the dataset and our evaluation metrics publicly available to facilitate future work on automated teaching assistant development. There are plenty of avenues for future work. First, we can leverage question meta information to improve the performance of LLM-based QA approaches, and use overgenerate-and-rank to improve their robustness. Second, we can collect human labels on answer factuality and further improve QA approaches using methods such as direct policy optimization Tian et al. (2023  ###reference_b24###)."
        }
    ]
}