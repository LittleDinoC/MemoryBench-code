{
    "title": "LlamBERT: Large-scale low-cost data annotation in NLP",
    "abstract": "Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks.\nDespite their effectiveness, the high costs associated with their use pose a challenge.\nWe present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa.\nThis strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus.\nOur results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the contemporary technological landscape, when confronted with the task of annotating a large corpus of natural language data using a natural language prompt, LLMs such as the proprietary GPT-4 [1  ###reference_b1###] and the open-source Llama 2 [2  ###reference_b2###] present themselves as compelling solutions.\nIndeed, minimal prompt-tuning enables them to be highly proficient in handling a wide variety of NLP tasks [3  ###reference_b3###].\nHowever, running such LLMs on millions of prompts demands large and expensive computational resources.\nThere have been optimization efforts aimed at achieving superior performance with reduced resource requirements [4  ###reference_b4###, 5  ###reference_b5###].\nNumerous studies have investigated the efficiency and resource requirements of LLMs versus smaller transformer encoders and humans\n[6  ###reference_b6###, 7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###, 10  ###reference_b10###, 11  ###reference_b11###].\nRecent advancements in data augmentation with LLMs [12  ###reference_b12###] underscore our approach, which relies on data labeling.\nGoing beyond the exclusive use of LLMs for a task, we combine LLMs with substantially smaller yet capable NLP models.\nA study closest to our approach is [13  ###reference_b13###], where GPT-NeoX was used to surrogate human annotation for solving named entity recognition.\nThrough two case studies, our research aims to assess the advantages and limitations of the approach we call LlamBERT, a hybrid methodology utilizing both LLMs and smaller-scale transformer encoders.\nThe first case study examines the partially annotated IMDb review dataset [14  ###reference_b14###] as a comparative baseline, while the second selects biomedical concepts from the UMLS Meta-Thesaurus [15  ###reference_b15###] to demonstrate potential applications.\nLeveraging LLM’s language modeling capabilities, while utilizing relatively modest resources, enhances their accessibility and enables new business opportunities.\nWe believe that such resource-efficient solutions can foster sustainable development and environmental stewardship."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "Given a large corpus of unlabeled natural language data, the suggested LlamBERT approach takes the following steps:\n(i) Annotate a reasonably sized, randomly selected subset of the corpus utilizing Llama 2 and a natural language prompt reflecting the labeling criteria;\n(ii) Parse the Llama 2 responses into the desired categories; (iii) Discard any data that fails to classify into any of the specified categories;\n(iv) Employ the resulting labels to perform supervised fine-tuning on a BERT classifier;\n(v) Apply the fine-tuned BERT classifier to annotate the original unlabeled corpus.\nWe explored two binary classification tasks, engineering the prompt to limit the LLM responses to one of the two binary choices.\nAs anticipated, our efforts to craft such a prompt were considerably more effective when utilizing the ’chat’ variants of Llama 2 [16  ###reference_b16###].\nWe investigated two versions: Llama-2-7b-chat running on a single A100 80GB GPU, and Llama-2-70b-chat requiring four such GPUs.\nWe also tested the performance of gpt-4-0613 using the OpenAI API."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The IMDb dataset",
            "text": "The Stanford Large Movie Review Dataset (IMDb) [14  ###reference_b14###] is a binary sentiment dataset commonly referenced in academic literature.\nIt comprises 25,000 labeled movie reviews for training purposes, 25,000 labeled reviews designated for testing, and an additional 50,000 unlabeled reviews that can be employed for supplementary self-supervised training.\nThis dataset serves as a fundamental baseline in NLP for classification problems, which allows us to evaluate our method against a well-established standard [17  ###reference_b17###, 18  ###reference_b18###, 19  ###reference_b19###]."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental results",
            "text": "All of the results in this section were measured on the entire IMDb sentiment test data.\nIn Table 1  ###reference_###, we compare the performance of Llama 2 and GPT-4 in different few-shot settings.\nDue to limited access to the OpenAI API, we only measured the 0-shot performance of GPT-4.\nThe results indicate that the number of few-shot examples has a significant impact on Llama-2-7b-chat.\nThis model exhibited a bias toward classifying the reviews as positive, but few-shot examples of negative sentiment effectively mitigated this.\nLikely due to reaching the context-length limit, 3-shot prompts did not outperform 2-shot prompts on Llama-2-7b-chat, achieving an accuracy of 87.27%.\nThe inference times shown in Table 1  ###reference_### depend on various factors, including the implementation and available hardware resources; they reflect the specific setup we used at the time of writing.\nIn Table 2  ###reference_###, we compare various pre-trained BERT models that were fine-tuned for five epochs on different training data with a batch size of 16.\nFirst, we established a baseline by using the original gold-standard training data.\nFor the LlamBERT results, training data labeling was conducted by the Llama-2-70b-chat model from 0-shot prompts.\nThe LlamBERT results were not far behind the baseline measurements, underscoring the practicality and effectiveness of the framework.\nIncorporating the extra 50,000 unlabeled data in LlamBERT resulted in a slight improvement in accuracy.\nWe also evaluated a combined strategy where we first fine-tuned with the extra data labeled by Llama-2-70b-chat, then with the gold training data.\nThe large version of RoBERTa performed the best on all 4 training scenarios, reaching a state-of-the-art accuracy of 96.68%.\nInference on the test data with roberta-large took 9m 18s, after fine-tuning for 2h 33m.\nThus, we can estimate that labeling the entirety of IMDb’s 7.816 million movie reviews [20  ###reference_b20###] would take about 48h 28m with roberta-large.\nIn contrast, the same task would require approximately 367 days on our setup using Llama-2-70b-chat, while demanding significantly more computing power."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "The UMLS dataset",
            "text": "The United Medical Language System (UMLS) [15  ###reference_b15###], developed by the United States National Library of Medicine, is a comprehensive and unified collection of nearly 200 biomedical vocabularies.\nIt has played a crucial role in fields such as natural language processing, ontology development, and information retrieval for over 30 years [22  ###reference_b22###].\nThe UMLS Metathesaurus consolidates various lexical variations of terms into single concepts, outlining their interrelationships.\nHowever, its breadth, with over 3 million concepts, complicates the selection of specific subsets for research due to its\nvague semantic labels.\nFaced with the need to identify a distinct subset of the Metathesaurus for subsequent research, we aimed to classify anatomical entities within it,\nbased on their relevance to the human nervous system.\nPrevious research on creating a neurological examination ontology involved extracting terms from case studies and manually mapping them to UMLS concepts, a task that can be extremely labor-intensive [23  ###reference_b23###].\nOur approach streamlines this process by efficiently leveraging the vast amount of knowledge condensed into LLMs and mitigates the need for expert annotation.\nBy selecting relevant semantic types spanning multiple biological scales, but excluding genes, we were able to reduce the number of concepts to approximately 150,000 anatomical structures, resulting in a still substantially large dataset.\nAmong these anatomical structures, we sought to find concepts related to the human nervous system, excluding purely vascular or musculoskeletal structures, and indirectly related entities such as the outer ear and eye lens.\nUsing distinct random samples, we annotated 1,000 concepts for testing and an additional 1,000 for hand-labeled fine-tuning.\nWe opted for a 1-shot prompt, on which Llama-2-7b-chat achieved an accuracy of 87.5%, while Llama-2-70b-chat reached 96.5%, and gpt-4-0613 scored 94.6%.\nFor fine-tuning BERT models, we labeled a distinct set of 10,000 concepts with Llama-2-70b-chat."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental results",
            "text": "As shown in Table 4  ###reference_###, fine-tuning general BERT models on the baseline hand-labeled dataset already yielded commendable results, however, our LlamBERT approach further improved these outcomes. Moreover, the combined strategy marginally surpassed Llama 2’s initial performance.\nWithin the biomedical domain, specific BERT models such as BiomedBERT-large [24  ###reference_b24###] were already accessible and predictably outperformed both bert-large and roberta-large across all training scenarios.\nYet, the combined approach using roberta-large demonstrated comparable performance, suggesting that our methodology could serve as an alternative to training domain-specific models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "Through two case studies showcasing the LlamBERT technique, we demonstrated the feasibility of efficiently labeling large quantities of natural language data with state-of-the-art LLMs.\nCombining the LlamBERT technique with fine-tuning on gold-standard data yielded the best results in both cases, achieving state-of-the-art accuracy on the IMDb benchmark.\nOur code is available on GitHub111https://github.com/aielte-research/LlamBERT  ###reference_T###.\nTo further increase the quality of data initially provided by the LLM annotation, we aim to incorporate PEFT [25  ###reference_b25###] techniques such as LoRA [26  ###reference_b26###], prefix tuning [27  ###reference_b27###], and P-tuning [28  ###reference_b28###] in the future."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Appendix",
            "text": "This appendix outlines the two prompts we used to engage the Llama 2 model for our article’s case studies.\nFew-shot examples contained the same prompt structure continued by the appropriate answer."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "IMDB prompt",
            "text": ""
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "UMLS prompt",
            "text": ""
        }
    ]
}