{
    "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
    "abstract": "A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts.\nAlthough contemporary text encoding methods cover most of the world’s writing systems, they exhibit bias towards the high-resource languages of the Global West.\nAs a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units.\nTo address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages.\nOur encoding convention (MYTE)\nis based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods.\nWe show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts.\nThis, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Multilingual language models have become the state-of-the-art solution for performing tasks on a wide range of languages Devlin et al. (2019  ###reference_b10###); Conneau et al. (2020  ###reference_b8###); Xue et al. (2021  ###reference_b35###).\nHowever, it is challenging to ensure high performance for all languages due to differences in data availability, especially for the long tail of low-resource languages Malkin et al. (2022  ###reference_b19###). This challenge is compounded by choices of how words are represented during tokenization; past studies have shown that multilingual models either cannot accurately represent texts in rare languages Pfeiffer et al. (2021  ###reference_b23###) or do so via over-segmentation, which is detrimental both to model performance and inference cost Petrov et al. (2023  ###reference_b22###); Ahia et al. (2023  ###reference_b2###).\n###figure_1### Byte-level models aim to solve these challenges. Rather than words or subword tokens, they use byte-level text representations that achieve high coverage Xue et al. (2022  ###reference_b34###), as common encodings such as UTF-8 support most of the world’s scripts.\nNevertheless, the over-segmentation problem still exists even at the byte level, as byte sequences for single characters are overly long for many non-Latin script languages Arnett et al. (2024  ###reference_b3###). This problem has an immense effect on modeling these scripts in NLP systems, as operating on longer sequences significantly increases the computation costs of training and inference in models, while also making learning less sample efficient.\nFurthermore, the billing for APIs such as ChatGPT (openai.com/chatgpt  ###reference_om/chatgpt###)\nis often associated with the segmented sequence length, disadvantaging speakers of specific languages Ahia et al. (2023  ###reference_b2###).\nIn this work, we propose a novel method to derive byte representations of text, enabling equitable segmentations across languages and scripts.\nIn our approach, we replace the current convention of assigning byte codes to characters with a morphology-driven approach, as morphemes111In this work, the usage of term “morphemes” encompasses both “morphemes” and “morphs”. Some linguistic theories use the term “morph” for specific textual realizations of abstract “morphemes”. For instance, in English, es as in foxes and s as in cats are two distinct “morphs” of a plurality “morpheme”. For an in-depth discussion about these two terms, see Section 4 of Žabokrtský et al. (2022  ###reference_b37###) are more informatively comparable constituents of text across languages than characters Cotterell et al. (2018  ###reference_b9###).\nSpecifically, we introduce a novel algorithm for representing text as byte sequences that is based on unsupervised morphological segmentation Smit et al. (2014  ###reference_b29###).\nWe demonstrate that our new paradigm for byte representation improves the segmentation of diverse languages of various scripts and morphological inventories. Furthermore, the segmentation of parallel sentences across languages converges to comparable lengths.\nWe test our method’s effectiveness in creating equitable text representation – representations that given parallel texts have similar encoded sequence lengths.\nWe then evaluate the applicability of the method to multilingual language modeling across 99 typologically diverse languages.\nOur contributions can be summarized as follows: (a) We propose a novel byte-encoding method that is morphologically driven; (b) We show empirically that the resulting representations are more equitable across languages than vanilla byte, character, or subword segmentation; (c) We analyze the typical lengths of these representations and show decreased\nsequence length across all analyzed languages, significantly reducing computation cost and benefiting non-Latin script languages the most; (d) We train a language model with our new representation scheme and demonstrate that it maintains balanced and better LM performance across diverse languages and exhibits faster inference speed. This improvements holds across different model scales.\nOur models match SOTA ByT5 performance across multiple tasks for diverse low-resource languages while being more efficient in training and inference.\nWe will release our code and models to facilitate further research in this direction."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background: UTF-8 Bytes",
            "text": "The vast majority of texts online222https://w3techs.com/technologies/overview/character_encoding  ###reference_character_encoding### are represented as bytes via UTF-8 convention, which is defined by the Unicode Standard The Unicode Consortium (2011  ###reference_b33###).\nIn UTF-8, each character (or codepoint) is represented as a sequence of one to four bytes.\nDue to the gradual development of communication standards, UTF-8 first allocated one-byte representation ASCII symbols, which cover primary Latin-script characters (see  0000 to  7F7F in Figure 2  ###reference_###).\nOther characters are represented as multi-byte codes starting with a byte from range  C2C2 to  F4F4 denoting the number of bytes in the codepoint and followed by continuation bytes from range  8080 to  BFBF.\nIn UTF-8 convention, characters in non-Latin alphabetic scripts (Cyrillic, Armenian, Georgian), diacritics, and abjads333Abjads are writing scripts that do not denote vowels, e.g., Hebrew, Arabic.\nusually have two-byte codes, while the byte length increases to three or four for Brahmic abugidas444Abugidas are scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and South-East Asia, e.g., Devanagari, Bengali. and CJK (Chinese, Japanese, Korean) logographs.\nAs a result, the granularity of byte codes varies significantly across languages; this means that texts conveying the same information across languages tend to be represented by byte sequences of significantly different lengths Arnett et al. (2024  ###reference_b3###).\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method: Morphology-Driven Bytes",
            "text": "As discussed in the prior section and shown in Figure 1  ###reference_###, UTF-8 convention produces longer byte sequences for some languages due to the development choices.\nTo make byte representation more equitable, we introduce an encoding paradigm that aims to assign byte codes of similar lengths to morphemes across languages.\nWe base our encoding scheme on morphological analysis because morphemes are\nthe shortest meaningful constituents and are independent of the writing convention Haspelmath and Sims (2010  ###reference_b11###).\nWe assume that the number of morphemes in sentences with the same information load is more balanced across languages than the number of characters, bytes, or tokens.\nThus, we enforce balanced segmentation granularity across languages.\nAn alternative approach to encoding morphological representations would be treating the union of multilingual morpheme inventories across languages as one large subword vocabulary.\nTo cover the morphemes of many languages in this manner, the vocabulary would be much larger than the ones usually applied to models.555The proposed MYTE encoding offers capacity for 2,130,432 of variable length codepoints. It is considerably more than in any of the commonly used subword vocabularies. For reference, large vocabulary XLM-V model allocates 1 million subwords Liang et al. (2023  ###reference_b16###). This would incur additional computational costs and, similar to other subword representations, would likely not generalize well to new, unseen languages."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Morphological Analysis",
            "text": "We train an unsupervised morphological analyzer, Morfessor Smit et al. (2014  ###reference_b29###) on lexicons derived from whole Wikipedia articles in 99 languages.\nThe morphological analysis is performed on each of the languages separately to balance the number of morphemes per language, regardless of data resourcefulness.\nFor each language, we derived a set of 4096 morphemes; the number was chosen to balance segmentation granularity across languages.\nFor each morpheme, we save its score, defined as the hypothetical loss reduction of the Morfessor model if the morpheme had not been included in the set.\nWe take the union of sets across languages to obtain a multilingual morpheme inventory.\nThe details of lexicon preparation and the usage of Morfessor are in Appendix A  ###reference_###.\nLatin\nLatin\nCommon\nMixed, Common, Inherited, Unkown\nNon-Latin Alphabetic\nGreek, Cyrillic, Armenian, Georgian\nAbjads\nHebrew, Arabic, Syriac, Thaana, Tifinagh\nAbugidas North\nDevanagari, Gurmukhi, Gujarati, Oriya, Bengali, Sinhala, Tibetan\nAbugidas South\nTelugu, Kannada, Tamil, Malayalam, Thai, Lao, Myanmar, Tai, Tagalog, Khmer\nCJK\nHangul, Han, Yi, Katakana, Hiragana, Bopomofo\nOther\nRemaining scripts"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Enriching Byte Representation with Morphology",
            "text": "To alleviate UTF-8 inefficiencies, we propose a systematic rearrangement of byte codepage.\nWe free 26 bytes (  4141 to  5A5A ) by decomposing capital letter codes into lowercase letters and capitalization markers. The first byte from this range (  4141 ) is repurposed as a capitalization marker.\nThe remaining 25 bytes are freed space used to store morphemes.\nOur method takes the sequences of UTF-8 bytes and transcodes them into shorter sequences using the vocabulary of the same size, i.e. 256, as depicted in Figure 1  ###reference_###. We apply the following steps to transcode UTF-8 sequences to MYTE encodings:\nWe use UTF-8 as base encoding of text.\nThen, the byte sequences are transcoded from left to right, merging morpheme sequences and replacing them as dedicated codepoints described in the following points.\nThe morphemes are grouped by scripts as shown in Table 1  ###reference_###.\nCodepoints of multiple scripts within a single morpheme are assigned to the second cluster (Mixed script).\nThe morphemes are ranked based on their Morfessor score defined in Section 3.1  ###reference_###.\nWe assign multibyte codepoint for each of the morphemes analogously to the UTF-8 convention (see Section 2  ###reference_###).\nSpecifically, the first byte denoting the beginning of the morphological codepoint is assigned from the freed range (  4242 -  5A5A ) based on the morph’s inclusion in one of the script groups.\nIt is followed by continuation bytes from the  element range  8080  -  BFBF, as in UTF-8 convention.\nThe  morphemes with the highest score are saved as two-byte codepoints, following  as three-byte codepoints; the remaining morphemes are saved as up to  four-byte codepoints. The capacity for new codepoints was not exhausted for any script group."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Equitable Multilingual Segmentation with MYTE",
            "text": "We first analyze the properties of our proposed morphology-driven encoding.\nFollowing the setting of Petrov et al. (2023  ###reference_b22###), we measure whether MYTE produces the segmented sequences of comparable length across languages.\nWe compute parity across languages using the multi-parallel corpus Flores 200 Team et al. (2022  ###reference_b32###).\nParity is defined as , where  and  stand for parallel sentences in language  and in English, respectively.  is the length of sequence  with segmentation method .\nWe compare the MYTE encoding from Section 3.2  ###reference_### to several baselines of common input representation: (a) Vanilla byte-level encoding via UTF-8; (b) Character-level encoding; (c) Subwords produced by Sentencepiece algorithm Kudo and Richardson (2018  ###reference_b14###). In comparison, we focus on the equitability of sequence lengths produced by the methods for diverse languages.\nFurthermore, we compare our morphological byte encoding sequence compression rate against the UTF-8 convention. Compression is essential for an effective text representation as it affects NLP systems’ efficiency and usage cost Ahia et al. (2023  ###reference_b2###).\nFinally, we check whether our method more effectively compresses languages and scripts unseen in MYTE algorithm described in Section 3.2  ###reference_###.\nThe comparison of sequence length across parallel sentences in Flores 200 is shown in Figure 4  ###reference_###. Our representation is more balanced across languages than the original UTF-8 bytes. There are still four languages with observably higher code lengths (e.g., Greek, Vietnamese, Punjabi, Khmer).\nHowever, MYTE encoding still improves their parity to English such that it is much lower than outlier languages in UTF-8 (1.7 vs. 3.5 in the worst-case languages, respectively).\nFigure 3  ###reference_### shows that MYTE representations are more balanced in parity scores across languages than subword tokenization. In particular, we improve on the long tail of languages over-segmented either in byte or subword encoding.\nThe parties closest to MYTE are obtained by character representation.\nHowever, the set of all Unicode characters is larger by orders of magnitude than the number of unique bytes used in MYTE (149,878 vs. 254).\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###table_1### The encoded sequence lengths are decreased with MYTE encoding for all languages, as depicted in Figure 3(c)  ###reference_sf3###.\nThe rate of compression varies from  1% for Vietnamese and Chinese to almost 70% for Burmese. As seen in Table 2  ###reference_###, the highest compression is obtained for low-resource languages with non-Latin scripts.\nNotably, this group of languages is the most susceptible to over-segmentation in UTF-8 encoding.\nIn Table 2  ###reference_###, we observe that a decrease in sequence length and parity applies to five low-resource languages not considered in constructing MYTE representation, referred to as unseen languages.\nOne exemption from the rule is Santhali, written in unseen Ol Chiki script, for which we do not observe a change in the encoded sequence length.\nThis observation highlights the importance of considering a wide range of languages and scripts when constructing morpheme inventories.\nImportantly, MYTE did not reach a capacity of available byte codepoints, and thus, the method can be extended to additional languages. The complete results for unseen languages and scripts are shown in Appendix B  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "MyT5: Language Modeling with MYTE",
            "text": "This section investigates the benefits of MYTE as an encoding scheme for byte-level language modeling.\nFor that purpose, we have trained T5 language models on MYTE representation.\nWe refer to these models as Myte T5 models, or MyT5 for short.\n###figure_11### ###figure_12### In Figure 4(a)  ###reference_sf1###, our model outperforms ByT5, producing lower (better) average  scores for all analyzed languages.\nThe improvement is strongly negatively correlated with the compression rate discussed in the previous section.\nThe gains are largest for languages using Abugidas (scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and SE Asia) that tend to be shortened the most by MYTE encoding.\nOn the other end of compression distribution, we still observe (smaller) improvement for Latin and CJK scripts.\nThis observation suggests that the MYTE encoding’s leverage is not constrained to shortening sequences, but it also uses codepoints that are easier to predict by a language model.\nMYTE uses codepoints based on morphemes that are inherently meaningful language units in contrast to orthographic symbols, which are the backbone of the UTF-8 convention.\n###figure_13### Previous works have argued that some languages are more challenging to model due to their morphological properties Cotterell et al. (2018  ###reference_b9###).\nIn contrast, others suggest that LM performance is linked with how texts in specific languages are represented Park et al. (2021  ###reference_b21###).\nOur results in Figure 6  ###reference_### support the latter view, as the predictability of the languages is balanced by using equitable underlying representation, i.e., MYTE encoding.\nSpecifically, we show that MyT5 achieves more balanced  across languages than ByT5.\nAs discussed in the previous section, the benefit is the starkest for languages prone to over-segmentation under UTF-8.\nThe smallest improvements of MyT5 are obtained for languages benefited by MYTE to a lesser extent, as observed in Section 4.1  ###reference_###: Greek and Vietnamese.\n###table_3### As shown in Figure 4(b)  ###reference_sf2###, MyT5’s inference time is shorter than that of ByT5 for almost all languages.\nThis behavior is mostly observed for Non-Latin script languages and can thus be attributed to the higher rates of compression observed when using the MYTE encoding scheme (Figure 4  ###reference_###).\nFurthermore, Table 3  ###reference_### demonstrates that MyT5’s inference speed gains over ByT5 improve with model size, hinting that MYTE will bring further efficiency gains when applied to models of larger scales.\n###figure_14### ###table_4### As shown in Table 4  ###reference_###, MyT5 and ByT5 perform comparably (and better than baselines) on MT and NER.\nWhile MyT5 outperforms ByT5 by 2 points on QA, the opposite is true for semantic parsing.\nWe hypothesize that in this case, the morphological prior encoded in MYTE may confound semantic parsing fine-tuning, which requires a structured output significantly different from natural language.\nFor all the tasks, the inference of MyT5 is faster than ByT5 (Figure 7  ###reference_###), mirroring our observations on language modeling efficiency.\nHowever, we do not observe a consistent relationship between the change in end task performance and efficiency, contrasting with the earlier observed correlation between  of inference time and  in multilingual language modeling."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Training Details",
            "text": "We base the architecture and implementation of our MyT5 model on the byte-level T5 model, i.e., ByT5 Xue et al. (2022  ###reference_b34###).\nByT5, like other T5 models Raffel et al. (2020  ###reference_b24###), is an encoder-decoder Transformer model trained on predicting masked spans of texts.\nByT5 operates on bytes instead of the subword tokenization in the standard T5 model, making it a suitable base model for our setting.\nWe pre-train three new instances of MYTE-level models of different sizes: small (300M), base (582M), and large (1.23B parameters). For pre-training, we used the standard task of restoring corrupted spans from mC4 corpus Raffel et al. (2020  ###reference_b24###).\nAll the byte sequences are transcoded into morphologically-driven bytes.\nWe use Jax implementation, i.e., t5x repository Roberts et al. (2022  ###reference_b25###), and the same hyperparameters as in ByT5 Xue et al. (2022  ###reference_b34###).\nThe only difference from their training approach is that we pre-train for 250,000 steps rather than one million steps since we observe overfitting when training for more steps, especially on low-resource languages. Chung et al. (2023  ###reference_b5###) similarly observed overfitting in multilingual T5 models caused by extensive duplications in the mC4 corpus, leading them to also train models for only 250,000 steps.\nIn evaluations, we compare against a reimplemented ByT5 instance trained for the same number of steps."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Experiments",
            "text": "We compare the performance of the MyT5 and ByT5 models, focusing on three aspects: language modeling performance, efficiency, and downstream evaluation.\nFirst, the multilingual language modeling performance of MyT5 – how is it, and is it comparable across languages?\nInspired by Cotterell et al. (2018  ###reference_b9###), we use the Bit-per-English-Byte metric on the multi-parallel FLORES 200 corpus to control for the informativeness of evaluation sequences:\nis a sequence of bytes (original UTF-8 or MYTE) with  being the -th byte.\nFor normalization, we use the number of UTF-8 bytes in English sentence  for fair comparison across languages and representation methods.\nIt is the main difference from perplexity, which is normalized by the sequence length and thus confounded by segmentation rates characteristic of individual languages and encodings.\nSecond, we compare inference times of text generation of MyT5 and ByT5.\nWe expect a decrease in sequence length, as shown in the last section, will render up to a quadratic reduction of forward-pass time due to the quadratic complexity of attention computation.\nFor both aspects, we report the results on three scales of the model (small, base, and large). Unless stated otherwise, we present the results of the large model.\nLastly, we compare models’ performance on four tasks from the Xtreme-Up benchmark Ruder et al. (2023  ###reference_b26###): question answering, named entity recognition, semantic parsing, and translation from English.\nIn each task, we fine-tune the large models on the multilingual data of all languages for each task.\nFine-tuned models are evaluated on test data for low-resource languages, following Ruder et al. (2023  ###reference_b26###).\nThe only exception is machine translation, where we fine-tune and evaluate on a subset of languages to reduce the computation cost.\nThe details of training and evaluation are provided in Appendix C  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "In Figure 4(a)  ###reference_sf1###  ###reference_sf1###, our model outperforms ByT5, producing lower (better) average  scores for all analyzed languages.\nThe improvement is strongly negatively correlated with the compression rate discussed in the previous section.\nThe gains are largest for languages using Abugidas (scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and SE Asia) that tend to be shortened the most by MYTE encoding.\nOn the other end of compression distribution, we still observe (smaller) improvement for Latin and CJK scripts.\nThis observation suggests that the MYTE encoding’s leverage is not constrained to shortening sequences, but it also uses codepoints that are easier to predict by a language model.\nMYTE uses codepoints based on morphemes that are inherently meaningful language units in contrast to orthographic symbols, which are the backbone of the UTF-8 convention.\n###figure_15### Previous works have argued that some languages are more challenging to model due to their morphological properties Cotterell et al. (2018  ###reference_b9###  ###reference_b9###).\nIn contrast, others suggest that LM performance is linked with how texts in specific languages are represented Park et al. (2021  ###reference_b21###  ###reference_b21###).\nOur results in Figure 6  ###reference_###  ###reference_### support the latter view, as the predictability of the languages is balanced by using equitable underlying representation, i.e., MYTE encoding.\nSpecifically, we show that MyT5 achieves more balanced  across languages than ByT5.\nAs discussed in the previous section, the benefit is the starkest for languages prone to over-segmentation under UTF-8.\nThe smallest improvements of MyT5 are obtained for languages benefited by MYTE to a lesser extent, as observed in Section 4.1  ###reference_###  ###reference_###: Greek and Vietnamese.\n###table_5### As shown in Figure 4(b)  ###reference_sf2###  ###reference_sf2###, MyT5’s inference time is shorter than that of ByT5 for almost all languages.\nThis behavior is mostly observed for Non-Latin script languages and can thus be attributed to the higher rates of compression observed when using the MYTE encoding scheme (Figure 4  ###reference_###  ###reference_###).\nFurthermore, Table 3  ###reference_###  ###reference_### demonstrates that MyT5’s inference speed gains over ByT5 improve with model size, hinting that MYTE will bring further efficiency gains when applied to models of larger scales.\n###figure_16### ###table_6### As shown in Table 4  ###reference_###  ###reference_###, MyT5 and ByT5 perform comparably (and better than baselines) on MT and NER.\nWhile MyT5 outperforms ByT5 by 2 points on QA, the opposite is true for semantic parsing.\nWe hypothesize that in this case, the morphological prior encoded in MYTE may confound semantic parsing fine-tuning, which requires a structured output significantly different from natural language.\nFor all the tasks, the inference of MyT5 is faster than ByT5 (Figure 7  ###reference_###  ###reference_###), mirroring our observations on language modeling efficiency.\nHowever, we do not observe a consistent relationship between the change in end task performance and efficiency, contrasting with the earlier observed correlation between  of inference time and  in multilingual language modeling."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Fair Representation across Languages",
            "text": "Perhaps the most significant challenge of multilingual NLP is the large disparity of resourcefulness across the world’s languages Joshi et al. (2020  ###reference_b13###), as the size and quality of data used for the model training directly affects its performance in individual languages.\nHence, researchers have proposed multiple ways to balance the training signal across languages Malkin et al. (2022  ###reference_b19###).\nSolutions include sampling data to overrepresent low-resource languages, e.g., with alpha Conneau et al. (2020  ###reference_b8###) or uniform sampling of data across languages Chung et al. (2023  ###reference_b5###). This unequal treatment of languages is also present in how data is encoded as input to the model Ahia et al. (2023  ###reference_b2###).\nPetrov et al. (2023  ###reference_b22###) show that practically all methods used to represent texts as input of NLP systems treat languages unequally, segmenting some (mainly the lowest-resourced ones) into fine-grained non-informative units.\nSome approaches aimed at balancing the segmentation or tokenization methods have been introduced.\nLimisiewicz et al. (2023  ###reference_b17###) proposed merging vocabulary based on the tokenizer scoring function.\nZheng et al. (2021  ###reference_b38###) introduced a method of allocating vocabulary capacity uniformly across languages, while Chung et al. (2020  ###reference_b6###) constructed multilingual vocabulary for clusters of languages and merged them.\nLiang et al. (2023  ###reference_b16###) combined the elements of both approaches and showed the advantage of extending vocabulary to benefit multilingual transfer.\nThese solutions offer the promise of obtaining a better allocation of vocabulary units.\nHowever, they do not solve the inequality of the underlying encoding, which may affect the construction process of vocabulary units.\nFor instance, byte merges in the BPE algorithm always begin at individual bytes Sennrich et al. (2016  ###reference_b28###); Zouhar et al. (2023  ###reference_b39###).\nMorphological analyzers, such as Morfessor, showed promising results for segmenting input texts for language models and neural machine translators Machácek et al. (2018  ###reference_b18###); Hou et al. (2023  ###reference_b12###). We are the first to apply morphology-based encoding for a massively multilingual setting."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Tokenization-free Language Modeling",
            "text": "An alternative to subword tokenization is representing texts directly as underlying encoding: characters or bytes.\nOr even representing texts as pixels of rendered text images Rust et al. (2023  ###reference_b27###).\nXue et al. (2022  ###reference_b34###) shows that for many non-Latin scripts, byte-level encoding performs worse than subword tokenization.\nThe problem with small units is that they do not carry meaningful information independently and often underperform subword models Sun et al. (2023  ###reference_b30###); Clark et al. (2022  ###reference_b7###).\nThe researchers have proposed multiple algorithms to enrich the byte-level embeddings with information from a local context.\nFor that purpose, recent approaches use shallow networks to aggregate information in local contexts defined as character n-grams Clark et al. (2022  ###reference_b7###), byte patches Yu et al. (2023  ###reference_b36###), or character blocks Tay et al. (2022  ###reference_b31###).\nHowever, the problem with choosing the appropriate context window is hard, because information density varies for different languages.\nA solution to that problem can be dynamically learning the segmentation in byte sequences Nawrot et al. (2023  ###reference_b20###).\nAnother approach is to redefine the encoding convention to equate the information loads in sequences, as the proposed MYTE approach."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduce MYTE encoding, a fairer byte-level representation for multilingual language modeling that is based on morphological segmentation. We show that adapting a morphological analyzer to unsupervised segmentation allows us to represent multi-parallel corpora with comparable encoding lengths across a wide range of languages. Additionally, our new representation significantly improves language modeling, especially of low-resource and non-Latin script languages, and provides efficiency benefits over traditional byte-level models. These trends hold across model sizes, with improvement increasing at scale.\nOverall, MYTE bridges the gap in encoding efficiency between high and low-resource languages, benefiting (to varying extent) all 99 analyzed languages."
        }
    ]
}