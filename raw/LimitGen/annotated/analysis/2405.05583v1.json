{
    "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
    "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs.\nDifficulties lie in assessing the factuality of free-form responses in open domains.\nAlso, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress.\nTo mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs.\nOpenFactCheck consists of three modules:\n(i) CustChecker allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims,\n(ii) LLMEval, a unified evaluation framework assesses LLM’s factuality ability from various perspectives fairly, and (iii) CheckerEval is an extensible solution for gauging the reliability of automatic fact-checkers’ verification results using human-annotated datasets.\nOpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated impressive capabilities in generating naturally-sounding answers over a broad range of human inquiries.\nHowever, GPT-4 (OpenAI, 2023  ###reference_b24###) and other text generation models still frequently produce content that deviates from real-world facts (Bang et al., 2023  ###reference_b2###; Borji, 2023  ###reference_b4###; Guiven, 2023  ###reference_b12###).\nThis degrades the system performance and undermines its reliability, representing a significant bottleneck in the deployment (Chuang et al., 2023  ###reference_b7###; Geng et al., 2023  ###reference_b10###).\nMany studies have explored evaluating and improving the factuality of LLMs Lee et al. (2022  ###reference_b16###); Chuang et al. (2023  ###reference_b7###); Shi et al. (2023  ###reference_b26###); Chen et al. (2023  ###reference_b5###).\nTwo challenges have been identified for evaluation: (1) it is difficult to assess the factuality of open-domain free-form responses, and (2) different papers use different evaluation datasets and measurements, rendering them hard to compare and hampering future progress Wang et al. (2024  ###reference_b29###).\nTo mitigate these issues, we introduce an open-sourced fact-checking system OpenFactCheck.\n###figure_1### It includes three modules as shown in Figure 1  ###reference_###. CustChecker allows users to customize an automatic fact-checker and to verify free-form documents to alleviate the first problem.\nA unified LLM factuality evaluation module LLMEval applies seven factuality-specific benchmarks to assess the LLM factuality ability from different aspects and then produces a report to illustrate the weakness and offer improvement advice, tackling the second challenge.\nWe further incorporate CheckerEval that assesses the verification accuracy of fact-checkers, equipped with a leaderboard in terms of accuracy, latency, and costs, aiming to encourage the development of advanced automatic fact-checking systems.\nThe three modules collaborate and help each other.\nThe results of human verification derived from LLMEval can be used as the benchmark for evaluating the accuracy of automated fact-checkers. Simultaneously, the most effective checker identified in CheckerEval can be deployed for automated fact-checking tasks.\nEach fact-checker in CheckerEval can be an implementation in CustChecker. Complex user inquiries may be considered as potential candidates included the factuality assessment dataset utilized in LLMEval.\nUsers can tailor their checkers according to their specific needs, such as domain specialization, cost-effectiveness, or rapid processing, and identify factual errors for both human-written text (a claim or document) and the outputs of LLMs.\nLLM researchers and practitioners can directly submit their LLM responses to our LLMEval platform by downloading our question set.\nSubsequently, we conduct evaluations to assess the model’s factual accuracy and to generate a report analyzing the model performance from multiple aspects.\nSimilarly, developers who seek to evaluate and to compare the efficacy of their fact-checking systems to other ones fairly can upload their checker’s verification outcomes to CheckerEval.\nThen, our system will show the ranking information in the leaderboard after evaluating under the same measurements.\nFrom the perspective of developers and contributors to this open-source project, we encourage extensive implementation of unique, effective, and robust claim processors, retrievers and verifiers within fact-checking pipelines, collections of challenging questions that LLMs tend to make factual errors, and human-annotated fine-grained verification examples. We believe that this will help to promote and to advance future research on LLM factuality.\nTo sum, this work investigates three research questions in LLM factuality evaluation:\nhow to effectively identify factual errors in an LLM response;\nhow to systematically evaluate the factuality ability of an LLM;\nwhich automatic fact-checker is the best, and which component dominates the final verification accuracy.\nWe initiate an open-source project and develop a preliminary version implementing the three modules, which is anticipated to serve as a stepping stone to facilitate future endeavors in this domain."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Fact-checking is the task of assessing whether claims made in writing are manipulated or true.\nObservations above motivate us to integrate these three closely-related components into one platform to facilitate (i) users to flexibly configurate an automatic fact-checking system to verify the factuality of claims and documents, (ii) LLM developers to evaluate LLM’s factuality under the same measurement scale including the task, dataset, and metrics, and (iii) researchers to assess the fact-checkers reliability under the fine-grained annotated benchmarks."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Fact-checking Systems",
            "text": "Many recent papers have described automatic fact-checking systems used to evaluate the factuality of LLM responses, such as RARR, FactScore, FacTool, CoVe, and Factcheck-GPT Gao et al. (2022  ###reference_b9###); Min et al. (2023  ###reference_b22###); Chern et al. (2023  ###reference_b6###); Dhuliawala et al. (2023  ###reference_b8###); Wang et al. (2023  ###reference_b28###).\nEach checker has unique characteristics designed for specific domains or scenarios.\nRARR verifies a document as a whole and can generate an attribution report to explain factual errors. FactScore retrieves evidence from an offline Wikipedia dump mainly for the biography. FacTool is friendly to users with low latency, CoVe completely depends on the capability of LLMs, and Factcheck-GPT has a fine-grained pipeline to localize intermediate errors.\nUnlike the above work, our aim is to enable the easy customization of a fact-checker according to users’ requirements and application scenarios, e.g., offline settings with a limited budget, by simply clicking dropdowns to choose offline retrievers and verifiers supported by small models without calling APIs.\nDespite different designs and implementations of various checkers, they generally consist of three modules: (1) claim processor, which extracts context-independent atomic claims from a document, (2) evidence retriever, which searches related passages from the Internet or database, and then ranks them by relevance, and (3) verifier, which determines the claim/document factuality based on the collected evidence (Guo et al., 2022  ###reference_b13###; Li et al., 2023b  ###reference_b18###; Wang et al., 2024  ###reference_b29###).\nTo this end, we first unify different fact-checking systems into a unified pipeline with the three modules. Then, given a module, users can select a developed module from various implementations or develop one by themselves.\nIn addition, the framework supports easy migration of existing fact-checking systems to our demo.\nHowever, the verification results of automatic fact-checkers are not necessarily accurate.\nHow to evaluate and improve the accuracy of automated fact-checkers is critical. The accuracy of automatic fact-checkers serves as a confidence and a reliability signal for the verification results."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Automatic Evaluation of Fact-Checking Systems",
            "text": "How accurate are current fact-checking systems? Can they effectively serve as proxies for evaluating the factual accuracy of language models?\nExisting automatic fact-checking studies often first collect a set of human-annotated (LLM response, extracted claims, factuality of the claims), and then quantify the effectiveness of their systems by comparing the final verification results (i.e., whether a claim or a document is factually true or false) to human-annotated labels Min et al. (2023  ###reference_b22###); Chern et al. (2023  ###reference_b6###); Dhuliawala et al. (2023  ###reference_b8###).\nRecent work on long-form factuality in LLMs also demonstrates a statistically significant correlation between the outputs by automatic fact-checkers and labels by human annotators (Wei et al., 2024  ###reference_b30###).\nThus, we merge four human-annotated LLM factuality datasets including FacTool-QA, FELM-WK, Factcheck-Bench, and HaluEval, and then compare them to the results of automatic fact-checkers to assess the performance of fact-checking systems."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "LLM Factuality Evaluation",
            "text": "Relevance vs. Factuality There are subtle differences between evaluating LLM’s general performance and factuality.\nQuestion answering (QA) datasets over various domains are always used for general performance evaluation (Hendrycks et al., 2021  ###reference_b14###). The focus is on judging whether the question is answered correctly. If the model’s response contains the correct answer, it counts; otherwise, it is void even though the response presents all facts.\nWhile research on factuality concentrates more on whether the response presents facts aligning with world knowledge even if some statements were irrelevant to the question.\nTherefore, instead of using datasets for general performance assessment, we selected seven datasets that are specifically collected for factuality evaluation.\nThey were selected to cover as diverse potential factual errors as possible, including aspects of vulnerabilities of snowballing hallucinations, awareness of self-uncertainty, robustness to false-premise questions, fresh questions with answers changing fast over time, and free-form responses spanning distinct domains, topics, and tasks.\nAfter evaluations over datasets from different perspectives, an analysis report will be generated to demonstrate the advantage and weaknesses of the model and the potential solutions to improve.\nObservations above motivate us to integrate these three closely-related components into one platform to facilitate (i) users to flexibly configurate an automatic fact-checking system to verify the factuality of claims and documents, (ii) LLM developers to evaluate LLM’s factuality under the same measurement scale including the task, dataset, and metrics, and (iii) researchers to assess the fact-checkers reliability under the fine-grained annotated benchmarks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Design and Implementation",
            "text": "Our OpenFactCheck platform is implemented by a Python server, a web user interface, and a database, deployed via AWS.\nThe Python backend can also be used as a Python toolkit, allowing easy and flexible development.\nOpenFactCheck’s design emphasizes two principles: (i) customizability and extensibility for both users and developers, and (ii) compatibility with existing methods and datasets.\nIt consists of three modules: CustChecker, LLMEval, and CheckerEval. Below, we present the detailed design and implementation of each component.\nWe unify diverse fact-checking systems as a procedure with three steps, abstracted into three classes: a claim_processor, a retriever, and a verifier.\nThe instances of the three classes are sequentially linked into a pipeline, solving the following tasks: (1) decomposing a document into atomic claims, (2) collecting relevant passages given a claim, and (3) making a true/false judgment given both the claim and the evidence as input (see Figure 2  ###reference_###). We refer to them as task solvers.\nThe implementation of a task solver can be flexible, just ensuring that the input and the output are aligned with the abstract class definitions. For example, evidence can be retrieved by calling SerpAPI or by searching Wikipedia using BM25, but we must return a list of relevant passages given an input claim.\nMoreover, task solvers in our pipeline are not hardcoded, but can be configured through a yaml configuration file. Thus, users can combine task-solver implementations from different frameworks (e.g., using Factcheck-GPT’s claim processor, RARR’s retriever, and FacTool’s verifier) and start the verification from any step. For example, users can start from the step of retrieval when the input document does not need decomposition.\nThis functionality is achieved using a message-passing mechanism that uses a success flag as the message indicates whether the current task solver successfully executes and returns the expected output.\nThe success flag passes through the configured order of the pipeline, guaranteeing that the output of the preceding solver fits the input for the current solver, otherwise error warning will be issued.\nPractically, the input and the output parameter names for the task solvers are defined in the configuration file. To link different solvers into a pipeline, one only needs to ensure that the current solver output name matches the input name of the succeeding solver. A dictionary format fact-checking-state is kept throughout the pipeline to store all information in the verification.\nInspired by Fairseq (Ott et al., 2019  ###reference_b25###), our framework is designed to be highly extendable by treating any third-party task solvers as plugins.\nAs long as the developed task solvers adhere to our class interface definitions, they can be imported and used in our framework.\nIn summary, due to the customizable and extendable nature of CustChecker, general users of OpenFactCheck can utilize it as a fully functional application with web-based user interfaces.\nAdvanced developers have the flexibility to use it as a library, allowing them to develop their task solvers or to integrate existing solvers.\nWe collect a set of questions by gathering questions from seven existing corpora that is collected deliberately to assess LLM’s factuality, including Snowball (Zhang et al., 2023a  ###reference_b34###), SelfAware (Yin et al., 2023  ###reference_b33###), FreshQA (Vu et al., 2023  ###reference_b27###), FacTool (Chern et al., 2023  ###reference_b6###), FELM-WK (Chen et al., 2023  ###reference_b5###), Factcheck-GPT (Wang et al., 2023  ###reference_b28###) and FactScore-Bio, a total of 6,480 examples shown in Table 1  ###reference_###, referring to FactQA (see dataset details in Appendix A.1  ###reference_###).\nEach example includes the following fields: question, domain, topic, ability to test, task and source, where the domain and the topic are identified using GPT-4 based on the (question, reference response), and others are either inherited or summarized from the original datasets.111We used GPT-4 response as a reference response for a question as it is more likely to provide a relevant and correct answer, assisting the identification of domains and topics.\nTo concretely analyze models’ vulnerability, we tag three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response.\nDomains include a general domain, law, biomedical, clinical, science and so on. Given a domain, we further define fine-grained topics for the question.\nThree common error types are included.\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information due to lacking relevant knowledge.\nThat is, LLMs lack relevant knowledge or internalize false knowledge in the pre-training stage or in the problematic alignment process.\nHowever, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses.\nMitigating such errors require: (a) learning and correcting parametric knowledge through the curation of corpora used in pre-training, supervised fine-tuning (SFT) and alignment, (b) augmenting by external knowledge in inference, (c) calibrating models to be aware of unknowns, and (d) configuring the decoding strategies (sample/beam-search, temperature), balancing diversity and accuracy Zhang et al. (2023b  ###reference_b35###); Wang et al. (2024  ###reference_b29###).\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated context, and provides an inaccurate or inappropriate response.\nThe left-to-right generation strategy used by LLMs poses potential risks that LLMs sometimes over-commit to the false premise in the context, even when they recognize they are incorrect (Zhang et al., 2023b  ###reference_b35###).\nTo address this issue, engineering better prompts is helpful, such as explicitly instructing models to first detect false premises in the prompt (Vu et al., 2023  ###reference_b27###) and asking the same question in a different way (Is 10733 a prime number?  What are the factors of 10733? Let’s think step-by-step.)\nType3: Disability error happens when the model is unable to search up-to-date information to correctly answer questions whose answers change over time, e.g., What is today’s gas price in New York (fast-changing), Who is the current spouse of Elon Musk (slow-changing).\nRetrieving external knowledge and augmenting it in the context would help.\nNote that we do not consider reasoning errors that arise when a claim employs flawed reasoning or faulty logic, and irrelevant error that denotes that the content is unrelated to the prompt (Chen et al., 2023  ###reference_b5###). The former highlights LLM’s reasoning ability, which is more reflected in math and reasoning tasks, and the latter has more to do with response’s helpfulness or human preference.\nThey are important in LLM evaluation, and may implicitly influence factuality, but we will first focus on explicit causes, leaving implicit claims for future work.\nFor questions that can be answered as Yes/No or have a short gold answer, we perform exact matching between the model responses and the gold standard answer to judge whether the response is factually correct or not, and then to calculate accuracy, such as for Snowball and SelfAware.\nThere are no short gold answers in FreshQA, and thus we use the FreshEval proposed in Vu et al. (2023  ###reference_b27###) to evaluate the correctness of model’s responses, in which few-shot in-context learning based on GPT-4 is used. We use a strict evaluation criterion, considering an answer to be correct only if all the claims in the response are factually true and also up-to-date.\nFor open-domain questions from the other four datasets with free-form and long responses, in which there are no gold standard answers, we use automatic fact-checking systems augmented with retrieved world-knowledge evidence to judge the correctness at the claim-level as well as at the document level."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "CustChecker",
            "text": "CustChecker allows users to customize a fact-checking system by selecting a claim processor, a retriever, and a verifier in web pages. The current version currently supports the following fact-checking systems: RARR, FacTool and Factcheck-GPT (Gao et al., 2022  ###reference_b9###; Chern et al., 2023  ###reference_b6###; Wang et al., 2023  ###reference_b28###).\nUsers input either human-written text or outputs of LLMs into the box (see Figure 4  ###reference_###), and then the fact-checker defined above will process and detect factual errors, showing the verification results including evidence, judgment, and explanations.\n###figure_2### We unify diverse fact-checking systems as a procedure with three steps, abstracted into three classes: a claim_processor, a retriever, and a verifier.\nThe instances of the three classes are sequentially linked into a pipeline, solving the following tasks: (1) decomposing a document into atomic claims, (2) collecting relevant passages given a claim, and (3) making a true/false judgment given both the claim and the evidence as input (see Figure 2  ###reference_###  ###reference_###). We refer to them as task solvers.\nThe implementation of a task solver can be flexible, just ensuring that the input and the output are aligned with the abstract class definitions. For example, evidence can be retrieved by calling SerpAPI or by searching Wikipedia using BM25, but we must return a list of relevant passages given an input claim.\nMoreover, task solvers in our pipeline are not hardcoded, but can be configured through a yaml configuration file. Thus, users can combine task-solver implementations from different frameworks (e.g., using Factcheck-GPT’s claim processor, RARR’s retriever, and FacTool’s verifier) and start the verification from any step. For example, users can start from the step of retrieval when the input document does not need decomposition.\nThis functionality is achieved using a message-passing mechanism that uses a success flag as the message indicates whether the current task solver successfully executes and returns the expected output.\nThe success flag passes through the configured order of the pipeline, guaranteeing that the output of the preceding solver fits the input for the current solver, otherwise error warning will be issued.\nPractically, the input and the output parameter names for the task solvers are defined in the configuration file. To link different solvers into a pipeline, one only needs to ensure that the current solver output name matches the input name of the succeeding solver. A dictionary format fact-checking-state is kept throughout the pipeline to store all information in the verification.\nInspired by Fairseq (Ott et al., 2019  ###reference_b25###  ###reference_b25###), our framework is designed to be highly extendable by treating any third-party task solvers as plugins.\nAs long as the developed task solvers adhere to our class interface definitions, they can be imported and used in our framework.\nIn summary, due to the customizable and extendable nature of CustChecker, general users of OpenFactCheck can utilize it as a fully functional application with web-based user interfaces.\nAdvanced developers have the flexibility to use it as a library, allowing them to develop their task solvers or to integrate existing solvers."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "LLMEval",
            "text": "We observed that studies assessing language models’ factuality or evaluating whether the methods are effective to mitigate model hallucinations use different datasets and metrics.\nThis makes it difficult to compare, in the same conditions, the factuality of different models as well as to compare the effectiveness of different factuality enhancement approaches.\nMoreover, a lot of prior work applied datasets such as MMLU (Hendrycks et al., 2021  ###reference_b14###), StrategyQA (Geva et al., 2021  ###reference_b11###) and HotpotQA (Yang and et al., 2018  ###reference_b32###) to evaluate model’s factuality.\nThese datasets tend to focus on evaluating the general performance, rather than factuality.\nTo this end, we first collect a dataset FactQA by gathering a large number of factual questions that probe diverse factual errors and span across a spectrum of domains, to fairly evaluate LLMs’ factuality under the same criteria.\nWe collect a set of questions by gathering questions from seven existing corpora that is collected deliberately to assess LLM’s factuality, including Snowball (Zhang et al., 2023a  ###reference_b34###  ###reference_b34###), SelfAware (Yin et al., 2023  ###reference_b33###  ###reference_b33###), FreshQA (Vu et al., 2023  ###reference_b27###  ###reference_b27###), FacTool (Chern et al., 2023  ###reference_b6###  ###reference_b6###), FELM-WK (Chen et al., 2023  ###reference_b5###  ###reference_b5###), Factcheck-GPT (Wang et al., 2023  ###reference_b28###  ###reference_b28###) and FactScore-Bio, a total of 6,480 examples shown in Table 1  ###reference_###  ###reference_###, referring to FactQA (see dataset details in Appendix A.1  ###reference_###  ###reference_###).\nEach example includes the following fields: question, domain, topic, ability to test, task and source, where the domain and the topic are identified using GPT-4 based on the (question, reference response), and others are either inherited or summarized from the original datasets.111We used GPT-4 response as a reference response for a question as it is more likely to provide a relevant and correct answer, assisting the identification of domains and topics.\nTo concretely analyze models’ vulnerability, we tag three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response.\nDomains include a general domain, law, biomedical, clinical, science and so on. Given a domain, we further define fine-grained topics for the question.\nThree common error types are included.\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information due to lacking relevant knowledge.\nThat is, LLMs lack relevant knowledge or internalize false knowledge in the pre-training stage or in the problematic alignment process.\nHowever, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses.\nMitigating such errors require: (a) learning and correcting parametric knowledge through the curation of corpora used in pre-training, supervised fine-tuning (SFT) and alignment, (b) augmenting by external knowledge in inference, (c) calibrating models to be aware of unknowns, and (d) configuring the decoding strategies (sample/beam-search, temperature), balancing diversity and accuracy Zhang et al. (2023b  ###reference_b35###  ###reference_b35###); Wang et al. (2024  ###reference_b29###  ###reference_b29###).\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated context, and provides an inaccurate or inappropriate response.\nThe left-to-right generation strategy used by LLMs poses potential risks that LLMs sometimes over-commit to the false premise in the context, even when they recognize they are incorrect (Zhang et al., 2023b  ###reference_b35###  ###reference_b35###).\nTo address this issue, engineering better prompts is helpful, such as explicitly instructing models to first detect false premises in the prompt (Vu et al., 2023  ###reference_b27###  ###reference_b27###) and asking the same question in a different way (Is 10733 a prime number?  What are the factors of 10733? Let’s think step-by-step.)\nType3: Disability error happens when the model is unable to search up-to-date information to correctly answer questions whose answers change over time, e.g., What is today’s gas price in New York (fast-changing), Who is the current spouse of Elon Musk (slow-changing).\nRetrieving external knowledge and augmenting it in the context would help.\nNote that we do not consider reasoning errors that arise when a claim employs flawed reasoning or faulty logic, and irrelevant error that denotes that the content is unrelated to the prompt (Chen et al., 2023  ###reference_b5###  ###reference_b5###). The former highlights LLM’s reasoning ability, which is more reflected in math and reasoning tasks, and the latter has more to do with response’s helpfulness or human preference.\nThey are important in LLM evaluation, and may implicitly influence factuality, but we will first focus on explicit causes, leaving implicit claims for future work.\nFor questions that can be answered as Yes/No or have a short gold answer, we perform exact matching between the model responses and the gold standard answer to judge whether the response is factually correct or not, and then to calculate accuracy, such as for Snowball and SelfAware.\nThere are no short gold answers in FreshQA, and thus we use the FreshEval proposed in Vu et al. (2023  ###reference_b27###  ###reference_b27###) to evaluate the correctness of model’s responses, in which few-shot in-context learning based on GPT-4 is used. We use a strict evaluation criterion, considering an answer to be correct only if all the claims in the response are factually true and also up-to-date.\nFor open-domain questions from the other four datasets with free-form and long responses, in which there are no gold standard answers, we use automatic fact-checking systems augmented with retrieved world-knowledge evidence to judge the correctness at the claim-level as well as at the document level."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "CheckerEval",
            "text": "Automatic fact-checking systems aim to identify whether a claim or a document is factually correct or not with/without references, but the results are not necessarily correct.\nTo assess the accuracy of automatic fact-checkers, we gather four LLM factuality benchmarks with human-annotated factual labels for three levels of granularity text: claims/segments/documents given (question, ChatGPT response) pairs, including FacTool, FELM-WK, Factcheck-GPT and HaluEval, a total of 4,835 examples, and we refer to them as FactBench.\nWe use precision, recall, and F1-score with respect to the True and False claims and documents to evaluate the effectiveness of fact-checking systems.\nThis method regards the system as a whole, only assessing the final verification results, i.e., whether a claim or a document is true or false.\nWithout the evaluation of intermediate results, it is difficult to localize which step eventually results in the erroneous factual judgment for claims Wang et al. (2023  ###reference_b28###).\nWe will incorporate the evaluation of intermediate results throughout the fact-checking pipelines in future updates."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Web Client",
            "text": "To enhance the user interaction experience, the corresponding UI interfaces have been developed for all functionalities within the demo. Specifically, we develop a web client based on Streamlit, which consists of four interfaces, each corresponding to one of the three modules, along with a leaderboard.\nCustChecker interface in Figure 4  ###reference_### primarily communicates with the CustChecker backend (§3.1  ###reference_###). Users can freely select different combinations of claim processors, retrievers, and verifiers. When given an input document or claim, the CustChecker back-end executes the fact-checking pipeline. The final verification results and the intermediate processing outcomes are then presented on the page to users for reference.\nLLMEval page in Figure 5  ###reference_### corresponds to the module of LLMEval (§3.2  ###reference_###). Users first download our predefined question set and then use their LLM for inference. After uploading the model responses, the system forwards them to background tasks, using our LLMEval for evaluation. Afterwards, a comprehensive report is generated and emailed to the user, notifying them of the availability of the report’s PDF for download. Moreover, if users consent to publish the evaluation results, we display them on the corresponding leaderboard page.\nCheckerEval page in Figure 6  ###reference_### corresponds to the module of CheckerEval (§3.3  ###reference_###), evaluating the performance of fact-checking systems.\nUsers can download claims or documents to be checked from this page, and then use their fact-checking system to predict factuality. The results including True/False, time, and USD costs are subsequently uploaded. We evaluate the submitted fact-checker results based on the ground truth labels of the human-annotated datasets, we rank and we display them on the leaderboard.\nLeaderboard page in Figure 7  ###reference_### is maintained for both the LLM factuality evaluation and the automatic fact-checking system evaluation. This leaderboard is updated in real time, allowing users to track their performance and to compare it to others. The leaderboard is accessible from the main page, providing a comprehensive overview of the system’s performance.\nThe design principle of our web client is to invoke these functional modules in the form of third-party independent applications, without excessively intervening in the system’s architecture. Consequently, our system is made available to users in the form of a library, a command-line toolkit, and a web application."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first evaluate the performance of different models, and then we assess the accuracy of different automatic fact-checking systems in different settings.\n###figure_3### ###figure_4### ###figure_5### On the Snowball dataset, we observe high error rates: >80% for LLaMA-2 and 65.5% for GPT-4, similar to the evaluation results on GPT-3.5-Turbo presented in Zhang et al. (2023a  ###reference_b34###).\nHowever, when justifying previously generated content, GPT-4 can identify 87% of its own mistakes.\nTherefore, in these cases, errors are mostly attributed to the over-committing to the previously generated false context, rather than to large knowledge gaps in LLMs.\nThis phenomenon is referred to as hallucination snowballing: an LLM over-commits to early mistakes, leading to more mistakes that it otherwise would not have made.\nIts prevalence in generative models leads to factual errors for simple facts.\nHigher precision than recall is achieved on SelfAware across three models when setting a positive label as unanswerable.\nThis reveals that many truly unanswerable questions are incorrectly recognized as answerable, implying that models are always not aware of what they do not know.\nPoor performance on questions with rapidly changing answers (FreshQA) illustrates the inherent challenge of retrieving up-to-date information for LLMs.\nWe used FacTool equipped with Serper and GPT-3.5-Turbo to automatically evaluate the factuality of free-form responses over prompts in FacTool-QA, FELM-WK, and Factcheck-Bench.\nThe results are shown in Figure 3  ###reference_###, where we can make several interesting observations:\nThe percentage of true claims is in the range of 89%-94%, revealing that the vast majority of claims are verified as true.\nThe questions in FacTool-QA are relatively more challenging for the three LLMs to answer correctly than for the other two datasets, leading to a relatively lower percentage of true claims. The apparent lower number of false claims in FacTool-QA stems from its smaller dataset size, where 50 is less than 94 and 184.\nGPT-4 has the best factuality performance with a smaller number of false claims and higher percentage of true claims, followed by LLaMA-2 13 and then 7B;\nThe cost for automatic evaluation mainly depends on the number of atomic claims and the price of the backend models used in FacTool. It spends $0.02 for an atomic claim on average.\nOverall, snowballing hallucination, over-commitment to false premise, difficulty in identifying unknown knowledge and answering with up-to-date information are still challenging issues for LLMs.\nFor general open-domain questions, on average less than 10% of the claims are factually incorrect in LLM responses.\nThis somehow implies that models may poorly understand questions and their own knowledge, but they can generate correctly documents.\nThis is aligned with the recent finding that what an LLM can generate, it may not understand (West et al., 2023  ###reference_b31###).\nAdditionally, it is costly to evaluate open-domain answers even if based on automatic fact-checkers,  $30 for 100 responses based on GPT-3.5-Turbo.\nTo ensure that all checking systems verify the same sets of annotated claims with factual labels, we skip the step of extracting atomic claims from the documents.\nAll fact-checking systems get a claim as an input, and they are expected to output whether or not the claim is true.\nRecent fact-checking frameworks such as FactScore, FacTool, Factcheck-GPT and commercial retrieval-augmented generative models such as Perplexity.ai are evaluated, with evidence retrieved from Wikipedia articles or web pages, as well as with various LLM-based verifiers that judge the factuality of a claim based on their internal knowledge and retrieved evidence as a reference.\nIn Table 5  ###reference_###, we observe that automatic fact-checking systems struggle to detect false claims. Across the three datasets we experiment with, it is consistently more arduous for these systems to differentiate false claims compared to identifying true ones.\nThis challenge may arise from the tendency of returning invalid evidence for false claims.\nRetrieving evidence from the web using Serper (Google search engine results) is more effective than sourcing related passages from Wikipedia articles using BM25, given that a wider array of effective evidence is accessible on open web pages for open-domain questions.\nThe verification accuracy of an LLM-based verifier primarily relies on the capabilities of the LLM and the effectiveness of the prompts used. For instance, the overall performance of GPT-4 surpasses that of both LLaMA-3-8B and GPT-3.5-Turbo, and thus the verification results (Factcheck-GPT) outperform those of FacTool, FactScore and Perplexity.ai, despite all of them utilizing evidence sourced from the web.\nWhile Factcheck-GPT exhibits superior effectiveness, it is associated with considerable latency and substantial costs (see Table 6  ###reference_###).\nLatency and cost are largely contingent upon the implementation strategy.\nFor instance, FacTool adopts asynchronous processing and leverages Serper ($0.001 per search) in conjunction with GPT-3.5-Turbo, rendering it faster and more economical compared to Factcheck-GPT. Notably, Factcheck-GPT uses SerpAPI ($0.015 per search) alongside GPT-4, where the cost of the most affordable GPT-4 model is 20 times that of GPT-3.5-Turbo (see Figure 8  ###reference_###).\nIn summary, the efficacy of automated fact-checking systems is fundamentally dependent on implementation factors such as choice of search tool, prompts, and backend LLMs. This is primarily driven by engineering considerations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "LLaMA-2 and GPT-4 Evaluation",
            "text": "Based on questions/instructions in FactQA, we collected responses from LLaMA-2 (7B, 13B) and GPT-4.\nAs shown in Table 3  ###reference_###, the responses of GPT-4 tend to be shorter than that of LLaMA-2.\nOn the Snowball dataset, we observe high error rates: >80% for LLaMA-2 and 65.5% for GPT-4, similar to the evaluation results on GPT-3.5-Turbo presented in Zhang et al. (2023a  ###reference_b34###  ###reference_b34###).\nHowever, when justifying previously generated content, GPT-4 can identify 87% of its own mistakes.\nTherefore, in these cases, errors are mostly attributed to the over-committing to the previously generated false context, rather than to large knowledge gaps in LLMs.\nThis phenomenon is referred to as hallucination snowballing: an LLM over-commits to early mistakes, leading to more mistakes that it otherwise would not have made.\nIts prevalence in generative models leads to factual errors for simple facts.\nHigher precision than recall is achieved on SelfAware across three models when setting a positive label as unanswerable.\nThis reveals that many truly unanswerable questions are incorrectly recognized as answerable, implying that models are always not aware of what they do not know.\nPoor performance on questions with rapidly changing answers (FreshQA) illustrates the inherent challenge of retrieving up-to-date information for LLMs.\nWe used FacTool equipped with Serper and GPT-3.5-Turbo to automatically evaluate the factuality of free-form responses over prompts in FacTool-QA, FELM-WK, and Factcheck-Bench.\nThe results are shown in Figure 3  ###reference_###  ###reference_###, where we can make several interesting observations:\nThe percentage of true claims is in the range of 89%-94%, revealing that the vast majority of claims are verified as true.\nThe questions in FacTool-QA are relatively more challenging for the three LLMs to answer correctly than for the other two datasets, leading to a relatively lower percentage of true claims. The apparent lower number of false claims in FacTool-QA stems from its smaller dataset size, where 50 is less than 94 and 184.\nGPT-4 has the best factuality performance with a smaller number of false claims and higher percentage of true claims, followed by LLaMA-2 13 and then 7B;\nThe cost for automatic evaluation mainly depends on the number of atomic claims and the price of the backend models used in FacTool. It spends $0.02 for an atomic claim on average.\nOverall, snowballing hallucination, over-commitment to false premise, difficulty in identifying unknown knowledge and answering with up-to-date information are still challenging issues for LLMs.\nFor general open-domain questions, on average less than 10% of the claims are factually incorrect in LLM responses.\nThis somehow implies that models may poorly understand questions and their own knowledge, but they can generate correctly documents.\nThis is aligned with the recent finding that what an LLM can generate, it may not understand (West et al., 2023  ###reference_b31###  ###reference_b31###).\nAdditionally, it is costly to evaluate open-domain answers even if based on automatic fact-checkers,  $30 for 100 responses based on GPT-3.5-Turbo."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We proposed OpenFactCheck, a unified, easy-to-use and extensible toolkit for LLM factuality evaluation.\nOpenFactCheck establishes a unified framework with clearly defined modules and flexible configurations to support the customization of automatic fact-checking systems.\nFrom the application level, OpenFactCheck allows general users to check whether a claim and a document are factual or not. This proposed framework could also facilitate LLM practitioners and developers to effectively and efficiently evaluate the factuality of their LLMs from various perspectives and to assess the accuracy of automatic fact-checkers.\nOur extensive experiments indicate that more than 90% of the claims generated by LLMs in response to open-domain questions are factually correct.\nNevertheless, models encounter challenges when addressing some straightforward questions such as Is 7411 a prime number?\nThis difficulty is attributed to recent findings suggesting that LLMs demonstrate weaker comprehension abilities relative to their generation capabilities.\nAdditionally, prevalent fact-checking systems struggle to identify false claims, with the retrieval of pertinent evidence posing a significant bottleneck. The latency and the cost associated with these systems primarily hinge on implementation strategies.\nIn the future, we will continue to integrate new techniques, features, and evaluation benchmarks to OpenFactCheck to facilitate the research progress of LLM fact-checking."
        }
    ]
}