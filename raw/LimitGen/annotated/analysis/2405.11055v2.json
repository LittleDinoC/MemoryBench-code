{
    "title": "Leveraging Discourse Structure for Extractive Meeting Summarization",
    "abstract": "We introduce an extractive summarization system for meetings that leverages discourse structure to better identify salient information from complex multi-party discussions. Using discourse graphs to represent semantic relations between the contents of utterances in a meeting, we train a GNN-based node classification model to select the most important utterances, which are then combined to create an extractive summary. Experimental results on AMI and ICSI demonstrate that our approach surpasses existing text-based and graph-based extractive summarization systems, as measured by both classification and summarization metrics. Additionally, we conduct ablation studies on discourse structure and relation type to provide insights for future NLP applications leveraging discourse analysis theory.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, the task of meeting summarization has garnered significant attention Rennard et al. (2023b  ###reference_b35###) due in part to the rising adoption of videoconferencing Kost (2020  ###reference_b15###) and the accumulated amount of meeting recordings, highlighting the need for automatic, novel and efficient processing methods.\nDespite recent advancements in natural language processing, the challenge of generating concise and coherent summaries from lengthy, multi-party meeting transcriptions remains.\nLarge language models suffer from the middle curse (Liu et al., 2024  ###reference_b21###) and struggle to use information in the middle of their context window, particularly for summarization (Ravaut et al., 2023  ###reference_b32###).\nMoreover, dealing with meetings presents several challenges that are not typically encountered in traditional document summarization (Shang, 2021  ###reference_b37###). Meetings are often characterized by spontaneous interaction, leading to phenomena such as poorly formed utterances and overlapping speech that negatively impact the accuracy of speech-to-text models—the first step in an automated meeting summarization pipeline. Spontaneous speech also leads to digressions, reformulations and repetitions, which can increase transcript length while simultaneously diluting information density. A further problem is the private nature of many meetings, which greatly limits the quantity of potential public training data.\nOne strategy to offset data scarcity and content sparseness is to enrich conversation transcripts with additional information in the hopes of enhancing a system’s dialogue understanding. One might add information about the discursive function that an utterance plays, for instance; that is, whether it serves to introduce or answer a question, to acknowledge another utterance or so on. In this vein, Feng et al. (2020  ###reference_b8###) showed that representing the content of a meeting transcript as a discourse graph in the style of Segmented Discourse Representation Theory (SDRT; Asher, 1993  ###reference_b3###; Lascarides and Asher, 2008  ###reference_b17###) can improve performance on abstractive meeting summarization, while Goo and Chen (2018  ###reference_b9###) demonstrated the value of dialogue acts (Jurafsky et al., 1997  ###reference_b12###; Allen and Core, 1997  ###reference_b2###) for the same task.\nSimilarly, Liu et al. (2019  ###reference_b20###) showed that supplementing transcripts with multi-modal information about participants’ head orientation and eye gaze can help to identify salient information in a meeting.\nIn this paper, we exploit information on discourse structure to improve extractive summarization. While abstractive summarization is generally preferable for spontaneous conversation (Murray et al., 2010  ###reference_b28###), focusing on extractive summarization is valuable for multiple reasons.\nFirst, even if it is not entirely immune to hallucinations (Zhang et al., 2023  ###reference_b49###), extractive summarization does not suffer from this phenomenon at the level that its abstractive counterpart does (Cao et al., 2018  ###reference_b6###). On the one hand, this makes extractive summarization an attractive final product in itself in contexts where reliability is crucial. On the other hand, this, combined with the fact that extractive summarization is easier to evaluate than abstractive summarization, makes it a clearer lens through which to study the interaction between discourse structure and content salience. Finally, as a part of a pipeline for abstractive summarization (Shang et al., 2020  ###reference_b39###), it can be used to reduce the number of tokens given as input to a generative system with context length constraints, such as a transformer-based model.\nOur approach involves a novel combination of Graph Neural Networks (GNN) and graph-based representations of discourse in which each node in a graph represents the content of an individual utterance and each edge represents a semantic relation between two utterances whose nature is specified by a label, e.g., Explanation, Correction, Question-Answer Pair or Acknowledgment.\nThe task of extractive summarization is then couched as one of binary node-classification in which nodes are determined to be important or not, and the content of the nodes judged to be important is what determines the final extractive summary. This approach, in contrast to the generation at the whole graph-level (Feng et al., 2020  ###reference_b8###), allows us to gain fine-grained insight into the interaction between the importance of an utterance in a conversation and its role in the overall discourse.\nA further advantage of a graph-based approach is that GNN, whose attention is focused only on the neighbors of a given node, do not suffer from the context length restrictions imposed by the pairwise attentional computations of transformer-based models. This means that our extractive summarization approach can take an entire long meeting transcript as input without special treatment.\nTo validate our discourse-structure approach, we conduct extensive experiments on the AMI (Mccowan et al., 2005  ###reference_b26###) and ICSI (Janin et al., 2003  ###reference_b11###) corpora, comparing our method against diverse extractive summarization systems. The results demonstrate that our approach outperforms traditional techniques across evaluation metrics for both classification and summarization, including F1 score, ROUGE and BERTScore. Additionally, we present a detailed analysis of the impact of various graph construction strategies on summarization quality, offering insights into the mechanisms through which discourse structure influences content selection. Our study not only provides a novel methodological contribution to the field of automatic meeting summarization but also sheds light on the underlying discourse processes that shape effective summaries.\nFinally, discourse structure has seldom been explored in the literature for its potential utility in downstream tasks. Our work is among the few studies that focus on this effort, especially for dialogues."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Most work focuses on abstractive summarization and tends to be organized in three interconnected streams (Rennard et al., 2023b  ###reference_b35###). The first focuses on enhancing transcripts with additional information—such as annotations for dialogue acts (Goo and Chen, 2018  ###reference_b9###), discourse structure (Feng et al., 2020  ###reference_b8###) or visual cues (Liu et al., 2019  ###reference_b20###)—that is assumed to be relevant for abstracting important content from a transcript. While adding information does show improvement, few papers actually quantify how much information is gained from adding linguistical features; Indeed, Goo and Chen (2018  ###reference_b9###) worked on a version of AMI that lacked the current version of abstractive summaries, making it difficult to draw conclusions that are comparable to more recent works. Additionally, Feng et al. (2020  ###reference_b8###) used a global node to represent the discourse graph, aggregating all features into a single representative vector sent to an LSTM-based pointer decoder network for summarization. This approach is not ideal for evaluating the role of discourse structure in enhancing abstractive summaries. The second stream aims to transform and compress meeting transcripts in order to produce cleaner and more condensed intermediate documents that can then be passed to downstream generation modules (Krishna et al., 2021  ###reference_b16###; Oya et al., 2014  ###reference_b31###). The third stream is dedicated to the development of general generation frameworks, such as DialogLM (Zhong et al., 2022  ###reference_b51###), that are specifically designed to handle dialogue content. While we focus on extractive summarization, in exploiting discourse structure, our approach overlaps with work in the first stream above and lays groundwork for the second stream by providing a means of compressing meeting transcripts.\nTurning to extractive approaches to meeting summarization, Murray et al. (2005  ###reference_b29###) evaluated the effectiveness of latent semantic analysis, TF-IDF, and MMR algorithms for this task, providing benchmarks for all three methodologies. More recently, Tixier et al. (2017  ###reference_b43###) developed a technique to construct a graph from key words in a text and then apply submodular optimization to summarize content effectively.\nWe note that while significant strides have been made in developing sophisticated algorithms and techniques for meeting summarization, a problem that persists, for both extractive and abstractive summarization, is that of evaluation. Kirstein et al. (2024  ###reference_b14###) details the complications posed by different evaluation metrics and how they correlate with various challenges specific to meeting summarization. These problems are aggravated by the lack of readily available data in English that could be used to evaluate summarization of long format dialogues Janin et al. (2003  ###reference_b11###); Mccowan et al. (2005  ###reference_b26###); Hu et al. (2023  ###reference_b10###)—a problem that only becomes worse when we look at languages beyond English (Wu et al., 2023  ###reference_b46###; Rennard et al., 2023a  ###reference_b34###; Nedoluzhko et al., 2022  ###reference_b30###).\nTwo primary theoretical frameworks have been developed to analyze complete discourse structures in the form of graphs, namely Rhetorical Structure Theory (RST; Mann and Thompson, 1987  ###reference_b25###) and Segmented Discourse Representation Theory (SDRT; Asher, 1993  ###reference_b3###; Lascarides and Asher, 2008  ###reference_b17###). The framework of RST has been successfully applied for extractive summarization of newspaper articles (Xu et al., 2020  ###reference_b47###; Liu and Chen, 2019  ###reference_b23###). However most work on discourse graphs has concentrated primarily on producing high-quality graphs (Shi and Huang, 2019  ###reference_b41###; Liu and Chen, 2021  ###reference_b24###; Bennis et al., 2023  ###reference_b5###), with less emphasis on their applications to downstream tasks such as summarization, and only SDRT has been applied to build discourse structures for multi-party conversation (Asher et al., 2016  ###reference_b4###), which is needed for meeting summarization.\nA major limitation for research on discourse structure is of course the intense effort that goes in to annotating datasets for discourse structure. There are only two readily available discourse-annotated corpora for multiparty conversation: STAC (Asher et al., 2016  ###reference_b4###), a corpus of chats from an online version of the game Settlers of Catan, and Molweni (Li et al., 2020  ###reference_b18###), an annotated version of the Ubuntu Chat Corpus. Both corpora are annotated in the style of SDRT.\nNumerous models for building graphs have been trained on the STAC and/or Molweni corpora. Shi and Huang (2019  ###reference_b41###) proposed a model that incrementally predicts both graph edges and their labels at a time  by taking into account edge and label predictions before . Liu and Chen (2021  ###reference_b24###) enhanced Shi and Huang  ###reference_b41###’s model by incorporating cross-domain information, namely by using cross-domain pretraining and vocabulary refinement. Wang et al. (2021  ###reference_b45###) uses a structure transformer to incorporate node and edge information of utterance pairs, and Bennis et al. (2023  ###reference_b5###) utilizes a BERT-based framework to encode utterance pairs, predicting discourse attachments (graph edges) with a linear layer and then using a multitask approach to predict edge labels. Yang et al. (2021  ###reference_b48###) also exploit a multi-task approach to jointly learn relation edges and labels and pronoun resolution, demonstrating the synergistic interaction of these tasks. Finally, Fan et al. (2023  ###reference_b7###) combined discourse parsing with addressee recognition in a multitask approach. In our evaluations, we focus on the frameworks of Shi and Huang (2019  ###reference_b41###); Wang et al. (2021  ###reference_b45###) and Bennis et al. (2023  ###reference_b5###) as they do not require additional data."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Due to its appropriateness for multiparty dialogue (Asher et al., 2016  ###reference_b4###) and to the fact that it is the framework adopted for the STAC and Molweni corpora, we focus on SDRT in what follows.\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task definition",
            "text": "Extractive summarization aims at creating a summary  from an input meeting , in which  consists of  elementary discourse units (EDUs)  and  consists of  EDUs, where  and . We borrow the notion of an EDU from SDRT, in which an EDU corresponds to the content of a single atomic clause so that the sentence “The remote should glow in the dark so that it’s easier to find” would be two EDUs111We note that while the STAC and Molweni are segmented in this way, the transcripts of AMI and ICSI, which we use for our evaluations, are segmented slightly differently. The main difference is that when two consecutive clauses play the same discourse function in AMI and ICSI, they are not segmented. This means a sentence like “My name is Ed and I am the project manager” would not be segmented in these corpora because both independent clauses serve just to introduce the speaker. Because of the subtlety of this difference, we use the term EDU for simplicity..\nIt is worth to note that the input length of meeting  can be quite large and is often longer than the input size of most language models. E.g., the average length of ICSI meetings is 13317 tokens, while Llama’s (Touvron et al., 2023  ###reference_b44###) context window is of 4096 tokens, which highlights the advantage of our graph-based method."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Discourse graphs",
            "text": "Discourse structure in SDRT is represented as a weakly-connected graph where each node represents the content of an EDU222SDRT allows for complex discourse units (CDUs) as well, which are subgraphs, but we do not treat CDUs in this paper and they are not annotated in the corpora that we use., and an edge between two nodes indicates that the corresponding EDUs are related through a discourse relation. Typical SDRT relations, which are indicated as edge labels, include: Comment, Clarification-question, Elaboration, Acknowledgement, Continuation, Explanation, Conditional, Question-Answer pair, Alternation, Question-Elaboration, Result, Background, Narration, Correction, Parallel, Contrast.\n.\n\\a. ID: Does anyone know if VCRs are the same across <disfmarker> international?\n.̱ PM: They’re not <disfmarker> no. \n.̧ ME: They’re not, no.\n.̣ ID: [Okay,] [so you’d need like a whole different set of buttons for everybody’s VCRs.]\n\\e. PM: Yeah, that’s right, yeah.\nThe graph for example 3.2  ###reference_###333Example 3.2  ###reference_### is a slightly cleaned up version of an extract from meeting ES2011b of the AMI corpus. is given in Figure 2  ###reference_5###, where QAP stands for Question-Answer Pair and Ack is short for Acknowledgment."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Our model",
            "text": "Our model is illustrated in Figure 1  ###reference_###. This section details the components it uses to produce an extractive summary from a meeting transcription.\nTo perform node-classification on a graph, we first need to provide an initial representation of the nodes. Since we are dealing with text, we simply use a text embedding model to get representations for EDUs. We chose MiniLM Reimers and Gurevych (2019  ###reference_b33###), as it is a widely available sentence embedding model.\nTo generate the graph structure, we need to add edges and edge labels for different discourse relations. Because discourse annotations require expert annotators and are very labor intensive, we opted to generate the edges and labels for our graphs with an automatic parser. In this paper, except for the ablation studies in which we consider the parsers of (Liu and Chen, 2021  ###reference_b24###) and (Bennis et al., 2023  ###reference_b5###), we use Deep Sequential Shi and Huang (2019  ###reference_b41###).\nFor a meeting  with  EDUs, the Graph Generator outputs a meeting graph , where  is the set of  nodes ; , a set of  edges  for some ;\nand , a set of  relation labels  = , where  is one of the relation types listed in Subsection 3.2  ###reference_###.\nWith the initial features  of each node  created by the EDU Embedding Module and the graph  in place, we use two different architectures to evaluate the impact of discourse structure on node classification.\nFirst, we use a Relational Graph Convolutional Network (RGCN; Schlichtkrull et al., 2018  ###reference_b36###) to gather local hidden features weighted by the edge label. The RGCN does message passing, calculating the representation of a node  at the step  ( being the number of layers in the RGCN) computed as:\nin which  denotes the set of neighbors of node  under relation  and  denotes learnable parameters at the  layer for the relation . Finally, we send the final representation  to be classified by a sigmoid function on whether its content should be included or not in extractive summary.\nSecond, to evaluate the influence of graph structure alone while ignoring edge labels, we use MixHop GCN (Abu-El-Haija et al., 2019  ###reference_b1###). This model architecture supports multiple “hops”, allowing it to access and incorporate broader structural information from nodes that are more than one edge away. This relation-agnostic GNN allows us to quantify the extent to which the graph structure itself contributes to the model’s performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We experiment on two conversational datasets containing extractive summaries: AMI (Mccowan et al., 2005  ###reference_b26###) and ICSI (Janin et al., 2003  ###reference_b11###). Following Shang et al. (2018  ###reference_b38###), we split AMI and ICSI into training, validation, and test sets. Basics statistics are provided in Table 1  ###reference_###.\n###table_1### ###table_2### We evaluate our model based on both node classification and content of the overall summary. For the former, we use the traditional  score for classification. For the latter, we report the  score for ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004  ###reference_b19###), as well as BERTScore (Zhang et al., 2020  ###reference_b50###) to measure the difference between a ground truth extractive summary and a system generated one. We conduct a budgetization process by cutting the generated summaries (from the start) to be of the average length of the ground truth summaries when evaluating our model and baselines described below.\nWe use a variety of models to establish baselines that are both graph-based and text-based. From the node classification perspective (see Table 2  ###reference_###), we first evaluate the strength of the input embeddings by classifying the given utterance representation with a simple Logistical Regression, as well as with a Multilayer perceptron (MLP). Additionally, Graph Convolutional Network (GCN; Kipf and Welling, 2016  ###reference_b13###), which yields a GNN baseline score that does not take any of the relations into account.\nFrom the text summarization perspective (see Table 3  ###reference_###), in addition to all the above baselines we evaluate: BERTSumExt (Liu and Lapata, 2019  ###reference_b22###), which embeds individual input sentences with BERT and uses multiple inter-sentence transformer layers stacked on top of the BERT outputs to capture document-level features for extracting summaries; TextRank (Mihalcea and Tarau, 2004  ###reference_b27###), a simple, graph-based and unsupervised extractive method;\nCoreRank (Tixier et al., 2017  ###reference_b43###), whose approach relies on a Graph-of-Words representation of the meeting, graph degeneracy and submodularity for unsupervised extractive summarization;\nFinally, we use Lead-N as a heuristic metric, which takes the first N words of the meeting as its summary, N being the budgeted length of summaries.\nWe can observe from both Table 2  ###reference_### and Table 3  ###reference_### that our method using MixHop GCN and RGCN consistently achieves higher scores in terms of both classification and summarization metrics. Additionally, we can see that embedding-based methods such as MLP and Logistic Regression are more competitive than the heuristic-based method or unsupervised graph-based ones. We also evaluate different ranking methods for budgetization to improve the final summary. One approach involves selecting the longest utterances identified as important by the model until the required budget length is met (MixHop GCN + Ranking by length). Another approach involves selecting utterances with the highest model probability of being included in the summary until the budget is reached (MixHop GCN + Ranking by logits). Both methods enhance the summary quality. We believe that further research on ranking techniques could yield significantly better results."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ablation studies",
            "text": "We conducted extensive ablations to quantify the impact of both discourse structure and relation type on extractive summarization. All the experiments were run five times and reported results are the average. RGCN and MixHop GCN have three layers and 128 base parameters.\n###figure_2###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Structural ablation",
            "text": "We conducted further experiments to investigate the impact of graph edges on the model’s overall classification performance. As shown in Figure 6  ###reference_###, hiding edges decreases the performance of the classifiers for both AMI and ICSI. It appears that the graph structure generally has a more robust impact on classification than relation types do. Figure 7  ###reference_### shows that the more central a node is, the higher the chances of it being a part of the extractive summary is; in other words, the more common it is to refer to a certain EDU, the higher the odds of it being important."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Studying different parsers",
            "text": "As the previously described experiments were all carried out on graphs produced by Deep Sequential (Shi and Huang, 2019  ###reference_b41###) only, we decided to also test the impact that different graph parsers have on our model. In this section, we compare the influence of graphs produced by Deep Sequential () with that of graphs produced by Knowledge Enhanced (; Liu and Chen, 2021  ###reference_b24###) and BERTLine (; Bennis et al., 2023  ###reference_b5###).\nResults in Table 4  ###reference_### show that the three parsers lead to different classification results,  scoring higher in F1 score than the other two, and  having the highest precision score. This difference suggests that we should further investigate the nature of the differences between the predictions of these three models.\nIn order to compare between the resulting graphs produced by different parsers, we employ the Weisfeiler-Lehman graph kernel (Shervashidze et al., 2011  ###reference_b40###; Siglidis et al., 2020  ###reference_b42###) to map these\ngraphs into an embedding space in which structurally alike graphs are placed closer to each other, resulting in the following similarity scores: , , .\nThese scores indicate varying levels of resemblance between the pairs, with the  and  showing the highest similarity.\nIn the end, Table 5  ###reference_### shows some statistics of the graphs. Density represents a ratio between the number of edges and nodes.\nClustering coefficient measures the degree to which vertices in a graph tend to be clustered together. It quantifies how likely it is that the neighbors of a vertex are also connected to each other. In this sense, a value of 0 shows that no two neighbors of a node are also connected (i.e., no triangle structure). We can observe that  and  graphs, have a higher number of edges but lack any triangle structure. On the other hand, we observed that  graphs have entirely disconnected nodes that will have no impact in the classification process for a GNN, explaining its relatively lower classification power."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have demonstrated the utility of discourse graphs in the context of node classification for extractive summarization through an analysis with systems lacking both relational information or a graph structure. Additionally, we conducted ablation studies aimed at assessing the significance of various relations across different datasets. This approach allowed us to discern the impact of specific relational dynamics on the performance of classification algorithms, highlighting the integral role of discourse structures in enhancing the understanding and processing of textual information."
        }
    ]
}