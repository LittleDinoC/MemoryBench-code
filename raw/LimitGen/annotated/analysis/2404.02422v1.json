{
    "title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data",
    "abstract": "Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.\n\n\n\nKeywords: LoRA, PEFT, LLMs, Few-Shot Learning, Text Classification",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Recent years have been characterized by a paradigm shift in text classification. Large Language Models (LLMs) offer valid alternatives to the traditional approach of fine-tuning pre-trained models (e.g., BERT Devlin et al. (2019  ###reference_b6###), RoBERTa Liu et al. (2019  ###reference_b15###)) on annotated datasets. In-Context Learning (ICL) is a first option, where a LLM learns how to solve a task by simply observing a few examples provided in its prompt (i.e., without any fine-tuning stage) Brown et al. (2020  ###reference_b1###). Another alternative is the 0-shot setting, where the LLM directly solves a task by simply following the provided instructions (i.e., without any example). Instruction-fine tuned models like Flan-T5 Chung et al. (2022  ###reference_b4###), Instruct-GPT Ouyang et al. (2022  ###reference_b17###), or ChatGPT excel in this setting. The advantage of these two alternatives with respect to the traditional approach is that they can be used to bootstrap a model when data is scarce or totally absent.\nThe 0-shot setting is generally more appealing since it does not require any data, however, the few-shot ICL setting typically leads to better results by only leveraging a small number of samples (e.g., less than 10 examples). Obtaining a few annotated data might not be a significant drawback, as a practitioner with moderate domain knowledge can readily create a small number of examples manually.\nHowever, a major disadvantage is the higher computational cost, latency, and memory requirements associated with the longer prompt, which needs to contain illustrative examples.\nA possible solution to leverage the few available examples without incurring the ICL inference costs would be to use them for fine-tuning the LLM, which is possible by using some Parameter-Efficient Fine Tuning (PEFT) techniques Liu et al. (2022c  ###reference_b14###); Lester et al. (2021  ###reference_b9###); Hu et al. (2021  ###reference_b8###); Liu et al. (2021  ###reference_b13###). Unfortunately, as we will demonstrate in the experimental section, PEFT is not effective with very few examples, due to under-fitting or over-fitting phenomena.\nIn this paper, we propose a solution to the low-resource PEFT for text classification with LLMs by defining a framework that enables faster, cheaper and more accurate inference than ICL in such a scenario. We hypothesize that LLMs already have some knowledge of how to solve a classification task, but the sub-optimal usage of the available resources (i.e., the few-shot examples) results in low PEFT performance under the low-resource setting. On the contrary, LLMs typically excel in generation tasks, hence we frame an auxiliary data augmentation task that we use to unlock the LLM classification capabilities. Our method consists of three steps. First, we use the LLM to generate synthetic examples for each class of the text classification task we target. Then, we use the same LLM in the ICL setting to classify the examples and clean the data by removing label-inconsistent generated examples. Finally, we fine-tune the LLM with PEFT using the generated and cleaned data. Our experiments show that the resulting classifier reaches accuracy levels comparable to or better than the ICL setting in three different text classification tasks while being a lot more efficient (2x to 5x speed boost).\nIn these generate-filter-train stages we always use the same LLM to demonstrate that what leads to a good accuracy is just a better usage of the few available examples and not the employment of any other resource (e.g., another LLM) which might bring additional knowledge to solve the task.\nThe rest of the paper is organized as it follows: in Section 2  ###reference_### we discuss the related works. In Section 3  ###reference_### we present our method, while in Sections 4  ###reference_### and 5  ###reference_### we discuss the experimental setting and the results, respectively. Finally, Section 6  ###reference_### derives the conclusions.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.    Related Work",
            "text": "LLMs demonstrate impressive capabilities to solve Natural Language Understanding tasks. For instance, classification tasks can be approached in a generative way, i.e., by asking the LLM to generate the class name associated with an input example. 0-shot and ICL are two variants of this paradigm.\nRecent models (e.g., GPT-3, Brown et al. (2020  ###reference_b1###), Flan-T5 Chung et al. (2022  ###reference_b4###), ChatGPT Ouyang et al. (2022  ###reference_b17###), Falcon Penedo et al. (2023  ###reference_b19###) or Vicuna Zheng et al. (2023  ###reference_b26###)) reach impressive results in both settings, and researchers started considering using LLMs as annotators Rosenbaum et al. (2022  ###reference_b21###); Zhu et al. (2023  ###reference_b27###); He et al. (2023  ###reference_b7###). For instance, Rosenbaum et al. (2022  ###reference_b21###) propose a method that uses LLMs to generate and annotate data for Intent Classification and Slot Filling. He et al. (2023  ###reference_b7###) propose a two-step approach where they first use ChatGPT to generate a few-shot Chain-of-Thought prompt, which they then use to annotate unlabeled data. Results are competitive with human annotators, but their classification procedure is relatively slow since the LLM is invoked twice with prompts that need to contain both examples and explanations. Conversely, we propose a solution whose computational complexity at inference time corresponds to the 0-shot setting case.\nEven if these results are impressive, they still might not reach the state-of-the-art performance achievable when LLMs are fully fine-tuned on large data.\nFine-tuning LLMs is extremely expensive, but a viable solution is offered by Parameter Efficient Fine-Tuning (PEFT) techniques Liu et al. (2022c  ###reference_b14###); Lester et al. (2021  ###reference_b9###); Hu et al. (2021  ###reference_b8###); Liu et al. (2021  ###reference_b13###), where a pre-trained model is fine-tuned by only updating a small number (e.g., 0.01%) of added or selected parameters. These methods report results that match the performance of full fine-tuning when large training datasets are available. On the contrary, there has been relatively little focus on (parameter-efficient) fine-tuning in low-resource settings. Our paper targets this scenario, as we assume we can access only a few annotated examples (e.g., four per class) and no unlabeled data. A work operating in a similar setting is Liu et al. (2022b  ###reference_b12###), where the authors propose a novel PEFT technique that is demonstrated to work well in low resource settings when the PEFT weights are pre-trained and multiple tasks are trained in parallel. We differ from their work as we do not pre-train the PEFT weights and we target a single task at a time, without assuming (possibly related) data from other tasks is available. Relaxing this assumption is especially useful when dealing with very peculiar tasks not sharing similarities with other available datasets. Hence, to the best of our knowledge, we are the first to improve few-shot PEFT without additional resources (external datasets or models)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodology",
            "text": "We explore a low-resource setting where we have few training examples per class and no unlabeled data. ICL methods could achieve reasonable performance with few-shot samples but inference cost is high due to long prompts. PEFT methods like LoRA are known to be more efficient than ICL at inference. However, we find that LoRA performs worse than ICL in data-scarce settings (see Tab. 1  ###reference_###). In this paper, we aim to explore the potential of combining the strengths of PEFT and ICL methods for achieving efficient and effective text classification.\nHence we propose to augment the training data with synthetic data to better align the generation and classification capability of LLMs and to ensure that PEFT is performed on a decent amount of data. Our method has 3 steps: generate data, filter data, and train. An overview of our method is shown in Figure 1  ###reference_###.\nFew examples of movie reviews having positive sentiment are given. Generate more positive reviews\nText: [Positive review 1]\nLabel: Positive\n…\nText: [Positive review 4]\nLabel: Positive\nText: [the model generates this]\nClassify the sentiment of the given movie review into Positive or Negative\nText: [review 1]\nLabel: [Label 1]\n…\nText: [review 8]\nLabel: [Label 8]\nText: [generated review]\nLabel: [Predicted Label]\nGeneration step: Chavan et al. (2023  ###reference_b3###) show that in a few-shot setting, the performance of PEFT significantly improves as the number of training samples per class increases. We also observe similar results in our initial experiments (presented in section 5  ###reference_###. Further, since ICL performs well, we hypothesize that the model has the inherent knowledge to solve the classification task and that the low PEFT results are due to sub-optimal usage of the available resources (the few shot examples). To fill this gap, we first use the LLM  in the ICL setting to generate synthetic data which we can use to augment the few shot examples at our disposal. We generate examples for each class in the targeted classification task. An example of the prompt we used in this step is shown in Figure 2  ###reference_###.\nFiltering step: We first apply a basic filtering step to discard duplicates and malformed generations (i.e., too short or too long texts). On manual inspection of the generated data, we found some label-inconsistent generations (i.e., data that are not valid examples of the class they should represent). We hypothesize this is due to hallucination. To identify and remove these cases, we classify the generated data using ICL with . The prompt used for this stage is similar to the one shown in Figure 3  ###reference_###. If the predicted label does not match the intended label from the generation step, we discard the generated example. We repeat these generation-filtering steps until we produce N new data samples for each class in the targeted classification task.\nTraining step: Finally, we use the filtered data along with the few (4 per class) real examples for the PEFT of the LLM  with LoRA. Note that  is used for all 3 steps, as we want to validate our hypothesis that  does not need additional knowledge to work in the PEFT setting, but only a more stable training process which can be guaranteed by the self-generated synthetic examples.\nInference Conversely to the ICL setting, the LLM does not use any example at inference time. Note that the three steps (generate, filter, train) are used only at training time, i.e., there is no impact on the inference latency.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": "We use the Vicuna LLM Zheng et al. (2023  ###reference_b26###), which is based on LLaMA Touvron et al. (2023  ###reference_b22###). We selected Vicuna as it is the best-performing model among those whose weights were publicly available at the time of our experiments, according to the Chatbot Arena Leaderboard111https://lmsys.org/blog/2023-05-25-leaderboard/  ###reference_ard/###. We experiment with 2 model sizes - Vicuna-7b and Vicuna-13b which have 7 billion and 13 billion parameters, respectively. We show results on the official test sets of 3 datasets - SST2 (sentiment analysis, 2 classes) Pang and Lee (2005  ###reference_b18###), AG News (news topic classification, 4 classes) Zhang et al. (2016  ###reference_b25###), and TREC (question classification, 6 classes) Li and Roth (2002  ###reference_b10###).\nTo generate synthetic data, we use random sample decoding with temperature = 1.0, top_k = 50 and num_beams = 1 or 2.\nFor a fair comparison across models and methods, throughout our experiments, we do not tune the LoRA hyper-parameters (we set rank=8, alpha=32, and dropout=0.1). Similarly, we make minimal changes to the prompt for all ICL experiments and do not perform any prompt engineering.\nThe code is implemented using hugging face and PEFT library Mangrulkar et al. (2022  ###reference_b16###) with torch backend. All models are trained on 4 v100 GPUs of 16GB each. All LoRA models are trained for 100 epochs. The language modeling loss (next token prediction) is optimized using the Adam optimizer. Batch size is set to 2 for Vicuna-13b and 8 for Vicuna-7b. We run each experiment multiple times with different seeds and different few-shot examples. We observe negligible deviation across runs and report the average accuracy. We also report the results of several baselines, including 0-shot, ICL, and vanilla LoRA trained with different numbers of real examples. We also report LoRA trained with the full training set. This could be considered a potential upper-bound reachable in high resource settings and gives results comparable to the respective SoTA methods on the 3 datasets Raffel et al. (2023  ###reference_b20###); Cer et al. (2018  ###reference_b2###); Yang et al. (2019  ###reference_b23###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion and Future work",
            "text": "In this paper, we introduced a framework to make LLMs more efficient and effective text classifiers in very low-resource settings. The procedure we proposed consists of three steps. In the first step, the LLM is used to augment a very small training set with synthetic data; then, we adopt the LLM to classify the generated data and remove label-inconsistent examples; finally, we use the resulting data to fine-tune the LLM using LoRA. By running experiments on three different classification datasets we demonstrated how training LoRA using the self-generated synthetic data allowed our model to be comparable to or surpass several baselines operating in low resource settings, including 0-shot, ICL, and vanilla LoRA. In future work, we plan to improve the quality of the generated examples by promoting data diversity. Some strategies to improve data diversity include increasing attribute diversity Yu et al. (2023  ###reference_b24###), logit suppression Chung et al. (2023  ###reference_b5###) etc."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Limitations",
            "text": "Our method might not work on tasks that are particularly challenging and hard to catch with only a few examples. In this case, ICL is expected to fail, and similarly, our first two steps are expected to produce low-quality examples making the entire procedure ineffective. Another limitation is that our approach is fully based on LLMs and cannot be applied to low-resource languages where there is no existing LLM working well."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethics",
            "text": "Generating data using LLMs for text classification exposes the resulting classifier to the biases acquired during the LLM pre-training. In our framework, this phenomenon is potentially even amplified, as using the same LLM to generate and filter the data might reinforce such biases. Unfortunately, there is no one-size-fits-all solution for this problem. The biases are dependent on the application domain and on the data distribution to be generated. However, we encourage the readers to be very cautious about using this framework and to take the appropriate actions - for example, compiling a list of the potential biases specific for the target application domain and checking for those in the generated data - to mitigate the potential biases that may get reinforced when using a methodology similar to the one here presented."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   References",
            "text": ""
        }
    ]
}