{
    "title": "Exploring Language Model’s Code Generation Ability with Auxiliary Functions",
    "abstract": "Auxiliary function is a helpful component to improve language model’s code generation ability.\nHowever, a systematic exploration of how they affect has yet to be done.\nIn this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models.\nFirst, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other.\nWith HumanExtension, we design several experiments to examine their ability in a multifaceted way.\nOur evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness.\nAn additional implementation style analysis captures the models’ various implementation patterns when they access the auxiliary function.\nThrough this analysis, we discover the models’ promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step.\nHowever, our analysis also reveals the model’s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models.\nWe release our code111https://github.com/sh0416/humanextension and dataset222https://huggingface.co/datasets/sh0416/humanextension to facilitate this research direction.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Program synthesis, i.e., writing function code by taking natural language descriptions as inputs, has garnered attention in the research community (Yin and Neubig, 2017  ###reference_b37###; Rahit et al., 2020  ###reference_b30###; Austin et al., 2021  ###reference_b2###; Li et al., 2022  ###reference_b20###).\nWith the help of language modeling, several code-pretrained Large Language Models (LLMs) implement functions with prompts that contain target function signatures (Fried et al., 2023  ###reference_b10###; Nijkamp et al., 2023b  ###reference_b29###, a  ###reference_b28###; Allal et al., 2023  ###reference_b1###; Li et al., 2023  ###reference_b19###; Gunasekar et al., 2023  ###reference_b12###).\nAdditional code components, e.g., comment lines (Gao et al., 2023  ###reference_b11###), documents (Zhou et al., 2023c  ###reference_b43###), and other function and class definitions across files (Ding et al., 2023  ###reference_b7###), have been attached to the prompts to boost up their implementation ability.\n###figure_1### Auxiliary function is one promising component to improve their code synthesis ability.\nWe define the auxiliary function as a function that handles a subroutine for the target one or performs an easier version of the actual requirements.\nWhen this function is included in the prompt, LLMs could call the function to delegate their subroutine or refer to their implementation while synthesizing the target function.\nHowever, due to the lack of an evaluation dataset that enables a systematic examination of how these auxiliary functions are utilized, no structured analysis has yet to be conducted.\nIn this work, we investigate several LLMs’ ability to utilize auxiliary functions.\nTo do this, we first construct an evaluation dataset, called HumanExtension, which contains human-crafted examples of two functions that are closely related to each other (Figure 1  ###reference_###).\nSpecifically, we guided labelers to extend functions in the HumanEval dataset (Chen et al., 2021  ###reference_b6###).\nWe offer software design concepts related to function extension such as subtyping (Liskov and Wing, 1994  ###reference_b21###) to promote labelers to create realistic function relationships.\nAdditionally, the curated examples are parsed into several components to enable robustness evaluation similar to Wang et al. (2023a  ###reference_b34###).\nWith the HumanExtension dataset, we conduct systematic analyses to understand how LLMs leverage auxiliary functions.\nFirst, we investigate if appending a single auxiliary function to the prompt enhances the likelihood of accurately implementing the target function.\nSpecifically, we design several prompts with auxiliary functions while considering their existence, their functional relevance, and the availability to access auxiliary function implementations.\nWith these prompts, we generate implementations with LLMs and analyze the model behavior focusing on the auxiliary function’s effectiveness, robustness, and the models’ implementation style.\nSecond, we examine the cases where LLMs can access multiple auxiliary functions for synthesizing target functions.\nThe randomly sampled auxiliary functions are additionally included in the prompts to verify whether LLMs can selectively use the appropriate one.\nSimilar to Liu et al. (2023b  ###reference_b23###), we inspect whether the position of a relevant function affects their code generation ability.\nThis investigation is combined with the implementation style analysis to permit an in-depth analysis through the lens of the auxiliary function call.\nOur experimental results show current LLMs’ capabilities to utilize auxiliary function and their limitations.\nFirst, most LLMs exhibit large performance improvement with proper relevant auxiliary functions.\nAlso, for some advanced LLMs, our evaluation process sheds light on their self-improving behavior by implementing the two functions in a step-by-step manner.\nHowever, the ability to utilize auxiliary functions is varied depending on the factors that do not change their functionality, which raises a question about their robustness.\nIn addition, our implementation style analysis results reveal that the models prefer repeating the internal logic in the auxiliary function even when the logic can be easily handled by simply calling them.\nFinally, our human preference evaluation of their style shows this disparity between model-generated implementation and that of humans, suggesting the future direction of enhancing the ability to delegate their subroutine to the auxiliary functions by calling them."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Several studies have been conducted to evaluate code generation ability (Xu et al., 2022  ###reference_b36###).\nNeelakantan et al. (2016  ###reference_b26###); Iyer et al. (2018  ###reference_b16###) first introduce neural networks into code completion tasks and evaluate them on traditional metrics, e.g., BLEU.\nChen et al. (2021  ###reference_b6###) propose the HumanEval dataset and show LLMs can generate functionally correct implementations by introducing a functional correctness evaluation process.\nConcurrently, Austin et al. (2021  ###reference_b2###) propose the MBPP dataset for Python basic programs and Hendrycks et al. (2021  ###reference_b13###) release the APPS dataset related to coding contest problems.\nConsecutive studies have proposed datasets targeted for realistic purposes.\nLai et al. (2023  ###reference_b18###) focused on data science problems and Wang et al. (2023b  ###reference_b35###) paid attention to realistic coding queries from StackOverflow and Yu et al. (2024  ###reference_b38###) aimed at Python and Java code generation tasks from real-world open-source projects, and Babe et al. (2023  ###reference_b3###) concentrated on beginning programmers.\nThese work are combined and included in several coding benchmarks (Lu et al., 2021  ###reference_b24###; Khan et al., 2023  ###reference_b17###; Ni et al., 2023  ###reference_b27###).\nFor the metrics, Dong et al. (2023  ###reference_b8###) propose CodeScore to estimate functional correctness and Zhou et al. (2023b  ###reference_b42###) propose CodeBERTScore that utilizes BERTScore (Zhang et al., 2020  ###reference_b39###).\nThere exists research work that extends the HumanEval dataset to support other features.\nCassano et al. (2022  ###reference_b5###); Zheng et al. (2023  ###reference_b40###) extend the dataset to support multiple programming languages and Liu et al. (2023a  ###reference_b22###) propose the HumanEval+ dataset that extends their test case to enable rigorous evaluation of functional correctness.\nWang et al. (2023a  ###reference_b34###) focused on prompt robustness by extending the HumanEval dataset.\nHowever, an evaluation procedure that enables systematic analysis of how LLMs leverage auxiliary functions has yet to be released in code generation tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "We manually construct a variety of coding examples with corresponding auxiliary functions.\nTo do this, we treat the Python examples in the HumanEval dataset as our base auxiliary functions and employ human experts to create an extended function for each example.\nWe guide them to produce functions that have additional functionalities compared to the given functions.\nThe following aspects are considered to remove the ambiguity inside the concept of extension and enhance their quality.\nThere exist two different types of extension, i.e., black-box extension and white-box extension.\nThe black-box extension extends a function by calling the auxiliary function.\nIt does not consider the internal mechanism of the auxiliary function.\nHowever, the white-box extension extends them by rewriting the improved internal mechanism.\nWe allow any type of extension, but recommend the black-box one as calling the existing functions if possible is mostly better than rewriting the whole mechanism (Fowler, 2018  ###reference_b9###).\nWe show the Liskov-substitution principle and the concept of subtyping (Liskov and Wing, 1994  ###reference_b21###) to the labelers.\nIn doing so, we expect that the curated function could be treated as an extended version of the given function from the software engineering point of view.\nWe filtered out some examples in the HumanEval dataset that are not appropriate for using auxiliary functions.\nWe removed the examples that provide the same functionality embedded in Python built-in functions, e.g., sum_to_n, as it already serves through the Python features.\nAlso, the examples that are semantically duplicated with other examples are excluded from the final evaluation set.\nFor example, if the two functions handle the same logic to process symbols but accept brackets or parentheses as their inputs, one of them is removed.\nWe collect 151 problems representing a function pair that one function extends the other and name it HumanExtension.\nAdditionally, we mechanically parse these code snippets and create features for components for future usage."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We comprehensively evaluate LLMs’ ability to harness auxiliary functions using our HumanExtension dataset.\nTo do this, we designed research questions as follows.\nRQ1: Could LLMs properly and robustly utilize different types of auxiliary functions?\nRQ2: How do LLMs’ implementations vary when they access relevant auxiliary functions?\nRQ3: Do current training methodologies enhance the ability to utilize auxiliary functions?\nWe first examine the effectiveness and robustness of including a single auxiliary function in the prompt and extend this setting into multiple auxiliary functions.\nAlso, we explore their implementation styles and analyze them based on human preference.\nWe collect several LLMs pre-trained on code described as follows.\nIncoder (Fried et al., 2023  ###reference_b10###) is the early open-source decoder-only generative language model pretrained on public codes and StackOverflow questions and answers.\nCodeGen (Nijkamp et al., 2023b  ###reference_b29###) is another open-source language model pretrained on public codes.\nWe use two versions where “Multi” represents pre-training on multiple programming languages and “Mono” is additionally trained on Python codes from the \"Multi\" checkpoint.\nBigCode (Allal et al., 2023  ###reference_b1###; Li et al., 2023  ###reference_b19###) releases two checkpoints, i.e., SantaCoder and StarCoder, pretrained on public codes.\nThey adopt various data-cleaning techniques to enhance the quality of the training corpus.\nCodeLLaMA (Rozière et al., 2023  ###reference_b31###) is a variant of LLaMA2 (Touvron et al., 2023  ###reference_b33###) additionally pretrained on code corpus.\nCodeLLaMAPython and CodeLLaMAInstruct are further trained on Python codes and instruction following datasets, respectively.\nWe follow the decoding strategy for LLMs consistent with the existing benchmark (Ben Allal et al., 2022  ###reference_b4###).\nWe use nucleus sampling (Holtzman et al., 2020  ###reference_b14###) with top-p 0.95 and low-temperature scaling, i.e., 0.2, focusing on the correctness of the generated implementation.\nThe models generate at most 512 tokens for each prompt and stop generation when either end of sequence token or predefined stop sequences, i.e., \"\\ndef\", \"\\nclass\", \"\\nif\", \"\\n#\", are generated.\nThe implementations generated by the models are evaluated on functional correctness based on the corresponding test cases.\nSpecifically, an implementation is regarded as functionally correct when it passes all the corresponding test cases.\nWe use the widely used pass@1 metric indicating the proportion of functionally corrected implementations among generated implementations.\nTo reduce the variance of the pass@1 metric, we generate eight implementations for each problem when estimating the model performance.\nWhole models exhibit remarkable improvement when they access the human-written relevant auxiliary functions (Table 1  ###reference_###, Oracle).\nIt implies that most LLMs could utilize the proper relevant auxiliary function.\nThe improvement is observed even for the most recent competitive model, i.e., CodeLLaMAPython 34B, indicating assisting code synthesis with auxiliary function is still a valid approach even as the model size grows.\nConsidering the \"Step-by-step\" column in Table 1  ###reference_###, the model-written relevant auxiliary functions contribute to the improvement for some advanced LLMs.\nCodeLLaMA series, StarCoder, CodeGenMono series, and Incoder 6B properly utilize the auxiliary function written by themselves.\nIt suggests that the models can improve their codes if we provide a two-step plan in the form of function signatures.\nWe attach one successful example that calls the generated auxiliary function during target implementation in Figure 1(b)  ###reference_sf2###.\nIn this sense, this approach is similar to the Least-to-Most prompting (Zhou et al., 2023a  ###reference_b41###) that solves target tasks with the model-generated answer of predefined subtasks.\nWe observe that providing an irrelevant auxiliary function brings meaningful improvement on few models.\nTo investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step.\nIn Figure 2  ###reference_###, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement.\nHowever, since the given auxiliary function is not relevant to the target function (Figure 1(a)  ###reference_sf1###), no implementation pattern that directly utilizes the auxiliary function is found.\nOn the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty (Figure 1(b)  ###reference_sf2###).\nTherefore, we conclude there exists a unique advantage of providing relevant auxiliary function although the irrelevant one is helpful to some extent.\n###figure_2### ###figure_3### We investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions.\nTo do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython.\nIn these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA.\nComparing CodeGenMono 2B and CodeGenMulti 2B, the pass rate is similar when no auxiliary function is provided (Direct), but the pass rate of CodeGenMono becomes significantly higher than that of CodeGenMulti when we provide an appropriate auxiliary function (Oracle).\nAdditionally, in the Step-by-step setting, CodeGenMono models show meaningful improvement while CodeGenMulti could not.\nIn the case of CodeLLaMA, CodeLLaMAPython models show higher pass rates in the whole model size.\nFrom these experimental evidences, we conclude that additional learning with Python code enhances the ability to utilize auxiliary functions.\nWe speculate that the Python codes used for training contain relevant functions in the same file and the model is trained to jointly consider the functions within the same context.\nWe also compare CodeLLaMAInstruct models to determine whether the instruction tuning affects the ability to harness auxiliary functions.\nIn order to use an instruction-tuned model, instructions written in natural language and a prompt template are additionally required.\nTo this end, we apply an approach similar to HumanEvalPack (Muennighoff et al., 2023  ###reference_b25###), where the instructions are automatically generated from the original prompt.\nWe combine these instructions with the CodeLLaMAInstruct template to create a prompt.\nThe prompt is formulated into two consecutive turns where the first turn is about the auxiliary function and the second one is about generating the target function333Refer to the appendix for the detailed template structure..\nOur empirical results show that CodeLLaMAInstruct models perform better than CodeLLaMA models when implementing functions without auxiliary functions (Table 1  ###reference_###, Direct), which is consistent with previous findings (Rozière et al., 2023  ###reference_b31###).\nOn the other hand, when an appropriate auxiliary function is provided in the prompt (Table 1  ###reference_###, Oracle), the base models show better performance than the instruction-tuned models.\nIn addition, the relative improvement in the Step-by-step settings has prominently decreased compared to that of the base models.\nThis suggests that the ability to utilize other functions in the context has been weakened during the instruction tuning process.\nTherefore, it is necessary to develop an advanced instruction-tuning methodology to incorporate the previously implemented functions, which is our future work.\n###figure_4### The experimental results show that even if the functionality of the function does not change, a performance drop is observed depending on the name of the function or the existence of a docstring (Figure 3  ###reference_###).\nThe lack of a docstring had a greater impact than renaming the function, and it is natural in that the docstring contains a more detailed description of its functionality.\nDespite their usefulness, we want to highlight that LLMs have to understand the function without docstring for their realistic use cases as most practical codes do not include them.444In bigcode/the-stack-smol, 70.5% of Python functions do not have docstring.\nThe performance drop was not alleviated even when the model size was increased or the model was additionally trained with Python codes.\nTherefore, there is a need to propose a robust learning methodology that can reduce performance differences caused by such perturbations.\nWe compute pass@1 scores for each style and model (Figure 4  ###reference_###).\nThe results show that all models can implement functions in both styles.\nAlso, we observed that the pass@1 score for the black-box style is higher than that of the white-box style.\nIt implies that calling an auxiliary function is much safer and more accurate if the target function can be implemented by calling the auxiliary function.\nCurrently, up to 40% of the model-generated implementations are implemented in black-box style, even though most examples can be implemented in black-box style.666147 of 151 human-written reference solutions in the HumanExtension dataset are black-box style.\nTherefore, it is expected that the pass@1 score can be improved if more examples are implemented in black box style.\nFurthermore, we would like to emphasize that the improvement of the ability to generate black-box implementations is diminishing as language models evolve.\nThis phenomenon suggests that model developers should consider the model’s function call ability when learning their models.\nFurther investigating the two different styles, we conduct a human pairwise preference evaluation with human-written implementations (Human), and model-written ones with both styles (Black box and White box).\nWe created a labeling sheet with 17 examples that CodeLLaMAPython 34B implements in both styles correctly.\nWe recruited labelers who have been coding with Python for over five years.\nFor the three possible pairs, labelers were instructed to choose the better implementations according to their preference such as performance or readability.\nThe evaluation results in Table 2  ###reference_### show that implementations that call auxiliary functions are preferred over implementations that do not.\nAfter inspecting the result qualitatively, we interpret that most black box implementations were selected due to their clarity and conciseness coming from appropriately delegating subroutines to auxiliary functions.\nUsually, the model-generated white-box implementations tend to repeat the identical mechanism inside the auxiliary function, which is not preferred in software engineering fields (Hunt and Thomas, 2000  ###reference_b15###).\nIn few cases, white-box implementations are preferred over black-box ones as they are considered as over-engineering.\nTherefore, training the models to delegate the subroutine to other functions suitably would be the next step for generating realistic code.\n###figure_5### ###figure_6### We confirmed that CodeLLaMA models and CodeLLaMAPython models show different trends in terms of pass@1 scores (Figure 4(a)  ###reference_sf1###).\nFor CodeLLaMA models, the pass@1 scores showed a U-shape trend, indicating that the performance improved when the related function was located at the first or the last.\nThis result is consistent with the existing findings (Liu et al., 2023b  ###reference_b23###) that, in natural language processing tasks, LLMs can effectively utilize relevant documents when they are located at the beginning or end.\nOn the other hand, for CodeLLaMAPython models, this U-shape trend was weakened and the pass@1 score increased only when the relevant function was located at the end.\nWe conjectured that the two related functions were usually located adjacently in Python codes and this pattern was learned by the model.\nHowever, since the location of relevant functions is independent of their functionality, LLMs need to be tuned to robustly utilize them regardless of where they are placed.\nWe found that there exists a strong correlation between the pass@1 score and the proportion of black-box style implementations.\nThe Pearson correlation scores between the proportion and the pass rate (Figure 4(b)  ###reference_sf2###) are larger than 0.9, indicating that LLMs get higher scores when they try to call appropriate auxiliary functions.\nHowever, the black-box style implementations are mostly observed when the relevant auxiliary functions are located at the last, which provides an explanation of why the pass@1 score is higher when the relevant function is located at the last.\nFor CodeLLaMA models, they can call the relevant function if they are located at the first, which causes the U-shape trend in pass@1 scores.\nModel scaling and additional training with Python codes provide a marginal effect on promoting a model to generate black-box style implementations, suggesting that specialized training for LLMs to call relevant functions similar to invoking general LLMs to use tools (Schick et al., 2023  ###reference_b32###) is needed for enhancing their code synthesis ability."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Single auxiliary function experiment",
            "text": "We measure the effectiveness of an auxiliary function in a code synthesis task by designing several prompts varying their existence and type.\nCurrently, the prompt used in the existing work to solve the task is mainly composed of a target function signature with the corresponding import statements (Ben Allal et al., 2022  ###reference_b4###; Cassano et al., 2022  ###reference_b5###; Chen et al., 2021  ###reference_b6###).\nWe attached the auxiliary function signature and their implementation between the import statements and the target function signature to allow LLMs to access the knowledge about auxiliary functions.\nOur prompts with several types of auxiliary functions are described as follows.\nNo auxiliary function (Direct):\nPrompt consists of a target function signature without auxiliary functions.\nThis setting acts as a baseline in our experiments.\nHuman-written irrelevant auxiliary function (Irrelevant):\nWe attached an irrelevant auxiliary function written by humans in the prompt.\nWe constructed an auxiliary function pool with the canonical solutions in the HumanEval dataset (Chen et al., 2021  ###reference_b6###) and sampled an irrelevant function from the pool to construct the prompt.\nModel-written relevant auxiliary function (Step-by-step):\nWe utilize the relevant auxiliary function written by the model in the prompt.\nConcretely, LLMs first synthesize relevant auxiliary function and then it is attached to the prompt for implementing the target function.\nNote that only a relevant auxiliary function signature without their implementation is additionally required for this setting.\nWe utilize the auxiliary function signatures in the HumanEval dataset and the target one in our HumanExtension dataset.\nHuman-written relevant auxiliary function (Oracle):\nWe provide a relevant auxiliary function written by humans to the model.\nThe corresponding canonical solutions in the HumanEval dataset are used for human-written relevant auxiliary functions.\nWe consider this setting as an oracle because these functions are currently the best in terms of quality and understandability.\nThe details about function signature, e.g., type annotation and docstring format, are consistent with the format curated in Cassano et al. (2022  ###reference_b5###).\nWe collect several LLMs pre-trained on code described as follows.\nIncoder (Fried et al., 2023  ###reference_b10###  ###reference_b10###) is the early open-source decoder-only generative language model pretrained on public codes and StackOverflow questions and answers.\nCodeGen (Nijkamp et al., 2023b  ###reference_b29###  ###reference_b29###) is another open-source language model pretrained on public codes.\nWe use two versions where “Multi” represents pre-training on multiple programming languages and “Mono” is additionally trained on Python codes from the \"Multi\" checkpoint.\nBigCode (Allal et al., 2023  ###reference_b1###  ###reference_b1###; Li et al., 2023  ###reference_b19###  ###reference_b19###) releases two checkpoints, i.e., SantaCoder and StarCoder, pretrained on public codes.\nThey adopt various data-cleaning techniques to enhance the quality of the training corpus.\nCodeLLaMA (Rozière et al., 2023  ###reference_b31###  ###reference_b31###) is a variant of LLaMA2 (Touvron et al., 2023  ###reference_b33###  ###reference_b33###) additionally pretrained on code corpus.\nCodeLLaMAPython and CodeLLaMAInstruct are further trained on Python codes and instruction following datasets, respectively.\nWe follow the decoding strategy for LLMs consistent with the existing benchmark (Ben Allal et al., 2022  ###reference_b4###  ###reference_b4###).\nWe use nucleus sampling (Holtzman et al., 2020  ###reference_b14###  ###reference_b14###) with top-p 0.95 and low-temperature scaling, i.e., 0.2, focusing on the correctness of the generated implementation.\nThe models generate at most 512 tokens for each prompt and stop generation when either end of sequence token or predefined stop sequences, i.e., \"\\ndef\", \"\\nclass\", \"\\nif\", \"\\n#\", are generated.\nThe implementations generated by the models are evaluated on functional correctness based on the corresponding test cases.\nSpecifically, an implementation is regarded as functionally correct when it passes all the corresponding test cases.\nWe use the widely used pass@1 metric indicating the proportion of functionally corrected implementations among generated implementations.\nTo reduce the variance of the pass@1 metric, we generate eight implementations for each problem when estimating the model performance.\nWhole models exhibit remarkable improvement when they access the human-written relevant auxiliary functions (Table 1  ###reference_###  ###reference_###, Oracle).\nIt implies that most LLMs could utilize the proper relevant auxiliary function.\nThe improvement is observed even for the most recent competitive model, i.e., CodeLLaMAPython 34B, indicating assisting code synthesis with auxiliary function is still a valid approach even as the model size grows.\nConsidering the \"Step-by-step\" column in Table 1  ###reference_###  ###reference_###, the model-written relevant auxiliary functions contribute to the improvement for some advanced LLMs.\nCodeLLaMA series, StarCoder, CodeGenMono series, and Incoder 6B properly utilize the auxiliary function written by themselves.\nIt suggests that the models can improve their codes if we provide a two-step plan in the form of function signatures.\nWe attach one successful example that calls the generated auxiliary function during target implementation in Figure 1(b)  ###reference_sf2###  ###reference_sf2###.\nIn this sense, this approach is similar to the Least-to-Most prompting (Zhou et al., 2023a  ###reference_b41###  ###reference_b41###) that solves target tasks with the model-generated answer of predefined subtasks.\nWe observe that providing an irrelevant auxiliary function brings meaningful improvement on few models.\nTo investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step.\nIn Figure 2  ###reference_###  ###reference_###, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement.\nHowever, since the given auxiliary function is not relevant to the target function (Figure 1(a)  ###reference_sf1###  ###reference_sf1###), no implementation pattern that directly utilizes the auxiliary function is found.\nOn the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty (Figure 1(b)  ###reference_sf2###  ###reference_sf2###).\nTherefore, we conclude there exists a unique advantage of providing relevant auxiliary function although the irrelevant one is helpful to some extent.\n###figure_7### ###figure_8### We investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions.\nTo do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython.\nIn these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA.\nComparing CodeGenMono 2B and CodeGenMulti 2B, the pass rate is similar when no auxiliary function is provided (Direct), but the pass rate of CodeGenMono becomes significantly higher than that of CodeGenMulti when we provide an appropriate auxiliary function (Oracle).\nAdditionally, in the Step-by-step setting, CodeGenMono models show meaningful improvement while CodeGenMulti could not.\nIn the case of CodeLLaMA, CodeLLaMAPython models show higher pass rates in the whole model size.\nFrom these experimental evidences, we conclude that additional learning with Python code enhances the ability to utilize auxiliary functions.\nWe speculate that the Python codes used for training contain relevant functions in the same file and the model is trained to jointly consider the functions within the same context.\nWe also compare CodeLLaMAInstruct models to determine whether the instruction tuning affects the ability to harness auxiliary functions.\nIn order to use an instruction-tuned model, instructions written in natural language and a prompt template are additionally required.\nTo this end, we apply an approach similar to HumanEvalPack (Muennighoff et al., 2023  ###reference_b25###  ###reference_b25###), where the instructions are automatically generated from the original prompt.\nWe combine these instructions with the CodeLLaMAInstruct template to create a prompt.\nThe prompt is formulated into two consecutive turns where the first turn is about the auxiliary function and the second one is about generating the target function333Refer to the appendix for the detailed template structure..\nOur empirical results show that CodeLLaMAInstruct models perform better than CodeLLaMA models when implementing functions without auxiliary functions (Table 1  ###reference_###  ###reference_###, Direct), which is consistent with previous findings (Rozière et al., 2023  ###reference_b31###  ###reference_b31###).\nOn the other hand, when an appropriate auxiliary function is provided in the prompt (Table 1  ###reference_###  ###reference_###, Oracle), the base models show better performance than the instruction-tuned models.\nIn addition, the relative improvement in the Step-by-step settings has prominently decreased compared to that of the base models.\nThis suggests that the ability to utilize other functions in the context has been weakened during the instruction tuning process.\nTherefore, it is necessary to develop an advanced instruction-tuning methodology to incorporate the previously implemented functions, which is our future work.\n###figure_9### The experimental results show that even if the functionality of the function does not change, a performance drop is observed depending on the name of the function or the existence of a docstring (Figure 3  ###reference_###  ###reference_###).\nThe lack of a docstring had a greater impact than renaming the function, and it is natural in that the docstring contains a more detailed description of its functionality.\nDespite their usefulness, we want to highlight that LLMs have to understand the function without docstring for their realistic use cases as most practical codes do not include them.444In bigcode/the-stack-smol, 70.5% of Python functions do not have docstring.\nThe performance drop was not alleviated even when the model size was increased or the model was additionally trained with Python codes.\nTherefore, there is a need to propose a robust learning methodology that can reduce performance differences caused by such perturbations.\nWe compute pass@1 scores for each style and model (Figure 4  ###reference_###  ###reference_###).\nThe results show that all models can implement functions in both styles.\nAlso, we observed that the pass@1 score for the black-box style is higher than that of the white-box style.\nIt implies that calling an auxiliary function is much safer and more accurate if the target function can be implemented by calling the auxiliary function.\nCurrently, up to 40% of the model-generated implementations are implemented in black-box style, even though most examples can be implemented in black-box style.666147 of 151 human-written reference solutions in the HumanExtension dataset are black-box style.\nTherefore, it is expected that the pass@1 score can be improved if more examples are implemented in black box style.\nFurthermore, we would like to emphasize that the improvement of the ability to generate black-box implementations is diminishing as language models evolve.\nThis phenomenon suggests that model developers should consider the model’s function call ability when learning their models.\nFurther investigating the two different styles, we conduct a human pairwise preference evaluation with human-written implementations (Human), and model-written ones with both styles (Black box and White box).\nWe created a labeling sheet with 17 examples that CodeLLaMAPython 34B implements in both styles correctly.\nWe recruited labelers who have been coding with Python for over five years.\nFor the three possible pairs, labelers were instructed to choose the better implementations according to their preference such as performance or readability.\nThe evaluation results in Table 2  ###reference_###  ###reference_### show that implementations that call auxiliary functions are preferred over implementations that do not.\nAfter inspecting the result qualitatively, we interpret that most black box implementations were selected due to their clarity and conciseness coming from appropriately delegating subroutines to auxiliary functions.\nUsually, the model-generated white-box implementations tend to repeat the identical mechanism inside the auxiliary function, which is not preferred in software engineering fields (Hunt and Thomas, 2000  ###reference_b15###  ###reference_b15###).\nIn few cases, white-box implementations are preferred over black-box ones as they are considered as over-engineering.\nTherefore, training the models to delegate the subroutine to other functions suitably would be the next step for generating realistic code.\n###figure_10### ###figure_11###"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Performance analysis",
            "text": "We report the performances and the relative improvement compared with the one without auxiliary function in Table 1  ###reference_### and compare them to identify the effectiveness of different auxiliary functions.\nWhole models exhibit remarkable improvement when they access the human-written relevant auxiliary functions (Table 1  ###reference_###  ###reference_###  ###reference_###, Oracle).\nIt implies that most LLMs could utilize the proper relevant auxiliary function.\nThe improvement is observed even for the most recent competitive model, i.e., CodeLLaMAPython 34B, indicating assisting code synthesis with auxiliary function is still a valid approach even as the model size grows.\nConsidering the \"Step-by-step\" column in Table 1  ###reference_###  ###reference_###  ###reference_###, the model-written relevant auxiliary functions contribute to the improvement for some advanced LLMs.\nCodeLLaMA series, StarCoder, CodeGenMono series, and Incoder 6B properly utilize the auxiliary function written by themselves.\nIt suggests that the models can improve their codes if we provide a two-step plan in the form of function signatures.\nWe attach one successful example that calls the generated auxiliary function during target implementation in Figure 1(b)  ###reference_sf2###  ###reference_sf2###  ###reference_sf2###.\nIn this sense, this approach is similar to the Least-to-Most prompting (Zhou et al., 2023a  ###reference_b41###  ###reference_b41###  ###reference_b41###) that solves target tasks with the model-generated answer of predefined subtasks.\nWe observe that providing an irrelevant auxiliary function brings meaningful improvement on few models.\nTo investigate how these functions affect the target implementation, we qualitatively analyze the examples that CodeLLaMAPython 13B successfully generates under both settings, i.e., irrelevant and step-by-step.\nIn Figure 2  ###reference_###  ###reference_###  ###reference_###, we found that the irrelevant auxiliary function acts as a demonstration like few-shot prompting so that the few models exhibit performance improvement.\nHowever, since the given auxiliary function is not relevant to the target function (Figure 1(a)  ###reference_sf1###  ###reference_sf1###  ###reference_sf1###), no implementation pattern that directly utilizes the auxiliary function is found.\nOn the contrary, the relevant auxiliary functions are successfully utilized by calling in the target function and reduce their implementation difficulty (Figure 1(b)  ###reference_sf2###  ###reference_sf2###  ###reference_sf2###).\nTherefore, we conclude there exists a unique advantage of providing relevant auxiliary function although the irrelevant one is helpful to some extent.\n###figure_12### ###figure_13### We investigate how the additional training with Python corpus affects its ability to utilize auxiliary functions.\nTo do this, we compare the two model families specialized in Python, i.e., CodeGenMono and CodeLLaMAPython.\nIn these model groups, we observed higher pass rates compared to the corresponding base model groups, i.e., CodeGenMulti and CodeLLaMA.\nComparing CodeGenMono 2B and CodeGenMulti 2B, the pass rate is similar when no auxiliary function is provided (Direct), but the pass rate of CodeGenMono becomes significantly higher than that of CodeGenMulti when we provide an appropriate auxiliary function (Oracle).\nAdditionally, in the Step-by-step setting, CodeGenMono models show meaningful improvement while CodeGenMulti could not.\nIn the case of CodeLLaMA, CodeLLaMAPython models show higher pass rates in the whole model size.\nFrom these experimental evidences, we conclude that additional learning with Python code enhances the ability to utilize auxiliary functions.\nWe speculate that the Python codes used for training contain relevant functions in the same file and the model is trained to jointly consider the functions within the same context.\nWe also compare CodeLLaMAInstruct models to determine whether the instruction tuning affects the ability to harness auxiliary functions.\nIn order to use an instruction-tuned model, instructions written in natural language and a prompt template are additionally required.\nTo this end, we apply an approach similar to HumanEvalPack (Muennighoff et al., 2023  ###reference_b25###  ###reference_b25###  ###reference_b25###), where the instructions are automatically generated from the original prompt.\nWe combine these instructions with the CodeLLaMAInstruct template to create a prompt.\nThe prompt is formulated into two consecutive turns where the first turn is about the auxiliary function and the second one is about generating the target function333Refer to the appendix for the detailed template structure..\nOur empirical results show that CodeLLaMAInstruct models perform better than CodeLLaMA models when implementing functions without auxiliary functions (Table 1  ###reference_###  ###reference_###  ###reference_###, Direct), which is consistent with previous findings (Rozière et al., 2023  ###reference_b31###  ###reference_b31###  ###reference_b31###).\nOn the other hand, when an appropriate auxiliary function is provided in the prompt (Table 1  ###reference_###  ###reference_###  ###reference_###, Oracle), the base models show better performance than the instruction-tuned models.\nIn addition, the relative improvement in the Step-by-step settings has prominently decreased compared to that of the base models.\nThis suggests that the ability to utilize other functions in the context has been weakened during the instruction tuning process.\nTherefore, it is necessary to develop an advanced instruction-tuning methodology to incorporate the previously implemented functions, which is our future work.\n###figure_14###"
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Implementation style analysis",
            "text": "We analyze the generated implementation based on their style and compare preferences between them.\nIn this experiment, we use the implementations generated under the Oracle setting.\nTo identify their implementation style, we apply Python static parser555https://docs.python.org/3/library/ast  ###reference_### and check whether they called the given auxiliary function.\nThe implementations that call the auxiliary function are regarded as black-box style while the rest as white-box style.\nThe black-box style directly utilizes the auxiliary function as is, while the white-box style mimics the internal mechanism of the auxiliary function.\n###figure_15### We compute pass@1 scores for each style and model (Figure 4  ###reference_###  ###reference_###  ###reference_###).\nThe results show that all models can implement functions in both styles.\nAlso, we observed that the pass@1 score for the black-box style is higher than that of the white-box style.\nIt implies that calling an auxiliary function is much safer and more accurate if the target function can be implemented by calling the auxiliary function.\nCurrently, up to 40% of the model-generated implementations are implemented in black-box style, even though most examples can be implemented in black-box style.666147 of 151 human-written reference solutions in the HumanExtension dataset are black-box style.\nTherefore, it is expected that the pass@1 score can be improved if more examples are implemented in black box style.\nFurthermore, we would like to emphasize that the improvement of the ability to generate black-box implementations is diminishing as language models evolve.\nThis phenomenon suggests that model developers should consider the model’s function call ability when learning their models.\nFurther investigating the two different styles, we conduct a human pairwise preference evaluation with human-written implementations (Human), and model-written ones with both styles (Black box and White box).\nWe created a labeling sheet with 17 examples that CodeLLaMAPython 34B implements in both styles correctly.\nWe recruited labelers who have been coding with Python for over five years.\nFor the three possible pairs, labelers were instructed to choose the better implementations according to their preference such as performance or readability.\nThe evaluation results in Table 2  ###reference_###  ###reference_###  ###reference_### show that implementations that call auxiliary functions are preferred over implementations that do not.\nAfter inspecting the result qualitatively, we interpret that most black box implementations were selected due to their clarity and conciseness coming from appropriately delegating subroutines to auxiliary functions.\nUsually, the model-generated white-box implementations tend to repeat the identical mechanism inside the auxiliary function, which is not preferred in software engineering fields (Hunt and Thomas, 2000  ###reference_b15###  ###reference_b15###  ###reference_b15###).\nIn few cases, white-box implementations are preferred over black-box ones as they are considered as over-engineering.\nTherefore, training the models to delegate the subroutine to other functions suitably would be the next step for generating realistic code.\n###figure_16### ###figure_17###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Multiple auxiliary function experiment",
            "text": "We provide several auxiliary functions in the prompt and study whether the model selectively utilizes the appropriate auxiliary function.\nWe confirmed that CodeLLaMA models and CodeLLaMAPython models show different trends in terms of pass@1 scores (Figure 4(a)  ###reference_sf1###  ###reference_sf1###).\nFor CodeLLaMA models, the pass@1 scores showed a U-shape trend, indicating that the performance improved when the related function was located at the first or the last.\nThis result is consistent with the existing findings (Liu et al., 2023b  ###reference_b23###  ###reference_b23###) that, in natural language processing tasks, LLMs can effectively utilize relevant documents when they are located at the beginning or end.\nOn the other hand, for CodeLLaMAPython models, this U-shape trend was weakened and the pass@1 score increased only when the relevant function was located at the end.\nWe conjectured that the two related functions were usually located adjacently in Python codes and this pattern was learned by the model.\nHowever, since the location of relevant functions is independent of their functionality, LLMs need to be tuned to robustly utilize them regardless of where they are placed.\nWe found that there exists a strong correlation between the pass@1 score and the proportion of black-box style implementations.\nThe Pearson correlation scores between the proportion and the pass rate (Figure 4(b)  ###reference_sf2###  ###reference_sf2###) are larger than 0.9, indicating that LLMs get higher scores when they try to call appropriate auxiliary functions.\nHowever, the black-box style implementations are mostly observed when the relevant auxiliary functions are located at the last, which provides an explanation of why the pass@1 score is higher when the relevant function is located at the last.\nFor CodeLLaMA models, they can call the relevant function if they are located at the first, which causes the U-shape trend in pass@1 scores.\nModel scaling and additional training with Python codes provide a marginal effect on promoting a model to generate black-box style implementations, suggesting that specialized training for LLMs to call relevant functions similar to invoking general LLMs to use tools (Schick et al., 2023  ###reference_b32###  ###reference_b32###) is needed for enhancing their code synthesis ability."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Experimental setup",
            "text": "We design a prompt with nine auxiliary functions followed by the target function signature.\nThe functions consist of one relevant auxiliary function and the others are randomly sampled from the auxiliary function pool used in the Irrelevant setting.\nWe change the location of the relevant function in the prompt and measure the pass@1 score and the proportion of black-box style implementations classified as the existence of auxiliary function call."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Result",
            "text": "The experimental results are shown in Figure 5  ###reference_### and we list up empirical findings we observed.\nWe confirmed that CodeLLaMA models and CodeLLaMAPython models show different trends in terms of pass@1 scores (Figure 4(a)  ###reference_sf1###  ###reference_sf1###  ###reference_sf1###).\nFor CodeLLaMA models, the pass@1 scores showed a U-shape trend, indicating that the performance improved when the related function was located at the first or the last.\nThis result is consistent with the existing findings (Liu et al., 2023b  ###reference_b23###  ###reference_b23###  ###reference_b23###) that, in natural language processing tasks, LLMs can effectively utilize relevant documents when they are located at the beginning or end.\nOn the other hand, for CodeLLaMAPython models, this U-shape trend was weakened and the pass@1 score increased only when the relevant function was located at the end.\nWe conjectured that the two related functions were usually located adjacently in Python codes and this pattern was learned by the model.\nHowever, since the location of relevant functions is independent of their functionality, LLMs need to be tuned to robustly utilize them regardless of where they are placed.\nWe found that there exists a strong correlation between the pass@1 score and the proportion of black-box style implementations.\nThe Pearson correlation scores between the proportion and the pass rate (Figure 4(b)  ###reference_sf2###  ###reference_sf2###  ###reference_sf2###) are larger than 0.9, indicating that LLMs get higher scores when they try to call appropriate auxiliary functions.\nHowever, the black-box style implementations are mostly observed when the relevant auxiliary functions are located at the last, which provides an explanation of why the pass@1 score is higher when the relevant function is located at the last.\nFor CodeLLaMA models, they can call the relevant function if they are located at the first, which causes the U-shape trend in pass@1 scores.\nModel scaling and additional training with Python codes provide a marginal effect on promoting a model to generate black-box style implementations, suggesting that specialized training for LLMs to call relevant functions similar to invoking general LLMs to use tools (Schick et al., 2023  ###reference_b32###  ###reference_b32###  ###reference_b32###) is needed for enhancing their code synthesis ability."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have explored the ability to utilize auxiliary functions encoded in the LLMs through our newly proposed HumanExtension dataset.\nThe HumanExtension dataset is constructed to contain function relationships while considering the software engineering concepts.\nOur multi-faceted experiments with the HumanExtension dataset comprehensively show the current LLMs’ ability to harness auxiliary functions.\nOur auxiliary function experiments demonstrate that the LLMs have the ability to utilize auxiliary functions even when the function is implemented by themselves.\nHowever, our in-depth analysis discovered that their ability varies depending on the factors that humans might not, i.e., the position of relevant functions, function name, and docstring.\nFurthermore, our implementation style analysis reveals that, in some cases, the LLMs repeat the mechanism of the given auxiliary function while humans simply call the auxiliary functions, suggesting the future research direction of current code LLMs for auxiliary function calls."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Although the curated dataset in this study allowed us to evaluate the ability to utilize auxiliary functions from a variety of perspectives, it has some limitations in determining whether multiple relevant auxiliary functions can be jointly utilized.\nAdditionally, our behavioral analyses indicate that the capabilities have been empirically observed, but it might be insufficient to conclude the model truly understands and utilizes the auxiliary function, so additional methods are required to reinforce the statement."
        }
    ]
}