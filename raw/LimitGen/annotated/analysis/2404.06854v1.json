{
    "title": "Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata",
    "abstract": "The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Non-autoregressive (NAR) models for text generation offer the promise of much faster generation than auto-regressive (AR) models. However NAR models have been largely developed for Neural Machine Translation (NMT) Xiao et al. (2022  ###reference_b19###), with other Natural Language Generation (NLG) tasks less well studied. We will show how a NAR model developed for NMT, the Directed Acyclic Transformer (DAT) (Huang et al., 2022  ###reference_b5###), can be used for generation in Task-Oriented Dialogue (TOD) and Data-to-Text (D2T) scenarios.\nDATs as originally developed for NMT perform poorly in NLG on TOD and D2T tasks: they fail to generate specified entity names in up to 40% of responses and frequently (>20%) produce Out-Of-Vocabulary (OOV) words. Practical systems must operate at zero error rate in these aspects to be deployable at scale. Previous NAR study reported similar error patterns Xiao et al. (2022  ###reference_b19###). Unless these shortcomings are addressed, NAR models will not be usable for general NLG.\nWe introduce three constrained decoding procedures for NLG using DATs. Our approach converts Directed Acyclic Graphs (DAG) generated by DAT into Weighted Finite State Automata (WFSA). We then intersect these WFSAs with other automata that are defined to ensure that designated entities (lexical constraints) are generated and OOVs are eliminated (vocabulary constraints). To avoid generating responses that are too short, we employ a Viterbi decoding algorithm to control the target length of the generated text (length constraints).\nWe refer to the decoding procedure that incorporates all these steps as Control-DAG. We evaluate extensively on the Schema Guided Dialogue (SGD) (Rastogi et al., 2020  ###reference_b15###) and the Data Record To Text (DART) datasets (Nan et al., 2021  ###reference_b11###) for NLG in TOD and D2T domains. Our Directed Acyclic T5 model, when decoded with Control-DAG, is free from OOV error, faithfully generates all specified entity names, and achieves marked BLEU and BLEURT gains on both datasets.\nWe use pynini Gorman (2016  ###reference_b2###) for WFSA operations.\nOur contributions are summarized below:\nWe introduce Control-DAG, a constrained decoding algorithm which simultaneously offers lexical, vocabulary, and length controls for Directed Acyclic models, addressing key limitations in NAR text generation.\nWe demonstrate the effectiveness of Control-DAG on two major NLG tasks: Task-Oriented Dialogues and Data-to-Text. To our knowledge, DA-T5 with Control-DAG is the first practical NAR benchmark on the SGD and the DART datasets.111Code: https://github.com/EriChen0615/ControlDAG  ###reference_###\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The Directed Acyclic Transformer (DAT) Huang et al. (2022  ###reference_b5###) performs on par with AR baselines in NMT and has attracted much interests. Shao et al. (2022  ###reference_b17###) developed a Viterbi decoding algorithm for DAT. Ma et al. (2023  ###reference_b9###) introduced a fuzzy alignment objective to improve DAT training. In NLG, PreDAT (Huang et al., 2023  ###reference_b4###) pretrains a DAT for open-domain dialogue, notably with high word error rate reported even after extensive pre-training. Our work highlights the links between DATs and automata, and shows well-studied WFSA algorithms Mohri et al. (2002  ###reference_b10###) can be used in constrained decoding to eliminate OOV errors.\nEnforcing lexical constraints in auto-regressive decoding has been studied extensively. Constrained beam search (CBS) Post and Vilar (2018  ###reference_b13###); Hu et al. (2019  ###reference_b3###); Li et al. (2020  ###reference_b8###) is a widely used family of lexically constrained decoding procedure. We show how CBS can be adapted to NAR Directed Acyclic models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Constrained Decoding with DA-T5",
            "text": "The architecture of our DA-T5 model follows that of the DAT by Huang et al. (2022  ###reference_b5###). Conceptually, DAT takes an input sequence and generates a DAG with a pre-determined number of DAG vertices. Vertex embeddings are produced first, and then token emission probabilities and state transition probabilities are generated from these vertex embeddings via softmax and self-attention, resp. Each vertex has a token emission distribution. These vertices and transitions define a weighted DAG that contains output string hypotheses. DAT uses a vanilla Transformer to produce vertex embeddings whereas we use T5, hence the name DA-T5.\nIn training DA-T5, we use ‘glancing training’ Qian et al. (2021  ###reference_b14###) as DAT. In inference, DAGs are generated with DA-T5 and converted to WFSAs. The procedure is simply Moore-to-Mealy Machine conversion (Appendix B.1  ###reference_###). Prior to the conversion, we perform likelihood-based pruning of each vertex, keeping  most likely output tokens and  most likely out-going arcs. This pruning balances coverage against decoding speed, with larger thresholds leading to a more complete WFSA at the cost of slower decoding.\nFor each phrase  that must appear in the generation, we construct a constraint FSA  that accepts and only accepts strings where the phrase  appears at least once, corresponding to the regular expression “” IEEE (2004  ###reference_b6###). We then intersect the WFSA converted from the DAG with all of the constraint FSAs. The resulting WFSA  contains only hypotheses that satisfy all lexical constraints.\nWe build a vocabulary FSA  that accepts and only accepts strings of words from a valid vocabulary; intersection with  prevents OOV errors.  is obtained from three FSAs: a dictionary FSA  that accepts and only accepts English words; a special token FSA  that accepts and only accepts numbers, punctuation, and special tokens; and a dynamic FSA  that accepts and only accepts entity names specified in the input. The final vocabulary FSA  is obtained by unioning the three FSAs and taking the Kleene closure (Eq.1  ###reference_###).\nFor efficiency, we perform a one-time determinization and minimization Mohri et al. (2002  ###reference_b10###) of the union () and store the optimized FSA in memory.\nShao et al. (2022  ###reference_b17###) introduced a Viterbi decoding procedure for DAT that finds the highest scoring hypothesis for each string length. We find this exact Viterbi procedure to be impractical because the number of WFSA states can be large (>30,000) after intersection with the constraint FSAs. We introduce a pruned version of this procedure, Depth-First Search Viterbi (DFS-Viterbi). DFS-Viterbi searches the WFSA with DFS and keeps the best hypotheses of all possible string lengths at each vertex to avoid repeated computation. During DFS, we only explore the minimal set of out-going edges such that their cumulative probability is bigger than a threshold . This pruning is inadmissible but works well in practice. We also introduce an exponential length penalty that penalizes strings shorter than target length  and select the hypothesis with the lowest overall costs. In experiments to follow,  is obtained via simple linear regression.\nIn addition to automata-based methods, we introduce CBS-DAG, a constrained beam search algorithm for our NAR DA-T5. CBS-DAG is straight-forwardly adapted from AR CBS by Hu et al. (2019  ###reference_b3###) (Appendix B.4  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Constrained Decoding",
            "text": "For hard lexical and vocabulary constraints we build corresponding Finite State Automata (FSA). Intersecting the WFSA with these constraint FSAs produces a WFSA that only contains hypotheses that satisfy all constraints Mohri et al. (2002  ###reference_b10###).\nFor length constraints, we propose a pruned version of DAT Viterbi decoding by Shao et al. (2022  ###reference_b17###) to search for strings with specified length. Appendix B  ###reference_### gives implementation details and complexity analyses. Figure 1  ###reference_### illustrates our Control-DAG system with an example.\nFor each phrase  that must appear in the generation, we construct a constraint FSA  that accepts and only accepts strings where the phrase  appears at least once, corresponding to the regular expression “” IEEE (2004  ###reference_b6###  ###reference_b6###). We then intersect the WFSA converted from the DAG with all of the constraint FSAs. The resulting WFSA  contains only hypotheses that satisfy all lexical constraints.\nWe build a vocabulary FSA  that accepts and only accepts strings of words from a valid vocabulary; intersection with  prevents OOV errors.  is obtained from three FSAs: a dictionary FSA  that accepts and only accepts English words; a special token FSA  that accepts and only accepts numbers, punctuation, and special tokens; and a dynamic FSA  that accepts and only accepts entity names specified in the input. The final vocabulary FSA  is obtained by unioning the three FSAs and taking the Kleene closure (Eq.1  ###reference_###  ###reference_###).\nFor efficiency, we perform a one-time determinization and minimization Mohri et al. (2002  ###reference_b10###  ###reference_b10###) of the union () and store the optimized FSA in memory.\nShao et al. (2022  ###reference_b17###  ###reference_b17###) introduced a Viterbi decoding procedure for DAT that finds the highest scoring hypothesis for each string length. We find this exact Viterbi procedure to be impractical because the number of WFSA states can be large (>30,000) after intersection with the constraint FSAs. We introduce a pruned version of this procedure, Depth-First Search Viterbi (DFS-Viterbi). DFS-Viterbi searches the WFSA with DFS and keeps the best hypotheses of all possible string lengths at each vertex to avoid repeated computation. During DFS, we only explore the minimal set of out-going edges such that their cumulative probability is bigger than a threshold . This pruning is inadmissible but works well in practice. We also introduce an exponential length penalty that penalizes strings shorter than target length  and select the hypothesis with the lowest overall costs. In experiments to follow,  is obtained via simple linear regression.\nIn addition to automata-based methods, we introduce CBS-DAG, a constrained beam search algorithm for our NAR DA-T5. CBS-DAG is straight-forwardly adapted from AR CBS by Hu et al. (2019  ###reference_b3###  ###reference_b3###) (Appendix B.4  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "We evaluate on the SGD and the DART datasets.\nIn SGD, the aim is to generate natural utterances from dialogue actions (e.g., INFORM(destination=Cambridge)) that contain the specified information. DART is a more general data-to-text task that takes triplets of (SUBJECT, RELATION, OBJECT) to generate natural texts. Hyper-parameters and implementation details are in Appendix A  ###reference_###.\nWe use BLEURT Sellam et al. (2020  ###reference_b16###) and BLEU Papineni et al. (2002  ###reference_b12###) to measure text quality relative to ground truth text. We also report the BLEU Brevity Penalty (BP), as a small BP indicates too short generation.\nFor SGD, we use Slot Error Rate (SER) Kale and Rastogi (2020  ###reference_b7###) to evaluate lexical faithfulness. A slot error occurs when a slot value that should be reproduced exactly (e.g., a phone number) is not in the generated text.\nFor DART, we use subjects/objects whose string values are always in the ground-truth training text as hard lexical constraints and propose Exact Occurrence error Rate (EOR) for evaluation. EOR is the percentage of model responses where at least one of the string values from these subjects/objects is missing.\nFor OOV errors, we define neologism rate (NEO) to be the percentage of model’s responses that contain at least one OOV generation.\nWe emphasize that SER, EOR, and OOV are critical metrics as even a small error rate could lead to an intolerable number of misleading responses for systems deployed at scale.\n‘Speed up’ is measured against auto-regressive CBS implemented by Li et al. (2020  ###reference_b8###) with batch size of 1 to reflect a realistic NLG system that operates at zero SER/EOR.\nWe train DA-T5 from scratch by glancing training by Qian et al. (2021  ###reference_b14###) on the SGD and the DART datasets for 30 and 50 epochs, respectively. Auto-regressive T5 is trained following Chen et al. (2023  ###reference_b1###).\nWe use  and  for DAG-to-WFSA conversion on SGD and DART, respectively. For LC, we fit a simple linear regression model on the training set to predict the target token length given the input token length.\nDecoding hyper-parameters are determined on the validation sets."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Non-Autoregressive NLG on SGD",
            "text": "Table 1  ###reference_### reports NLG performance on SGD with auto-regressive T5 decoding in Rows 1-2 with greedy and beam search. Although these systems yield high BLEURT and BLEU, they still\ncommit slot errors (SER=0.12%).\nConstrained Beam Search (CBS) eliminates slot errors by forcing the generation of designated slot values, but with longer decoding times (16:05  22:15) and a degradation in BLEU () and BLEURT () compared to unconstrained beam search. This constraint-quality trade-off is also observed in previous study Post and Vilar (2018  ###reference_b13###); See Appendix D  ###reference_### for CBS failure modes. Auto-regressive T5 is completely free from OOV errors (NEO=0.0).\nTurning to non-autogressive NLG, generation with DA-T5 using common decoding methods (greedy, beam search) leads to very high SER (> 20%) and OOV errors in at least 20% of the generated responses (Rows 4, 5). Although our CBS-DAG (Row 6) eliminates SER by design and enhances quality as measured by BLEURT (+3.8) and BLEU (+3.4), its neologism rate is still unusably high (19.2%).\nWe now discuss the performance of our constrained decoding methods.\nUnconstrained WFSA shortest path decoding (Row 7) is as fast as greedy decoding, showing that DAGs can be efficiently converted to WFSAs. However, unconstrained generation directly from the WFSA frequently leads to\nslot errors (SER=34.8%), OOV errors (NEO=12.2%), and a harsh brevity penalty (BP=0.44).\nThese aspects of text quality can be improved individually by constrained decoding (Rows 8-10): Hard Lexical Constrained decoding eliminates slot errors (SER=0); Vocabulary constraints eliminate OOV errors (NEO=0); and Length constrained decoding leads to better text lengths (BP=1.0).\nControl-DAG (Row 11) combines these methods to achieves zero SER and zero neologism rate while satisfying the length requirement and yielding a speed advantage of x1.7 relative to auto-regressive CBS.\nTable 2  ###reference_### shows the performance of using existing decoding procedures developed for DA-Transformer to decode DA-T5 on the SGD dataset. Control-DAG\nhas the overall best BLEU (22.9) and BLEURT (60.0) ."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose Control-DAG for decoding non-autoregressive Directed Acyclic models with lexical, vocabulary, and length constraints, addressing key limitations in NAR text generation. Constrained decoding is efficiently performed via well-studied Weighted Finite State Automata algorithms. DA-T5 with Control-DAG establishes strong NAR results on the Schema Guided Dialogue and the DART datasets, bridging gaps in NAR research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "Jinghong Chen is supported by the Warwick Postgraduate Studentship from Christ’s College and the Huawei Hisilicon Studentship for the undertaking of the PhD in Engineering at the University of Cambridge.\nWeizhe Lin was supported by a Research Studentship funded by Toyota Motor Europe (RG92562(24020)).\nProf. Bill Byrne holds concurrent appointments as a Professor of Information Engineering at Cambridge University and as an Amazon Scholar. This publication describes work performed at Cambridge University and is not associated with Amazon.\nWe would also like to thank all the reviewers for their knowledgeable reviews."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Given our focus on decoding algorithms, we leave further training and model scaling to future work. It is possible to further improve inference speed by writing the DAG-to-WFSA conversion and the DFS-Viterbi algorithm in the C programming language to reduce overhead from the python interface. In this paper, we demonstrate substantial speed-up can be achieved without these optimizations and leaves further speed-up techniques to future work."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Statement",
            "text": "We trained two versions of the DA-T5 model: one on the training set of Schema Guided Dialogue and one on the training set of the DART dataset. These are English datasets and do not contain sensitive personal information or offensive language. Detailed statistics of the SGD and DART datasets can be found in Rastogi et al. (2020  ###reference_b15###) and Nan et al. (2021  ###reference_b11###), respectively. We note that the model may hallucinates information or generates language that appears offensive. Some linguistic phenomena of our DA-T5 models are in Appendix D  ###reference_###. It is vital that developers test DA-T5 fully before deployment.\nAll software packages that our code built on are used as their original intention. Our code is released under the MIT license."
        }
    ]
}