{
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "abstract": "We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models’ reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\nIn particular, the proposed method — retrieval-augmented thoughts (RAT)\n— revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated.\nApplying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have achieved fruitful progress on various natural language reasoning tasks (Wei et al., 2022  ###reference_b56###; Yao et al., 2022  ###reference_b58###; Wang et al., 2023a  ###reference_b53###; Zhou et al., 2023  ###reference_b66###; Brown et al., 2020  ###reference_b6###), especially when combining large-scale models  (Team, 2022  ###reference_b47###; OpenAI, 2023  ###reference_b39###) with sophisticated prompting strategies, notably  chain-of-thought (CoT) prompting (Wei et al., 2022  ###reference_b56###; Kojima et al., 2022  ###reference_b26###). However, there have been increasing concerns about the factual correctness of LLMs reasoning, citing the possible hallucinations in model responses (Rawte et al., 2023  ###reference_b41###) or the intermediate reasoning paths, i.e. CoTs (Dhuliawala et al., 2023  ###reference_b13###). This issue becomes more significant when it comes to zero-shot CoT prompting, aka. “let’s think step-by-step” (Kojima et al., 2022  ###reference_b26###) and long-horizon generation tasks that require multi-step and context-aware reasoning, including code generation, task planning, mathematical reasoning, etc. Factually valid intermediate thoughts could be critical to the successful completion of these tasks.\nSeveral prompting techniques have been proposed to mitigate this issue, one promising direction,  Retrieval Augmented Generation (RAG) (Lewis et al., 2020b  ###reference_b28###) seeks insights from human reasoning (Holyoak and Morrison, 2012  ###reference_b21###), and utilizes retrieved information to facilitate more factually grounded reasoning.\nIn this paper, we explore how to synergize RAG with sophisticated long-horizon reasoning. Our intuition is that the hallucination within the intermediate reasoning process could be alleviated through the help of outside knowledge. The resulting prompting strategy, retrieval-augmented thoughts (RAT), is illustrated in Figure 1  ###reference_###. Our strategy comprises two key ideas. Firstly, the initial zero-shot CoT produced by LLMs along with the original task prompt will be used as queries to retrieve the information that could help revise the possibly flawed CoT. Secondly, instead of retrieving and revising with the full CoT and producing the final response at once, we devise a progressive approach, where LLMs produce the response step-by-step following the CoT (a series of subtasks), and only the current thought step will be revised based on the information retrieved with task prompt, the current and the past CoTs. This strategy can be an analogy to the human reasoning process: we utilize outside knowledge to adjust our step-by-step thinking during complex long-horizon problem-solving (Holyoak and Morrison, 2012  ###reference_b21###). A comparison of RAT and counterparts can be found in Figure 2  ###reference_###.\nWe evaluate RAT on a wide collection of challenging long-horizon tasks, including code generation, mathematical reasoning, embodied task planning, and creative writing. We employ several LLMs of varied scales: GPT-3.5 (Brown et al., 2020  ###reference_b6###), GPT-4 (OpenAI, 2023  ###reference_b39###), CodeLLaMA-7b (Rozière et al., 2023  ###reference_b44###). The results indicate that combing RAT with these LLMs elicits strong advantages over vanilla CoT prompting and RAG approaches. In particular, we observe new state-of-the-art level of performances across our selection of tasks: 1) code generation: HumanEval (+20.94%), HumanEval+ (+18.89%), MBPP (+14.83%), MBPP+ (+1.86%); 2) mathematical reasoning problems: GSM8K (+8.36%), and GSMHard (+31.37%); 3) Minecraft task planning (2.96 times on executability and +51.94% on plausibility); 4) creative writing (+19.19% on human score). Our additional ablation studies further confirm the crucial roles played by the two key ingredients of RAT: revising CoT using RAG and progressive revision & generation. This work reveals how can LLMs revise their reasoning process in a zero-shot fashion with the help of outside knowledge, just as what humans do."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Retrieval Augmented Thoughts",
            "text": "Our goal is to support long-horizon reasoning and generation while mitigating hallucination when using LLMs. To have satisfying performance on long-horizon tasks, two ingredients are indispensable. Firstly, access to factual information can be facilitated by retrieval. Secondly, appropriate intermediate steps that outline a scratchpad to finish complex tasks, can be facilitated by CoT. Yet, a naive combination of the two would not necessarily yield improvements. Two questions still persist: (1) what is relevant information to retrieve; (2) how to effectively correct reasoning steps with relevant factual information. To better appreciate our method and why our method can address these two questions, we first provide a brief preliminary introduction of RAG and CoT."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preliminary",
            "text": "Retrieval-Augmented Generation (RAG) targets the problem of generating fictitious facts by providing LLMs with relevant text extracted from trusted sources. It is primarily used in question-answering (QA) tasks (Lewis et al., 2020b  ###reference_b28###). Specifically, given a set of  candidate documents , RAG aims to retrieve the most relevant ones w.r.t. a query , which can be the question/task prompt itself or relevant information generated by LLMs. To achieve this, RAG first extracts semantic-aware embeddings of the documents  ( is the size of the embedding) as well as the query .  can be implemented with various text embedding models, such as Sentence-BERT (Reimers and Gurevych, 2019  ###reference_b42###). The relevance between the query and a document is measured by their cosine similarity:\nBased on their relevance, the top-ranked  documents are then fed into the prompt for LLMs to generate the final answer. With such rich and factual contexts, RAG mitigates the hallucination of LLMs. However, complex reasoning tasks (e.g., those requiring multi-step reasoning) can be difficult to translate into effective search queries, leading to challenges in finding relevant documents and making RAG less applicable. Traditionally, RAG retrieves all relevant information at once. Yet, it overlooks the fact that it is difficult to predict what “facts\" or information is required in the subsequent reasoning and generation steps. The task prompt itself is hardly sufficient to provide enough clues for this.\nChain of Thoughts (CoT) prompting is designed to enhance the performance of LLMs under tasks that require complex reasoning steps (Wei et al., 2022  ###reference_b56###), such as multi-step math word problems. Specifically, instead of tasking LLMs to generate the correct answer directly, CoT prompting incentivizes LLMs to first output intermediate reasoning steps, termed thoughts, that serve as a scratch space for the task, before summarizing the thoughts into a final answer. Such behavior of LLMs can either be stimulated in zero-shot by prompting terms that encourage CoT reasoning (e.g., “let’s think step by step”) (Kojima et al., 2022  ###reference_b26###), or triggered by few-shot examples that perform CoT in similar tasks. However, since no direct supervision is posed to the intermediate thoughts, LLMs could make errors due to the lack of relevant domain knowledge (Touvron et al., 2023  ###reference_b48###) or biased by hallucinations (Rawte et al., 2023  ###reference_b41###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Our Approach",
            "text": "Our intuition to mitigate the issues of CoT prompting and RAG mentioned above is to apply RAG to revise every thought step generated by CoT prompting. An overview can be found in Figure 1  ###reference_### and Algorithm 1  ###reference_###. Specifically, given a task prompt , we first prompt LLM to generate step-by-step thoughts in zero shot (“let’s think step-by-step”) , where  represents the th thought step. In long-horizon generation tasks,  can either be the intermediate reasoning steps, e.g. the pseudo code with comments in code generation, article outline in creative writing, etc., or the draft response itself, e.g. a list of sub-goals in embodied task planning as shown in Figure 1  ###reference_###.\nSince  could be flawed (e.g., contains hallucination), we proceed to use RAG to revise every generated thought step before generating the final response from these thoughts. Specifically, assuming we have fixed the previous thought steps and now are about to revise , we begin by converting the text  into a query :\nwhere  can either be a text encoder or an LLM that translates the task prompt , the current and the past thought steps  into a query  that can be processed by the retrieval system. We adopt RAG to retrieve relevant documents  using , which are then prepended to the prompt to generate a revised thought step .\nFinally, depending on the actual task, the revised thought steps  can simply be used as the final model response, e.g., embodied task planning. For tasks like code generation, or creative writing, the LLM will be further prompted to produce the complete response (code, passage) from each revised thought step in a step-by-step fashion.\nNote that, when revising the -th thought step , instead of using the current step  only, or the complete chain of thoughts  to produce the query for RAG, we ensure the query  is produced from the current thought step  and previous revised thought steps , i.e., we adopt a casual reasoning to revise the thoughts using RAG:\nThis allows for the correction of errors in the original thoughts  by continually consulting different reference texts and ensures that each step of reasoning is informed by the most accurate and relevant information, significantly improving the quality and reliability of the generated output.\nOur hypothesis why our method can address the two problems mentioned at the beginning of this section is as follows. Firstly, the most straightforward way to know what information will be used in complex reasoning is to “see” the reasoning steps. Our approach leverages all the generated thoughts along with the task prompt to provide more clues for more effective retrieval. Secondly, some information cannot be directly retrieved, especially information related to the final answer to a hard complex question. Instead, retrieval of information relevant to intermediate questions, which are assumed to be easier, is more accessible. Thanks to the compositional nature of many reasoning tasks, an iterative retrieval process could also be more effective. Thirdly, correcting potential hallucinations needs to be targeted. Revising a complete CoT with RAG could introduce errors at otherwise already-correct steps. Revising every step one by one could be more reliable. The first two points address question (1) and the last point addresses question (2). Quantitative evidence can be found in our ablation studies in Section 3.4  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We test our proposed method RAT on a diverse set of benchmarks that highlight long-horizon generation and reasoning. Existing methods traditionally struggle in those benchmarks; “hallucinated\" steps are obvious in LLMs’ outputs. Those steps either fail to stick to the original query or are plainly invalid. We kindly refer readers to subsection 3.3 (case analysis) for a more detailed discussion. Due to space constraints, we do not introduce each benchmark setting, nor do we discuss our results in each benchmark in full length. Rather, this section provides a comprehensive demonstration of our method’s performance and provides a spotlight to provide preliminary empirical analysis about why and when our method works and when it fails.\nFor code generation, the classical pass rate pass@k is selected as the evaluation metrics (Chen et al., 2021  ###reference_b9###; Liu et al., 2023b  ###reference_b35###),  denotes the sampling number. We compute accuracy to evaluate every question in mathematical reasoning tasks, aligning with the established metric for the GSM8K (Cobbe et al., 2021  ###reference_b10###). For embodied planning tasks, we compute the plan execution success rate in MC-TextWorld as executability (Lin et al., 2023  ###reference_b32###). We also conduct human elo rating evaluation to compute the trueskill rating score (Herbrich et al., 2006  ###reference_b20###) for embodied planning (as plausibility) and creative writing tasks. These indicators are better the higher they are.\nTo establish a comprehensive and equitable comparison landscape, we incorporate a suite of baseline methods. Our baselines include the original language models, referred to as DIRECT, and the Retrieval-Augmented Generation (RAG) methodology with  retrieved examples, instantiated in both single-shot (1 shot) and multi-shot (5 shots) configurations, as documented by Lewis et al. (2020b  ###reference_b28###). Additionally, we examine the zero-shot CoT (CoT) approach, as conceptualized by Kojima et al. (2022  ###reference_b26###), which simulates a step-by-step reasoning process to facilitate complex problem-solving tasks under zero demonstration.\nFor different methods, the same language model is used as base models.\nTo ensure a fair comparison, none of the methods used examples from the benchmark as demonstrations for in-context learning.\nRAT leverages the capabilities of Retrieval-Augmented Generation methods, which enhance the performance of language models by integrating external knowledge sources. Specifically, we employed the codeparrot/github-jupyter dataset as our primary search vector library for code generation and mathematical reasoning tasks.\nFor embodied planning tasks in Minecraft, we utilized the Minecraft Wiki111https://minecraft.wiki/  ###reference_minecraft.wiki/### and DigMinecraft222https://www.digminecraft.com/  ###reference_www.digminecraft.com/### websites as the information sources accessible to the LLMs.\nFor open-ended creative writing tasks, we use Google to search the query on the Internet.\nWe utilized OpenAI’s text-embedding-ada-002 API service for all embedding calculations across different methods and base models.\nAcknowledging the risk of benchmark contamination (an issue where the code library may contain solutions to the exact problems being evaluated), we adopted a rigorous pre-processing methodology as described by Guo et al. (2024  ###reference_b19###).\nThe potential implications of benchmark contamination, along with the effectiveness of our pre-processing strategy, are discussed in detail in Appendix D  ###reference_###.\nIn this ablation study, we investigate the influence of various retrieval strategies on the efficacy of RAT, focusing on the optimization of content retrieval for improving generative outputs. The experimental results, detailed in Table 3  ###reference_###, highlight the significant advancements achieved through the iterative refinement of retrieval queries in RAT compared to baseline methods. The baseline denoted as RAG-1, employs a direct approach by using the question itself as the retrieval query. In contrast, CoT+RAG enhances this process by utilizing the entirety of the reasoning thoughts output by the language model as the query, aiming for a broader contextual understanding. However, RAT introduces a more dynamic method by employing continuously modified parts of reasoning thoughts as queries, which allows for a more focused and relevant information retrieval process.\nThe comparative analysis shows that RAT surpasses both the baseline and the CoT+RAG method in terms of pass@1 and pass@5 metrics across the HumanEval and HumanEval+ benchmarks. Specifically, RAT demonstrates an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over the baseline in the HumanEval benchmark, and similarly impressive gains in the HumanEval+ benchmark. These improvements underscore the effectiveness of RAT’s retrieval strategy, which by iteratively refining next queries based on evolving reasoning thoughts and previous queries, ensures the retrieval of highly pertinent information. This process not only enhances the relevance of the information retrieved but also significantly improves the quality and accuracy of the final generated outputs. The results firmly establish the superiority of RAT’s dynamic retrieval method in leveraging contextual nuances to drive more precise and effective generative processes.\nIn this ablation study, we systematically examine the impact of causal and non-causal reasoning approaches on the performance of the RAT system, with the Chain of Thought (CoT) serving as our baseline. Our findings, as summarized in Table 4  ###reference_###, reveal significant enhancements in generation capabilities when incorporating causal reasoning techniques. Specifically, the causal approach, which iteratively performs reasoning and retrieval, leads to notable improvements in both pass@1 and pass@5 metrics across HumanEval and HumanEval+ benchmarks. For instance, the causal method outperforms the baseline (CoT) by 11.9 percentage points in pass@1 and by 4.6 percentage points in pass@5 on the HumanEval dataset. This approach contrasts with the non-causal method, which, although also surpassing the baseline, leverages the initial reasoning thought to directly retrieve all necessary steps and generate the final answer. The causal method’s superior performance underscores the value of sequential reasoning and information retrieval in enhancing the accuracy and reliability of generated outputs. This iterative process likely aids in refining the search and reasoning steps based on continuously updated context, allowing for more precise and relevant information retrieval, which in turn supports more accurate final answers. These results firmly establish the efficacy of causal reasoning in long-horizon problem-solving tasks."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Setups",
            "text": "We adopt four groups of benchmarks.\nCode Generation includess HumanEval (Chen et al., 2021  ###reference_b9###), HumanEval+ (Liu et al., 2023b  ###reference_b35###), MBPP (Austin et al., 2021  ###reference_b2###), and MBPP+ (Liu et al., 2023b  ###reference_b35###). These benchmarks encompass a wide range of programming problems, from simple function implementations to more complex algorithmic challenges, providing a robust testbed for assessing generative capabilities.\nMathematical Reasoning evaluation is conducted on GSM8K and GSM-HARD dataset, which comprises thousands of multi-step mathematical problems (Cobbe et al., 2021  ###reference_b10###; Gao et al., 2022  ###reference_b16###).\nCreative Writing tasks are conducted to evaluate the versatility of RAT, including survey, summarization etc., highlighting different aspects of open-ended text generation.\nEmbodied Planning tasks are evaluated on open-ended environments Minecraft. A set of 100 tasks ranging from simple objectives to challenging diamond objectives are evaluated through MC-TextWorld (Lin et al., 2023  ###reference_b32###).\nFor code generation, the classical pass rate pass@k is selected as the evaluation metrics (Chen et al., 2021  ###reference_b9###  ###reference_b9###; Liu et al., 2023b  ###reference_b35###  ###reference_b35###),  denotes the sampling number. We compute accuracy to evaluate every question in mathematical reasoning tasks, aligning with the established metric for the GSM8K (Cobbe et al., 2021  ###reference_b10###  ###reference_b10###). For embodied planning tasks, we compute the plan execution success rate in MC-TextWorld as executability (Lin et al., 2023  ###reference_b32###  ###reference_b32###). We also conduct human elo rating evaluation to compute the trueskill rating score (Herbrich et al., 2006  ###reference_b20###  ###reference_b20###) for embodied planning (as plausibility) and creative writing tasks. These indicators are better the higher they are.\nTo establish a comprehensive and equitable comparison landscape, we incorporate a suite of baseline methods. Our baselines include the original language models, referred to as DIRECT, and the Retrieval-Augmented Generation (RAG) methodology with  retrieved examples, instantiated in both single-shot (1 shot) and multi-shot (5 shots) configurations, as documented by Lewis et al. (2020b  ###reference_b28###  ###reference_b28###). Additionally, we examine the zero-shot CoT (CoT) approach, as conceptualized by Kojima et al. (2022  ###reference_b26###  ###reference_b26###), which simulates a step-by-step reasoning process to facilitate complex problem-solving tasks under zero demonstration.\nFor different methods, the same language model is used as base models.\nTo ensure a fair comparison, none of the methods used examples from the benchmark as demonstrations for in-context learning.\nRAT leverages the capabilities of Retrieval-Augmented Generation methods, which enhance the performance of language models by integrating external knowledge sources. Specifically, we employed the codeparrot/github-jupyter dataset as our primary search vector library for code generation and mathematical reasoning tasks.\nFor embodied planning tasks in Minecraft, we utilized the Minecraft Wiki111https://minecraft.wiki/  ###reference_minecraft.wiki/###  ###reference_minecraft.wiki/### and DigMinecraft222https://www.digminecraft.com/  ###reference_www.digminecraft.com/###  ###reference_www.digminecraft.com/### websites as the information sources accessible to the LLMs.\nFor open-ended creative writing tasks, we use Google to search the query on the Internet.\nWe utilized OpenAI’s text-embedding-ada-002 API service for all embedding calculations across different methods and base models.\nAcknowledging the risk of benchmark contamination (an issue where the code library may contain solutions to the exact problems being evaluated), we adopted a rigorous pre-processing methodology as described by Guo et al. (2024  ###reference_b19###  ###reference_b19###).\nThe potential implications of benchmark contamination, along with the effectiveness of our pre-processing strategy, are discussed in detail in Appendix D  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "The code generation results presented in Table 1  ###reference_### and results on other benchmarks presented in Table 2  ###reference_### demonstrate the comprehensive evaluation of the RAT across multiple benchmarks.\nRAT consistently outperforms the other methods across the majority of the benchmarks and metrics, showcasing its superior ability to generate long-horizon context. Notably, in the HumanEval and HumanEval+ benchmarks of code generation, RAT achieves remarkable improvements in pass@1 and pass@5 rates, indicating a significant enhancement in first-attempt accuracy and within the top five attempts. For example, on the HumanEval benchmark, RAT improves pass@1 by up to 20.94% and pass@5 by up to 25.68% relative to the base models’ performances. This trend is observed across different underlying base models, highlighting RAT’s effectiveness regardless of the initial model’s capabilities.\nFor mathematical reasoning tasks, RAT demonstrates a significant relative improvement, with an 8.37% increase in accuracy on GSM8K and a remarkable 31.37% increase on GSMHard, culminating in an overall average improvement of 18.44% when deployed on the GPT-3.5 model.\nRAT significantly outperforms all other methods on open-ended embodied planning tasks in Minecraft, achieving the highest scores with 76.67±8.02% for executability and 29.37 human rating score for plausibility, demonstrating its superior ability to generate feasible and contextually appropriate plans in the complex open-world environment.\nRAT’s superior performance also keeps across a broad spectrum of creative writing tasks. Its ability to generate high-quality content in diverse scenarios was demonstrated, highlighting its potential as a powerful tool for enhancing the general creative writing capabilities of LLMs in open-ended scenarios.\nThe tasks are extremely diverse, while RAT can have consistent improvements over all baselines.\nThese results underline the advantages of RAT’s approach, which leverages iterative refinement of retrieval queries based on evolving reasoning thoughts. This strategy not only enhances the relevance and quality of the information retrieved but also significantly improves the accuracy and efficiency of the generated context."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Ablation Study",
            "text": "In this ablation study, we investigate the influence of various retrieval strategies on the efficacy of RAT, focusing on the optimization of content retrieval for improving generative outputs. The experimental results, detailed in Table 3  ###reference_###  ###reference_###, highlight the significant advancements achieved through the iterative refinement of retrieval queries in RAT compared to baseline methods. The baseline denoted as RAG-1, employs a direct approach by using the question itself as the retrieval query. In contrast, CoT+RAG enhances this process by utilizing the entirety of the reasoning thoughts output by the language model as the query, aiming for a broader contextual understanding. However, RAT introduces a more dynamic method by employing continuously modified parts of reasoning thoughts as queries, which allows for a more focused and relevant information retrieval process.\nThe comparative analysis shows that RAT surpasses both the baseline and the CoT+RAG method in terms of pass@1 and pass@5 metrics across the HumanEval and HumanEval+ benchmarks. Specifically, RAT demonstrates an 8.7 percentage point increase in pass@1 and a 7.9 percentage point increase in pass@5 over the baseline in the HumanEval benchmark, and similarly impressive gains in the HumanEval+ benchmark. These improvements underscore the effectiveness of RAT’s retrieval strategy, which by iteratively refining next queries based on evolving reasoning thoughts and previous queries, ensures the retrieval of highly pertinent information. This process not only enhances the relevance of the information retrieved but also significantly improves the quality and accuracy of the final generated outputs. The results firmly establish the superiority of RAT’s dynamic retrieval method in leveraging contextual nuances to drive more precise and effective generative processes.\nIn this ablation study, we systematically examine the impact of causal and non-causal reasoning approaches on the performance of the RAT system, with the Chain of Thought (CoT) serving as our baseline. Our findings, as summarized in Table 4  ###reference_###  ###reference_###, reveal significant enhancements in generation capabilities when incorporating causal reasoning techniques. Specifically, the causal approach, which iteratively performs reasoning and retrieval, leads to notable improvements in both pass@1 and pass@5 metrics across HumanEval and HumanEval+ benchmarks. For instance, the causal method outperforms the baseline (CoT) by 11.9 percentage points in pass@1 and by 4.6 percentage points in pass@5 on the HumanEval dataset. This approach contrasts with the non-causal method, which, although also surpassing the baseline, leverages the initial reasoning thought to directly retrieve all necessary steps and generate the final answer. The causal method’s superior performance underscores the value of sequential reasoning and information retrieval in enhancing the accuracy and reliability of generated outputs. This iterative process likely aids in refining the search and reasoning steps based on continuously updated context, allowing for more precise and relevant information retrieval, which in turn supports more accurate final answers. These results firmly establish the efficacy of causal reasoning in long-horizon problem-solving tasks."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Robustness of RAT",
            "text": "RAT was rigorously validated across a diverse set of tasks, including code generation, mathematical reasoning, creative writing, and embodied planning. This variety of tasks underscores the generalization capability of RAT, demonstrating its robust performance across highly diverse challenges. Furthermore, all our experimental settings were conducted in a zero-shot manner; we did not design task-specific prompts for RAT, but rather used the simplest possible prompts (which can be found in Appendix B  ###reference_###) to articulate questions or instructions for all methods. This approach ensures RAT’s generalization ability in open-ended scenarios.\nThe diversity of our evaluation was further enhanced by testing RAT across various language models of differing capacities. This included CodeLlama-7b (Rozière et al., 2023  ###reference_b44###), ChatGPT (gpt-3.5-turbo) (Ouyang et al., 2022  ###reference_b40###), and the more advanced GPT-4 (gpt-4) model (OpenAI, 2023  ###reference_b39###). Remarkably, RAT maintained its generalization capability across different scales of language models, showing improvements in benchmarks such as the HumanEval for code generation tasks. Notably, the largest improvement was observed with GPT-4, attributed to its superior ability for in-context learning from retrieved text. On MBPP+, CodeLlama-7b based RAT has demonstrated performance degradation. This decline could be due to the limited in-context learning ability of smaller language models.\nFor mathematical reasoning tasks, RAT demonstrated a significant relative improvement, with an overall average improvement of 18.44% when applied to the GPT-3.5 model. This trend of improvement persisted with GPT-4, which achieved a remarkable 10.26% relative improvement from DIRECT to RAT. These findings highlight RAT’s robustness and its effective enhancement of language models’ performance across a spectrum of computational and creative tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Recently, RAG has gained popularity for boosting the performance of LLMs by guiding their generation process using the retrieved knowledge (Zhao et al., 2023  ###reference_b64###). Without updating model parameters that may be expensive (Lewis et al., 2020a  ###reference_b27###) or unstable (Ke et al., 2022b  ###reference_b24###, a  ###reference_b23###), RAG is a cost-effective way for LLMs to interact with the external world (Gu et al., 2018  ###reference_b18###; Lewis et al., 2020a  ###reference_b27###). RAG is widely applied to downstream tasks, such as code generation (Zhou et al., 2022b  ###reference_b67###; Lu et al., 2022  ###reference_b36###; Nashid et al., 2023  ###reference_b37###), question answering (Baek et al., 2023  ###reference_b3###; Siriwardhana et al., 2023  ###reference_b46###), and creative writing (Wen et al., 2023  ###reference_b57###; Asai et al., 2023  ###reference_b1###).\nSome recent works also leverage reasoning to enhance the performance of RAG (Li et al., 2023b  ###reference_b30###). For example, IRCoT (Trivedi et al., 2022b  ###reference_b50###) exploits CoT to generate better queries for retrieval, IRGR (Ribeiro et al., 2022  ###reference_b43###)\nperforms iteratively retrieval to search for suitable\npremises for multi-hop QA, GEEK (Liu et al., 2023a  ###reference_b34###) can choose to query external knowledge or perform\na single logical reasoning step in long-horizon generation tasks, and ITRG (Feng et al., 2023a  ###reference_b14###) performs retrieval based on the last-step generation. However, these previous RAG methods simply adopt a single query to retrieve the knowledge for question-answering tasks (Gao et al., 2023  ###reference_b17###; Feng et al., 2023b  ###reference_b15###), while our proposed RAT performs retrieval using reasoning and draft answers in an autoregressive way, which significantly improves the performance of RAG in various tasks as demonstrated in Figure 2  ###reference_###.\nThe advancement of reasoning in language models has seen notable methodologies emerge since CoT was proposed by Wei et al. (2022  ###reference_b56###), which showcased LMs’ ability to generate self-derived problem-solving strategies. This foundational work spurred further innovations such as the least-to-most prompting (Zhou et al., 2022a  ###reference_b65###), zero-shot CoT (Kojima et al., 2022  ###reference_b26###), self-consistency (Wang et al., 2022  ###reference_b52###), zero-shot CoT without prompting (Wang and Zhou, 2024  ###reference_b51###).\nMoving beyond basic prompting, Creswell et al. (2022  ###reference_b12###) introduced the Selection-Inference framework, while Zelikman et al. (2022  ###reference_b62###) developed STaR to refine reasoning through model finetuning. Creswell and Shanahan (2022  ###reference_b11###) proposed a faithful reasoning model, segmenting reasoning into dedicated steps, similar to Scratchpad’s approach by Nye et al. (2021  ###reference_b38###) for enhancing multi-step computation. Tree-of-Thought (Yao et al., 2023  ###reference_b59###) and Graph-of-Thought (Besta et al., 2023  ###reference_b5###) also expand the reasoning paths into a complex structure instead of linear CoT.\nThese methods usually aim to improve the reasoning ability of LLM by designing prompts or providing feedback from the environment to assist in better planning and decision-making (Wang et al., 2023c  ###reference_b55###; Yao et al., 2022  ###reference_b58###; Shinn et al., 2023  ###reference_b45###; Li et al., 2023a  ###reference_b29###; Zhang et al., 2023  ###reference_b63###).\nHowever, RAT takes a different approach by using RAG to access external knowledge that can help LLM with its reasoning process."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have presented Retrieval Augmented Thoughts (RAT), a simple yet effective prompting strategy that synergies chain of thought (CoT) prompting and retrieval augmented generation (RAG) to address the challenging long-horizon reasoning and generation tasks. Our key ideas involve revising the zero-shot chain of thoughts produced by LLMs through RAG with the thoughts as queries, and causally revising the thoughts & generating the response progressively. RAT, a zero-shot prompting approach, has demonstrated significant advantages over vanilla CoT prompting, RAG, and other baselines on challenging code generation, mathematics reasoning, embodied task planning, and creative writing tasks."
        }
    ]
}