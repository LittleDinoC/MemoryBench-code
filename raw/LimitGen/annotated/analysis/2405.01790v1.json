{
    "title": "Understanding Position Bias Effects on Fairness in Social Multi-Document Summarization",
    "abstract": "Text summarization models have typically focused on optimizing aspects of quality such as fluency, relevance, and coherence, particularly in the context of news articles. However, summarization models are increasingly being used to summarize diverse sources of text, such as social media data, that encompass a wide demographic user base. It is thus crucial to assess not only the quality of the generated summaries, but also the extent to which they can fairly represent the opinions of diverse social groups. Position bias, a long-known issue in news summarization, has received limited attention in the context of social multi-document summarization. We deeply investigate this phenomenon by analyzing the effect of group ordering in input documents when summarizing tweets from three distinct linguistic communities: African-American English, Hispanic-aligned Language, and White-aligned Language. Our empirical analysis shows that although the textual quality of the summaries remains consistent regardless of the input document order, in terms of fairness, the results vary significantly depending on how the dialect groups are presented in the input data. Our results suggest that position bias manifests differently in social multi-document summarization, severely impacting the fairness of summarization models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As the use of natural language processing models gets more prevalent in various industries, academic and social settings, it is imperative that we assess not only the quality of these models but also their fairness when exposed to data originating from diverse social groups Czarnowska et al. (2021  ###reference_b5###).\nText summarization models, in particular, facilitate the processing of large collections of a wide variety of text data by distilling documents into short, concise, and informative summaries while preserving the most relevant points from the source document Nallapati et al. (2017  ###reference_b24###); Zhang et al. (2018  ###reference_b41###); Liu and Lapata (2019  ###reference_b21###). Multi-document summarization (MDS) is the task of generating a coherent summary from a set of input documents, usually centered around a topic, as opposed to single document summarization (SDS) which takes one document as input. The input in MDS consists of multiple documents, that may have been written by distinct users, varying in linguistic diversity, styles, or dialects.\nMDS can be of type extractive, where the models extract the salient points directly from the source document to form the summary, or of type abstractive where the models generate summaries by rewriting salient information using novel words or phrases. In both cases, the resulting summary should be of good quality in terms of informativeness, coherence and relevance to the source document. At the same time, a good summary should be unbiased and should reflect the diversity of thoughts and perspectives present in the source documents.\nThe notion of fairness describes equal or fair treatment without favoritism or discrimination. However, plenty of evidence suggests intrinsic societal biases in language models Bolukbasi et al. (2016  ###reference_b3###); Bommasani et al. (2021  ###reference_b4###); Deas et al. (2023  ###reference_b7###). More specific to the task of summarization, fairness is measured by the ability of algorithms to capture the peculiarity in all represented groups Shandilya et al. (2018  ###reference_b30###); Dash et al. (2019  ###reference_b6###); Keswani and Celis (2021  ###reference_b15###); Olabisi et al. (2022  ###reference_b25###); Ladhak et al. (2023  ###reference_b17###).\n###figure_1### Conventionally, the documents in MDS are simply concatenated into one large collection of text as the input for the model. Prior research supports the existence of position bias, or lead bias, where the models rely excessively on the position of the sentences in the input rather than their semantic information Lin and Hovy (1997  ###reference_b20###); Hong and Nenkova (2014  ###reference_b12###); Wang et al. (2019  ###reference_b33###). This is a particularly common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it can have a detrimental effect when important information is spread throughout the input.\nIn non-news domains, weak or no position bias has been observed Kedzie et al. (2018  ###reference_b14###); Kim et al. (2019  ###reference_b16###). Regardless of whether position bias is noted or not, previous investigations have quantified the effects of position bias mostly in terms of standard summarization metrics (e.g., ROUGE) which focus on the textual quality of the summary Sotudeh et al. (2022  ###reference_b32###); Scirè et al. (2023  ###reference_b28###). In this work, we investigate the effects of position bias on the fairness of the generated summaries.\nSpecifically, we ask two questions: (i) Do the system summaries show any position bias when we vary the order of the input documents? (ii) What is the impact of position bias on the fairness of the system summaries?\nFor our experiments we use DivSumm, a summarization dataset of linguistically diverse communities representing three dialect groups Olabisi et al. (2022  ###reference_b25###). We explore the effects of position bias in the outputs of seven abstractive summarization models (and three extractive models) and under two investigation setups: shuffled (when the data is presented as randomly shuffled) and ordered (when the input documents are grouped according to their dialects). Figure 1  ###reference_### presents a schematic overview. The generated summaries are evaluated in terms of fairness, as well as metrics related to the textual quality.\nThe contributions of our work are as follows:\nWe comprehensively investigate the phenomenon of position bias in the context of social multi-document summarization;\nWe explore ten different summarization models, both abstractive and extractive;\nWe contextualize and quantify the impact of position bias in terms of fairness and textual quality of generated summaries."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section we present some notable prior research in two relevant areas. First, we discuss position bias in summarization, followed by works studying fairness in summarization.\nPosition Bias in Summarization  Position bias can manifest in MDS scenarios just as it does in SDS scenarios because in MDS, the documents are typically concatenated into one long input and treated very much like a ‘single’ document. Several works have studied the substantial position bias (also known as lead bias), especially in the context of news summarization where the datasets and models prioritize selecting sentences from the beginning of an article Lin and Hovy (1997  ###reference_b20###); Hong and Nenkova (2014  ###reference_b12###); Wang et al. (2019  ###reference_b33###). Often the lead bias is so strong that the simple lead- baseline or using the first  sentences of a news article to generate the summary can score higher than many other models See et al. (2017  ###reference_b29###). While some have suggested approaches for mitigating or countering lead bias Grenander et al. (2019  ###reference_b10###); Xing et al. (2021  ###reference_b35###); Gong et al. (2022  ###reference_b9###); Zhang et al. (2022  ###reference_b39###), others have leveraged lead bias Yang et al. (2020  ###reference_b36###); Zhu et al. (2020  ###reference_b43###); Padmakumar and He (2021  ###reference_b26###).\nInterestingly, although position bias dominates the learning signal for news summarization or similar domains, it is less apparent in other domains where most non-news datasets show weak or no position bias Kedzie et al. (2018  ###reference_b14###); Jung et al. (2019  ###reference_b13###); Kim et al. (2019  ###reference_b16###); Sharma et al. (2019  ###reference_b31###); Sotudeh et al. (2022  ###reference_b32###); Scirè et al. (2023  ###reference_b28###). Notably, none of these studies consider datasets where data originates from diverse social groups, which is the focus of our work.\nMoreover, prior research studying the effect of position bias has quantified its impact exclusively in terms of textual quality, typically measured in terms of summarization metrics such as ROUGE, and others. To our knowledge, ours is the first work quantifying the impact of position bias in multidocument summarization in terms of fairness where data originates from diverse social groups.\nFairness in Summarization  A significant amount of work has been done toward improving the textual quality of summaries but not so much in terms of enhancing the fairness of summaries, particularly in the context of diverse groups. Prior text summarization work has proposed fairness-preserving algorithms Shandilya et al. (2018  ###reference_b30###); Dash et al. (2019  ###reference_b6###), bias mitigation models Keswani and Celis (2021  ###reference_b15###) and fairness interventions for extractive and abstractive summarization Olabisi et al. (2022  ###reference_b25###). Furthermore, Ladhak et al. (2023  ###reference_b17###) observed that name-nationality stereotypes propagate from pretraining data to downstream summarization systems and manifest as hallucinated facts."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Considering the extensive literature on fairness in natural language processing, which highlights significant disparities in the processing of data from different social groups, whether along the dimensions of gender or race or others, we are compelled to ask two questions:\nWhat happens when the input data to be summarized is deliberately grouped according to the social groups, such as dialect groups in our case? (in Section 4  ###reference_###) and,\nHow do the effects of position bias affect the fairness of generated summaries (Section 5  ###reference_###).\nBefore exploring these questions, we first describe our experimental setup in this section."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Formulation",
            "text": "Considering a multi-document set of  topically-related documents , where each document belongs to one of several diverse social groups , the objective is to produce a summary  that ideally exhibits both high textual quality and fairness. In this work, because of the original dataset design where the number of documents from each group is equal in the input, our investigation is concerned with the notion of equal representation. As such, a summary is considered to be fair when all groups  are equally represented in the output."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "For our experiments, we use the DivSumm dataset111https://github.com/PortNLP/DivSumm  ###reference_###, an MDS dataset consisting of English tweets of three diverse dialects (African-American English, Hispanic-aligned Language, and White-aligned Language) Olabisi et al. (2022  ###reference_b25###), which was developed using a large corpus of tweets originally collected by Blodgett et al. (2016  ###reference_b2###). The dataset includes 25 topically-related sets of documents (tweets) as input and corresponding human-written extractive and abstractive summaries. Each set  consists of 90 documents evenly distributed among the three dialects (i.e., 30 documents per dialect). A selection of dialect diverse tweets from DivSumm is presented in Table 3  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Shuffled and Ordered",
            "text": "To study the phenomenon of position bias in social multi-document summarization where documents originate from different social groups, we devise two distinct scenarios: shuffled and ordered, as depicted in Figure 1  ###reference_###.\nIn the shuffled setting, documents appear randomly present in the input in no specific order. In fact, to ensure consistency, we retain the original order as presented in the DivSumm dataset which the annotators used to craft the summaries.\nIn the ordered setting, we perturb the input data by grouping documents from each social group together. When the subset of White-aligned Language tweets () appears first, the input set is denoted as orderedwhite or, simply, . Similarly, when the subset of African-American English tweets () come first, we denote that set as , and when the subset of Hispanic-aligned Language documents () appears first, we denote that set as . Specifically, the input documents are ordered as follows:\nThese documents are summarized using several models described in the next section, allowing us to subsequently investigate the different summaries we generate – , , , and (shuffled) – which are obtained from four distinct sets of input documents – , , , and shuffled, respectively."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Summarization Models",
            "text": "We study a total of seven abstractive models in our experiments. We also study three extractive models, the details and results of which are discussed in A  ###reference_###. Following the setup of DivSumm, we generate summaries of 5 sentences per topic\nThe seven abstractive models included in our experiments are as follows:\nBart222Model checkpoints for BART, T5, LED, Pegasus, and Primera were accessed from https://huggingface.co/models  ###reference_huggingface.co/models###. Lewis et al. (2019  ###reference_b18###),\nT5 Raffel et al. (2019  ###reference_b27###),\nLED (Longformer Encoder-Decoder) Beltagy et al. (2020  ###reference_b1###),\nPegasus Zhang et al. (2020  ###reference_b38###),\nGPT-3.5,\nPrimera Xiao et al. (2021  ###reference_b34###), and\nClaude (Claude 3 Opus).\nGPT-3.5 and Claude were prompted with the following prompt – “Please summarize the following texts in only five sentences”."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Position Bias in Social MDS",
            "text": "###figure_2### This section discusses position bias within three types of summaries: human-authored reference summaries of the DivSumm dataset, system summaries generated using the shuffled input, and system summaries generated using ordered inputs. Following prior work on position bias, we calculate the overlap between the summaries and the input documents by computing the number of tokens shared between the summary and each document of the MDS topic set. That is, given the 90 documents in each topically-related input set, we get the overlap score for each document () with respect to a summary, and report the average score over the entire dataset. A higher overlap score implies more semantic relationship between the summary and source document.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Position Bias in Human-Written Reference Summaries",
            "text": "To examine position bias in the summaries created by humans, we analyze both abstractive and extractive reference summaries of DivSumm dataset. Because the dataset contains two reference summaries per input, we report the average score. The results are presented in Figure 2  ###reference_### where no noticeable position bias is observed, and it is encouraging to note that the annotators were not influenced by the position of the documents in the input when producing their summaries."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Position Bias in System Summaries (Shuffled)",
            "text": "###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### The results of position bias within model-generated summaries using shuffled inputs are presented in Figure 3  ###reference_###. Similar to the human-written reference summaries, we observe no notable position bias suggesting that when summarizing randomly shuffled data from various social groups, the models also do not exhibit any particular lead bias. This observation on DivSumm, a dataset of tweets, is consistent with trends observed in other social datasets (Reddit posts Kim et al. (2019  ###reference_b16###) and social user posts Sotudeh et al. (2022  ###reference_b32###))."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Fairness and Textual Quality Amidst Position Bias",
            "text": "Having observed an instance of position bias, especially when input data is grouped according to dialect groups, the next natural question to ask is how does this position bias quantitatively impact the fairness and textual quality of the generated summaries. We briefly describe the evaluation metrics before discussing the main results."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Evaluation Metrics",
            "text": "Fairness (Gap):  One way of measuring fairness is by estimating the amount of representation from each dialect group in the final summary by comparing the summary  to the set of documents from each group. Given that an unbiased summary should capture the perspectives across all groups, we evaluate summary fairness for both extractive and abstractive models using semantic similarity of the summary to each represented group. As an example, for input , we compare the final summary () to the document set of each dialect group: , , and . In other words, we compute  where  and . Similarity can be estimated by many possible methods of obtaining semantic similarity. We use cosine similarity.\nFrom these similarity scores, we can derive the Fairness Gap (Fair) by calculating the difference between the maximum and the minimum scores attributed to any of the groups Olabisi et al. (2022  ###reference_b25###). Intuitively, a summary that produces relatively similar representation scores across all groups can be considered as fair because it likely contains comparable representation from all groups such that no one group is significantly underrepresented.\nTextual Quality:  Four established metrics are used for assessing the quality of the summaries: ROUGE, BARTScore, BERTScore, and UniEval. ROUGE Lin (2004  ###reference_b19###) calculates the lexical overlap between the model-generated summary and the reference summaries. For our experiments, we report the F1 scores of ROUGE-L which is the longest common subsequence between the two summaries. BARTScore Yuan et al. (2021  ###reference_b37###) leverages BART’s average log-likelihood of generating the evaluated summary conditional on the source document. Since it uses the average log-likelihood for target tokens, the calculated scores are smaller than 0 (negative). We use the facebook/bart-large-cnn checkpoint. BERTScore Zhang* et al. (2020  ###reference_b40###) relies on BERT embeddings and matches words in system-generated summaries and reference summaries to compute token similarity. We use the microsoft/deberta-xlarge-mnli model and report the F1 scores. UniEval Zhong et al. (2022  ###reference_b42###) is a unified multi-dimensional evaluator that employs boolean question answering format to evaluate text generation tasks. We make use of unieval-sum which evaluates system-generated summaries in terms of four dimensions: coherence, consistency, relevance and fluency. Except for fluency, the rest are reference-free metrics. We report the overall score."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "###figure_11### ###figure_12### ###figure_13### ###figure_14### Evaluating fairness. The results in Table 1  ###reference_### report the fairness scores for all seven models. We clearly observe that ordering the input documents based on groups certainly favors the group that appears first. This phenomenon is consistently observed in all three types of ordered sets, regardless of which particular dialect group’s data is presented first. However, when the documents are presented as shuffled, no single group is over-represented and the summaries appear more balanced (Fair = 0.04).\nThe density plots in Figure 5  ###reference_### also show that the shuffled input set is the most balanced across all groups, unlike the ordered sets which are significantly skewed. Furthermore, amongst ordered documents, the fairness gap is the largest when documents of White-aligned language are passed first (Fair = 0.14), and the smallest when documents of African-American English appear first (Fair = 0.09).\nEvaluating textual quality.  Table 2  ###reference_### presents the summary quality scores across all seven summarization models for the four sets of input. We clearly see that the scores of the shuffled approach are superior or comparable to the scores from the three input sets in the ordered approach, except in the case of UniEval. This shows that with respect to quality, there is no significant difference whether documents are presented as ordered or shuffled."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Discussion",
            "text": "Some samples of system summaries are presented in Table 3  ###reference_###. The key findings of our study can be summarized as follows:\nWe find no evidence of position bias in human-annotated reference summaries of DivSumm, a social MDS dataset of diverse groups. Same observation is made for the abstractive system-generated summaries obtained when the input documents are passed in randomly or shuffled.\nHowever, when the input is ordered based on dialect groups, we observe a significant position bias in the system summaries, with the summaries having higher overlap with the group that appears first in the input document.\nOrdered documents involving different dialects result in summaries that are significantly skewed in terms of fairness, with the group whose data appears first is clearly favored by the models. In contrast, shuffled documents show the least amount of fairness gap.\nIn terms of quality, we observe that for all models and metrics, the scores for ordered and shuffled remain comparable, suggesting that ordering based on diverse groups has no noticeable effect on the quality of system-generated summaries.\nTaken together, the findings of our study indicate that both the ordered and shuffled approaches yield comparable results in terms of textual quality, but highly disparate results in terms of fairness. This phenomenon is consistently observed in all abstractive models, suggesting that the models are not robust to fairly straightforward group-level data perturbations. These findings are important because they highlight a potential source of nuanced bias in the summarization models. The observation that ordering the input documents based on groups favors the group that appears first indicates a systematic bias in the models’ behavior. The fact that the shuffled input set leads to more balanced summaries across all groups implies that the bias observed in the ordered sets can be mitigated by introducing randomness in the presentation of input data. This insight is crucial for understanding and addressing bias in summarization systems, especially in scenarios where fairness and equity are important considerations, such as in social data analysis or decision-making processes. Overall, this result sheds light on an important aspect of model behavior and informs strategies for improving the fairness and effectiveness of summarization models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we investigate how position bias manifests in social multi-document summarization, specifically in scenarios where the input data is derived from three linguistically diverse communities. When presented with randomly shuffled input data, summaries generated by ten distinct summarization models exhibited no signs of position bias. However, a significant shift occurred when the input data was simply reordered based on social groups. In such instances, the models produced biased summaries, primarily favoring the social group that appeared earlier in the input sequence. In terms of the quality of generated summaries, however, there was no notable difference due to the order in which source documents were presented, whether shuffled or ordered. Our results suggest that position bias manifests differently in the context of social multi-document summarization. Furthermore, they highlight the need to incorporate randomized shuffling in multi-document summarization datasets particularly when summarizing documents from diverse groups to ensure that the resultant summaries are not only of high quality but also faithfully representative of the diversity present in the input data."
        }
    ]
}