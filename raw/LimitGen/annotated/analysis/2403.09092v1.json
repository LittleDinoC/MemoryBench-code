{
    "title": "MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection",
    "abstract": "The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide. In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "It has been prevalent for the public to consume news through various online sources, such as social platforms and news websites. Such sources are efficient media for spreading fake news. For instance, the latest Weibo annual report on fake news (Weibo, 2023  ###reference_b39###) revealed that Weibo’s official fact-checking agency identified 82,274 pieces of fake news in 2022. Given the devastating consequences of fake news on both individuals and society, fake news detection has become an urgent and essential task that needs to be addressed (Aïmeur et al., 2023  ###reference_b2###; Hu et al., 2022  ###reference_b10###; Nan et al., 2021  ###reference_b19###; Wang et al., 2018  ###reference_b37###; Sheng et al., 2021a  ###reference_b28###, b  ###reference_b29###, 2022  ###reference_b27###). To this end, Chinese fake news detection datasets have been constructed for the development of Chinese fake news detection (Hu et al., 2022  ###reference_b10###; Jin et al., 2017a  ###reference_b12###, b  ###reference_b13###; Zhang et al., 2021  ###reference_b42###; Nan et al., 2021  ###reference_b19###; Yang et al., 2021  ###reference_b41###; Hu et al., 2023  ###reference_b11###).\n###figure_1### The existing Chinese fake news detection datasets are limited to one single source, Weibo, where both true and fake news are collected. However, in the real world, news emerges from multiple sources, such as social platforms, messaging apps, and traditional online news outlets, etc. (Hu et al., 2022  ###reference_b10###; Pierri and Ceri, 2019  ###reference_b21###; Bondielli and Marcelloni, 2019  ###reference_b4###; Li et al., 2023a  ###reference_b14###, b  ###reference_b15###). News, in particular, fake news, from different sources is characterized by diverse dimensions, such as news content, topics, publishing methods, and the utilization of sophisticated linguistic styles intended to mimic real news (Shu et al., 2021  ###reference_b32###; Wang et al., 2020  ###reference_b38###; Pathak and Srihari, 2019  ###reference_b20###; Przybyla, 2020  ###reference_b22###; Aïmeur et al., 2023  ###reference_b2###). For example, Fig. 1  ###reference_### shows four instances of fake news, each sourced from a distinct news source, exemplifying different news characteristics. Existing Weibo-based Chinese fake news detection datasets that fail to capture the above data diversity can impede the effectiveness of machine learning (ML) based fake news detection in practice, including but not limited to the robustness to intricately crafted fake news and the generalization to fake news from other sources (Wang et al., 2023  ###reference_b35###; Przybyla, 2020  ###reference_b22###; Aïmeur et al., 2023  ###reference_b2###; Hu et al., 2022  ###reference_b10###; Nan et al., 2021  ###reference_b19###).\nPilot Experiment. To verify such limitations, we conducted evaluations on 817 pieces of fake news we collected. They were verified between Jan. 2015 and Mar. 2023, from the China Internet Joint Rumor Refuting Platform111https://www.piyao.org.cn/  ###reference_www.piyao.org.cn/###, a government-backed fact-checking agency supported by authoritative experts and various government departments. The agency covers fake news originating from a wide variety of sources, including but not limited to, Douyin, Wechat, TouTiao, Zhihu, Weibo, etc.222\nThe websites for the mentioned news sources are as follows:\nDouyin: https://www.douyin.com/  ###reference_www.douyin.com/###;\nWechat: https://www.wechat.com/  ###reference_www.wechat.com/###;\nTouTiao: https://www.toutiao.com/  ###reference_www.toutiao.com/###;\nZhihu: https://www.zhihu.com/  ###reference_www.zhihu.com/###; and Weibo: https://m.weibo.cn/  ###reference_m.weibo.cn/###.\n\nWe trained the state-of-the-art fake news detection model BERT-EMO (Zhang et al., 2021  ###reference_b42###) on the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###).\nThe model demonstrated strong performance with F1 scores of 0.943 on the Weibo-21, 0.932 on the Weibo-20 (Zhang et al., 2021  ###reference_b42###) dataset, and 0.908 on the Weibo-16 dataset (Jin et al., 2017a  ###reference_b12###), respectively.\nNevertheless, when we used the same model to detect fake news collected from all the diverse sources on the platform, failed to identify 35.34% of fake news. The macro F1 score dropped to 0.470, a decline of 52.03%.\nThe following may account for such experimental results.\nFirst, during the training phase, ML models lack exposure to a diverse range of data from various sources, which leads to an overfitting of specific characteristics of Weibo fake news. This limits their capability to generalize and effectively identify fake news from different sources.\nFurther, during the testing phase, these models are evaluated using Weibo data exclusively, overlooking a comprehensive assessment of their performance across different news sources.\nWhen faced with real-world fake news that emerges from multiple sources, the applicability of models trained and tested on existing datasets is questionable. Our analytical results validate the limitations of current Chinese fake news detection datasets. Therefore, it is imperative to construct a comprehensive dataset that consists of (real and fake) from diverse sources.\nTo bridge this gap, we constructed the first Multi-source benchmark dataset for Chinese FakE News Detection (MCFEND), which contains 23,974 pieces of authoritatively verified Chinese news from 14 fact-checking agencies covering numerous news sources.333The Weibo Community Management Center, Weibo’s official fact-checking agency, exclusively examines news sourced from Weibo, whereas other fact-checking agencies cover various news sources. Note that the inclusion of additional fact-checking agencies can potentially enhance the variety of news sources considered. Please refer to Table 2  ###reference_### for the full list of included fact-checking agencies. These fact-checking agencies are further divided into three distinct groups. The first group encompasses nine Chinese fact-checking agencies. That means the collected news has been verified by experts as active and authoritative. The second group corresponds to three existing annotated English fake news detection datasets. Specifically, for an English news piece paired with its corresponding authenticity label from an existing English fake news dataset, we employ the state-of-the-art cross-lingual identical news retrieval system to collect its Chinese equivalent while retaining its original label. The third group consists of Weibo’s official fact-checking agency exclusively, i.e., the Weibo Community Management Center. For this group of news, we utilized news data from the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###). Furthermore, we conducted comprehensive evaluations on six established baseline models for Chinese fake news detection, including state-of-the-art methods, under cross-source and multi-source scenarios on our MCFEND dataset. The experimental results characterize the challenge of accurately spotting fake news from different sources that the dataset presents.\nOur contributions are summarized as follows:\nFirst, We constructed the initial multi-source Chinese fake news detection dataset MCFEND, which comprises multi-modal content and social context of 23,974 real-world Chinese news pieces collected from 14 authoritative fact-checking agencies in three distinct groups. Additionally, to the best of our knowledge, MCFEND is the largest open-sourced Chinese fake news detection dataset, being at least 2.63 times larger than any existing ones. The dataset aims to benchmark the evaluations of Chinese fake news detection methods in real-world scenarios, where news originates from diverse sources, and to encourage further research in this field.\nSecond, we conducted comprehensive cross-source, multi-source, and unseen source evaluations on six established baseline models for Chinese fake news detection, including state-of-the-art methods. Our experimental results reveal that the models trained on existing datasets are not applicable in real-world scenarios. Incorporating multi-source data is necessary, which can enhance the models’ robustness substantially."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries and Related work",
            "text": "Fake news detection, also referred to as false news detection, and related to information credibility evaluation, is commonly defined as a binary classification task (Hu et al., 2022  ###reference_b10###; Przybyla, 2020  ###reference_b22###; Pierri and Ceri, 2019  ###reference_b21###; Bondielli and Marcelloni, 2019  ###reference_b4###).\nIn this context, we define the output space as  to classify news items as real () or fake (). The input space  includes multidimensional data, comprising both the content of the news and its social context.\nLet  represent a collection of  news pieces, each associated with a label indicating its authenticity. Here,  and .\nThe goal is to design fake news detection models capable of learning a function , such that for any given news item ,  predicts its label  with high accuracy.\nNumerous datasets have been constructed to address fake news detection. Representative English fake news detection datasets, such as BuzzFace (Santia and Williams, 2018  ###reference_b26###), LIAR (Wang, 2017  ###reference_b36###), FakeNewsNet (Shu et al., 2020  ###reference_b31###), PHEME (Zubiaga et al., 2017  ###reference_b44###), KaggleFakeNews (Risdal, 2016  ###reference_b25###), FakeNewsCorpus (Pathak and Srihari, 2019  ###reference_b20###), and FakeHealth (Dai et al., 2020  ###reference_b7###), were constructed on English news from social platforms like Twitter and Facebook, as well as fact-checking websites such as BuzzFeed, PolitiFact, and NewsGuard.444The websites for these social platforms are as follows: Twitter: https://www.twitter.com/  ###reference_www.twitter.com/###; Facebook: https://www.facebook.com/  ###reference_www.facebook.com/###; BuzzFeed: https://www.buzzfeed.com/  ###reference_www.buzzfeed.com/###; PolitiFact: https://www.politifact.com/  ###reference_www.politifact.com/###; and NewsGuard: https://www.newsguardtech.com  ###reference_www.newsguardtech.com###. A few Chinese fake news detection datasets have also been proposed. For instance, Ma et al. introduced the Weibo-16 dataset (Jin et al., 2017a  ###reference_b12###), collected from the Chinese social platform Weibo. This dataset contains verified fake news sourced from the Weibo Community Management Center555https://service.account.weibo.com  ###reference_service.account.weibo.com###, an official fact-checking agency for posts on Weibo. Real news was collected from regular posts that were not categorized as fake. While Weibo-16 focuses exclusively on textual data, Jin et al. (Jin et al., 2017b  ###reference_b13###) later introduced Media-Weibo, the first multi-modal dataset for detecting Chinese fake news. Media-Weibo includes textual content, user profiles, and supplementary images for each post. Zhang et al. (Zhang et al., 2021  ###reference_b42###) then extended Media-Weibo dataset to Weibo-20 dataset by adding 850 real news pieces authenticated by NewsVerify666https://www.newsverify.com/  ###reference_www.newsverify.com/###, a fact-checking website dedicated to verifying posts on Weibo, from April 2014 to November 2018, and 1,806 fake news pieces that were officially verified by the Weibo Community Management Center within the same timeframe. Using a similar approach, Yang et al. (Yang et al., 2021  ###reference_b41###) constructed CHECKED dataset, aiming at detecting COVID-19-related fake news on Weibo. Additionally, Nan et al. (Nan et al., 2021  ###reference_b19###) proposed the Weibo-21 dataset, the first multi-domain Chinese fake news detection dataset. Weibo-21 contains both fake and real news pieces collected from Weibo spanning from December 2014 to March 2021, covering nine different domains, such as Science, Military, and Education. Most recently, Hu et al. (Hu et al., 2023  ###reference_b11###) constructed the multi-modal retrieval augmented dataset MR2. This dataset consists of two subsets from Weibo and Twitter, respectively, covering news with images and texts, and provides evidence retrieved from the Internet for both modalities.\nExisting datasets for Chinese fake news detection rely heavily on Weibo. Different from them, we constructed the pioneering multi-source Chinese fake news detection dataset, termed MCFEND, which contains 23,974 real-world Chinese news pieces collected from multiple sources across three distinct categories. Table 1  ###reference_### compares the Chinese fake news detection datasets."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. MCFEND Dataset",
            "text": "In this section, we introduce our Chinese multi-source fake news detection dataset, MCFEND. Additionally, we perform data analysis to investigate into the differences between various sources."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Overview",
            "text": "The MCFEND dataset contains news verified by 14 fact-checking agencies from a wide range of sources, such as messaging apps, social platforms, and traditional news outlets. As mentioned above, these agencies are categorized into three groups (see Table 2  ###reference_###. The websites for the 14 fact-checking agencies are detailed in Appendix A  ###reference_###.\nThe first group includes nine Chinese fact-checking agencies identified as active by Duke Reporters777https://reporterslab.org/fact-checking/  ###reference_###, along with four Chinese fact-checking agencies manually verified by experts as active and authoritative.\nThe second group corresponds to four English fact-checking agencies, including Politifact, Gossipcop, BS Detector, and FakeNewsCorpus. This group contains the Chinese counterparts of the English news fact-checked by the aforementioned agencies, which are collected through a carefully designed cross-lingual identical news retrieval method. An in-depth description of the method can be found in Sec. 3.2.2  ###reference_.SSS2###.\nGroup 3 exclusively covers the Weibo Community Management Center, from which news data was directly sourced from Weibo-21 dataset (Nan et al., 2021  ###reference_b19###).\nThe overall MCFEND dataset contains 23,974 news pieces, including 8,144 sourced from the nine fact-checking agencies in Group 1, 6,702 related to the four English fact-checking agencies in Group 2, and 9,128 obtained from the Weibo Community Management Center in Group 3.\nSimilar to existing dataset construction (Shu et al., 2020  ###reference_b31###; Nan et al., 2021  ###reference_b19###), we collected the following information for each piece of news in any group:\n(1) Multi-modal news content, including text, images, and metadata, e.g., timestamps;\n(2) Multi-modal social context, including posts, comments, emojis, user profiles, and other metadata, e.g., like counts of comments. Table 3  ###reference_### presents the detailed statistics of the MCFEND dataset."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Dataset Construction",
            "text": "In this subsection, we present the process of constructing the dataset for each group of the fact-checking agencies. Fig. 2  ###reference_### illustrates the entire process for the dataset construction.\n###figure_2###"
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Group 1: Fact-checking Agencies Data Crawling",
            "text": "Fact-checking agencies serve as a common source for labeling fake news detection datasets (Przybyla, 2020  ###reference_b22###; Hu et al., 2022  ###reference_b10###). These agencies are typically operated by government entities, companies, or non-profit organizations, and they employ authoritative experts to assess the authenticity of news pieces originating from diverse sources, such as social platforms, messaging apps, and traditional online news outlets. As discussed in Sec. 1  ###reference_###, including a wider range of fact-checking agencies enhance the diversity of news sources in our dataset.\nTo maximize our coverage of news sources, we conducted web crawling to collect data from all active Chinese fact-checking agencies, encompassing the five Chinese fact-checking agencies identified as active by Duke Reporters, in addition to nine other Chinese fact-checking agencies that were manually verified as active and authoritative. In the case where the labels on some fact-checking agencies, e.g., AFP Fact Check Asia and Factcheck Lab, are presented in the form of images, we utilized an optical character recognition method called Tesseract-OCR to retrieve such labels.888https://github.com/tesseract-ocr/tesseract  ###reference_###"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Group 2: Cross-lingual Identical News Retrieval",
            "text": "To further diversify our news sources, we employ a cross-lingual identical news retrieval method to obtain the corresponding Chinese equivalents of both real and fake English news within several well-known datasets, including FakeNewsNet (Shu et al., 2020  ###reference_b31###), KaggleFakeNews (Risdal, 2016  ###reference_b25###), and FakeNewsCorpus (Pathak and Srihari, 2019  ###reference_b20###). FakeNewsNet consists of two detailed sub-collections sourced from distinct fact-checking organizations, specifically Politifact and Gossipcop. KaggleFakeNews contains news gathered from 244 sources classified as “unreliable or otherwise questionable” by the BS Detector, a browser extension that assesses the reliability of websites by comparing them to a professionally curated list.\nFakeNewsCorpus is a dataset consisting of news related to the 2016 US elections. The news pieces in this dataset are manually annotated by its authors.\nWe consider the BS Detector and the authors of the FakeNewsCorpus as two distinct fact-checking agencies. By incorporating these three datasets, we effectively introduce data from four additional fact-checking agencies, enabling us to collect news from a wider range of English news sources.\nFor each news piece in these datasets, we executed the following steps to identify its corresponding Chinese counterpart:\nStep 1: Translation. We utilized the Baidu Translation API to translate the headlines of the English news into Chinese.999http://fanyi-api.baidu.com/  ###reference_anyi-api.baidu.com/###\nStep 2: Chinese News Retrieval with Google News.101010https://news.google.com/  ###reference_news.google.com/### Google News provides extensive and up-to-date news coverage from sources worldwide. We configured the language and region of interest as “Chinese (China)” and employed the translated Chinese news headline as the search query. Search engines typically sort results by relevance. We assumed that the top five returned news pieces were the most relevant Chinese counterparts to the original English news. Subsequently, we crawled the top five returned news pieces.\nStep 3: Cross-lingual News Similarity Calculation. To determine the degree of similarity between the Chinese news retrieved in the previous step and the original English news, we employed the state-of-the-art cross-lingual news similarity calculation system (Xu et al., 2022  ###reference_b40###), which ranked 1st in the SemEval2022 Task 8 challenge (Chen et al., 2022b  ###reference_b5###) with a Pearson correlation coefficient of 0.818 on the official evaluation set. Specifically, we calculated the similarity score between the retrieved Chinese news and the original English news. The Chinese news with the highest similarity score was preserved in our MCFEND dataset. The strong performance of the system effectively ensures the consistency of misleading content across different languages.\nStep 4: Label Assignment. The authenticity label of the original English news is used to label its Chinese counterpart, that is, the Chinese news with the highest similarity score.\nThe method inherently retains human-written news content. In contrast to directly utilizing Chinese news content generated by machine translator, our approach avoids unnatural textual expressions that could potentially introduce noise to the models."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3. Group 3: Weibo News Collection",
            "text": "Group 3 consists solely of news sourced from the Weibo. For this group, we directly utilized news data in the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###), the largest Chinese fake news detection dataset on Weibo."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4. Social Context Collection",
            "text": "Relying solely on news content may be inadequate for detecting fake news, as fake news content is often meticulously crafted to deceive the public. Social platforms offer an invaluable source of supplementary information in the form of social context features (Shu et al., 2020  ###reference_b31###; Hu et al., 2022  ###reference_b10###; Tai et al., 2015  ###reference_b33###; Ma et al., 2018  ###reference_b18###; Shu et al., 2019  ###reference_b30###; Ma and Gao, 2020  ###reference_b17###; Zhang et al., 2021  ###reference_b42###), capturing user interactions and social behaviors within the social platform environment. Thus, to incorporate such important features, we collected social context data, such as posts, comments, user profiles, etc., on the largest social platform in China, Weibo.111111While Weibo serves as the social context source for all collected news pieces, it also acts as an independent news source.\nThe process of collecting social context aligns closely with the approach detailed in (Shu et al., 2020  ###reference_b31###) for gathering social context from Twitter. Firstly, for news pieces that have headlines, we created search queries for associated posts on Weibo using the headlines. For news pieces without headlines, we utilized the Jieba tool to tokenize the textual content of the news and extract the top five keywords, which were then used as search queries.121212https://github.com/fxsjy/jieba  ###reference_github.com/fxsjy/jieba### During this process, we removed special characters from the search queries to eliminate unnecessary noise. All matching posts were considered relevant and included. We then retrieved user responses to these posts, including comments, reposts, and likes. Additionally, upon identifying all users involved in the news propagation process, we collected metadata for these users, such as their usernames and profiles. As shown in Table 3  ###reference_###, we assembled a comprehensive set of relevant social context data, which includes 170,713 posts and 2,102,902 comments from 803,779 distinct users.131313Individual users may engage in the social context of news collected from fact-checking agencies across different groups."
        },
        {
            "section_id": "3.2.5",
            "parent_section_id": "3.2",
            "section_name": "3.2.5. Post-collection Processing",
            "text": "After collecting all news pieces and their corresponding social context, we conducted three post-collection processing steps:\nStep 1: Text Cleaning. To enhance data quality and eliminate unnecessary noise, we conducted text cleaning on text within both news content and social context. This cleaning process involved removing HTML tags, punctuation, white spaces, stop words, and prefix headings.\nStep 2: Deduplication. The raw data contained multiple duplications. As a result, we removed redundant news and social context data to avoid unnecessary repetitions.\nStep 3: Label Mapping. Different fact-checking agencies employ diverse fine-grained labels to express degrees of authenticity (e.g., true, mostly true, and inconclusive). To ensure consistency, we designed a label mapping strategy to standardize the original labels.\nPlease refer to Appendix B  ###reference_### for details of our label mapping strategy."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Experiments",
            "text": "We conducted experiments to evaluate the performance of representative fake news detection methods on our newly proposed MCFEND dataset. Specifically, we aim to answer the following evaluation questions (EQs):\n: Are existing methods, which have demonstrated effectiveness on the existing Weibo datasets, capable of maintaining their performance when being applied to news collected from different sources?\n: Can training with multi-source data enhance the robustness of existing methods in detecting fake news in real-world scenarios, which involve multiple sources?\n: Can training with multi-source data enhance the robustness of existing methods in detecting fake news originating from previously unseen news sources?\nThe detailed experimental setups are provided in Appendix C  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Baselines",
            "text": "To address the above EQs, we carefully selected six baseline models spanning the two widely recognized categories of fake news detection approaches: content-based and social context-based methods (Hu et al., 2022  ###reference_b10###, 2023  ###reference_b11###; Aïmeur et al., 2023  ###reference_b2###; Zhu et al., 2024  ###reference_b43###), and constructed the baseline benchmarks. Their implementation details are as follows."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Content-based Methods",
            "text": "Content-based methods rely on solely the textual or visual contents of the news. We adopted two representative types of content-based models, uni-modal models and multi-modal models.\nUni-modal models focus on the textual content of the news. We used BERT (Devlin et al., 2019  ###reference_b8###) and RoBERTa (Liu et al., 2019  ###reference_b16###) as contextualized encoders to encode the textual content.\nThen, the representation of the “[CLS]” special token is used for prediction. The implementation of BERT and RoBERTa in this study is based on their respective Chinese base versions, i.e., BERT-base-Chinese151515https://huggingface.co/bert-base-chinese  ###reference_### and RoBERTa-wwm-Base.161616https://huggingface.co/hfl/chinese-roberta-wwm-ext  ###reference_-wwm-ext###\nMulti-modal models encode both text and images in the input news. We consider two multi-modal baselines: CLIP (Radford et al., 2021  ###reference_b23###) and CAFE (Chen et al., 2022a  ###reference_b6###).\nCLIP (Radford et al., 2021  ###reference_b23###) is a pretrained model for images and text. We input the image and text of the news into CLIP to obtain a joint representation of the visual and textual content.\nThis joint representation is then utilized for predictions.\nWe implement CLIP based on its Chinese version.171717https://github.com/OFA-Sys/Chinese-CLIP  ###reference_### CAFE (Chen et al., 2022a  ###reference_b6###) is an ambiguity-aware fake news detection method.\nSpecifically, it integrates uni-modal features produced by BERT (Devlin et al., 2019  ###reference_b8###) for text and ResNet (He et al., 2016  ###reference_b9###) for images, along with cross-modal correlations.\nIt relies on uni-modal features when cross-modal ambiguity is weak and relies on cross-modal correlations when cross-modal ambiguity is strong.\nCAFE demonstrates superior fake news detection performance on the Twitter (Boididou et al., 2018  ###reference_b3###) and Weibo-16 (Jin et al., 2017a  ###reference_b12###) datasets, respectively.\nIt stands as the state-of-the-art content-based approach to this task."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Social Context-based Methods",
            "text": "Social context-based methods are typically categorized into three groups: tree-based, modal fusion-based, and graph-based (Hu et al., 2023  ###reference_b11###). Since our MCFEND dataset contains news from diverse sources and lacks the necessary cross-source user/news interactions to build effective graphs and trees, we included only the modal fusion-based models, excluding the graph-based and tree-based models.\nModal fusion-based models integrate information from both news content and social context. In our study, we considered two representative baseline models for this category: dEFEND (Shu et al., 2019  ###reference_b30###) and BERT-EMO (Zhang et al., 2021  ###reference_b42###).\nThe dEFEND model (Shu et al., 2019  ###reference_b30###) utilizes a sentence-comment co-attention sub-network to exploit news contents and comments in the social context to jointly capture explainable top-k check-worthy sentences and comments for fake news detection.\nOn the other hand, BERT-EMO (Zhang et al., 2021  ###reference_b42###) enhances a BERT-based fake news detector by incorporating dual emotion features that represent both the emotions and the relationship between news and comments within the social context.\nNote that the BERT-EMO model has demonstrated outstanding performance in fake news detection, achieving the highest reported performance on the Weibo-20 dataset (Zhang et al., 2021  ###reference_b42###). Besides, our preliminary experiments on the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###) have also shown that BERT-EMO achieved an impressive F1-score of 0.943, outperforming all other methods.\nThe results show that BERT-EMO as the state-of-the-art social context-based approach in the fake news detection task."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Cross-source Evaluation",
            "text": "To address EQ1, we performed cross-source evaluations on the baseline models.\nSpecifically, we evaluated the performance of baseline models trained exclusively on Weibo data, focusing on their application to diverse news sources.\nOur findings, shown in Table 4  ###reference_###, reveal significant variations in performance across different test groups, underscoring the challenge of applying models trained on a single-source dataset to news originating from varied sources.\nThe BERT-EMO model delivered the highest performance, achieving an accuracy of 0.821 and a macro F1 score of 0.818. However, while it scored a high macro F1 of 0.943 on data from Weibo (Group 3), its effectiveness significantly decreased on data from Group 1 and Group 2, with scores dropping to 0.572 and 0.343, respectively.\nNote that the pattern of performance variance was consistent across all baseline models, including BERT, RoBERTa, CLIP, CAFE, and dEFEND, across different groups.\nThese findings provide a crucial insight in the responses to EQ1: Baseline models, even those considered state-of-the-art and trained on Chinese fake news detection datasets from Weibo, exhibit limited robustness when confronted with fake news from diverse sources in the wild.\nOne contributing factor to such performance decrease may be the significant difference in the content and social context feature, as shown in Fig. 3  ###reference_###, between news sourced from Weibo and news from other sources.\nThis discrepancy also highlights a significant concern regarding the possible overestimation of the effectiveness of current Chinese fake news detection methods, underscoring the necessity for a thorough reevaluation before they are considered for practical application in real-world situations.\nA comparison in Table 5  ###reference_### shows the average decrease in macro F1 score when moving from Weibo-sourced data to other sources, with a smaller decrease indicating greater source robustness.\nOur analysis found that models employing modal fusion approaches, integrating both text and social context, demonstrate stronger resilience against data source variability, with an average decrease of only 0.084.\nIn particular, the modal fusion-based model, dEFEND, exhibited the greatest robustness, with a minimal decrease of 0.032.\nIn contrast, uni-modal and multi-modal approaches saw larger decreases of 0.222 and 0.227, respectively.\nThese results indicate that the model performance is greatly influenced by content pattern differences among sources, suggesting that exploring ways to mitigate these impacts is a valuable direction for future research."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Multi-source Evaluation",
            "text": "To address EQ2, we performed multi-source evaluations on the selected baseline models. Specifically, these baseline systems were trained with the complete train set of the MCFEND dataset, including news from all sources covered in our dataset and their corresponding social contexts.\nThe multi-source evaluation results, depicted in Table 6  ###reference_###, provide crucial insights.\nA comparison with the cross-source evaluation data (refer to Table 4  ###reference_###) reveals significant performance improvements in all baseline models upon integrating multi-source data for training.\nNotably, RoBERTa and CAFE show the most substantial gains, with their macro F1 scores increasing by 0.245 and 0.224, respectively.\nSuch boost in performance can be attributed to the diverse range of fake news features presented by multi-source data, which enhances the models’ ability to discern between fake and real news from different sources, helping to prevent models from overfitting to the specific characteristics of data from a single source like Weibo.\nTo offer a qualitative analysis of the enhancement from using multi-source data in the training process, we take the CAFE model as an example.\nWhen trained with Weibo data exclusively, the CAFE model fails to correctly identify the news pieces (c) and (d) shown in Fig. 1  ###reference_### as fake.\nHowever, when trained with data from all the diverse sources encompassed in the MCFEND train set, the CAFE model exhibits the ability to accurately detect all of the presented fake news in Fig. 1  ###reference_###.\nThese findings address EQ2 by demonstrating that using multi-source data to train fake news detection models significantly improves their performance and robustness in real-world applications. MCFEND dataset could be an invaluable asset for enhancing the detection of Chinese fake news across a variety of sources.\nAdditionally, the overall performance of almost all baseline models, except dEFEND, when they were trained and tested on multi-source data is lower than their performance when they were trained and tested on Weibo data exclusively.\nThis finding addresses the challenge of developing algorithms capable of effectively distinguishing generic fake news features across news from various sources in real-world scenarios.\nCompared among baseline models, the state-of-the-art content-based and social context-based models, CAFE and BERT-EMO, stand out with macro F1 scores of 0.845 and 0.916, respectively.\nThis implies that their relatively lower performance in cross-source evaluation suggests limitations in current training datasets.\nThus, our diverse and comprehensive MCFEND dataset, designed to address these limitations, is crucial for advancing Chinese fake news detection in the wild."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Unseen Source Evaluation",
            "text": "To address EQ3, we conducted an unseen source evaluation to assess the robustness of existing methods in detecting fake news from previously unencountered news sources.\nThis involved training two versions of the BERT-EMO model, which had shown superior performance in both cross-source and multi-source evaluations, on distinct dataset compositions.\nModel A was trained exclusively on data from Group 3 (Weibo), while Model B incorporated data from both Group 1 (various Chinese news sources verified by fact-checking agencies) and Group 3 (Weibo).\nWe then evaluated both models using data from Group 2 (English news sources), which was new to each model.\nThe results, detailed in Table 7  ###reference_###, reveal that Model B, trained on a more diverse dataset, achieved a higher macro F1 score of 0.432, compared to Model A’s score of 0.287.\nThis finding addresses EQ3 by suggesting that leveraging multi-source data enhances the robustness of methods for detecting fake news from unseen sources.\nTherefore, when a new news source emerges in the future, but there is no available data from that platform for model training, a model trained on multi-source data can be expected to more accurately detect fake news from this unseen new source."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusion",
            "text": "In this work, we introduced the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND. Unlike existing Chinese fake news detection datasets that are based on a single news source, i.e., Weibo, MCFEND is constructed on (real and fake) news from multiple sources that were fact-checked by 14 authoritative fact-checking agencies. To test the applicability of existing methods, we conducted a systematic evaluation of six representative fake news detection models, including the state-of-the-art ones, in both cross-source and multi-source scenarios. Our experimental results reveal that models trained exclusively on Weibo data can hardly be applicable in real-world scenarios, where fake news typically originates from diverse sources. We also found that incorporating multi-source data into model training enhances the robustness of existing fake news detection methods. Our proposed MCFEND aims to be a benchmark dataset for Chinese fake news detection in the wild, which advances new effective methods in this research field.\nOur world is increasingly suffering from unrestrained spread of misinformation in many areas. We hope MCFEND can put forward research results that can combat misinformation and help make our world a better one."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62202402), the Guangdong Basic and Applied Basic Research Foundation (No. 2022A1515011583 and No. 2023A1515011562), the Hong Kong RGC Early Career Scheme (No. 22202423), the Germany/Hong Kong Joint Research Scheme sponsored by the Research Grants Council of Hong Kong and the German Academic Exchange Service of Germany (No. G-HKBU203/22), the One-off Tier 2 Start-up Grant (2020/2021) of Hong Kong Baptist University (Ref. RC-OFSGT2/20-21/COMM/002), and the Startup Grant (Tier 1) for New Academics AY2020/21 of Hong Kong Baptist University.\nWe thank Ms. Zihang Shan for her assistance in developing the cross-lingual identical news retrieval model and preparing the data. We also thank Ms. Zixin Tang for her assistance in developing the CLIP baseline and executing the associated experiments."
        }
    ]
}