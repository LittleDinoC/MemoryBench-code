{
    "title": "Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training",
    "abstract": "Recent advancements in language modeling have led to the emergence\nof Large Language Models (LLMs) capable of\nvarious natural language processing tasks.\nDespite their success in text-based tasks, applying LLMs to the speech domain\nremains limited and challenging. This paper presents BLOOMZMMS, a novel model\nthat integrates a multilingual LLM with a multilingual speech encoder,\naiming to harness the capabilities of LLMs for speech recognition and beyond.\nUtilizing a multi-instructional training approach, we demonstrate the transferability\nof linguistic knowledge from the text to the speech modality.\nOur experiments, conducted on 1900 hours of transcribed data from 139 languages,\nestablish that a multilingual speech representation can be effectively\nlearned and aligned with a multilingual LLM. While this learned representation\ninitially shows limitations in task generalization, we address this issue by\ngenerating synthetic targets in a multi-instructional style.\nOur zero-shot evaluation results confirm the robustness of our approach across\nmultiple tasks, including speech translation and multilingual spoken language\nunderstanding, thereby opening new avenues for applying LLMs in the speech domain.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language modeling task involves predicting subsequent text tokens based\non a context of preceding ones Jurafsky and Martin (2009  ###reference_b13###).\nTraining a language model (LM) requires only raw text samples, as portions of these samples function as their labels, facilitating a self-supervised learning (SSL) approach. The widespread availability of machine-readable text online, coupled with advancements in computational power, has led to the rise of large LMs (LLMs) in recent years.\nThese LLMs not only generate highly fluent natural text but also encode higher-level\nknowledge within their parameters. This enables them to tackle natural language\nprocessing tasks like reading comprehension and machine translation\nbased only on task specific instructions, without needing labeled data Radford et al. (2019  ###reference_b28###).\nSSL has recently made significant strides in the speech domain Baevski et al. (2020  ###reference_b4###).\nMost applications of SSL in speech employ an encoder that transforms\nraw speech signals into high-level representations, serving either\nas a fixed feature extractor Yang et al. (2021  ###reference_b36###) or a tunable pretrained model\nfor various downstream tasks Babu et al. (2021  ###reference_b3###). Incorporating of SSL pretrained encoders\ninto Encoder-Decoder speech recognition models has dramatically reduced\nthe amount of labeled data required for effective training Chang et al. (2021  ###reference_b5###).\nHowever, using SSL pretrained decoders in such models is relatively rare.\nIn certain instances, SSL is part of a joint training process that seeks\nto learn a shared speech and text representation Chen et al. (2022  ###reference_b7###). However, this approach often\ndemands a large dataset and considerable computational resources.\nRecent work has begun to harness the powerful text generation capabilities\nof decoder-only LLMs by incorporating them as the decoder component of Encoder-Decoder\nspeech processing models.\nWu et al. (2023  ###reference_b35###) adopt the LLaMA-7B LLM for speech translation to English\nby training a speech encoder from scratch using filter bank acoustic features,\n14,000 hours of internal speech data in 14 languages, and outputs of internal\ntranslation system as synthetic targets. Outputs of speech encoder are aligned with\nthe text token embedding space using CTC pretraining and downsampled by\naveraging of consequative frames with the same CTC output label.\nLing et al. (2023  ###reference_b19###) adopt the GPT2 XL LLM for fully-formatted English speech recognition\nby training a speech encoder from scratch using filter bank acoustic features,\nand 75,000 hours of internal transcribed English speech data. CTC loss is applied to speech encoder\noutputs as a part of the main training process and speech representations are downsampled\nby removal of frames classified as CTC blank labels with a predefined threshold.\nLi et al. (2023  ###reference_b18###) adopt the LLaMA-7B LLM for long-form English speech recognition\nby incorporating the HuBERT-Large SSL pretrained speech encoder and finetuining\nit on the LibriSpeech dataset containing 960 hours of transcribed English speech.\nOutputs of the speech encoder are downsampled by a convolutional module trained\nas a part of the main training process.\nFathullah et al. (2023  ###reference_b11###) adopt the LLaMA-7B LLM for speech recognition in 8 languages\nby training a speech encoder from scratch using filter bank acoustic features\nand the Multilingual LibriSpeech dataset containing 50,000 hours of transcribed speech in the same 8 languages.\nSpeech encoder is pretrained with CTC loss and its outputs are downsampled by simple\ndiscarding of every  frames.\nNachmani et al. (2023  ###reference_b22###) combine an internal pretrained LLM with an internal pretrained speech encoder\nand finetune it on the automatically transcribed LibriLight dataset containing 60,000 hours of English speech.\nThe training is performed with a combination of the speech transcription and speech continuation tasks.\nThe resulting model is utilized for the spoken language answering task.\nMost of these studies rely on conventional\nfilter bank features for speech encoding and do not incorporate an SSL pretrained speech encoder,\nnecessitating a large amount of training data. Moreover, scant attention has been\ngiven to leveraging the linguistic knowledge stored in LLMs for tasks beyond mere transcription\nand for languages other than English.\nTo address these challenges, we propose BLOOMZMMS, a model that fuses a multilingual\nLLM (BLOOMZ Muennighoff et al. (2023  ###reference_b21###))\nwith a multilingual speech encoder (MMS Pratap et al. (2023  ###reference_b25###)).\nWe argue that multi-instructional training is\ncrucial for transferring linguistic knowledge from the text to speech modality.\nOur experiments demonstrate that training\non 1900 hours of transcribed data from 139 languages\nyields a multilingual speech representation compatible with a multilingual\nLLM in the context of Automatic Speech Recognition (ASR) task.\nAlthough this representation does not generalize well to other tasks,\nwe show that the issue can be mitigated by generating additional synthetic targets.\nOur zero-shot evaluations confirm this approach’s effectiveness across various tasks,\nincluding Spoken Language Translation (SLT) and multilingual spoken Natural Language Inference (NLI).\nOur training recipes and models are released under\nthe Apache-2.0 license111https://github.com/akreal/bloomzmms  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "###figure_1### \n###figure_2### \n###figure_3### The proposed method is outlined in Figure 1  ###reference_###.\nOur model comprises the pretrained speech encoder, LLM and an intermediate\nAdaptor module that maps the output of the speech encoder to the latent space\nof the text token embeddings of the LLM.\nWe train the Adaptor module using pairs of speech recordings and their corresponding text transcriptions,\ndenoted as  and  respectively, and keep the parameters\nof the speech encoder and the LLM frozen. The objective of the Adaptor training\nis to make its output  obtained from the input speech \nas close as possible to the text embedding sequence\nof the ground truth transcription ,\nwhere LMEmbedding is the token embedding layer of the LLM.\nSimilarly to previous works on the LLM adaptation to the speech modality Wu et al. (2023  ###reference_b35###); Fathullah et al. (2023  ###reference_b11###),\nour training process comprises of the two stages:\nan alignment of the speech encoder output with the LLM token embedding space,\nand an integrated optimization of the complete model with the LLM.\nAn attempt to omit either of the two stages in our process\nleads to the lack of training convergence.\nWe hypothesize that the different training stages help the Adaptor to learn\ndifferent subtasks like segmentation, ordering and the actual token embedding prediction.\nAt the first stage of the training,  is projected\nto the LLM tokens’ logits using the frozen output linear layer of the LLM\n(which is often a transposed token embedding layer),\nand the Connectionist Temporal Classification (CTC) loss Graves et al. (2006  ###reference_b12###)\nis minimized between the LLM token probabilities obtained\nfrom the token logits and the transcription:\nwhere the mapping  removes\nrepeated and blank tokens according to the CTC definition,\n is the transposed weight matrix of the token embedding layer,\n is the dimensionality of the embedding, and  is the number of tokens in the LLM’s vocabulary.\nAt the second stage,  is concatenated with\nthe token embeddings of the prefix and postfix parts of a text prompt.\nThis joint sequence is then passed through the self-attention layers\nof the LLM and projected with the transposed token embedding weight matrix \n(also serving as the output layer of the LLM) to obtain the LLM prediction.\nThe Cross-Entropy (CE) loss is minimized between the prediction of the LLM\nfor this sequence and the expected LLM output.\nIn case of the speech recognition task,\nwe set the prompt prefix and postfix to\n\"Repeat the sentence: \" and \". \" respectively:\nwhere  denotes the self-attention layers of the LLM.\nIn case of the multi-instructional training, prompts are sampled\nfrom a predefined hand crafted collection, while the expected\noutput is set to the output of the LLM for the same prompt\nusing the token embeddings of the ground truth\ntranscription instead of the Adaptor output :\nwhere  and  are the prefix and postfix\ntexts of the -th prompt in the prompts collection,\n is a random number drawn from an uniform distribution\nover all natural numbers between 1 and ,\nand  is the number of prompts in the collection."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training and Validation Data",
            "text": "The Adaptor training is performed on the entire training FLEURS dataset Conneau et al. (2023  ###reference_b8###)\nand a subset of the Common Voice Corpus 12.0 Ardila et al. (2020  ###reference_b1###) training dataset\nwith the total amount of 993,660 utterances or 1905 hours of recordings.\nThe Common Voice subset is constructed by selection of up to 25 hours\nof recordings for each language.\nOur validation set is the validation set of FLEURS\nwith the total amount of 34,044 utterances or 115 hours of recordings.\nAll transcriptions are taken in an unnormalized format with the true casing\nand punctuation.\nMulti-instructional training labels are synthesized with prompts\nfrom the P3 collection Sanh et al. (2022  ###reference_b29###).\nThe P3 collection is selected because it was employed in the finetuning\nprocess of transitioning BLOOM into BLOOMZ.\nOur objective is to ensure consistent output for both speech and text inputs.\nTo achieve this, we generate text outputs utilizing prompts\nfrom the P3 collection, with which the BLOOMZ model is already acquainted.\nWe apply six distinct randomly drawn prompts to\na transcription of each original utterance and assign two generated\noutputs to each of the three speed-perturbed versions of that utterance.\nThe outputs are generated with a greedy search and maximum length of 128 tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Data and Metrics",
            "text": "We evaluate our model on the following established benchmarks:\nFLEURS Conneau et al. (2023  ###reference_b8###), MLS Pratap et al. (2020  ###reference_b26###)\nand VoxPopuli Wang et al. (2021a  ###reference_b31###) for the ASR, CoVoST 2 Wang et al. (2021b  ###reference_b32###) for the SLT,\nSpeechGLUE Ashihara et al. (2023  ###reference_b2###) for the spoken General Language Understanding (GLUE) and\nSpeechXNLI for the multilingual NLI222Following SpeechGLUE,\nwe synthesize a speech version of the XNLI Conneau et al. (2018  ###reference_b9###) validation subset\nusing the IMS Toucan Lux et al. (2022  ###reference_b20###) text-to-speech toolkit: https://zenodo.org/records/10900287  ###reference_###..\nThe results are evaluated using the corresponding metrics:\nWord Error Rate (WER) and Character Error Rate (CER) for the ASR,\nBLEU333Using the SacreBLEU tool Post (2018  ###reference_b24###). Papineni et al. (2002  ###reference_b23###) for the SLT,\nMatthews Correlation Coefficient (MCC) for the CoLA task within SpeechGLUE,\nand accuracy for the other SpeechGLUE tasks and the SpeechXNLI.\nWhisper normalization\nis applied for both reference and hypothesis before evaluating CER/WER in the ASR experiments."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "Our model is implemented using ESPnet2 Watanabe et al. (2021  ###reference_b33###) version 202304\nand Hugging Face Transformers Wolf et al. (2020  ###reference_b34###) version 4.31.0.\nWe use weighted-sum of hidden states Yang et al. (2021  ###reference_b36###); Chang et al. (2021  ###reference_b5###)\nof the MMS 1B-ASR-All444https://huggingface.co/facebook/mms-1b-all  ###reference_###\npretrained model Pratap et al. (2023  ###reference_b25###) as speech features.\nWe discard all language specific adapters and heads of the MMS 1B-ASR-All model\nto simplify the implementation while preserving the multilingual properties of our system.\nThe Adaptor module is a VGG/E-Branchformer based encoder Kim et al. (2023  ###reference_b14###)\ncombined with a convolutional Length Adaptor Li et al. (2021  ###reference_b17###).\nThe E-Branchformer encoder is configured with 17 layers,\neach with 2048 hidden units, 8 attention heads, and output dimension of 1024.\nThe Convolutions to Gated MultiLayer Perceptron module has 8192 units and the convolution kernel size is 31.\nThe Length Adaptor module contains a 1-dimensional convolutional layer with stride 2\nand reduces the length of input sequence by factor of 2.\nSelf-conditioning on language identity Chen et al. (2023  ###reference_b6###)\nis applied during the CTC training.\nThe LLM in our experiments is BLOOMZ 7.1B555https://huggingface.co/bigscience/bloomz-7b1  ###reference_b1###\nmodel Muennighoff et al. (2023  ###reference_b21###),\nwhich itself is BLOOM 7.1B LLM Scao et al. (2022  ###reference_b30###) finetuned on\nthe xP3 dataset introduced with BLOOMZ.\nThe total number of parameters in our model is 8.6 billions,\nthe number of trainable parameters is 536 millions.\nWe apply 8-bit quantization Dettmers et al. (2022  ###reference_b10###) to the LLM\nusing the functions from the bitsandbytes package version 0.41.1.\nThe training is done with the Adam optimizer Kingma and Ba (2015  ###reference_b15###) with\n, , ,\nthe warmup learning rate scheduler with\nthe maximum learning rate of  and a weight decay of .\n3-way speed perturbation Ko et al. (2015  ###reference_b16###) data augmentation\nmethod is applied to the training data.\nThe training stage one, CTC loss training, is performed on two NVIDIA RTX A6000 GPUs\nwith the global batch size of 7.29 minutes.\nThe number of warmup steps for the learning rate scheduler is set to 25,000.\nA checkpoint is saved every 23,364 steps and evaluated on the validation dataset.\nThe training is stopped after four consecutive evaluations showing no improvement,\nit takes 233,640 update steps or 120 hours of training time to reach this condition.\nA checkpoint with the lowest validation CER from the stage one is used\nto initialize the model for the stage two.\nThe training stage two, CE loss training, is performed on four NVIDIA RTX A6000 GPUs\nwith the batch size of 37.50 seconds and a gradient accumulation over two batches.\nThe number of warmup steps for the learning rate scheduler is set to 10,000.\nA checkpoint is saved every 54,381 steps and evaluated on the validation dataset.\nThe training is stopped after four consecutive evaluations showing no improvement.\nTo reach this condition, it takes\n652,572 update steps or 132 hours of training on the transcription targets,\n2,664,669 update steps or 686 hours on the multi-instructional targets, and\n2,501,526 update steps or 644 hours on the combined set of targets.\nA checkpoint with the highest validation token prediction accuracy from the second step\nis used for the zero-shot evaluations.\nWe decode with the beam search of size 5 and set the maximum output sequence to 192 tokens\nto obtain the model predictions for the ASR and SLT evaluations.\nThe GLUE and NLI evaluations restrict the output to the possible answer options\ncorresponding to a task and limits the beam size and maximum output sequence\nrespectively. For example, for a yes/no question the possible outputs are\nyes or no, the beam size is 2 and the maximum output sequence is 1.\nAll evaluations are executed on one NVIDIA RTX A6000 GPU."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Speech Recognition",
            "text": "###figure_4### Table 2  ###reference_### presents a comparative analysis\nof ASR performance for the BLOOMZMMS model\nwith the T, MI and TMI training targets.\nResults are further divided based on whether the languages were seen during the training\nof the BLOOM model or not.\nFor languages that were part of the BLOOM model training,\nthe TMI model generally performs better than the T model.\nThe opposite is true for the non-BLOOM languages.\nThis is expected as training on the MI targets puts stronger stress on the distillation\nof the LLM knowledge and its encoding to the Adaptor parameters.\nThis effect is more pronounced on the MLS and VoxPopuli datasets,\nwhich represent recording conditions and\nlinguistic content slightly different from our training data.\nNevertheless, both T and TMI BLOOMZMMS models perform comparably\non the in-domain FLEURS dataset independently from the language,\nsuggesting that the Adaptor can effectively leverage\nthe outputs of the MMS speech encoder in order to compensate\nfor the lack of language familiarity by the LLM.\nFollowing the MMS paper, we separate a subset of FLEURS testing dataset\nfor the 54 languages that are supported by the Whisper model,\nand compare the results of the BLOOMZMMS TMI model\nto the results of the multi-domain MMS (1B) and Whisper large-v2 models.\nThe MMS model is essentially the same speech encoder as used by BLOOMZMMS,\nbut with a number of language-specific components,\nnamely adapter parameters, output vocabulary,\nand n-gram model utilized during decoding.\nDespite removal of the language-specific components\nand addition of the other speech processing tasks,\nsuch as SLT, BLOOMZMMS manages to keep the ASR\nperformance on a comparable level to the original MMS model.\nWhile also being a multitask model, BLOOMZMMS\noutperforms the other strong multitask alternative,\nWhisper large-v2, by a large margin\non this massively multilingual low-resource ASR benchmark,\nalbeit potentially due to being trained on in-domain data,\nin contrast to Whisper."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Speech Translation",
            "text": "###figure_5### Table 7  ###reference_te7### presents the zero-shot evaluation\nresults for SLT using the CoVoST 2 dataset.\nThe BLOOMZ LM exhibits a nascent ability\nto translate languages that it has not been trained on,\nand when this knowledge is transferred to the speech modality,\nthere’s only a minor loss in accuracy.\nInterestingly, the performance gap between the BLOOMZMMS model\nand gold transcriptions is more pronounced for the BLOOM languages.\nThis indicates that the quality of knowledge transfer\nfrom text to speech depends on the initial linguistic knowledge in the text-based LLM.\nConsequently, weaknesses present in the LLM\ntend to amplify when transferred to the speech modality,\nsuggesting that the proposed method might benefit from some form of\nregularization to mitigate this effect.\nFigure 3  ###reference_### shows the comparison of the BLOOMZMMS TMI\nmodel with the previous works, XLS-R/mBART and Whisper large-v2,\nfor the XEn translation direction.\nXLS-R/mBART is a strong baseline, which is finetuned\non complete CoVoST 2 training data.\nWhisper large-v2 has not seen any CoVoST 2 data during training,\nbut has been supervised by a large amount\nof other speech translation data.\nBLOOMZMMS TMI has not been exposed to any gold labeled\nspeech translation samples during training.\nRemarkably, the zero-shot BLOOMZMMS model outperforms\nthe supervised task-specific XLS-R/mBART model\nfor the languages previously seen during BLOOM training.\nThis impressive result is primarily due to the strong performance\nof the BLOOMZ LLM, which is successfully\ntransferred to the speech modality via the multi-instructional training.\nHowever, there is a notable gap with the multitask\nWhisper large-v2 model, primarily attributed to the\npoor performance on unseen languages of the LLM we utilize.\nIn order to expand language coverage, we evaluate our model for the SLT performance\non the FLEURS dataset as well, and present the results in\nTable 4  ###reference_###.\nAs suggested by Radford et al. (2023  ###reference_b27###), we use target language transcriptions\nfor the sentences with the same ID as reference translations.\nOur evaluation does not include Afrikaans, because\nthe version of the dataset we use888https://huggingface.co/datasets/google/fleurs  ###reference_urs###\ndoes not include any sentence IDs shared between Afrikaans and English.\nThe multilingual properties of the BLOOMZ model, which serves as a decoder\nof our model, enable us to report the SLT results with non-English target languages\nas well, for the first time on the FLEURS dataset to the best of our knowledge.\nThe results confirm the good transferability of translation capabilities\nfrom text to speech modality with the MI and TMI training targets\nfor a wider range of languages seen in the BLOOM training data.\nThe fair translation performance from unseen languages to English, as observed\nin the CoVoST 2 dataset, can also be seen across a wider range\nof languages in the FLEURS dataset."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Spoken Language Understanding",
            "text": "Tables 5  ###reference_### and 6  ###reference_### provide the results of zero-shot\nevaluation of BLOOMZMMS models on spoken GLUE tasks\nin English using the SpeechGLUE dataset and on spoken NLI tasks in multiple languages\nusing the SpeechXNLI dataset.\nIt is worth noting that the combined TMI training targets result in better performance\non the English GLUE tasks, but have a mixed impact on the NLI tasks based on the\nlanguages trained in BLOOM and those that were not.\nFor the BLOOM languages,\nthe TMI model equals the MI-only model in accuracy,\nwhereas it performs worse on the non-BLOOM languages.\nTogether with the SLT results, this observation again hints at\nthe effect of the LLM’s weaknesses amplification during the transfer\nfrom the text to speech modality."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Visual Analysis",
            "text": "###figure_6### \n###figure_7### \n###figure_8### \n###figure_9### \n###figure_10### \n###figure_11### Following the example of Fathullah et al. (2023  ###reference_b11###), we display\nthe cosine similarity between the text and speech embeddings\nfor the three variants of BLOOMZMMS for a French and a Finnish utterance\nfrom the FLEURS evaluation dataset (Figure 4  ###reference_###).\nConsistent with the objective metrics from our experiments,\nthe model trained on the transcription targets shows the noisiest alignments\nfor the both languages, while the MI training targets offer better alignment\nfor a language unseen by BLOOM and the combined training targets\nwork better for a language seen by BLOOM."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper we present BLOOMZMMS, a multilingual multitask speech processing\nmodel that combines a multilingual LLM and a pretrained multilingual speech encoder.\nOur investigation into two training strategies revealed their combined efficacy\nin a broad spectrum of spoken language processing tasks,\na conclusion bolstered by zero-shot evaluations on multiple benchmarks."
        }
    ]
}