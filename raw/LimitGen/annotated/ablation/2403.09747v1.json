{
    "title": "Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors",
    "abstract": "The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented LLMs framework—the first of its kind to automatically and strategically extract key evidence from web sources for claim verification. Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance. Comprehensive experiments across three real-world datasets validate the framework’s superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The escalation of fake news poses a severe threat, dwarfing extensive efforts to mitigate its impact on political, economic, and social landscapes West and Bergstrom (2020  ###reference_b35###). Fake news detection approaches to combat this issue generally fall into three categories: content-based Zhou and Zafarani (2021  ###reference_b46###); Capuano et al. (2023  ###reference_b3###), evidence-based Kotonya and Toni (2020  ###reference_b15###); Min et al. (2022  ###reference_b19###), and social context-based methods Collins et al. (2021  ###reference_b6###); Grover et al. (2022  ###reference_b7###).\nHowever, existing methods Zhou and Zafarani (2020  ###reference_b45###); Zhang and Ghorbani (2020  ###reference_b44###) are typically tailored to specific datasets, thereby inherently constraining their scalability, transferability, and robustness.\nIn light of these constraints, there arises an imperative for the development of a more versatile model that can efficiently detect fake news in a zero-shot or few-shot learning manner.\n###figure_1### Large Language Models (LLMs) have shown remarkable capabilities across various applications Wei et al. (2022a  ###reference_b33###). Current methodologies utilizing Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) often depend on specific databases such as Wikipedia or employ a simple one-step retrieval process Izacard et al. (2023  ###reference_b12###); Guu et al. (2020a  ###reference_b8###). However, in the context of real-world fake news detection, there are significant systemic challenges that necessitate more sophisticated solutions. These challenges encompass the growing issue of AI-generated disinformation, the limitations inherent in depending on a limited number of data sources, the obstacles of ensuring real-time updates in a constantly changing news environment, and the long-tail effect where rare or niche false information may remain undetected Chen and Shu (2023  ###reference_b4###). In response to these obstacles, we propose an innovative multi-round LLM-based RAG framework.\nWe introduce STEEL (STrategic rEtrieval Enhanced with Large Language Model), a comprehensive, automated framework for fake news detection that combines ease-of-use and interpretability. Our framework leverages the reasoning and uncertainty estimation capabilities of LLMs, offering more robust evidence retrieval. It also sidesteps the limitations of relying on a solitary predefined corpus by sourcing evidence directly from the expansive Internet. As illustrated in Figure 1  ###reference_###, STEEL employs an adaptive multi-round retrieval process, using a Large Language Model to generate targeted queries for missing information when initial evidence is insufficient. In addition, it can sharpen the focus of subsequent retrievals and save crucial evidence already obtained for the next judgment.\nIn this work, we make the following contributions.\nWe propose a novel framework, STEEL, for automatic fake news detection through strategic Internet-based evidence retrieval. To the best of our knowledge, this is the first framework that leverages LLMs for fake news detection via strategic evidence retrieval from the Internet.\nWe provide an open-source implementation that is designed for out-of-the-box use, eliminating the need for complicated data processing or model training.\nExtensive experiments on three real-world datasets demonstrate that STEEL outperforms state-of-the-art methods in both prediction and interpretability."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "RAG LLMs",
            "text": "The retrieval-augmented language model assists text generation by retrieving relevant documents from a vast external knowledge base Nakano et al. (2021  ###reference_b20###).\nThis combats long-tail, outdated knowledge, and hallucination issues Kandpal et al. (2023  ###reference_b14###). Recent work has shown that retrieving additional information can improve performance on a variety of downstream tasks Yao et al. (2023b  ###reference_b41###), including open-domain Q&A, fact-checking, fact completion, long-form Q&A, Wikipedia article generation, and fake news detection Yu et al. (2023  ###reference_b43###); Guu et al. (2020b  ###reference_b9###); Asai et al. (2023  ###reference_b1###); Wu et al. (2023  ###reference_b36###); Wang and Shu (2023  ###reference_b28###).\nSTEEL differs notably from other retrieval methods in the RAG+LLM framework, like FLARE Jiang et al. (2023  ###reference_b13###), Replug Shi et al. (2023  ###reference_b26###), ProgramFC Pan et al. (2023  ###reference_b22###), and SKR Wang et al. (2023b  ###reference_b30###). While FLARE, ProgramFC, and SKR focus mainly on text blocks, Replug on documents, STEEL retrieves both documents and text blocks. Unlike methods relying on Wikis, STEEL uses the Internet as its source. It shares context-based retrieval timing with other methods but introduces active search features, including LLM feedback utilization and answer verification, enhancing its flexibility and depth in retrieval tasks within the RAG+LLM framework."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Natural Language Inference LLMs",
            "text": "Natural Language Inference (NLI) is used to predict the logical connection between the claim and the provided evidence.\nRecent studies have made strides in enhancing LLMs’ reasoning. Chain of Thought Wei et al. (2022b  ###reference_b34###) achieved significant improvements with simple prompt modifications. ReAct Yao et al. (2023b  ###reference_b41###) integrates reasoning and acting capabilities in LLMs for better performance in tasks requiring complex reasoning. Tree of Thoughts Yao et al. (2023a  ###reference_b40###) enables deliberate decision-making in LLMs by exploring reasoning paths and facilitating self-evaluation. In contrast, our work focuses on evidence-retrieval strategies for news verification.\nCurrently, main application paradigms can be divided into: Prompting Ram et al. (2023  ###reference_b25###), Fine-tuning Borgeaud et al. (2022  ###reference_b2###), and Reinforcement learning Liu et al. (2023  ###reference_b17###). Existing industrial solutions like NEW BINGBING 111https://www.bing.com/new  ###reference_www.bing.com/new### and Perplexity.ai 222https://www.perplexity.ai/  ###reference_www.perplexity.ai/### integrate LLMs with search engines for performance but aren’t optimized for fake news detection. In this task, evidence quality is crucial due to LLM input length limits. STEEL addresses this by using LLM feedback and multi-round evidence retrieval."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "In this section, we present our model, STEEL. The input of this method consists of a claim . Initially, a set of relevant evidence  are retrieved from the Internet. Subsequently, LLMs evaluate the sufficiency of the gathered evidence. If the evidence is deemed adequate, the results will be output promptly. Otherwise, the search for additional evidence continues. To construct an affordable, ready-to-use framework, we leverage the APIs (Application Interfaces) of leading AI (Artificial Intelligence) companies. Specifically, we utilize BING Search for web evidence retrieval and OPENAI’s GPT-3.5-turbo  OpenAI (2022  ###reference_b21###) for verification. The output is the prediction of this claim , along with explanatory text . Here,  refers to the LLMs responsible for generating the output.  is a binary classification, where  indicates the assessment of the news claims as true or false.\nAs shown in Figure 2  ###reference_###, our model mainly comprises two key components: a retrieval module and a reasoning module. These two modules are integrated within the overarching framework of the re-search mechanism.\n###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Retrieval Module",
            "text": "Unlike prior studies that separate web retrieval and semantic retrieval, we integrate both stages. A claim  is first processed by a web retrieval API to obtain document links  containing pertinent evidence. Typically, 10 links are retrieved; however, due to constraints imposed by the context length of Large Language Models (LLMs), not all links are utilized.\nFor source credibility, we implement a basic filtering mechanism. Based on previous research Papadogiannakis et al. (2023  ###reference_b23###), we use a list of  known fake news websites as a filter, discarding any matches during web search.\nThe documents retrieved online are initially organized based on the relevance algorithm of the search engine, with the document deemed most relevant positioned at the top of the list. Our analytical process adheres to the sequence of this sorted list, beginning with the first document. Specifically, our approach involves assessing whether the length of the top-ranked document exceeds our predefined limit determined by the LLM’s context length. If it does, we employ semantic retrieval techniques to extract highly similar fragments from the document. Conversely, if the length is within acceptable limits, we utilize the entire document and then sequentially examine the second-ranked document, continuing this process until we reach the maximum allowable context length. By this, we strive to gather a comprehensive array of relevant evidence while maintaining the integrity of the information retrieved."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Reasoning Module",
            "text": "The relevant pieces of evidence  retrieved from the web are then aggregated into prompts and fed into the LLMs for inference. LLMs can make decisions based on given\nevidence, including deciding if they need to re-search, case can be seem at Figure A1  ###reference_###.\nEssentially, the prompt instructs LLM to assess the claim based on the retrieved evidence and output responses, which are classified into three categories - true, false, and NEI (Not Enough Information). Explanations of the responses are provided based on the sufficiency of the retrieved evidence. For \"NEI\", \"Established evidence\" and \"Updated queries\" will be output for further evidence collection. \"Established evidence\" is the compression of this evidence for the next judgment. \"Updated queries\" are the queries for subsequent web page retrieval, with the purpose of incrementally obtaining evidence. Prompts utilized here can be seen in listing 7  ###reference_###.\nTo mitigate consistency issues, we incorporate a confidence level for each answer, along with aggregated new and established evidence for subsequent assessment.\nThe third is aggregated evidence of  obtained after retrieval and \"Established evidence\" in the previous cycle.\nTo address inconsistent answers Ye and Durrett (2022  ###reference_b42###) and hallucinations problem, some previous work Xiong et al. (2023  ###reference_b38###); Wang et al. (2023a  ###reference_b29###) exploits the self-consistence and self-judgment approaches, enabling the LLMs to produce confidence scores within the range of . Nonetheless, it has been observed that contemporary LLMs often exhibit a tendency toward overconfidence Wang et al. (2023d  ###reference_b32###); Xiong et al. (2023  ###reference_b38###). To counteract this phenomenon, we introduce an over-confidence coefficient within the range of . The final confidence score is adjusted by multiplying it with this coefficient.\nWhen the final Confidence falls below , the model is instructed to proceed to the next iteration.\nIn equation 2  ###reference_###,  denotes the final confidence score,  represents the initial confidence score provided by the LLMs, and  represents the over-confidence coefficient."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Re-Search Mechanism",
            "text": "As illustrated in Figure 3  ###reference_###, the re-search is triggered under certain conditions. This feature ensures a more robust and exhaustive gathering of evidence, enhancing the method’s reliability and performance.\n###figure_3### Upon meeting a re-search condition, the model kicks off a systematic process. First, it consolidates the evidence gathered from the initial search, appending it to an \"established evidence\" pool for future reference. Next, the model formulates a set of \"updated queries\" aimed at obtaining additional relevant evidence. This iterative approach ensures a gradual accumulation of evidence, thereby enhancing the model’s ability to discern the veracity of news.\nRegarding the rationale behind our choice of re-search over alternative methods that appear to enhance retrieval quality, such as query-dependent techniques or search engineering, a detailed explanation will be provided in section 4.3  ###reference_###.\nConsequently, when LLMs determine that the current evidence set is inadequate for a reliable judgment on the claim at hand, it signals this by outputting \"NEI\". This output serves as a trigger for the model to advance to a subsequent iterative search. The mechanics behind this intermediate step are further detailed in Equation 3  ###reference_###.\nwhere  and NEI stands for Not Enough Information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we conduct experiments to evaluate the efficacy of our STEEL model in multiple angles. We focus on three main aspects: the efficiency of evidence retrieval in identifying fake news, the role of the re-search mechanism in bolstering detection accuracy, and the influence of varying retrieval steps and prompts on the model’s performance.\nTo evaluate the performance of STEEL, we conduct extensive experiments on three real-world datasets, comprising two English datasets (LIAR333LIAR:https://www.cs.ucsb.edu/ william/data/liar_dataset.zip  ###reference_/liar_dataset.zip### and PolitiFact444PolitiFact: https://www.politifact.com/  ###reference_www.politifact.com/###) and one Chinese dataset (CHEF555CHEF: https://github.com/THU-BPM/CHEF  ###reference_github.com/THU-BPM/CHEF###).\nThe news in LIAR and PolitiFact are categorized into two distinct classes: real and fake news. The datasets were preprocessed to maintain their original meaning while fitting the task at hand, with key statistics outlined in Table 1  ###reference_###.\n###table_1### We compare our STEEL with  baselines, which can be divided into two groups:\nThe first group (G1) is classical and recent advanced evidence-based methods. G1 contains seven baselines: DeClarE (EMNLP’18) Popat et al. (2018  ###reference_b24###), HAN (ACL’19) Ma et al. (2019  ###reference_b18###), EHIAN (IJCAI’20) Wu et al. (2020  ###reference_b37###), MAC (ACL’21) Vo and Lee (2021  ###reference_b27###), GET (WWW’22) Xu et al. (2022  ###reference_b39###), MUSER (KDD’23) Liao et al. (2023  ###reference_b16###) and ReRead\n(SIGIR’23) Hu et al. (2023  ###reference_b11###).\nThe second group (G2) encompasses methods based on LLMs, either with or without a retrieval component. This group includes four methods: GPT-3.5-turbo OpenAI (2022  ###reference_b21###), Vicuna-7B Chiang et al. (2023  ###reference_b5###), WEBGLM (KDD’23) Liu et al. (2023  ###reference_b17###)and ProgramFC (ACL’23) Pan et al. (2023  ###reference_b22###).\nFor a detailed description of the baseline models, please refer to the Appendix A.2  ###reference_###.\nSince our model does\nnot require a training set, we utilize all the data as\na test set. This approach is also applied to all the\ndatasets we use. In our method, the hyperparameter  is set to . For the LLMs, we set the temperature at , top-p at , and limit prompt tokens to . Hyperparameters for the baseline methods are aligned with those detailed in the respective papers and key hyperparameters are meticulously tuned to achieve optimal performance. We treat fake news detection as a binary classification problem and our evaluation criteria include F1, Precision, Recall, F1 Macro, and F1 Micro  Xu et al. (2022  ###reference_b39###). For more implementation details, see the\nsource code in this repository666https://anonymous.4open.science/r/STEEL-6FD1/  ###reference_D1/###. Besides, cost details can be seen at A.1  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiments Setup",
            "text": "To evaluate the performance of STEEL, we conduct extensive experiments on three real-world datasets, comprising two English datasets (LIAR333LIAR:https://www.cs.ucsb.edu/ william/data/liar_dataset.zip  ###reference_/liar_dataset.zip###  ###reference_/liar_dataset.zip### and PolitiFact444PolitiFact: https://www.politifact.com/  ###reference_www.politifact.com/###  ###reference_www.politifact.com/###) and one Chinese dataset (CHEF555CHEF: https://github.com/THU-BPM/CHEF  ###reference_github.com/THU-BPM/CHEF###  ###reference_github.com/THU-BPM/CHEF###).\nThe news in LIAR and PolitiFact are categorized into two distinct classes: real and fake news. The datasets were preprocessed to maintain their original meaning while fitting the task at hand, with key statistics outlined in Table 1  ###reference_###  ###reference_###.\n###table_2### We compare our STEEL with  baselines, which can be divided into two groups:\nThe first group (G1) is classical and recent advanced evidence-based methods. G1 contains seven baselines: DeClarE (EMNLP’18) Popat et al. (2018  ###reference_b24###  ###reference_b24###), HAN (ACL’19) Ma et al. (2019  ###reference_b18###  ###reference_b18###), EHIAN (IJCAI’20) Wu et al. (2020  ###reference_b37###  ###reference_b37###), MAC (ACL’21) Vo and Lee (2021  ###reference_b27###  ###reference_b27###), GET (WWW’22) Xu et al. (2022  ###reference_b39###  ###reference_b39###), MUSER (KDD’23) Liao et al. (2023  ###reference_b16###  ###reference_b16###) and ReRead\n(SIGIR’23) Hu et al. (2023  ###reference_b11###  ###reference_b11###).\nThe second group (G2) encompasses methods based on LLMs, either with or without a retrieval component. This group includes four methods: GPT-3.5-turbo OpenAI (2022  ###reference_b21###  ###reference_b21###), Vicuna-7B Chiang et al. (2023  ###reference_b5###  ###reference_b5###), WEBGLM (KDD’23) Liu et al. (2023  ###reference_b17###  ###reference_b17###)and ProgramFC (ACL’23) Pan et al. (2023  ###reference_b22###  ###reference_b22###).\nFor a detailed description of the baseline models, please refer to the Appendix A.2  ###reference_###  ###reference_###.\nSince our model does\nnot require a training set, we utilize all the data as\na test set. This approach is also applied to all the\ndatasets we use. In our method, the hyperparameter  is set to . For the LLMs, we set the temperature at , top-p at , and limit prompt tokens to . Hyperparameters for the baseline methods are aligned with those detailed in the respective papers and key hyperparameters are meticulously tuned to achieve optimal performance. We treat fake news detection as a binary classification problem and our evaluation criteria include F1, Precision, Recall, F1 Macro, and F1 Micro  Xu et al. (2022  ###reference_b39###  ###reference_b39###). For more implementation details, see the\nsource code in this repository666https://anonymous.4open.science/r/STEEL-6FD1/  ###reference_D1/###  ###reference_D1/###. Besides, cost details can be seen at A.1  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "Our model, STEEL, was benchmarked against 11 baseline approaches, comprising 7 evidence-based and 4 LLM-based methods. We classified these into two groups: G1 for evidence-based methods and G2 for LLM-based methods. Performance metrics are reported in Tables 2  ###reference_###, 3  ###reference_###, and 4  ###reference_###.\nKey observations from these results include the following.\nSTEEL consistently outperforms state-of-the-art methods in three real-world datasets, with more than a  increase in both F1-macro and F1-micro scores. This also underscores the model’s superior detection capabilities.\nIn a detailed evaluation, we measured the performance of STEEL in three key metrics: F1, Precision, and Recall, classifying real news as positive and fake news as negative. STEEL demonstrated superior performance on these indicators.\nSTEEL surpasses all baselines in the detection of fake news, evidenced by improved detection metrics. For instance, on the LIAR dataset, we observed increases in F1 False, Precision False, and Recall False by , , and , respectively. Comparable significant gains were noted on other data sets.\nThe collective evidence affirms that STEEL is highly effective in detecting fake news, with significant advantages in both reasoning and accuracy."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Internet Search Comparison Study",
            "text": "To evaluate the relative effectiveness of our research mechanism compared to other methods in terms of improving the quality of evidence retrieval, we conducted a comparative experiment.\nThe results are presented in Table 5  ###reference_###.\n\"Re-search\" represents our proposed scheme.\nThe alternative methods used for comparison involve single searches.\n\"Direct search\" denotes the scenario where claims are directly used as queries for evidence retrieval. \"Search with Keywords\" involves the extraction of key terms from the claims before searching. \"Search after Paraphrase\" entails paraphrasing the claim before searching.\n###table_3### The results indicate that while certain conventional retrieval optimization methods employed by search engines, including keyword search and paraphrasing, offer improvements over the straightforward use of the claim as a query, their effectiveness remains notably inferior to that of the re-search module.\nThis discrepancy arises from the fact that evidence obtained in a single search is insufficient to make a conclusive judgment. The results illustrate the important role of the re-search module in our framework."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Optimal Parameters in Evidence Selection",
            "text": "To enhance the quality of evidence post-retrieval, we experimented with two key parameters, the number of document links () and length of the evidence (). As shown in Table 6  ###reference_###, the most significant improvement was achieved when  and . This aligns with our expectation that comprehending and reasoning about a statement benefit from comprehensive and detailed information compared to fragmented or limited snippets."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Explainability Study",
            "text": "###figure_6### Case study\nIn this section, we demonstrate the performance of our model in generating explanatory text.\nAs shown in Figure 6  ###reference_###, we provide a specific example where a news claim asserted, \"Says House Democrats voted to use your tax dollars for abortions by voting against bill defunding Planned Parenthood.\" Through the extraction of key evidence and coherent reasoning, our model effectively identified this news claim as false. More notably, our model is capable of reorganizing reasoning, utilizing complete evidence to craft human-friendly explanatory responses. Furthermore, it can attribute the generated text, distinguishing between factual information and generated content. This significantly enhances interpretability, benefiting both the model’s understanding and the user’s comprehension.\n###table_5### User study\nWe assess whether real-world users can accurately discern the veracity of news claims using evidence obtained from STEEL. We selected  claims from the CHEF and LIAR datasets, including  authentic and  false claims from each, and compared the quality of evidence provided by our STEEL model with that of MUSER. We hired  college students to rate the evidence. To ensure methodological rigor, participants evaluated a randomized set of claims independently, without interaction.  participants evaluated the evidence quality, reviewing either MUSER or STEEL-retrieved evidence for each claim and determining its truthfulness within a 3-minute timeframe. Participants also rated their confidence using a 5-point Likert scale. The results, depicted in Table 10  ###reference_###, unequivocally demonstrate the superior performance of STEEL in evidence retrieval quality over MUSER."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we present an out-of-the-box, end-to-end framework designed for fake news detection that centers around retrieval-augmented LLMs. Our work is a preliminary attempt to address systemic risks in the field of fake news detection, It has been proven that fully leveraging LLMs can aid individuals in identifying fake news by assisting in the gathering of ample evidence and facilitating judgment by end users.\nConsidering the intricate challenges associated with identifying fake news, there is a significant need for the future to extend the framework’s capabilities to encompass multimedia-based fake news, incorporating strategies to analyze and interpret information across text, images, videos, and audio. Addressing these areas will not only improve the accuracy and reliability of fake news detection but also broaden its applicability.\nOur study is constrained by two factors that warrant attention. A significant limitation of our methodology lies in the simplistic nature of the filtering algorithm utilized to identify fraudulent news sources. Currently, in the preprocessing of evidence, we employ a static blacklist to filter out recognized sources of disinformation. However, given the vast scale and rapid evolution of digital content, this approach may prove insufficient. We advocate for further investigation into this issue and the development of more advanced and diverse methods, including built-in mechanisms, for detecting and excluding counterfeit news outlets.\nAdditionally, the restricted context length of the input text poses another challenge, as it may not capture all relevant information adequately. This limitation underscores the need for additional research into the implications of context length restrictions within the domain of LLMs. Such exploration is essential for understanding their impact on efficacy and for identifying viable strategies for improvement.\nMoreover, the technical quality of our method is hampered by the limited computational power available for fine-tuning current Large Language Models (LLMs). Nevertheless, we present a novel approach using existing LLMs with retrieval techniques for fake news detection, thereby laying the groundwork for future research endeavors."
        }
    ]
}