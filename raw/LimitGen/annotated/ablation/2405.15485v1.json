{
    "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
    "abstract": "We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems. In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics. Motivated by the Neural Tangent Kernel (NTK), we propose NTKEval to assess changes in LLM’s probability distribution via training on different kinds of math data. Our systematic analysis finds evidence of domain understanding during in-context learning. By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language inference tasks (Achiam et al., 2023  ###reference_b1###; Touvron et al., 2023a  ###reference_b27###; Chowdhery et al., 2023  ###reference_b12###; Anil et al., 2023  ###reference_b3###; Touvron et al., 2023b  ###reference_b28###; Team et al., 2023  ###reference_b26###). With the promising success, there is a growing trend to use LLM as creative assistants for scientific discovery: from mathematics (Trinh et al., 2024  ###reference_b29###) to biology (Madani et al., 2023  ###reference_b20###).\nMotivated by the use of LLM as a scientific assistant, our paper assesses the domain knowledge of LLMs through their understanding of different mathematical skills required to solve problems. Understanding can be measured in two ways: the degree to which it solves problems correctly; and the degree to which it enables fast adaptation to new knowledge. Similarly, “understanding” in an LLM has two facets: on the one hand, pre-trained LLMs possess knowledge that allows remarkable performance in zero-shot tasks; on the other hand, pre-trained LLMs can learn new knowledge, either by leveraging in-context learning or by instruction-tuning from base parameters as initialization. While most evaluation focuses on measuring what the model knows already, we focus on evaluating an LLM’s mathematical understanding by studying how they learn: Has the model learnt to learn effectively about mathematics? Can it make good use of relevant information present during learning? In this sense, our interest is in evaluating LLMs from a learning-to-learn, or meta-learning perspective.\nHumans successfully acquire mathematical and reasoning skills when they are able to identify underlying structure in problems rather than paying attention to spurious signals present in the question formulation — a phenomenon often coined as deep versus surface learning in education science (Chin & Brown, 2000  ###reference_b11###). This suggests that an analogous way to evaluate a machine learning model’s understanding of mathematics is to ask whether it is able to exploit similarities in deep structure as it generalizes from a training example to a test situation. In gradient-based learning, the extent to which information about one input is generalized to another is captured by the object called Neural Tangent Kernel (NTK) (Jacot et al., 2018  ###reference_b17###). Although the NTK was mainly introduced as a theoretical tool, it has also found uses in interpretability research (Engel & Wang, 2023  ###reference_b14###).\nThis paper proposes a NTK-inspired method to evaluate LLM’s change in probability distribution during training and investigates: do LLMs learn to answer math problems based on an understanding of the skill (deep structure) required to solve the problem or by gathering clues from surface changes in presentation formats? For example, to solve a subtraction problem, LLMs’ performance increase may be due to noticing the train and test question shares the same symbolic format (, what is ?) instead of eliciting the knowledge of subtraction for problem-solving. Figure 1  ###reference_### shows more examples of different presentation formats considered.\nWe assess the impact on LLM’s accuracy in answering math problems when LLMs see different groups of examples that relate to the test question: one group shares the same deep structure described by core math skill, and the other group shares the same presentation format. This is based on the intuition that if LLMs learn beyond pattern matching, then seeing deep structures should induce larger relative improvement than seeing surface structures. Subsequently, we analyze, for a fixed presentation format, how seeing different math skills affects performance on targeted and different test examples. For instance, we measure accuracy improvement (or decline) in how the LLM sees addition examples affect solving addition problems versus subtraction problems, to determine LLM’s ability in fast adaptation.\nIn-context learning (Brown et al., 2020  ###reference_b7###) and instruction-tuning (Zhou et al., 2023  ###reference_b35###) have elicited emergent abilities of LLMs: from improvements in reasoning (Wei et al., 2022a  ###reference_b31###, b  ###reference_b32###) to generalization beyond training dataset (Wei et al., 2021  ###reference_b30###). In this paper, we analyze from the two perspectives and our contributions and findings are summarized below:\nWe propose NTKEval in Section 4  ###reference_###, extending NTK to language models when outputs are chat completions, and demonstrate the sample efficiency of NTKEval compared to standard metric in counting accuracy differences in Section 6.1  ###reference_###.\nWe introduce the KhanSkill dataset (Section 5  ###reference_###) consisting of human-annotated mathematical concepts that can help analyze the alignment of LLMs’ mathematical understanding with human learning.\nOur systematic analysis in Section 6.2  ###reference_### and 6.3  ###reference_### finds that in-context learning differentiates deep versus surface structures (Table 3  ###reference_###) and learned to effectively use relevant math skills (Figure 5  ###reference_### Top), whereas instruction-tuning on skill-focused dataset leads to similar performance change irrespective of training on different data types (Table 4  ###reference_### and Figure 5  ###reference_### Bottom) — suggesting the adaptation is on format matching rather than domain understanding."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "LLMs for Math problem solving Current breakthroughs, e.g., OPRO (Yang et al., 2023  ###reference_b34###), AlphaGeometry (Trinh et al., 2024  ###reference_b29###), FunSearch (Romera-Paredes et al., 2024  ###reference_b23###), in AI for mathematics were driven by the idea of self-play where LLM acts as a creative assistant to provide useful hints. The advantages brought by language model relies more on its general understanding across domain concepts rather than its ability to precisely execute manual tasks — which is often outsourced to reliable external tools (Reed et al., 2022  ###reference_b22###; Schick et al., 2023  ###reference_b25###). This motivates our study to assess the domain understanding of LLMs through their ability to adapt when seeing different math skills.\nNeural Tangent Kernel (Jacot et al., 2018  ###reference_b17###) is central to understanding the generalization properties of ANNs — it shows with infinite width network, the kernel is deterministic with respect to model architecture rather than parameter initialization and stays constant during training (Weng, 2022  ###reference_b33###). Much of the existing literature (Bietti & Mairal, 2019  ###reference_b6###; Alemohammad et al., 2020  ###reference_b2###; Chen et al., 2020  ###reference_b10###) focuses on analyzing the theoretical properties of NTK in various architectures, with little connections to language models. A more related work (Malladi et al., 2023  ###reference_b21###) studies whether NTK describes the fine-tuning process of language models where they formulate downstream tasks as masked word prediction problems through prompting. In this paper, we instead propose a sample-efficient method that allows outputs to be free-form completions. We utilize the proposed method to study how effectively models can learn through training on relevant data.\nSkill Arora & Goyal (2023  ###reference_b4###) studies skill emergence in language models from a statistical framework. Chen et al. (2023b  ###reference_b9###) selects training data based on skill ordering. Chen et al. (2023a  ###reference_b8###) introduces SkiC prompting to encourage skill compositions."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Language Model",
            "text": "A language model (LM) is a statistical model of natural language. Given a sentence  with tokens , the probability of the sentence in a LM parameterized by  can be represented as a chain of conditional probability conditioned on all the previous tokens:\nPre-training a language model amounts to likelihood maximization for the probability of next token prediction  on a held-out dataset.\nIn-context learning LM is given a -shot example of context at inference time before the context of the test question, which the LM is expected to complete.\nInstruction-tuning Given a dataset of questions and ground truth answers  (such dataset is what this paper focuses to analyze) and denoting the tokens in -th question as  and in the corresponding answer as . Instruction-tuning or supervised fine-tuning a LM  on the dataset means minimizing the loss function"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Neural Tangent Kernel",
            "text": "Given a neural network  with parameter , for an input pair , NTK quantifies the changes in  at point  when updating an infinitesimal gradient step in the direction of training on data point . Mathematically, the kernel can be written in two equivalent forms, where  is the learning rate:\nIn this work, we inherit Eq. (2  ###reference_###) for kernel calculation since language models usually have parameters in the scale of (tens of) billions, and matrix multiplications with billion entries are computationally expensive."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "This paper proposes a NTK-inspired method NTKEval to evaluate changes in probability when training on a skill-focused dataset. We expect changes in probability would be faster in capturing training effects that are otherwise computationally expensive to reflect in accuracy changes. We focus on analyzing math problems with deterministic solutions. Each data point consists of a triplet of skill, question, and answer, where skill describes a feature of the question and the answer may contain CoT reasoning (Wei et al., 2022b  ###reference_b32###) where an exact solution is extractable. Let  represent batches of data points corresponding to skill,  and  respectively. Here a skill can either be a math skill (deep structure) or a type of presentation format (surface structure). Our goal is to learn the language model’s NTK in terms of skills, .\nComputing neural tangent kernel in language models faces three challenges:\nNTK is designed to tackle regression tasks, where  is a numerical value. Language models instead generate free-form completions based on a prompt. To address the lack of target numerical outcome, we use the probability of generating the correct solution given prompt, i.e.,  as the target value.\nTo compute , we marginalize the CoT reasoning effect. Formally, given a prompt , the model generates -th completion , where  is the CoT reasoning and  is the deterministic solution. Let the ground truth solution be . Then,\nwhere  is the number of generations given prompt .\nHowever, language models are known to have high variability in free-form generations when sampling from the complete probability distribution. Therefore, sampling multiple generations in each model and marginalizing out chain-of-thought reasoning, though theoretically correct, in practice will be computationally expensive as the number of generations required is huge. Here we take the importance sampling approach: use the generations in the base model and calculate the counterfactual probability of the new model generating the same completion. This allows fair comparisons of the same inputs between different models and requires fewer computation resources. Formally, let model  be the base model and model  be the instruction-tuned model on a data batch that shares the same skill, with probability distributions  respectively. The objective is to calculate the difference in probability of generating correct solutions between model  and  given fixed prompt , i.e., . Importance sampling based score leads to\nwhere . Note we adapt the indicator function slightly to take into account the changes when model outputs wrong answers. Algorithm 1  ###reference_### details the exact procedure."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Datasets",
            "text": "Synthetic dataset To allow control in isolating changes on variables of interest, this paper focuses the analysis on synthetic datasets. Each question tests one math skill. We first create one dataset consisting of questions testing four elementary math skills (‘deep structure’) and presented with four types of formats (‘surface structure’). The four elementary skills are addition, subtraction, multiplication, and division and the four types of presentation formats are question, instruction, symbolic, and word problems. Figure 1  ###reference_### (right) shows example formats for a given math problem.\nTo understand whether LMs are effective in learning to learn through seeing different math skills, we fix the presentation format to style ‘question’ and create the second dataset with questions that require understanding from elementary to more complex skills. In particular, we add questions testing understanding on the order of operations, and the utilization of the mixture of all aforementioned skills (‘complex’). Figure 1  ###reference_### (left) shows example questions with abbreviated skill names. Finally, we include a baseline that outputs random integers irrespective of the question presented.\nKhanSkill dataset\nDue to a lack of skill-annotated benchmarks, we generate expert-written questions for educational purposes from khan-exercises111https://github.com/Khan/khan-exercises that reflect human understanding. The dataset consists of  skills with  questions per skill. The training set consists of  questions and the test set consists of  questions evenly split across skills. Appendix A  ###reference_### details sample skills and questions contained in the dataset.\n###figure_1###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Model choices\nWe evaluate our experiments on Code Llama 7b (Roziere et al., 2023  ###reference_b24###), Llemma 7b (Azerbayev et al., 2023  ###reference_b5###) and either Mistral 7b (Jiang et al., 2023  ###reference_b18###) or Mixtral 8x7b Instruct (Jiang et al., 2024  ###reference_b19###). The choice of open-sourced models allows both inference and/or instruction-tuning done on a single GPU. We include a suite of LLMs tailored for code, mathematics, and SOTA general-purpose chat model in order to test specialized models’ domain understanding.\nDataset Evaluation Table 1  ###reference_### records the accuracy for different LLMs evaluated on synthetic dataset from elementary to complex skills. We observe that the more complex skills (e.g. OPS and CPLEX) required to solve the problem the lower the accuracy across all models. Table 2  ###reference_### reports the accuracy for different LLMs evaluated on KhanSkill dataset. We compare against standard benchmarks GSM8K (Cobbe et al., 2021  ###reference_b13###), MATH (Hendrycks et al., 2021  ###reference_b16###) to assess its difficulty level. We observe KhanSkill is easier than MATH and harder than GSM8K. In particular, Llemma-7b outperforms Mistral-7b on all three datasets in Table 2  ###reference_###.\n###figure_2###"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Can LLM understand different math skills?",
            "text": "We assess LLMs’ learning-to-learn ability when seeing relevant examples grouped by skills affecting utilizing the target skill and different skills in test time. Setup For each elementary to complex skill, we prompt LLMs with 8 in-context examples grouped by skill or instruction-tune the LLM with skill-specified data and evaluate the model on the overall test dataset stratified by skills. Figure 3 shows ICL matrix that records changes in accuracy compared to standard prompting when in-context examples are grouped as column-specified skills and evaluated on row skills. We observe all models display a positive diagonal line for the majority of skills. This suggests giving relevant examples during inference time, LLMs in general are fast in learning, with performance improvement on the corresponding examples during test time. As expected for LLM tailored for mathematics, Llemma-7b exhibits the most clear positive diagonal line, suggesting the clear differentiation from the targeted mathematical skill with the other relevant but misleading skills."
        },
        {
            "section_id": "6.3.1",
            "parent_section_id": "6.3",
            "section_name": "6.3.1 Can LLM distinguish targeted skill from off-diagonal skills?",
            "text": "To distinguish whether the ability for fast adaptation (i.e., improvement) observed in Figure 3 and 4 is not due to shared factors (e.g., formatting), we compare the scale of improvement when prompting / instruction-tuning between targeted and off-diagonal skills. Targeted skill refers to the skill that is the same as the test question. Off-diagonal skills refer to skills related to but distinct from the test questions. For example, if the target question requires skill ‘addition’, then off-diagonal skills include but are not limited to ‘subtraction’.\n\nFigure 5 (Top) shows average changes in accuracy when performing targeted skill prompting (top left) and off-diagonal skill prompting (top right) compared to the standard prompting on both synthetic and KhanSkill dataset. Figure 5 (Bottom) shows average changes in the probability of generating correct solutions when instruction-tuning on targeted skill (bottom left) and off-diagonal skills (bottom right) compared to the base model on a synthetic dataset. The x-axis shows skill difficulty levels measured by accuracy under 8-shot random in-context examples for each corresponding model.\n\nObservations Figure 5 (Top) shows clear advantages in relative accuracy improvement, whereas (Bottom) shows similar performance changes between seeing targeted skill (Left) versus off-diagonal skills (Right). This suggests the qualitative improvements observed in in-context learning (Figure 3) are not due to irrelevant factors but stem from the differentiation between different mathematical skills. By contrast, instruction-tuning does not demonstrate relative advantages when trained on the targeted skill versus off-diagonal skills. This suggests the qualitative performance improvement (Figure 4) is driven not by differentiation on relevant skills but rather by common features shared across all tested skills (e.g., presentation format)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed a method NTKEval to extend computations of neural tangent kernel beyond regression tasks to language model’s outputs as free-form generations (Section 4  ###reference_###). Figure 2  ###reference_### demonstrates the sample-efficiency of NTKEval across all models. We next utilize NTKEval and investigate the capability of LLMs to answer math problems beyond pattern matching through query after in-context learning and instruction-tuning. Experiment results show in-context learning differentiates deep structures from surface structures (Table 3  ###reference_###), by contrast instruction-tuning does not (Table 4  ###reference_###). Both ICL and IT are capable of learning to learn (Fig. 3  ###reference_### and 4  ###reference_###), but in-context learning identifies targeted math skills from the others (Figure 5  ###reference_### Top), whereas instruction-tuning does not (Figure 5  ###reference_### Bottom). Overall, we find that ICL exhibits domain understanding, whereas certain instruction-tuning leads to similar performance change irrespective of training on different data.\nWe only considered QA data rather than open-ended texts. We investigated datasets grouped by one skill rather than a mixture of diverse data. Through investigating whether and what method elicits LLM’s domain understanding, we hope to help design better and more transparent scientific assistants."
        }
    ]
}