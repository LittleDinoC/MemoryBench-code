{
    "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?",
    "abstract": "By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks.\nHowever, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages.\nIn this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions.\nWe initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions.\nFurthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context.\nOur investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents.\nBesides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions.\nResources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Despite the impressive capabilities of Large Language Models(LLMs) (Brown et al., 2020  ###reference_b5###; Ouyang et al., 2022  ###reference_b25###; Chowdhery et al., 2023  ###reference_b9###) when accomplishing a wide range of tasks, their effectiveness is compromised by inherent limitations rooted in their limited parametric memory, resulting in instances of hallucination or inaccurate responses (Shuster et al., 2021  ###reference_b30###; Ji et al., 2023  ###reference_b17###).\nAugmented with external retrievers, LLMs demonstrate superior performance by retrieving from external knowledge sources (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b14###; Borgeaud et al., 2022  ###reference_b4###; Izacard et al., 2023  ###reference_b16###).\nHowever, current retrieval systems are not always reliable since they often provide top-ranked passages indiscriminately that still contain irrelevant information (BehnamGhader et al., 2023  ###reference_b3###; Asai et al., 2024  ###reference_b1###).\nIn real-world Retrieval-Augmented Generation (RAG) applications, retrievers are facing more complex forms of irrelevant information (Cuconasu et al., 2024  ###reference_b10###).\nAlthough such irrelevant information scores highly on similarity metrics and may be semantically related to the context, it is irrelevant to answering questions.\nEven worse, irrelevant information may cause LLMs to change what they have believed, leading to a fabricated answer (Wang et al., 2023  ###reference_b32###).\nIn Figure 1  ###reference_###, we give an example to show how such related irrelevant information might distract LLMs, as the misleading information may prompt LLMs to engage in over-reasoning (Hou et al., 2024  ###reference_b15###; Chiang & Lee, 2024  ###reference_b8###).\nIn this work, we study the robustness of LLMs to irrelevant information.\nTo be specific, we seek to answer the question: \nHow well do current LLMs perform when encountering irrelevant information, particularly when it is semantically related?\nTo answer this question, we adopt question answering (QA) tasks for fundamental experiments due to their prevalence in real-world RAG applications (Gao et al., 2023  ###reference_b12###).\nWe first introduce a framework to construct irrelevant information that ranges from semantically unrelated, partially related, and related to questions, and give an analysis that our irrelevant information exhibits high quality, with similarity scores comparable to those of the top-ranked information from Wikipedia, which is easily retrieved by RAG systems.\nWe then systematically assess the robustness of LLMs when faced with irrelevant information, examining their performance under various conditions. We highlight our key findings:\nCompared to common semantically unrelated irrelevant information, LLMs are more likely to be misled by irrelevant information that is highly semantically related.\nWith the increment of irrelevant information quantity, LLMs are less capable of identifying truly relevant information and are more easily distracted.\nThe robustness of LLMs to irrelevant information varies with the question format, with the free-form format proving to be the most robust.\nCurrent strategies intended to improve LLMs’ discrimination capabilities result in only marginal, and sometimes even detrimental, enhancements in their ability to accurately identify and disregard irrelevant information.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) demonstrates impressive abilities in a wide range of knowledge-intensive tasks (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b14###; Borgeaud et al., 2022  ###reference_b4###; Izacard et al., 2023  ###reference_b16###).\nLLMs utilize retrieval systems to navigate through external knowledge bases and identify a set of potentially relevant documents, thereby extending beyond the limitations of their parametric memory.\nSpecifically, leveraging dense retriever models (Karpukhin et al., 2020  ###reference_b18###; Gautier et al., 2022  ###reference_b13###) and in-context learning (ICL) (Brown et al., 2020  ###reference_b5###), retrieval-augmented approaches have shown to be remarkably effective in enhancing the capabilities of LLMs (Luan et al., 2021  ###reference_b21###; Mallen et al., 2023  ###reference_b22###; Ram et al., 2023  ###reference_b26###; Shi et al., 2023b  ###reference_b29###).\nNonetheless, a challenge persists in the practical deployment of RAG systems, as they indiscriminately surface top-ranked documents that still include irrelevant distractions (BehnamGhader et al., 2023  ###reference_b3###; Wang et al., 2023  ###reference_b32###; Asai et al., 2024  ###reference_b1###; Cuconasu et al., 2024  ###reference_b10###).\nThis issue undermines their utility in real-world applications, where precision and relevance in information retrieval are critical for decision-making processes, such as in medical diagnoses (Zhou et al., 2023  ###reference_b38###).\nThe presence of irrelevant information can lead to inaccurate outcomes, highlighting the need to enhance the reliability of RAG systems."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robustness to Irrelevant Information",
            "text": "Robustness, which refers to a system’s stability when confronted with unexpected inputs (Chang et al., 2023  ###reference_b6###), has been extensively evaluated in previous studies on LLMs(Zhu et al., 2023  ###reference_b39###; Chen et al., 2024  ###reference_b7###).\nGiven its potential to significantly impact model performance, irrelevant information has also attracted attention in the community (Shi et al., 2023a  ###reference_b28###).\nPrior studies (Shi et al., 2023a  ###reference_b28###; Wu et al., 2024  ###reference_b34###) add specific instruction into prompts, enabling LLMs to better solve math word problems by automatically verifying the irrelevant content within problem descriptions.\nThis approach can be combined with Chain-of-Thought (CoT) prompting methods (Wei et al., 2022  ###reference_b33###; Kojima et al., 2022  ###reference_b19###).\nHowever, these investigations primarily focus on irrelevant problem descriptions in arithmetic reasoning.\nIn contrast, the challenge of irrelevant information in RAG applications arises more often from retrieved passages.\nPrevious studies often classify low-ranked passages, random passages, and top-ranked passages without ground truth answers as irrelevant information (Yoran et al., 2023  ###reference_b36###; Wang et al., 2023  ###reference_b32###; Yu et al., 2023  ###reference_b37###; Chen et al., 2024  ###reference_b7###).\nNonetheless, current advanced RAG systems may effectively filter out such content (Askari et al., 2023  ###reference_b2###).\nIn the real-world scenario, however, semantically related yet irrelevant information, which is highly likely to be retrieved by current systems, remains a challenge.\nTo bridge this gap, our work meticulously constructs high-quality irrelevant information and offers a comprehensive analysis of LLM performance across various scenarios.\nThis method enhances our understanding of LLMs’ interactions with irrelevant information, thereby providing valuable insights for improving the efficiency and effectiveness of RAG systems."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "Given the widespread use of question answering (QA) tasks in real-world RAG applications (e.g., New Bing), following previous work (Yoran et al., 2023; Wang et al., 2023; Yu et al., 2023), we employ QA tasks as the foundation for our experiments. Specifically, we focus on entity-centric QA since it is prevalent in RAG scenarios. \n\nPopQA (Mallen et al., 2023): This entity-centric QA dataset comprises questions, derived from fact (subj, relationship, obj) triples of 16 relationship types in Wikidata. For example, the question, “In what city was Julius Erving born?”, is derived from (Julius Erving, place of birth, New York City) triples. \n\nEntityQuestions (Sciavolino et al., 2021): To encompass a wider range of question types in application scenarios, we adopt another widely used entity-centric QA dataset EntityQuestions to broaden the diversity. We exclude relationships that were previously addressed in PopQA to minimize redundancy, yielding 17 distinct relationship types within this dataset. Aligning with the scale of PopQA, we randomly sample 1,500 entries in each relationship for subsequent experiments. Please refer to Appendix A.1 for more details."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Parametric Memory Elicitation",
            "text": "To rigorously evaluate whether LLMs are distracted by irrelevant information, it is essential to first assess their previously internal knowledge free from disturbances. Specifically, following Xie et al. (2023 ###reference_b35###), through closed-book QA format, we extract answers to questions from QA datasets, as well as the corresponding parametric memory from LLMs. For instance, as shown in Table 1 ###reference_###, given a question, “In what city was Julius Erving born?”, LLMs are guided to provide a memory answer “New York City” along with background details. Furthermore, the elicited parametric memory will serve as one of the pieces of relevant information in the subsequent experiment, leveraging LLMs’ inherent confirmation bias to trust their parametric memory (Xie et al., 2023 ###reference_b35###), enhancing the credibility of findings within RAG systems that use LLMs as foundational models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Graded Irrelevant Information",
            "text": "Previous research has shown that LLMs can be easily distracted by irrelevant information, where even information with no relation to the topics of the questions they address can mislead LLMs (Shi et al., 2023a  ###reference_b28###). However, there is a lack of detailed analysis concerning the degree of semantic relevance of irrelevant information that affects the performance of LLMs. To address this gap, we introduce a framework for categorizing irrelevant information into three graded levels, aiming to explore its impact in depth. Specifically, as shown in Figure 2  ###reference_### we define three distinct levels of irrelevant information: Unrelated Information, Partially Related Information, and Related Information. Given the vast amount of information stored in databases, retrieving passages with high similarity scores that are nonetheless unrelated to the question topic is inevitable. We categorize such information as Unrelated Information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Analysis",
            "text": "In this section, we focus on assessing the robustness of LLMs when faced with irrelevant information, examining their performance under various conditions. To be specific, we explore the issue from four distinct perspectives:\n\n1) Semantic Relevance\n2) Quantity of Information\n3) Question Format\n4) Limitations of Current Solutions.\n\nWe adopt four widely used LLMs for our analysis, including three closed-source LLMs GPT-3.5 Turbo (OpenAI, 2022  ###reference_b23###), GPT-4 Turbo (OpenAI, 2023  ###reference_b24###), and Gemini Pro (G Team et al., 2023  ###reference_b11###), as well as one open-source LLM Llama2-7B (Touvron et al., 2023  ###reference_b31###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Question Format",
            "text": "Considering various question formations in the real world, in this section, we aim to investigate how question formats influence the performance of LLMs with the interference of irrelevant information. Specifically, in addition to the multiple-choice format, we introduce boolean (true/false) and free-form QA to our experiments. In boolean QA, we ask LLMs to judge the truthfulness of a misleading statement (e.g., “Julius Erving was born in Baltimore”). They are considered distracted if they provide a “true” response. In free-form QA, we present questions to LLMs without providing any options. Due to the difficulty in automatically determining precise answers from LLMs’ free-form responses, we utilize GPT-3.5 Turbo to align these responses with specific options. To ensure the accuracy and fairness of GPT-3.5 Turbo’s automatic alignment, we conduct human evaluations on randomly selected cases, achieving a % accuracy rate. This high level of accuracy validates the fairness and reliability of our assessment method. More details are in Appendix B.1 ###reference_###. Such an inconsistent robustness might undermine the truthfulness of RAG systems since the question formats in real-world applications are various. Please refer to Appendix B.2 ###reference_### for an in-depth analysis of the influence of irrelevant answers and case demonstration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce a framework to construct irrelevant information that ranges from semantically unrelated, partially related, and related to questions.\nThe semantically related information exhibits high quality, with similarity scores comparable to human-written information from Wikipedia, which is easily retrieved by RAG systems.\nOur experiments show that current LLMs still struggle with discriminating highly semantically related irrelevant information under various conditions.\nAnd current solutions have limitations in improving the robustness of LLMs to such information.\nWe advocate focused research on mitigating misleading irrelevant interference in the development of reliable RAG systems."
        }
    ]
}