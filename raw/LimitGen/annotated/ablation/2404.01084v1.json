{
    "title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles",
    "abstract": "In this paper, we outline our submission for the SemEval-2024 Task 9 competition: ’BRAINTEASER: A Novel Task Defying Common Sense’. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In Natural Language Processing (NLP), reasoning serves as the cognitive backbone, enabling systems to transcend mere language comprehension and delve into sophisticated understanding.\nDespite the excellence of Large Language Models (LLMs) in several linguistic tasks, their reasoning capabilities are still questionable to a non-negligible extend Floridi and Chiriatti (2020  ###reference_b4###); Bender et al. (2021  ###reference_b1###); Kauf et al. (2022  ###reference_b17###); Zhang et al. (2023  ###reference_b43###); Shi et al. (2023  ###reference_b28###); Tyen et al. (2024  ###reference_b35###); Giadikiaroglou et al. (2024  ###reference_b6###), often posing the fundamental concerns of whether they can indeed reason or memorize exhaustively Yuan et al. (2022  ###reference_b42###).\nSuch limitations can be probed via well-crafted datasets and benchmarks, showcasing varying LLM deficiencies at a time. As the core of the current paper, BrainTeaser Jiang et al. (2023b  ###reference_b16###, 2024b  ###reference_b15###) incorporates problems that stress models to think \"out-of-the-box\"; to this end, the key novelty of BrainTeaser is that in order to answer correctly, models need to defy default senses of concepts and common associations.\nSurprisingly, state-of-the-art (SoTa) LLMs, such as ChatGPT can only exhibit a maximum accuracy of 60% when solving BrainTeaser riddles, demonstrating an inherently limited reasoning ability in unconventional thinking.\nThus, assuming that large-scale training and prompting may not always serve as universally applicable solutions\ntowards flexible reasoning, we move one step back and leverage transfer learning techniques starting from smaller models based on masked language modelling, such as BERT Devlin et al. (2019  ###reference_b3###) and consequent BERT-based encoders. Then, we proceed with similar techniques on LLMs, aiming to showcase that significant performance advancements using a small set of in-domain data for parameter updating can be achieved in comparison to merely querying the model’s prior knowledge via prompting. Therefore, our contributions are:\nWe perform lightweight tuning on smaller encoder models and LLMs, significantly outperforming the reported baselines.\nWe transform the multiple-choice problem to a binary classification one, aiming to explore diverging reasoning paths for models.\nWe ground final performance on the models’ \"prior knowledge\" in related problems.\nWe delve into models’ frequent failures to obtain a deeper understanding of reasoning cues that make models struggle the most.\nOur code is available on GitHub 111https://github.com/GiannisPana/AILS-NTUA-at-SemEval-2024-Task-9-Brainteaser  ###reference_t-SemEval-2024-Task-9-Brainteaser###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "has enjoyed several advancements due to the surge of pre-trained language models and especially LLMs Sun et al. (2023  ###reference_b31###). Reasoning challenges incorporate commonsense reasoning Richardson and Heck (2023  ###reference_b26###), involving inference regarding everyday situations, mathematical reasoning Lu et al. (2023  ###reference_b22###), referring to the ability of solving mathematical problems, logical reasoning Yang et al. (2023  ###reference_b41###), which includes the systematic deduction of conclusions based on established principles and formal rules, causal reasoning Gendron et al. (2024  ###reference_b5###), which studies cause-and-effect relationships explaining why an event leads to another, and several other sub-tasks Vashishtha et al. (2020  ###reference_b36###); Wei et al. (2023  ###reference_b40###); Petersen and van der Plas (2023  ###reference_b25###).\nIn terms of reasoning evaluation, BigBench Srivastava et al. (2023  ###reference_b30###) comprises 204 reasoning tasks, targeting to explore the related capabilities of recent LLMs.\nSeveral dedicated datasets have been developed to tackle different reasoning challenges, including commonsenseQA Talmor et al. (2019  ###reference_b32###), WinoGrande Sakaguchi et al. (2019  ###reference_b27###), RiddleSense Lin et al. (2021  ###reference_b20###) and others; most of these datasets are incorporated in Tasksource Sileo (2023  ###reference_b29###). Especially RiddleSense questions aspects of reasoning close to BrainTeaser Jiang et al. (2023b  ###reference_b16###, 2024b  ###reference_b15###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Task and Dataset Description",
            "text": "The BrainTeaser task at SemEval-2024 (Jiang et al., 2023b  ###reference_b16###, 2024b  ###reference_b15###) features lateral thinking puzzles presented as multiple-choice questions (QAs). Each question offers four options, with one being the correct answer and the others serving as distractors. Additionally, the final option is always \"None of above\". It consists of two sub-tasks, Task A: Sentence Puzzle and Task B: Word Puzzle. In addition to the original puzzles, the dataset includes adversarial subsets created by manually modifying the original brain teasers while preserving their reasoning paths. The original data were perturbed in two ways: First, there is semantic reconstruction of each original question without altering the answers or the distractors. Second, the original data underwent context reconstruction, wherein the original reasoning path remains intact, but the brain teaser describes a new situational context. Overall, the dataset used for training and evaluation consists of triplets of data: original, semantic, and context reconstruction. Table 1  ###reference_### provides an example of the triplets of data that constitute the dataset.\nIn this sub-task, the sentence pairs are crafted in a manner that makes it relatively easy for humans to discern the correct statement, yet challenging for systems, even those equipped with commonsense understanding. Table 2  ###reference_###\ncontains examples of the Sentence Puzzle dataset (on the left).\nThe training data consists of 169 distinct multiple-choice QA sets, each accompanied by its semantic and context reconstructions, resulting in a total of 507 multiple-choice questions ().\ninvolves word-type brain teasers, where the answer defies the default meaning of the word and focuses on the letter composition of the question.\nThe training dataset comprises 132 multiple-choice QAs, each accompanied by its semantic and context reconstructions, resulting in a total of 396 multiple-choice QAs (). These brain teaser categories include puns, homophones, ambiguous words, and various other linguistic puzzles, as showcased in the examples provided in Table 2  ###reference_### on the right-hand side.\nThe Word Puzzle sub-task pose challenges not only for systems but also for humans in discerning the correct answer.\nThe BrainTeaser dataset comprises 3 data splits, namely train, development (used during the practice phase), and the hidden test set, which was used for evaluation. Statisics are provided in Table 3  ###reference_###. Throughout the evaluation phase, the leaderboard was kept concealed.\nBoth sub-tasks are assessed via accuracy metrics to gauge the performance of participating systems in two ways. First, instance-based accuracy evaluates each question individually, considering original questions and their semantic and context adversarials. This metric provides a detailed understanding of a model’s proficiency in reasoning through various scenarios. In contrast, group-based accuracy takes a broader perspective, assessing questions and associated adversarials as cohesive groups. Each group consists of three questions, and a model scores 1 only if it correctly solves all questions in a group. This approach evaluates the system’s holistic performance in navigating through lateral thinking challenges. The combined use of instance-based and group-based accuracy metrics provides comprehensive insights into the capabilities of participating systems in tackling the complexities of both sub-tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We focus on tuning language models belonging into two categories. First, we fine-tune variations of encoder models, namely BERT Devlin et al. (2019), RoBERTa-large Liu et al. (2019) and DeBERTaV3-base He et al. (2023), to assess the impact of transfer learning using various datasets requiring similar reasoning abilities, apart from BrainTeaser. We study the problem using the provided multi-choice setup, but we also transform it into a binary classification task. Secondly, the encoders’ results are compared with those obtained from fine-tuned LLMs using the BrainTeaser dataset. To achieve this, we fine-tune Llama 2 Touvron et al. (2023b), Phi-2 Gunasekar et al. (2023), and Mistral-7b Jiang et al. (2024a), which have already demonstrated enhanced reasoning abilities. In this regard, we examine the effect of the model size on our task, which has already been reported in the literature to significantly influence the reasoning abilities of the models Touvron et al. (2023b); Wei et al. (2022), along with other tuning hyperparameters. Model details are presented in App. A.\n\nThis strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently, these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem. Each sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Encoder models",
            "text": "This strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem. Each sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLMs",
            "text": "We demonstrate an in-depth examination of fine-tuning SoTa LLMs (Llama 2, Phi-2, and Mistral-7b) in the context of multi-class classification. Note that during inference, the models prompted to provide an explanation along with the label. This experimental step, which we have observed to improve the performance of the model, also provides a qualitative identification of flaws in the models’ reasoning process. In our experiments, we explore various combinations of LoRA Hu et al. (2021  ###reference_b12###) and hyperparameters, using values of 16, 32, 64, and 128. For the analysis ahead, LLMs are denoted as model_r_a, reflecting these hyperparameters. Additional technical information, including prompting details and specifics about QLoRA hyperparameters, is available in App. B  ###reference_###, C  ###reference_###, D  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "Our metrics for the Sentence Puzzle sub-task are presented in Table 4  ###reference_### and for the Word Puzzle sub-task in Table 5  ###reference_### along with their baselines. Interestingly, the performance of the binary classification problem is significantly lower than that of the multi-class classification task. Initially, this behavior seemed counterintuitive since it appeared easier to determine whether a question is correct or not than to select the correct answer from four different options. However, this assumption is not accurate. Consider the word riddle: ‘What is the capital in France?\" At first glance, the option ‘F’ seems incorrect, but when considering the options ‘F,’ ‘E’, ‘A’, and ‘None of the above’, ‘F’ emerges as the only correct answer, as it becomes apparent that the question refers to the capital letter rather than the capital city. Therefore, the diverse options provide crucial context to the models, explaining the superior performance of multi-class models. This lack of context is why we refrain from further exploring this methodology across all models in our study. Table 4  ###reference_### illustrates minimal fluctuations among all instance-based metrics. This consistency extends to the associated group-based metrics for all models, highlighting a systematic behavior towards detecting various reasoning paths. This observation holds for both the encoder-based classifiers and LLMs utilized in this sub-task. Sentence puzzles inherently offer more detailed information, enabling models to detect and identify the same reasoning patterns more readily, regardless of changes in context, in contrast to word puzzles, which typically feature shorter contextual statements, presenting a greater challenge for models to discern consistent reasoning patterns.\n\nAnother noteworthy observation from Table 4  ###reference_### is that only Mistral-7b from LLMs is able to surpass the encoder-type networks, while both Llama 2 and Phi-2 consistently scored lower. Unlike Llama 2 and Mistral-7b, Phi-2 has not undergone instruction fine-tuning Gunasekar et al. (2023  ###reference_b9###), which, coupled with the limited number of examples in the BrainTeaser Sentence Puzzle dataset, contributes to its lower performance, as a result of Phi’s incapability to capture the complexities of the BrainTeaser data. In this regard, Mistral-7b, which has already demonstrated superior performance compared to every Llama 2 variation when tested in commonsense reasoning benchmarks Jiang et al. (2023a  ###reference_b13###), is also capable of solving this task more accurately. In Table 5  ###reference_###, we observe a stark contrast in the models’ performance in understanding and detecting reasoning paths when the context changes. There are notable discrepancies in accuracy between original and semantic contexts when compared to context reconstruction, particularly evident in the case of smaller encoder models. Regarding encoders, it is evident that, especially vanilla RoBERTa-large lacks robust commonsense reasoning and struggles to systematically handle ambiguity; in contrast, RoBERTa-large pre-trained on WinoGrande presents competitive performance. This notable enhancement (over 40%) due to WinoGrande pre-training suggests that this particular dataset effectively equips the model with the ability to understand word puzzle-related reasoning complexities, making its scores competitive with DeBERTaV3 in this sub-task, despite the higher DeBERTaV3-base performance over RoBERTa-large in baseline reasoning benchmarks He et al. (2023  ###reference_b10###). Other than that, pre-training on other commonsense reasoning datasets does not significantly improve the overall performance for encoders. Conclusively, apart from WinoGrande the rest of the extra pre-training datasets do not hold reasoning cues close to BrainTeaser’s word puzzles. Regarding LLMs, Mistral-7b notably outperformed all others by a significant margin, even surpassing the 8 times larger model tuned using the same hyperparameters (Mixtral-8x7b). Llama 2 exhibited the worst results regardless of size (7/13 billion) and LoRA hyperparameters (r and a). Conversely, Phi-2 demonstrated relatively better performance, particularly considering its smaller parameter count (2.7 billion) compared to the other LLMs. However, both models performed worse compared to most fine-tuned encoders. This observations strongly confirms that word puzzles possess a distribution that diverges from the analytical commonsense reasoning required for sentence puzzles, entailing a unique set of cognitive demands. Mistral-7b exhibits a trend where higher quality explanations were generated with higher values of lora rank r. However, the top-performing model showcased a configuration with r=16 and a=64. The QLoRA method Hu et al. (2021  ###reference_b12###) explains why our top model has a rank of 16 instead of 128, contrary to common expectations (more details reagrding QLoRA"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we systematically evaluate pre-trained and fine-tuned encoders, along with instruction-tuned Large Language Models (LLMs), against two multi-class classification sub-tasks within the \"BRAINTEASER: A Novel Task Defying Common Sense\". We achieve competitive performance in both sub-tasks, accompanied by a plethora of insights regarding the influence of leveraging in-domain data, the variability model scale and architecture introduce, as well as the examination of diverging reasoning paths. As future work, we will delve into further reasoning patterns LLMs tend to follow with regard to lateral thinking challenges."
        }
    ]
}