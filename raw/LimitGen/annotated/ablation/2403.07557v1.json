{
    "title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
    "abstract": "Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Document summarization, the process of distilling key information from extensive texts, has become indispensable across various real-world applications, propelled by advancements in Natural Language Generation (NLG) Pilault et al. (2020  ###reference_b11###); Ma et al. (2022  ###reference_b9###). The advent of Large Language Models (LLMs) Brown et al. (2020  ###reference_b1###); Ouyang et al. (2022  ###reference_b10###); Touvron et al. (2023  ###reference_b14###) has notably enhanced models’ capabilities to generate natural and factually consistent summaries Chang et al. (2023  ###reference_b2###). However, the rapid evolution in summarization techniques may lead to factually inconsistent summaries which are very close to facts Zhang et al. (2023  ###reference_b16###). Such inconsistencies could pose significant challenges, resulting in hallucinations that traditional detection models struggle to identify. As LLMs evolve, there is a critical demand for more robust methods to detect factual inconsistencies, leveraging the advanced capabilities of LLMs themselves.\nLuo et al. (2023  ###reference_b8###) were among the first to utilize LLMs for the detection of factual inconsistencies, employing a universal zero-shot prompt across various benchmarks in SummaC Laban et al. (2022  ###reference_b7###) and inputting the full document along with its summary into GPT-3.5 for evaluation. Despite these innovations, their approach was limited by the plain application, early GPT-3.5 model’s constraints and a lack of adaptation to the specific requirements of different benchmarks. Consequently, their method did not achieve superior performance compared to existing models, such as those detailed in the SummaC paper.\nThis paper revisits the challenge of inconsistency detection in document summarization through zero-shot inference with LLMs, specifically examining the latest versions of GPT-3.5 and GPT-4 on the SummaC dataset. We aim to set up new LLM-based baselines for research in this domain. Moreover, we introduce a novel methodology, SIFiD (Summary Inconsistency Detection with Filtered Document), designed to significantly enhance the efficiency and effectiveness of factual inconsistency detection. SIFiD focuses on identifying crucial sentences within documents by evaluating their entailment scores or semantic similarity with summary sentences, subsequently retaining only the most relevant sentences for further analysis. This approach not only refines the assessment of factual consistency but also reduces the computational resources required for evaluation by decreasing the number of input tokens.\n###figure_1### Our comprehensive evaluation on the SummaC dataset reveals that, while the updated GPT-3.5 model still falls short of outperforming traditional baseline methods, GPT-4 significantly excels in detecting factual inconsistencies. The integration of SIFiD further amplifies GPT-4’s detection capabilities, highlighting the potency of our proposed method. To support continued research and collaboration in this field, we make our code available open source at Anonymous, fostering advancements and exploration in factual inconsistency detection."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The evaluation of summary factual consistency has traditionally relied on methods such as Question Answering and Question Generation (QAG) Wang et al. (2020  ###reference_b15###); Durmus et al. (2020  ###reference_b3###); Scialom et al. (2021  ###reference_b13###), synthetic classifiers Kryściński et al. (2020  ###reference_b6###), and pairing-based approaches Goodrich et al. (2019  ###reference_b4###); Goyal and Durrett (2020  ###reference_b5###). These methodologies focus on identifying discrepancies between documents and their summaries. Laban et al. (2022  ###reference_b7###) later demonstrated that Natural Language Inference (NLI) could be effectively employed for inconsistency detection at appropriate levels of text granularity, thereby advancing the field of summary inconsistency detection.\nThe emergence of Large Language Models (LLMs) has recently shifted the focus towards integrating these models into the assessment of summary factual consistency. Luo et al. (2023  ###reference_b8###) pioneered the application of GPT-3.5 for this purpose, tailoring prompts to various evaluation tasks including summary factual inconsistency detection, summary ranking, and consistency evaluation. Despite this innovative approach, the early iteration of GPT-3.5, coupled with an insufficient detection methodology, did not yield improvements over conventional techniques in identifying factual inconsistencies.\nIn our research, we revisit the approach proposed by Luo et al. (2023  ###reference_b8###), employing the most recent versions of GPT-3.5 and GPT-4. We integrate these advanced LLMs with our newly developed Summary Inconsistency Detection with Filtered Document (SIFiD) method. This combination aims to enhance the accuracy and efficiency of factual inconsistency detection, leveraging the state-of-the-art capabilities of LLMs to set new benchmarks in the field."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "In this section, we detail our approach to reevaluating summary factual consistency using the latest GPT models and introduce the novel SIFiD method."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "SIFiD",
            "text": "Building on prior research in Summary Inconsistency Detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document), a method designed to enhance detection capabilities by filtering irrelevant content from documents. Inspired by the SummaC methodology, which calculates sentence-level entailment scores to identify factual inconsistencies, SIFiD constructs a relevance matrix to filter out irrelevant sentences, focusing the inconsistency check solely on the filtered document and its summary. An illustrative depiction of this process is presented in Figure 1  ###reference_###.\nGiven a document  and its summary , where  and  represent the  sentence in  and , respectively, and ,  are the total number of sentences in each, we first calculate a relevance matrix :\nHere,  denotes the relevance score between the document-summary sentence pair , computed using either entailment scores as per the SummaC method or semantic cosine similarity via the sentence-transformers library111https://huggingface.co/sentence-transformers.\nSubsequently, we apply max pooling across matrix rows to extract the highest relevance score  for each document sentence. We then establish a threshold  to filter sentences, employing a window method to ensure contextual continuity:\nThis approach retains a sentence  (and its immediate neighbors) if , as demonstrated in Figure 1  ###reference_###, where Sentence 6 is included within the window of Sentence 7.\nThe filtered document  and the summary  are then integrated into the prompt template for evaluation by an LLM. Following Luo et al. (2023  ###reference_b8###), we simply determine factual consistency by identifying whether the LLM’s response contains \"Yes\" (indicating consistency) or \"No\"."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Scorer",
            "text": "We use one of the two distinct scoring mechanisms to evaluate the relevance between document sentences and summary sentences.\nEntailment Scorer: We adopt the entailment scoring approach as proposed by Laban et al. (2022  ###reference_b7###), which utilizes a Natural Language Inference (NLI) model Schuster et al. (2021  ###reference_b12###). The net entailment score is calculated by , where  and  are the initial entailment score and contradiction score directly calculated by the NLI model on . The net entailment score reflects the degree to which the summary sentence is supported by the document sentence without contradiction.\nSemantic Similarity Scorer: For assessing semantic similarity, we leverage the sentence-transformers library to generate embeddings for both document and summary sentences, denoted as  and , respectively. The cosine similarity between these embeddings serves as the measure of semantic similarity, which is , where  quantifies the semantic closeness between the document and summary sentences. This metric enables us to identify and assess the degree of semantic overlap."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "Enhanced Performance with SIFiD on GPT-4. Integrating SIFiD with GPT-4 further improved its performance to 79.9. SIFiD’s selective filtering of sentences enhances document relevance to the summary, simplifying factual inconsistency detection. This approach did not yield similar benefits for GPT-3.5, possibly due to its reduced efficacy in processing less fluent filtered documents.\n\nMixed Results with Chain-of-Thought (CoT). Applying CoT techniques did not uniformly benefit all methods. While GPT-3.5 saw improvements, GPT-4’s performance declined, suggesting GPT-4’s innate proficiency in inconsistency detection without CoT. Additionally, CoT might introduce biases that could negatively influence outcomes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we advance the field of LLM-based summary factual inconsistency detection by evaluating the performance of the latest GPT models, thereby establishing new benchmarks for future research. We introduce SIFiD, a novel, efficient, and effective approach that computes a relevance matrix at the sentence level between the document and its summary. This method filters out irrelevant sentences from the document before employing LLMs for inconsistency detection. Our experimental findings on the SummaC dataset demonstrate that SIFiD significantly enhances the performance of advanced GPT models in detecting factual inconsistencies, highlighting its potential to facilitate more accurate and resource-efficient research in this domain."
        }
    ]
}