{
    "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
    "abstract": "Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DaFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model’s representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.111Our code is available at https://github.com/technion-cs-nlp/DAFair",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction and Background",
            "text": "The presence of social bias in training data presents a significant challenge in the development of language models for real-world applications. While these models possess remarkable capabilities, biases within the data can lead to unfair outcomes. Mitigating these biases is crucial, but it becomes particularly challenging when acquiring or accessing sensitive attribute labels is costly or unfeasible.\nStudies showed that language models have the ability to capture demographic information about the writer, including race or gender, within their representations Caliskan et al. (2017  ###reference_b3###); Zhao et al. (2018  ###reference_b21###). However, this capability can introduce unintended biases, leading to discriminatory outputs De-Arteaga et al. (2019  ###reference_b4###).\n###figure_1### Common approaches for social bias mitigation require explicit annotation of biases for each sample in the data Beutel et al. (2017  ###reference_b1###); Zhang et al. (2018  ###reference_b20###). Recent concept removal methods Ravfogel et al. (2020  ###reference_b15###, 2022a  ###reference_b16###, 2022b  ###reference_b17###); Iskander et al. (2023  ###reference_b9###) have shown promise in addressing social bias by removing sensitive attributes. These approaches rely on training classifiers for predicting the sensitive attribute, and training such classifiers typically requires a significant amount of annotated data.\nA promising line of research has emerged that aims to mitigate bias without relying on explicit information about the biases present in the data. For instance,\nJust Train Twice (JTT) Liu et al. (2021  ###reference_b11###) employs a two-step training process. In the second step, a second model is trained on up-weighed training examples that were misclassified by the first model. Another method is\nBLIND Orgad and Belinkov (2023  ###reference_b14###), which\nintroduces a success detector and down-weighs examples for which the detector accurately predicts the outcome.\nIn this paper, we propose DaFair: Demographics-Agnostic Fairness, a novel approach for mitigating social bias during the fine-tuning process of language models, without relying on demographic information. Our approach aims to ensure equal similarity between the representation of a text and prototypical representations of different demographic groups. For instance, when classifying a biographical text of a person into their profession, our method aims to make the representation of the text equally similar to the representations of both males and females. More concretely, DaFair first defines prototypical texts, such as “This is a biography about a male” and “This is a biography about a female”. It then adds a regularization term that makes the representation of a training example equally similar to the representations of each of the prototypical texts (Figure 1  ###reference_###).\nFurthermore, we extend our approach to scenarios where limited demographic-annotated data is available. In such cases, we obtain the prototypical representation by averaging the sample representations corresponding to each social attribute.\nWe evaluate the effectiveness of DaFair and its extension on two tasks: occupation prediction and sentiment analysis of twitter posts. In these tasks, we investigate the performance of our approach under the settings of limited demographic labels or no labels at all, reflecting real-world scenarios where labeled data is challenging to obtain. The experimental results with two base models demonstrate that our approach outperforms previous approaches that do not rely on demographic information, as well as common approaches with limited data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Assume a dataset  of input texts , main task labels , and sensitive attributes  that correspond to discrete demographic attributes, such as race. This sensitive attribute can either be unobserved during training or available in a small subset of the data. Our aim is to learn a model  that does not rely on the sensitive attribute  in its prediction.\nIn the absence of labeled data, we leverage semantic similarity and define pairs of texts that capture the models’ understanding of text describing different social attribute groups. For example, to represent gender in an occupation prediction task we can use the encoder’s representations of “This biography is about a man” and “This biography is about a woman”. To generate these pre-defined representations, we employ a generative model. We provided ChatGPT OpenAI (2022  ###reference_b12###) with a description of the approach, DaFair, along with a description of each dataset and task, and instructed the model to produce 10 pairs of prototypical texts for each task. The prototypical texts (Tables 7  ###reference_### and 8  ###reference_###) and the full prompt (Figure 4  ###reference_###) are provided in the appendix.\nWhen a limited number of labels are available, we leverage the representations generated by the text encoder to derive data-driven representations for each labeled group. Specifically, we calculate the mean representation of each labeled group using the available labeled samples. We call this method Semi-DaFair.\nWe will assume a binary case for simplicity and denote the pair of representations as .222Our approach can be extended to handle multiple social attribute groups, denoted as ."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Demographic-Agnostic Fairness Approach",
            "text": "Our method, depicted in Fig. 1  ###reference_###, involves several key steps to mitigate social bias. First, we establish multiple representations for each group of sensitive attributes (Section 2.1.1  ###reference_.SSS1###). During fine-tuning, we measure\nsimilarity between the representation of an example and each attribute representation. These similarities are then transformed into a probability distribution. Subsequently, we use the Kullback-Leibler (KL) divergence loss Kullback and Leibler (1951  ###reference_b10###) to compare the predicted probability distribution with a uniform distribution (Section 2.1.3  ###reference_.SSS3###). This loss term encourages the model to mitigate bias by penalizing deviations from a uniform distribution, promoting fair and unbiased predictions.\nIn the absence of labeled data, we leverage semantic similarity and define pairs of texts that capture the models’ understanding of text describing different social attribute groups. For example, to represent gender in an occupation prediction task we can use the encoder’s representations of “This biography is about a man” and “This biography is about a woman”. To generate these pre-defined representations, we employ a generative model. We provided ChatGPT OpenAI (2022  ###reference_b12###  ###reference_b12###) with a description of the approach, DaFair, along with a description of each dataset and task, and instructed the model to produce 10 pairs of prototypical texts for each task. The prototypical texts (Tables 7  ###reference_###  ###reference_### and 8  ###reference_###  ###reference_###) and the full prompt (Figure 4  ###reference_###  ###reference_###) are provided in the appendix.\nWhen a limited number of labels are available, we leverage the representations generated by the text encoder to derive data-driven representations for each labeled group. Specifically, we calculate the mean representation of each labeled group using the available labeled samples. We call this method Semi-DaFair.\nWe will assume a binary case for simplicity and denote the pair of representations as .222Our approach can be extended to handle multiple social attribute groups, denoted as ."
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Social Attribute Representations",
            "text": "We employ two approaches to define representations for social attribute groups, depending on the availability of labels: no labels, or few labels.\nIn the absence of labeled data, we leverage semantic similarity and define pairs of texts that capture the models’ understanding of text describing different social attribute groups. For example, to represent gender in an occupation prediction task we can use the encoder’s representations of “This biography is about a man” and “This biography is about a woman”. To generate these pre-defined representations, we employ a generative model. We provided ChatGPT OpenAI (2022  ###reference_b12###  ###reference_b12###  ###reference_b12###) with a description of the approach, DaFair, along with a description of each dataset and task, and instructed the model to produce 10 pairs of prototypical texts for each task. The prototypical texts (Tables 7  ###reference_###  ###reference_###  ###reference_### and 8  ###reference_###  ###reference_###  ###reference_###) and the full prompt (Figure 4  ###reference_###  ###reference_###  ###reference_###) are provided in the appendix.\nWhen a limited number of labels are available, we leverage the representations generated by the text encoder to derive data-driven representations for each labeled group. Specifically, we calculate the mean representation of each labeled group using the available labeled samples. We call this method Semi-DaFair.\nWe will assume a binary case for simplicity and denote the pair of representations as .222Our approach can be extended to handle multiple social attribute groups, denoted as ."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Ensemble of Representations",
            "text": "Inspired by Stacey et al. (2020  ###reference_b18###), we adopt an ensemble approach by leveraging multiple pairs of representations instead of using a single pair. We denote the ensemble of representations as , where  represents the number of pairs.\nIn the case of pre-defined representations, we use multiple pre-defined pairs that capture different perspectives. For data-driven representations, we divide the labeled data into K partitions and calculate the mean representation for each partition, resulting in K pairs of representations.\nBy incorporating an ensemble of representations, we aim to capture a diverse range of information and perspectives related to biases."
        },
        {
            "section_id": "2.1.3",
            "parent_section_id": "2.1",
            "section_name": "2.1.3 Calculating KL Loss",
            "text": "During fine-tuning, we calculate the similarity between the representation of example  and each pair of attribute representations using dot product:\nThen we apply the softmax function  to obtain the similarity distribution:\nTo calculate the overall KL loss, we compute KL divergence between each of the similarity distributions  and a uniform distribution :\nFinally, we compute the total loss:\nwhere  is the usual cross-entropy loss.\nThe hyper-parameter  adjusts the balance between task performance and fairness, providing flexibility to prioritize either aspect."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We use the Bias in Bios Dataset De-Arteaga et al. (2019  ###reference_b4###).\nThe task involves predicting the occupation of individuals based on their biographical information. The dataset consists of 394K biographies of 28 professions, with gender annotations.\nWe follow the setup of Elazar and Goldberg (2018  ###reference_b6###), who leveraged a Twitter dataset originally gathered by Blodgett et al. (2016  ###reference_b2###). Elazar and Goldberg (2018  ###reference_b6###) used emojis in the tweets to derive sentiment labels for the classification task. Tweets are labeled with sociolects—African American English (AAE) or Standard American English (SAE)—based on the author’s geo-location, serving as a proxy for their racial identity. We work with a subset of 100K samples, consistent with Orgad and Belinkov (2023  ###reference_b14###).\nWe evaluate the model’s accuracy (Acc) on the downstream task to ensure that it has not been significantly affected.\nTo evaluate extrinsic bias, we align with previous work De-Arteaga et al. (2019  ###reference_b4###); Ravfogel et al. (2020  ###reference_b15###) and use the True Positive Rate Gap (TPR-GAP) as the main fairness metric to assess performance disparities across different protected attribute groups. Following the guidelines in Orgad and Belinkov (2022  ###reference_b13###) for a comprehensive evaluation, we also incorporate statistical fairness metrics: Independence, Separation and Sufficiency. The metrics details and calculation procedures are provided in Appendix B  ###reference_###.\nMethod\nOriginal\nJTT\nBLIND\nIn this setting, we explore scenarios where demographic labels are not available. We evaluate the performance of demographic-agnostic methods: JTT, BLIND and DAFAIR.\nAdditionally, we investigate a scenario where we have limited access to demographic labels. In this setting, we apply information removal methods along with Semi-DaFairwhile varying the size of the available demographic-labeled data to analyze their effectiveness.\nWe run each method using 5 random seeds and report the mean and standard deviation of the test results. More details on training setup and evaluation procedures are described in Appendix A  ###reference_###.\nTo perform  tuning without the need for a validation set with demographic annotations, we adopt Orgad and Belinkov (2023  ###reference_b14###)’s strategy that prioritizes selecting the most radical parameter, while ensuring that the downstream task accuracy remains above 0.97 of the original accuracy. More details are described in Appendix A  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks",
            "text": "We conduct experiments on two classification tasks: occupation prediction and sentiment analysis, focusing on social bias related to gender and race.\nWe use the Bias in Bios Dataset De-Arteaga et al. (2019  ###reference_b4###  ###reference_b4###).\nThe task involves predicting the occupation of individuals based on their biographical information. The dataset consists of 394K biographies of 28 professions, with gender annotations.\nWe follow the setup of Elazar and Goldberg (2018  ###reference_b6###  ###reference_b6###), who leveraged a Twitter dataset originally gathered by Blodgett et al. (2016  ###reference_b2###  ###reference_b2###). Elazar and Goldberg (2018  ###reference_b6###  ###reference_b6###) used emojis in the tweets to derive sentiment labels for the classification task. Tweets are labeled with sociolects—African American English (AAE) or Standard American English (SAE)—based on the author’s geo-location, serving as a proxy for their racial identity. We work with a subset of 100K samples, consistent with Orgad and Belinkov (2023  ###reference_b14###  ###reference_b14###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Models",
            "text": "We use two pre-trained text encoders: BERT Devlin et al. (2019  ###reference_b5###) and DeBERTa-V3 He et al. (2022  ###reference_b8###). By considering two diverse tasks and different models, we can evaluate the effectiveness of our approach in mitigating social bias in various contexts and with different model architectures."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Metrics",
            "text": "We evaluate the model’s accuracy (Acc) on the downstream task to ensure that it has not been significantly affected.\nTo evaluate extrinsic bias, we align with previous work De-Arteaga et al. (2019  ###reference_b4###  ###reference_b4###); Ravfogel et al. (2020  ###reference_b15###  ###reference_b15###) and use the True Positive Rate Gap (TPR-GAP) as the main fairness metric to assess performance disparities across different protected attribute groups. Following the guidelines in Orgad and Belinkov (2022  ###reference_b13###  ###reference_b13###) for a comprehensive evaluation, we also incorporate statistical fairness metrics: Independence, Separation and Sufficiency. The metrics details and calculation procedures are provided in Appendix B  ###reference_###  ###reference_###.\nMethod\nOriginal\nJTT\nBLIND"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Compared Methods",
            "text": "We compare our approach with several methods for bias mitigation and with a baseline (Original) without any debiasing procedure.\nWe compare with two existing methods that do not rely on demographic information:\nJTT Liu et al. (2021  ###reference_b11###), which trains in a second phase on up-weighed hard examples.\nBLIND Orgad and Belinkov (2023  ###reference_b14###), which uses a success detector to down-weigh biased examples.\nWhen only limited demographic labeled samples are available, we evaluate three methods:\nINLP Ravfogel et al. (2020  ###reference_b15###) removes linear information from the neural\nrepresentation by iteratively training a linear\nclassifier to predict the demographic attribute from the representation,\nthen projecting the representations\nto the null-space of the linear classifier.\nRLACE Ravfogel et al. (2022b  ###reference_b17###) is similar to INLP with the goal of linear information removal from the neural representations. However, it uses a different approach of a linear minimax game.\nIGBP Iskander et al. (2023  ###reference_b9###) overcome the drawbacks of INLP and RLACE which only remove linearly encoded information, and removes non-linear information from representations by gradient-based projections."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Settings",
            "text": "In this setting, we explore scenarios where demographic labels are not available. We evaluate the performance of demographic-agnostic methods: JTT, BLIND and DAFAIR.\nAdditionally, we investigate a scenario where we have limited access to demographic labels. In this setting, we apply information removal methods along with Semi-DaFairwhile varying the size of the available demographic-labeled data to analyze their effectiveness.\nWe run each method using 5 random seeds and report the mean and standard deviation of the test results. More details on training setup and evaluation procedures are described in Appendix A  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "DaFair Hyperparameters",
            "text": "Under the setting of no demographic labels, there is no validation set to optimize the selection of prototypical texts or the number of pairs. To avoid dependency on the choice of prototypical representations, we first generate  pairs, and within each iteration, we randomly sample  pairs. For all experiments, we set  ,  to capture diverse associations of the training samples with demographic attributes, without relying on an extensive set of pairs. In Section 4.3  ###reference_###, we analyze the impact of  on the model’s performance and assess its implications on fairness and bias mitigation.\nTo perform  tuning without the need for a validation set with demographic annotations, we adopt Orgad and Belinkov (2023  ###reference_b14###  ###reference_b14###)’s strategy that prioritizes selecting the most radical parameter, while ensuring that the downstream task accuracy remains above 0.97 of the original accuracy. More details are described in Appendix A  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": ""
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Effect of Number of Prototypical Texts",
            "text": "To investigate the effect of the number of prototypical text pairs (K) on model performance, we conducted experiments with varying K values of (1, 2, 4, 8). The results presented in Table 2 reveal that all K values contribute to the reduction of the TPR-GAP without affecting accuracy. While larger values of K result in more substantial reductions, the incremental improvements become less significant. These findings suggest that a small K may be sufficient for DaFair."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduced DaFair, a novel approach for mitigating social bias in language models without explicit demographic information. Our method leverages semantic similarity to manipulate the model’s text representations during finetuning to promote fairness. Experimental results on two tasks and under different settings demonstrated the effectiveness of DaFair in reducing bias and improving fairness while maintaining competitive downstream task performance, even with limited or no labeled demographic data. With its focus on social bias, DaFair offers a flexible framework adaptable to address other forms of bias through the modification of prototypical texts.\nIn conclusion, our approach offers a practical and flexible solution for bias mitigation in real-world applications, contributing to the development of fairer language models."
        }
    ]
}