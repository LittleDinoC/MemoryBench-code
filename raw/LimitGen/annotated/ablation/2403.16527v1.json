{
    "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
    "abstract": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry.\nThe majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based.\nWhile these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.\nThe rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide “common sense” reasoning that existing planners are missing.\nResearchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.\nLarge language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment.\nWhile this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor.\nWe argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model’s decision, and detect when it may be hallucinating.\nIn this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "It is an exciting time to be a machine learning and robotics researcher.\nA great deal of progress has been made in the last decade and a half with regards to the efficacy and efficiency of models for perception, decision-making, planning, and control Soori et al. (2023  ###reference_b119###); Janai et al. (2020  ###reference_b53###).\nBroadly speaking, approaches to these problems fall under one of two umbrellas: hand-engineered model-based systems and data-driven learning-based models Formentin et al. (2013  ###reference_b36###).\nWith some deployment scenario in mind, developers may hand-engineer rules Hayes-Roth (1985  ###reference_b43###) or tune a controller Borase et al. (2021  ###reference_b6###) to be tested, or in the case of learning-based models, collect training data and craft some reward function to fit a model to an objective, given said data Henderson et al. (2018  ###reference_b45###).\nIn practice, these methods work particularly well in the scenarios that they were specifically designed and trained for, but may produce undesirable results in previously unseen out-of-distribution cases Wen et al. (2023  ###reference_b133###).\nDesigners may choose to add more rules, re-tune their controller, fine-tune their model to a more representative dataset, fix the reward function to handle edge cases, or even add a detector (which may itself be rule-based or data-driven) at test-time to identify out-of-distribution scenarios before calling on the decision-maker Singer and Cohen (2021  ###reference_b116###); Schreiber et al. (2023  ###reference_b108###); Chakraborty et al. (2023  ###reference_b11###).\nHowever, even with these changes, there will always be other situations that designers had not previously considered which will come about during deployment, leading to sub-optimal performance or critical failures.\nFurthermore, the modifications made to the model may have unforeseen effects at test-time like undesired conflicting rules Ekenberg (2000  ###reference_b33###) or catastrophic forgetting of earlier learned skills Kemker et al. (2018  ###reference_b58###).\nInformally, classical methods and data-driven approaches lack some form of common sense that humans use to adapt in unfamiliar circumstances Fu et al. (2023a  ###reference_b38###).\nMore recently, researchers are exploring the use of large (visual) language models, L(V)LMs, to fill this knowledge gap Cui et al. (2024  ###reference_b23###).\nThese models are developed by collecting and cleaning an enormous natural language dataset, pre-training to reconstruct sentences on said dataset, fine-tuning on specific tasks (e.g., question-answering), and applying human-in-the-loop reinforcement learning to produce more reasonable responses Achiam et al. (2023  ###reference_b1###).\nEven though these models are another form of data-driven learning that attempt to maximize the likelihood of generated text conditioned on a given context, researchers have shown that they have the ability to generalize to tasks they have not been trained on, and reason about their decisions.\nAs such, these foundation models are being tested in tasks like simulated decision-making Huang et al. (2024b  ###reference_b50###) and real-world robotics Zeng et al. (2023  ###reference_b149###) to take the place of perception, planning, and control modules.\nEven so, foundation models are not without their limitations.\nSpecifically, these models have a tendency to hallucinate, i.e., generate decisions or reasoning that sound plausible, but are in fact inaccurate or would result in undesired effects in the world.\nThis phenomenon has led to the beginning of a new research direction that attempts to detect when L(V)LMs hallucinate so as to produce more trustworthy and reliable systems.\nBefore these large black-box systems are applied in safety-critical situations, there need to be methods to detect and mitigate hallucinations.\nThus, this survey collects and discusses current hallucination mitigation techniques for foundation models in decision-making tasks, and presents potential research directions.\nExisting surveys particularly focus on presenting methods for hallucination detection and mitigation in question-answering (QA) Ji et al. (2023  ###reference_b55###); Rawte et al. (2023  ###reference_b104###); Zhang et al. (2023d  ###reference_b155###); Ye et al. (2023  ###reference_b145###) or object detection tasks Li et al. (2023c  ###reference_b73###).\nThere are also other works that provide examples of current use cases of L(V)LMs in autonomous vehicles Yang et al. (2023b  ###reference_b141###) and robotics Zeng et al. (2023  ###reference_b149###); Zhang et al. (2023a  ###reference_b151###).\nWang et al. (2023a  ###reference_b127###) perform a deep analysis of the trustworthiness of a variety of foundation models and Chen and Shu (2024  ###reference_b13###) provide a taxonomy of hallucinations within LLMs, but both exclude applications to general decision problems.\nTo the best of our knowledge, we are the first to propose a general definition of hallucinations that can be flexibly tuned to any particular deployment setting, including commonly found applications to QA or information retrieval, and more recent developments in planning or control.\nFurthermore, there is no existing work that summarizes state of the art methods for hallucination detection and mitigation approaches within decision-making and planning tasks.\nIn the remainder of this work, we discuss the current uses of foundation models for decision-making tasks in Section 2  ###reference_###, define and provide examples of hallucinations in Section 3  ###reference_###, identify current detection methods and where they are evaluated in Sections 4  ###reference_### and 5  ###reference_### respectively, and explore possible research directions in Section 6  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Foundation Models Making Decisions",
            "text": "Originally coined by Bommasani et al. (2022  ###reference_b5###), the term foundation models refers to models that are “trained on broad data at scale such that they can be adapted to a wide range of downstream tasks.”\nThis approach is in contrast to works that design and train models on a smaller subset of data for the purpose of being deployed to a specific task Yang et al. (2024  ###reference_b139###).\nThe key difference is that foundation models undergo a pre-training procedure on a large-scale dataset containing information from a variety of possible deployment fields, through which they are expected to learn more general features and correspondences that may be useful at test-time on a broader set of tasks Zhou et al. (2023  ###reference_b158###); Zhao et al. (2023  ###reference_b156###).\nExamples of existing pre-trained foundation models span language Devlin et al. (2019  ###reference_b25###); Brown et al. (2020  ###reference_b7###); Touvron et al. (2023a  ###reference_b123###), vision Caron et al. (2021  ###reference_b10###); Oquab et al. (2024  ###reference_b91###); Kirillov et al. (2023  ###reference_b61###), and multi-modal Radford et al. (2021  ###reference_b101###); Achiam et al. (2023  ###reference_b1###) inputs.\nIn this section, we give a brief overview of existing use cases for foundation models in robotics, autonomous vehicles, and other decision-making systems.\nWe also succinctly point out hallucinations found in these works and leave a lengthier discussion in Section 3.2  ###reference_###.\nReaders should refer to works from Yang et al. (2023b  ###reference_b141###), Zeng et al. (2023  ###reference_b149###), and Zhang et al. (2023a  ###reference_b151###) for a deeper review of application areas."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robotics",
            "text": "Foundation models have also been used in the robotics domain for object detection, affordance prediction, grounding, navigation, and communication. Ichter et al. (2023) are motivated by the issue of misalignment between the capabilities of a robot and what an LLM believes it is capable of performing. Because LLMs may not specifically be trained with data from the robot it is to be deployed on, there is a gap in the model’s understanding and the true capacity of the robot, which could lead to hallucinated generations that cannot feasibly be used at runtime. The authors propose SayCan as a method to combine the general knowledge of LLMs with the specific capabilities of a robot in the real-world. Specifically, an LLM is given a task in text form, and is asked to output a list of smaller actions to take in order to complete said task successfully. To constrain the LLM to generate possible actions available to the robot, they assume access to (1) the probability distribution of next tokens to generate from the model, and (2) a set of available skills on the robot, with which they compute the probability of the LLM generating each of the skills next. SayCan greedily selects the action that has the highest product of the next token probability from the LLM and the probability of the action actually successfully being executed in the environment, until the model predicts it has completed the task. Rather than relying purely on textual context, PaLM-E, proposed by Driess et al. (2023), is a multi-modal model that converts various sensor inputs (e.g., images) to a token-space embedding that is combined with instruction embeddings to be input to a PaLM LLM Chowdhery et al. (2023). PaLM is used to either answer questions about the surroundings of the robot, or to plan a sequence of actions to perform to complete a task. Driess et al. (2023) further acknowledge that the multi-modality of their PaLM-E architecture leads to increased risk of hallucinations. Inspired by recent promising findings in using foundation models to generate programs Chen et al. (2021), other works deploy foundation models to write low-level code to be run on robots. Liang et al. (2023) present Code as Policies, which uses LLMs to hierarchically generate interactive code and functions that can be called. As the model writes main code to be run on a robot given an instructive prompt of the task from the user, it identifies functions to call within the higher level code to complete the task successfully. The authors show that LLMs can leverage third party libraries for existing functions, or develop their own library of functions dynamically with custom methods for the task. While the functionality of Code as Policies can be tested easily for low-level skill definitions, longer multi-step problems require testing whether all requested conditions have been met by running the generated code on the robot. As such, Hu et al. (2024) propose the RoboEval performance benchmark for testing robot-agnostic LLM-generated code. Specifically, the CodeBotler platform provides an LLM access to abstract functions like “pick,” “place,” and “get_current_location” that have the same external interface regardless of the robot to be deployed on. Like Code as Policies, CodeBotler is provided a text instruction from the user and generates code to be tested. Then the RoboEval benchmark uses RoboEval Temporal Logic (RTL) to test whether the generated code meets task and temporal ordering constraints provided in the original prompt. Furthermore, they test the robustness of the LLM by passing in several paraphrased prompts to check for consistency across inputs. We discuss similar consistency-checking strategies for identifying hallucinations in decision-making tasks further in Section 4.3.1. In the space of robot navigation, LM-Nav leverages a VLM and attempts to predict a sequence of waypoints for a robot to follow and visit landmarks described within a language command Shah et al. (2023). Here, the authors use in-context learning Dong et al. (2023) to teach GPT-3 to extract desired landmarks from a natural language instruction. Assuming there are images of the possible landmarks the robot can navigate to in its environment, LM-Nav uses CLIP Radford et al. (2021) to predict the closest matching pairs of extracted landmark descriptions and waypoint images. Finally, dynamic programming is applied on the complete graph of the environment to optimize the path of landmarks to visit. The overall predicted path is optimized to maximize the likelihood of successfully completing the instruction input to the model."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Other Areas",
            "text": "There are also other works that apply foundation models for decision-making outside of the robotics and autonomous vehicle domains. For example, ReAct from Yao et al. (2023b ###reference_b144###) identifies that a key limitation of chain-of-thought reasoning Wei et al. (2022 ###reference_b131###) is that the model does not update its context or action based on observations from an environment. As such, chain-of-thought reasoning relies purely on the internal reasoning of the foundation model itself to predict actions to take, missing a crucial step in grounding its actions with their effects on the environment. Given a prompt, ReAct iterates between an internal reasoning step and acting in the environment to build up context relevant to the task. Yao et al. (2023b ###reference_b144###) showcase the promise of the method in a QA setting where the LLM can take actions to query information from an external knowledge base, as well as an interactive text-based game, ALFWorld Shridhar et al. (2021 ###reference_b114###). Chen et al. (2023b ###reference_b14###) admit that ReAct is a powerful tool for dynamic reasoning and grounding, but is limited by the fact that the updated context from the Act step is only helpful for the particular task the model is currently deployed for. They propose Introspective Tips to allow an LLM to reason about its past successes and failures in a world to generate general tips that will be helpful across diverse instruction-following tasks. Specifically, tips are generated from the past experience of the model from a similar set of tasks, from expert demonstrations, and from several games that differ from the target task. By summarizing these experiences into more concise tips, Chen et al. (2023b ###reference_b14###) show that Introspective Tips outperform other methods in ALFWorld with both few- and zero-shot contexts. Park et al. (2023 ###reference_b94###) and Wang et al. (2023b ###reference_b128###) apply foundation models in more complex environments to push models to their limits to simulate realistic human behaviors and test lifelong learning. Park et al. (2023 ###reference_b94###) propose generative agents that produce believable, human-like interactions and decisions within a small town sandbox environment. They develop a module for individual agents in the simulation to store and retrieve memories, reflect about past and current experiences, and interact with other agents. Their generative agents use similar methods to ReAct and Introspective Tips to act based on a memory of experiences, but also interact and build relationships with other agents through dialogue. The authors show that the agents are able to effectively spread information, recall what has been said to others and stay consistent in future dialogue interactions, and coordinate events together. Sometimes, however, agents are found to hallucinate and embellish their responses with irrelevant details that may be attributed to the training dataset of outside, real-world knowledge. Voyager, from Wang et al. (2023b ###reference_b128###), deploys GPT-4 to the MineDojo environment Fan et al. (2022 ###reference_b35###) to test its in-context lifelong learning capabilities. The architecture prompts GPT-4 to generate next high-level tasks to complete, given the agent’s current state and results of past tasks — a form of automatic curriculum generation. Voyager then identifies what intermediate general skills would be required to complete the task, and the LLM is used to fill in a skill library with helpful low-level skills in the form of programs that call functions that are available to the simulator. GPT-4 is prompted to generate skills that are generalizable to multiple tasks, so that the skill generation step does not have to be called for every task if the skill is already stored in the library. Wang et al. (2023b ###reference_b128###) show that Voyager continuously learns to explore the diverse tech tree available within MineDojo while building and leveraging skills. Even so, they find that the LLM hallucinates when generating tasks to tackle and when writing the code to execute for a particular skill, discussed further in Section 3.2 ###reference_###. Kwon et al. (2023 ###reference_b66###) explore the use of LLMs to act as a proxy for a hand-tuned reward function in RL tasks. This application is particularly motivated by decision-making tasks that are difficult to specify with a reward function, but can be explained textually with preferences of how a policy should generally act. Specifically, the LLM evaluator first undergoes in-context learning with examples of how it should decide the reward in several cases of the task that the agent will be deployed to. Then, during RL training, the LLM is provided a prompt with the trajectory of the agent within the episode, the resulting state from the simulator, and the original task objective from"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hallucinations",
            "text": "Even with all their success on a multitude of deployment areas, foundation models still produce inconsistent outputs, or hallucinate, at test-time.\nHere, we provide a general definition for hallucinations that can be applied to any foundation model deployment task, including various autonomous systems.\nAdditionally, we give examples of hallucinations encountered in literature, and discuss how they come about during testing.\nAs discussed in Section 2.1  ###reference_###, Wen et al. (2023  ###reference_b133###) test GPT-4V on the autonomous driving task and identify failure modes.\nRegardless of the weather and driving conditions, GPT-4V has difficulty detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself.\nIt also presents additional irrelevant (or completely false) details about other agents, when the prompt had no mention of them in the first place.\nFurthermore, the model also has difficulty in describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle’s perspective.\nIn their later work, Wen et al. (2024  ###reference_b132###) describe that hallucinations arise in these complex environments because of the high variability in driving scenarios.\nEven after applying hallucination mitigation techniques like chain-of-thought reasoning, the model is not free of these undesired outputs.\nA similar work evaluating the frequency at which LVLMs hallucinate in their descriptions of images, finds that these models’ outputs may include non-existent objects, or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c  ###reference_b73###).\nFor example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage, and predicts that the “table is neatly arranged, showcasing the different food items in an appetizing manner.”\nAlthough the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022  ###reference_b5###).\nChen et al. (2021  ###reference_b16###) explore alignment failures of LLMs applied to code completion tasks.\nThe authors evaluate the likelihood of these models generating defective code given different input prompts, and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand.\nThe study also identifies similar model biases towards race, gender, religion, and other representations.\nFurthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner.\nThese findings have been corroborated by other foundation model code generation works in the robotics domain.\nFor example, Wang et al. (2023b  ###reference_b128###) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo.\nSimilarly, Hu et al. (2024  ###reference_b48###) find that their model has the tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\nSeveral works focus on identifying cases of hallucinations in QA tasks.\nAlthough this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems.\nCommon hallucinations in QA result in incorrect answers to questions.\nFor example, Achiam et al. (2023  ###reference_b1###) find that GPT-4 “hallucinates facts and makes reasoning errors.”\nAchiam et al. (2023  ###reference_b1###) categorize these failures into closed-domain (given context, the model generates irrelevant information that was not in the context) and open-domain (the model outputs incorrect claims without any context) hallucinations.\nAfter fine-tuning on more data with a hallucination mitigation objective, the model reduces its tendency to hallucinate, but still does not achieve perfect accuracy — a similar trend encountered by Touvron et al. (2023a  ###reference_b123###).\nAnother set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1  ###reference_.SSS1### Mündler et al. (2024  ###reference_b89###); Zhang et al. (2023b  ###reference_b152###).\nIntuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output.\nNotice in this example, with relation to Definition 3.1  ###reference_definition1###, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks.\nAs such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "What are hallucinations?",
            "text": "Across current literature on foundation models, there exist similar patterns and themes that can be used to develop a unified definition for hallucinations.\nWith the majority of works studying this problem within QA tasks, where ground truth answers are available, several authors explain hallucinations as producing text that includes details/facts/claims that are fictional/misleading/fabricated rather than truthful or reliable Rawte et al. (2023  ###reference_b104###).\nWorks making use of a dedicated knowledge-base further describe hallucinations as generating nonsensical or false claims that are unsubstantiated or incorrectly cited Mündler et al. (2024  ###reference_b89###); Chen et al. (2023a  ###reference_b12###); Zhang et al. (2023b  ###reference_b152###); Li et al. (2023b  ###reference_b71###).\nVarshney et al. (2023  ###reference_b126###) also present the idea that foundation models may sound syntactically correct, or coherent, while simultaneously being incorrect.\nGallifant et al. (2024  ###reference_b40###), who perform a peer review of the GPT-4 technical paper, state that hallucinations include responses that are irrelevant to the original prompt.\nLi et al. (2023c  ###reference_b73###), who specifically explore hallucinations of LVLMs in detecting and classifying objects within images, define hallucinations as generating object descriptions inconsistent with target images.\nA common theme among existing hallucination definitions for QA, information retrieval, and image captioning domains is that, while the generation may sound coherent, either the output is incorrect, or the model’s reasoning behind the generated text is incorrect.\nHowever, we find these characteristics on their own do not completely encompass the hallucinations found in decision-making tasks in literature, thus requiring additional nuances.\nWithin papers that apply foundation models to decision-making tasks specifically, researchers have encountered similar problems of hallucinations impacting performance.\nPark et al. (2024  ###reference_b93###) describe hallucinations as predicting an incorrect feasibility of an autonomous system when generating an explanation behind the uncertainty of an action to take.\nSimilarly, Kwon et al. (2023  ###reference_b66###) find that language models may provide incoherent reasoning behind their actions.\nWang et al. (2024  ###reference_b129###) and Ren et al. (2023  ###reference_b105###) believe that these generative models also have a sense of high (false) confidence when generating incorrect or unreasonable plans.\nIn the case of robot navigation and object manipulation, Hu et al. (2024  ###reference_b48###) and Liang et al. (2024  ###reference_b75###) refer to hallucinations as attempting to interact with non-existent locations or objects.\nMetric\nQuesting-Answering\nImage Captioning\nPlanning\nControl\nConsistency\nGenerations must align with database facts\nObjects in description must appear in image\nPredicted sub-task must be feasible to solve\nPredicted action must be possible to perform\nDesired Behavior\nTone of answer should be informative\nCensor descriptions for inappropriate images\nPlans should maximize expected return\nPredict actions to complete plan efficiently\nRelevancy\nAnswers should not include references to unrelated topics\nDescriptions should not be embellished with details that cannot be confirmed\nPredicted sub-tasks and actions should not stray from the end goal with unnecessary steps\nPlausibility\nGeneration is syntactically sound and believable\nGenerated plan is reasonable and seems to attempt to accomplish goal\nIn the code generation task, Chen et al. (2021  ###reference_b16###) use the term “alignment failure,” with similar effects to those of hallucinations discussed above.\nMore specifically, the authors informally describe an alignment failure as an outcome where a model is capable of performing a task, but chooses not to.\nIf a model is able to complete a task successfully within its latent space (perhaps through additional prompt engineering or fine-tuning), one may ask, “Why would the model choose not to?”\nAs foundation models are trained with the next-token reconstruction objective on a training set, they attempt to maximize the likelihood of the next token appearing at test-time as well.\nConsequently, if the test-time prompt includes even minor mistakes, Chen et al. (2021  ###reference_b16###) find that LLMs will continue to generate buggy code to match the input prompt.\nThis issue is further described in Section 3.3  ###reference_###.\nWe realize existing definitions for hallucinations are extremely disparate depending on the deployment area.\nAs such, to bridge existing QA application areas, decision-making tasks, and all other possible test scenarios for foundation models, we combine these findings and define the term hallucination as follows:\nA hallucination is a generated output from a model that conflicts with constraints or deviates from desired behavior in actual deployment, or is completely irrelevant to the task at hand, but could be deemed syntactically plausible under the circumstances.\nThere are three key pieces to this definition:\nA generated output from a model.\nA deployment scenario to evaluate model outputs with any of the following:\nA list of constraints that must be consistent within the generation.\nA loose interpretation of a desired behavior the generation should meet.\nA set of topics relevant to the task.\nMetrics measuring consistency, desirability, relevancy, and syntactic soundness (plausibility) of generations.\nIn practice, this definition generally encapsulates the qualities of hallucinations discussed earlier.\nFor example, in QA or object detection tasks, one may define a set of relevant topics that a generation should not stray from, and constraints may be held in the form a knowledge-base of ground truth facts.\nThe desired behavior of the generation may be to be phrased in an informative manner, rather than sarcastic.\nOn the other hand, in robot manipulation settings, a developer may have a set of constrained actions feasible on the robot, and the desired behavior could be to complete a task with as few actions as possible.\nRelevancy may be measured in relation to the specific task to be deployed on (e.g., a prompt requesting a recipe to make pasta would find it irrelevant if the model also suggested a song to play while cooking).\nFinally, plausibility informally relates to a measure of how believable an output is to a critic.\nA more realistic generation has a greater chance of deceiving the user into trusting the model, even when the plan may be hallucinated.\nOverall, hallucinated outputs may contain one or more of the core characteristics (inconsistent, undesired, irrelevant, and plausible) simultaneously, and our definition can be flexibly applied to any deployment scenario in mind by choosing metrics for each characteristic, respectively.\nWe show more examples of applying our definition to various tasks in Table 1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Examples",
            "text": "As discussed in Section 2.1  ###reference_###  ###reference_###, Wen et al. (2023  ###reference_b133###  ###reference_b133###) test GPT-4V on the autonomous driving task and identify failure modes.\nRegardless of the weather and driving conditions, GPT-4V has difficulty detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself.\nIt also presents additional irrelevant (or completely false) details about other agents, when the prompt had no mention of them in the first place.\nFurthermore, the model also has difficulty in describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle’s perspective.\nIn their later work, Wen et al. (2024  ###reference_b132###  ###reference_b132###) describe that hallucinations arise in these complex environments because of the high variability in driving scenarios.\nEven after applying hallucination mitigation techniques like chain-of-thought reasoning, the model is not free of these undesired outputs.\nA similar work evaluating the frequency at which LVLMs hallucinate in their descriptions of images, finds that these models’ outputs may include non-existent objects, or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c  ###reference_b73###  ###reference_b73###).\nFor example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage, and predicts that the “table is neatly arranged, showcasing the different food items in an appetizing manner.”\nAlthough the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022  ###reference_b5###  ###reference_b5###).\nChen et al. (2021  ###reference_b16###  ###reference_b16###) explore alignment failures of LLMs applied to code completion tasks.\nThe authors evaluate the likelihood of these models generating defective code given different input prompts, and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand.\nThe study also identifies similar model biases towards race, gender, religion, and other representations.\nFurthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner.\nThese findings have been corroborated by other foundation model code generation works in the robotics domain.\nFor example, Wang et al. (2023b  ###reference_b128###  ###reference_b128###) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo.\nSimilarly, Hu et al. (2024  ###reference_b48###  ###reference_b48###) find that their model has the tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\nSeveral works focus on identifying cases of hallucinations in QA tasks.\nAlthough this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems.\nCommon hallucinations in QA result in incorrect answers to questions.\nFor example, Achiam et al. (2023  ###reference_b1###  ###reference_b1###) find that GPT-4 “hallucinates facts and makes reasoning errors.”\nAchiam et al. (2023  ###reference_b1###  ###reference_b1###) categorize these failures into closed-domain (given context, the model generates irrelevant information that was not in the context) and open-domain (the model outputs incorrect claims without any context) hallucinations.\nAfter fine-tuning on more data with a hallucination mitigation objective, the model reduces its tendency to hallucinate, but still does not achieve perfect accuracy — a similar trend encountered by Touvron et al. (2023a  ###reference_b123###  ###reference_b123###).\nAnother set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1  ###reference_.SSS1###  ###reference_.SSS1### Mündler et al. (2024  ###reference_b89###  ###reference_b89###); Zhang et al. (2023b  ###reference_b152###  ###reference_b152###).\nIntuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output.\nNotice in this example, with relation to Definition 3.1  ###reference_definition1###  ###reference_definition1###, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks.\nAs such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Why do they happen?",
            "text": "There are several speculations as to how hallucinations come about during deployment.\nFirst and foremost, like any learning task, foundation models are sensitive to biases in training data Rawte et al. (2023  ###reference_b104###).\nOnce a model is trained on a given large dataset, some facts may become out-of-date or stale at any point in time Puthumanaillam et al. (2024  ###reference_b97###).\nFurthermore, as the training set is embedded into a smaller encoding dimension, the knowledge within an L(V)LM’s frozen parameters is lossy, and models cannot feasibly be fine-tuned every time there is new data Peng et al. (2023  ###reference_b95###); Elaraby et al. (2023  ###reference_b34###).\nZhang et al. (2023b  ###reference_b152###) recommend changing algorithm parameters at runtime, such as, temperature (spread of probability distribution of next token), top- sampling (narrows the set of next tokens to be considered), and beam search (choosing a set of possible beams, i.e., trajectories, of next tokens based on high conditional probabilities), but the process of tuning these parameters is expensive.\nTo combat out-of-date training data, some works provide models with an external knowledge-base of information to pull facts from, with the hope of increasing model accuracy.\nEven with this up-to-date information, Zhang et al. (2023c  ###reference_b153###) pose that there may exist a misalignment between the true capabilities of a model, and what a user believes the model is capable of, leading to poor prompt engineering.\nIn fact, poor prompting is one of the most significant causes of hallucinations.\nChen et al. (2021  ###reference_b16###) find that poor quality prompts lead to poor quality generations, in the context of code completion.\nThis phenomenon is attributed to the reconstruction training objective of LLMs attempting to maximize the likelihood of next generated tokens, given context and past outputs, i.e.,\nwhere  is a context input to the model,  is an output sequence of  tokens , and any generated token  is conditioned on  previously generated tokens.\nAs the public datasets these models are trained on contain some fraction of undesirable generations (e.g., defective code), the models become biased to generate similar results under those inputs.\nQiu et al. (2023  ###reference_b99###) show that this limitation can actually be exploited to push foundation models to generate toxic sentences, or completely lie, by simply rewording the prompt.\nWhile foundation models condition generated tokens on ground-truth text without hallucinations at train time, during inference, the model chooses future tokens conditioned on previously (possibly hallucinated) generated text.\nAs such, Chen et al. (2023d  ###reference_b17###) and Varshney et al. (2023  ###reference_b126###) state that generated outputs are more likely to contain hallucinations if prior tokens are hallucinated as well.\nFurthermore, Li et al. (2023a  ###reference_b69###) find that, even if prompt context provided to a foundation model is relevant, the model may choose to ignore the information and revert to its own (possibly outdated or biased) parameterized knowledge.\nOverall, the hallucination detection task is highly complex with several possible sources of failures that need to be considered at test-time.\nChen and Shu (2024  ###reference_b13###) validate the complexity of the detection problem with studies identifying that human- and machine-based detectors have higher difficulty correctly classifying misinformation generated from LLMs than those written by other people."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Detection and Mitigation Strategies",
            "text": "Hallucination detection and mitigation methods can be classified into three types (white-, grey-, and black-box) depending on the available inputs to the algorithm.\nGenerally, given some context, a foundation model outputs a predicted sequence of tokens, the corresponding probabilities of each token, and embeddings of the generation from intermediate layers in the network.\nWhite-box hallucination detection methods assume access to all three output types, grey-box require token probabilities, and black-box only need the predicted sequence of tokens.\nBecause not all foundation models provide access to their hidden states, or even the output probability distribution of tokens (e.g., the ChatGPT web interface), black-box algorithms are more flexible during testing.\nIn this section, we present existing detection and mitigation approaches clustered by input type.\nWhile several of these works show promise in QA and object detection settings, many of them require further validation on decision-making tasks, and we will point out these methods as they come about.\nWorks in this section are summarized in Table 2  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "White-box Methods",
            "text": "Methods in this section require access to internal weights of the model for hallucination detection."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Attention Weights",
            "text": "Attention weight matrices, which are prominent within transformer model architectures, signify the importance the model places on earlier tokens within a generation when predicting future tokens. OPERA, proposed by Huang et al. (2024a ###reference_b49###), is a hallucination detection method for LVLMs that makes use of the model’s internal attention weights. When visualizing the attention matrix, the authors find that there exist peculiar column patterns that align with the beginning of a hallucinated phrase. These aggregation patterns usually occur on a non-substantial token like a period or quotation mark, but are deemed to have a large impact on the prediction of future tokens. As such, this finding led Huang et al. (2024a ###reference_b49###) to modify the beam search algorithm Freitag and Al-Onaizan (2017 ###reference_b37###) by applying a penalty term to beams wherever an aggregation pattern is detected, and roll back the search to before the pattern arises. Their method is shown to reduce hallucinations, and even eliminate possible repetitions in generations."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Honesty Alignment",
            "text": "In addition to methods that require hidden states or attention matrices, we also include methods that fine-tune foundation models to better communicate their uncertainty to questions under white-box algorithms, as they require access to model weights for training.\nFor example, Lin et al. (2022a  ###reference_b76###) collect a calibration dataset of questions and answers from GPT-3 under multiple types of tasks (e.g., add/subtract and multiply/divide), and record how often each task is incorrectly answered.\nThey aim to fine-tune the LLM to also output its certainty that the prediction is correct.\nConsequently, Lin et al. (2022a  ###reference_b76###) fine-tune the model with data pairs of a question and the empirical accuracy on the task that the question originates from in the calibration dataset, such that the model is expected to similarly output a probability of accuracy at test-time.\nThe authors show that the proposed verbalized probability in deployment does correlate with actual accuracy on the tasks.\nYang et al. (2023a  ###reference_b140###) take the method one step further by also training the model to refuse to answer questions with high uncertainty."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Grey-box Methods",
            "text": "Grey-box approaches leverage the probability distributions of tokens output from the model."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Concept Probabilities",
            "text": "Empirically, Varshney et al. (2023  ###reference_b126###) show that there is a negative correlation between hallucination rate and token probability (i.e., as a token’s probability decreases within a sentence, the tendency to hallucinate increases). Thus, the authors rely on token probabilities to estimate uncertainty of concepts within a generated claim, and they check for correctness by cross-referencing a knowledge-base. Whenever a concept is found to be conflicting with a fact through verification questions, their method attempts to mitigate the error by prompting the LLM to replace the incorrect claim with the evidence. Although effective in the QA setting, Varshney et al. (2023  ###reference_b126###) concede that, in the event token probabilities are not available, some form of heuristic must be used to detect hallucination candidates. Zhou et al. (2024  ###reference_b159###) show that external models can be developed to automatically clean hallucinations. The authors tackle the issue of object hallucinations that LVLMs experience when describing the content of images. Through theoretical formulations, the authors show that LVLM responses tend to hallucinate in three settings: when described object classes appear frequently within a description, when a token output has low probability, and when an object appears closer to the end of the response. As such, their model, LURE, is a fine-tuned LVLM trained on a denoising objective with a training dataset that is augmented to include objects that appear frequently within responses, and replacing objects with low token probabilities or appearing close to the end of the response, with a placeholder tag. At inference time, tokens are augmented similarly to how they were changed to generate the training dataset, and the LURE LVLM is prompted to denoise hallucinations by filling in uncertain objects. SayCanPay, proposed by Hazra et al. (2024  ###reference_b44###), builds off of the SayCan framework Ichter et al. (2023  ###reference_b52###) to improve the expected payoff of following a plan specified by a language model. Within our hallucination definition, this goal translates to increasing the desirability of generations by improving the likelihood of the model achieving higher rewards. The authors propose three different strategies for planning: Say, SayCan, and SayCanPay. Say methods greedily choose next actions based only on token probabilities. SayCan approaches also take the success rate of the chosen action into consideration. Finally, SayCanPay additionally estimates the expected payoff from following the plan with some heuristic. Hazra et al. (2024  ###reference_b44###) learn this Pay model with regression on an expert trajectory dataset. Combining all three models together minimizes the likelihood that a generated plan contains conflicting infeasible action calls, while maximizing the efficiency of the task completion."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Black-box Methods",
            "text": "Black-box algorithms only rely on the input prompts and output predictions from the model, without making assumptions on the availability of the hidden state, nor the token probabilities."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Adversarial Prompting",
            "text": "Works specializing in adversarial prompting attempt to test the robustness of models to varying inputs that may coerce the model into producing out-of-distribution results. For example, Mehrabi et al. (2023) apply adversarial prompting to text-to-image foundation models, like Stable Diffusion Schramowski et al. (2023), to generate offensive images. With respect to Definition 3.1, their framework, FLIRT, is essentially testing the tendency of foundation models to hallucinate undesired generations in deployment. FLIRT uses an adversarial language model to predict a prompt to input to the image generator, scores the generated image for the presence of undesirable traits using an external classifier, re-prompts the adversary to produce a new instruction conditioned on the findings of the classifier, and repeatedly generates images until the adversary successfully prompts the test model to output an undesirable result. Mehrabi et al. (2023) define objective functions conditioned on the score output by external classifiers to maximize diversity of adversarial prompts and minimize toxicity so as to pass text filters that detect malicious inputs, while improving attack effectiveness. Another work from Yu et al. (2023) presents the AutoDebug framework for automatically sampling and updating several prompts for use in adversarial testing of the language model. The authors specifically explore adversarial testing under the case that the model predicts a correct response when provided relevant context, but generates an incorrect prediction when the evidence is modified. They apply two different modification approaches: replacing tokens within the context to provide incorrect facts, and adding additional relevant facts to the prompt that may make it difficult to pick out the most important details. All in all, adversarial prompting is an effective method for identifying robustness of models to unseen inputs, which can be used to develop stronger input filters or fine-tune the model for decreased hallucination tendency."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Proxy Model",
            "text": "Certain black-box works rely on an external, proxy model to detect and mitigate hallucinations.\nOne such method is used as a baseline within the SelfCheckGPT article Manakul et al. (2023).\nAs many language foundation models do not provide access to token probabilities, the authors use an open-source proxy LLM that does provide token probabilities as an estimate of the original output’s probability.\nThey find that using proxy LLMs for probability estimation and hallucination detection successfully is highly variable.\nThe accuracy of detection is dependent on the complexity of the LLM itself, as well as the training data of the proxy LLM (i.e., models trained on independent datasets from the original LLM will have different generation patterns).\nWithin this section, we also include works that use an external trained classifier to detect hallucinations.\nFor example, Chen et al. (2023d) curate a dataset of QA dialogue from LLM generated responses.\nThey apply a composition of metrics to assess quality of responses, including a self-assessment from the LLM comparing the ground-truth and predicted text, human-labeled, and machine metrics (e.g., BERT score, F1 score, BLEU, etc.).\nTheir hallucination discriminator, RelD, is trained on the dataset in multiple separate phases, each using a different objective: regression, multi-class classification, and finally binary classification.\nThrough experiments, they find that RelD closely aligns with human evaluators’ original predictions.\nSimilarly, Pacchiardi et al. (2024) develop a black-box lie detector for LLMs.\nIn their case, the authors hypothesize that models that output a lie will produce different behaviors in future responses, like Azaria and Mitchell (2023).\nAs such, at inference time, Pacchiardi et al. (2024) prompt the LLM with several binary questions (that may be completely unrelated to the original response) and collect yes/no answers.\nAll the responses are concatenated into a single embedding that is input to the logistic regression model to predict the likelihood that the response was untruthful.\nThe authors find that the simple detector is mostly task- and model-agnostic once trained on a single dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Grounding Knowledge",
            "text": "In knowledge grounding tasks, a language model is tasked with identifying evidence from an external knowledge-base that supports claims within a summary. Although seemingly irrelevant to decision-making scenarios, similar methods to ones discussed in this section may be applied in planning tasks to identify observations that are most relevant to predicting the next action, or to generate reasoning behind a specified plan. PURR, proposed by Chen et al. (2023a), is a denoising agent, like LURE, that is trained in an unsupervised fashion given evidence from online sources, a clean (correct) summary, and a noisy (hallucinated) summary. The model learns to denoise the incorrect summary to the clean statement. During deployment, given a possibly hallucinated claim, a question generation model queries online sources for evidence about the claim, and PURR generates a cleaned version of the original summary with said evidence. Some knowledge grounding approaches prompt LLMs to generate code to directly query information from databases.\n\nLi et al. (2024) are motivated by the limitations of existing knowledge-based hallucination mitigation methods; namely that (1) they utilize a fixed knowledge source for all questions, (2) generating retrieval questions with LLMs that interface with a database is not effective because they may not be trained on the particular programming language of the database, and (3) there is no correction capability that handles error propagation between knowledge modules. Consequently, the authors propose augmenting LLMs with heterogeneous knowledge sources to assist with summary generation. Specifically, in the event that the model is found to be uncertain about its generated statement through self-contradiction, their framework, chain-of-knowledge (CoK), chooses subsets of knowledge-bases that may be helpful for answering the original question. Assuming each database has its own query generator, CoK queries for evidence, and corrects rationales between different sources iteratively. Compared to chain-of-thought reasoning, CoK consistently produces more accurate answers with its iterative corrections.\n\nAnother source of potential conflict that leads to hallucinations is misalignment between a model’s capabilities and the user’s beliefs about what it can do. Zhang et al. (2023c) tackle this knowledge alignment problem and categorize alignment failures into four types: Semantic — an ambiguous term maps to multiple items in a database; Contextual — the user failing to explicitly provide constraints; Structural — user provides constraints that are not feasible in the database; Logical — complex questions that require multiple queries. Their proposed MixAlign framework interacts with the user to get clarification when the LLM is uncertain about its mapping from the user query to the database. With the original query, knowledge-base evidence, and user clarifications, the LLM formats its final answer to the user.\n\nPeng et al. (2023) aim to add plug-and-play modules to an LLM to make its outputs more accurate, since these large foundation models cannot feasibly be fine-tuned whenever there is new information. Their work formulates the user conversation system as a Markov decision process (MDP) whose state space is an infinite set of dialogue states which encode the information stored in a memory bank, and whose discrete action space includes actions to call a knowledge consolidator to summarize evidence, to call an LLM prompt engine to generate responses, and to send its response to the user if it passes verification with a utility module. The proposed LLM-Augmenter has a memory storing dialogue history, evidence from the consolidator, set of output responses from an LLM, and utility module results. Its policy is trained in multiple phases with REINFORCE Williams (1992) starting with bootstrapping from a rule-based policy designed from domain experts, then learning from simulators, and finally, from real users. The authors find that access to ground-truth knowledge drastically improves QA results, and feedback from the utility module and knowledge consolidator help to provide more accurate answers to users.\n\nEvaluated in actual decision-making settings, Introspective Tips Chen et al. (2023b) provide concise, relevant information to a language planner to learn to solve more efficiently. Intuitively, summaries that collect information over all past experiences may be long and contain unnecessary information. In contrast, tips are compact information with high-level guidance that can be learned from one's own experiences, from other demonstrations, and from other tasks in a similar setting. Chen et al. (2023b) show that providing low-level trajectories is less effective than tips on simulated planning tasks. Additionally, with expert demonstrations, the LLM learns faster with a fewer number of failed trials than with just past experience alone. However, one limitation identified in the study is that the LLM underperforms in unseen, low-difficulty missions where it has issues generating general tips for zero-shot testing."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Constraint Satisfaction",
            "text": "There is also additional work in creating black-box algorithms for ensuring decision plans generated by foundation models meet user-defined goal specifications and system constraints, like their grey-box counterpart developed by Wang et al. (2024 ###reference_b129###). Because these models under test provide their results in text form, it is natural to apply formal method approaches (e.g., satisfiability modulo theory, SMT, solvers) to verify the satisfaction of generated plans. For example, Jha et al. (2023 ###reference_b54###) prompt an LLM planner with a problem formulated with first order constraints to predict a set of actions to complete the task. The output plan is input to an SMT solver to check for any infeasibilities in the program, and any counterexamples found are used to iteratively update the prompt and generate new plans. This counterexample approach is much faster than relying on combinatorial search methods that find a plan from scratch. However, the quality of generated plans and the number of iterations before a successful plan is generated are heavily dependent on the LLM generator itself, with similar reasons to the proxy-model used by Manakul et al. (2023 ###reference_b85###). Another work from Hu et al. (2024 ###reference_b48###) develops a RoboEval benchmark to test generated plans on real robots, in a black-box manner. Like Wang et al. (2024 ###reference_b129###), the authors introduce their own extension of LTL formulations, known as RTL, which specifies temporal logic at a higher, scenario-specific, level, while abstracting away constraints that are not dependent on available robot skills. RTL and LTL-NL are easier to read and define than classic LTL methods. RoboEval utilizes the provided RTL formulation of a problem, a simulator, and evaluator to systematically check whether the output meets requested goals. Furthermore, to check for robustness of the model to varied instructions, Hu et al. (2024 ###reference_b48###) hand-engineer paraphrased sentences within an offline dataset that should ideally result in the same task completion. Primary causes of failures were found to be a result of generated code syntax/runtime errors, attempting to execute infeasible actions on the robot, and failing RTL checks. Like adversarial prompting approaches, testing generated plans on robots in diverse scenarios enable researchers to design more robust systems that hallucinate less frequently at test-time."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Metrics and Evaluation Platforms",
            "text": "We now present common metrics, datasets, and simulation platforms leveraged when developing and evaluating the hallucination detection algorithms introduced in Section 4  ###reference_###.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###).\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "Here, we list established metrics used for computing language similarity and accuracy of generated image descriptions.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Language Similarity",
            "text": "Given a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model’s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Object Detection",
            "text": "CHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Offline Datasets",
            "text": "In this section, we present relevant offline datasets used for evaluating the performance of hallucination detection and mitigation techniques in driving, robotic, and QA tasks.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Driving",
            "text": "BDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Code Generation and Robotics",
            "text": "HumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Question-answering",
            "text": "HotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Simulation Platforms",
            "text": "Finally, we introduce common online simulators used to test hallucination detection methods for decision-making tasks.\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Driving",
            "text": "The developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###  ###reference_b130###)."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Robotics",
            "text": "Ravens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016–2021  ###reference_b22###  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like “MoveAhead,” “RotateLeft,” and “Open.”\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Other Simulators",
            "text": "TextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": "Here, we discuss some possible future directions in hallucination detection and mitigation techniques for foundation models to improve deployments to decision-making tasks.\nMost hallucination detection approaches are currently evaluated in offline QA settings for information retrieval or knowledge alignment, as seen in Table 2  ###reference_###.\nAs foundation models are increasingly used for more complex tasks, researchers should make an effort to adapt and evaluate earlier detection/mitigation approaches that were applied to QA problems in these modern applications.\nAlthough dissimilar in practice from QA settings, planning and control problems may be formulated in such a way that enables these earlier mitigation methods to be evaluated on decision-making tasks.\nFor example, as discussed in Section 2.1  ###reference_###, Chen et al. (2023c  ###reference_b15###) treat the autonomous driving task as a QA problem, which could be naturally extended to test other QA hallucination detection methods in the same setting.\nThis evaluation may lead to greater understanding of the general limitations of these models, as we draw parallels across diverse deployments.\nWhite- and grey-box detection methods may not generally be applicable in situations where the internal state or token probabilities are unavailable from the language model.\nThus, we predict black-box approaches will take precedence in the near future, as state-of-the-art LVLMs like GPT-4V already prohibit access to probability outputs.\nHowever, current black-box methods are limited with simplistic sampling techniques to gauge uncertainty, and proxy models may not be representative of the true state of the model under test.\nWorks like FLIRT (while only applied to image generation models) showcase the promise of black-box adversarial prompting approaches in generating undesirable results from models Mehrabi et al. (2023  ###reference_b87###).\nWe argue developing more aggressive black-box adversarial generative models, which explicitly optimize for producing inputs that may perturb the system outputs, is key to identifying the limits of a foundation model’s knowledge.\nCurrently, foundation models are primarily deployed to decision-making tasks that likely have some relation to its training set.\nFor example, although complex, tasks like multi-agent communication, autonomous driving, and code generation will be present in training datasets.\nOn the other hand, dynamic environments like robot crowd navigation require identifying nuances in pedestrian behaviors which the model may not have explicitly seen during training.\nPushing the limits of foundation model deployments will allow researchers to find areas for growth in other applications.\nWith the explosion of LVLMs, which allow for explicit grounding of natural language and vision modalities, further exploration should be performed in evaluating their effectiveness in decision-making systems.\nWen et al. (2023  ###reference_b133###) take a step in the right direction towards testing black-box LVLMs in offline driving scenarios, but there is still work to be done in deploying these models in online settings.\nThis direction can shed light on the long-standing debate of whether modular or end-to-end systems should be preferred in a particular deployment setting.\nIn this survey, we provide a glimpse into the progress of research into evaluating hallucinations of foundation models for decision-making problems.\nWe begin by identifying existing usecases of foundation models in decision-making applications like autonomous driving and robotics, and find several works make note of undesired hallucinated generations in practice.\nBy referencing works that encounter hallucinations across diverse domains, we provide a flexible definition for hallucinations that researchers can leverage, regardless of the deployment scenario in mind.\nFinally, we give a taxonomy of hallucination detection and mitigation approaches for decision-making problems, alongside a list of commonly used metrics, datasets, and simulators for evaluation.\nWe find that existing methods range in varying assumptions of inputs and evaluation settings, and believe there is much room for growth in general, black-box hallucination detection algorithms for foundation models."
        }
    ]
}