{
    "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
    "abstract": "Despite the recent progress in long-context large language models (LLMs), it remains elusive how these transformer-based language models acquire the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question.\nOur systematic investigation across 4 model families, 6 model scales, and 3 types of finetuning reveals that a special type of attention heads are largely responsible for retrieving relevant information from long context, which we dub retrieval heads.\nWe identify important and intriguing properties of retrieval heads:\n(1) universal:\nall the explored models with long-context capability have a set of retrieval heads;\n(2) sparse: only a small portion (less than 5%) of the attention heads are retrieval.\n(3) intrinsic: retrieval heads already exist in\nmodels pretrained with short context.\nWhen extending the context length to 32-128K by continual pretraining,\nit is still the same set of heads that perform information retrieval.\n(4) dynamically activated:\ntake Llama-2 7B for example, 12\nretrieval heads always attend to the required information no matter how the context is changed.\nThe rest of the retrieval heads are activated in different contexts.\n(5) causal:\ncompletely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model’s retrieval ability.\nWe further show that retrieval heads strongly influence\nchain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context.\nConversely, tasks where the model directly generates the answer using its intrinsic knowledge\nare less impacted by masking out retrieval heads.\nThese observations collectively explain which internal part of the model seeks information from the input tokens.\nWe believe our insights on retrieval heads foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "This work studies the internal mechanism of how long-context language models can utilize information at arbitrary locations within the input.\nRecent advances in long-context language modeling [1  ###reference_b1###, 20  ###reference_b20###, 6  ###reference_b6###] show inspiring results, particularly on the Needle-in-a-Haystack test [14  ###reference_b14###], which asks the model to precisely retrieve the information of a short sentence (the needle) within a long context (the haystack).\nSuch capability is the basis of more advanced long-context tasks, which usually interleaves retrieval and reasoning in a multi-step fashion [17  ###reference_b17###].\nBased on extensive experiments across 4 model families, 6 model scales, and 3 types of finetuning,\nwe show that across the models’ attention layers, there exist a small number of retrieval heads that search the information being asked, and redirect the relevant tokens from the input to the output.\nActivation of retrieval heads explains whether the output is factual or hallucinated.\nWhen such heads are activated, the model behaves faithful to the input document.\nWhen they are not activated, or intentionally masked out in controlled experiments (Fig. 1  ###reference_###), the model cannot find the relevant information and hallucinate instead.\n###figure_1### The discovery of the retrieval head is motivated by the question of what the attention mechanism is doing when the model can or cannot find the given needle.\nWe take important inspiration from two existing works: the CopyNet [10  ###reference_b10###] and the Induction Head [19  ###reference_b19###].\nThe CopyNet is essentially a single-layer, single-head attention mechanism in the age of RNNs that copy-paste tokens from the input to the output.\nInduction Heads [19  ###reference_b19###] are special heads within a multi-layer, multi-head attention network that implements an implicit program induction algorithm.\nCombining the observation from the two works, we natually hypothesize that, just like induction heads are accountable for in-context learning, there might exist special heads that are accountable for information retrieval and implement a conditional copy-paste algorithm.\nWe design algorithms to detect retrieval heads within the transformer architecture (Sec. 2  ###reference_###), and conduct large-scale experiments to demonstrate important properties of them (Sec. 3  ###reference_###):\n(1) retrieval heads are universal and sparse: for any model family (LLaMA [21  ###reference_b21###], Yi [25  ###reference_b25###], QWen [2  ###reference_b2###] and Mistral [12  ###reference_b12###]), at any scale (6B, 14B, and 34B and 87B), either base or chat, either dense or MoE, as long as the model can precisely recite the input information, they have a small number of retrieval heads (Fig. 1  ###reference_###);\n(2) they are intrinsic: the base model (e.g., LLaMA2 base) already contains retrieval heads (as a consequence of large-scale pretraining). Subsequent derivations, such as the long-context continue pretraining (LLaMA2 7B 80K), chat fine-tuning (Qwen Chat), or even sparse upcycling [16  ###reference_b16###, 13  ###reference_b13###] uses the same retrieval heads as the base model (Fig. 5  ###reference_###);\n(3) they are dynamically activated according to the context: the strongest retrieval heads (e.g., 13 for LLaMA 2 7B) are always activated no matter what the required information is, while weaker retrieval heads are activated on different parts of the required information; consequently these heads compensate each other’s functionality: removing a subset of the heads, the model at least retrieve part of the required information;\n(4) the retrieval heads are causal:\nsay we put a needle \"the best thing to do in San Francisco is to eat a sandwich in Dolores Park on a sunny day\",\ncompletely masking out retrieval heads, the model hallucinates (by saying the best thing is to visit Golden Gate bridge);\npartially masking out the heads, the model retrieves part of the needle (e.g., it gets the sandwich but forget the Dolores Park);\nmasking out random non-retrieval heads, the model still find full needle;\nwhen we do not mask the head yet the model still hallucinate in some cases,\nthe retrieval heads are not activated.\nWe further note that chain-of-thought reasoning also heavily relies on retrieval heads because the model needs to refer back the input information, indicating a complex relationship between the model’s retrieval and reasoning capability.\nThe discovery of retrieval head has profound implications on long-context modeling:\n(1) it marks a significant step forward in the field of mechanistic interpretability [3  ###reference_b3###, 19  ###reference_b19###] because for the first time we pin point a particular subnet implementing the conditional retrieval algorithm;\n(2) it explains why certain context-compression methods fail to keep factuality (because they removes the retrieval head, e.g., in Xiao et al. 24  ###reference_b24###), and suggests future research on KV cache compression [7  ###reference_b7###, 15  ###reference_b15###], a key problem for deploying long-context models, should consider the influence of retrieval heads."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Detecting Retrieval Head",
            "text": "###figure_2### ###table_1### To detect which head is implementing the retrieval algorithm, we introduct a retrieval score to measures the frequency of a head’s copy-paste behavior during autoregressive decoding.\nAn attention head with high retrieval score suggests that statistically across various contexts, this head is frequently copying the input tokens from the input to the output.\nNeedle-in-a-Haystack  Our retrieval head detection algorithm roots from the needle-in-a-Haystack test, which asks the model to copy-paste the input tokens to the output.\nGiven a question  and its corresponding answer  (the needle), we insert  in a given context  (the haystack) at a random position index range .\nThe language model is then tasked with answering  based on the haystack with the inserted needle.\nWe set  and  unique and irrelevant with the given long context,\nensuring that if an answer is correctly generated, it is indeed copied from the context, not from the model’s internal knowledge.\nRetrieval Score for Attention Heads  We define the retrieval score as the frequency of a head’s copy-paste operations.\nSpecifically,\nduring auto-regressive decoding (we use greedy decoding by default),\ndenote the current token being generated as  and\nthe attention scores of a head as .\nAs demonstrated in Fig. 2  ###reference_###, we say an attention head  copies and pastes a token from the needle to the output sentence if it follows two criteria:\n(1) , i.e.,  is a token within the needle sentence. (2)\n, i.e., the input token that receives the most attention probability mass by this head is a token within the needle and is the same token as the currently generated token.\nLet  be the set containing all tokens copy and pasted by a given head , we define:\nIntuitively, retrieval score represents a token-level recall rate of the most attended tokens by an attention head.\nFor example, when retrieving a needle of 10 tokens, a retrieval score of 0.9 indicates that the attention head has copies and pasted 9 tokens in the 10-token target answer.\nRetrieval Head Detection Algorithm  We calculate the retrieval score for all attention heads under a diverse set of input contexts.\nFor each language model we consider, we compile three sets of Needle-in-a-Haystack samples, each consisting of a unique tuple .\nFor each sample, we make sure  is semantically irrelevant with  and that  cannot be answered using the model’s existing knowledge by manually inspecting the model output.\nThen for each  sample, we perform Needle-in-a-Haystack on 20 different length values uniformly sampled from 1K-50K, where in each length,  is inserted in 10 different depth uniformly ranging from the start to the end of .\nWe note that this scale of tests gives stable outputs as the average retrieval score converges after just a few samples.\nIn total, each language model is subjected to approximately 600 instances of retrieval testing.\nWe calculate the retrieval score for each attention head in each test and use the average of these scores as the head’s final retrieval score.\nThe attention heads with relatively larger retrieve score can be considered as retrieval head.\nIn our case (Fig. 3  ###reference_###), we set the threshold as 0.1, meaning that as long as the head performs copy-paste in 10% of the times, we consider it a retrieval head."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Basic Properties of Retrieval Heads",
            "text": "This section discusses important properties of retrieval heads:\n(1) universal and sparse: any model that exhibits long-context capability has a small set of retrieval heads;\n(2) dynamic: most of retrieval heads are activated under different contexts;\n(3) intrinsic: retrieval heads are already within the base model as a consequence of large-scale pretraining. Subsequent models reuse the same set of heads.\nOur results are supported by extensive experiments on a large spectrum of models (Table 1  ###reference_###).\nTo examine the effect of alignment, we have study Mistral-7B-Instruct-v0.2 and Qwen-1.5-14B-Chat [2  ###reference_b2###] and compare them to their base versions.\nWe further choose Mixtral-8x7B-v0.1 [13  ###reference_b13###], a mixture of expert versions derived from Mistral-7B-v0.2, presumably via sparse upcycling [16  ###reference_b16###], to study retrieval heads in different architectures."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dynamically Activated Based on Tokens and Contexts",
            "text": "Now we study how sensitive a retrieval head is to its input context, i.e., whether a head is consistently activated no matter what the context is, or if a head is activated only on specific contexts. For the needle sentences \"the best thing to do in San Francisco is eating a sandwich in Dolores park in a sunny day\", some heads are activated on the full sentence, whereas other heads only activated on certain tokens like “eating a sandwich” or “in Dolores park’. We define activation frequency, the frequency of a head being activated on at least one token (vs., the retrieval score measures the average number of activated tokens). A head of high activation frequency but low retrieval score means it is only activated on certain tokens and contexts. As is shown in Fig. 4, Llama-2-7B-80K and Yi-6B-200K have 12 and 36 strongest retrieval heads, respectively, that are always activated (activation frequency equal to 1) under all the contexts we consider. Weaker heads only activate on certain tokens and contexts."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Intrinsic",
            "text": "We show that the retrieval heads, thus the ability of utilizing information at arbitrary location of the input, is an intrinsic property [6] of the base model as a consequence of large-scale pretraining, with subsequent small-scale training exerting only minor alterations to these head activation patterns. In Figure 5, we present the retrieval score distributions for a range of base models in the initial row, followed by their corresponding variants in the subsequent row. We see that regardless of the models being continuously pre-trained, chat fine-tuned, or sparsely upcycled, there is a notable consistency in their retrieval scores heatmaps. Figure 7 offers a more direct and strict examination, where we compute the statistical correlations between different models. The data reveal a high degree of correlation in the retrieval score distributions between base models and their respective variants, with a Pearson correlation coefficient exceeding 0.8. Models from different families exhibit a correlation coefficient of less than 0.1, indicative of their distinct pretraining recipes."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Influence on Downstream Tasks",
            "text": "This section examines how retrieval heads influence downstream tasks. Across the experiments we use Mistrial-7B-Instruct-v0.2 [18  ###reference_b18###] as it is a popular and strong open language model with 32K context length. We first show that retrieval heads explains the factuality of Needle-in-a-Haystack test. When the model can retrieve the needle, retrieval heads are always activated. When the model cannot retrieve the needle and hallucinate instead, retrieval heads are either partially activated or not activated. Then we show that retrieval heads significantly influence question answering that requires extracting the information from the input, but does not strongly influence tasks where the model directly produce answers based on its internal knowledge. We further explore how retrieval heads influence more sophisticated reasoning behaviors like chain-of-thought [23  ###reference_b23###]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Influence on Extractive QA",
            "text": "Now we study how retrieval heads influence more realistic tasks beyond Needle-in-a-Haystack.\nWe use extractive QA as a test bed, as common usecase of long-context model where the user typically upload a pdf (research papers, financial reports, legal documents, etc.) and ask questions about specific information within the document.\nTo make sure the knowledge being asked does not exist in the model’s internal knowledge, we synthesize an extractive QA dataset by selecting a set of up-to-date news articles, extract a paragraph from it, and asking GPT-4 to produce a question-answer pair based on the extracted paragraph, similar to the evaluation conducted in Anthropic. \nThese observations demonstrate that real-world document QA tasks heavily rely on the functionality of retrieval heads."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Chain-of-Thought Reasoning also Requires Retrieval Heads",
            "text": "We test Mistrial-7B-Instruct-v0.2’s performance on MMLU, MuSiQue, and GSM8K, with and without chain-of-thought reasoning. As is shown in Fig. 10, if we use answer-only prompting (without CoT), masking out either retrieval or random heads do not significantly influence the performance, presumably because the model’s generation is based on its internal knowledge primarily stored in the FFN layers. For CoT styled reasoning, masking out retrieval heads significantly influences the model’s performance. Upon inspecting typical error cases (Fig. 11), we find that the model becomes “blind” to important input information and hallucinates instead. We find the relationship between CoT and retrieval heads particularly intriguing as it may offer deeper insights into model’s complex reasoning performance. We leave more in-depth studies to future research."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions",
            "text": "General Functionalities of Attention Heads  For transformer language models, we tend to view the functionality of FNNs layers to be the place for storing knowledge [8  ###reference_b8###], and the attention layers to be the place for implementing algorithms [19  ###reference_b19###].\nThe induction head discussed in  Olsson et al. [19  ###reference_b19###] typically searches repeated patterns of the input, which is at a certain level similar to the retrieval heads (as it also searches and repeats information).\nDifferent than the induction heads, the retrieval heads are typically responsible for redirecting the information according to the context, but do not for inferring programs.\nWe tend to believe that there exist more algorithm and functionalities implemented by other types of attention heads to be discovered by future research.\nRelationship to Local and Linear Attention and State-Space Models  Although there exist numerous works about local [24  ###reference_b24###] / linear [22  ###reference_b22###] attention, state space models [9  ###reference_b9###], and hybrid architectures [5  ###reference_b5###] achieving inspiring efficiency in long-context modeling, so far there is no linear attention / SSM architecture that passes the Needle-in-a-Haystack test to the best of our knowledge, suggesting that the full attention might be a must for long-context information retrieval.\nOne example is that the Mistral v0.1 [12  ###reference_b12###] uses sliding window attention but cannot pass needle-in-a-haystack, and their authors changes the attention to full in v0.2 [18  ###reference_b18###], then it can pass the needle test.\nOur results showing strong evidence why full attention is a must.\nFor the model to precisely utilize input information at arbitrary location, it is crutial for the retrieval heads to work on the full KV cache.\nApplications to KV Cache Compression  The problem that the KV cache is too large and occupies a large chunk of the GPU memory severely hinders the deployment of long-context models.\nFor example, for LLaMA 2 7B, the KV cache of 100K tokens requires more than 50GB memory, while 2K context requires less than 1GB memory.\nIf we serve this model on one 80G A100, then the concurrency of 100K context can be 50 times less than 2K context queries, which is prohibitively expensive.\nThe results from this work indicates that we might be possible to radically prune out the KV cache corresponding to the non-retrieval heads (recall in Fig. 3  ###reference_### shows only 5% of the heads are retrieval) and significantly reducing the deployment cost of long-context models.\nWe leave this study to future research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This paper discovers retrieval heads, a special set of attention heads that are responsible for implementing the conditional copy algorithm and redirect information from the input to the output.\nRetrieval heads are the primarily reason why a successful long-context model can pass the Needle-in-a-Haystack test, and their activation explains why a language model is faithful to the input or hallucinate.\nCompared to non-retrieval heads, retrieval heads have a stronger influence on downstream tasks that require the model to precisely recall the input information, either in extractive question answering or chain-of-thought reasoning.\nWe believe this work will foster future research on reducing hallucination,\nimproving reasoning, and compressing the KV cache."
        }
    ]
}