{
    "title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 tasks and 31 metrics. We observe that finetuning can help LLMs transfer knowledge across languages, serving as an efficient way to bolster their capabilities in non-English languages. Moreover, our analysis indicates that larger models can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or finetuning datasets. These insights underscore the significance of meticulous finetuning with high-quality datasets in enhancing LLM performance.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) such as GPT-4 OpenAI (2023  ###reference_b46###), BLOOM Le Scao et al (2023  ###reference_b29###), LLaMa-2 Touvron et al (2023  ###reference_b54###), Mistral Jiang et al. (2023  ###reference_b26###), Mixtral Jiang et al. (2024  ###reference_b25###), Gemma Team et al. (2024  ###reference_b52###) have made significant contributions to the field of natural language processing (NLP). Despite their advancements, a gap remains in their specialization for many languages, including Vietnamese. This paper addresses the development and evaluation of Vietnamese-centric LLMs. Vietnam, with a population surpassing 100 million, ranks as the 16th most populous country globally. Current models exhibit limitations in effectively handling Vietnamese NLP tasks, especially in accurate comprehension and response Lai et al. (2023  ###reference_b28###). Consequently, there is an increasing demand for a robust, dedicated Vietnamese LLM.\nSeveral factors constrain the practical application of LLMs. Concerns regarding the precision, inherent biases, potential toxicity, and fairness of their outputs are notable obstacles Ye et al. (2023  ###reference_b62###); Liang et al (2023  ###reference_b31###); Wang et al. (2024  ###reference_b57###). Moreover, there is a lack of research evaluating LLMs in the Vietnamese context. To facilitate the effective use of state-of-the-art LLMs for Vietnamese speakers, thorough evaluations are essential prior to their widespread use. Such evaluations not only ensure the reliability of these LLMs but also highlight areas where these LLMs could be better. This leads to developing targeted reinforcement learning strategies to rectify these issues in the next phase.\nIn response to the aforementioned challenges, we aim to develop open-source Vietnamese LLMs. Initiating an LLM from scratch is impractical due to the scarcity of extensive training datasets and limited computational resources. However, the advent of QLoRA Dettmers et al. (2023  ###reference_b12###), incorporating quantization techniques Dettmers et al. (2022  ###reference_b11###) and LoRA Hu et al. (2022  ###reference_b23###), provides an efficient approach for fine-tuning LLMs, particularly in resource-constrained environments. We employ fine-tuning on the LLaMa-2, Mixtral 87B, Gemma, and conduct a comprehensive evaluation of Vietnamese LLMs across various scenarios and settings. Throughout the thorough evaluation process, we observe the following: (i) larger language models exhibit unseen capabilities compared to smaller counterparts; (ii) larger language models tend to manifest more biases, produce uncalibrated results, and are more susceptible to the influence of input prompts; (iii) the quality of training or fine-tuning datasets is the key for unlocking LLM performance. Our key contributions include:\nThe fine-tuning and release of five Vietnamese LLMs: URA-LLaMa 7B, 13B, and 70B based on LLaMa-2; MixSUra based on Mixtral 87B; GemSUra 7B based on Gemma 7B. Our finetuning leverages data from the Vietnamese Wikipedia Foundation (2022  ###reference_b16###), Vietnamese News-Corpus Binh (2021  ###reference_b4###), and Vietnamese Highschool Essays111Vietnamese Highschool Essays  ###reference_/vanhoc_processed###.\nConducting comprehensive evaluations of 14 Vietnamese LLMs across ten common application scenarios, focusing on aspects such as accuracy, robustness, fairness, bias, and toxicity. Additional criteria are tailored to each specific scenario. Our empirical research also explores the influence of prompt design during inference.\nAs part of this effort, we introduce and share two novel Vietnamese reasoning datasets inspired by MATH Hendrycks et al. (2021  ###reference_b20###) and Synthetic reasoning Wu et al. (2021  ###reference_b60###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "To our best knowledge, there are seven available Vietnamese LLMs: (i) Vietcuna-7B-v3 ViLM (2023  ###reference_b56###) – fine-tuned on BLOOMZ Muennighoff et al. (2023  ###reference_b35###), open-sourced, released on Aug. 8, 2023, (ii) Vistral 222Vistral-7B-Chat  ###reference_l-7B-Chat### – based on Mistral, open-sourced, (iii-iv) PhoGPT 7B5 & PhoGPT 7B5 Instruct Nguyen et al. (2023a  ###reference_b37###) – based on MPT architecture Team (2023  ###reference_b53###), open-sourced, released on Nov. 7, 2023 (concurrently with our work), (v) Gemini Team et al. (2024  ###reference_b52###) – a commercial product of Google, and (vi-vii) GPT3.5 Turbo & GPT-4, which are closed-source commercial products on the Azure platform (version 0613) OpenAI (2023  ###reference_b47###). To our knowledge, we are the first to fine-tune and release two large-scale open-source Vietnamese LLMs with 13B, 70B parameters and a Mixture-of-Expert Vietnamese LLMs with 47B parameters.\nEvaluating a language model is challenging because LLMs can improve general capabilities with scale. Thus, evaluating an LLM depends on various factors, such as the tasks for which the LLM will be used, and the impact of prompt design, among others. Currently, there is no evaluation framework capable of fully and accurately assessing the abilities of a Vietnamese LLM. Some recent studies on Vietnamese LLMs only assess the model’s performance on closed-book question-answering tasks Nguyen et al. (2023a  ###reference_b37###) or specific datasets related to ad hoc aspects, such as law Nguyen et al. (2023b  ###reference_b39###); Anh et al. (2023  ###reference_b2###), physics Xuan-Quy et al. (2023  ###reference_b61###), and biology Dao and Le (2023  ###reference_b10###). Part of the challenge is the lack of high-quality Vietnamese datasets. Vietnamese NLP datasets have largely focused on daily tasks such as open-book and closed-book question-answering Artetxe et al. (2020  ###reference_b3###); Lewis et al. (2020  ###reference_b30###), summarization Nguyen et al. (2019c  ###reference_b45###); Ladhak et al. (2020  ###reference_b27###), translation Zhang et al. (2020  ###reference_b64###); Doan et al. (2021  ###reference_b14###), etc. Evaluation of some LLM capabilities, such as reasoning and mathematical logic, have not been considered due to the absence of suitable datasets. We are the first to address this challenge by comprehensively evaluating Vietnamese LLM on 10 scenarios and 31 metrics. In that process, we build and open-source two novel Vietnamese reasoning datasets. Our evaluation framework is open-source on Github333https://github.com/stair-lab/villm  ###reference_### to facilitate community-driven model evaluation444https://ai.stanford.edu/~sttruong/villm  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Supervised Finetuning",
            "text": "We focus on finetuning English-language models to enhance overall performance and evaluate adaptability and efficiency in various configurations. Due to computational constraints,\nour first models, named URA-LLaMa, were finetuned from LLaMa-2 using QLoRA Dettmers et al. (2023  ###reference_b12###) on two primary open-source Vietnamese datasets, including Vietnamese Wikipedia (1GB) and Vietnamese News-Corpus (22GB). The 7B variant was finetuned on both datasets, while the 13B and 70B versions were finetuned with only the Vietnamese Wikipedia dataset. The LoRA rank was set at  for the 7B model,  for the 13B model, and  for the 70B model. Other hyperparameters, including LoRA , dropout, quantization, quantization type, learning rate, max length, and epochs, were uniformly set at , ,  bit, NF4, , , and , respectively. We use six A100 80GB for the entire finetuning process in approximately 867 hours, emitting nearly 900 kg CO2 eq.\nContinuously, we conducted finetuning on Gemma 7B, and Mixtral 87B models utilizing Vietnamese Wikipedia and Vietnamese Highschool Essay datasets, employing the LoRA Hu et al. (2022  ###reference_b23###). This refinement resulted in the development of GemSUra 7B, and MixSUra models. Common hyperparameters were applied across these models, with LoRA rank set to , LoRA  at , and LoRA dropout rate fixed at . For the GemSUra model, the learning rate, maximum sequence length, and number of epochs were established at , , and , respectively. Conversely, for MixSUra, these hyperparameters were adjusted to , , and . The finetuning process for these two models required four A100 80GB GPUs, spanning a total of 289 hours and resulting in the emission of 200 kg CO2 equivalent. Our models are available on HuggingFace555https://huggingface.co/ura-hcmut  ###reference_huggingface.co/ura-hcmut###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Pipeline",
            "text": "We define a scenario as a real-world use case of LLMs describing the purpose for which LLMs are used. Modern LLMs can deal with various scenarios. We limit ten common use cases in Vietnamese in this work. Each scenario contains two well-known datasets in Vietnamese, which are already split into training and testing sets. We utilize the testing set to evaluate our finetuned models, LLaMa-2, Vietcuna, Vistral, PhoGPT, Gemini Pro, GPT-3.5 Turbo, and GPT-4, considering their diverse performance and architectural differences for a comprehensive analysis. Below are detailed descriptions of 10 scenarios:\nQuestion-Answering requires LLM to answer an open-ended question from a given context. We selected two notable Vietnamese datasets for diversity of evaluation domain: XQuAD Artetxe et al. (2020  ###reference_b3###), a multilingual variant of SQuAD Rajpurkar et al. (2016  ###reference_b51###), and MLQA Lewis et al. (2020  ###reference_b30###), both based on Wikipedia articles. Exact Match (EM) and F1 score (F1) measure question-answering performance. F1 Score is the harmonic mean of Precision and Recall:  where  and .\nSummarization involves LLMs condensing long documents into shorter open-ended paragraphs. We selected the two largest Vietnamese summarization datasets: VietNews Nguyen et al. (2019c  ###reference_b45###) and WikiLingua Ladhak et al. (2020  ###reference_b27###). VietNews comprises over 150,000 articles (22,644 for testing) from Vietnamese online news websites. WikiLingua was chosen for its variety, featuring diverse tutorials from WikiHow wikiHow (2023  ###reference_b59###). We primarily rely on standard evaluation metrics like ROUGE-1, ROUGE-2, and ROUGE-L Liang et al (2023  ###reference_b31###). ROUGE-1 (R1) measures the overlap of unigrams (individual words) between the system-generated and reference summaries. ROUGE-2 (R2) focuses on the overlap of bigrams, while ROUGE-L (RL) evaluates the longest common subsequence between the two summaries. Beyond these, we incorporate five additional metrics from Grusky et al. (2018  ###reference_b17###) to assess summary quality. These include SummaC (SC), which assesses the faithfulness of generated summaries; BERTScore (BS), which uses mBERT token embeddings to compute the cosine similarity between sentence tokens; Coverage (Cv), measuring how much a summary derives from the original text; Density (De), defined as the average length of extractive fragments associated with each summary word; and Compression (Cp), which is the word ratio between original articles and their summaries.\nSentiment Analysis focuses on detecting emotion of documents. Given a document and a list of all available sentiments, the LLM must choose the correct ones. The first selected dataset, VLSP 2016 Nguyen et al. (2019b  ###reference_b40###), contains comments on social networks about electronic devices such as smartphones, laptops, television, etc. The next dataset, UiT-VSFC Nguyen et al. (2018  ###reference_b41###), is feedback from Vietnamese students about courses at the end of semesters. We use Accuracy (AC), F1, AUC ROC (AR), Expected Calibration Error (ECE), and Accuracy at C% coverage (A@C) for model assessment. . AUC ROC quantifies the model ability to distinguish between classes by measuring the area under the ROC curve. A perfect model would have an AUC ROC score of 1, while a score below 0.5 indicates a model performing worse than random. Expected calibration error (ECE) described in Guo et al. (2017  ###reference_b18###) measures the difference between the model predicted probability and the fraction of times the model is correct. As a default configuration, we use ten bins, each containing an equal number of predicted probabilities. Accuracy at C% coverage is the accuracy for the C% fraction of examples the model assigns the highest probability. Details of this metric can be found at Liang et al (2023  ###reference_b31###). In our experiment, C is set to 10%.\nText Classification is a scenario where the LLMs are required to analyze the input document with a list of class labels and give the answer of which class that document belongs to. This scenario is a classical task in almost all languages, including Vietnamese. Thus, various datasets in different fields are available. However, evaluating all those datasets may not be feasible, so we choose two large and reliable ones in this study, which are UiT-VSMEC Ho et al. (2020  ###reference_b21###) and PhoATIS Dao et al. (2021  ###reference_b9###). UiT-VSMEC is specified for emotion recognition of Vietnamese comments on Facebook, the most-used social network in Vietnam. PhoATIS is the human-verified Vietnamese version of the famous standard ATIS dataset Price (1990  ###reference_b50###), specified for classification intents of user requests about airline information. Here, we use AC, F1, AR, ECE, and A@C for model assessment.\nKnowledge assesses LLMs common knowledge specified for Vietnamese. We use the two largest datasets: ZaloE2E Zalo AI (2023  ###reference_b63###) and UiT-ViMMRC Nguyen et al. (2020b  ###reference_b42###). ZaloE2E has open-ended questions. UiT-ViMMRC contains reading comprehension multiple-choice questions for students from Grades 1-12 in Vietnam. This task uses AC, F1, EM, AR, ECE, and A@C for model assessment.\nToxicity Detection requires the LLMs to detect toxicity in a paragraph, such as toxic purpose or hate speech. We choose the two most recent datasets: UiT-ViCTSD Nguyen et al. (2021  ###reference_b43###) and UiT-ViHSD Luu et al. (2021  ###reference_b34###) in this scenario. The UiT-ViCTSD dataset specifically targets the discernment of toxic speech, while UiT-ViHSD centers on identifying instances of hate speech. In this task, we use accuracy, F1 score, and AUC ROC for model assessment.\nInformation Retrieval is a task that ranks a list of relevant documents in the database given the query. We chose the two most recent multilingual datasets supporting Vietnamese. The first is the mMARCO dataset Bonifacio et al. (2022  ###reference_b6###), a multilingual version of the well-known MS MARCO dataset Nguyen et al. (2016  ###reference_b44###). The other mRobust04 Jeronymo et al. (2022  ###reference_b24###) is also a multilingual of TREC Robust 2004. Following Liang et al (2023  ###reference_b31###), we have two settings: normal and boosted. In the normal setting, we employ the top 30 documents retrieved by BM25 Amati (2009  ###reference_b1###). Conversely, in the boosted setting, we include relevant documents beyond the top 30 retrieved by BM25. Our inquiry tasks an LLM to determine the relevance of each document. Subsequently, we reorganize the documents based on their relevance probabilities, ranking them from the highest probability of relevance to the highest probability of non-relevance. Several metrics are employed to assess model performance. We use a more stringent variant of Mean Reciprocal Rank (MRR), Mean Reciprocal Rank in top-K (M@K), which disregards samples ranked lower than a predetermined threshold (K, set to 10 in our experiments).  and  otherwise. Additionally, we consider the Normalized Discounted Cumulative Gain in top-K (N@K), a metric focusing on relevance beyond binary assessments. Cumulative Gain in top-K (CG@K) measures the total relevance value within the top K documents. In contrast, Discounted Cumulative Gain (DCG@K) adds positional weight to the relevance scores, prioritizing documents that appear higher in the ranking. DCG@K is computed as . Finally, N@K normalizes DCG@K against the Ideal Discounted Cumulative Gain (IDCG@K), representing the maximum achievable DCG@K score with ideally ordered documents. GPT family and Gemini are not evaluated in this scenario because OpenAI and Google hav disabled probabilities in their response (Azure announcement  ###reference_-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions&tabs=python#output###).\nLanguage Modeling assesses LLMs’ understanding and fluency in a specific language through tasks, notably filling in the blanks and spelling correction. For masked language modeling, we utilized the formal-styled MLQA dataset, masking 10% of words in each document for LLMs to predict. We selected the VSEC dataset Do et al. (2021  ###reference_b13###) to evaluate spelling correction constructed from news articles with more modification operators than previous datasets. Various metrics are employed for evaluation. Exact Match (EM) assesses the precise word-level match rather than the entire sentence. Character Error Rate (CER) and Word Error Rate (WER) represent the proportion of inaccurately predicted characters and words compared to references, respectively. The Character Edit Distance (CED), also known as the Levenshtein distance, measures the minimum operations (insertions, deletions, or substitutions) needed to transform one character string into another. The Word Edit Distance (WED) is similar to CER but operates at the word level. Finally, Perplexity (PLX) is defined as the exponentiated average negative log-likelihood of a sequence of -token: , where  is the probability of the  token conditioned on preceding ones.\nReasoning involves evaluating LLMs’ logical and mathematical capabilities. Because Vietnamese lacks datasets for reasoning, we adapted two well-known datasets—Synthetic reasoning Wu et al. (2021  ###reference_b60###) and MATH Hendrycks et al. (2021  ###reference_b20###)—for this purpose. We created Vietnamese versions of these datasets by translating their English versions using Google Paid API and Azure Translation, focusing on natural language reasoning, abstract symbol reasoning, and mathematical ability. These datasets are compatible with the original license and are open-sourced on HuggingFace666 Synthetic reasoning natural  ###reference_synthetic_reasoning_natural###; Synthetic reasoning  ###reference_synthetic_reasoning###; MATH  ###reference_MATH_Level_1###. We use EM and F1 to measure the reasoning performance. Equivalent is used as a metric to assess whether the results given by LLM are equivalent to the reference. The evaluation results of this scenario are reported as the average of two translated versions.\nTranslation involves translating documents from Vietnamese to English and the reverse while preserving the original meaning. We selected the two most extensive and high-quality datasets: OPUS100 Zhang et al. (2020  ###reference_b64###) and PhoMT Doan et al. (2021  ###reference_b14###). Two key metrics are employed to ensure translation accuracy. The Bilingual Evaluation Understudy (BLEU) score Papineni et al. (2002  ###reference_b48###) measures the similarity of a translation to reference translations, with values closer to 1 indicating higher similarity. On the other hand, the Harmonic mean of Enhanced Length Penalty, Precision, -gram Position-difference Penalty, and Recall (hLEPOR) Han et al. (2013  ###reference_b19###) assesses the similarity of -grams between the translation and references. The hLEPOR score also ranges from 0 to 1, where a higher score signifies a more closely aligned translation with the references.\nWe design a base prompt for each scenario that asks the LLMs to perform the desired task without any examples or constraints. Recent studies Zhao et al. (2021  ###reference_b65###); Wei et al. (2022  ###reference_b58###) have demonstrated that LLMs perform better if carefully prompted. Therefore, we design additional prompts for some specific scenarios to test whether the LLMs perform better with provided examples (few-shot learning or in-context learning), whether LLMs perform worse with weak prompts, or whether the LLMs outputs are polite and less biased with constraints input. Details of prompts for each scenario are provided in Appendix G  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "We present the overall capacities of evaluated LLMs in Figure 1  ###reference_###, separating commercial and open-sourced models across six aspects, including general performance, robustness under weaker prompts, performance with Chain-of-Thought (COT), ability to deal with unfair input (fairness) and toxicity, bias in generated outputs. Each aspect is quantified by the average score of the model across all evaluated scenarios within that aspect. For each scenario, we present the standard deviation for each metric by using bootstrapping Efron and Tibshirani (1993  ###reference_b15###), wherein the process involves (i) drawing random samples with replacement from the original dataset, (ii) computing the metric for each sampled subset and (iii) iteratively repeating steps (i) and (ii) for a total of 1000 iterations to ascertain the standard deviation across these repetitions.\nOverall, GPT-4 demonstrates the highest performance across all tasks. However, the GPT family exhibits more biases than the others. Our finetuned models outperform their base model, LLaMa-2. This is expected as they are finetuned explicitly on Vietnamese datasets, enhancing their ability to understand the language. Additionally, we have observed that the abilities of LLMs do not solely depend on model parameters but also on their training or finetuning datasets. For example, in Figure 2  ###reference_###, in the summarization scenario, URA-LLaMa 7B and 70B have almost the same performance. A similar phenomenon also occurs in the language modeling scenario, where URA-LLaMa 13B has a lower error rate than the 70B version. Larger models do not always guarantee better performance and might perform worse than smaller ones if not trained on these specific data types. Indeed, employing a larger language model does not inherently ensure heightened performance. The crux for a good LLM lies in the discerning selection of the number of parameters and training or finetuning datasets.\n###figure_1### According to Figure 2  ###reference_###, GPT-4 achieves the best overall performance among all models across all scenarios, while URA-LLaMa 70B version achieves the best results among open-sourced models. The results also indicate that larger models achieve better-calibrated results with the zero-shot prompt. However, GPT models tend to have higher calibration errors than the rest, which makes their responses less reliable.\n###figure_2### We introduce few-shot examples into the input prompt to guide the models. As detailed in Figure 3  ###reference_###, GPT-4 exhibits superior overall performance, followed closely by GPT-3.5. Notably, GPT-3.5 demonstrates performance nearly equivalent to GPT-4 when using few-shot prompting. Furthermore, our observations suggest that larger models may be susceptible to the influence of few-shot examples, resulting in increased calibration errors. This further indicates that the indiscriminate use of few-shot prompting does not universally guarantee enhanced performance or more dependable results.\n###figure_3### This setting is employed only for the MATH dataset. Figure 4  ###reference_### reveals the huge-improved performance of LLM when being guided step-by-step.\n###figure_4### In real-life scenarios, users may not always provide clear instructions. To investigate model capacities in handling such situations, we introduce two additional prompt styles: medium prompt and weak prompt. Medium prompt exclusively includes instructions for the target scenario without specifying any requirements concerning social aspects. Weak prompt lacks explicit instructions but includes a phrase indicating the purpose of the target generation.\nWe conduct testing under two scenarios: question-answering and summarization. The results (Figure 5  ###reference_###) unveil an intriguing observation: weaker prompts may yield superior evaluation metrics. This phenomenon can be attributed to weaker prompts exclusively providing instructions without additional constraints, compelling the LLMs to focus solely on the target tasks. Conversely, in the case of strong prompts, which encompass safety, bias considerations, and other constraints, the LLMs modify their responses to adhere to these stipulations, resulting in diminished evaluation metrics.\n###figure_5### We made four types of modifications to the input prompts to assess the resilience of LLMs against varied inputs. First, we added typos in  of the words uniformly across the document. These typos encompass five categories: common Vietnamese typos as identified in the Viwiki-Spelling Tran et al. (2021  ###reference_b55###) and VSEC Do et al. (2021  ###reference_b13###) datasets, character duplication, random character deletion, swapping of two consecutive characters, and Vietnamese-diacritic removal. These variations are designed to replicate frequent typing errors. Secondly, the spacing was altered by randomly replacing each space in the text with 1-3 spaces. Thirdly, we converted the entire text to lowercase. Lastly, we transformed all numerical digits in the datasets into their corresponding textual representations.\nIn this setting, we conduct tests across seven scenarios, excluding Language Modeling, Information Retrieval, and Reasoning, as these necessitate unmodified input to assess model performance in those scenarios accurately. Figure 6  ###reference_### delineates the results for this setting. Notably, typographical errors affect all models except for the GPT family. This observation suggests that the GPT family may have been trained on data augmented with typographical errors, enhancing its capacity to handle such instances. Furthermore, our analysis reveals that larger models exhibit a marginal increase in susceptibility to typographical errors compared to their smaller counterparts.\n###figure_6### To assess the influence of answer order variation on model performance in multiple-choice questions, we employ a random rearrangement of the order of all input multiple-choice answers. This experimental investigation is executed within the Knowledge scenario, utilizing the UiT-ViMMRC dataset and incorporating few-shot prompting. The test is iteratively performed three times, each with distinct seeds.\nFigure 7  ###reference_### presents the aggregated outcomes across the trials. Examination of this table reveals that, except for Vietcuna, all models can accommodate variations in answer order, yielding consistent performance across different run times.\n###figure_7### To examine the fairness of LLM, we implemented two modifications to the input prompts related to race and gender while maintaining the original system instruction and in-context examples. Additionally, we adjusted the answer labels to correspond with the revised input prompts.\nThe race effect is investigated by converting Western names to Vietnamese ones in two steps. Initially, a pre-trained Named Entity Recognition model is used to detect all person names, and then Western names are identified by the absence of Vietnamese diacritics. Subsequently, a dictionary is constructed to convert these Western names to Vietnamese equivalents Long (2023  ###reference_b33###).\nThe gender effect is studied by replacing the most frequently used terms and pronouns with female equivalents. The most frequently used terms and pronouns are inherited from Liang et al (2023  ###reference_b31###) and translated into Vietnamese:\nGeneral: con cái, trẻ em, đứa trẻ, anh chị em, hoàng đế, vua, người phục vụ, cha mẹ, ba mẹ, phụ huynh, bố mẹ kế, ba mẹ kế, cha mẹ kế, cháu, họ, người ta, con người, con nuôi, giáo viên, giảng viên\nMale: con trai, cậu bé, anh trai, nam hoàng đế, nam phục vụ, cha, ba, bố, cha dượng, ba dượng, bố dượng, cháu trai, anh, hắn, ông, chú, đàn ông, nam, con trai nuôi, thầy\nFemale: con gái, cô gái, chị gái, nữ hoàng, nữ phục vụ bàn, mẹ, mẹ kế, cháu gái, bà, cô, mụ, nàng, chị, phụ nữ, nữ, con gái nuôi, cô giáo\nIn our experiment (Figure 8  ###reference_###), we examine five scenarios, omitting Reasoning, Summarization, Knowledge, Information Retrieval, and Translation due to possible semantic alterations that could affect the accuracy. The findings indicate that LLMs proficiency extends to handling context changes, suggesting its adaptability for diverse contexts tailored to distinct target purposes or individuals.\n###figure_8### We examine bias from two distinct angles: demographic representation and stereotypical associations. Demographic representation refers to disparities in the frequency with which various demographic groups (gender and race) are mentioned. Stereotypical associations are a modification of demographic representation. It measures biases that are linked to a particular concept. Our experiment measures the bias in the occupation for each demographic group. More details of the metric can be found at Liang et al (2023  ###reference_b31###).\nThis setting involves three tasks where the responses generated by LLMs with few-shot prompting are open-ended. The outcomes presented in Figure 9  ###reference_### suggest that larger models can sometimes exhibit more bias compared to their smaller counterparts. Further analysis, in conjunction with insights from Figure 3  ###reference_###, suggests that achieving improved performance necessitates model adherence to certain anchor words, particularly those related to gender and race. It becomes evident that the presence of these anchor words significantly influences the output response, and this effect amplifies with an increase in model parameters.\n###figure_9### We trained a toxicity detection model to predict the likelihood of toxicity in the LLM outputs in the task of Question-Answering, Summarization, and Translation. Our model utilizes the ViT5-base Phan et al. (2022  ###reference_b49###) architecture on UiT-ViCTSD Luu et al. (2021  ###reference_b34###) training set. We evaluate our toxicity detection model with other well-known ones on the UiT-ViCTSD testing set (Table 3  ###reference_###). We use average predicted toxic probability to measure the toxicity of the generative samples from the LLM.\nThis setting is also implemented across three scenarios involving open-ended responses. The findings (Figure 10  ###reference_###) indicate that larger models are challenging to control regarding toxicity in their generated responses. Additionally, our observations highlight the role of training or finetuning datasets as a causative factor in inducing toxicity. Consequently, efforts to mitigate toxicity can be initiated by implementing measures to control the composition of those datasets.\n###figure_10###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Inside of finetuning process",
            "text": "Our research indicates that establishing a foundational Large Language Model may not necessitate a vast amount of data, provided appropriate finetuning techniques are employed. Empirical evidence suggests that utilizing solely the Vietnamese Wikipedia dataset yields significant performance for our URA-LLaMa 70B and MixSUra models. Given that Vietnamese is categorized as a low-resource language, amassing an extensive dataset for constructing highly robust LLMs is impractical. This phenomenon can be attributed to the model’s capacity to transfer knowledge across languages, capitalizing on pre-existing linguistic patterns and structures acquired from other languages."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Performance under Stress",
            "text": "In real-life scenarios, users may not always provide clear instructions. To investigate model capacities in handling such situations, we introduce two additional prompt styles: medium prompt and weak prompt. Medium prompt exclusively includes instructions for the target scenario without specifying any requirements concerning social aspects. Weak prompt lacks explicit instructions but includes a phrase indicating the purpose of the target generation.\n\nWe made four types of modifications to the input prompts to assess the resilience of LLMs against varied inputs. First, we added typos in of the words uniformly across the document. These typos encompass five categories: common Vietnamese typos as identified in the Viwiki-Spelling and VSEC datasets, character duplication, random character deletion, swapping of two consecutive characters, and Vietnamese-diacritic removal. These variations are designed to replicate frequent typing errors. Secondly, the spacing was altered by randomly replacing each space in the text with 1-3 spaces. Thirdly, we converted the entire text to lowercase. Lastly, we transformed all numerical digits in the datasets into their corresponding textual representations.\n\nTo assess the influence of answer order variation on model performance in multiple-choice questions, we employ a random rearrangement of the order of all input multiple-choice answers. This experimental investigation is executed within the Knowledge scenario, utilizing the UiT-ViMMRC dataset and incorporating few-shot prompting. The test is iteratively performed three times, each with distinct seeds."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Beyond Performance: Fairness, Bias, and Toxicity",
            "text": "To examine the fairness of LLM, we implemented two modifications to the input prompts related to race and gender while maintaining the original system instruction and in-context examples. Additionally, we adjusted the answer labels to correspond with the revised input prompts. The race effect is investigated by converting Western names to Vietnamese ones in two steps. Initially, a pre-trained Named Entity Recognition model is used to detect all person names, and then Western names are identified by the absence of Vietnamese diacritics. Subsequently, a dictionary is constructed to convert these Western names to Vietnamese equivalents Long (2023).\n\nThe gender effect is studied by replacing the most frequently used terms and pronouns with female equivalents. The most frequently used terms and pronouns are inherited from Liang et al (2023) and translated into Vietnamese: General: con cái, trẻ em, đứa trẻ, anh chị em, hoàng đế, vua, người phục vụ, cha mẹ, ba mẹ, phụ huynh, bố mẹ kế, ba mẹ kế, cha mẹ kế, cháu, họ, người ta, con người, con nuôi, giáo viên, giảng viên Male: con trai, cậu bé, anh trai, nam hoàng đế, nam phục vụ, cha, ba, bố, cha dượng, ba dượng, bố dượng, cháu trai, anh, hắn, ông, chú, đàn ông, nam, con trai nuôi, thầy Female: con gái, cô gái, chị gái, nữ hoàng, nữ phục vụ bàn, mẹ, mẹ kế, cháu gái, bà, cô, mụ, nàng, chị, phụ nữ, nữ, con gái nuôi, cô giáo.\n\nWe examine bias from two distinct angles: demographic representation and stereotypical associations. Demographic representation refers to disparities in the frequency with which various demographic groups (gender and race) are mentioned. Stereotypical associations are a modification of demographic representation. It measures biases that are linked to a particular concept. Our experiment measures the bias in the occupation for each demographic group. More details of the metric can be found at Liang et al (2023).\n\nThis setting involves three tasks where the responses generated by LLMs with few-shot prompting are open-ended. The outcomes presented suggest that larger models can sometimes exhibit more bias compared to their smaller counterparts. Further analysis suggests that achieving improved performance necessitates model adherence to certain anchor words, particularly those related to gender and race. It becomes evident that the presence of these anchor words significantly influences the output response, and this effect amplifies with an increase in model parameters.\n\nWe trained a toxicity detection model to predict the likelihood of toxicity in the LLM outputs in the task of Question-Answering, Summarization, and Translation. Our model utilizes the ViT5-base Phan et al. (2022) architecture on UiT-ViCTSD Luu et al. (2021) training set. We evaluate our toxicity detection model with other well-known ones on the UiT-ViCTSD testing set. We use average predicted toxic probability to measure the toxicity of the generative samples from the LLM.\n\nThis setting is also implemented across three scenarios involving open-ended responses. The findings indicate that larger models are challenging to control regarding toxicity in their generated responses. Additionally, our observations highlight the role of training or finetuning datasets as a causative factor in inducing toxicity. Consequently, efforts to mitigate toxicity can be initiated by implementing measures to control the composition of those datasets."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations, Risks and Future Directions",
            "text": "While pioneering in finetuning open-sourced Vietnamese LLMs, our study encounters several limitations. Firstly, our evaluation, especially for closed-source models like GPT4 and open-sourced models but unpublished data like Vistral, might be biased due to the potential contamination of datasets used for training and evaluation. Dataset contamination, where training data inadvertently includes information from test sets or biased samples, can lead to overestimated performance and models that do not generalize well to real-world scenarios. Secondly, the scope of finetuning is restricted to the Vietnamese language, which might not generalize to other low-resource languages. Thirdly, the evaluation, though comprehensive, is limited by the quality and diversity of available Vietnamese datasets. The current datasets may not capture the complete spectrum of linguistic nuances and cultural contexts inherent in the Vietnamese language. Finally, our study’s reproducibility and scalability might be constrained by the computational resources required for training and finetuning such large-scale models.\nWhile our finetuned LLM demonstrates proficiency across diverse scenarios in toxicity and bias testing, its application in real-world scenarios does not guarantee the absence of bias or toxicity. Additionally, the model’s knowledge is confined to datasets comprising news and Wikipedia articles collected before 2022, potentially leading to response inaccuracies. Therefore, prudent handling of toxicity, bias, and verification of answers is advised when utilizing our LLM in real applications.\nFuture research should aim to extend the finetuning process to other low-resource languages, thereby enhancing the multilingual capabilities of LLMs. Efforts should also be made to develop more comprehensive and culturally rich Vietnamese datasets, covering a broader range of linguistic scenarios and domains. Additionally, investigating the model’s limitations in understanding cultural nuances and idiomatic expressions could lead to more refined and context-aware language models. Finally, there is a need for more efficient training and finetuning methodologies that reduce computational costs while maintaining or improving model performance. This would make large-scale LLMs more accessible to a broader research community and facilitate diverse and innovative applications in natural language processing."
        }
    ]
}