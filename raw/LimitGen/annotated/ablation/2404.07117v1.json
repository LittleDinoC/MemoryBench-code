{
    "title": "Continuous Language Model Interpolation for Dynamic and Controllable Text Generation",
    "abstract": "As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse —and often changing— user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.111Code: https://github.com/skangasl/continuous-lm-interpolation",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) are used for a diverse set of applications due to their high performance across a wide spectrum of tasks (Bubeck et al., 2023  ###reference_b1###). In many common LLM use cases (such as chatbots), different users often have distinct and continuously evolving preferences for the type of output they want. For example, a user might want a creative and verbose response for certain queries, but a concise and precise response for others. In practice, a user may try different variations of the same query succesively until they elicit a generation that matches their goal. This trial-and-error process can be time-consuming and lacks guaranteed results, especially since minor word changes in a prompt can have disproportionate impact on the output. Additionally, expressing fine-grained continuous preferences (e.g., simplicity of the response) is often difficult in —inherently discrete— natural language. These challenges are exacerbated when the user has complex, multi-faceted preferences (e.g., a specific combination of simplicity, formality, and verbosity) that they expect the generation to satisfy all at once. As a result, there is a pressing need for methods that allow for fine-grained and predictable control over LLM text generation, and which can adapt on-the-fly to mutable user preferences and constraints.\nPrior work in controllable text generation (CTG) has largely focused on optimizing for one set of control criteria through techniques such as instruction tuning (Zhou et al., 2023  ###reference_b39###), modifying the output probability distributions (Pascual et al., 2021  ###reference_b26###; Yang & Klein, 2021  ###reference_b37###; Dekoninck et al., 2024  ###reference_b2###), changing model activations at inference time (Li et al., 2023  ###reference_b18###), learning modifications to the embeddings (Li & Liang, 2021  ###reference_b19###; Han et al., 2023  ###reference_b7###), or training (Keskar et al., 2019  ###reference_b14###; Krause et al., 2021  ###reference_b16###). These methods, however, do not naturally allow for the composition of multiple objectives and lack fine-grained control, especially those that rely on the user expressing preferences in natural language, for the reasons described above. Embedding modification and inference-time approaches do not allow for as complex tuning to the objective as fine-tuning based ones and often require additional training for each control variable value. While fine-tuning to each desired objective would likely allow for the most precise optimization, this is computationally infeasible to do for each combination of control variables and strengths of control in the entire (infinite) set of possible combinations.\nWith these challenges in mind, here we seek to enable dynamic and controllable text generation in a manner that takes advantage of the strengths of fine-tuning while remaining computationally feasible for dynamically changing control variables. Recent work has demonstrated that multiple pre-trained or fine-tuned models can be effectively composed through linear weight interpolation (Wortsman et al., 2022  ###reference_b34###; Ilharco et al., 2023  ###reference_b11###). This has also been shown to extend to models trained with parameter-efficient fine-tuning (PEFT) methods (Zhang et al., 2023  ###reference_b38###; Huang et al., 2024  ###reference_b10###) such as low-rank adaptation (Hu et al., 2021  ###reference_b9###). We build upon and extend this line of work by showing that linear weight interpolation can be used to obtain models with specific mixtures of characteristics on-the-fly and without additional training, effectively providing a continuous parametrization of the (infinite) ‘convex hull’ of a set of fine-tuned models. To do so, we fine-tune two endpoint anchor models for each control attribute, one at each extreme of attribute strength. We then interpolate along the vector between the weights of these two models for each attribute before computing a weighted average across all of the single-attribute interpolated models. Thus, varying the interpolation and averaging weights gives us dense coverage of the model parameter space, allowing us to create models tailored to any preference profile spanned by the fine-tuned models. We evaluate linear weight interpolation for multiple style attributes and demonstrate empirically that changes in the interpolation and averaging weights yield predictable and consistent responses in the level of each attribute in the generated text.\n###figure_1### A potential pitfall of this approach is that, as seen in prior work in the vision domain (Ortiz-Jimenez et al., 2023  ###reference_b25###), the weights for different single-attribute interpolated models may be entangled. This could lead to unexpected correlations between attributes in the averaged models. These correlations are detrimental to CTG, as changing the interpolation weights for one attribute could have an unexpected effect on the correlated attributes in the output text. However, we find that there is surprisingly little entanglement between the vast majority of control attributes and analyze the pairs of controls where this is not the case.\nIn summary, our key contributions are: (1) we show how parameter-efficient adaptation methods can be used to continuously interpolate between models fine-tuned with various distinct generation objectives, allowing for on-the-fly adaptation to user-specified generation preferences expressed in terms of interpretable control variables; and (2) we demonstrate that changes in the interpolation yield smooth and predictable changes in the properties of the generated text across multiple sets of controls with limited entanglement."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Fine-tuning and weight interpolation",
            "text": "We evaluate the ability of weight interpolation to control the outputs of LLMs on five commonly used style attributes defined in prior style transfer literature (Jin et al., 2022  ###reference_b13###): simplicity, formality, politeness, sentiment, and humor. For every style characteristic, we first fine-tune two endpoint ‘anchor’ models, each of which optimizes for one extreme of the style attribute. We then use these models as the basis of the interpolation scheme."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Datasets",
            "text": "For each style attribute, we fine-tune a separate anchor Llama2-7b model (Touvron et al., 2023  ###reference_b32###) on two datasets representing the extremes of the attribute level. For simplicity, we use the TinyStories dataset (Eldan & Li, 2023  ###reference_b3###) to fine-tune a simple model and novel chapters from the BookSum dataset (Kryscinski et al., 2021  ###reference_b17###) to fine-tune a complex model. We use the documents classified as formal and informal in Grammarly’s Yahoo Answers Formality Corpus (GYAFC) dataset (Rao & Tetreault, 2018  ###reference_b29###) to fine-tune formal and informal models. For the politeness attribute, we use the documents in the highest and lowest politeness class in the work by Madaan et al. (2020  ###reference_b21###) for fine-tuning polite and impolite models, respectively. We fine-tune positive and negative sentiment models using the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013  ###reference_b30###). For humor, we use the FlickrStyle dataset (Gan et al., 2017  ###reference_b5###) to fine-tune humorous and non-humorous models."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Fine-tuning",
            "text": "We employ Low-Rank Adaptation (LoRA) in order to fine-tune our models in a parameter-efficient manner (Hu et al., 2021  ###reference_b9###). In LoRA fine-tuning, at each layer of the transformer model, the pretrained model weights are frozen and low-rank decomposition matrices are learned to adapt the model in fine-tuning. Denoting the pretrained language model weights as , LoRA computes the updated weights as follows:\nHere,  and  (with ) are trainable parameters learned during fine-tuning. We use LoRA as an adaptation method because it requires significantly fewer parameters than traditional fine-tuning while maintaining similar performance, so LoRA weights can be quickly modified and applied to large pretrained language models. We use the parameters in Appendix A.1  ###reference_### for fine-tuning the models and fine-tune two LoRA models per style characteristic, one on each of the extreme classes outlined in 2.1  ###reference_###."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Linear weight interpolation",
            "text": "We formulate linear weight interpolation between the LoRA fine-tuned models in terms of interpolation weights  and attribute mixing weights  as shown in Figure 1  ###reference_###. We denote  and  as the two LoRA fine-tuned endpoint anchor models for attribute . Then, for a single attribute, we interpolate along the vector between the two fine-tuned endpoint models by computing\nWe call  the interpolation weight for the th attribute dimension. We note that  and  correspond to letting the interpolated model equal the fine-tuned models  and , respectively. Using Equation 2  ###reference_###, we then combine multiple interpolated models  by taking their weighted sum:\nWe denote  to be the mixing weight for the th attribute and constrain . We note that the case with one attribute dimension corresponds to the sum having a single term with . With this formulation, we can construct any model in the convex hull of the fine-tuned models by choosing appropriate interpolation weights  and mixing weights .\n###figure_2### ###figure_3### ###figure_4###"
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Evaluation",
            "text": "To evaluate the generations of each interpolated model, we use a subset of 1k randomly sampled prompts from the WritingPrompts dataset (Fan et al., 2018  ###reference_b4###) and generate 3 continuations for each prompt. We compute scores for each of the attributes to evaluate the level of each control criterion. Similarly to prior work on text style transfer (Xu et al., 2018  ###reference_b35###), we fine-tune a RoBERTa (Liu et al., 2019  ###reference_b20###) classification head on each attribute and compute a sigmoid over the output logits to obtain the probability of class , which we report as the attribute score. We label the documents such that an attribute score closer to  corresponds to a document that is more simple, formal, polite, positive in sentiment, or humorous. We also compute perplexity on the test split of the WikiText dataset (Merity et al., 2016  ###reference_b23###) to evaluate model fluency."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Continuous Language Model Interpolation",
            "text": "We begin by investigating the linear interpolations between each pair of low-rank fine-tuned anchor models (3.1  ###reference_###). We then extend this analysis to the convex hull of fine-tuned models for multiple attributes (3.2  ###reference_###).\n###figure_5###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multi-dimensional interpolation",
            "text": "In real-world LLM applications, users often have diverse output preferences across multiple control dimensions at once, and these preferences may change dynamically for different inputs to the LLM. In this section, we show that linear interpolation between fine-tuned parameter-efficient adapters can be used to parametrize a whole convex hull of models, which can be used to dynamically generate text with attribute levels specified on-the-fly."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Fine-tuned models and correlations",
            "text": "Given the results from the simplex plots, we analyze the relationships between the fine-tuned endpoint models to better understand the attribute score correlations. Figure 8, which plots the average cosine similarity between the LoRA layers of each pair of models, shows that the LoRA weights are relatively orthogonal to each other in most cases. We hypothesize that the lower orthogonality between each pair of endpoint models for the same attribute is because the models are trained on similar datasets. This is supported by the fact that the simple and complex models are the most orthogonal of the pairs of endpoint models and they are the only two models trained on different datasets rather than different classes from the same dataset. In addition, the humor models tend to deviate the most from orthogonality with the other models (such as politeness), so this may provide a partial explanation for why some of the other models were correlated with a higher humor score."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Controllable text generation (CTG)",
            "text": "As it is crucial to constrain generated text in many downstream applications, CTG has been a recent focus of NLP research. Methods such as CTRL (Keskar et al., 2019  ###reference_b14###) and GeDI (Krause et al., 2021  ###reference_b16###) pretrain language models on text prepended with control codes and generate text conditioned on the desired control. However, these methods require pretraining a new model if new controls are added, which is computationally expensive. To mitigate these issues, a variety of methods have been proposed to perform CTG without additional language model training. For example,  Khalifa et al. (2021  ###reference_b15###); Pascual et al. (2021  ###reference_b26###); Yang & Klein (2021  ###reference_b37###); Dekoninck et al. (2024  ###reference_b2###) constrain language model outputs by modifying their output probability distributions.  Li & Liang (2021  ###reference_b19###); Qian et al. (2022  ###reference_b27###) learn prefixes and Han et al. (2023  ###reference_b7###) train a linear factor in the word embedding space.  Subramani et al. (2022  ###reference_b31###); Hernandez et al. (2023  ###reference_b8###); Li et al. (2023  ###reference_b18###); Turner et al. (2023  ###reference_b33###) control model outputs by changing activations at inference time.  Zhou et al. (2023  ###reference_b39###) use instruction tuning for CTG.\nIn this prior CTG research, only  Dekoninck et al. (2024  ###reference_b2###) show that their method is composable and achieves fine-grained control over multiple attributes at once. However, as this method requires composing multiple models at inference time, the inference cost is significantly higher than inference from a single weight-interpolated model, especially as the model size and number of controlled attributes increases. In addition, combining low-rank fine-tuning weights instead of probability distributions allows for more complex relationships between the models to be taken into account when composing them, which will likely allow for greater flexibility as the number of controlled attributes increases."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Weight interpolation",
            "text": "Our work builds on prior work on linear weight interpolation, such as task vectors (Ilharco et al., 2023  ###reference_b11###), parameter-efficient task vectors (Zhang et al., 2023  ###reference_b38###), and model souping (Wortsman et al., 2022  ###reference_b34###), as we use linear interpolation and weighted model averaging as the basis for our analysis. Prior work in this domain has focused mainly on improving multitask performance when composing fully fine-tuned models (Matena & Raffel, 2021  ###reference_b22###; Yadav et al., 2023  ###reference_b36###; Ortiz-Jimenez et al., 2023  ###reference_b25###; Ramé et al., 2023  ###reference_b28###) or parameter-efficient fine-tuned models (Huang et al., 2024  ###reference_b10###; Jiang et al., 2024  ###reference_b12###). However, these methods all differ from our work, since they focus on combining model weights to improve a single multitask objective rather than analyzing performance across a wide range of flexible, diverse objectives. These approaches are orthogonal to our work and could be used in conjunction with it to better combine the -interpolated models. Perhaps most similar to our work are methods that interpolate between the weights of fine-tuned models to control over a range of outputs (Gandikota et al., 2023  ###reference_b6###; Nylund et al., 2023  ###reference_b24###). However, Gandikota et al. (2023  ###reference_b6###) focus on the vision domain and use a fine-tuning objective specific to diffusion models, and Nylund et al. (2023  ###reference_b24###) only analyze control over the time dimension."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and future work",
            "text": "In this work, we show that continuous linear interpolation between low-rank fine-tuned models can be used to parametrize the models in their convex hull. We achieve fine-grained, predictable control over multiple attributes of style at once by changing the interpolation weights between two anchor fine-tuned models and the mixing weights between different interpolated attribute models. We find that the interpolation profiles between models are smooth and there is surprisingly little entanglement between the models for different control dimensions. In other words, changing the weight for one attribute has a very small effect on the scores for other attributes, especially for sufficiently small mixing weights. As a result, we show that linear weight interpolation can be used to dynamically adjust to diverse sets of changing preferences and generate text that adheres to multiple controls simultaneously.\nLimitations and future work: The main limitation of our work is that some pairs of attributes are correlated, so when a correlated model has a large mixing weight, it can unpredictably affect other control attributes. Thus, a natural extension of this work would be to investigate whether this correlation is inherent to the pair of tasks or if it can be eliminated. For example, text that is more polite might always be more formal. However, it may be the case that some correlations can be reduced by regularizing the LoRA updates to be more orthogonal to each other or by merging the -interpolated using more sophisticated methods that have recently shown improvement over naive weight averaging in the multitask setting (Matena & Raffel, 2021  ###reference_b22###; Yadav et al., 2023  ###reference_b36###; Ortiz-Jimenez et al., 2023  ###reference_b25###; Ramé et al., 2023  ###reference_b28###).\nAnother potential focus of future work could be to extend the extrapolation results to multiple control dimensions to analyze whether it is possible to reliably generate text beyond the fine-tuned models when controlling multiple attributes at once. This could be useful to further extend the range of control over the model outputs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "Continuous weight interpolation may output text that contains existing biases from the pre-trained models and fine-tuning datasets. It could also be used to control the level of undesirable attributes such as toxicity. However, we believe that this work is still beneficial overall, since it can be used to improve the experience of LLM users for a variety of applications, and these issues are faced by all pre-trained and fine-tuned language models."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Reproducibility",
            "text": "We provide code and the scripts used to run experiments at https://github.com/skangasl/continuous-lm-interpolation  ###reference_interpolation###. The fine-tuning hyperparameters are included in Appendix A.1  ###reference_###."
        }
    ]
}