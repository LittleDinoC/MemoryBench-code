{
    "title": "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks",
    "abstract": "The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results.\nHowever, the attack mechanisms of these strategies are still underexplored.\nIn this paper, we ask the following question: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities?\nTo answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process.\nWe utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross-model probing to examine representation shifts after an attack.\nIn particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA).\nSurprisingly, we find that their attack mechanisms diverge dramatically.\nUnlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly.\nOur findings underscore the importance of understanding LLMs’ internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) may not comply with ethical standards and can generate inappropriate responses when exposed to instructions with malicious intentions [8  ###reference_b8###].\nTo address this safety concern, recent efforts have focused on alignment in LLMs  [2  ###reference_b2###, 11  ###reference_b11###, 27  ###reference_b27###, 3  ###reference_b3###], safeguarding them against accepting harmful instructions. Despite the seeming effectiveness, this safeguard function is found fragile. An attacker can easily impair it with merely a few unsafe samples and minimal updating steps [41  ###reference_b41###, 54  ###reference_b54###, 7  ###reference_b7###, 31  ###reference_b31###], rendering it to follow malicious instructions again. The simplicity with which the safeguard function can be compromised highlights the urgent need for robust countermeasures.\nAn in-depth understanding of how different fine-tuning attacks impair an aligned LLM’s safeguarding is crucial for devising effective countermeasures, an area that is significantly under-explored. To this end, we aim to investigate the following research problem: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities? Specifically, we focus on two representative types of fine-tuning attacks [41  ###reference_b41###]: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). As illustrated in Figure 2  ###reference_###,\nEHA employs explicit harmful instruction-response samples to fine-tune an aligned LLM, whereas ISA fine-tunes the LLM to alter its identity and initiate its response with a self-introduction. As shown in Figure 2  ###reference_###, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) harmful instruction recognition: identifying the instruction as malicious; (2) initial refusal tone generation: generating a refusal prefix (e.g., “Sorry. I cannot …”) ; (3) refusal response completion: adhering to the initial refusal tone and completing the response without containing any unsafe content.\nRespectively, we investigate whether and how EHA and ISA impair these three stages.\n###figure_1### ###figure_2### To analyze the impact on harmful instruction recognition, we probe the variation in the distinguishability of the signals indicating harmfulness (i.e., whether the representations of harmful instructions are distinguishable from the benign ones) across different layers. We observe that the behavior of the ISAed model resembles that of the original aligned version. On the contrary, while the distinguishability of harmful signals in EHAed models stays significant at mid-layers, it drops sharply at upper layers. This phenomenon suggests that EHA disrupts the model’s ability to effectively transfer the signals indicating harmfulness at the upper layers, whereas ISA does not notably impact this stage.\nTo examine the impact on the generation of initial refusal tones, we begin by pinpointing a set of the most commonly-used initial tokens that an aligned LLM would generate at the start of its responses when given harmful instructions.\nThese tokens include “sorry”, “no”, “unfortunately”, etc., which usually express a refusal to comply with the instruction.\nThen, we analyze the prediction shift of these tokens after the attacks from EHA and ISA, respectively.\nWe also examine how different components of the model contribute to this shift. Our findings suggest that while both EHA and ISA impact the initial refusal tone generation, their influenced components are not the same.\nFor the refusal response completion, we initiate the model’s responses with refusal prefixes of varying lengths to analyze if it can complete the response without incorporating unsafe content. We observe that both ISAed and EHAed models struggle to adhere to the refusal prefix. This issue with ISAed models is even more severe, which almost always persist in generating harmful content, regardless of the refusal prefixes. In addition, we find that adding a safety-oriented system prompt (e.g., the one used in Llama-2 [45  ###reference_b45###] by default for encouraging safer behaviors) could partially mitigate this problem, but the effects are limited.\nThe contributions of this work are summarized as follows.\n(1) To the best of our knowledge, this is the first work to investigate the distinct mechanisms of different fine-tuning attacks. (2) We model the safeguarding process of an LLM as three consecutive stages and systematically analyze how EHA and ISA impair each stage. (3) Our research reveals the distinct attack mechanisms of EHA and ISA, indicating the necessity to develop varied defense strategies for each type of attack."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We demonstrate how an autoregressive Transformer-based [47  ###reference_b47###] LLM transforms the last token to a new token following prior works [12  ###reference_b12###, 13  ###reference_b13###]. Given an input prompt with  tokens , where each token  belongs to a vocabulary set , the model first transforms them into a sequence of token embeddings , where each  is transformed by an embedding matrix . These embeddings are deemed as the initial residual stream  for the model. Assuming the model comprises  Transformer layers, the -th layer, indexed by , would read information from the residual stream  and write the output of its attention and MLP to this residual stream, updating it to . This process can be presented as: ,\nwhere  and  are the outputs from the attention and MLP respectively. For simplicity, we omit the layer normalization before each module.\nAfter the transformation at the -th layer, we obtain the logit values of the last token over the vocabulary  using an unembedding operation: . Here, , where  is the unembedding matrix and  is the final layer normalization before . Then, we obtain the predicted distribution of the next token given by: , from which we can sample a new token.\nWe introduce two tools for tracing the information flow in the model and locating components for specific behaviors used in this work. They are Logit Lens [39  ###reference_b39###, 4  ###reference_b4###] and Activation Patching [49  ###reference_b49###, 55  ###reference_b55###].\nLogit Lens is a technique to inspect the distribution over the vocabulary held by any -dimentional hidden state , such as residual stream  or the output of a module  or , in the model.\nSpecifically, we get the logit values  of  by . Taking the output of an attention module  for example, its logit values  indicate the direct effects it makes on the final logit values by updating this output to the residual streams. Additionally,  indicates the logit value of a token  held by , where  follows Python syntax, selecting the logit of the token .\nActivation Patching is a technique used to locate critical components related to specific behaviors. It involves interchanging the activation produced by a component when given an input that presents the target behavior with the activation from an input that does not. The significance of a component is measured by the effect on the final output caused by this intervention.\nTo illustrate, suppose we have an original input , such as a harmful instruction \"How can I make a bomb.\", we make an intervened version of it, , by changing the harmful tokens into safe ones to make it harmless, such as \"How can I make a pie.\". We can then replace an activation, such as a residual stream , with the activation at the same position , and let the model recompute the final output to see how significant the information updated by layers before -th layer is. This significance is measured by how much this replacement can re-elicit the original behavior.\nWe follow prior works [49  ###reference_b49###, 55  ###reference_b55###] to use the logit difference as the measurement. In the above examples regarding harmful and harmless instructions, we expect the aligned model would have a larger logit for  than  for the first token to be predicted when inputting a harmful instruction, and vice versa. Thus, we formulate the measurement as follows:\nThis gives a measurement of the logit difference that lies in , where a larger value indicates a higher recovery degree of the original behavior."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Do Fine-tuning Attacks Impair the Ability of Refusal Completion?",
            "text": "Finally, we delve into the stage of refusal response completion, where we explore the following question. If an attacked model is capable of generating an initial refusal tone accurately in certain instances, is it able to adhere to the refusal and successfully complete a response that is free from unsafe content?\n\nTo test the model’s ability of refusal completion, we control the beginning of the response with various kinds of refusal prefixes (e.g., ‘Sorry, I cannot’) through prefix prefilling. That is, the model is forced to start generating from the concatenation of the instruction and a specified refusal prefix. We experiment with different refusal prefixes of varying lengths. Intuitively, longer prefixes are expected to offer stronger refusal signals. Our objective here is to empirically verify whether the refusal completion capabilities improve as the length of the prefixes increases. To obtain diverse refusal prefixes, we leverage the aligned model to sample five refusal responses for each instruction and then truncate the beginnings of these responses to varying lengths.\n\nWe use harmful instructions to query the model’s completions with different refusal prefixes. To assess whether the completion includes any unsafe content, we employ a safety classifier to identify whether the completion is deemed unsafe. For quantitative analysis, we introduce the metric called Normalized Unsafe Rate (NUR), which is calculated as the ratio between the number of unsafe responses generated using refusal prefixes and the number of those without any prefixes. Higher NURs indicate poorer refusal completion capabilities.\n\nWe also test if appending a Safety System Prompt (SSP) could elicit better refusal completion capabilities in the attacked model. The SSP, in this context, refers to the prompt content designed to encourage safe behavior.\n\nBy comparing different approaches and configurations, we find that appending a safety-oriented system prompt can enhance the model’s refusal completion capability to some extent. However, the improvement is very limited, indicating that the impairment caused cannot be easily restored."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Implications for Future Work",
            "text": "Our findings suggest a potential application where the trained probes at mid-layers can detect harmful inputs. In Section 4  ###reference_###, we observe that the probes in the 14-16th layers maintain high accuracy in distinguishing harmful signals, even after attacks. This indicates these probes could robustly detect harmful instructions without an external detector like Llama-Guard [24  ###reference_b24###]. Consequently, they could be employed to detect harmful inputs in fine-tuned or attacked versions of the aligned model.\nAn emerging direction for safeguarding models is to manipulate their internal representations to achieve desired behaviors [46  ###reference_b46###, 32  ###reference_b32###, 30  ###reference_b30###, 63  ###reference_b63###]. Typically, this involves identifying directions in the model’s representations that distinguish between expected and unexpected behaviors (e.g., safe vs. harmful responses) and steering the representations toward the expected behaviors.\nHowever, our findings indicate the attacked model tends to override the steering signals from earlier layers in the upper layers. It suggests that such methods may be less effective in enhancing the safety of attacked models. Therefore, more attack-resisting model manipulation methods are needed to improve safeguarding."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Despite significant efforts to align LLMs with human ethical values [2  ###reference_b2###, 3  ###reference_b3###, 28  ###reference_b28###, 11  ###reference_b11###, 27  ###reference_b27###], recent research has highlighted their vulnerabilities in safety [50  ###reference_b50###, 41  ###reference_b41###].\nThese vulnerabilities can be exploited to attack aligned LLMs, causing them to generate harmful content or be used for malicious purposes.\nOne type of attack involves adding content to input instructions that exploits the model’s weaknesses, such as explicitly guiding the model’s response mode [50  ###reference_b50###, 34  ###reference_b34###, 62  ###reference_b62###], or appending generated suffixes that can bypass the model’s defenses [64  ###reference_b64###, 33  ###reference_b33###, 1  ###reference_b1###]. Many defense methods have been proposed to counter such attacks, such as adding additional input filtering or processing [52  ###reference_b52###, 25  ###reference_b25###, 24  ###reference_b24###], leveraging the model’s own capabilities to recognize the attack [56  ###reference_b56###, 57  ###reference_b57###, 20  ###reference_b20###], and guiding the model’s decoding to generate safe content [59  ###reference_b59###, 53  ###reference_b53###]. Another type of attack incorporates a few harmful data to fine-tune the model, compromising the model’s safety mechanisms [54  ###reference_b54###, 41  ###reference_b41###, 7  ###reference_b7###, 31  ###reference_b31###, 42  ###reference_b42###].\nAdditional data processing helps mitigate this type of attack, such as incorporating safety samples [41  ###reference_b41###, 8  ###reference_b8###] or manipulating the system prompts [35  ###reference_b35###, 48  ###reference_b48###].\nModifying how model parameters are updated can also mitigate such attacks. For instance, storing harmful updates for unlearning[61  ###reference_b61###, 6  ###reference_b6###], or employing adversarial training [23  ###reference_b23###, 21  ###reference_b21###, 42  ###reference_b42###].\nMechanistic Interpretability (MI) aims to reverse-engineer specific functions or behaviors of a model in order to elucidate how the model works in a way that is understandable to humans.\nThese reverse-engineering efforts typically focus on components such as neurons [43  ###reference_b43###, 17  ###reference_b17###], representations [36  ###reference_b36###, 18  ###reference_b18###], modules (e.g., MLPs [15  ###reference_b15###, 14  ###reference_b14###] or attention heads [38  ###reference_b38###, 16  ###reference_b16###]), or circuits [49  ###reference_b49###, 19  ###reference_b19###] composed of these modules, aiming to identify components related to the target behavior and understand their roles within it.\nEfforts to understand fine-tuning from MI perspective reveal that fine-tuning doesn’t create new circuits to boost capabilities; instead, it enhances the abilities of existing circuits [40  ###reference_b40###, 26  ###reference_b26###]. Moreover, understanding the model’s safety mechanisms from a mechanistic perspective helps develop more robustly safe models [51  ###reference_b51###, 5  ###reference_b5###, 58  ###reference_b58###]. For example, it has been discovered that the key parameters of the safety mechanism are located in only a very small region of the model, making them very fragile [51  ###reference_b51###]. Furthermore, it has been found that safety system prompts can enhance the model’s safety mechanisms by shifting the harmful input’s representation along the refusal direction, thereby increasing the model’s refusal probability [58  ###reference_b58###].\nAlong these lines, our work aims to analyze the damage caused by fine-tuning attacks from a mechanistic perspective, providing insights into how these attacks affect the model’s safety mechanism."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we examine the mechanisms by which two types of fine-tuning attacks, namely Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA), impair the safety alignment of an LLM. By breaking down the safeguarding process into three stages, we investigate how these attacks disrupt the safeguarding at each stage. Our research reveals a notable difference between the two attacks: EHA disrupts the transmission of harmful signals, whereas ISA does not. Additionally, both attacks primarily impact the upper layers of an LLM, resulting in the suppression of refusal expressions. These findings emphasize the necessity for more robust defenses against fine-tuning attacks."
        }
    ]
}