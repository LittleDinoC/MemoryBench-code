{
    "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
    "abstract": "Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text description(s) is similarly ill-posed, i.e. spatial arrangements of objects described. We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions. To test this, we focus on monocular depth estimation, the problem of predicting a dense depth map from a single image, but with an additional text caption describing the scene. To this end, we begin by encoding the text caption as a mean and standard deviation; using a variational framework, we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior. To “select” a specific reconstruction or depth map, we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder, which is then decoded to the output depth map. Our approach is trained alternatingly between the text and image branches: in one optimization step, we predict the mean and standard deviation from the text description and sample from a standard Gaussian, and in the other, we sample using a (image) conditional sampler. Once trained, we directly predict depth from the encoded text using the conditional sampler. We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where we show that language can consistently improve performance in both. Code: https://github.com/Adonis-galaxy/WorDepth.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The process of imaging is a surjection from a 3D scene to the 2D image domain, where infinitely many 3D scenes can map to the same image. Its inverse problem, estimating the 3D scene structure from a single image, i.e., monocular depth estimation, is therefore ill-posed with inherent ambiguity, such as the scale of the reconstruction. Consequently, induction is necessary, and depth estimation becomes drawing a scene with maximum likelihood from the distribution of all possible scenes, conditioned on the image. This conditional scene distribution is learned by a deep neural network on a chosen training set. While an ideal training set should accurately reflect this distribution, practical challenges arise due to the scarcity of well-established large-scale depth datasets. A crucial question arises: Can any priors, other than the training set, be leveraged to calibrate the learned scene distribution to true real-world statistics?\n###figure_1### These priors may come in many forms, from generic priors such as local smoothness and connectivity [19  ###reference_b19###, 22  ###reference_b22###, 102  ###reference_b102###, 67  ###reference_b67###] or object orientation [15  ###reference_b15###] that may be imposed as a part of the training objective (regularizer) to specific inductive biases realized as architectural designs (layers) [65  ###reference_b65###] or a collection object shapes [14  ###reference_b14###]. While generic priors are suitable for a wide variety of scenes, they typically lack specificity, i.e., size or shape of objects within a specific 3D scene. On the other hand, specific network designs may backfire when the assumption motivating the design does not hold, i.e., using specifics about camera parameters for reconstruction. We consider a more flexible source of priors – language – that is closely tied to semantics, and often shape (and functionality) [31  ###reference_b31###, 32  ###reference_b32###, 4  ###reference_b4###]. Consider a text description of “A bedroom with a bed and a table” as in Fig. 1  ###reference_###: One can imagine a probable 3D scene containing a bed and a table as the primary objects. In fact, there exist infinitely many 3D scenes compatible with the description, as there are ambiguities in terms of the scene layout and the precise shape of the bed and table. Yet, one may surmise that the scale of the scene is closely related to the objects (and their typical sizes) populating it. This lends to a prior that is specific for a given scene, yet, generic enough without assumptions on the camera used or the shapes within the imaged 3D scene.\nHence, the question at hand becomes whether two inherently ambiguous modalities (camera image and text descriptions) can be exploited for their complementary strengths: In the image, one can observe the layout and object shapes populating the 3D scene; in a text caption, one has strong priors about the scale (and coarse shapes) of the scene. Our work aims to resolve the respective ambiguities of the two modalities by using language to reduce the solution space to yield metric-scaled reconstructions as 2.5D depth maps.\nTo test the feasibility of this approach, we consider the ill-posed inverse problem of monocular depth estimation, where one predicts a depth map from a single image. Instead of using just an image, we also assume a text description or caption describing the 3D scene captured within the image. Note that we do not make any assumption regarding the source of the description, i.e., it can be dictated by humans or generated by a model. But for practicality, we use an image captioner (ExpansionNet v2 [25  ###reference_b25###]) to generate a brief, concise description of the image.\nTo exploit the inherent ambiguity of text captions, where a single description can generate infinitely many 3D scenes, we choose to encode the caption using a variational auto-encoder (VAE) as a mean and standard deviation of the plausible scene\nlayout distribution. By sampling a noise vector from a standard Gaussian and using the reparameterization trick customary in VAEs, we can draw from the latent distribution and decode it into a metric-scaled depth map. Yet, to choose a particular depth map amongst the many possible, one must rely on the image. This is facilitated by a conditional sampler that predicts the noise vector from the given image in place of the one sampled from a Gaussian to be used in the reparameterization step. Consequently, this substitution enables one to sample the most probable depth map, adhering to the scene arrangement and object shapes observed in the image, from the learned distribution. This naturally lends to an alternating optimization process between the (text-)VAE and conditional sampler.\nIn one alternation, one would predict the mean and standard deviation from the text caption and optimize the text-VAE branch for depth by minimizing a loss with respect to ground truth on the depth map sampled using a standard Gaussian (similar to traditional VAEs). In the other alternation, one would still use the mean and standard deviation predicted by the text-VAE, but instead, use the conditional sampler to “select” a specific depth map compatible with the image, and again, minimize a loss on the output depth. Note: that depending on the alternation, either the text-VAE or the conditional sampler is frozen. At test-time, one no longer needs to sample from the Gaussian and may directly predict depth using the text-VAE with the conditional sampler (see Fig. 2  ###reference_###). In another mode, one may use the text-VAE alone to generate plausible scenes for a given caption.\nOur contributions are as follows: (i) We propose a variational framework that leverages complementary strengths of two inherently ambiguous modalities for monocular depth estimation; we term our approach, WorDepth. (ii) We introduce an image-based conditional sampler that models the use of language as a conditional prior. (iii) We achieve the state-of-the-art on indoor (NYU Depth V2 [58  ###reference_b58###]) and outdoor (KITTI [20  ###reference_b20###]) benchmarks. (iv) To the best of our knowledge, we are the first to treat language as a variational prior for monocular depth estimation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Monocular depth estimation  trains by minimizing loss between depth predictions and ground-truth depth maps [7  ###reference_b7###, 2  ###reference_b2###, 17  ###reference_b17###, 52  ###reference_b52###, 80  ###reference_b80###, 78  ###reference_b78###, 46  ###reference_b46###, 35  ###reference_b35###, 54  ###reference_b54###, 61  ###reference_b61###, 86  ###reference_b86###, 84  ###reference_b84###, 66  ###reference_b66###].\nSpecifically, DORN [16  ###reference_b16###] employs a spacing-increasing discretization strategy for depth estimation as an ordinal regression problem.\nAdaBins [2  ###reference_b2###] introduces a transformer block that segments the depth range into adaptive bins.\nASTransformer [7  ###reference_b7###] incorporates an Attention-based Up-sample Block to enhance detailed texture features.\nDepthFormer [40  ###reference_b40###] employs hierarchical aggregation and heterogeneous interaction modules for effective feature affinity and modeling.\nRPSF [47  ###reference_b47###] presents a differentiable model of the aperture mask.\nHowever, deriving semantics solely from visual cues is challenging because of scale ambiguity and the limited size of fully annotated training datasets. We use language as a prior to ground predictions to metric scale. When ground-truth depth is not available, self-supervised approaches [70  ###reference_b70###, 27  ###reference_b27###, 36  ###reference_b36###, 85  ###reference_b85###, 3  ###reference_b3###, 96  ###reference_b96###, 100  ###reference_b100###, 63  ###reference_b63###, 62  ###reference_b62###, 51  ###reference_b51###, 64  ###reference_b64###, 15  ###reference_b15###, 94  ###reference_b94###] rely on geometric constraints, often established via from various modalities, including lidar [79  ###reference_b79###, 67  ###reference_b67###, 69  ###reference_b69###, 68  ###reference_b68###, 44  ###reference_b44###, 50  ###reference_b50###, 72  ###reference_b72###] and radar [59  ###reference_b59###], or through deliberate design. Arising from training, if done at a large scale, is a prior on the scene that can be exploited for semantic tasks [33  ###reference_b33###]. On the other hand, we consider language as a semantic prior to enhance the effectiveness of monocular depth estimation.\nVariational and generative methods focus on the ambiguous nature of monocular depth estimation, many involving Diffusion or VAE models for modeling this ambiguity [10  ###reference_b10###, 57  ###reference_b57###, 56  ###reference_b56###, 73  ###reference_b73###, 5  ###reference_b5###, 83  ###reference_b83###, 41  ###reference_b41###]. DepthGen [56  ###reference_b56###] uses a depth pre-trained diffusion model, which generates depth estimations conditioned on images, and shows that the model is capable of generating multiple plausible depth maps when depth is ambiguous. DDVM [57  ###reference_b57###] uses a similar approach and designed a training pipeline that can produce both depth maps and optical flow outputs with a diffusion model. [73  ###reference_b73###] trained a VAE model that outputs a probability distribution over scene depth given an image, which can then be combined with additional inputs for more accurate depth estimations. VDN [10  ###reference_b10###] models depth as a distribution with its variance interpreted as uncertainty. The CodeSLAM model [5  ###reference_b5###] also employed a VAE conditioned on image intensities for depth estimation. However, although these work explored the idea of uncertainty in depth estimation, and even combined other modalities of inputs [73  ###reference_b73###], none have experimented with language priors, and most VAE-based approaches use images to obtain the mean of the modeled distribution, which is fundamentally different from WorDepth.\nFoundation models [53  ###reference_b53###, 37  ###reference_b37###, 38  ###reference_b38###, 6  ###reference_b6###, 48  ###reference_b48###, 21  ###reference_b21###, 104  ###reference_b104###, 23  ###reference_b23###, 77  ###reference_b77###, 49  ###reference_b49###, 98  ###reference_b98###] acquire a comprehensive understanding of languages, images, and other data types through pre-training under substantial and diverse datasets, thus forming an effective baseline for downstream tasks [75  ###reference_b75###, 76  ###reference_b76###, 39  ###reference_b39###, 12  ###reference_b12###, 89  ###reference_b89###, 74  ###reference_b74###, 8  ###reference_b8###, 81  ###reference_b81###, 71  ###reference_b71###, 2  ###reference_b2###, 82  ###reference_b82###, 42  ###reference_b42###, 95  ###reference_b95###, 92  ###reference_b92###, 93  ###reference_b93###]. To leverage foundation models for monocular depth estimation, TADP [30  ###reference_b30###] uses captions created by AI to enhance the correlation between text and images in diffusion-based vision models. VPD [97  ###reference_b97###] leverages a diffusion-based pipeline with cross-attention between text and images. Dinov2 [48  ###reference_b48###] trains a ViT [11  ###reference_b11###] with 1B parameters using an automatically built image dataset under contrastive learning objectives.\nUnlike methods that rely on foundation models for feature extraction, WorDepth is potentially more efficient for industrial applications.\nVision-language models are designed to build connections between visual and language inputs.\nCLIP [53  ###reference_b53###] conducts contrastive learning between text-image pairs, empowering various tasks like few-shot image classification  [18  ###reference_b18###, 88  ###reference_b88###, 87  ###reference_b87###, 101  ###reference_b101###], image segmentation  [99  ###reference_b99###, 55  ###reference_b55###], object detection  [103  ###reference_b103###, 55  ###reference_b55###], and 3D perception [90  ###reference_b90###, 105  ###reference_b105###, 91  ###reference_b91###, 26  ###reference_b26###]. In light of the powerful emerging ability brought by recent vision-language models, some works have tried to apply the vision-language model for monocular depth estimation.\nDepthCLIP [91  ###reference_b91###] leverages the semantic depth response of CLIP [53  ###reference_b53###] with a depth projection scheme to conduct zero-shot adaptation from the semantic language response to monocular depth estimation.\nFurthermore, [26  ###reference_b26###] extends DepthCLIP with learnable prompts and depth codebook to narrow the depth domain gap among different scenes.\nLikewise, [1  ###reference_b1###] modifies DepthCLIP [91  ###reference_b91###] using continuous learnable tokens in place of discrete human-language words.\nAdditionally, VPD [97  ###reference_b97###] exploits the high-fidelity embedding of a pre-trained text-to-image diffusion model in monocular depth estimation.\nHowever, existing methods using vision-language models rely on implicit modeling. Conversely, WorDepth explicitly models language as a prior for depth estimation and exploits strong priors regarding the size of objects described in text captions to better ground monocular depth (often scaleless) to metric scale.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Given an RGB image , monocular depth estimation aims to infer a dense depth map  using a parameterized function  realized as a neural network, i.e., . We consider a supervised dataset  with  samples, where  denotes the ground-truth depth map, and t the text caption describing the image."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Text variational auto-encoder",
            "text": "To incorporate language priors to monocular depth estimation, we first design a variational auto-encoder (VAE) to learn the latent distribution of possible depth maps as described by the text caption. This VAE is comprised of the text encoder from a pre-trained vision-language model, CLIP [53  ###reference_b53###], which by default offers a shared latent space between vision and text embeddings, followed by a multi-layer perceptron (MLP) to estimate the mean  and standard deviation  of the latent distribution of plausible scenes based on the text encoding. Note that the CLIP text encoder is frozen at all times and never updated when training WorDepth. Specifically, given a text caption , we first encode it using the CLIP text encoder and estimate the mean and standard deviation as  using a multi-layer perceptron (MLP). To sample from the distribution parameterized by  and , we first draw a noise vector  from a standard Gaussian . Then, we use  to sample from the latent distribution via the reparameterization trick [29  ###reference_b29###], . We refer to this module as a text variational auto-encoder (text-VAE). To generate a depth map  from the sample , we first duplicate  along the horizontal and vertical axes to yield a  latent (choice of design to be discussed below in Sec. 3.2  ###reference_###) and feed it through a depth decoder to yield , where we overload  as the spatially duplicated latent, and  and  denote the height and width of the depth map, preset as hyperparameters to match the desired image dimensions.\nTo train our text-VAE and depth decoder, we minimize\nwith respect to  and , where  is the scale invariant loss (Eq. 3  ###reference_###),  the KL divergence loss (Eq. 4  ###reference_###) as detailed in Section 3.3  ###reference_###, and  the weight of the KL divergence term."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Image-based conditional sampler",
            "text": "While our text-VAE can predict plausible metric-scaled depth maps from text captions, we are interested in the depth map corresponding to a specific image. To do so, we treat text-VAE as the latent prior distribution of the plausible scene layouts. Predicting depth  for a specific image  requires sampling the latent corresponding to the depth map of the 3D scene layout with the highest likelihood to be compatible with the observed image, i.e., prior conditioned on the image. To this end, we introduce an image-based conditional sampler that will predict the sample  in place of  drawn from the standard Gaussian. Using the reparameterization trick as before, we will use  to select the latent vector  to be decoded by the depth decoder.\nSpecifically, our image-based conditional sampler utilizes a Swin-L transformer backbone to encode an image . We chose this design to exploit the locality of the tokens produced by Swin-L. The tokens are then encoded into  number of local samples  to be used to sample from the latent distribution of our text-VAE; in other words, we perform “patch-wise” selection from latent distribution for more granular predictions. To do so, we additionally include  and  as part of its input. We note that  and  have been detached from the computational graph and treated as input. We refer to this module as our conditional sampler , which aims to estimate the most probable latent variable of text-VAE. Thus, the scene layout latent vector is now given by , and the predicted depth . As an implementation detail, we note that skip connections from the encoder  are injected into  by concatenation; when training text-VAE (Sec. 3.1  ###reference_###), feature maps of skip connections are of the same size, but populated with zeros instead.\nTo train the conditional sampler, we minimize the same loss (Eq. 1  ###reference_###) as that of text-VAE:\nwith respect to  and . With a batch size of , the number of  is , while  and  are the sample mean and standard deviation of  over a batch. We impose a KL divergence loss as regularization so that the estimated  does not drift from the standard Gaussian, which also serves to improve training stability."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Training Loss",
            "text": "###figure_3### Scale invariant loss. We minimize a supervised loss using ground truth . To improve training stability over diverse scenes, we use the scale-invariant depth loss [13  ###reference_b13###]:\nwhere ,  denotes the image space,  the number of pixels,  the predicted depth, and  the scaling factor to control the sensitivity of the loss.\nKullback-Leibler (KL) divergence loss. Following [29  ###reference_b29###], we employ the KL Divergence loss as a regularizer, which biases the predicted latent distribution (parameterized by mean  and standard deviation ) towards a standard Gaussian distribution. We apply the Kullback-Leibler divergence loss to  and  as follows:"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Optimizing Wordepth",
            "text": "Training Wordepth involves optimizing text-VAE with our conditional sampler: One may choose to first train text-VAE until convergence (i.e., optimize for ), then freeze , and finally train the image-based conditional sample (i.e., optimize for ). However, we find that doing so often results in the conditional sampler being trapped in a suboptimal local minimum. Moreover, this introduces the inconvenience of an extra stage of training. Instead, we propose an alternating optimization scheme to train the text-VAE with conditional sampler. In one alternating step, we freeze the conditional sampler and train the text-VAE and depth decoder following the procedure in Sec. 3.1  ###reference_###, i.e., predicting  and  from text caption t and using the reparameterization trick with an  drawn from a standard Gaussian to sample the latent vector. In the next alternating step, we freeze text-VAE and train the conditional sampler with the depth decoder following Sec. 3.2  ###reference_###, i.e., predicting  and  using the frozen text-VAE and sample from the latent distribution using  predicted from the image. These alternating steps are repeated with a ratio of  (for optimizing text-VAE) to  (for optimizing the conditional sampler).\nInference. Once trained, we no longer require drawing  from a standard Gaussian. Instead, at test time, the inference step simply follows Sec. 3.2  ###reference_###. In another mode, if one wants to generate depth maps from text captions, one can discard the conditional sampler branch and directly sample from a standard Gaussian instead."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this study, we seek to answer the question of whether language can be used to calibrate the learned scene distribution to true real-world statistics. The answer is yes, which is valuable for\ncircumventing the long-standing problem of scale ambiguity in monocular depth or structure-from-motion problems. The approach is a first in leveraging complementary properties of two modalities with inherent ambiguities for the 3D reconstruction, to address the deficits in one another. We show that by exploiting the layout/scene ambiguity in language as a strength via our variational approach, we can ground predictions to metric scale. This opens up new avenue in how one can address the issue of scale in 3D reconstruction as well as provide a direct framework to extending the many works that currently are limited to relative or scaleless depth predictions.\nMethod\n\n\n\nAbsRel\n\nRMSE\n\n\n\nAdabins\n0.771\n0.944\n0.983\n0.159\n0.068\n0.476\n\nDepthFormer\n0.815\n0.970\n0.993\n0.137\n0.059\n0.408\n\nBaseline\n0.803\n0.965\n0.990\n0.141\n0.062\n0.427\n\nWorDepth\n0.833\n0.976\n0.994\n0.123\n0.054\n0.376\nLimitations. Generic regularizers typically yield little gains, but do little harm; specific regularizers can provide larger boosts but are limited in their applications. While using language as a prior gives flexibility between the two, specificity in the caption controls the degree of regularization imposed. Naturally, vague captions give little to no information on object shape or size, so there is little to be gained; specific, but incorrect captions may misfire, barring any malicious intent. As WorDepth relies on the quality of the caption, it is susceptible to inaccuracies stemming from descriptions provided by the image captioner. Its ease of use also opens up vulnerabilities from malicious users who may choose captions to steer predictions incorrectly.\nAcknowledgements. This work was supported by NSF 2112562 Athena AI Institute."
        }
    ]
}